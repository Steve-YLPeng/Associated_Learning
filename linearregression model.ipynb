{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "06dd4036",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.optim import optimizer\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 0) prepare data\n",
    "feature_numpy, target_numpy = datasets.make_regression(n_samples=100, n_features=1, noise=20, random_state=1234)\n",
    "\n",
    "feature = torch.from_numpy(feature_numpy.astype(np.float32))\n",
    "target = torch.from_numpy(target_numpy.astype(np.float32))\n",
    "target = target.view(target.shape[0], 1)\n",
    "\n",
    "n_samples, n_features = feature.shape\n",
    "\n",
    "# 1) model\n",
    "class LinearRegression(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        \n",
    "        # define layers\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "    \n",
    "        return self.linear(x)\n",
    "\n",
    "model = LinearRegression(n_features, 1)\n",
    "\n",
    "# 2) loss and optimizer\n",
    "learning_rate = 0.01\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 3) training loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    # forward pass and loss\n",
    "    y_predicted = model(feature)\n",
    "    loss = criterion(y_predicted, target)\n",
    "\n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    optimizer.step()\n",
    "    \n",
    "    # init optimizer\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'epoch: {epoch+1}, loss = {loss.item(): .4f}')\n",
    "\n",
    "# show in image\n",
    "predicted = model(feature).detach().numpy()\n",
    "plt.plot(feature_numpy, target_numpy, 'ro')\n",
    "plt.plot(feature_numpy, predicted, 'b')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b0f28f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[col_sparse] = df[col_sparse].fillna('-1', )\n",
    "df[col_dense] = df[col_dense].fillna(0,)\n",
    "\n",
    "for feat in col_sparse:\n",
    "    lbe = LabelEncoder()\n",
    "    df[feat] = lbe.fit_transform(df[feat])\n",
    "    \n",
    "mms = MinMaxScaler(feature_range=(0,1))\n",
    "df[col_dense] = mms.fit_transform(df[col_dense])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8f78ad47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>I1</th>\n",
       "      <th>I2</th>\n",
       "      <th>I3</th>\n",
       "      <th>I4</th>\n",
       "      <th>I5</th>\n",
       "      <th>I6</th>\n",
       "      <th>I7</th>\n",
       "      <th>I8</th>\n",
       "      <th>I9</th>\n",
       "      <th>...</th>\n",
       "      <th>C17</th>\n",
       "      <th>C18</th>\n",
       "      <th>C19</th>\n",
       "      <th>C20</th>\n",
       "      <th>C21</th>\n",
       "      <th>C22</th>\n",
       "      <th>C23</th>\n",
       "      <th>C24</th>\n",
       "      <th>C25</th>\n",
       "      <th>C26</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.004673</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.001312</td>\n",
       "      <td>0.010714</td>\n",
       "      <td>6.180042e-07</td>\n",
       "      <td>0.000247</td>\n",
       "      <td>0.000603</td>\n",
       "      <td>0.002933</td>\n",
       "      <td>0.000409</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>81</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>4231</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1044</td>\n",
       "      <td>33</td>\n",
       "      <td>580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.009346</td>\n",
       "      <td>0.027046</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.007143</td>\n",
       "      <td>6.180042e-06</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.004222</td>\n",
       "      <td>0.002933</td>\n",
       "      <td>0.001227</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>564</td>\n",
       "      <td>418</td>\n",
       "      <td>1</td>\n",
       "      <td>1105</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1070</td>\n",
       "      <td>32</td>\n",
       "      <td>887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000324</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>1.180388e-04</td>\n",
       "      <td>0.025150</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030792</td>\n",
       "      <td>0.002727</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>180</td>\n",
       "      <td>546</td>\n",
       "      <td>3</td>\n",
       "      <td>3869</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2044</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002159</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>1.782324e-03</td>\n",
       "      <td>0.012657</td>\n",
       "      <td>0.019300</td>\n",
       "      <td>0.019062</td>\n",
       "      <td>0.163463</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>170</td>\n",
       "      <td>546</td>\n",
       "      <td>3</td>\n",
       "      <td>5036</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2524</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>0.000259</td>\n",
       "      <td>0.110714</td>\n",
       "      <td>1.854013e-05</td>\n",
       "      <td>0.016274</td>\n",
       "      <td>0.001206</td>\n",
       "      <td>0.055718</td>\n",
       "      <td>0.009134</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1069</td>\n",
       "      <td>546</td>\n",
       "      <td>3</td>\n",
       "      <td>2258</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1741</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>0</td>\n",
       "      <td>0.023364</td>\n",
       "      <td>0.003347</td>\n",
       "      <td>0.000748</td>\n",
       "      <td>0.092857</td>\n",
       "      <td>3.380483e-04</td>\n",
       "      <td>0.005425</td>\n",
       "      <td>0.003016</td>\n",
       "      <td>0.038123</td>\n",
       "      <td>0.003545</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>528</td>\n",
       "      <td>63</td>\n",
       "      <td>2</td>\n",
       "      <td>1209</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1953</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000216</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.721072e-03</td>\n",
       "      <td>0.009863</td>\n",
       "      <td>0.001206</td>\n",
       "      <td>0.013196</td>\n",
       "      <td>0.021541</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>226</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>3048</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1704</td>\n",
       "      <td>1</td>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003347</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.491570e-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>691</td>\n",
       "      <td>89</td>\n",
       "      <td>0</td>\n",
       "      <td>1691</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1583</td>\n",
       "      <td>0</td>\n",
       "      <td>1675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013118</td>\n",
       "      <td>0.002701</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>1.382475e-03</td>\n",
       "      <td>0.004192</td>\n",
       "      <td>0.007238</td>\n",
       "      <td>0.060117</td>\n",
       "      <td>0.006953</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>862</td>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>2832</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>274</td>\n",
       "      <td>23</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>1.891711e-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035191</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>187</td>\n",
       "      <td>546</td>\n",
       "      <td>3</td>\n",
       "      <td>2258</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1741</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label        I1        I2        I3        I4            I5        I6  \\\n",
       "0         0  0.004673  0.000108  0.001312  0.010714  6.180042e-07  0.000247   \n",
       "1         1  0.009346  0.027046  0.000031  0.007143  6.180042e-06  0.000164   \n",
       "2         0  0.000000  0.000324  0.000000  0.028571  1.180388e-04  0.025150   \n",
       "3         0  0.000000  0.002159  0.000046  0.014286  1.782324e-03  0.012657   \n",
       "4         0  0.000000  0.000162  0.000259  0.110714  1.854013e-05  0.016274   \n",
       "...     ...       ...       ...       ...       ...           ...       ...   \n",
       "9995      0  0.023364  0.003347  0.000748  0.092857  3.380483e-04  0.005425   \n",
       "9996      0  0.000000  0.000216  0.000046  0.000000  2.721072e-03  0.009863   \n",
       "9997      0  0.000000  0.003347  0.000046  0.000000  7.491570e-02  0.000000   \n",
       "9998      0  0.000000  0.013118  0.002701  0.125000  1.382475e-03  0.004192   \n",
       "9999      0  0.000000  0.000270  0.000000  0.035714  1.891711e-02  0.000000   \n",
       "\n",
       "            I7        I8        I9  ...  C17   C18  C19  C20   C21  C22  C23  \\\n",
       "0     0.000603  0.002933  0.000409  ...    7    81   63    0  4231    7    0   \n",
       "1     0.004222  0.002933  0.001227  ...    8   564  418    1  1105    7    1   \n",
       "2     0.000000  0.030792  0.002727  ...    7   180  546    3  3869    5    0   \n",
       "3     0.019300  0.019062  0.163463  ...    3   170  546    3  5036    5    1   \n",
       "4     0.001206  0.055718  0.009134  ...    7  1069  546    3  2258    7    1   \n",
       "...        ...       ...       ...  ...  ...   ...  ...  ...   ...  ...  ...   \n",
       "9995  0.003016  0.038123  0.003545  ...    0   528   63    2  1209    3    1   \n",
       "9996  0.001206  0.013196  0.021541  ...    0   226   63    0  3048    7    0   \n",
       "9997  0.000000  0.000000  0.000000  ...    0   691   89    0  1691    7    1   \n",
       "9998  0.007238  0.060117  0.006953  ...    8   862   55    1  2832    7    0   \n",
       "9999  0.000000  0.035191  0.000136  ...    0   187  546    3  2258    7    2   \n",
       "\n",
       "       C24  C25   C26  \n",
       "0     1044   33   580  \n",
       "1     1070   32   887  \n",
       "2     2044   39     0  \n",
       "3     2524   39     0  \n",
       "4     1741   39     0  \n",
       "...    ...  ...   ...  \n",
       "9995  1953    1     2  \n",
       "9996  1704    1   147  \n",
       "9997  1583    0  1675  \n",
       "9998   274   23   165  \n",
       "9999  1741   39     0  \n",
       "\n",
       "[10000 rows x 40 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad319671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(635, 125)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_target = data.columns[:6]\n",
    "col_feature1 = data.columns[6:33].to_list() # 27 cols\n",
    "col_feature2 = data.columns[33:43].to_list() # 10 cols\n",
    "col_feature3 = data.columns[43:103].to_list() # 60 cols\n",
    "col_feature4 = data.columns[103:].to_list() # 28 cols\n",
    "y = data[col_target]\n",
    "x = data[col_feature1 + col_feature2 + col_feature3 + col_feature4]\n",
    "x = x.fillna(0)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2f394a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 92, 100,  78,  78,  84,  68])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_train, clean_test, train_label, test_label = train_test_split(x, y, test_size=0.2)\n",
    "train_label.to_numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e22348e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensor_point5_i_value     0\n",
      "sensor_point6_i_value     0\n",
      "sensor_point7_i_value     0\n",
      "sensor_point8_i_value     0\n",
      "sensor_point9_i_value     0\n",
      "sensor_point10_i_value    0\n",
      "dtype: int64\n",
      "sensor_point5_i_value     0\n",
      "sensor_point6_i_value     0\n",
      "sensor_point7_i_value     0\n",
      "sensor_point8_i_value     0\n",
      "sensor_point9_i_value     0\n",
      "sensor_point10_i_value    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print((y == 0).sum())\n",
    "print((y.isna()).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57f55723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      22\n",
      "238     1\n",
      "286     3\n",
      "621     1\n",
      "dtype: int64 \n",
      "\n",
      "0    10\n",
      "dtype: int64 \n",
      "\n",
      "0      20\n",
      "16      2\n",
      "18      3\n",
      "21      5\n",
      "131     5\n",
      "162    10\n",
      "164     3\n",
      "167     1\n",
      "171     1\n",
      "573    10\n",
      "dtype: int64 \n",
      "\n",
      "0      15\n",
      "27      5\n",
      "65      5\n",
      "111     1\n",
      "117     1\n",
      "579     1\n",
      "dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for col in [col_feature1,col_feature2,col_feature3,col_feature4]:\n",
    "    print((x[col]==0).sum(axis=0).value_counts().sort_index(),\"\\n\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d93b4abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    321\n",
      "2     28\n",
      "3     14\n",
      "4     62\n",
      "5    210\n",
      "dtype: int64 \n",
      "\n",
      "0    635\n",
      "dtype: int64 \n",
      "\n",
      "0      62\n",
      "10    345\n",
      "11      4\n",
      "12      3\n",
      "15     33\n",
      "20     69\n",
      "25      3\n",
      "30    109\n",
      "38      2\n",
      "40      5\n",
      "dtype: int64 \n",
      "\n",
      "0     52\n",
      "1    380\n",
      "2     86\n",
      "3     25\n",
      "6      4\n",
      "7     88\n",
      "dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for col in [col_feature1,col_feature2,col_feature3,col_feature4]:\n",
    "    print((x[col]==0).sum(axis=1).value_counts().sort_index(),\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bbac891a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]/home/AL_main_new/utils.py:99: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  x = torch.tensor([x for x,y in batch], dtype=torch.float32)\n",
      "Train 0 | out_loss 79.63187408447266: 100%|███████| 8/8 [00:00<00:00, 20.38it/s]\n",
      "Train 1 | out_loss 79.25166320800781: 100%|██████| 8/8 [00:00<00:00, 695.60it/s]\n",
      "Train 2 | out_loss 78.55249786376953: 100%|██████| 8/8 [00:00<00:00, 633.23it/s]\n",
      "Train 3 | out_loss 78.12552642822266: 100%|██████| 8/8 [00:00<00:00, 704.66it/s]\n",
      "Train 4 | out_loss 77.69197082519531: 100%|██████| 8/8 [00:00<00:00, 641.07it/s]\n",
      "Train 5 | out_loss 77.02802276611328: 100%|██████| 8/8 [00:00<00:00, 704.56it/s]\n",
      "Train 6 | out_loss 76.23699951171875: 100%|██████| 8/8 [00:00<00:00, 699.82it/s]\n",
      "Train 7 | out_loss 75.69731140136719: 100%|██████| 8/8 [00:00<00:00, 703.52it/s]\n",
      "Train 8 | out_loss 75.17314147949219: 100%|██████| 8/8 [00:00<00:00, 696.66it/s]\n",
      "Train 9 | out_loss 74.6423110961914: 100%|███████| 8/8 [00:00<00:00, 709.26it/s]\n",
      "Train 10 | out_loss 73.97625732421875: 100%|█████| 8/8 [00:00<00:00, 704.51it/s]\n",
      "Train 11 | out_loss 73.28217315673828: 100%|█████| 8/8 [00:00<00:00, 705.46it/s]\n",
      "Train 12 | out_loss 72.73091125488281: 100%|█████| 8/8 [00:00<00:00, 720.39it/s]\n",
      "Train 13 | out_loss 72.18336486816406: 100%|█████| 8/8 [00:00<00:00, 718.48it/s]\n",
      "Train 14 | out_loss 71.63249969482422: 100%|█████| 8/8 [00:00<00:00, 714.71it/s]\n",
      "Train 15 | out_loss 71.08665466308594: 100%|█████| 8/8 [00:00<00:00, 714.14it/s]\n",
      "Train 16 | out_loss 70.54080963134766: 100%|█████| 8/8 [00:00<00:00, 718.23it/s]\n",
      "Train 17 | out_loss 69.99604797363281: 100%|█████| 8/8 [00:00<00:00, 708.99it/s]\n",
      "Train 18 | out_loss 69.44864654541016: 100%|█████| 8/8 [00:00<00:00, 718.56it/s]\n",
      "Train 19 | out_loss 68.90335083007812: 100%|█████| 8/8 [00:00<00:00, 712.14it/s]\n",
      "Train 20 | out_loss 68.35618591308594: 100%|█████| 8/8 [00:00<00:00, 723.37it/s]\n",
      "Train 21 | out_loss 67.81575775146484: 100%|█████| 8/8 [00:00<00:00, 464.24it/s]\n",
      "Train 22 | out_loss 67.27030181884766: 100%|█████| 8/8 [00:00<00:00, 534.80it/s]\n",
      "Train 23 | out_loss 66.72710418701172: 100%|█████| 8/8 [00:00<00:00, 678.24it/s]\n",
      "Train 24 | out_loss 66.18389129638672: 100%|█████| 8/8 [00:00<00:00, 690.93it/s]\n",
      "Train 25 | out_loss 65.64099884033203: 100%|█████| 8/8 [00:00<00:00, 692.46it/s]\n",
      "Train 26 | out_loss 65.10099792480469: 100%|█████| 8/8 [00:00<00:00, 646.40it/s]\n",
      "Train 27 | out_loss 64.55370330810547: 100%|█████| 8/8 [00:00<00:00, 707.12it/s]\n",
      "Train 28 | out_loss 64.01380920410156: 100%|█████| 8/8 [00:00<00:00, 631.96it/s]\n",
      "Train 29 | out_loss 63.47240447998047: 100%|█████| 8/8 [00:00<00:00, 686.48it/s]\n",
      "Train 30 | out_loss 62.92677688598633: 100%|█████| 8/8 [00:00<00:00, 708.66it/s]\n",
      "Train 31 | out_loss 62.3917236328125: 100%|██████| 8/8 [00:00<00:00, 708.33it/s]\n",
      "Train 32 | out_loss 61.84816360473633: 100%|█████| 8/8 [00:00<00:00, 709.28it/s]\n",
      "Train 33 | out_loss 61.30694580078125: 100%|█████| 8/8 [00:00<00:00, 706.23it/s]\n",
      "Train 34 | out_loss 60.76362991333008: 100%|█████| 8/8 [00:00<00:00, 710.67it/s]\n",
      "Train 35 | out_loss 60.227989196777344: 100%|████| 8/8 [00:00<00:00, 708.08it/s]\n",
      "Train 36 | out_loss 59.68289566040039: 100%|█████| 8/8 [00:00<00:00, 704.90it/s]\n",
      "Train 37 | out_loss 59.14426040649414: 100%|█████| 8/8 [00:00<00:00, 707.88it/s]\n",
      "Train 38 | out_loss 58.60676193237305: 100%|█████| 8/8 [00:00<00:00, 711.89it/s]\n",
      "Train 39 | out_loss 58.06916046142578: 100%|█████| 8/8 [00:00<00:00, 699.47it/s]\n",
      "Train 40 | out_loss 57.5296630859375: 100%|██████| 8/8 [00:00<00:00, 707.72it/s]\n",
      "Train 41 | out_loss 56.98972702026367: 100%|█████| 8/8 [00:00<00:00, 705.93it/s]\n",
      "Train 42 | out_loss 56.4539680480957: 100%|██████| 8/8 [00:00<00:00, 708.42it/s]\n",
      "Train 43 | out_loss 55.911231994628906: 100%|████| 8/8 [00:00<00:00, 702.78it/s]\n",
      "Train 44 | out_loss 55.37491989135742: 100%|█████| 8/8 [00:00<00:00, 703.62it/s]\n",
      "Train 45 | out_loss 54.842037200927734: 100%|████| 8/8 [00:00<00:00, 706.22it/s]\n",
      "Train 46 | out_loss 54.30598449707031: 100%|█████| 8/8 [00:00<00:00, 704.53it/s]\n",
      "Train 47 | out_loss 53.7697868347168: 100%|██████| 8/8 [00:00<00:00, 702.81it/s]\n",
      "Train 48 | out_loss 53.233177185058594: 100%|████| 8/8 [00:00<00:00, 707.03it/s]\n",
      "Train 49 | out_loss 52.700439453125: 100%|███████| 8/8 [00:00<00:00, 719.23it/s]\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]/home/AL_main_new/utils.py:99: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  x = torch.tensor([x for x,y in batch], dtype=torch.float32)\n",
      "Train 0 | out_loss 80.1839370727539: 100%|████████| 8/8 [00:00<00:00, 20.12it/s]\n",
      "Train 1 | out_loss 79.67149353027344: 100%|██████| 8/8 [00:00<00:00, 428.14it/s]\n",
      "Train 2 | out_loss 78.92416381835938: 100%|██████| 8/8 [00:00<00:00, 443.49it/s]\n",
      "Train 3 | out_loss 78.22335815429688: 100%|██████| 8/8 [00:00<00:00, 441.58it/s]\n",
      "Train 4 | out_loss 77.4666519165039: 100%|███████| 8/8 [00:00<00:00, 447.09it/s]\n",
      "Train 5 | out_loss 76.669921875: 100%|███████████| 8/8 [00:00<00:00, 445.53it/s]\n",
      "Train 6 | out_loss 75.89876556396484: 100%|██████| 8/8 [00:00<00:00, 451.51it/s]\n",
      "Train 7 | out_loss 75.08525848388672: 100%|██████| 8/8 [00:00<00:00, 448.61it/s]\n",
      "Train 8 | out_loss 74.25202941894531: 100%|██████| 8/8 [00:00<00:00, 440.42it/s]\n",
      "Train 9 | out_loss 73.59396362304688: 100%|██████| 8/8 [00:00<00:00, 448.85it/s]\n",
      "Train 10 | out_loss 72.81153869628906: 100%|█████| 8/8 [00:00<00:00, 444.61it/s]\n",
      "Train 11 | out_loss 71.83638000488281: 100%|█████| 8/8 [00:00<00:00, 451.74it/s]\n",
      "Train 12 | out_loss 70.8773193359375: 100%|██████| 8/8 [00:00<00:00, 438.42it/s]\n",
      "Train 13 | out_loss 70.07412719726562: 100%|█████| 8/8 [00:00<00:00, 452.14it/s]\n",
      "Train 14 | out_loss 69.27417755126953: 100%|█████| 8/8 [00:00<00:00, 445.17it/s]\n",
      "Train 15 | out_loss 68.60704040527344: 100%|█████| 8/8 [00:00<00:00, 440.95it/s]\n",
      "Train 16 | out_loss 67.88280487060547: 100%|█████| 8/8 [00:00<00:00, 448.79it/s]\n",
      "Train 17 | out_loss 67.0840072631836: 100%|██████| 8/8 [00:00<00:00, 442.89it/s]\n",
      "Train 18 | out_loss 66.30034637451172: 100%|█████| 8/8 [00:00<00:00, 451.92it/s]\n",
      "Train 19 | out_loss 65.51168823242188: 100%|█████| 8/8 [00:00<00:00, 445.73it/s]\n",
      "Train 20 | out_loss 64.74617004394531: 100%|█████| 8/8 [00:00<00:00, 448.23it/s]\n",
      "Train 21 | out_loss 63.93751907348633: 100%|█████| 8/8 [00:00<00:00, 449.68it/s]\n",
      "Train 22 | out_loss 63.1587028503418: 100%|██████| 8/8 [00:00<00:00, 442.76it/s]\n",
      "Train 23 | out_loss 62.37384033203125: 100%|█████| 8/8 [00:00<00:00, 447.77it/s]\n",
      "Train 24 | out_loss 61.589019775390625: 100%|████| 8/8 [00:00<00:00, 443.07it/s]\n",
      "Train 25 | out_loss 60.80298614501953: 100%|█████| 8/8 [00:00<00:00, 451.61it/s]\n",
      "Train 26 | out_loss 60.02083206176758: 100%|█████| 8/8 [00:00<00:00, 428.70it/s]\n",
      "Train 27 | out_loss 59.23554229736328: 100%|█████| 8/8 [00:00<00:00, 447.53it/s]\n",
      "Train 28 | out_loss 58.45291519165039: 100%|█████| 8/8 [00:00<00:00, 442.61it/s]\n",
      "Train 29 | out_loss 57.674530029296875: 100%|████| 8/8 [00:00<00:00, 442.05it/s]\n",
      "Train 30 | out_loss 56.89305114746094: 100%|█████| 8/8 [00:00<00:00, 448.06it/s]\n",
      "Train 31 | out_loss 56.114540100097656: 100%|████| 8/8 [00:00<00:00, 439.47it/s]\n",
      "Train 32 | out_loss 55.335609436035156: 100%|████| 8/8 [00:00<00:00, 451.61it/s]\n",
      "Train 33 | out_loss 54.55278778076172: 100%|█████| 8/8 [00:00<00:00, 439.74it/s]\n",
      "Train 34 | out_loss 53.77313995361328: 100%|█████| 8/8 [00:00<00:00, 442.83it/s]\n",
      "Train 35 | out_loss 52.997188568115234: 100%|████| 8/8 [00:00<00:00, 448.38it/s]\n",
      "Train 36 | out_loss 52.225120544433594: 100%|████| 8/8 [00:00<00:00, 447.61it/s]\n",
      "Train 37 | out_loss 51.45038604736328: 100%|█████| 8/8 [00:00<00:00, 453.63it/s]\n",
      "Train 38 | out_loss 50.671749114990234: 100%|████| 8/8 [00:00<00:00, 447.17it/s]\n",
      "Train 39 | out_loss 49.89935302734375: 100%|█████| 8/8 [00:00<00:00, 453.89it/s]\n",
      "Train 40 | out_loss 49.128543853759766: 100%|████| 8/8 [00:00<00:00, 447.31it/s]\n",
      "Train 41 | out_loss 48.356048583984375: 100%|████| 8/8 [00:00<00:00, 451.19it/s]\n",
      "Train 42 | out_loss 47.58466720581055: 100%|█████| 8/8 [00:00<00:00, 448.34it/s]\n",
      "Train 43 | out_loss 46.814754486083984: 100%|████| 8/8 [00:00<00:00, 449.02it/s]\n",
      "Train 44 | out_loss 46.049095153808594: 100%|████| 8/8 [00:00<00:00, 451.26it/s]\n",
      "Train 45 | out_loss 45.27891540527344: 100%|█████| 8/8 [00:00<00:00, 442.27it/s]\n",
      "Train 46 | out_loss 44.51369857788086: 100%|█████| 8/8 [00:00<00:00, 448.31it/s]\n",
      "Train 47 | out_loss 43.74678039550781: 100%|█████| 8/8 [00:00<00:00, 368.69it/s]\n",
      "Train 48 | out_loss 42.99005126953125: 100%|█████| 8/8 [00:00<00:00, 429.74it/s]\n",
      "Train 49 | out_loss 42.22278594970703: 100%|█████| 8/8 [00:00<00:00, 420.52it/s]\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]/home/AL_main_new/utils.py:99: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  x = torch.tensor([x for x,y in batch], dtype=torch.float32)\n",
      "Train 0 | out_loss 80.209716796875: 100%|█████████| 8/8 [00:00<00:00, 20.22it/s]\n",
      "Train 1 | out_loss 78.41635131835938: 100%|██████| 8/8 [00:00<00:00, 331.80it/s]\n",
      "Train 2 | out_loss 76.33370208740234: 100%|██████| 8/8 [00:00<00:00, 323.90it/s]\n",
      "Train 3 | out_loss 74.88867950439453: 100%|██████| 8/8 [00:00<00:00, 329.79it/s]\n",
      "Train 4 | out_loss 73.07209014892578: 100%|██████| 8/8 [00:00<00:00, 331.96it/s]\n",
      "Train 5 | out_loss 71.3323745727539: 100%|███████| 8/8 [00:00<00:00, 323.34it/s]\n",
      "Train 6 | out_loss 69.38390350341797: 100%|██████| 8/8 [00:00<00:00, 313.26it/s]\n",
      "Train 7 | out_loss 67.72940826416016: 100%|██████| 8/8 [00:00<00:00, 329.46it/s]\n",
      "Train 8 | out_loss 66.05982208251953: 100%|██████| 8/8 [00:00<00:00, 327.69it/s]\n",
      "Train 9 | out_loss 64.23028564453125: 100%|██████| 8/8 [00:00<00:00, 329.05it/s]\n",
      "Train 10 | out_loss 63.09996032714844: 100%|█████| 8/8 [00:00<00:00, 329.26it/s]\n",
      "Train 11 | out_loss 61.79642105102539: 100%|█████| 8/8 [00:00<00:00, 327.74it/s]\n",
      "Train 12 | out_loss 60.281192779541016: 100%|████| 8/8 [00:00<00:00, 328.97it/s]\n",
      "Train 13 | out_loss 58.84437561035156: 100%|█████| 8/8 [00:00<00:00, 315.77it/s]\n",
      "Train 14 | out_loss 57.55911636352539: 100%|█████| 8/8 [00:00<00:00, 329.86it/s]\n",
      "Train 15 | out_loss 55.30004119873047: 100%|█████| 8/8 [00:00<00:00, 325.78it/s]\n",
      "Train 16 | out_loss 53.536865234375: 100%|███████| 8/8 [00:00<00:00, 307.44it/s]\n",
      "Train 17 | out_loss 51.67790603637695: 100%|█████| 8/8 [00:00<00:00, 329.92it/s]\n",
      "Train 18 | out_loss 49.964290618896484: 100%|████| 8/8 [00:00<00:00, 319.16it/s]\n",
      "Train 19 | out_loss 48.405517578125: 100%|███████| 8/8 [00:00<00:00, 333.78it/s]\n",
      "Train 20 | out_loss 46.795196533203125: 100%|████| 8/8 [00:00<00:00, 321.71it/s]\n",
      "Train 21 | out_loss 45.20578384399414: 100%|█████| 8/8 [00:00<00:00, 329.82it/s]\n",
      "Train 22 | out_loss 43.82072448730469: 100%|█████| 8/8 [00:00<00:00, 331.69it/s]\n",
      "Train 23 | out_loss 42.397579193115234: 100%|████| 8/8 [00:00<00:00, 328.68it/s]\n",
      "Train 24 | out_loss 40.49637222290039: 100%|█████| 8/8 [00:00<00:00, 315.32it/s]\n",
      "Train 25 | out_loss 38.947078704833984: 100%|████| 8/8 [00:00<00:00, 304.85it/s]\n",
      "Train 26 | out_loss 37.39653396606445: 100%|█████| 8/8 [00:00<00:00, 275.86it/s]\n",
      "Train 27 | out_loss 35.851104736328125: 100%|████| 8/8 [00:00<00:00, 310.46it/s]\n",
      "Train 28 | out_loss 34.33168411254883: 100%|█████| 8/8 [00:00<00:00, 293.84it/s]\n",
      "Train 29 | out_loss 32.82575607299805: 100%|█████| 8/8 [00:00<00:00, 316.13it/s]\n",
      "Train 30 | out_loss 31.304658889770508: 100%|████| 8/8 [00:00<00:00, 323.88it/s]\n",
      "Train 31 | out_loss 29.852407455444336: 100%|████| 8/8 [00:00<00:00, 330.66it/s]\n",
      "Train 32 | out_loss 28.386613845825195: 100%|████| 8/8 [00:00<00:00, 330.03it/s]\n",
      "Train 33 | out_loss 26.939476013183594: 100%|████| 8/8 [00:00<00:00, 330.19it/s]\n",
      "Train 34 | out_loss 25.519277572631836: 100%|████| 8/8 [00:00<00:00, 332.97it/s]\n",
      "Train 35 | out_loss 24.14809799194336: 100%|█████| 8/8 [00:00<00:00, 326.50it/s]\n",
      "Train 36 | out_loss 22.798662185668945: 100%|████| 8/8 [00:00<00:00, 329.99it/s]\n",
      "Train 37 | out_loss 21.497499465942383: 100%|████| 8/8 [00:00<00:00, 324.04it/s]\n",
      "Train 38 | out_loss 20.252208709716797: 100%|████| 8/8 [00:00<00:00, 328.61it/s]\n",
      "Train 39 | out_loss 19.0393123626709: 100%|██████| 8/8 [00:00<00:00, 313.10it/s]\n",
      "Train 40 | out_loss 17.936738967895508: 100%|████| 8/8 [00:00<00:00, 285.65it/s]\n",
      "Train 41 | out_loss 16.878198623657227: 100%|████| 8/8 [00:00<00:00, 248.62it/s]\n",
      "Train 42 | out_loss 15.98067855834961: 100%|█████| 8/8 [00:00<00:00, 294.79it/s]\n",
      "Train 43 | out_loss 15.143105506896973: 100%|████| 8/8 [00:00<00:00, 325.39it/s]\n",
      "Train 44 | out_loss 14.48554801940918: 100%|█████| 8/8 [00:00<00:00, 329.37it/s]\n",
      "Train 45 | out_loss 13.999011993408203: 100%|████| 8/8 [00:00<00:00, 329.37it/s]\n",
      "Train 46 | out_loss 13.713170051574707: 100%|████| 8/8 [00:00<00:00, 329.91it/s]\n",
      "Train 47 | out_loss 13.566171646118164: 100%|████| 8/8 [00:00<00:00, 328.11it/s]\n",
      "Train 48 | out_loss 13.668951988220215: 100%|████| 8/8 [00:00<00:00, 333.09it/s]\n",
      "Train 49 | out_loss 13.953893661499023: 100%|████| 8/8 [00:00<00:00, 336.34it/s]\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]/home/AL_main_new/utils.py:99: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  x = torch.tensor([x for x,y in batch], dtype=torch.float32)\n",
      "Train 0 | out_loss 80.70198822021484: 100%|███████| 8/8 [00:00<00:00, 20.20it/s]\n",
      "Train 1 | out_loss 79.37141418457031: 100%|██████| 8/8 [00:00<00:00, 261.25it/s]\n",
      "Train 2 | out_loss 76.00825500488281: 100%|██████| 8/8 [00:00<00:00, 249.21it/s]\n",
      "Train 3 | out_loss 74.83383178710938: 100%|██████| 8/8 [00:00<00:00, 261.77it/s]\n",
      "Train 4 | out_loss 73.23185729980469: 100%|██████| 8/8 [00:00<00:00, 254.42it/s]\n",
      "Train 5 | out_loss 70.96790313720703: 100%|██████| 8/8 [00:00<00:00, 261.21it/s]\n",
      "Train 6 | out_loss 69.69562530517578: 100%|██████| 8/8 [00:00<00:00, 257.83it/s]\n",
      "Train 7 | out_loss 68.27301025390625: 100%|██████| 8/8 [00:00<00:00, 265.61it/s]\n",
      "Train 8 | out_loss 66.56065368652344: 100%|██████| 8/8 [00:00<00:00, 258.31it/s]\n",
      "Train 9 | out_loss 64.94966888427734: 100%|██████| 8/8 [00:00<00:00, 262.80it/s]\n",
      "Train 10 | out_loss 62.722476959228516: 100%|████| 8/8 [00:00<00:00, 263.97it/s]\n",
      "Train 11 | out_loss 60.365726470947266: 100%|████| 8/8 [00:00<00:00, 264.49it/s]\n",
      "Train 12 | out_loss 58.46863555908203: 100%|█████| 8/8 [00:00<00:00, 262.97it/s]\n",
      "Train 13 | out_loss 57.29116439819336: 100%|█████| 8/8 [00:00<00:00, 253.01it/s]\n",
      "Train 14 | out_loss 55.8953857421875: 100%|██████| 8/8 [00:00<00:00, 260.49it/s]\n",
      "Train 15 | out_loss 54.10263442993164: 100%|█████| 8/8 [00:00<00:00, 259.71it/s]\n",
      "Train 16 | out_loss 52.48109817504883: 100%|█████| 8/8 [00:00<00:00, 241.39it/s]\n",
      "Train 17 | out_loss 50.859710693359375: 100%|████| 8/8 [00:00<00:00, 261.19it/s]\n",
      "Train 18 | out_loss 49.17475128173828: 100%|█████| 8/8 [00:00<00:00, 260.84it/s]\n",
      "Train 19 | out_loss 47.44289779663086: 100%|█████| 8/8 [00:00<00:00, 263.43it/s]\n",
      "Train 20 | out_loss 46.02058792114258: 100%|█████| 8/8 [00:00<00:00, 259.60it/s]\n",
      "Train 21 | out_loss 43.9930534362793: 100%|██████| 8/8 [00:00<00:00, 258.02it/s]\n",
      "Train 22 | out_loss 42.46625900268555: 100%|█████| 8/8 [00:00<00:00, 251.89it/s]\n",
      "Train 23 | out_loss 40.730491638183594: 100%|████| 8/8 [00:00<00:00, 246.78it/s]\n",
      "Train 24 | out_loss 39.2940673828125: 100%|██████| 8/8 [00:00<00:00, 253.93it/s]\n",
      "Train 25 | out_loss 38.9849853515625: 100%|██████| 8/8 [00:00<00:00, 264.94it/s]\n",
      "Train 26 | out_loss 37.07102966308594: 100%|█████| 8/8 [00:00<00:00, 261.55it/s]\n",
      "Train 27 | out_loss 36.00474548339844: 100%|█████| 8/8 [00:00<00:00, 236.57it/s]\n",
      "Train 28 | out_loss 34.03815460205078: 100%|█████| 8/8 [00:00<00:00, 257.86it/s]\n",
      "Train 29 | out_loss 32.811214447021484: 100%|████| 8/8 [00:00<00:00, 258.10it/s]\n",
      "Train 30 | out_loss 31.94162940979004: 100%|█████| 8/8 [00:00<00:00, 238.77it/s]\n",
      "Train 31 | out_loss 30.253469467163086: 100%|████| 8/8 [00:00<00:00, 240.10it/s]\n",
      "Train 32 | out_loss 30.13692855834961: 100%|█████| 8/8 [00:00<00:00, 241.46it/s]\n",
      "Train 33 | out_loss 29.37782859802246: 100%|█████| 8/8 [00:00<00:00, 251.41it/s]\n",
      "Train 34 | out_loss 27.908700942993164: 100%|████| 8/8 [00:00<00:00, 246.52it/s]\n",
      "Train 35 | out_loss 26.49312973022461: 100%|█████| 8/8 [00:00<00:00, 226.42it/s]\n",
      "Train 36 | out_loss 25.005998611450195: 100%|████| 8/8 [00:00<00:00, 262.06it/s]\n",
      "Train 37 | out_loss 21.362024307250977: 100%|████| 8/8 [00:00<00:00, 261.04it/s]\n",
      "Train 38 | out_loss 21.59775161743164: 100%|█████| 8/8 [00:00<00:00, 263.73it/s]\n",
      "Train 39 | out_loss 20.24393081665039: 100%|█████| 8/8 [00:00<00:00, 261.35it/s]\n",
      "Train 40 | out_loss 19.445348739624023: 100%|████| 8/8 [00:00<00:00, 259.22it/s]\n",
      "Train 41 | out_loss 18.922348022460938: 100%|████| 8/8 [00:00<00:00, 265.79it/s]\n",
      "Train 42 | out_loss 18.063411712646484: 100%|████| 8/8 [00:00<00:00, 270.05it/s]\n",
      "Train 43 | out_loss 18.678688049316406: 100%|████| 8/8 [00:00<00:00, 267.07it/s]\n",
      "Train 44 | out_loss 18.636152267456055: 100%|████| 8/8 [00:00<00:00, 257.86it/s]\n",
      "Train 45 | out_loss 17.506397247314453: 100%|████| 8/8 [00:00<00:00, 258.13it/s]\n",
      "Train 46 | out_loss 16.118213653564453: 100%|████| 8/8 [00:00<00:00, 262.12it/s]\n",
      "Train 47 | out_loss 15.315125465393066: 100%|████| 8/8 [00:00<00:00, 256.18it/s]\n",
      "Train 48 | out_loss 14.668306350708008: 100%|████| 8/8 [00:00<00:00, 262.55it/s]\n",
      "Train 49 | out_loss 14.476733207702637: 100%|████| 8/8 [00:00<00:00, 256.22it/s]\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]/home/AL_main_new/utils.py:99: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  x = torch.tensor([x for x,y in batch], dtype=torch.float32)\n",
      "Train 0 | out_loss 80.37500762939453: 100%|███████| 8/8 [00:00<00:00, 19.10it/s]\n",
      "Train 1 | out_loss 78.81150817871094: 100%|██████| 8/8 [00:00<00:00, 199.20it/s]\n",
      "Train 2 | out_loss 74.7491455078125: 100%|███████| 8/8 [00:00<00:00, 204.01it/s]\n",
      "Train 3 | out_loss 74.28646850585938: 100%|██████| 8/8 [00:00<00:00, 201.45it/s]\n",
      "Train 4 | out_loss 71.90930938720703: 100%|██████| 8/8 [00:00<00:00, 201.03it/s]\n",
      "Train 5 | out_loss 69.00135040283203: 100%|██████| 8/8 [00:00<00:00, 118.95it/s]\n",
      "Train 6 | out_loss 67.6754150390625: 100%|███████| 8/8 [00:00<00:00, 196.80it/s]\n",
      "Train 7 | out_loss 66.5774154663086: 100%|███████| 8/8 [00:00<00:00, 202.85it/s]\n",
      "Train 8 | out_loss 64.30731964111328: 100%|██████| 8/8 [00:00<00:00, 207.16it/s]\n",
      "Train 9 | out_loss 64.85501861572266: 100%|██████| 8/8 [00:00<00:00, 206.53it/s]\n",
      "Train 10 | out_loss 63.77495193481445: 100%|█████| 8/8 [00:00<00:00, 204.28it/s]\n",
      "Train 11 | out_loss 63.24276351928711: 100%|█████| 8/8 [00:00<00:00, 206.29it/s]\n",
      "Train 12 | out_loss 64.66304779052734: 100%|█████| 8/8 [00:00<00:00, 204.98it/s]\n",
      "Train 13 | out_loss 63.305179595947266: 100%|████| 8/8 [00:00<00:00, 199.63it/s]\n",
      "Train 14 | out_loss 63.36079406738281: 100%|█████| 8/8 [00:00<00:00, 198.36it/s]\n",
      "Train 15 | out_loss 62.14793395996094: 100%|█████| 8/8 [00:00<00:00, 196.24it/s]\n",
      "Train 16 | out_loss 61.23788070678711: 100%|█████| 8/8 [00:00<00:00, 204.66it/s]\n",
      "Train 17 | out_loss 60.4580078125: 100%|█████████| 8/8 [00:00<00:00, 196.91it/s]\n",
      "Train 18 | out_loss 60.25556945800781: 100%|█████| 8/8 [00:00<00:00, 198.92it/s]\n",
      "Train 19 | out_loss 58.71955108642578: 100%|█████| 8/8 [00:00<00:00, 210.74it/s]\n",
      "Train 20 | out_loss 58.181617736816406: 100%|████| 8/8 [00:00<00:00, 211.67it/s]\n",
      "Train 21 | out_loss 57.202674865722656: 100%|████| 8/8 [00:00<00:00, 212.64it/s]\n",
      "Train 22 | out_loss 56.13126754760742: 100%|█████| 8/8 [00:00<00:00, 214.07it/s]\n",
      "Train 23 | out_loss 54.253841400146484: 100%|████| 8/8 [00:00<00:00, 212.98it/s]\n",
      "Train 24 | out_loss 54.26271057128906: 100%|█████| 8/8 [00:00<00:00, 214.74it/s]\n",
      "Train 25 | out_loss 52.986698150634766: 100%|████| 8/8 [00:00<00:00, 211.35it/s]\n",
      "Train 26 | out_loss 51.90414810180664: 100%|█████| 8/8 [00:00<00:00, 198.03it/s]\n",
      "Train 27 | out_loss 50.88888168334961: 100%|█████| 8/8 [00:00<00:00, 182.61it/s]\n",
      "Train 28 | out_loss 50.32403564453125: 100%|█████| 8/8 [00:00<00:00, 208.08it/s]\n",
      "Train 29 | out_loss 49.51617431640625: 100%|█████| 8/8 [00:00<00:00, 205.82it/s]\n",
      "Train 30 | out_loss 48.05425262451172: 100%|█████| 8/8 [00:00<00:00, 209.22it/s]\n",
      "Train 31 | out_loss 47.201263427734375: 100%|████| 8/8 [00:00<00:00, 194.49it/s]\n",
      "Train 32 | out_loss 46.14252471923828: 100%|█████| 8/8 [00:00<00:00, 174.83it/s]\n",
      "Train 33 | out_loss 45.170372009277344: 100%|████| 8/8 [00:00<00:00, 184.26it/s]\n",
      "Train 34 | out_loss 44.36216354370117: 100%|█████| 8/8 [00:00<00:00, 210.63it/s]\n",
      "Train 35 | out_loss 43.215763092041016: 100%|████| 8/8 [00:00<00:00, 210.97it/s]\n",
      "Train 36 | out_loss 41.94840621948242: 100%|█████| 8/8 [00:00<00:00, 182.48it/s]\n",
      "Train 37 | out_loss 41.321205139160156: 100%|████| 8/8 [00:00<00:00, 207.61it/s]\n",
      "Train 38 | out_loss 40.39054489135742: 100%|█████| 8/8 [00:00<00:00, 201.75it/s]\n",
      "Train 39 | out_loss 38.691898345947266: 100%|████| 8/8 [00:00<00:00, 207.63it/s]\n",
      "Train 40 | out_loss 38.087284088134766: 100%|████| 8/8 [00:00<00:00, 208.93it/s]\n",
      "Train 41 | out_loss 37.20523452758789: 100%|█████| 8/8 [00:00<00:00, 213.33it/s]\n",
      "Train 42 | out_loss 36.593990325927734: 100%|████| 8/8 [00:00<00:00, 207.69it/s]\n",
      "Train 43 | out_loss 35.34302520751953: 100%|█████| 8/8 [00:00<00:00, 211.15it/s]\n",
      "Train 44 | out_loss 34.38832473754883: 100%|█████| 8/8 [00:00<00:00, 171.12it/s]\n",
      "Train 45 | out_loss 33.6642951965332: 100%|██████| 8/8 [00:00<00:00, 166.46it/s]\n",
      "Train 46 | out_loss 32.989402770996094: 100%|████| 8/8 [00:00<00:00, 190.56it/s]\n",
      "Train 47 | out_loss 31.71337127685547: 100%|█████| 8/8 [00:00<00:00, 212.29it/s]\n",
      "Train 48 | out_loss 30.71175765991211: 100%|█████| 8/8 [00:00<00:00, 205.93it/s]\n",
      "Train 49 | out_loss 29.941850662231445: 100%|████| 8/8 [00:00<00:00, 216.46it/s]\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]/home/AL_main_new/utils.py:99: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  x = torch.tensor([x for x,y in batch], dtype=torch.float32)\n",
      "Train 0 | out_loss 80.66100311279297: 100%|███████| 8/8 [00:00<00:00, 19.41it/s]\n",
      "Train 1 | out_loss 80.04766845703125: 100%|██████| 8/8 [00:00<00:00, 159.56it/s]\n",
      "Train 2 | out_loss 76.83219146728516: 100%|██████| 8/8 [00:00<00:00, 184.47it/s]\n",
      "Train 3 | out_loss 76.01897430419922: 100%|██████| 8/8 [00:00<00:00, 186.30it/s]\n",
      "Train 4 | out_loss 76.9652099609375: 100%|███████| 8/8 [00:00<00:00, 177.57it/s]\n",
      "Train 5 | out_loss 76.62203979492188: 100%|██████| 8/8 [00:00<00:00, 179.41it/s]\n",
      "Train 6 | out_loss 72.59370422363281: 100%|██████| 8/8 [00:00<00:00, 184.34it/s]\n",
      "Train 7 | out_loss 73.07462310791016: 100%|██████| 8/8 [00:00<00:00, 183.33it/s]\n",
      "Train 8 | out_loss 71.96461486816406: 100%|██████| 8/8 [00:00<00:00, 185.63it/s]\n",
      "Train 9 | out_loss 70.82769012451172: 100%|██████| 8/8 [00:00<00:00, 179.19it/s]\n",
      "Train 10 | out_loss 68.93132019042969: 100%|█████| 8/8 [00:00<00:00, 181.68it/s]\n",
      "Train 11 | out_loss 69.43951416015625: 100%|█████| 8/8 [00:00<00:00, 186.13it/s]\n",
      "Train 12 | out_loss 68.33834838867188: 100%|█████| 8/8 [00:00<00:00, 176.25it/s]\n",
      "Train 13 | out_loss 66.50336456298828: 100%|█████| 8/8 [00:00<00:00, 170.95it/s]\n",
      "Train 14 | out_loss 64.53569793701172: 100%|█████| 8/8 [00:00<00:00, 185.77it/s]\n",
      "Train 15 | out_loss 64.85191345214844: 100%|█████| 8/8 [00:00<00:00, 183.41it/s]\n",
      "Train 16 | out_loss 63.683258056640625: 100%|████| 8/8 [00:00<00:00, 173.41it/s]\n",
      "Train 17 | out_loss 61.80734634399414: 100%|█████| 8/8 [00:00<00:00, 178.10it/s]\n",
      "Train 18 | out_loss 61.87030029296875: 100%|█████| 8/8 [00:00<00:00, 135.36it/s]\n",
      "Train 19 | out_loss 60.105064392089844: 100%|████| 8/8 [00:00<00:00, 178.08it/s]\n",
      "Train 20 | out_loss 58.656986236572266: 100%|████| 8/8 [00:00<00:00, 185.18it/s]\n",
      "Train 21 | out_loss 57.972740173339844: 100%|████| 8/8 [00:00<00:00, 186.24it/s]\n",
      "Train 22 | out_loss 57.290401458740234: 100%|████| 8/8 [00:00<00:00, 187.00it/s]\n",
      "Train 23 | out_loss 55.64370346069336: 100%|█████| 8/8 [00:00<00:00, 183.27it/s]\n",
      "Train 24 | out_loss 54.76457595825195: 100%|█████| 8/8 [00:00<00:00, 185.12it/s]\n",
      "Train 25 | out_loss 54.064029693603516: 100%|████| 8/8 [00:00<00:00, 175.89it/s]\n",
      "Train 26 | out_loss 52.72576141357422: 100%|█████| 8/8 [00:00<00:00, 183.29it/s]\n",
      "Train 27 | out_loss 52.17194366455078: 100%|█████| 8/8 [00:00<00:00, 183.01it/s]\n",
      "Train 28 | out_loss 50.56935501098633: 100%|█████| 8/8 [00:00<00:00, 185.58it/s]\n",
      "Train 29 | out_loss 50.13287353515625: 100%|█████| 8/8 [00:00<00:00, 183.77it/s]\n",
      "Train 30 | out_loss 48.821815490722656: 100%|████| 8/8 [00:00<00:00, 183.96it/s]\n",
      "Train 31 | out_loss 47.86432647705078: 100%|█████| 8/8 [00:00<00:00, 138.82it/s]\n",
      "Train 32 | out_loss 46.671722412109375: 100%|████| 8/8 [00:00<00:00, 165.68it/s]\n",
      "Train 33 | out_loss 46.059268951416016: 100%|████| 8/8 [00:00<00:00, 184.93it/s]\n",
      "Train 34 | out_loss 44.59551239013672: 100%|█████| 8/8 [00:00<00:00, 180.75it/s]\n",
      "Train 35 | out_loss 44.14033889770508: 100%|█████| 8/8 [00:00<00:00, 183.30it/s]\n",
      "Train 36 | out_loss 42.99190902709961: 100%|█████| 8/8 [00:00<00:00, 182.84it/s]\n",
      "Train 37 | out_loss 41.36393737792969: 100%|█████| 8/8 [00:00<00:00, 161.46it/s]\n",
      "Train 38 | out_loss 41.03105545043945: 100%|█████| 8/8 [00:00<00:00, 180.64it/s]\n",
      "Train 39 | out_loss 39.91022872924805: 100%|█████| 8/8 [00:00<00:00, 124.54it/s]\n",
      "Train 40 | out_loss 38.787437438964844: 100%|████| 8/8 [00:00<00:00, 161.21it/s]\n",
      "Train 41 | out_loss 37.313236236572266: 100%|████| 8/8 [00:00<00:00, 186.57it/s]\n",
      "Train 42 | out_loss 37.483985900878906: 100%|████| 8/8 [00:00<00:00, 183.61it/s]\n",
      "Train 43 | out_loss 35.931549072265625: 100%|████| 8/8 [00:00<00:00, 186.41it/s]\n",
      "Train 44 | out_loss 34.869056701660156: 100%|████| 8/8 [00:00<00:00, 186.29it/s]\n",
      "Train 45 | out_loss 34.22628402709961: 100%|█████| 8/8 [00:00<00:00, 187.49it/s]\n",
      "Train 46 | out_loss 32.9979133605957: 100%|██████| 8/8 [00:00<00:00, 138.48it/s]\n",
      "Train 47 | out_loss 32.45005416870117: 100%|█████| 8/8 [00:00<00:00, 185.26it/s]\n",
      "Train 48 | out_loss 31.098251342773438: 100%|████| 8/8 [00:00<00:00, 182.81it/s]\n",
      "Train 49 | out_loss 29.84471893310547: 100%|█████| 8/8 [00:00<00:00, 184.68it/s]\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]/home/AL_main_new/utils.py:99: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  x = torch.tensor([x for x,y in batch], dtype=torch.float32)\n",
      "Train 0 | out_loss 80.49638366699219: 100%|███████| 8/8 [00:00<00:00, 16.79it/s]\n",
      "Train 1 | out_loss 80.04119873046875: 100%|██████| 8/8 [00:00<00:00, 118.01it/s]\n",
      "Train 2 | out_loss 76.40736389160156: 100%|██████| 8/8 [00:00<00:00, 147.76it/s]\n",
      "Train 3 | out_loss 76.17723846435547: 100%|██████| 8/8 [00:00<00:00, 154.46it/s]\n",
      "Train 4 | out_loss 76.74988555908203: 100%|██████| 8/8 [00:00<00:00, 154.47it/s]\n",
      "Train 5 | out_loss 75.42984008789062: 100%|██████| 8/8 [00:00<00:00, 152.31it/s]\n",
      "Train 6 | out_loss 73.28553771972656: 100%|██████| 8/8 [00:00<00:00, 150.25it/s]\n",
      "Train 7 | out_loss 72.70917510986328: 100%|██████| 8/8 [00:00<00:00, 145.92it/s]\n",
      "Train 8 | out_loss 72.0929183959961: 100%|███████| 8/8 [00:00<00:00, 120.35it/s]\n",
      "Train 9 | out_loss 70.52079010009766: 100%|██████| 8/8 [00:00<00:00, 145.89it/s]\n",
      "Train 10 | out_loss 70.58097076416016: 100%|█████| 8/8 [00:00<00:00, 136.88it/s]\n",
      "Train 11 | out_loss 69.11646270751953: 100%|█████| 8/8 [00:00<00:00, 151.67it/s]\n",
      "Train 12 | out_loss 66.93928527832031: 100%|█████| 8/8 [00:00<00:00, 156.08it/s]\n",
      "Train 13 | out_loss 67.93284606933594: 100%|█████| 8/8 [00:00<00:00, 154.32it/s]\n",
      "Train 14 | out_loss 66.1832504272461: 100%|██████| 8/8 [00:00<00:00, 149.70it/s]\n",
      "Train 15 | out_loss 63.46704864501953: 100%|█████| 8/8 [00:00<00:00, 153.58it/s]\n",
      "Train 16 | out_loss 63.335628509521484: 100%|████| 8/8 [00:00<00:00, 156.67it/s]\n",
      "Train 17 | out_loss 63.478546142578125: 100%|████| 8/8 [00:00<00:00, 159.28it/s]\n",
      "Train 18 | out_loss 60.86749267578125: 100%|█████| 8/8 [00:00<00:00, 160.19it/s]\n",
      "Train 19 | out_loss 60.61685562133789: 100%|█████| 8/8 [00:00<00:00, 157.75it/s]\n",
      "Train 20 | out_loss 59.437198638916016: 100%|████| 8/8 [00:00<00:00, 141.37it/s]\n",
      "Train 21 | out_loss 57.191322326660156: 100%|████| 8/8 [00:00<00:00, 159.45it/s]\n",
      "Train 22 | out_loss 56.53840637207031: 100%|█████| 8/8 [00:00<00:00, 159.17it/s]\n",
      "Train 23 | out_loss 56.59222412109375: 100%|█████| 8/8 [00:00<00:00, 147.07it/s]\n",
      "Train 24 | out_loss 55.05754470825195: 100%|█████| 8/8 [00:00<00:00, 157.09it/s]\n",
      "Train 25 | out_loss 53.24253463745117: 100%|█████| 8/8 [00:00<00:00, 154.30it/s]\n",
      "Train 26 | out_loss 53.05616760253906: 100%|█████| 8/8 [00:00<00:00, 156.52it/s]\n",
      "Train 27 | out_loss 51.72322082519531: 100%|█████| 8/8 [00:00<00:00, 153.73it/s]\n",
      "Train 28 | out_loss 50.88424301147461: 100%|█████| 8/8 [00:00<00:00, 145.61it/s]\n",
      "Train 29 | out_loss 49.558128356933594: 100%|████| 8/8 [00:00<00:00, 153.51it/s]\n",
      "Train 30 | out_loss 48.816585540771484: 100%|████| 8/8 [00:00<00:00, 155.70it/s]\n",
      "Train 31 | out_loss 47.87942123413086: 100%|█████| 8/8 [00:00<00:00, 156.35it/s]\n",
      "Train 32 | out_loss 46.62918472290039: 100%|█████| 8/8 [00:00<00:00, 155.34it/s]\n",
      "Train 33 | out_loss 46.05249786376953: 100%|█████| 8/8 [00:00<00:00, 151.29it/s]\n",
      "Train 34 | out_loss 44.904022216796875: 100%|████| 8/8 [00:00<00:00, 140.08it/s]\n",
      "Train 35 | out_loss 43.66764831542969: 100%|█████| 8/8 [00:00<00:00, 156.54it/s]\n",
      "Train 36 | out_loss 42.97712707519531: 100%|█████| 8/8 [00:00<00:00, 157.61it/s]\n",
      "Train 37 | out_loss 41.55887222290039: 100%|█████| 8/8 [00:00<00:00, 153.37it/s]\n",
      "Train 38 | out_loss 41.14066696166992: 100%|█████| 8/8 [00:00<00:00, 157.38it/s]\n",
      "Train 39 | out_loss 39.571266174316406: 100%|████| 8/8 [00:00<00:00, 158.37it/s]\n",
      "Train 40 | out_loss 39.06513214111328: 100%|█████| 8/8 [00:00<00:00, 151.93it/s]\n",
      "Train 41 | out_loss 37.817745208740234: 100%|████| 8/8 [00:00<00:00, 155.76it/s]\n",
      "Train 42 | out_loss 36.9181022644043: 100%|██████| 8/8 [00:00<00:00, 152.06it/s]\n",
      "Train 43 | out_loss 35.98994827270508: 100%|█████| 8/8 [00:00<00:00, 151.90it/s]\n",
      "Train 44 | out_loss 35.015953063964844: 100%|████| 8/8 [00:00<00:00, 152.69it/s]\n",
      "Train 45 | out_loss 34.088470458984375: 100%|████| 8/8 [00:00<00:00, 159.12it/s]\n",
      "Train 46 | out_loss 33.0456428527832: 100%|██████| 8/8 [00:00<00:00, 157.98it/s]\n",
      "Train 47 | out_loss 32.16447830200195: 100%|█████| 8/8 [00:00<00:00, 154.97it/s]\n",
      "Train 48 | out_loss 31.2313232421875: 100%|██████| 8/8 [00:00<00:00, 157.47it/s]\n",
      "Train 49 | out_loss 30.243083953857422: 100%|████| 8/8 [00:00<00:00, 157.19it/s]\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]/home/AL_main_new/utils.py:99: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  x = torch.tensor([x for x,y in batch], dtype=torch.float32)\n",
      "Train 0 | out_loss 80.43840026855469: 100%|███████| 8/8 [00:00<00:00, 18.43it/s]\n",
      "Train 1 | out_loss 80.06683349609375: 100%|██████| 8/8 [00:00<00:00, 144.17it/s]\n",
      "Train 2 | out_loss 75.89752960205078: 100%|██████| 8/8 [00:00<00:00, 136.50it/s]\n",
      "Train 3 | out_loss 73.77307891845703: 100%|██████| 8/8 [00:00<00:00, 138.62it/s]\n",
      "Train 4 | out_loss 76.5927505493164: 100%|███████| 8/8 [00:00<00:00, 142.75it/s]\n",
      "Train 5 | out_loss 73.44481658935547: 100%|██████| 8/8 [00:00<00:00, 139.82it/s]\n",
      "Train 6 | out_loss 74.0082015991211: 100%|███████| 8/8 [00:00<00:00, 140.36it/s]\n",
      "Train 7 | out_loss 70.87808990478516: 100%|██████| 8/8 [00:00<00:00, 141.13it/s]\n",
      "Train 8 | out_loss 70.86625671386719: 100%|██████| 8/8 [00:00<00:00, 139.45it/s]\n",
      "Train 9 | out_loss 69.96812438964844: 100%|██████| 8/8 [00:00<00:00, 141.78it/s]\n",
      "Train 10 | out_loss 67.69815063476562: 100%|█████| 8/8 [00:00<00:00, 141.81it/s]\n",
      "Train 11 | out_loss 70.253662109375: 100%|███████| 8/8 [00:00<00:00, 142.75it/s]\n",
      "Train 12 | out_loss 68.1557846069336: 100%|██████| 8/8 [00:00<00:00, 142.84it/s]\n",
      "Train 13 | out_loss 64.00973510742188: 100%|█████| 8/8 [00:00<00:00, 142.88it/s]\n",
      "Train 14 | out_loss 62.50696563720703: 100%|█████| 8/8 [00:00<00:00, 140.74it/s]\n",
      "Train 15 | out_loss 68.0792465209961: 100%|██████| 8/8 [00:00<00:00, 141.18it/s]\n",
      "Train 16 | out_loss 62.74382019042969: 100%|█████| 8/8 [00:00<00:00, 141.24it/s]\n",
      "Train 17 | out_loss 60.00397872924805: 100%|█████| 8/8 [00:00<00:00, 131.87it/s]\n",
      "Train 18 | out_loss 59.98108673095703: 100%|█████| 8/8 [00:00<00:00, 136.44it/s]\n",
      "Train 19 | out_loss 59.88547134399414: 100%|█████| 8/8 [00:00<00:00, 140.00it/s]\n",
      "Train 20 | out_loss 58.59825134277344: 100%|█████| 8/8 [00:00<00:00, 109.79it/s]\n",
      "Train 21 | out_loss 56.43394088745117: 100%|█████| 8/8 [00:00<00:00, 136.97it/s]\n",
      "Train 22 | out_loss 56.95343017578125: 100%|█████| 8/8 [00:00<00:00, 140.01it/s]\n",
      "Train 23 | out_loss 55.99816131591797: 100%|█████| 8/8 [00:00<00:00, 140.32it/s]\n",
      "Train 24 | out_loss 53.85346221923828: 100%|█████| 8/8 [00:00<00:00, 135.83it/s]\n",
      "Train 25 | out_loss 53.15435028076172: 100%|█████| 8/8 [00:00<00:00, 135.86it/s]\n",
      "Train 26 | out_loss 52.35555648803711: 100%|█████| 8/8 [00:00<00:00, 141.91it/s]\n",
      "Train 27 | out_loss 50.99073791503906: 100%|█████| 8/8 [00:00<00:00, 141.77it/s]\n",
      "Train 28 | out_loss 50.244163513183594: 100%|████| 8/8 [00:00<00:00, 140.80it/s]\n",
      "Train 29 | out_loss 49.34428024291992: 100%|█████| 8/8 [00:00<00:00, 126.05it/s]\n",
      "Train 30 | out_loss 47.96443176269531: 100%|█████| 8/8 [00:00<00:00, 137.70it/s]\n",
      "Train 31 | out_loss 47.20622634887695: 100%|█████| 8/8 [00:00<00:00, 139.75it/s]\n",
      "Train 32 | out_loss 46.28666687011719: 100%|█████| 8/8 [00:00<00:00, 141.79it/s]\n",
      "Train 33 | out_loss 45.33556365966797: 100%|█████| 8/8 [00:00<00:00, 141.62it/s]\n",
      "Train 34 | out_loss 44.0860595703125: 100%|██████| 8/8 [00:00<00:00, 140.55it/s]\n",
      "Train 35 | out_loss 42.966468811035156: 100%|████| 8/8 [00:00<00:00, 139.56it/s]\n",
      "Train 36 | out_loss 42.547943115234375: 100%|████| 8/8 [00:00<00:00, 141.62it/s]\n",
      "Train 37 | out_loss 41.174407958984375: 100%|████| 8/8 [00:00<00:00, 141.84it/s]\n",
      "Train 38 | out_loss 40.04161071777344: 100%|█████| 8/8 [00:00<00:00, 141.58it/s]\n",
      "Train 39 | out_loss 39.454776763916016: 100%|████| 8/8 [00:00<00:00, 101.82it/s]\n",
      "Train 40 | out_loss 38.65217590332031: 100%|█████| 8/8 [00:00<00:00, 131.81it/s]\n",
      "Train 41 | out_loss 37.292667388916016: 100%|████| 8/8 [00:00<00:00, 122.88it/s]\n",
      "Train 42 | out_loss 36.50733184814453: 100%|█████| 8/8 [00:00<00:00, 125.54it/s]\n",
      "Train 43 | out_loss 35.220394134521484: 100%|████| 8/8 [00:00<00:00, 140.79it/s]\n",
      "Train 44 | out_loss 34.6325798034668: 100%|██████| 8/8 [00:00<00:00, 129.10it/s]\n",
      "Train 45 | out_loss 33.52259826660156: 100%|█████| 8/8 [00:00<00:00, 137.61it/s]\n",
      "Train 46 | out_loss 32.61160659790039: 100%|█████| 8/8 [00:00<00:00, 128.43it/s]\n",
      "Train 47 | out_loss 31.42249870300293: 100%|█████| 8/8 [00:00<00:00, 125.30it/s]\n",
      "Train 48 | out_loss 30.882131576538086: 100%|████| 8/8 [00:00<00:00, 137.67it/s]\n",
      "Train 49 | out_loss 29.71238136291504: 100%|█████| 8/8 [00:00<00:00, 140.58it/s]\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/AL_main_new/dis_train_al.py\", line 372, in <module>\n",
      "    main()\n",
      "  File \"/home/AL_main_new/dis_train_al.py\", line 325, in main\n",
      "    model = model.cuda()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 689, in cuda\n",
      "    return self._apply(lambda t: t.cuda(device))\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 579, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 579, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 579, in _apply\n",
      "    module._apply(fn)\n",
      "  [Previous line repeated 2 more times]\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 602, in _apply\n",
      "    param_applied = fn(param)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 689, in <lambda>\n",
      "    return self._apply(lambda t: t.cuda(device))\n",
      "KeyboardInterrupt\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]/home/AL_main_new/utils.py:99: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  x = torch.tensor([x for x,y in batch], dtype=torch.float32)\n",
      "Train 0 | out_loss 80.24359130859375: 100%|███████| 8/8 [00:00<00:00, 17.96it/s]\n",
      "Train 1 | out_loss 79.88316345214844: 100%|██████| 8/8 [00:00<00:00, 115.40it/s]\n",
      "Train 2 | out_loss 74.56044006347656: 100%|██████| 8/8 [00:00<00:00, 115.69it/s]\n",
      "Train 3 | out_loss 75.41255187988281: 100%|██████| 8/8 [00:00<00:00, 113.81it/s]\n",
      "Train 4 | out_loss 77.45304870605469: 100%|██████| 8/8 [00:00<00:00, 116.12it/s]\n",
      "Train 5 | out_loss 74.6749267578125: 100%|███████| 8/8 [00:00<00:00, 113.16it/s]\n",
      "Train 6 | out_loss 72.18120574951172: 100%|██████| 8/8 [00:00<00:00, 110.27it/s]\n",
      "Train 7 | out_loss 70.08329010009766: 100%|██████| 8/8 [00:00<00:00, 115.48it/s]\n",
      "Train 8 | out_loss 70.05838775634766: 100%|██████| 8/8 [00:00<00:00, 118.37it/s]\n",
      "Train 9 | out_loss 69.59609985351562: 100%|██████| 8/8 [00:00<00:00, 118.00it/s]\n",
      "Train 10 | out_loss 71.2622299194336: 100%|██████| 8/8 [00:00<00:00, 118.24it/s]\n",
      "Train 11 | out_loss 68.6736068725586: 100%|██████| 8/8 [00:00<00:00, 115.01it/s]\n",
      "Train 12 | out_loss 66.06277465820312: 100%|█████| 8/8 [00:00<00:00, 105.02it/s]\n",
      "Train 13 | out_loss 62.693443298339844: 100%|████| 8/8 [00:00<00:00, 106.75it/s]\n",
      "Train 14 | out_loss 64.92950439453125: 100%|██████| 8/8 [00:00<00:00, 91.77it/s]\n",
      "Train 15 | out_loss 62.49617385864258: 100%|█████| 8/8 [00:00<00:00, 110.71it/s]\n",
      "Train 16 | out_loss 62.999176025390625: 100%|████| 8/8 [00:00<00:00, 108.11it/s]\n",
      "Train 17 | out_loss 64.04316711425781: 100%|█████| 8/8 [00:00<00:00, 113.24it/s]\n",
      "Train 18 | out_loss 60.260093688964844: 100%|████| 8/8 [00:00<00:00, 116.76it/s]\n",
      "Train 19 | out_loss 57.43130111694336: 100%|█████| 8/8 [00:00<00:00, 107.74it/s]\n",
      "Train 20 | out_loss 57.15825653076172: 100%|█████| 8/8 [00:00<00:00, 113.22it/s]\n",
      "Train 21 | out_loss 59.28297805786133: 100%|█████| 8/8 [00:00<00:00, 114.91it/s]\n",
      "Train 22 | out_loss 61.188053131103516: 100%|████| 8/8 [00:00<00:00, 116.47it/s]\n",
      "Train 23 | out_loss 57.561241149902344: 100%|████| 8/8 [00:00<00:00, 116.58it/s]\n",
      "Train 24 | out_loss 52.083160400390625: 100%|████| 8/8 [00:00<00:00, 117.20it/s]\n",
      "Train 25 | out_loss 51.23246765136719: 100%|█████| 8/8 [00:00<00:00, 105.43it/s]\n",
      "Train 26 | out_loss 52.92039108276367: 100%|█████| 8/8 [00:00<00:00, 113.09it/s]\n",
      "Train 27 | out_loss 53.06496047973633: 100%|█████| 8/8 [00:00<00:00, 117.16it/s]\n",
      "Train 28 | out_loss 51.48799133300781: 100%|█████| 8/8 [00:00<00:00, 116.15it/s]\n",
      "Train 29 | out_loss 49.765201568603516: 100%|████| 8/8 [00:00<00:00, 117.96it/s]\n",
      "Train 30 | out_loss 48.504295349121094: 100%|████| 8/8 [00:00<00:00, 116.18it/s]\n",
      "Train 31 | out_loss 47.523651123046875: 100%|████| 8/8 [00:00<00:00, 114.75it/s]\n",
      "Train 32 | out_loss 46.82326889038086: 100%|█████| 8/8 [00:00<00:00, 114.54it/s]\n",
      "Train 33 | out_loss 46.12373733520508: 100%|█████| 8/8 [00:00<00:00, 109.43it/s]\n",
      "Train 34 | out_loss 44.91117477416992: 100%|█████| 8/8 [00:00<00:00, 106.40it/s]\n",
      "Train 35 | out_loss 43.690608978271484: 100%|████| 8/8 [00:00<00:00, 100.76it/s]\n",
      "Train 36 | out_loss 42.81014633178711: 100%|█████| 8/8 [00:00<00:00, 117.05it/s]\n",
      "Train 37 | out_loss 41.954315185546875: 100%|████| 8/8 [00:00<00:00, 116.91it/s]\n",
      "Train 38 | out_loss 40.88855743408203: 100%|█████| 8/8 [00:00<00:00, 116.78it/s]\n",
      "Train 39 | out_loss 39.87163162231445: 100%|█████| 8/8 [00:00<00:00, 115.06it/s]\n",
      "Train 40 | out_loss 38.933223724365234: 100%|████| 8/8 [00:00<00:00, 113.88it/s]\n",
      "Train 41 | out_loss 37.942832946777344: 100%|████| 8/8 [00:00<00:00, 116.44it/s]\n",
      "Train 42 | out_loss 36.99890899658203: 100%|█████| 8/8 [00:00<00:00, 111.95it/s]\n",
      "Train 43 | out_loss 36.015830993652344: 100%|█████| 8/8 [00:00<00:00, 93.29it/s]\n",
      "Train 44 | out_loss 35.035118103027344: 100%|████| 8/8 [00:00<00:00, 115.60it/s]\n",
      "Train 45 | out_loss 34.14024353027344: 100%|█████| 8/8 [00:00<00:00, 117.93it/s]\n",
      "Train 46 | out_loss 33.12535858154297: 100%|█████| 8/8 [00:00<00:00, 117.95it/s]\n",
      "Train 47 | out_loss 32.184024810791016: 100%|████| 8/8 [00:00<00:00, 116.65it/s]\n",
      "Train 48 | out_loss 31.242414474487305: 100%|████| 8/8 [00:00<00:00, 115.23it/s]\n",
      "Train 49 | out_loss 30.31960678100586: 100%|█████| 8/8 [00:00<00:00, 115.56it/s]\n"
     ]
    }
   ],
   "source": [
    "# LinearAL \n",
    "\n",
    "data = \"paint\"\n",
    "model =  \"linearal\"\n",
    "for layer in range(1,11):\n",
    "#for layer in [3]:\n",
    "    log = f\"result/{data}_{model}_l{layer}.log\"\n",
    "    !python3 dis_train_al.py --dataset {data} --model {model} --epoch 500 --num-layer {layer} --task regression  > {log}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6ae70865",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/AL_main_new/new dataset.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B140.115.59.235/home/AL_main_new/new%20dataset.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B140.115.59.235/home/AL_main_new/new%20dataset.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B140.115.59.235/home/AL_main_new/new%20dataset.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m args\u001b[39m.\u001b[39mtask \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mregression\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B140.115.59.235/home/AL_main_new/new%20dataset.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m args\u001b[39m.\u001b[39mfeature_dim \u001b[39m=\u001b[39m \u001b[39m40\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B140.115.59.235/home/AL_main_new/new%20dataset.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m df \u001b[39m=\u001b[39m load_dataset(\u001b[39m'\u001b[39m\u001b[39mLL0_296_ailerons\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "from mit_d3m import load_dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "args.task = \"regression\"\n",
    "args.feature_dim = 40\n",
    "df = load_dataset('LL0_296_ailerons')\n",
    "\n",
    "col_feature = df.X.columns[1:]\n",
    "#col_target= df.y.columns[:]\n",
    "\n",
    "y = df.y\n",
    "x = df.X[col_feature]\n",
    "x = x.fillna(0)\n",
    "feature_train, feature_test, train_target, test_target = train_test_split(x, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "833b414e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8800,)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target.to_numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7675b11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training\n",
      "  0%|                                                   | 0/138 [00:00<?, ?it/s]/home/AL_main_new/utils.py:99: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  x = torch.tensor([x for x,y in batch], dtype=torch.float32)\n",
      "Train 0 | out_loss 0.09604600071907043: 100%|█| 138/138 [00:00<00:00, 174.00it/s\n",
      "Train Epoch0 out_loss -54904.53125\n",
      "Test Epoch0 layer0 out_loss 0.3226132094860077\n",
      "Test Epoch0 layer1 out_loss 0.261636346578598\n",
      "Test Epoch0 layer2 out_loss 0.0825749933719635\n",
      "Train 1 | out_loss 0.0734870508313179: 100%|█| 138/138 [00:00<00:00, 332.80it/s]\n",
      "Train Epoch1 out_loss -32141.44140625\n",
      "Test Epoch1 layer0 out_loss 0.259891539812088\n",
      "Test Epoch1 layer1 out_loss 0.191255584359169\n",
      "Test Epoch1 layer2 out_loss 0.06840277463197708\n",
      "Train 2 | out_loss 0.06319232285022736: 100%|█| 138/138 [00:00<00:00, 334.77it/s\n",
      "Train Epoch2 out_loss -23766.646484375\n",
      "Test Epoch2 layer0 out_loss 0.34146255254745483\n",
      "Test Epoch2 layer1 out_loss 0.13012146949768066\n",
      "Test Epoch2 layer2 out_loss 0.05562148615717888\n",
      "Train 3 | out_loss 0.05656816065311432: 100%|█| 138/138 [00:00<00:00, 292.12it/s\n",
      "Train Epoch3 out_loss -19044.921875\n",
      "Test Epoch3 layer0 out_loss 0.32723450660705566\n",
      "Test Epoch3 layer1 out_loss 0.15748746693134308\n",
      "Test Epoch3 layer2 out_loss 0.056333377957344055\n",
      "Train 4 | out_loss 0.058965373784303665: 100%|█| 138/138 [00:00<00:00, 333.99it/\n",
      "Train Epoch4 out_loss -20693.3515625\n",
      "Test Epoch4 layer0 out_loss 0.23291215300559998\n",
      "Test Epoch4 layer1 out_loss 0.14360038936138153\n",
      "Test Epoch4 layer2 out_loss 0.05996169149875641\n",
      "Train 5 | out_loss 0.06477592140436172: 100%|█| 138/138 [00:00<00:00, 330.98it/s\n",
      "Train Epoch5 out_loss -24972.80859375\n",
      "Test Epoch5 layer0 out_loss 0.1989891529083252\n",
      "Test Epoch5 layer1 out_loss 0.17528913915157318\n",
      "Test Epoch5 layer2 out_loss 0.06678901612758636\n",
      "Train 6 | out_loss 0.06665418297052383: 100%|█| 138/138 [00:00<00:00, 335.56it/s\n",
      "Train Epoch6 out_loss -26442.09765625\n",
      "Test Epoch6 layer0 out_loss 0.25958555936813354\n",
      "Test Epoch6 layer1 out_loss 0.16691675782203674\n",
      "Test Epoch6 layer2 out_loss 0.06859374046325684\n",
      "Train 7 | out_loss 0.06661653518676758: 100%|█| 138/138 [00:00<00:00, 330.59it/s\n",
      "Train Epoch7 out_loss -26412.236328125\n",
      "Test Epoch7 layer0 out_loss 0.24077747762203217\n",
      "Test Epoch7 layer1 out_loss 0.12532556056976318\n",
      "Test Epoch7 layer2 out_loss 0.06274313479661942\n",
      "Train 8 | out_loss 0.05879414454102516: 100%|█| 138/138 [00:00<00:00, 333.37it/s\n",
      "Train Epoch8 out_loss -20573.345703125\n",
      "Test Epoch8 layer0 out_loss 0.20965445041656494\n",
      "Test Epoch8 layer1 out_loss 0.08589303493499756\n",
      "Test Epoch8 layer2 out_loss 0.053600385785102844\n",
      "Train 9 | out_loss 0.04935203492641449: 100%|█| 138/138 [00:00<00:00, 327.98it/s\n",
      "Train Epoch9 out_loss -14495.6640625\n",
      "Test Epoch9 layer0 out_loss 0.19961246848106384\n",
      "Test Epoch9 layer1 out_loss 0.06862387806177139\n",
      "Test Epoch9 layer2 out_loss 0.044294558465480804\n",
      "Train 10 | out_loss 0.04145931079983711: 100%|█| 138/138 [00:00<00:00, 327.83it/\n",
      "Train Epoch10 out_loss -10229.6220703125\n",
      "Test Epoch10 layer0 out_loss 0.19439083337783813\n",
      "Test Epoch10 layer1 out_loss 0.055396370589733124\n",
      "Test Epoch10 layer2 out_loss 0.03816363960504532\n",
      "Train 11 | out_loss 0.03598954156041145: 100%|█| 138/138 [00:00<00:00, 324.10it/\n",
      "Train Epoch11 out_loss -7708.22021484375\n",
      "Test Epoch11 layer0 out_loss 0.1954207867383957\n",
      "Test Epoch11 layer1 out_loss 0.044552069157361984\n",
      "Test Epoch11 layer2 out_loss 0.03370403125882149\n",
      "Train 12 | out_loss 0.033009979873895645: 100%|█| 138/138 [00:00<00:00, 327.10it\n",
      "Train Epoch12 out_loss -6484.57177734375\n",
      "Test Epoch12 layer0 out_loss 0.19915945827960968\n",
      "Test Epoch12 layer1 out_loss 0.04896198585629463\n",
      "Test Epoch12 layer2 out_loss 0.030901910737156868\n",
      "Train 13 | out_loss 0.03031942993402481: 100%|█| 138/138 [00:00<00:00, 326.53it/\n",
      "Train Epoch13 out_loss -5470.4189453125\n",
      "Test Epoch13 layer0 out_loss 0.20217952132225037\n",
      "Test Epoch13 layer1 out_loss 0.04390402138233185\n",
      "Test Epoch13 layer2 out_loss 0.029252953827381134\n",
      "Train 14 | out_loss 0.028342517092823982: 100%|█| 138/138 [00:00<00:00, 328.59it\n",
      "Train Epoch14 out_loss -4780.17919921875\n",
      "Test Epoch14 layer0 out_loss 0.20083770155906677\n",
      "Test Epoch14 layer1 out_loss 0.04242235794663429\n",
      "Test Epoch14 layer2 out_loss 0.0267756637185812\n",
      "Train 15 | out_loss 0.026281066238880157: 100%|█| 138/138 [00:00<00:00, 328.03it\n",
      "Train Epoch15 out_loss -4109.96484375\n",
      "Test Epoch15 layer0 out_loss 0.19978222250938416\n",
      "Test Epoch15 layer1 out_loss 0.04167638346552849\n",
      "Test Epoch15 layer2 out_loss 0.024747926741838455\n",
      "Train 16 | out_loss 0.024404438212513924: 100%|█| 138/138 [00:00<00:00, 319.88it\n",
      "Train Epoch16 out_loss -3543.8310546875\n",
      "Test Epoch16 layer0 out_loss 0.19769570231437683\n",
      "Test Epoch16 layer1 out_loss 0.04088999330997467\n",
      "Test Epoch16 layer2 out_loss 0.02304157055914402\n",
      "Train 17 | out_loss 0.022718843072652817: 100%|█| 138/138 [00:00<00:00, 322.56it\n",
      "Train Epoch17 out_loss -3071.064208984375\n",
      "Test Epoch17 layer0 out_loss 0.19135752320289612\n",
      "Test Epoch17 layer1 out_loss 0.04159956052899361\n",
      "Test Epoch17 layer2 out_loss 0.021511325612664223\n",
      "Train 18 | out_loss 0.021092673763632774: 100%|█| 138/138 [00:00<00:00, 320.10it\n",
      "Train Epoch18 out_loss -2647.01953125\n",
      "Test Epoch18 layer0 out_loss 0.19044850766658783\n",
      "Test Epoch18 layer1 out_loss 0.04013523831963539\n",
      "Test Epoch18 layer2 out_loss 0.019946927204728127\n",
      "Train 19 | out_loss 0.019490405917167664: 100%|█| 138/138 [00:00<00:00, 326.74it\n",
      "Train Epoch19 out_loss -2259.991943359375\n",
      "Test Epoch19 layer0 out_loss 0.18159028887748718\n",
      "Test Epoch19 layer1 out_loss 0.037707265466451645\n",
      "Test Epoch19 layer2 out_loss 0.018440138548612595\n",
      "Train 20 | out_loss 0.0180747639387846: 100%|█| 138/138 [00:00<00:00, 327.53it/s\n",
      "Train Epoch20 out_loss -1943.4786376953125\n",
      "Test Epoch20 layer0 out_loss 0.17604206502437592\n",
      "Test Epoch20 layer1 out_loss 0.036906421184539795\n",
      "Test Epoch20 layer2 out_loss 0.01719488576054573\n",
      "Train 21 | out_loss 0.016849378123879433: 100%|█| 138/138 [00:00<00:00, 329.64it\n",
      "Train Epoch21 out_loss -1688.7608642578125\n",
      "Test Epoch21 layer0 out_loss 0.1622474640607834\n",
      "Test Epoch21 layer1 out_loss 0.03498287871479988\n",
      "Test Epoch21 layer2 out_loss 0.016101809218525887\n",
      "Train 22 | out_loss 0.015835756435990334: 100%|█| 138/138 [00:00<00:00, 326.87it\n",
      "Train Epoch22 out_loss -1491.5718994140625\n",
      "Test Epoch22 layer0 out_loss 0.1604360193014145\n",
      "Test Epoch22 layer1 out_loss 0.03373132646083832\n",
      "Test Epoch22 layer2 out_loss 0.015226462855935097\n",
      "Train 23 | out_loss 0.015027128159999847: 100%|█| 138/138 [00:00<00:00, 322.38it\n",
      "Train Epoch23 out_loss -1343.03173828125\n",
      "Test Epoch23 layer0 out_loss 0.15551608800888062\n",
      "Test Epoch23 layer1 out_loss 0.03306618332862854\n",
      "Test Epoch23 layer2 out_loss 0.014548564329743385\n",
      "Train 24 | out_loss 0.014399304054677486: 100%|█| 138/138 [00:00<00:00, 324.39it\n",
      "Train Epoch24 out_loss -1233.0712890625\n",
      "Test Epoch24 layer0 out_loss 0.10881383717060089\n",
      "Test Epoch24 layer1 out_loss 0.032333921641111374\n",
      "Test Epoch24 layer2 out_loss 0.014082590118050575\n",
      "Train 25 | out_loss 0.013872340321540833: 100%|█| 138/138 [00:00<00:00, 325.39it\n",
      "Train Epoch25 out_loss -1144.400390625\n",
      "Test Epoch25 layer0 out_loss 0.10693833976984024\n",
      "Test Epoch25 layer1 out_loss 0.031928785145282745\n",
      "Test Epoch25 layer2 out_loss 0.01346407551318407\n",
      "Train 26 | out_loss 0.013252245262265205: 100%|█| 138/138 [00:00<00:00, 326.99it\n",
      "Train Epoch26 out_loss -1044.2894287109375\n",
      "Test Epoch26 layer0 out_loss 0.10394231975078583\n",
      "Test Epoch26 layer1 out_loss 0.03234267234802246\n",
      "Test Epoch26 layer2 out_loss 0.012902350164949894\n",
      "Train 27 | out_loss 0.012687437236309052: 100%|█| 138/138 [00:00<00:00, 325.28it\n",
      "Train Epoch27 out_loss -957.087646484375\n",
      "Test Epoch27 layer0 out_loss 0.10250382870435715\n",
      "Test Epoch27 layer1 out_loss 0.031243957579135895\n",
      "Test Epoch27 layer2 out_loss 0.012352971360087395\n",
      "Train 28 | out_loss 0.01218477450311184: 100%|█| 138/138 [00:00<00:00, 327.95it/\n",
      "Train Epoch28 out_loss -882.6755981445312\n",
      "Test Epoch28 layer0 out_loss 0.09840995073318481\n",
      "Test Epoch28 layer1 out_loss 0.03036608174443245\n",
      "Test Epoch28 layer2 out_loss 0.01188281923532486\n",
      "Train 29 | out_loss 0.01165946014225483: 100%|█| 138/138 [00:00<00:00, 318.27it/\n",
      "Train Epoch29 out_loss -808.1228637695312\n",
      "Test Epoch29 layer0 out_loss 0.09668073058128357\n",
      "Test Epoch29 layer1 out_loss 0.02954941987991333\n",
      "Test Epoch29 layer2 out_loss 0.011339117772877216\n",
      "Train 30 | out_loss 0.011184513568878174: 100%|█| 138/138 [00:00<00:00, 319.58it\n",
      "Train Epoch30 out_loss -743.5465698242188\n",
      "Test Epoch30 layer0 out_loss 0.0948183536529541\n",
      "Test Epoch30 layer1 out_loss 0.028555214405059814\n",
      "Test Epoch30 layer2 out_loss 0.010846524499356747\n",
      "Train 31 | out_loss 0.010699581354856491: 100%|█| 138/138 [00:00<00:00, 314.65it\n",
      "Train Epoch31 out_loss -680.38330078125\n",
      "Test Epoch31 layer0 out_loss 0.09191551804542542\n",
      "Test Epoch31 layer1 out_loss 0.027729257941246033\n",
      "Test Epoch31 layer2 out_loss 0.010461816564202309\n",
      "Train 32 | out_loss 0.010277317836880684: 100%|█| 138/138 [00:00<00:00, 322.23it\n",
      "Train Epoch32 out_loss -627.6622924804688\n",
      "Test Epoch32 layer0 out_loss 0.08975411206483841\n",
      "Test Epoch32 layer1 out_loss 0.0266818068921566\n",
      "Test Epoch32 layer2 out_loss 0.010047174990177155\n",
      "Train 33 | out_loss 0.009866401553153992: 100%|█| 138/138 [00:00<00:00, 324.32it\n",
      "Train Epoch33 out_loss -578.3955688476562\n",
      "Test Epoch33 layer0 out_loss 0.08435403555631638\n",
      "Test Epoch33 layer1 out_loss 0.02567128650844097\n",
      "Test Epoch33 layer2 out_loss 0.009636576287448406\n",
      "Train 34 | out_loss 0.009502631612122059: 100%|█| 138/138 [00:00<00:00, 321.15it\n",
      "Train Epoch34 out_loss -536.4590454101562\n",
      "Test Epoch34 layer0 out_loss 0.08063679188489914\n",
      "Test Epoch34 layer1 out_loss 0.025106826797127724\n",
      "Test Epoch34 layer2 out_loss 0.009213369339704514\n",
      "Train 35 | out_loss 0.009106959216296673: 100%|█| 138/138 [00:00<00:00, 320.18it\n",
      "Train Epoch35 out_loss -492.633544921875\n",
      "Test Epoch35 layer0 out_loss 0.07604766637086868\n",
      "Test Epoch35 layer1 out_loss 0.023601386696100235\n",
      "Test Epoch35 layer2 out_loss 0.008885873481631279\n",
      "Train 36 | out_loss 0.008796578273177147: 100%|█| 138/138 [00:00<00:00, 319.63it\n",
      "Train Epoch36 out_loss -459.55859375\n",
      "Test Epoch36 layer0 out_loss 0.07252395153045654\n",
      "Test Epoch36 layer1 out_loss 0.023079628124833107\n",
      "Test Epoch36 layer2 out_loss 0.008571485057473183\n",
      "Train 37 | out_loss 0.008466075174510479: 100%|█| 138/138 [00:00<00:00, 322.95it\n",
      "Train Epoch37 out_loss -425.60089111328125\n",
      "Test Epoch37 layer0 out_loss 0.07002755254507065\n",
      "Test Epoch37 layer1 out_loss 0.021976253017783165\n",
      "Test Epoch37 layer2 out_loss 0.008266552351415157\n",
      "Train 38 | out_loss 0.008138669654726982: 100%|█| 138/138 [00:00<00:00, 329.83it\n",
      "Train Epoch38 out_loss -393.2433776855469\n",
      "Test Epoch38 layer0 out_loss 0.06771605461835861\n",
      "Test Epoch38 layer1 out_loss 0.02114049158990383\n",
      "Test Epoch38 layer2 out_loss 0.007940614596009254\n",
      "Train 39 | out_loss 0.007840457372367382: 100%|█| 138/138 [00:00<00:00, 329.49it\n",
      "Train Epoch39 out_loss -364.8813781738281\n",
      "Test Epoch39 layer0 out_loss 0.06535002589225769\n",
      "Test Epoch39 layer1 out_loss 0.02057633176445961\n",
      "Test Epoch39 layer2 out_loss 0.007634059060364962\n",
      "Train 40 | out_loss 0.007563027087599039: 100%|█| 138/138 [00:00<00:00, 323.24it\n",
      "Train Epoch40 out_loss -339.4466552734375\n",
      "Test Epoch40 layer0 out_loss 0.06281089782714844\n",
      "Test Epoch40 layer1 out_loss 0.019643615931272507\n",
      "Test Epoch40 layer2 out_loss 0.007364656310528517\n",
      "Train 41 | out_loss 0.007266479544341564: 100%|█| 138/138 [00:00<00:00, 318.46it\n",
      "Train Epoch41 out_loss -313.2720642089844\n",
      "Test Epoch41 layer0 out_loss 0.06043514236807823\n",
      "Test Epoch41 layer1 out_loss 0.018993332982063293\n",
      "Test Epoch41 layer2 out_loss 0.007140324451029301\n",
      "Train 42 | out_loss 0.007024493534117937: 100%|█| 138/138 [00:00<00:00, 320.27it\n",
      "Train Epoch42 out_loss -292.68896484375\n",
      "Test Epoch42 layer0 out_loss 0.05855570361018181\n",
      "Test Epoch42 layer1 out_loss 0.018288807943463326\n",
      "Test Epoch42 layer2 out_loss 0.006858143489807844\n",
      "Train 43 | out_loss 0.006784218363463879: 100%|█| 138/138 [00:00<00:00, 314.31it\n",
      "Train Epoch43 out_loss -272.9411926269531\n",
      "Test Epoch43 layer0 out_loss 0.057096485048532486\n",
      "Test Epoch43 layer1 out_loss 0.018001848831772804\n",
      "Test Epoch43 layer2 out_loss 0.006617656908929348\n",
      "Train 44 | out_loss 0.006544626317918301: 100%|█| 138/138 [00:00<00:00, 319.12it\n",
      "Train Epoch44 out_loss -253.93382263183594\n",
      "Test Epoch44 layer0 out_loss 0.05612236633896828\n",
      "Test Epoch44 layer1 out_loss 0.01750359684228897\n",
      "Test Epoch44 layer2 out_loss 0.0063964081928133965\n",
      "Train 45 | out_loss 0.006318728905171156: 100%|█| 138/138 [00:00<00:00, 319.69it\n",
      "Train Epoch45 out_loss -236.6387939453125\n",
      "Test Epoch45 layer0 out_loss 0.05504263564944267\n",
      "Test Epoch45 layer1 out_loss 0.016841929405927658\n",
      "Test Epoch45 layer2 out_loss 0.006133414804935455\n",
      "Train 46 | out_loss 0.0061034816317260265: 100%|█| 138/138 [00:00<00:00, 320.01i\n",
      "Train Epoch46 out_loss -220.72425842285156\n",
      "Test Epoch46 layer0 out_loss 0.05340253561735153\n",
      "Test Epoch46 layer1 out_loss 0.016371583566069603\n",
      "Test Epoch46 layer2 out_loss 0.00593276834115386\n",
      "Train 47 | out_loss 0.0059085930697619915: 100%|█| 138/138 [00:00<00:00, 316.97i\n",
      "Train Epoch47 out_loss -206.79078674316406\n",
      "Test Epoch47 layer0 out_loss 0.05298859253525734\n",
      "Test Epoch47 layer1 out_loss 0.01584785245358944\n",
      "Test Epoch47 layer2 out_loss 0.005772165488451719\n",
      "Train 48 | out_loss 0.005727848038077354: 100%|█| 138/138 [00:00<00:00, 321.28it\n",
      "Train Epoch48 out_loss -194.27215576171875\n",
      "Test Epoch48 layer0 out_loss 0.051613226532936096\n",
      "Test Epoch48 layer1 out_loss 0.015485621057450771\n",
      "Test Epoch48 layer2 out_loss 0.0055649541318416595\n",
      "Train 49 | out_loss 0.005555198527872562: 100%|█| 138/138 [00:00<00:00, 322.15it\n",
      "Train Epoch49 out_loss -182.67784118652344\n",
      "Test Epoch49 layer0 out_loss 0.050602156668901443\n",
      "Test Epoch49 layer1 out_loss 0.015291412360966206\n",
      "Test Epoch49 layer2 out_loss 0.005371780600398779\n",
      "Train 50 | out_loss 0.005358218215405941: 100%|█| 138/138 [00:00<00:00, 320.91it\n",
      "Train Epoch50 out_loss -169.88279724121094\n",
      "Test Epoch50 layer0 out_loss 0.04975834861397743\n",
      "Test Epoch50 layer1 out_loss 0.014776958152651787\n",
      "Test Epoch50 layer2 out_loss 0.005252758506685495\n",
      "Train 51 | out_loss 0.0052160234190523624: 100%|█| 138/138 [00:00<00:00, 327.61i\n",
      "Train Epoch51 out_loss -160.93348693847656\n",
      "Test Epoch51 layer0 out_loss 0.04928283765912056\n",
      "Test Epoch51 layer1 out_loss 0.014754047617316246\n",
      "Test Epoch51 layer2 out_loss 0.00505302008241415\n",
      "Train 52 | out_loss 0.005039550364017487: 100%|█| 138/138 [00:00<00:00, 328.40it\n",
      "Train Epoch52 out_loss -150.16146850585938\n",
      "Test Epoch52 layer0 out_loss 0.04913366213440895\n",
      "Test Epoch52 layer1 out_loss 0.014225723221898079\n",
      "Test Epoch52 layer2 out_loss 0.0048948354087769985\n",
      "Train 53 | out_loss 0.0048720696941018105: 100%|█| 138/138 [00:00<00:00, 324.31i\n",
      "Train Epoch53 out_loss -140.2813720703125\n",
      "Test Epoch53 layer0 out_loss 0.04831686243414879\n",
      "Test Epoch53 layer1 out_loss 0.013582139275968075\n",
      "Test Epoch53 layer2 out_loss 0.004773698281496763\n",
      "Train 54 | out_loss 0.00473471824079752: 100%|█| 138/138 [00:00<00:00, 322.07it/\n",
      "Train Epoch54 out_loss -132.42774963378906\n",
      "Test Epoch54 layer0 out_loss 0.048387907445430756\n",
      "Test Epoch54 layer1 out_loss 0.013268524780869484\n",
      "Test Epoch54 layer2 out_loss 0.0046021644957363605\n",
      "Train 55 | out_loss 0.004578110761940479: 100%|█| 138/138 [00:00<00:00, 286.97it\n",
      "Train Epoch55 out_loss -123.74708557128906\n",
      "Test Epoch55 layer0 out_loss 0.047818832099437714\n",
      "Test Epoch55 layer1 out_loss 0.013282381929457188\n",
      "Test Epoch55 layer2 out_loss 0.004442985635250807\n",
      "Train 56 | out_loss 0.004433845169842243: 100%|█| 138/138 [00:00<00:00, 328.16it\n",
      "Train Epoch56 out_loss -116.0088119506836\n",
      "Test Epoch56 layer0 out_loss 0.04682609438896179\n",
      "Test Epoch56 layer1 out_loss 0.012834994122385979\n",
      "Test Epoch56 layer2 out_loss 0.004296041559427977\n",
      "Train 57 | out_loss 0.004300769418478012: 100%|█| 138/138 [00:00<00:00, 319.52it\n",
      "Train Epoch57 out_loss -109.0905532836914\n",
      "Test Epoch57 layer0 out_loss 0.04556513950228691\n",
      "Test Epoch57 layer1 out_loss 0.013179413974285126\n",
      "Test Epoch57 layer2 out_loss 0.004204827826470137\n",
      "Train 58 | out_loss 0.004164710640907288: 100%|█| 138/138 [00:00<00:00, 319.57it\n",
      "Train Epoch58 out_loss -102.23509979248047\n",
      "Test Epoch58 layer0 out_loss 0.046020880341529846\n",
      "Test Epoch58 layer1 out_loss 0.012430427595973015\n",
      "Test Epoch58 layer2 out_loss 0.004030491691082716\n",
      "Train 59 | out_loss 0.004033695440739393: 100%|█| 138/138 [00:00<00:00, 316.13it\n",
      "Train Epoch59 out_loss -95.8420181274414\n",
      "Test Epoch59 layer0 out_loss 0.04570484533905983\n",
      "Test Epoch59 layer1 out_loss 0.012152547016739845\n",
      "Test Epoch59 layer2 out_loss 0.003939858637750149\n",
      "Train 60 | out_loss 0.003929477650672197: 100%|█| 138/138 [00:00<00:00, 317.62it\n",
      "Train Epoch60 out_loss -90.90247344970703\n",
      "Test Epoch60 layer0 out_loss 0.04387802258133888\n",
      "Test Epoch60 layer1 out_loss 0.011811360716819763\n",
      "Test Epoch60 layer2 out_loss 0.003777752397581935\n",
      "Train 61 | out_loss 0.0038017178885638714: 100%|█| 138/138 [00:00<00:00, 320.36i\n",
      "Train Epoch61 out_loss -85.02349090576172\n",
      "Test Epoch61 layer0 out_loss 0.043261174112558365\n",
      "Test Epoch61 layer1 out_loss 0.011562498286366463\n",
      "Test Epoch61 layer2 out_loss 0.0036744887474924326\n",
      "Train 62 | out_loss 0.0036995154805481434: 100%|█| 138/138 [00:00<00:00, 308.77i\n",
      "Train Epoch62 out_loss -80.46055603027344\n",
      "Test Epoch62 layer0 out_loss 0.04407452791929245\n",
      "Test Epoch62 layer1 out_loss 0.011306602507829666\n",
      "Test Epoch62 layer2 out_loss 0.0035805420484393835\n",
      "Train 63 | out_loss 0.0035977282095700502: 100%|█| 138/138 [00:00<00:00, 318.38i\n",
      "Train Epoch63 out_loss -76.03971099853516\n",
      "Test Epoch63 layer0 out_loss 0.04334605485200882\n",
      "Test Epoch63 layer1 out_loss 0.01115401927381754\n",
      "Test Epoch63 layer2 out_loss 0.003497145604342222\n",
      "Train 64 | out_loss 0.0034986594691872597: 100%|█| 138/138 [00:00<00:00, 321.65i\n",
      "Train Epoch64 out_loss -71.85526275634766\n",
      "Test Epoch64 layer0 out_loss 0.042399413883686066\n",
      "Test Epoch64 layer1 out_loss 0.011004634201526642\n",
      "Test Epoch64 layer2 out_loss 0.003363036783412099\n",
      "Train 65 | out_loss 0.003405002411454916: 100%|█| 138/138 [00:00<00:00, 316.27it\n",
      "Train Epoch65 out_loss -68.00688934326172\n",
      "Test Epoch65 layer0 out_loss 0.042327988892793655\n",
      "Test Epoch65 layer1 out_loss 0.010914242826402187\n",
      "Test Epoch65 layer2 out_loss 0.003258367534726858\n",
      "Train 66 | out_loss 0.003308679908514023: 100%|█| 138/138 [00:00<00:00, 322.79it\n",
      "Train Epoch66 out_loss -64.15792846679688\n",
      "Test Epoch66 layer0 out_loss 0.04123833775520325\n",
      "Test Epoch66 layer1 out_loss 0.010403397493064404\n",
      "Test Epoch66 layer2 out_loss 0.003204221837222576\n",
      "Train 67 | out_loss 0.0032130922190845013: 100%|█| 138/138 [00:00<00:00, 319.05i\n",
      "Train Epoch67 out_loss -60.44750213623047\n",
      "Test Epoch67 layer0 out_loss 0.03910326585173607\n",
      "Test Epoch67 layer1 out_loss 0.010259381495416164\n",
      "Test Epoch67 layer2 out_loss 0.0031146558467298746\n",
      "Train 68 | out_loss 0.003139479085803032: 100%|█| 138/138 [00:00<00:00, 318.83it\n",
      "Train Epoch68 out_loss -57.664188385009766\n",
      "Test Epoch68 layer0 out_loss 0.039239250123500824\n",
      "Test Epoch68 layer1 out_loss 0.010248489677906036\n",
      "Test Epoch68 layer2 out_loss 0.003030680352821946\n",
      "Train 69 | out_loss 0.0030716601759195328: 100%|█| 138/138 [00:00<00:00, 321.22i\n",
      "Train Epoch69 out_loss -55.15702819824219\n",
      "Test Epoch69 layer0 out_loss 0.03883266821503639\n",
      "Test Epoch69 layer1 out_loss 0.00997911673039198\n",
      "Test Epoch69 layer2 out_loss 0.0029694517143070698\n",
      "Train 70 | out_loss 0.0029790436383336782: 100%|█| 138/138 [00:00<00:00, 323.12i\n",
      "Train Epoch70 out_loss -51.821537017822266\n",
      "Test Epoch70 layer0 out_loss 0.0377330556511879\n",
      "Test Epoch70 layer1 out_loss 0.009861853905022144\n",
      "Test Epoch70 layer2 out_loss 0.0028729590121656656\n",
      "Train 71 | out_loss 0.002912646858021617: 100%|█| 138/138 [00:00<00:00, 324.37it\n",
      "Train Epoch71 out_loss -49.49324417114258\n",
      "Test Epoch71 layer0 out_loss 0.036998651921749115\n",
      "Test Epoch71 layer1 out_loss 0.009586887434124947\n",
      "Test Epoch71 layer2 out_loss 0.00283124390989542\n",
      "Train 72 | out_loss 0.0028456845320761204: 100%|█| 138/138 [00:00<00:00, 322.22i\n",
      "Train Epoch72 out_loss -47.19828414916992\n",
      "Test Epoch72 layer0 out_loss 0.03750685229897499\n",
      "Test Epoch72 layer1 out_loss 0.009435474872589111\n",
      "Test Epoch72 layer2 out_loss 0.0027248593978583813\n",
      "Train 73 | out_loss 0.0027776744682341814: 100%|█| 138/138 [00:00<00:00, 314.37i\n",
      "Train Epoch73 out_loss -44.921939849853516\n",
      "Test Epoch73 layer0 out_loss 0.03900284692645073\n",
      "Test Epoch73 layer1 out_loss 0.009236232377588749\n",
      "Test Epoch73 layer2 out_loss 0.0026746289804577827\n",
      "Train 74 | out_loss 0.0027025348972529173: 100%|█| 138/138 [00:00<00:00, 314.31i\n",
      "Train Epoch74 out_loss -42.47109603881836\n",
      "Test Epoch74 layer0 out_loss 0.039944302290678024\n",
      "Test Epoch74 layer1 out_loss 0.009226380847394466\n",
      "Test Epoch74 layer2 out_loss 0.002615822944790125\n",
      "Train 75 | out_loss 0.0026512988843023777: 100%|█| 138/138 [00:00<00:00, 321.05i\n",
      "Train Epoch75 out_loss -40.83842086791992\n",
      "Test Epoch75 layer0 out_loss 0.03794580698013306\n",
      "Test Epoch75 layer1 out_loss 0.008916276507079601\n",
      "Test Epoch75 layer2 out_loss 0.0025463218335062265\n",
      "Train 76 | out_loss 0.002596331527456641: 100%|█| 138/138 [00:00<00:00, 317.14it\n",
      "Train Epoch76 out_loss -39.1215934753418\n",
      "Test Epoch76 layer0 out_loss 0.036424312740564346\n",
      "Test Epoch76 layer1 out_loss 0.008774248883128166\n",
      "Test Epoch76 layer2 out_loss 0.002488570287823677\n",
      "Train 77 | out_loss 0.0025400531012564898: 100%|█| 138/138 [00:00<00:00, 299.58i\n",
      "Train Epoch77 out_loss -37.401058197021484\n",
      "Test Epoch77 layer0 out_loss 0.03724803403019905\n",
      "Test Epoch77 layer1 out_loss 0.008650087751448154\n",
      "Test Epoch77 layer2 out_loss 0.0024649514816701412\n",
      "Train 78 | out_loss 0.0025045506190508604: 100%|█| 138/138 [00:00<00:00, 316.42i\n",
      "Train Epoch78 out_loss -36.335147857666016\n",
      "Test Epoch78 layer0 out_loss 0.03586576506495476\n",
      "Test Epoch78 layer1 out_loss 0.00855829194188118\n",
      "Test Epoch78 layer2 out_loss 0.002458089729771018\n",
      "Train 79 | out_loss 0.0024351419415324926: 100%|█| 138/138 [00:00<00:00, 315.08i\n",
      "Train Epoch79 out_loss -34.29440689086914\n",
      "Test Epoch79 layer0 out_loss 0.03538947179913521\n",
      "Test Epoch79 layer1 out_loss 0.008396608754992485\n",
      "Test Epoch79 layer2 out_loss 0.0023481380194425583\n",
      "Train 80 | out_loss 0.002387130632996559: 100%|█| 138/138 [00:00<00:00, 311.56it\n",
      "Train Epoch80 out_loss -32.91639709472656\n",
      "Test Epoch80 layer0 out_loss 0.036208443343639374\n",
      "Test Epoch80 layer1 out_loss 0.008228284306824207\n",
      "Test Epoch80 layer2 out_loss 0.002293677069246769\n",
      "Train 81 | out_loss 0.0023413049057126045: 100%|█| 138/138 [00:00<00:00, 323.97i\n",
      "Train Epoch81 out_loss -31.62676239013672\n",
      "Test Epoch81 layer0 out_loss 0.033924054354429245\n",
      "Test Epoch81 layer1 out_loss 0.008125620894134045\n",
      "Test Epoch81 layer2 out_loss 0.002286021364852786\n",
      "Train 82 | out_loss 0.002308111870661378: 100%|█| 138/138 [00:00<00:00, 310.79it\n",
      "Train Epoch82 out_loss -30.70819091796875\n",
      "Test Epoch82 layer0 out_loss 0.03401290625333786\n",
      "Test Epoch82 layer1 out_loss 0.008259644731879234\n",
      "Test Epoch82 layer2 out_loss 0.0022324128076434135\n",
      "Train 83 | out_loss 0.0022637860383838415: 100%|█| 138/138 [00:00<00:00, 321.89i\n",
      "Train Epoch83 out_loss -29.502002716064453\n",
      "Test Epoch83 layer0 out_loss 0.03303857520222664\n",
      "Test Epoch83 layer1 out_loss 0.007969526574015617\n",
      "Test Epoch83 layer2 out_loss 0.0021639468614012003\n",
      "Train 84 | out_loss 0.002213607309386134: 100%|█| 138/138 [00:00<00:00, 320.44it\n",
      "Train Epoch84 out_loss -28.164810180664062\n",
      "Test Epoch84 layer0 out_loss 0.03395819291472435\n",
      "Test Epoch84 layer1 out_loss 0.007975267246365547\n",
      "Test Epoch84 layer2 out_loss 0.002138015115633607\n",
      "Train 85 | out_loss 0.00218232162296772: 100%|█| 138/138 [00:00<00:00, 306.29it/\n",
      "Train Epoch85 out_loss -27.346214294433594\n",
      "Test Epoch85 layer0 out_loss 0.032673630863428116\n",
      "Test Epoch85 layer1 out_loss 0.007778738159686327\n",
      "Test Epoch85 layer2 out_loss 0.002107493346557021\n",
      "Train 86 | out_loss 0.0021441462449729443: 100%|█| 138/138 [00:00<00:00, 321.76i\n",
      "Train Epoch86 out_loss -26.363197326660156\n",
      "Test Epoch86 layer0 out_loss 0.03190561756491661\n",
      "Test Epoch86 layer1 out_loss 0.007590278051793575\n",
      "Test Epoch86 layer2 out_loss 0.0020597197581082582\n",
      "Train 87 | out_loss 0.0021027063485234976: 100%|█| 138/138 [00:00<00:00, 310.17i\n",
      "Train Epoch87 out_loss -25.31572723388672\n",
      "Test Epoch87 layer0 out_loss 0.03161326423287392\n",
      "Test Epoch87 layer1 out_loss 0.0074952831491827965\n",
      "Test Epoch87 layer2 out_loss 0.002060853410512209\n",
      "Train 88 | out_loss 0.0020810256246477365: 100%|█| 138/138 [00:00<00:00, 320.33i\n",
      "Train Epoch88 out_loss -24.77581787109375\n",
      "Test Epoch88 layer0 out_loss 0.03227747976779938\n",
      "Test Epoch88 layer1 out_loss 0.0073782033286988735\n",
      "Test Epoch88 layer2 out_loss 0.002009400399401784\n",
      "Train 89 | out_loss 0.0020356294699013233: 100%|█| 138/138 [00:00<00:00, 316.37i\n",
      "Train Epoch89 out_loss -23.66350746154785\n",
      "Test Epoch89 layer0 out_loss 0.031451575458049774\n",
      "Test Epoch89 layer1 out_loss 0.007207086309790611\n",
      "Test Epoch89 layer2 out_loss 0.0019935020245611668\n",
      "Train 90 | out_loss 0.0020086634904146194: 100%|█| 138/138 [00:00<00:00, 312.32i\n",
      "Train Epoch90 out_loss -23.014421463012695\n",
      "Test Epoch90 layer0 out_loss 0.030047280713915825\n",
      "Test Epoch90 layer1 out_loss 0.007127969525754452\n",
      "Test Epoch90 layer2 out_loss 0.0019820949528366327\n",
      "Train 91 | out_loss 0.001972438069060445: 100%|█| 138/138 [00:00<00:00, 318.60it\n",
      "Train Epoch91 out_loss -22.156051635742188\n",
      "Test Epoch91 layer0 out_loss 0.030272195115685463\n",
      "Test Epoch91 layer1 out_loss 0.007020304910838604\n",
      "Test Epoch91 layer2 out_loss 0.0018983156187459826\n",
      "Train 92 | out_loss 0.0019477891037240624: 100%|█| 138/138 [00:00<00:00, 321.20i\n",
      "Train Epoch92 out_loss -21.580923080444336\n",
      "Test Epoch92 layer0 out_loss 0.029370246455073357\n",
      "Test Epoch92 layer1 out_loss 0.0069158319383859634\n",
      "Test Epoch92 layer2 out_loss 0.0019343397580087185\n",
      "Train 93 | out_loss 0.0019134152680635452: 100%|█| 138/138 [00:00<00:00, 317.06i\n",
      "Train Epoch93 out_loss -20.790945053100586\n",
      "Test Epoch93 layer0 out_loss 0.029205286875367165\n",
      "Test Epoch93 layer1 out_loss 0.006897411309182644\n",
      "Test Epoch93 layer2 out_loss 0.0018606395460665226\n",
      "Train 94 | out_loss 0.0018860745476558805: 100%|█| 138/138 [00:00<00:00, 316.75i\n",
      "Train Epoch94 out_loss -20.172657012939453\n",
      "Test Epoch94 layer0 out_loss 0.029964348301291466\n",
      "Test Epoch94 layer1 out_loss 0.006740929093211889\n",
      "Test Epoch94 layer2 out_loss 0.0018204294610768557\n",
      "Train 95 | out_loss 0.0018632134888321161: 100%|█| 138/138 [00:00<00:00, 313.05i\n",
      "Train Epoch95 out_loss -19.66250991821289\n",
      "Test Epoch95 layer0 out_loss 0.02877959795296192\n",
      "Test Epoch95 layer1 out_loss 0.0066668870858848095\n",
      "Test Epoch95 layer2 out_loss 0.001796236028894782\n",
      "Train 96 | out_loss 0.0018373732455074787: 100%|█| 138/138 [00:00<00:00, 315.55i\n",
      "Train Epoch96 out_loss -19.093358993530273\n",
      "Test Epoch96 layer0 out_loss 0.029700463637709618\n",
      "Test Epoch96 layer1 out_loss 0.006651187315583229\n",
      "Test Epoch96 layer2 out_loss 0.001791211194358766\n",
      "Train 97 | out_loss 0.0018159006722271442: 100%|█| 138/138 [00:00<00:00, 319.67i\n",
      "Train Epoch97 out_loss -18.6264591217041\n",
      "Test Epoch97 layer0 out_loss 0.028902659192681313\n",
      "Test Epoch97 layer1 out_loss 0.00651009613648057\n",
      "Test Epoch97 layer2 out_loss 0.0017615940887480974\n",
      "Train 98 | out_loss 0.001786986947990954: 100%|█| 138/138 [00:00<00:00, 310.08it\n",
      "Train Epoch98 out_loss -18.006427764892578\n",
      "Test Epoch98 layer0 out_loss 0.02940601296722889\n",
      "Test Epoch98 layer1 out_loss 0.0063911969773471355\n",
      "Test Epoch98 layer2 out_loss 0.001724702538922429\n",
      "Train 99 | out_loss 0.0017644655890762806: 100%|█| 138/138 [00:00<00:00, 314.21i\n",
      "Train Epoch99 out_loss -17.530353546142578\n",
      "Test Epoch99 layer0 out_loss 0.029140636324882507\n",
      "Test Epoch99 layer1 out_loss 0.006337091792374849\n",
      "Test Epoch99 layer2 out_loss 0.0017064138082787395\n",
      "Train 100 | out_loss 0.0017435088520869613: 100%|█| 138/138 [00:00<00:00, 318.39\n",
      "Train Epoch100 out_loss -17.09282875061035\n",
      "Test Epoch100 layer0 out_loss 0.029436226934194565\n",
      "Test Epoch100 layer1 out_loss 0.006303389091044664\n",
      "Test Epoch100 layer2 out_loss 0.0016825421480461955\n",
      "Train 101 | out_loss 0.0017145201563835144: 100%|█| 138/138 [00:00<00:00, 312.31\n",
      "Train Epoch101 out_loss -16.496158599853516\n",
      "Test Epoch101 layer0 out_loss 0.029499422758817673\n",
      "Test Epoch101 layer1 out_loss 0.00617034500464797\n",
      "Test Epoch101 layer2 out_loss 0.0016633077757433057\n",
      "Train 102 | out_loss 0.0017035624478012323: 100%|█| 138/138 [00:00<00:00, 318.91\n",
      "Train Epoch102 out_loss -16.27324104309082\n",
      "Test Epoch102 layer0 out_loss 0.029170673340559006\n",
      "Test Epoch102 layer1 out_loss 0.006168697029352188\n",
      "Test Epoch102 layer2 out_loss 0.0016709021292626858\n",
      "Train 103 | out_loss 0.0016894475556910038: 100%|█| 138/138 [00:00<00:00, 307.75\n",
      "Train Epoch103 out_loss -15.988191604614258\n",
      "Test Epoch103 layer0 out_loss 0.029476581141352654\n",
      "Test Epoch103 layer1 out_loss 0.006063045002520084\n",
      "Test Epoch103 layer2 out_loss 0.0016672865021973848\n",
      "Train 104 | out_loss 0.0016627945005893707: 100%|█| 138/138 [00:00<00:00, 316.29\n",
      "Train Epoch104 out_loss -15.456415176391602\n",
      "Test Epoch104 layer0 out_loss 0.02947339415550232\n",
      "Test Epoch104 layer1 out_loss 0.0059442888014018536\n",
      "Test Epoch104 layer2 out_loss 0.001616580761037767\n",
      "Train 105 | out_loss 0.0016456512967124581: 100%|█| 138/138 [00:00<00:00, 318.71\n",
      "Train Epoch105 out_loss -15.118831634521484\n",
      "Test Epoch105 layer0 out_loss 0.028923122212290764\n",
      "Test Epoch105 layer1 out_loss 0.005934298504143953\n",
      "Test Epoch105 layer2 out_loss 0.0016165158012881875\n",
      "Train 106 | out_loss 0.0016327239573001862: 100%|█| 138/138 [00:00<00:00, 323.35\n",
      "Train Epoch106 out_loss -14.866572380065918\n",
      "Test Epoch106 layer0 out_loss 0.02909870818257332\n",
      "Test Epoch106 layer1 out_loss 0.005807566922158003\n",
      "Test Epoch106 layer2 out_loss 0.0015922596212476492\n",
      "Train 107 | out_loss 0.0016081553185358644: 100%|█| 138/138 [00:00<00:00, 319.84\n",
      "Train Epoch107 out_loss -14.392664909362793\n",
      "Test Epoch107 layer0 out_loss 0.029208602383732796\n",
      "Test Epoch107 layer1 out_loss 0.0057539562694728374\n",
      "Test Epoch107 layer2 out_loss 0.0016121375374495983\n",
      "Train 108 | out_loss 0.0015901461010798812: 100%|█| 138/138 [00:00<00:00, 312.03\n",
      "Train Epoch108 out_loss -14.049833297729492\n",
      "Test Epoch108 layer0 out_loss 0.028920698910951614\n",
      "Test Epoch108 layer1 out_loss 0.005807757843285799\n",
      "Test Epoch108 layer2 out_loss 0.0015460319118574262\n",
      "Train 109 | out_loss 0.0015868894988670945: 100%|█| 138/138 [00:00<00:00, 319.70\n",
      "Train Epoch109 out_loss -13.988260269165039\n",
      "Test Epoch109 layer0 out_loss 0.02912355400621891\n",
      "Test Epoch109 layer1 out_loss 0.005579737015068531\n",
      "Test Epoch109 layer2 out_loss 0.0015248720301315188\n",
      "Train 110 | out_loss 0.0015521810855716467: 100%|█| 138/138 [00:00<00:00, 315.38\n",
      "Train Epoch110 out_loss -13.339774131774902\n",
      "Test Epoch110 layer0 out_loss 0.02878763899207115\n",
      "Test Epoch110 layer1 out_loss 0.005522791296243668\n",
      "Test Epoch110 layer2 out_loss 0.0015220558270812035\n",
      "Train 111 | out_loss 0.0015412521315738559: 100%|█| 138/138 [00:00<00:00, 321.29\n",
      "Train Epoch111 out_loss -13.138567924499512\n",
      "Test Epoch111 layer0 out_loss 0.028575284406542778\n",
      "Test Epoch111 layer1 out_loss 0.005527079105377197\n",
      "Test Epoch111 layer2 out_loss 0.0015374163631349802\n",
      "Train 112 | out_loss 0.0015260449144989252: 100%|█| 138/138 [00:00<00:00, 316.26\n",
      "Train Epoch112 out_loss -12.860922813415527\n",
      "Test Epoch112 layer0 out_loss 0.028167827054858208\n",
      "Test Epoch112 layer1 out_loss 0.005392900668084621\n",
      "Test Epoch112 layer2 out_loss 0.001478746416978538\n",
      "Train 113 | out_loss 0.0015088572399690747: 100%|█| 138/138 [00:00<00:00, 314.85\n",
      "Train Epoch113 out_loss -12.550447463989258\n",
      "Test Epoch113 layer0 out_loss 0.02830800600349903\n",
      "Test Epoch113 layer1 out_loss 0.005375853274017572\n",
      "Test Epoch113 layer2 out_loss 0.0014802615623921156\n",
      "Train 114 | out_loss 0.001491735689342022: 100%|█| 138/138 [00:00<00:00, 315.57i\n",
      "Train Epoch114 out_loss -12.244671821594238\n",
      "Test Epoch114 layer0 out_loss 0.028302298858761787\n",
      "Test Epoch114 layer1 out_loss 0.005289486143738031\n",
      "Test Epoch114 layer2 out_loss 0.0014472398906946182\n",
      "Train 115 | out_loss 0.001492852228693664: 100%|█| 138/138 [00:00<00:00, 314.62i\n",
      "Train Epoch115 out_loss -12.264527320861816\n",
      "Test Epoch115 layer0 out_loss 0.027490373700857162\n",
      "Test Epoch115 layer1 out_loss 0.005235145799815655\n",
      "Test Epoch115 layer2 out_loss 0.0014911270700395107\n",
      "Train 116 | out_loss 0.0014665706548839808: 100%|█| 138/138 [00:00<00:00, 276.61\n",
      "Train Epoch116 out_loss -11.801587104797363\n",
      "Test Epoch116 layer0 out_loss 0.02682132087647915\n",
      "Test Epoch116 layer1 out_loss 0.005167356226593256\n",
      "Test Epoch116 layer2 out_loss 0.0014462873805314302\n",
      "Train 117 | out_loss 0.0014539393596351147: 100%|█| 138/138 [00:00<00:00, 310.46\n",
      "Train Epoch117 out_loss -11.582024574279785\n",
      "Test Epoch117 layer0 out_loss 0.02707108110189438\n",
      "Test Epoch117 layer1 out_loss 0.005288262851536274\n",
      "Test Epoch117 layer2 out_loss 0.0014293404528871179\n",
      "Train 118 | out_loss 0.0014289612881839275: 100%|█| 138/138 [00:00<00:00, 318.24\n",
      "Train Epoch118 out_loss -11.153426170349121\n",
      "Test Epoch118 layer0 out_loss 0.02661610208451748\n",
      "Test Epoch118 layer1 out_loss 0.005132587626576424\n",
      "Test Epoch118 layer2 out_loss 0.0014408123679459095\n",
      "Train 119 | out_loss 0.0014199248980730772: 100%|█| 138/138 [00:00<00:00, 319.91\n",
      "Train Epoch119 out_loss -11.000205039978027\n",
      "Test Epoch119 layer0 out_loss 0.02695058286190033\n",
      "Test Epoch119 layer1 out_loss 0.005068555008620024\n",
      "Test Epoch119 layer2 out_loss 0.0013691162457689643\n",
      "Train 120 | out_loss 0.0014036362990736961: 100%|█| 138/138 [00:00<00:00, 314.18\n",
      "Train Epoch120 out_loss -10.72645378112793\n",
      "Test Epoch120 layer0 out_loss 0.02611524611711502\n",
      "Test Epoch120 layer1 out_loss 0.004958372097462416\n",
      "Test Epoch120 layer2 out_loss 0.0013524085516110063\n",
      "Train 121 | out_loss 0.001389852026477456: 100%|█| 138/138 [00:00<00:00, 314.08i\n",
      "Train Epoch121 out_loss -10.497284889221191\n",
      "Test Epoch121 layer0 out_loss 0.02606501244008541\n",
      "Test Epoch121 layer1 out_loss 0.004960573744028807\n",
      "Test Epoch121 layer2 out_loss 0.0013530770083889365\n",
      "Train 122 | out_loss 0.0013832043623551726: 100%|█| 138/138 [00:00<00:00, 314.56\n",
      "Train Epoch122 out_loss -10.387547492980957\n",
      "Test Epoch122 layer0 out_loss 0.025893427431583405\n",
      "Test Epoch122 layer1 out_loss 0.004905981011688709\n",
      "Test Epoch122 layer2 out_loss 0.0013525402173399925\n",
      "Train 123 | out_loss 0.0013697908725589514: 100%|█| 138/138 [00:00<00:00, 309.96\n",
      "Train Epoch123 out_loss -10.167759895324707\n",
      "Test Epoch123 layer0 out_loss 0.02562023140490055\n",
      "Test Epoch123 layer1 out_loss 0.004808744881302118\n",
      "Test Epoch123 layer2 out_loss 0.001335386885330081\n",
      "Train 124 | out_loss 0.0013424884527921677: 100%|█| 138/138 [00:00<00:00, 313.86\n",
      "Train Epoch124 out_loss -9.727012634277344\n",
      "Test Epoch124 layer0 out_loss 0.02558884769678116\n",
      "Test Epoch124 layer1 out_loss 0.004818418528884649\n",
      "Test Epoch124 layer2 out_loss 0.0013102019438520074\n",
      "Train 125 | out_loss 0.0013325376203283668: 100%|█| 138/138 [00:00<00:00, 320.05\n",
      "Train Epoch125 out_loss -9.568575859069824\n",
      "Test Epoch125 layer0 out_loss 0.02557394467294216\n",
      "Test Epoch125 layer1 out_loss 0.004763542674481869\n",
      "Test Epoch125 layer2 out_loss 0.0012872460065409541\n",
      "Train 126 | out_loss 0.001328030601143837: 100%|█| 138/138 [00:00<00:00, 317.50i\n",
      "Train Epoch126 out_loss -9.497212409973145\n",
      "Test Epoch126 layer0 out_loss 0.024835655465722084\n",
      "Test Epoch126 layer1 out_loss 0.004702795762568712\n",
      "Test Epoch126 layer2 out_loss 0.0013029954861849546\n",
      "Train 127 | out_loss 0.001310517080128193: 100%|█| 138/138 [00:00<00:00, 315.60i\n",
      "Train Epoch127 out_loss -9.222159385681152\n",
      "Test Epoch127 layer0 out_loss 0.024941513314843178\n",
      "Test Epoch127 layer1 out_loss 0.004816772881895304\n",
      "Test Epoch127 layer2 out_loss 0.0012631189310923219\n",
      "Train 128 | out_loss 0.001298530725762248: 100%|█| 138/138 [00:00<00:00, 315.36i\n",
      "Train Epoch128 out_loss -9.036028861999512\n",
      "Test Epoch128 layer0 out_loss 0.02405092678964138\n",
      "Test Epoch128 layer1 out_loss 0.004624206107109785\n",
      "Test Epoch128 layer2 out_loss 0.001250716159120202\n",
      "Train 129 | out_loss 0.0012841862626373768: 100%|█| 138/138 [00:00<00:00, 318.88\n",
      "Train Epoch129 out_loss -8.815526962280273\n",
      "Test Epoch129 layer0 out_loss 0.02379034273326397\n",
      "Test Epoch129 layer1 out_loss 0.004577229730784893\n",
      "Test Epoch129 layer2 out_loss 0.001238257740624249\n",
      "Train 130 | out_loss 0.0012796181254088879: 100%|█| 138/138 [00:00<00:00, 319.92\n",
      "Train Epoch130 out_loss -8.745821952819824\n",
      "Test Epoch130 layer0 out_loss 0.02339121326804161\n",
      "Test Epoch130 layer1 out_loss 0.0045127226039767265\n",
      "Test Epoch130 layer2 out_loss 0.0012257994385436177\n",
      "Train 131 | out_loss 0.0012660648208111525: 100%|█| 138/138 [00:00<00:00, 312.03\n",
      "Train Epoch131 out_loss -8.540462493896484\n",
      "Test Epoch131 layer0 out_loss 0.023459242656826973\n",
      "Test Epoch131 layer1 out_loss 0.004537051077932119\n",
      "Test Epoch131 layer2 out_loss 0.0012409129412844777\n",
      "Train 132 | out_loss 0.0012574023567140102: 100%|█| 138/138 [00:00<00:00, 313.11\n",
      "Train Epoch132 out_loss -8.410364151000977\n",
      "Test Epoch132 layer0 out_loss 0.023202791810035706\n",
      "Test Epoch132 layer1 out_loss 0.004456165246665478\n",
      "Test Epoch132 layer2 out_loss 0.0012320250971242785\n",
      "Train 133 | out_loss 0.0012392103672027588: 100%|█| 138/138 [00:00<00:00, 319.96\n",
      "Train Epoch133 out_loss -8.140040397644043\n",
      "Test Epoch133 layer0 out_loss 0.02341843955218792\n",
      "Test Epoch133 layer1 out_loss 0.00447760708630085\n",
      "Test Epoch133 layer2 out_loss 0.0011931095505133271\n",
      "Train 134 | out_loss 0.0012308483710512519: 100%|█| 138/138 [00:00<00:00, 318.11\n",
      "Train Epoch134 out_loss -8.017098426818848\n",
      "Test Epoch134 layer0 out_loss 0.022735049948096275\n",
      "Test Epoch134 layer1 out_loss 0.004390707705169916\n",
      "Test Epoch134 layer2 out_loss 0.001187785528600216\n",
      "Train 135 | out_loss 0.0012093684636056423: 100%|█| 138/138 [00:00<00:00, 319.92\n",
      "Train Epoch135 out_loss -7.705120086669922\n",
      "Test Epoch135 layer0 out_loss 0.022743351757526398\n",
      "Test Epoch135 layer1 out_loss 0.004510956350713968\n",
      "Test Epoch135 layer2 out_loss 0.001260280259884894\n",
      "Train 136 | out_loss 0.0012052702950313687: 100%|█| 138/138 [00:00<00:00, 321.88\n",
      "Train Epoch136 out_loss -7.646221160888672\n",
      "Test Epoch136 layer0 out_loss 0.022354213520884514\n",
      "Test Epoch136 layer1 out_loss 0.00430954759940505\n",
      "Test Epoch136 layer2 out_loss 0.0011950660264119506\n",
      "Train 137 | out_loss 0.001195707474835217: 100%|█| 138/138 [00:00<00:00, 318.22i\n",
      "Train Epoch137 out_loss -7.509574890136719\n",
      "Test Epoch137 layer0 out_loss 0.022190051153302193\n",
      "Test Epoch137 layer1 out_loss 0.004294413607567549\n",
      "Test Epoch137 layer2 out_loss 0.0012976281577721238\n",
      "Train 138 | out_loss 0.0011910399189218879: 100%|█| 138/138 [00:00<00:00, 318.89\n",
      "Train Epoch138 out_loss -7.443267822265625\n",
      "Test Epoch138 layer0 out_loss 0.02249814197421074\n",
      "Test Epoch138 layer1 out_loss 0.004248248413205147\n",
      "Test Epoch138 layer2 out_loss 0.001159749343059957\n",
      "Train 139 | out_loss 0.0011758193140849471: 100%|█| 138/138 [00:00<00:00, 319.05\n",
      "Train Epoch139 out_loss -7.228843688964844\n",
      "Test Epoch139 layer0 out_loss 0.02158273756504059\n",
      "Test Epoch139 layer1 out_loss 0.004215007647871971\n",
      "Test Epoch139 layer2 out_loss 0.0011309743858873844\n",
      "Train 140 | out_loss 0.0011711807455867529: 100%|█| 138/138 [00:00<00:00, 318.10\n",
      "Train Epoch140 out_loss -7.164046287536621\n",
      "Test Epoch140 layer0 out_loss 0.02192682959139347\n",
      "Test Epoch140 layer1 out_loss 0.004207364749163389\n",
      "Test Epoch140 layer2 out_loss 0.0011810739524662495\n",
      "Train 141 | out_loss 0.0011471464531496167: 100%|█| 138/138 [00:00<00:00, 318.14\n",
      "Train Epoch141 out_loss -6.832413673400879\n",
      "Test Epoch141 layer0 out_loss 0.021768057718873024\n",
      "Test Epoch141 layer1 out_loss 0.004165809601545334\n",
      "Test Epoch141 layer2 out_loss 0.001133136684074998\n",
      "Train 142 | out_loss 0.0011406175326555967: 100%|█| 138/138 [00:00<00:00, 320.54\n",
      "Train Epoch142 out_loss -6.743504047393799\n",
      "Test Epoch142 layer0 out_loss 0.0218492578715086\n",
      "Test Epoch142 layer1 out_loss 0.004181466996669769\n",
      "Test Epoch142 layer2 out_loss 0.0011075243819504976\n",
      "Train 143 | out_loss 0.001133927027694881: 100%|█| 138/138 [00:00<00:00, 322.14i\n",
      "Train Epoch143 out_loss -6.652929782867432\n",
      "Test Epoch143 layer0 out_loss 0.021349357441067696\n",
      "Test Epoch143 layer1 out_loss 0.004172697197645903\n",
      "Test Epoch143 layer2 out_loss 0.0010913455625995994\n",
      "Train 144 | out_loss 0.001125073991715908: 100%|█| 138/138 [00:00<00:00, 320.90i\n",
      "Train Epoch144 out_loss -6.533900260925293\n",
      "Test Epoch144 layer0 out_loss 0.0213782899081707\n",
      "Test Epoch144 layer1 out_loss 0.004103022161871195\n",
      "Test Epoch144 layer2 out_loss 0.0011098758550360799\n",
      "Train 145 | out_loss 0.0011145442258566618: 100%|█| 138/138 [00:00<00:00, 324.69\n",
      "Train Epoch145 out_loss -6.393545150756836\n",
      "Test Epoch145 layer0 out_loss 0.021208271384239197\n",
      "Test Epoch145 layer1 out_loss 0.004152501467615366\n",
      "Test Epoch145 layer2 out_loss 0.0011290814727544785\n",
      "Train 146 | out_loss 0.0011024324921891093: 100%|█| 138/138 [00:00<00:00, 321.14\n",
      "Train Epoch146 out_loss -6.233722686767578\n",
      "Test Epoch146 layer0 out_loss 0.02110428363084793\n",
      "Test Epoch146 layer1 out_loss 0.003990634810179472\n",
      "Test Epoch146 layer2 out_loss 0.0010705175809562206\n",
      "Train 147 | out_loss 0.0010963848326355219: 100%|█| 138/138 [00:00<00:00, 318.92\n",
      "Train Epoch147 out_loss -6.154572486877441\n",
      "Test Epoch147 layer0 out_loss 0.020815791562199593\n",
      "Test Epoch147 layer1 out_loss 0.004034395329654217\n",
      "Test Epoch147 layer2 out_loss 0.0010630759643390775\n",
      "Train 148 | out_loss 0.001084033166989684: 100%|█| 138/138 [00:00<00:00, 317.26i\n",
      "Train Epoch148 out_loss -5.994282245635986\n",
      "Test Epoch148 layer0 out_loss 0.020513994619250298\n",
      "Test Epoch148 layer1 out_loss 0.003941103350371122\n",
      "Test Epoch148 layer2 out_loss 0.0010415799915790558\n",
      "Train 149 | out_loss 0.0010774416150525212: 100%|█| 138/138 [00:00<00:00, 322.98\n",
      "Train Epoch149 out_loss -5.909477710723877\n",
      "Test Epoch149 layer0 out_loss 0.020316073670983315\n",
      "Test Epoch149 layer1 out_loss 0.003919375594705343\n",
      "Test Epoch149 layer2 out_loss 0.001032192842103541\n",
      "Train 150 | out_loss 0.001066481345333159: 100%|█| 138/138 [00:00<00:00, 320.97i\n",
      "Train Epoch150 out_loss -5.769627094268799\n",
      "Test Epoch150 layer0 out_loss 0.019888674840331078\n",
      "Test Epoch150 layer1 out_loss 0.003958356566727161\n",
      "Test Epoch150 layer2 out_loss 0.001076837070286274\n",
      "Train 151 | out_loss 0.0010637654922902584: 100%|█| 138/138 [00:00<00:00, 323.90\n",
      "Train Epoch151 out_loss -5.735184669494629\n",
      "Test Epoch151 layer0 out_loss 0.019749436527490616\n",
      "Test Epoch151 layer1 out_loss 0.003869406646117568\n",
      "Test Epoch151 layer2 out_loss 0.0010216057999059558\n",
      "Train 152 | out_loss 0.0010477503528818488: 100%|█| 138/138 [00:00<00:00, 317.74\n",
      "Train Epoch152 out_loss -5.533915996551514\n",
      "Test Epoch152 layer0 out_loss 0.019568508490920067\n",
      "Test Epoch152 layer1 out_loss 0.003829249180853367\n",
      "Test Epoch152 layer2 out_loss 0.0010491968132555485\n",
      "Train 153 | out_loss 0.001055806060321629: 100%|█| 138/138 [00:00<00:00, 301.38i\n",
      "Train Epoch153 out_loss -5.634768009185791\n",
      "Test Epoch153 layer0 out_loss 0.019443070515990257\n",
      "Test Epoch153 layer1 out_loss 0.0040712361223995686\n",
      "Test Epoch153 layer2 out_loss 0.0010017777094617486\n",
      "Train 154 | out_loss 0.001041177660226822: 100%|█| 138/138 [00:00<00:00, 314.12i\n",
      "Train Epoch154 out_loss -5.4521965980529785\n",
      "Test Epoch154 layer0 out_loss 0.01960715278983116\n",
      "Test Epoch154 layer1 out_loss 0.0037959616165608168\n",
      "Test Epoch154 layer2 out_loss 0.0010034618899226189\n",
      "Train 155 | out_loss 0.0010268855839967728: 100%|█| 138/138 [00:00<00:00, 313.17\n",
      "Train Epoch155 out_loss -5.276270866394043\n",
      "Test Epoch155 layer0 out_loss 0.01914776675403118\n",
      "Test Epoch155 layer1 out_loss 0.0038333835545927286\n",
      "Test Epoch155 layer2 out_loss 0.0009952179389074445\n",
      "Train 156 | out_loss 0.001019287039525807: 100%|█| 138/138 [00:00<00:00, 314.71i\n",
      "Train Epoch156 out_loss -5.183732509613037\n",
      "Test Epoch156 layer0 out_loss 0.01908094249665737\n",
      "Test Epoch156 layer1 out_loss 0.003718686755746603\n",
      "Test Epoch156 layer2 out_loss 0.0009926120983436704\n",
      "Train 157 | out_loss 0.0010088197886943817: 100%|█| 138/138 [00:00<00:00, 309.47\n",
      "Train Epoch157 out_loss -5.057385444641113\n",
      "Test Epoch157 layer0 out_loss 0.018871081992983818\n",
      "Test Epoch157 layer1 out_loss 0.003699082648381591\n",
      "Test Epoch157 layer2 out_loss 0.0009996577864512801\n",
      "Train 158 | out_loss 0.0010053484002128243: 100%|█| 138/138 [00:00<00:00, 302.66\n",
      "Train Epoch158 out_loss -5.015768527984619\n",
      "Test Epoch158 layer0 out_loss 0.018471593037247658\n",
      "Test Epoch158 layer1 out_loss 0.0036597182042896748\n",
      "Test Epoch158 layer2 out_loss 0.0009841998107731342\n",
      "Train 159 | out_loss 0.0009900734294205904: 100%|█| 138/138 [00:00<00:00, 316.22\n",
      "Train Epoch159 out_loss -4.834348201751709\n",
      "Test Epoch159 layer0 out_loss 0.018403975293040276\n",
      "Test Epoch159 layer1 out_loss 0.0036407511215656996\n",
      "Test Epoch159 layer2 out_loss 0.0009550133254379034\n",
      "Train 160 | out_loss 0.0009806083980947733: 100%|█| 138/138 [00:00<00:00, 316.89\n",
      "Train Epoch160 out_loss -4.723332405090332\n",
      "Test Epoch160 layer0 out_loss 0.018347177654504776\n",
      "Test Epoch160 layer1 out_loss 0.0036787663120776415\n",
      "Test Epoch160 layer2 out_loss 0.000974987808149308\n",
      "Train 161 | out_loss 0.0009743833215907216: 100%|█| 138/138 [00:00<00:00, 319.62\n",
      "Train Epoch161 out_loss -4.6508989334106445\n",
      "Test Epoch161 layer0 out_loss 0.01790432631969452\n",
      "Test Epoch161 layer1 out_loss 0.0035936820786446333\n",
      "Test Epoch161 layer2 out_loss 0.0009464576723985374\n",
      "Train 162 | out_loss 0.0009738903609104455: 100%|█| 138/138 [00:00<00:00, 318.83\n",
      "Train Epoch162 out_loss -4.6451802253723145\n",
      "Test Epoch162 layer0 out_loss 0.017601411789655685\n",
      "Test Epoch162 layer1 out_loss 0.0035983517300337553\n",
      "Test Epoch162 layer2 out_loss 0.0009329579188488424\n",
      "Train 163 | out_loss 0.0009639085037633777: 100%|█| 138/138 [00:00<00:00, 307.53\n",
      "Train Epoch163 out_loss -4.530049800872803\n",
      "Test Epoch163 layer0 out_loss 0.017452403903007507\n",
      "Test Epoch163 layer1 out_loss 0.0036155139096081257\n",
      "Test Epoch163 layer2 out_loss 0.000935345480684191\n",
      "Train 164 | out_loss 0.0009507050854153931: 100%|█| 138/138 [00:00<00:00, 313.11\n",
      "Train Epoch164 out_loss -4.379593372344971\n",
      "Test Epoch164 layer0 out_loss 0.01719515025615692\n",
      "Test Epoch164 layer1 out_loss 0.0035273577086627483\n",
      "Test Epoch164 layer2 out_loss 0.0009473296231590211\n",
      "Train 165 | out_loss 0.0009469524375163019: 100%|█| 138/138 [00:00<00:00, 305.37\n",
      "Train Epoch165 out_loss -4.337204456329346\n",
      "Test Epoch165 layer0 out_loss 0.017489925026893616\n",
      "Test Epoch165 layer1 out_loss 0.0034965630620718002\n",
      "Test Epoch165 layer2 out_loss 0.0009252508752979338\n",
      "Train 166 | out_loss 0.0009390015038661659: 100%|█| 138/138 [00:00<00:00, 314.18\n",
      "Train Epoch166 out_loss -4.24796199798584\n",
      "Test Epoch166 layer0 out_loss 0.01678270474076271\n",
      "Test Epoch166 layer1 out_loss 0.0035122002009302378\n",
      "Test Epoch166 layer2 out_loss 0.0009157413151115179\n",
      "Train 167 | out_loss 0.0009300130768679082: 100%|█| 138/138 [00:00<00:00, 318.59\n",
      "Train Epoch167 out_loss -4.147968769073486\n",
      "Test Epoch167 layer0 out_loss 0.016567938029766083\n",
      "Test Epoch167 layer1 out_loss 0.0034466804936528206\n",
      "Test Epoch167 layer2 out_loss 0.0009439511341042817\n",
      "Train 168 | out_loss 0.000923674029763788: 100%|█| 138/138 [00:00<00:00, 316.45i\n",
      "Train Epoch168 out_loss -4.078023910522461\n",
      "Test Epoch168 layer0 out_loss 0.01653047278523445\n",
      "Test Epoch168 layer1 out_loss 0.0035030199214816093\n",
      "Test Epoch168 layer2 out_loss 0.0008956891833804548\n",
      "Train 169 | out_loss 0.0009149290854111314: 100%|█| 138/138 [00:00<00:00, 320.85\n",
      "Train Epoch169 out_loss -3.9823312759399414\n",
      "Test Epoch169 layer0 out_loss 0.01642196625471115\n",
      "Test Epoch169 layer1 out_loss 0.003395560896024108\n",
      "Test Epoch169 layer2 out_loss 0.0008857808425091207\n",
      "Train 170 | out_loss 0.000913062016479671: 100%|█| 138/138 [00:00<00:00, 319.52i\n",
      "Train Epoch170 out_loss -3.9620118141174316\n",
      "Test Epoch170 layer0 out_loss 0.01607345975935459\n",
      "Test Epoch170 layer1 out_loss 0.0035016608890146017\n",
      "Test Epoch170 layer2 out_loss 0.0009079298470169306\n",
      "Train 171 | out_loss 0.0009024034952744842: 100%|█| 138/138 [00:00<00:00, 315.01\n",
      "Train Epoch171 out_loss -3.8468456268310547\n",
      "Test Epoch171 layer0 out_loss 0.01594981551170349\n",
      "Test Epoch171 layer1 out_loss 0.0033479873090982437\n",
      "Test Epoch171 layer2 out_loss 0.00086104596266523\n",
      "Train 172 | out_loss 0.0008966311579570174: 100%|█| 138/138 [00:00<00:00, 314.32\n",
      "Train Epoch172 out_loss -3.7850356101989746\n",
      "Test Epoch172 layer0 out_loss 0.015907425433397293\n",
      "Test Epoch172 layer1 out_loss 0.003360832342877984\n",
      "Test Epoch172 layer2 out_loss 0.0009257554193027318\n",
      "Train 173 | out_loss 0.0008876647334545851: 100%|█| 138/138 [00:00<00:00, 316.28\n",
      "Train Epoch173 out_loss -3.6898159980773926\n",
      "Test Epoch173 layer0 out_loss 0.015545559115707874\n",
      "Test Epoch173 layer1 out_loss 0.0033103807363659143\n",
      "Test Epoch173 layer2 out_loss 0.0008435560157522559\n",
      "Train 174 | out_loss 0.0008875967469066381: 100%|█| 138/138 [00:00<00:00, 315.50\n",
      "Train Epoch174 out_loss -3.689096450805664\n",
      "Test Epoch174 layer0 out_loss 0.015361347235739231\n",
      "Test Epoch174 layer1 out_loss 0.0032995345536619425\n",
      "Test Epoch174 layer2 out_loss 0.0008573358063586056\n",
      "Train 175 | out_loss 0.0008798815542832017: 100%|█| 138/138 [00:00<00:00, 316.51\n",
      "Train Epoch175 out_loss -3.607933521270752\n",
      "Test Epoch175 layer0 out_loss 0.015172774903476238\n",
      "Test Epoch175 layer1 out_loss 0.003271836321800947\n",
      "Test Epoch175 layer2 out_loss 0.0008768396219238639\n",
      "Train 176 | out_loss 0.0008706485969014466: 100%|█| 138/138 [00:00<00:00, 313.63\n",
      "Train Epoch176 out_loss -3.511735439300537\n",
      "Test Epoch176 layer0 out_loss 0.014970931224524975\n",
      "Test Epoch176 layer1 out_loss 0.003275561612099409\n",
      "Test Epoch176 layer2 out_loss 0.0008480158285237849\n",
      "Train 177 | out_loss 0.0008589619537815452: 100%|█| 138/138 [00:00<00:00, 317.25\n",
      "Train Epoch177 out_loss -3.3914241790771484\n",
      "Test Epoch177 layer0 out_loss 0.014902726747095585\n",
      "Test Epoch177 layer1 out_loss 0.0032954555936157703\n",
      "Test Epoch177 layer2 out_loss 0.0008323639049194753\n",
      "Train 178 | out_loss 0.0008566654287278652: 100%|█| 138/138 [00:00<00:00, 317.77\n",
      "Train Epoch178 out_loss -3.367980480194092\n",
      "Test Epoch178 layer0 out_loss 0.01505272462964058\n",
      "Test Epoch178 layer1 out_loss 0.003302148310467601\n",
      "Test Epoch178 layer2 out_loss 0.0008326007518917322\n",
      "Train 179 | out_loss 0.0008497560047544539: 100%|█| 138/138 [00:00<00:00, 301.47\n",
      "Train Epoch179 out_loss -3.2978034019470215\n",
      "Test Epoch179 layer0 out_loss 0.014688238501548767\n",
      "Test Epoch179 layer1 out_loss 0.003189271781593561\n",
      "Test Epoch179 layer2 out_loss 0.0008930783369578421\n",
      "Train 180 | out_loss 0.0008426454733125865: 100%|█| 138/138 [00:00<00:00, 313.45\n",
      "Train Epoch180 out_loss -3.2261757850646973\n",
      "Test Epoch180 layer0 out_loss 0.01432304922491312\n",
      "Test Epoch180 layer1 out_loss 0.003163360757753253\n",
      "Test Epoch180 layer2 out_loss 0.0008868264849297702\n",
      "Train 181 | out_loss 0.0008329296833835542: 100%|█| 138/138 [00:00<00:00, 312.72\n",
      "Train Epoch181 out_loss -3.129281997680664\n",
      "Test Epoch181 layer0 out_loss 0.014112270437180996\n",
      "Test Epoch181 layer1 out_loss 0.003110775025561452\n",
      "Test Epoch181 layer2 out_loss 0.0007957206689752638\n",
      "Train 182 | out_loss 0.0008329970296472311: 100%|█| 138/138 [00:00<00:00, 315.08\n",
      "Train Epoch182 out_loss -3.129948139190674\n",
      "Test Epoch182 layer0 out_loss 0.013888676650822163\n",
      "Test Epoch182 layer1 out_loss 0.00309220002964139\n",
      "Test Epoch182 layer2 out_loss 0.0007852910202927887\n",
      "Train 183 | out_loss 0.0008258700836449862: 100%|█| 138/138 [00:00<00:00, 318.74\n",
      "Train Epoch183 out_loss -3.0595803260803223\n",
      "Test Epoch183 layer0 out_loss 0.01372764352709055\n",
      "Test Epoch183 layer1 out_loss 0.0030812090262770653\n",
      "Test Epoch183 layer2 out_loss 0.0007901799981482327\n",
      "Train 184 | out_loss 0.0008111590286716819: 100%|█| 138/138 [00:00<00:00, 311.55\n",
      "Train Epoch184 out_loss -2.9162418842315674\n",
      "Test Epoch184 layer0 out_loss 0.013654178939759731\n",
      "Test Epoch184 layer1 out_loss 0.0030573077965527773\n",
      "Test Epoch184 layer2 out_loss 0.0008165154722519219\n",
      "Train 185 | out_loss 0.0008056969963945448: 100%|█| 138/138 [00:00<00:00, 309.90\n",
      "Train Epoch185 out_loss -2.8636772632598877\n",
      "Test Epoch185 layer0 out_loss 0.013496657833456993\n",
      "Test Epoch185 layer1 out_loss 0.0031061943154782057\n",
      "Test Epoch185 layer2 out_loss 0.0008602967136539519\n",
      "Train 186 | out_loss 0.0008030906901694834: 100%|█| 138/138 [00:00<00:00, 312.92\n",
      "Train Epoch186 out_loss -2.8387250900268555\n",
      "Test Epoch186 layer0 out_loss 0.01325999479740858\n",
      "Test Epoch186 layer1 out_loss 0.00307457079179585\n",
      "Test Epoch186 layer2 out_loss 0.0008328569820150733\n",
      "Train 187 | out_loss 0.0007949511054903269: 100%|█| 138/138 [00:00<00:00, 318.33\n",
      "Train Epoch187 out_loss -2.7613015174865723\n",
      "Test Epoch187 layer0 out_loss 0.013098981231451035\n",
      "Test Epoch187 layer1 out_loss 0.003061346709728241\n",
      "Test Epoch187 layer2 out_loss 0.000782809394877404\n",
      "Train 188 | out_loss 0.0007928663399070501: 100%|█| 138/138 [00:00<00:00, 316.39\n",
      "Train Epoch188 out_loss -2.7416040897369385\n",
      "Test Epoch188 layer0 out_loss 0.012959721498191357\n",
      "Test Epoch188 layer1 out_loss 0.0029920628294348717\n",
      "Test Epoch188 layer2 out_loss 0.0008242460899055004\n",
      "Train 189 | out_loss 0.0007946766563691199: 100%|█| 138/138 [00:00<00:00, 308.19\n",
      "Train Epoch189 out_loss -2.7587082386016846\n",
      "Test Epoch189 layer0 out_loss 0.012754571624100208\n",
      "Test Epoch189 layer1 out_loss 0.0029717390425503254\n",
      "Test Epoch189 layer2 out_loss 0.0007861963240429759\n",
      "Train 190 | out_loss 0.000774899497628212: 100%|█| 138/138 [00:00<00:00, 315.38i\n",
      "Train Epoch190 out_loss -2.5739517211914062\n",
      "Test Epoch190 layer0 out_loss 0.012516764923930168\n",
      "Test Epoch190 layer1 out_loss 0.0029426743276417255\n",
      "Test Epoch190 layer2 out_loss 0.000771857041399926\n",
      "Train 191 | out_loss 0.0007715658284723759: 100%|█| 138/138 [00:00<00:00, 313.15\n",
      "Train Epoch191 out_loss -2.5432653427124023\n",
      "Test Epoch191 layer0 out_loss 0.012411759234964848\n",
      "Test Epoch191 layer1 out_loss 0.0030305946711450815\n",
      "Test Epoch191 layer2 out_loss 0.0007381688337773085\n",
      "Train 192 | out_loss 0.0007681971183046699: 100%|█| 138/138 [00:00<00:00, 309.77\n",
      "Train Epoch192 out_loss -2.512392044067383\n",
      "Test Epoch192 layer0 out_loss 0.01230765599757433\n",
      "Test Epoch192 layer1 out_loss 0.0029130668845027685\n",
      "Test Epoch192 layer2 out_loss 0.0007286797626875341\n",
      "Train 193 | out_loss 0.0007600393728353083: 100%|█| 138/138 [00:00<00:00, 308.64\n",
      "Train Epoch193 out_loss -2.4381930828094482\n",
      "Test Epoch193 layer0 out_loss 0.012243231758475304\n",
      "Test Epoch193 layer1 out_loss 0.0030319674406200647\n",
      "Test Epoch193 layer2 out_loss 0.0007286169566214085\n",
      "Train 194 | out_loss 0.000755352433770895: 100%|█| 138/138 [00:00<00:00, 317.94i\n",
      "Train Epoch194 out_loss -2.395918369293213\n",
      "Test Epoch194 layer0 out_loss 0.011936787515878677\n",
      "Test Epoch194 layer1 out_loss 0.002856905572116375\n",
      "Test Epoch194 layer2 out_loss 0.0007304995669983327\n",
      "Train 195 | out_loss 0.0007486128015443683: 100%|█| 138/138 [00:00<00:00, 312.43\n",
      "Train Epoch195 out_loss -2.3355839252471924\n",
      "Test Epoch195 layer0 out_loss 0.01190109457820654\n",
      "Test Epoch195 layer1 out_loss 0.0028476337902247906\n",
      "Test Epoch195 layer2 out_loss 0.0007114394102245569\n",
      "Train 196 | out_loss 0.0007432359852828085: 100%|█| 138/138 [00:00<00:00, 271.40\n",
      "Train Epoch196 out_loss -2.2878429889678955\n",
      "Test Epoch196 layer0 out_loss 0.011873496696352959\n",
      "Test Epoch196 layer1 out_loss 0.002846501301974058\n",
      "Test Epoch196 layer2 out_loss 0.0007189831230789423\n",
      "Train 197 | out_loss 0.000741237192414701: 100%|█| 138/138 [00:00<00:00, 303.15i\n",
      "Train Epoch197 out_loss -2.2701852321624756\n",
      "Test Epoch197 layer0 out_loss 0.011577686294913292\n",
      "Test Epoch197 layer1 out_loss 0.002798296045511961\n",
      "Test Epoch197 layer2 out_loss 0.0007052026339806616\n",
      "Train 198 | out_loss 0.0007377046276815236: 100%|█| 138/138 [00:00<00:00, 313.58\n",
      "Train Epoch198 out_loss -2.2390880584716797\n",
      "Test Epoch198 layer0 out_loss 0.011318379081785679\n",
      "Test Epoch198 layer1 out_loss 0.0027756388299167156\n",
      "Test Epoch198 layer2 out_loss 0.000745428551454097\n",
      "Train 199 | out_loss 0.0007272323127835989: 100%|█| 138/138 [00:00<00:00, 310.60\n",
      "Train Epoch199 out_loss -2.1477739810943604\n",
      "Test Epoch199 layer0 out_loss 0.01127389445900917\n",
      "Test Epoch199 layer1 out_loss 0.0027905490715056658\n",
      "Test Epoch199 layer2 out_loss 0.0007140777888707817\n",
      "Train 200 | out_loss 0.0007253932417370379: 100%|█| 138/138 [00:00<00:00, 312.58\n",
      "Train Epoch200 out_loss -2.131876230239868\n",
      "Test Epoch200 layer0 out_loss 0.011255292221903801\n",
      "Test Epoch200 layer1 out_loss 0.00273237400688231\n",
      "Test Epoch200 layer2 out_loss 0.0007129556615836918\n",
      "Train 201 | out_loss 0.0007114656618796289: 100%|█| 138/138 [00:00<00:00, 310.28\n",
      "Train Epoch201 out_loss -2.0127665996551514\n",
      "Test Epoch201 layer0 out_loss 0.010934275574982166\n",
      "Test Epoch201 layer1 out_loss 0.0027657996397465467\n",
      "Test Epoch201 layer2 out_loss 0.0007047528633847833\n",
      "Train 202 | out_loss 0.0007223566644825041: 100%|█| 138/138 [00:00<00:00, 311.36\n",
      "Train Epoch202 out_loss -2.1057119369506836\n",
      "Test Epoch202 layer0 out_loss 0.01086688693612814\n",
      "Test Epoch202 layer1 out_loss 0.0027059291023761034\n",
      "Test Epoch202 layer2 out_loss 0.0007493983721360564\n",
      "Train 203 | out_loss 0.0007124855765141547: 100%|█| 138/138 [00:00<00:00, 313.43\n",
      "Train Epoch203 out_loss -2.0214123725891113\n",
      "Test Epoch203 layer0 out_loss 0.010818217881023884\n",
      "Test Epoch203 layer1 out_loss 0.0027149738743901253\n",
      "Test Epoch203 layer2 out_loss 0.0007245044107548892\n",
      "Train 204 | out_loss 0.0007038896437734365: 100%|█| 138/138 [00:00<00:00, 315.40\n",
      "Train Epoch204 out_loss -1.9489455223083496\n",
      "Test Epoch204 layer0 out_loss 0.010590607300400734\n",
      "Test Epoch204 layer1 out_loss 0.0026630365755409002\n",
      "Test Epoch204 layer2 out_loss 0.0006963421474210918\n",
      "Train 205 | out_loss 0.0006927041104063392: 100%|█| 138/138 [00:00<00:00, 314.28\n",
      "Train Epoch205 out_loss -1.8559679985046387\n",
      "Test Epoch205 layer0 out_loss 0.010462800972163677\n",
      "Test Epoch205 layer1 out_loss 0.0027036310639232397\n",
      "Test Epoch205 layer2 out_loss 0.0006742995465174317\n",
      "Train 206 | out_loss 0.0006910352967679501: 100%|█| 138/138 [00:00<00:00, 312.26\n",
      "Train Epoch206 out_loss -1.8422226905822754\n",
      "Test Epoch206 layer0 out_loss 0.010476300492882729\n",
      "Test Epoch206 layer1 out_loss 0.0027067740447819233\n",
      "Test Epoch206 layer2 out_loss 0.0006681812810711563\n",
      "Train 207 | out_loss 0.000689181440975517: 100%|█| 138/138 [00:00<00:00, 313.83i\n",
      "Train Epoch207 out_loss -1.826993465423584\n",
      "Test Epoch207 layer0 out_loss 0.010817334987223148\n",
      "Test Epoch207 layer1 out_loss 0.0026493147015571594\n",
      "Test Epoch207 layer2 out_loss 0.0006617690087296069\n",
      "Train 208 | out_loss 0.0006874795071780682: 100%|█| 138/138 [00:00<00:00, 307.39\n",
      "Train Epoch208 out_loss -1.8130455017089844\n",
      "Test Epoch208 layer0 out_loss 0.010469735600054264\n",
      "Test Epoch208 layer1 out_loss 0.0026246237102895975\n",
      "Test Epoch208 layer2 out_loss 0.0006494307890534401\n",
      "Train 209 | out_loss 0.0006894340622238815: 100%|█| 138/138 [00:00<00:00, 312.62\n",
      "Train Epoch209 out_loss -1.8290693759918213\n",
      "Test Epoch209 layer0 out_loss 0.01035922672599554\n",
      "Test Epoch209 layer1 out_loss 0.00260625290684402\n",
      "Test Epoch209 layer2 out_loss 0.0006417585536837578\n",
      "Train 210 | out_loss 0.0006727881846018136: 100%|█| 138/138 [00:00<00:00, 320.63\n",
      "Train Epoch210 out_loss -1.6941032409667969\n",
      "Test Epoch210 layer0 out_loss 0.010166948661208153\n",
      "Test Epoch210 layer1 out_loss 0.0025577181950211525\n",
      "Test Epoch210 layer2 out_loss 0.0006600389024242759\n",
      "Train 211 | out_loss 0.0006788669852539897: 100%|█| 138/138 [00:00<00:00, 318.31\n",
      "Train Epoch211 out_loss -1.743006944656372\n",
      "Test Epoch211 layer0 out_loss 0.010086792521178722\n",
      "Test Epoch211 layer1 out_loss 0.002536621643230319\n",
      "Test Epoch211 layer2 out_loss 0.0006395767559297383\n",
      "Train 212 | out_loss 0.0006636579637415707: 100%|█| 138/138 [00:00<00:00, 310.82\n",
      "Train Epoch212 out_loss -1.6214795112609863\n",
      "Test Epoch212 layer0 out_loss 0.01004994660615921\n",
      "Test Epoch212 layer1 out_loss 0.0025415527634322643\n",
      "Test Epoch212 layer2 out_loss 0.0006590117700397968\n",
      "Train 213 | out_loss 0.0006631645956076682: 100%|█| 138/138 [00:00<00:00, 309.88\n",
      "Train Epoch213 out_loss -1.6175832748413086\n",
      "Test Epoch213 layer0 out_loss 0.009936848655343056\n",
      "Test Epoch213 layer1 out_loss 0.002606844762340188\n",
      "Test Epoch213 layer2 out_loss 0.0006598877371288836\n",
      "Train 214 | out_loss 0.0006587589741684496: 100%|█| 138/138 [00:00<00:00, 314.90\n",
      "Train Epoch214 out_loss -1.5829226970672607\n",
      "Test Epoch214 layer0 out_loss 0.009910926222801208\n",
      "Test Epoch214 layer1 out_loss 0.0025294218212366104\n",
      "Test Epoch214 layer2 out_loss 0.0006204670644365251\n",
      "Train 215 | out_loss 0.0006493738619610667: 100%|█| 138/138 [00:00<00:00, 316.40\n",
      "Train Epoch215 out_loss -1.50984787940979\n",
      "Test Epoch215 layer0 out_loss 0.009802468121051788\n",
      "Test Epoch215 layer1 out_loss 0.0024967954959720373\n",
      "Test Epoch215 layer2 out_loss 0.0006602774374186993\n",
      "Train 216 | out_loss 0.0006477781571447849: 100%|█| 138/138 [00:00<00:00, 317.31\n",
      "Train Epoch216 out_loss -1.4975261688232422\n",
      "Test Epoch216 layer0 out_loss 0.00950681883841753\n",
      "Test Epoch216 layer1 out_loss 0.002504125004634261\n",
      "Test Epoch216 layer2 out_loss 0.0006474832771345973\n",
      "Train 217 | out_loss 0.0006466852501034737: 100%|█| 138/138 [00:00<00:00, 319.71\n",
      "Train Epoch217 out_loss -1.4891066551208496\n",
      "Test Epoch217 layer0 out_loss 0.009770471602678299\n",
      "Test Epoch217 layer1 out_loss 0.0024488659109920263\n",
      "Test Epoch217 layer2 out_loss 0.0006161116762086749\n",
      "Train 218 | out_loss 0.000641818915028125: 100%|█| 138/138 [00:00<00:00, 318.96i\n",
      "Train Epoch218 out_loss -1.451784610748291\n",
      "Test Epoch218 layer0 out_loss 0.0092994449660182\n",
      "Test Epoch218 layer1 out_loss 0.0025207879953086376\n",
      "Test Epoch218 layer2 out_loss 0.0006066365167498589\n",
      "Train 219 | out_loss 0.0006371230119839311: 100%|█| 138/138 [00:00<00:00, 319.29\n",
      "Train Epoch219 out_loss -1.4160418510437012\n",
      "Test Epoch219 layer0 out_loss 0.009331521578133106\n",
      "Test Epoch219 layer1 out_loss 0.002493432490155101\n",
      "Test Epoch219 layer2 out_loss 0.0006174767040647566\n",
      "Train 220 | out_loss 0.0006384065491147339: 100%|█| 138/138 [00:00<00:00, 319.26\n",
      "Train Epoch220 out_loss -1.4257831573486328\n",
      "Test Epoch220 layer0 out_loss 0.009121737442910671\n",
      "Test Epoch220 layer1 out_loss 0.002412105444818735\n",
      "Test Epoch220 layer2 out_loss 0.0006373421638272703\n",
      "Train 221 | out_loss 0.0006266227574087679: 100%|█| 138/138 [00:00<00:00, 313.82\n",
      "Train Epoch221 out_loss -1.3370623588562012\n",
      "Test Epoch221 layer0 out_loss 0.009075230918824673\n",
      "Test Epoch221 layer1 out_loss 0.0023975945077836514\n",
      "Test Epoch221 layer2 out_loss 0.0007578050717711449\n",
      "Train 222 | out_loss 0.0006208663689903915: 100%|█| 138/138 [00:00<00:00, 316.11\n",
      "Train Epoch222 out_loss -1.294321060180664\n",
      "Test Epoch222 layer0 out_loss 0.00892437994480133\n",
      "Test Epoch222 layer1 out_loss 0.002386931562796235\n",
      "Test Epoch222 layer2 out_loss 0.0005887128063477576\n",
      "Train 223 | out_loss 0.0006151476409286261: 100%|█| 138/138 [00:00<00:00, 316.41\n",
      "Train Epoch223 out_loss -1.2522504329681396\n",
      "Test Epoch223 layer0 out_loss 0.008807720616459846\n",
      "Test Epoch223 layer1 out_loss 0.002370374044403434\n",
      "Test Epoch223 layer2 out_loss 0.000603844178840518\n",
      "Train 224 | out_loss 0.0006154461880214512: 100%|█| 138/138 [00:00<00:00, 315.13\n",
      "Train Epoch224 out_loss -1.2544379234313965\n",
      "Test Epoch224 layer0 out_loss 0.008725345134735107\n",
      "Test Epoch224 layer1 out_loss 0.0023495606146752834\n",
      "Test Epoch224 layer2 out_loss 0.0006113343988545239\n",
      "Train 225 | out_loss 0.0006060987943783402: 100%|█| 138/138 [00:00<00:00, 313.72\n",
      "Train Epoch225 out_loss -1.1864726543426514\n",
      "Test Epoch225 layer0 out_loss 0.008823261596262455\n",
      "Test Epoch225 layer1 out_loss 0.002340238308534026\n",
      "Test Epoch225 layer2 out_loss 0.0005811633891426027\n",
      "Train 226 | out_loss 0.0006104362546466291: 100%|█| 138/138 [00:00<00:00, 318.65\n",
      "Train Epoch226 out_loss -1.217881679534912\n",
      "Test Epoch226 layer0 out_loss 0.008796555921435356\n",
      "Test Epoch226 layer1 out_loss 0.0023735957220196724\n",
      "Test Epoch226 layer2 out_loss 0.0006540660979226232\n",
      "Train 227 | out_loss 0.0005978649714961648: 100%|█| 138/138 [00:00<00:00, 307.99\n",
      "Train Epoch227 out_loss -1.1274731159210205\n",
      "Test Epoch227 layer0 out_loss 0.008457979187369347\n",
      "Test Epoch227 layer1 out_loss 0.0023765091318637133\n",
      "Test Epoch227 layer2 out_loss 0.0005727204843424261\n",
      "Train 228 | out_loss 0.000604093074798584: 100%|█| 138/138 [00:00<00:00, 320.32i\n",
      "Train Epoch228 out_loss -1.1720302104949951\n",
      "Test Epoch228 layer0 out_loss 0.00833395030349493\n",
      "Test Epoch228 layer1 out_loss 0.002363039879128337\n",
      "Test Epoch228 layer2 out_loss 0.0005846342537552118\n",
      "Train 229 | out_loss 0.0005943806027062237: 100%|█| 138/138 [00:00<00:00, 306.76\n",
      "Train Epoch229 out_loss -1.1027462482452393\n",
      "Test Epoch229 layer0 out_loss 0.0084223048761487\n",
      "Test Epoch229 layer1 out_loss 0.0022572327870875597\n",
      "Test Epoch229 layer2 out_loss 0.0005878262454643846\n",
      "Train 230 | out_loss 0.0005927675520069897: 100%|█| 138/138 [00:00<00:00, 312.19\n",
      "Train Epoch230 out_loss -1.09134840965271\n",
      "Test Epoch230 layer0 out_loss 0.008212060667574406\n",
      "Test Epoch230 layer1 out_loss 0.002252290491014719\n",
      "Test Epoch230 layer2 out_loss 0.0005670911050401628\n",
      "Train 231 | out_loss 0.0005903665442019701: 100%|█| 138/138 [00:00<00:00, 309.70\n",
      "Train Epoch231 out_loss -1.074439525604248\n",
      "Test Epoch231 layer0 out_loss 0.008104673586785793\n",
      "Test Epoch231 layer1 out_loss 0.0022625711280852556\n",
      "Test Epoch231 layer2 out_loss 0.0005941514391452074\n",
      "Train 232 | out_loss 0.0005865878192707896: 100%|█| 138/138 [00:00<00:00, 309.07\n",
      "Train Epoch232 out_loss -1.0479698181152344\n",
      "Test Epoch232 layer0 out_loss 0.008080655708909035\n",
      "Test Epoch232 layer1 out_loss 0.002243003575131297\n",
      "Test Epoch232 layer2 out_loss 0.0005602907622233033\n",
      "Train 233 | out_loss 0.0005920220864936709: 100%|█| 138/138 [00:00<00:00, 313.78\n",
      "Train Epoch233 out_loss -1.086094856262207\n",
      "Test Epoch233 layer0 out_loss 0.0081325126811862\n",
      "Test Epoch233 layer1 out_loss 0.0022359113208949566\n",
      "Test Epoch233 layer2 out_loss 0.0005559602868743241\n",
      "Train 234 | out_loss 0.0005810211296193302: 100%|█| 138/138 [00:00<00:00, 314.98\n",
      "Train Epoch234 out_loss -1.009284257888794\n",
      "Test Epoch234 layer0 out_loss 0.007823813706636429\n",
      "Test Epoch234 layer1 out_loss 0.0022255394142121077\n",
      "Test Epoch234 layer2 out_loss 0.00055536167928949\n",
      "Train 235 | out_loss 0.000583167071454227: 100%|█| 138/138 [00:00<00:00, 308.98i\n",
      "Train Epoch235 out_loss -1.0241553783416748\n",
      "Test Epoch235 layer0 out_loss 0.00798095390200615\n",
      "Test Epoch235 layer1 out_loss 0.002205156721174717\n",
      "Test Epoch235 layer2 out_loss 0.0005481366533786058\n",
      "Train 236 | out_loss 0.0005791616858914495: 100%|█| 138/138 [00:00<00:00, 314.20\n",
      "Train Epoch236 out_loss -0.9964437484741211\n",
      "Test Epoch236 layer0 out_loss 0.007834160700440407\n",
      "Test Epoch236 layer1 out_loss 0.0021674761082977057\n",
      "Test Epoch236 layer2 out_loss 0.0005670126411132514\n",
      "Train 237 | out_loss 0.0005682185874320567: 100%|█| 138/138 [00:00<00:00, 321.46\n",
      "Train Epoch237 out_loss -0.9217123985290527\n",
      "Test Epoch237 layer0 out_loss 0.00780493812635541\n",
      "Test Epoch237 layer1 out_loss 0.0021586923394352198\n",
      "Test Epoch237 layer2 out_loss 0.0005351423751562834\n",
      "Train 238 | out_loss 0.0005667089717462659: 100%|█| 138/138 [00:00<00:00, 313.50\n",
      "Train Epoch238 out_loss -0.911514163017273\n",
      "Test Epoch238 layer0 out_loss 0.007907448336482048\n",
      "Test Epoch238 layer1 out_loss 0.0023390543647110462\n",
      "Test Epoch238 layer2 out_loss 0.0005908136372454464\n",
      "Train 239 | out_loss 0.0005640896270051599: 100%|█| 138/138 [00:00<00:00, 314.47\n",
      "Train Epoch239 out_loss -0.893885612487793\n",
      "Test Epoch239 layer0 out_loss 0.007580706384032965\n",
      "Test Epoch239 layer1 out_loss 0.002177091781049967\n",
      "Test Epoch239 layer2 out_loss 0.0005363596137613058\n",
      "Train 240 | out_loss 0.0005641156458295882: 100%|█| 138/138 [00:00<00:00, 305.91\n",
      "Train Epoch240 out_loss -0.8940621614456177\n",
      "Test Epoch240 layer0 out_loss 0.00737058836966753\n",
      "Test Epoch240 layer1 out_loss 0.0021063671447336674\n",
      "Test Epoch240 layer2 out_loss 0.0005431133904494345\n",
      "Train 241 | out_loss 0.0005586508777923882: 100%|█| 138/138 [00:00<00:00, 313.99\n",
      "Train Epoch241 out_loss -0.857540488243103\n",
      "Test Epoch241 layer0 out_loss 0.007303904742002487\n",
      "Test Epoch241 layer1 out_loss 0.00210972991771996\n",
      "Test Epoch241 layer2 out_loss 0.0005747557152062654\n",
      "Train 242 | out_loss 0.00056230160407722: 100%|█| 138/138 [00:00<00:00, 318.02it\n",
      "Train Epoch242 out_loss -0.8818973302841187\n",
      "Test Epoch242 layer0 out_loss 0.00769140524789691\n",
      "Test Epoch242 layer1 out_loss 0.002093491842970252\n",
      "Test Epoch242 layer2 out_loss 0.0005397365894168615\n",
      "Train 243 | out_loss 0.0005593081586994231: 100%|█| 138/138 [00:00<00:00, 312.43\n",
      "Train Epoch243 out_loss -0.8619165420532227\n",
      "Test Epoch243 layer0 out_loss 0.007396703120321035\n",
      "Test Epoch243 layer1 out_loss 0.0021607079543173313\n",
      "Test Epoch243 layer2 out_loss 0.0005357236950658262\n",
      "Train 244 | out_loss 0.0005561684374697506: 100%|█| 138/138 [00:00<00:00, 294.18\n",
      "Train Epoch244 out_loss -0.8410691022872925\n",
      "Test Epoch244 layer0 out_loss 0.0073739574290812016\n",
      "Test Epoch244 layer1 out_loss 0.0020861653611063957\n",
      "Test Epoch244 layer2 out_loss 0.0005727055831812322\n",
      "Train 245 | out_loss 0.0005539082339964807: 100%|█| 138/138 [00:00<00:00, 311.25\n",
      "Train Epoch245 out_loss -0.8261364698410034\n",
      "Test Epoch245 layer0 out_loss 0.007314779330044985\n",
      "Test Epoch245 layer1 out_loss 0.002041085623204708\n",
      "Test Epoch245 layer2 out_loss 0.0005536594544537365\n",
      "Train 246 | out_loss 0.0005509880720637739: 100%|█| 138/138 [00:00<00:00, 307.18\n",
      "Train Epoch246 out_loss -0.8069322109222412\n",
      "Test Epoch246 layer0 out_loss 0.007001684978604317\n",
      "Test Epoch246 layer1 out_loss 0.0020330785773694515\n",
      "Test Epoch246 layer2 out_loss 0.000516773434355855\n",
      "Train 247 | out_loss 0.0005458826199173927: 100%|█| 138/138 [00:00<00:00, 313.90\n",
      "Train Epoch247 out_loss -0.773604154586792\n",
      "Test Epoch247 layer0 out_loss 0.007109431084245443\n",
      "Test Epoch247 layer1 out_loss 0.0020237527787685394\n",
      "Test Epoch247 layer2 out_loss 0.0005212550167925656\n",
      "Train 248 | out_loss 0.0005421966779977083: 100%|█| 138/138 [00:00<00:00, 317.72\n",
      "Train Epoch248 out_loss -0.7497321367263794\n",
      "Test Epoch248 layer0 out_loss 0.007307886146008968\n",
      "Test Epoch248 layer1 out_loss 0.0020198847632855177\n",
      "Test Epoch248 layer2 out_loss 0.0005260179750621319\n",
      "Train 249 | out_loss 0.0005462504923343658: 100%|█| 138/138 [00:00<00:00, 318.13\n",
      "Train Epoch249 out_loss -0.7759941816329956\n",
      "Test Epoch249 layer0 out_loss 0.0069675990380346775\n",
      "Test Epoch249 layer1 out_loss 0.002080281963571906\n",
      "Test Epoch249 layer2 out_loss 0.0005721082561649382\n",
      "Train 250 | out_loss 0.0005404922412708402: 100%|█| 138/138 [00:00<00:00, 304.41\n",
      "Train Epoch250 out_loss -0.7387468814849854\n",
      "Test Epoch250 layer0 out_loss 0.007013858761638403\n",
      "Test Epoch250 layer1 out_loss 0.002012452110648155\n",
      "Test Epoch250 layer2 out_loss 0.0005956134991720319\n",
      "Train 251 | out_loss 0.0005387829733081162: 100%|█| 138/138 [00:00<00:00, 313.47\n",
      "Train Epoch251 out_loss -0.7277669906616211\n",
      "Test Epoch251 layer0 out_loss 0.006804156117141247\n",
      "Test Epoch251 layer1 out_loss 0.0019852553959935904\n",
      "Test Epoch251 layer2 out_loss 0.0005824991385452449\n",
      "Train 252 | out_loss 0.0005318204639479518: 100%|█| 138/138 [00:00<00:00, 310.92\n",
      "Train Epoch252 out_loss -0.6834039688110352\n",
      "Test Epoch252 layer0 out_loss 0.006668284069746733\n",
      "Test Epoch252 layer1 out_loss 0.0020048890728503466\n",
      "Test Epoch252 layer2 out_loss 0.0005283295758999884\n",
      "Train 253 | out_loss 0.000537427666131407: 100%|█| 138/138 [00:00<00:00, 313.16i\n",
      "Train Epoch253 out_loss -0.7190847396850586\n",
      "Test Epoch253 layer0 out_loss 0.006656155455857515\n",
      "Test Epoch253 layer1 out_loss 0.0019896149169653654\n",
      "Test Epoch253 layer2 out_loss 0.0005273104761727154\n",
      "Train 254 | out_loss 0.0005268535460345447: 100%|█| 138/138 [00:00<00:00, 320.14\n",
      "Train Epoch254 out_loss -0.6521047353744507\n",
      "Test Epoch254 layer0 out_loss 0.006515295244753361\n",
      "Test Epoch254 layer1 out_loss 0.0019359397701919079\n",
      "Test Epoch254 layer2 out_loss 0.0005062940181232989\n",
      "Train 255 | out_loss 0.0005257284501567483: 100%|█| 138/138 [00:00<00:00, 306.07\n",
      "Train Epoch255 out_loss -0.6450564861297607\n",
      "Test Epoch255 layer0 out_loss 0.006654398050159216\n",
      "Test Epoch255 layer1 out_loss 0.0019201645627617836\n",
      "Test Epoch255 layer2 out_loss 0.0005183835164643824\n",
      "Train 256 | out_loss 0.0005255764699541032: 100%|█| 138/138 [00:00<00:00, 318.65\n",
      "Train Epoch256 out_loss -0.644102931022644\n",
      "Test Epoch256 layer0 out_loss 0.00671194726601243\n",
      "Test Epoch256 layer1 out_loss 0.0019138352945446968\n",
      "Test Epoch256 layer2 out_loss 0.0005026384023949504\n",
      "Train 257 | out_loss 0.0005179301369935274: 100%|█| 138/138 [00:00<00:00, 312.95\n",
      "Train Epoch257 out_loss -0.5966129302978516\n",
      "Test Epoch257 layer0 out_loss 0.006646817084401846\n",
      "Test Epoch257 layer1 out_loss 0.0019125667167827487\n",
      "Test Epoch257 layer2 out_loss 0.0004942078376188874\n",
      "Train 258 | out_loss 0.000528433418367058: 100%|█| 138/138 [00:00<00:00, 311.92i\n",
      "Train Epoch258 out_loss -0.6620273590087891\n",
      "Test Epoch258 layer0 out_loss 0.00662704324349761\n",
      "Test Epoch258 layer1 out_loss 0.001967904856428504\n",
      "Test Epoch258 layer2 out_loss 0.0005945676239207387\n",
      "Train 259 | out_loss 0.0005221644532866776: 100%|█| 138/138 [00:00<00:00, 310.17\n",
      "Train Epoch259 out_loss -0.6228266954421997\n",
      "Test Epoch259 layer0 out_loss 0.0069184498861432076\n",
      "Test Epoch259 layer1 out_loss 0.0018667898839339614\n",
      "Test Epoch259 layer2 out_loss 0.0004907992552034557\n",
      "Train 260 | out_loss 0.0005188201903365552: 100%|█| 138/138 [00:00<00:00, 308.17\n",
      "Train Epoch260 out_loss -0.6021066904067993\n",
      "Test Epoch260 layer0 out_loss 0.006288014817982912\n",
      "Test Epoch260 layer1 out_loss 0.0018787364242598414\n",
      "Test Epoch260 layer2 out_loss 0.0005528662586584687\n",
      "Train 261 | out_loss 0.0005201934254728258: 100%|█| 138/138 [00:00<00:00, 309.39\n",
      "Train Epoch261 out_loss -0.6105990409851074\n",
      "Test Epoch261 layer0 out_loss 0.006476627662777901\n",
      "Test Epoch261 layer1 out_loss 0.0019047880778089166\n",
      "Test Epoch261 layer2 out_loss 0.0004984987317584455\n",
      "Train 262 | out_loss 0.0005135351093485951: 100%|█| 138/138 [00:00<00:00, 314.75\n",
      "Train Epoch262 out_loss -0.5696312189102173\n",
      "Test Epoch262 layer0 out_loss 0.006297474727034569\n",
      "Test Epoch262 layer1 out_loss 0.0018671939615160227\n",
      "Test Epoch262 layer2 out_loss 0.0004949761787429452\n",
      "Train 263 | out_loss 0.0005155402468517423: 100%|█| 138/138 [00:00<00:00, 311.43\n",
      "Train Epoch263 out_loss -0.5819144248962402\n",
      "Test Epoch263 layer0 out_loss 0.006600393448024988\n",
      "Test Epoch263 layer1 out_loss 0.0018329343292862177\n",
      "Test Epoch263 layer2 out_loss 0.0005009864689782262\n",
      "Train 264 | out_loss 0.0005247186636552215: 100%|█| 138/138 [00:00<00:00, 291.68\n",
      "Train Epoch264 out_loss -0.6387426853179932\n",
      "Test Epoch264 layer0 out_loss 0.006135378032922745\n",
      "Test Epoch264 layer1 out_loss 0.0018584560602903366\n",
      "Test Epoch264 layer2 out_loss 0.0005567538319155574\n",
      "Train 265 | out_loss 0.0005104258307255805: 100%|█| 138/138 [00:00<00:00, 305.56\n",
      "Train Epoch265 out_loss -0.5506831407546997\n",
      "Test Epoch265 layer0 out_loss 0.006289919372648001\n",
      "Test Epoch265 layer1 out_loss 0.0018505733460187912\n",
      "Test Epoch265 layer2 out_loss 0.0005356616456992924\n",
      "Train 266 | out_loss 0.0005161521839909256: 100%|█| 138/138 [00:00<00:00, 316.00\n",
      "Train Epoch266 out_loss -0.5856715440750122\n",
      "Test Epoch266 layer0 out_loss 0.006215229164808989\n",
      "Test Epoch266 layer1 out_loss 0.0018388323951512575\n",
      "Test Epoch266 layer2 out_loss 0.00048097060061991215\n",
      "Train 267 | out_loss 0.0005112046492286026: 100%|█| 138/138 [00:00<00:00, 313.41\n",
      "Train Epoch267 out_loss -0.5554195642471313\n",
      "Test Epoch267 layer0 out_loss 0.0061752209439873695\n",
      "Test Epoch267 layer1 out_loss 0.0018716284539550543\n",
      "Test Epoch267 layer2 out_loss 0.0005244538187980652\n",
      "Train 268 | out_loss 0.0005027948063798249: 100%|█| 138/138 [00:00<00:00, 314.25\n",
      "Train Epoch268 out_loss -0.5046628713607788\n",
      "Test Epoch268 layer0 out_loss 0.006342221517115831\n",
      "Test Epoch268 layer1 out_loss 0.0018071014201268554\n",
      "Test Epoch268 layer2 out_loss 0.0004960003425367177\n",
      "Train 269 | out_loss 0.0005050604231655598: 100%|█| 138/138 [00:00<00:00, 310.36\n",
      "Train Epoch269 out_loss -0.5182543992996216\n",
      "Test Epoch269 layer0 out_loss 0.006040831562131643\n",
      "Test Epoch269 layer1 out_loss 0.001760857179760933\n",
      "Test Epoch269 layer2 out_loss 0.0004849520046263933\n",
      "Train 270 | out_loss 0.0004963079700246453: 100%|█| 138/138 [00:00<00:00, 312.57\n",
      "Train Epoch270 out_loss -0.46608901023864746\n",
      "Test Epoch270 layer0 out_loss 0.005796030629426241\n",
      "Test Epoch270 layer1 out_loss 0.0017778252949938178\n",
      "Test Epoch270 layer2 out_loss 0.0004996583447791636\n",
      "Train 271 | out_loss 0.0005034926580265164: 100%|█| 138/138 [00:00<00:00, 315.94\n",
      "Train Epoch271 out_loss -0.5088440179824829\n",
      "Test Epoch271 layer0 out_loss 0.006040247622877359\n",
      "Test Epoch271 layer1 out_loss 0.0017577275866642594\n",
      "Test Epoch271 layer2 out_loss 0.00048117514234036207\n",
      "Train 272 | out_loss 0.0004956411430612206: 100%|█| 138/138 [00:00<00:00, 314.80\n",
      "Train Epoch272 out_loss -0.4621521234512329\n",
      "Test Epoch272 layer0 out_loss 0.005762515123933554\n",
      "Test Epoch272 layer1 out_loss 0.0018395200604572892\n",
      "Test Epoch272 layer2 out_loss 0.0004928517155349255\n",
      "Train 273 | out_loss 0.0005006794817745686: 100%|█| 138/138 [00:00<00:00, 314.28\n",
      "Train Epoch273 out_loss -0.4920283555984497\n",
      "Test Epoch273 layer0 out_loss 0.0060630072839558125\n",
      "Test Epoch273 layer1 out_loss 0.0017406608676537871\n",
      "Test Epoch273 layer2 out_loss 0.0004725902690552175\n",
      "Train 274 | out_loss 0.0005016433424316347: 100%|█| 138/138 [00:00<00:00, 306.18\n",
      "Train Epoch274 out_loss -0.49777936935424805\n",
      "Test Epoch274 layer0 out_loss 0.006140844896435738\n",
      "Test Epoch274 layer1 out_loss 0.0017134511144831777\n",
      "Test Epoch274 layer2 out_loss 0.000491723942104727\n",
      "Train 275 | out_loss 0.0004934185999445617: 100%|█| 138/138 [00:00<00:00, 315.24\n",
      "Train Epoch275 out_loss -0.4490680694580078\n",
      "Test Epoch275 layer0 out_loss 0.005900832824409008\n",
      "Test Epoch275 layer1 out_loss 0.0017439834773540497\n",
      "Test Epoch275 layer2 out_loss 0.00047645767335779965\n",
      "Train 276 | out_loss 0.0004951275186613202: 100%|█| 138/138 [00:00<00:00, 300.35\n",
      "Train Epoch276 out_loss -0.45912396907806396\n",
      "Test Epoch276 layer0 out_loss 0.006068672519177198\n",
      "Test Epoch276 layer1 out_loss 0.0017897466896101832\n",
      "Test Epoch276 layer2 out_loss 0.0005176377599127591\n",
      "Train 277 | out_loss 0.0004907737602479756: 100%|█| 138/138 [00:00<00:00, 317.13\n",
      "Train Epoch277 out_loss -0.43357419967651367\n",
      "Test Epoch277 layer0 out_loss 0.005649181082844734\n",
      "Test Epoch277 layer1 out_loss 0.0018677955958992243\n",
      "Test Epoch277 layer2 out_loss 0.0004735268885269761\n",
      "Train 278 | out_loss 0.0005025523714721203: 100%|█| 138/138 [00:00<00:00, 315.52\n",
      "Train Epoch278 out_loss -0.5032122135162354\n",
      "Test Epoch278 layer0 out_loss 0.005792382173240185\n",
      "Test Epoch278 layer1 out_loss 0.0016767267370596528\n",
      "Test Epoch278 layer2 out_loss 0.0005126151372678578\n",
      "Train 279 | out_loss 0.00048826876445673406: 100%|█| 138/138 [00:00<00:00, 310.4\n",
      "Train Epoch279 out_loss -0.418978214263916\n",
      "Test Epoch279 layer0 out_loss 0.00543923070654273\n",
      "Test Epoch279 layer1 out_loss 0.001671829610131681\n",
      "Test Epoch279 layer2 out_loss 0.00047226675087586045\n",
      "Train 280 | out_loss 0.0004923483356833458: 100%|█| 138/138 [00:00<00:00, 308.24\n",
      "Train Epoch280 out_loss -0.44278812408447266\n",
      "Test Epoch280 layer0 out_loss 0.0054307980462908745\n",
      "Test Epoch280 layer1 out_loss 0.0017186145996674895\n",
      "Test Epoch280 layer2 out_loss 0.00050462776562199\n",
      "Train 281 | out_loss 0.0004873160505667329: 100%|█| 138/138 [00:00<00:00, 309.56\n",
      "Train Epoch281 out_loss -0.41344594955444336\n",
      "Test Epoch281 layer0 out_loss 0.005386122036725283\n",
      "Test Epoch281 layer1 out_loss 0.0016478663310408592\n",
      "Test Epoch281 layer2 out_loss 0.0004700541903730482\n",
      "Train 282 | out_loss 0.0004908335977233946: 100%|█| 138/138 [00:00<00:00, 306.28\n",
      "Train Epoch282 out_loss -0.43392419815063477\n",
      "Test Epoch282 layer0 out_loss 0.005412064027041197\n",
      "Test Epoch282 layer1 out_loss 0.0016702262219041586\n",
      "Test Epoch282 layer2 out_loss 0.00046657081111334264\n",
      "Train 283 | out_loss 0.0004825979995075613: 100%|█| 138/138 [00:00<00:00, 306.96\n",
      "Train Epoch283 out_loss -0.3862088918685913\n",
      "Test Epoch283 layer0 out_loss 0.005327003076672554\n",
      "Test Epoch283 layer1 out_loss 0.001654727617278695\n",
      "Test Epoch283 layer2 out_loss 0.0004788439837284386\n",
      "Train 284 | out_loss 0.0004846674855798483: 100%|█| 138/138 [00:00<00:00, 314.06\n",
      "Train Epoch284 out_loss -0.3981229066848755\n",
      "Test Epoch284 layer0 out_loss 0.005606450606137514\n",
      "Test Epoch284 layer1 out_loss 0.0016271002823486924\n",
      "Test Epoch284 layer2 out_loss 0.0005382629460655153\n",
      "Train 285 | out_loss 0.00048367329873144627: 100%|█| 138/138 [00:00<00:00, 310.0\n",
      "Train Epoch285 out_loss -0.39239370822906494\n",
      "Test Epoch285 layer0 out_loss 0.005860616452991962\n",
      "Test Epoch285 layer1 out_loss 0.0016278040129691362\n",
      "Test Epoch285 layer2 out_loss 0.000468360201921314\n",
      "Train 286 | out_loss 0.0004775641136802733: 100%|█| 138/138 [00:00<00:00, 315.60\n",
      "Train Epoch286 out_loss -0.35744190216064453\n",
      "Test Epoch286 layer0 out_loss 0.00528327189385891\n",
      "Test Epoch286 layer1 out_loss 0.0017507591983303428\n",
      "Test Epoch286 layer2 out_loss 0.0004594932252075523\n",
      "Train 287 | out_loss 0.0004779498849529773: 100%|█| 138/138 [00:00<00:00, 300.38\n",
      "Train Epoch287 out_loss -0.3596365451812744\n",
      "Test Epoch287 layer0 out_loss 0.0052240765653550625\n",
      "Test Epoch287 layer1 out_loss 0.0016095180762931705\n",
      "Test Epoch287 layer2 out_loss 0.00045665563084185123\n",
      "Train 288 | out_loss 0.000489944068249315: 100%|█| 138/138 [00:00<00:00, 308.14i\n",
      "Train Epoch288 out_loss -0.4287310838699341\n",
      "Test Epoch288 layer0 out_loss 0.005229520611464977\n",
      "Test Epoch288 layer1 out_loss 0.001617702771909535\n",
      "Test Epoch288 layer2 out_loss 0.00048431247705593705\n",
      "Train 289 | out_loss 0.00047695281682536006: 100%|█| 138/138 [00:00<00:00, 306.8\n",
      "Train Epoch289 out_loss -0.35396814346313477\n",
      "Test Epoch289 layer0 out_loss 0.005188299342989922\n",
      "Test Epoch289 layer1 out_loss 0.0015749314334243536\n",
      "Test Epoch289 layer2 out_loss 0.0004568214062601328\n",
      "Train 290 | out_loss 0.00047512364108115435: 100%|█| 138/138 [00:00<00:00, 308.4\n",
      "Train Epoch290 out_loss -0.3436034917831421\n",
      "Test Epoch290 layer0 out_loss 0.0052224425598979\n",
      "Test Epoch290 layer1 out_loss 0.0016550907166674733\n",
      "Test Epoch290 layer2 out_loss 0.0004580648383125663\n",
      "Train 291 | out_loss 0.00047561226529069245: 100%|█| 138/138 [00:00<00:00, 313.7\n",
      "Train Epoch291 out_loss -0.3463679552078247\n",
      "Test Epoch291 layer0 out_loss 0.00518103176727891\n",
      "Test Epoch291 layer1 out_loss 0.001648364239372313\n",
      "Test Epoch291 layer2 out_loss 0.0004567438445519656\n",
      "Train 292 | out_loss 0.0004727244086097926: 100%|█| 138/138 [00:00<00:00, 270.00\n",
      "Train Epoch292 out_loss -0.33006906509399414\n",
      "Test Epoch292 layer0 out_loss 0.0051541514694690704\n",
      "Test Epoch292 layer1 out_loss 0.001568791689351201\n",
      "Test Epoch292 layer2 out_loss 0.0004770202504005283\n",
      "Train 293 | out_loss 0.00047575030475854874: 100%|█| 138/138 [00:00<00:00, 309.7\n",
      "Train Epoch293 out_loss -0.347149133682251\n",
      "Test Epoch293 layer0 out_loss 0.005010516848415136\n",
      "Test Epoch293 layer1 out_loss 0.0015509212389588356\n",
      "Test Epoch293 layer2 out_loss 0.00046207342529669404\n",
      "Train 294 | out_loss 0.00047702869051136076: 100%|█| 138/138 [00:00<00:00, 301.3\n",
      "Train Epoch294 out_loss -0.3543999195098877\n",
      "Test Epoch294 layer0 out_loss 0.005114274565130472\n",
      "Test Epoch294 layer1 out_loss 0.0016118602361530066\n",
      "Test Epoch294 layer2 out_loss 0.0004584742127917707\n",
      "Train 295 | out_loss 0.00047330884262919426: 100%|█| 138/138 [00:00<00:00, 312.0\n",
      "Train Epoch295 out_loss -0.3333582878112793\n",
      "Test Epoch295 layer0 out_loss 0.0050904112868011\n",
      "Test Epoch295 layer1 out_loss 0.0015626726672053337\n",
      "Test Epoch295 layer2 out_loss 0.00044939064537175\n",
      "Train 296 | out_loss 0.00047651820932514966: 100%|█| 138/138 [00:00<00:00, 314.1\n",
      "Train Epoch296 out_loss -0.3515009880065918\n",
      "Test Epoch296 layer0 out_loss 0.005330382846295834\n",
      "Test Epoch296 layer1 out_loss 0.0015473563689738512\n",
      "Test Epoch296 layer2 out_loss 0.0004623222630470991\n",
      "Train 297 | out_loss 0.00046929428935982287: 100%|█| 138/138 [00:00<00:00, 310.2\n",
      "Train Epoch297 out_loss -0.31083500385284424\n",
      "Test Epoch297 layer0 out_loss 0.004889931529760361\n",
      "Test Epoch297 layer1 out_loss 0.0015362250851467252\n",
      "Test Epoch297 layer2 out_loss 0.0004562813264783472\n",
      "Train 298 | out_loss 0.0004691397480200976: 100%|█| 138/138 [00:00<00:00, 310.12\n",
      "Train Epoch298 out_loss -0.3099721670150757\n",
      "Test Epoch298 layer0 out_loss 0.004984547384083271\n",
      "Test Epoch298 layer1 out_loss 0.0015076994895935059\n",
      "Test Epoch298 layer2 out_loss 0.00048199421144090593\n",
      "Train 299 | out_loss 0.00046988253598101437: 100%|█| 138/138 [00:00<00:00, 312.6\n",
      "Train Epoch299 out_loss -0.3141237497329712\n",
      "Test Epoch299 layer0 out_loss 0.004913513083010912\n",
      "Test Epoch299 layer1 out_loss 0.0016385233029723167\n",
      "Test Epoch299 layer2 out_loss 0.000475111766718328\n",
      "Best loss 0.00044939064537175 at L2\n",
      "Figure(640x480)\n",
      "Figure(640x480)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/AL_main_new/dis_train_al.py\", line 409, in <module>\n",
      "    main()\n",
      "  File \"/home/AL_main_new/dis_train_al.py\", line 401, in main\n",
      "    plotResult(model,'result/'+ path_name, args.task)\n",
      "  File \"/home/AL_main_new/dis_train_al.py\", line 48, in plotResult\n",
      "    plt.plot(history[\"train_out\"], \"k\", label='train_out' )\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/matplotlib/pyplot.py\", line 2757, in plot\n",
      "    return gca().plot(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/matplotlib/axes/_axes.py\", line 1632, in plot\n",
      "    lines = [*self._get_lines(*args, data=data, **kwargs)]\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/matplotlib/axes/_base.py\", line 312, in __call__\n",
      "    yield from self._plot_args(this, kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/matplotlib/axes/_base.py\", line 490, in _plot_args\n",
      "    x, y = index_of(xy[-1])\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/matplotlib/cbook/__init__.py\", line 1652, in index_of\n",
      "    y = _check_1d(y)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/matplotlib/cbook/__init__.py\", line 1304, in _check_1d\n",
      "    return np.atleast_1d(x)\n",
      "  File \"<__array_function__ internals>\", line 5, in atleast_1d\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/numpy/core/shape_base.py\", line 65, in atleast_1d\n",
      "    ary = asanyarray(ary)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/_tensor.py\", line 757, in __array__\n",
      "    return self.numpy()\n",
      "RuntimeError: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.\n"
     ]
    }
   ],
   "source": [
    "# LinearAL ailerons\n",
    "\n",
    "data = \"ailerons\"\n",
    "\n",
    "model =  \"linearal\"\n",
    "#for layer in range(1,11):\n",
    "for layer in [3]:\n",
    "    log = f\"result/{data}_{model}_l{layer}.log\"\n",
    "    !python3 dis_train_al.py --dataset {data} --model {model} --epoch 300 --num-layer {layer} --lr 0.00001 --task regression # > {log}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchmetrics\n",
      "  Downloading torchmetrics-0.9.3-py3-none-any.whl (419 kB)\n",
      "\u001b[K     |████████████████████████████████| 419 kB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.3.1 in /opt/conda/lib/python3.9/site-packages (from torchmetrics) (1.12.0+cu113)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /opt/conda/lib/python3.9/site-packages (from torchmetrics) (1.21.5)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from torchmetrics) (21.3)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch>=1.3.1->torchmetrics) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging->torchmetrics) (3.0.4)\n",
      "Installing collected packages: torchmetrics\n",
      "Successfully installed torchmetrics-0.9.3\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ea8a7f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0008725115898554677\n",
      "0.0004098966061174112\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "print(statistics.mean(y))\n",
    "print(statistics.stdev(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46d15db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
