{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06dd4036",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.optim import optimizer\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.nn.functional import mse_loss\n",
    "from torchmetrics.functional import r2_score\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "house_dataset = fetch_california_housing()\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    house_dataset.data,\n",
    "    columns=house_dataset.feature_names\n",
    ")\n",
    "df.loc[:,\"Price\"] = house_dataset.target\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "df.loc[:,:] = scaler.fit_transform(df)\n",
    "\n",
    "col_feature = house_dataset.feature_names\n",
    "col_target = [\"Price\"]\n",
    "\n",
    "y = torch.Tensor(df[col_target].to_numpy())\n",
    "x = torch.Tensor(df[col_feature].to_numpy())\n",
    "\n",
    "feature_train, feature_test, train_target, test_target = train_test_split(x, y, test_size=0.2)\n",
    "\n",
    "n_samples, n_features = x.shape\n",
    "\n",
    "# 1) model\n",
    "class LinearRegression(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        \n",
    "        # define layers\n",
    "        self.linear =  nn.Sequential(\n",
    "                nn.Linear(input_dim, 300),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(300, 128),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(128, output_dim),\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "    \n",
    "        return self.linear(x)\n",
    "\n",
    "model = LinearRegression(n_features, 1)\n",
    "\n",
    "# 2) loss and optimizer\n",
    "learning_rate = 0.001\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0172766",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.3791e-01, -9.2485e-01,  1.8690e+00,  ..., -8.8901e-02,\n",
       "          1.5816e+00, -1.5874e+00],\n",
       "        [ 6.3467e-01, -3.6864e-01,  1.0537e-02,  ..., -7.2468e-03,\n",
       "         -8.4829e-01,  8.1344e-01],\n",
       "        [ 3.9755e+00, -6.0702e-01,  6.9107e-01,  ..., -2.2362e-02,\n",
       "         -7.3592e-01,  5.1396e-01],\n",
       "        ...,\n",
       "        [ 8.7843e-01, -1.1632e+00,  3.3450e-01,  ..., -2.7074e-02,\n",
       "         -1.0168e+00,  1.1129e+00],\n",
       "        [-4.3119e-01, -6.0702e-01, -4.0641e-02,  ..., -1.1610e-03,\n",
       "          7.8568e-01, -4.3439e-01],\n",
       "        [-6.7606e-01,  1.1411e+00, -3.2529e-01,  ...,  2.0181e-02,\n",
       "          1.0947e+00, -1.3877e+00]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0f28f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss =  0.0756, R2 = 0.9243568181991577\n",
      "test: loss =  0.0757, R2 = 0.9243043661117554\n",
      "epoch: 2, loss =  0.0757, R2 = 0.9243043661117554\n",
      "test: loss =  0.0757, R2 = 0.9242651462554932\n",
      "epoch: 3, loss =  0.0757, R2 = 0.9242651462554932\n",
      "test: loss =  0.0757, R2 = 0.9242469668388367\n",
      "epoch: 4, loss =  0.0757, R2 = 0.9242469668388367\n",
      "test: loss =  0.0757, R2 = 0.9242547154426575\n",
      "epoch: 5, loss =  0.0757, R2 = 0.9242547154426575\n",
      "test: loss =  0.0757, R2 = 0.924291729927063\n",
      "epoch: 6, loss =  0.0757, R2 = 0.924291729927063\n",
      "test: loss =  0.0756, R2 = 0.9243543744087219\n",
      "epoch: 7, loss =  0.0756, R2 = 0.9243543744087219\n",
      "test: loss =  0.0755, R2 = 0.9244295358657837\n",
      "epoch: 8, loss =  0.0755, R2 = 0.9244295358657837\n",
      "test: loss =  0.0755, R2 = 0.9244977831840515\n",
      "epoch: 9, loss =  0.0755, R2 = 0.9244977831840515\n",
      "test: loss =  0.0754, R2 = 0.9245430827140808\n",
      "epoch: 10, loss =  0.0754, R2 = 0.9245430827140808\n",
      "test: loss =  0.0754, R2 = 0.9245599508285522\n",
      "epoch: 11, loss =  0.0754, R2 = 0.9245599508285522\n",
      "test: loss =  0.0754, R2 = 0.924554705619812\n",
      "epoch: 12, loss =  0.0754, R2 = 0.924554705619812\n",
      "test: loss =  0.0754, R2 = 0.9245400428771973\n",
      "epoch: 13, loss =  0.0754, R2 = 0.9245400428771973\n",
      "test: loss =  0.0755, R2 = 0.9245272874832153\n",
      "epoch: 14, loss =  0.0755, R2 = 0.9245272874832153\n",
      "test: loss =  0.0755, R2 = 0.9245226383209229\n",
      "epoch: 15, loss =  0.0755, R2 = 0.9245226383209229\n",
      "test: loss =  0.0755, R2 = 0.9245266914367676\n",
      "epoch: 16, loss =  0.0755, R2 = 0.9245266914367676\n",
      "test: loss =  0.0754, R2 = 0.9245374798774719\n",
      "epoch: 17, loss =  0.0754, R2 = 0.9245374798774719\n",
      "test: loss =  0.0754, R2 = 0.924553394317627\n",
      "epoch: 18, loss =  0.0754, R2 = 0.924553394317627\n",
      "test: loss =  0.0754, R2 = 0.9245737791061401\n",
      "epoch: 19, loss =  0.0754, R2 = 0.9245737791061401\n",
      "test: loss =  0.0754, R2 = 0.9245980381965637\n",
      "epoch: 20, loss =  0.0754, R2 = 0.9245980381965637\n",
      "test: loss =  0.0754, R2 = 0.9246245622634888\n",
      "epoch: 21, loss =  0.0754, R2 = 0.9246245622634888\n",
      "test: loss =  0.0753, R2 = 0.9246504902839661\n",
      "epoch: 22, loss =  0.0753, R2 = 0.9246504902839661\n",
      "test: loss =  0.0753, R2 = 0.9246727824211121\n",
      "epoch: 23, loss =  0.0753, R2 = 0.9246727824211121\n",
      "test: loss =  0.0753, R2 = 0.924689769744873\n",
      "epoch: 24, loss =  0.0753, R2 = 0.924689769744873\n",
      "test: loss =  0.0753, R2 = 0.924701452255249\n",
      "epoch: 25, loss =  0.0753, R2 = 0.924701452255249\n",
      "test: loss =  0.0753, R2 = 0.9247094392776489\n",
      "epoch: 26, loss =  0.0753, R2 = 0.9247094392776489\n",
      "test: loss =  0.0753, R2 = 0.9247158169746399\n",
      "epoch: 27, loss =  0.0753, R2 = 0.9247158169746399\n",
      "test: loss =  0.0753, R2 = 0.9247219562530518\n",
      "epoch: 28, loss =  0.0753, R2 = 0.9247219562530518\n",
      "test: loss =  0.0753, R2 = 0.924728512763977\n",
      "epoch: 29, loss =  0.0753, R2 = 0.924728512763977\n",
      "test: loss =  0.0752, R2 = 0.9247353076934814\n",
      "epoch: 30, loss =  0.0752, R2 = 0.9247353076934814\n",
      "test: loss =  0.0752, R2 = 0.9247421622276306\n",
      "epoch: 31, loss =  0.0752, R2 = 0.9247421622276306\n",
      "test: loss =  0.0752, R2 = 0.9247490763664246\n",
      "epoch: 32, loss =  0.0752, R2 = 0.9247490763664246\n",
      "test: loss =  0.0752, R2 = 0.9247563481330872\n",
      "epoch: 33, loss =  0.0752, R2 = 0.9247563481330872\n",
      "test: loss =  0.0752, R2 = 0.9247645139694214\n",
      "epoch: 34, loss =  0.0752, R2 = 0.9247645139694214\n",
      "test: loss =  0.0752, R2 = 0.9247736930847168\n",
      "epoch: 35, loss =  0.0752, R2 = 0.9247736930847168\n",
      "test: loss =  0.0752, R2 = 0.9247837066650391\n",
      "epoch: 36, loss =  0.0752, R2 = 0.9247837066650391\n",
      "test: loss =  0.0752, R2 = 0.9247941374778748\n",
      "epoch: 37, loss =  0.0752, R2 = 0.9247941374778748\n",
      "test: loss =  0.0752, R2 = 0.9248046278953552\n",
      "epoch: 38, loss =  0.0752, R2 = 0.9248046278953552\n",
      "test: loss =  0.0752, R2 = 0.9248147010803223\n",
      "epoch: 39, loss =  0.0752, R2 = 0.9248147010803223\n",
      "test: loss =  0.0752, R2 = 0.9248244166374207\n",
      "epoch: 40, loss =  0.0752, R2 = 0.9248244166374207\n",
      "test: loss =  0.0751, R2 = 0.9248337745666504\n",
      "epoch: 41, loss =  0.0751, R2 = 0.9248337745666504\n",
      "test: loss =  0.0751, R2 = 0.9248427748680115\n",
      "epoch: 42, loss =  0.0751, R2 = 0.9248427748680115\n",
      "test: loss =  0.0751, R2 = 0.9248514175415039\n",
      "epoch: 43, loss =  0.0751, R2 = 0.9248514175415039\n",
      "test: loss =  0.0751, R2 = 0.9248595237731934\n",
      "epoch: 44, loss =  0.0751, R2 = 0.9248595237731934\n",
      "test: loss =  0.0751, R2 = 0.924866795539856\n",
      "epoch: 45, loss =  0.0751, R2 = 0.924866795539856\n",
      "test: loss =  0.0751, R2 = 0.924872875213623\n",
      "epoch: 46, loss =  0.0751, R2 = 0.924872875213623\n",
      "test: loss =  0.0751, R2 = 0.9248772859573364\n",
      "epoch: 47, loss =  0.0751, R2 = 0.9248772859573364\n",
      "test: loss =  0.0751, R2 = 0.9248797297477722\n",
      "epoch: 48, loss =  0.0751, R2 = 0.9248797297477722\n",
      "test: loss =  0.0751, R2 = 0.9248795509338379\n",
      "epoch: 49, loss =  0.0751, R2 = 0.9248795509338379\n",
      "test: loss =  0.0751, R2 = 0.9248759746551514\n",
      "epoch: 50, loss =  0.0751, R2 = 0.9248759746551514\n",
      "test: loss =  0.0751, R2 = 0.9248678088188171\n",
      "epoch: 51, loss =  0.0751, R2 = 0.9248678088188171\n",
      "test: loss =  0.0751, R2 = 0.9248534440994263\n",
      "epoch: 52, loss =  0.0751, R2 = 0.9248534440994263\n",
      "test: loss =  0.0751, R2 = 0.9248304963111877\n",
      "epoch: 53, loss =  0.0751, R2 = 0.9248304963111877\n",
      "test: loss =  0.0752, R2 = 0.9247957468032837\n",
      "epoch: 54, loss =  0.0752, R2 = 0.9247957468032837\n",
      "test: loss =  0.0752, R2 = 0.924744725227356\n",
      "epoch: 55, loss =  0.0752, R2 = 0.924744725227356\n",
      "test: loss =  0.0753, R2 = 0.9246715307235718\n",
      "epoch: 56, loss =  0.0753, R2 = 0.9246715307235718\n",
      "test: loss =  0.0754, R2 = 0.9245679378509521\n",
      "epoch: 57, loss =  0.0754, R2 = 0.9245679378509521\n",
      "test: loss =  0.0756, R2 = 0.9244238138198853\n",
      "epoch: 58, loss =  0.0756, R2 = 0.9244238138198853\n",
      "test: loss =  0.0758, R2 = 0.9242260456085205\n",
      "epoch: 59, loss =  0.0758, R2 = 0.9242260456085205\n",
      "test: loss =  0.0760, R2 = 0.9239611625671387\n",
      "epoch: 60, loss =  0.0760, R2 = 0.9239611625671387\n",
      "test: loss =  0.0764, R2 = 0.9236155152320862\n",
      "epoch: 61, loss =  0.0764, R2 = 0.9236155152320862\n",
      "test: loss =  0.0768, R2 = 0.9231858253479004\n",
      "epoch: 62, loss =  0.0768, R2 = 0.9231858253479004\n",
      "test: loss =  0.0773, R2 = 0.9226840734481812\n",
      "epoch: 63, loss =  0.0773, R2 = 0.9226840734481812\n",
      "test: loss =  0.0778, R2 = 0.9221619963645935\n",
      "epoch: 64, loss =  0.0778, R2 = 0.9221619963645935\n",
      "test: loss =  0.0783, R2 = 0.9217121005058289\n",
      "epoch: 65, loss =  0.0783, R2 = 0.9217121005058289\n",
      "test: loss =  0.0785, R2 = 0.9214813113212585\n",
      "epoch: 66, loss =  0.0785, R2 = 0.9214813113212585\n",
      "test: loss =  0.0784, R2 = 0.9216090440750122\n",
      "epoch: 67, loss =  0.0784, R2 = 0.9216090440750122\n",
      "test: loss =  0.0778, R2 = 0.9221692085266113\n",
      "epoch: 68, loss =  0.0778, R2 = 0.9221692085266113\n",
      "test: loss =  0.0769, R2 = 0.9230652451515198\n",
      "epoch: 69, loss =  0.0769, R2 = 0.9230652451515198\n",
      "test: loss =  0.0759, R2 = 0.9240471124649048\n",
      "epoch: 70, loss =  0.0759, R2 = 0.9240471124649048\n",
      "test: loss =  0.0752, R2 = 0.924807608127594\n",
      "epoch: 71, loss =  0.0752, R2 = 0.924807608127594\n",
      "test: loss =  0.0748, R2 = 0.9251465201377869\n",
      "epoch: 72, loss =  0.0748, R2 = 0.9251465201377869\n",
      "test: loss =  0.0749, R2 = 0.9250566959381104\n",
      "epoch: 73, loss =  0.0749, R2 = 0.9250566959381104\n",
      "test: loss =  0.0753, R2 = 0.924698531627655\n",
      "epoch: 74, loss =  0.0753, R2 = 0.924698531627655\n",
      "test: loss =  0.0757, R2 = 0.9243048429489136\n",
      "epoch: 75, loss =  0.0757, R2 = 0.9243048429489136\n",
      "test: loss =  0.0759, R2 = 0.9240767955780029\n",
      "epoch: 76, loss =  0.0759, R2 = 0.9240767955780029\n",
      "test: loss =  0.0759, R2 = 0.9241143465042114\n",
      "epoch: 77, loss =  0.0759, R2 = 0.9241143465042114\n",
      "test: loss =  0.0756, R2 = 0.9243860244750977\n",
      "epoch: 78, loss =  0.0756, R2 = 0.9243860244750977\n",
      "test: loss =  0.0752, R2 = 0.9247610569000244\n",
      "epoch: 79, loss =  0.0752, R2 = 0.9247610569000244\n",
      "test: loss =  0.0749, R2 = 0.925080418586731\n",
      "epoch: 80, loss =  0.0749, R2 = 0.925080418586731\n",
      "test: loss =  0.0747, R2 = 0.9252364039421082\n",
      "epoch: 81, loss =  0.0747, R2 = 0.9252364039421082\n",
      "test: loss =  0.0748, R2 = 0.9252135753631592\n",
      "epoch: 82, loss =  0.0748, R2 = 0.9252135753631592\n",
      "test: loss =  0.0749, R2 = 0.9250765442848206\n",
      "epoch: 83, loss =  0.0749, R2 = 0.9250765442848206\n",
      "test: loss =  0.0751, R2 = 0.9249248504638672\n",
      "epoch: 84, loss =  0.0751, R2 = 0.9249248504638672\n",
      "test: loss =  0.0751, R2 = 0.9248424172401428\n",
      "epoch: 85, loss =  0.0751, R2 = 0.9248424172401428\n",
      "test: loss =  0.0751, R2 = 0.9248667359352112\n",
      "epoch: 86, loss =  0.0751, R2 = 0.9248667359352112\n",
      "test: loss =  0.0750, R2 = 0.924981415271759\n",
      "epoch: 87, loss =  0.0750, R2 = 0.924981415271759\n",
      "test: loss =  0.0748, R2 = 0.9251341819763184\n",
      "epoch: 88, loss =  0.0748, R2 = 0.9251341819763184\n",
      "test: loss =  0.0747, R2 = 0.9252655506134033\n",
      "epoch: 89, loss =  0.0747, R2 = 0.9252655506134033\n",
      "test: loss =  0.0746, R2 = 0.9253361225128174\n",
      "epoch: 90, loss =  0.0746, R2 = 0.9253361225128174\n",
      "test: loss =  0.0746, R2 = 0.9253391027450562\n",
      "epoch: 91, loss =  0.0746, R2 = 0.9253391027450562\n",
      "test: loss =  0.0747, R2 = 0.9252958297729492\n",
      "epoch: 92, loss =  0.0747, R2 = 0.9252958297729492\n",
      "test: loss =  0.0747, R2 = 0.9252411127090454\n",
      "epoch: 93, loss =  0.0747, R2 = 0.9252411127090454\n",
      "test: loss =  0.0748, R2 = 0.9252066612243652\n",
      "epoch: 94, loss =  0.0748, R2 = 0.9252066612243652\n",
      "test: loss =  0.0748, R2 = 0.925209641456604\n",
      "epoch: 95, loss =  0.0748, R2 = 0.925209641456604\n",
      "test: loss =  0.0747, R2 = 0.9252491593360901\n",
      "epoch: 96, loss =  0.0747, R2 = 0.9252491593360901\n",
      "test: loss =  0.0747, R2 = 0.9253106117248535\n",
      "epoch: 97, loss =  0.0747, R2 = 0.9253106117248535\n",
      "test: loss =  0.0746, R2 = 0.9253737926483154\n",
      "epoch: 98, loss =  0.0746, R2 = 0.9253737926483154\n",
      "test: loss =  0.0746, R2 = 0.9254217743873596\n",
      "epoch: 99, loss =  0.0746, R2 = 0.9254217743873596\n",
      "test: loss =  0.0745, R2 = 0.9254462122917175\n",
      "epoch: 100, loss =  0.0745, R2 = 0.9254462122917175\n",
      "test: loss =  0.0745, R2 = 0.9254485368728638\n",
      "epoch: 101, loss =  0.0745, R2 = 0.9254485368728638\n",
      "test: loss =  0.0745, R2 = 0.925437331199646\n",
      "epoch: 102, loss =  0.0745, R2 = 0.925437331199646\n",
      "test: loss =  0.0746, R2 = 0.9254236221313477\n",
      "epoch: 103, loss =  0.0746, R2 = 0.9254236221313477\n",
      "test: loss =  0.0746, R2 = 0.925416886806488\n",
      "epoch: 104, loss =  0.0746, R2 = 0.925416886806488\n",
      "test: loss =  0.0746, R2 = 0.9254222512245178\n",
      "epoch: 105, loss =  0.0746, R2 = 0.9254222512245178\n",
      "test: loss =  0.0745, R2 = 0.9254398941993713\n",
      "epoch: 106, loss =  0.0745, R2 = 0.9254398941993713\n",
      "test: loss =  0.0745, R2 = 0.9254661202430725\n",
      "epoch: 107, loss =  0.0745, R2 = 0.9254661202430725\n",
      "test: loss =  0.0745, R2 = 0.9254953265190125\n",
      "epoch: 108, loss =  0.0745, R2 = 0.9254953265190125\n",
      "test: loss =  0.0745, R2 = 0.9255222678184509\n",
      "epoch: 109, loss =  0.0745, R2 = 0.9255222678184509\n",
      "test: loss =  0.0744, R2 = 0.9255433678627014\n",
      "epoch: 110, loss =  0.0744, R2 = 0.9255433678627014\n",
      "test: loss =  0.0744, R2 = 0.9255574941635132\n",
      "epoch: 111, loss =  0.0744, R2 = 0.9255574941635132\n",
      "test: loss =  0.0744, R2 = 0.9255653619766235\n",
      "epoch: 112, loss =  0.0744, R2 = 0.9255653619766235\n",
      "test: loss =  0.0744, R2 = 0.9255692958831787\n",
      "epoch: 113, loss =  0.0744, R2 = 0.9255692958831787\n",
      "test: loss =  0.0744, R2 = 0.9255719780921936\n",
      "epoch: 114, loss =  0.0744, R2 = 0.9255719780921936\n",
      "test: loss =  0.0744, R2 = 0.9255756139755249\n",
      "epoch: 115, loss =  0.0744, R2 = 0.9255756139755249\n",
      "test: loss =  0.0744, R2 = 0.9255817532539368\n",
      "epoch: 116, loss =  0.0744, R2 = 0.9255817532539368\n",
      "test: loss =  0.0744, R2 = 0.9255908727645874\n",
      "epoch: 117, loss =  0.0744, R2 = 0.9255908727645874\n",
      "test: loss =  0.0744, R2 = 0.9256028532981873\n",
      "epoch: 118, loss =  0.0744, R2 = 0.9256028532981873\n",
      "test: loss =  0.0744, R2 = 0.9256167411804199\n",
      "epoch: 119, loss =  0.0744, R2 = 0.9256167411804199\n",
      "test: loss =  0.0743, R2 = 0.9256317019462585\n",
      "epoch: 120, loss =  0.0743, R2 = 0.9256317019462585\n",
      "test: loss =  0.0743, R2 = 0.9256467223167419\n",
      "epoch: 121, loss =  0.0743, R2 = 0.9256467223167419\n",
      "test: loss =  0.0743, R2 = 0.925661027431488\n",
      "epoch: 122, loss =  0.0743, R2 = 0.925661027431488\n",
      "test: loss =  0.0743, R2 = 0.925674319267273\n",
      "epoch: 123, loss =  0.0743, R2 = 0.925674319267273\n",
      "test: loss =  0.0743, R2 = 0.9256864190101624\n",
      "epoch: 124, loss =  0.0743, R2 = 0.9256864190101624\n",
      "test: loss =  0.0743, R2 = 0.9256973266601562\n",
      "epoch: 125, loss =  0.0743, R2 = 0.9256973266601562\n",
      "test: loss =  0.0743, R2 = 0.9257073998451233\n",
      "epoch: 126, loss =  0.0743, R2 = 0.9257073998451233\n",
      "test: loss =  0.0743, R2 = 0.925716757774353\n",
      "epoch: 127, loss =  0.0743, R2 = 0.925716757774353\n",
      "test: loss =  0.0743, R2 = 0.9257256984710693\n",
      "epoch: 128, loss =  0.0743, R2 = 0.9257256984710693\n",
      "test: loss =  0.0742, R2 = 0.9257345199584961\n",
      "epoch: 129, loss =  0.0742, R2 = 0.9257345199584961\n",
      "test: loss =  0.0742, R2 = 0.9257433414459229\n",
      "epoch: 130, loss =  0.0742, R2 = 0.9257433414459229\n",
      "test: loss =  0.0742, R2 = 0.9257522821426392\n",
      "epoch: 131, loss =  0.0742, R2 = 0.9257522821426392\n",
      "test: loss =  0.0742, R2 = 0.925761342048645\n",
      "epoch: 132, loss =  0.0742, R2 = 0.925761342048645\n",
      "test: loss =  0.0742, R2 = 0.92577064037323\n",
      "epoch: 133, loss =  0.0742, R2 = 0.92577064037323\n",
      "test: loss =  0.0742, R2 = 0.9257801175117493\n",
      "epoch: 134, loss =  0.0742, R2 = 0.9257801175117493\n",
      "test: loss =  0.0742, R2 = 0.9257897734642029\n",
      "epoch: 135, loss =  0.0742, R2 = 0.9257897734642029\n",
      "test: loss =  0.0742, R2 = 0.925799548625946\n",
      "epoch: 136, loss =  0.0742, R2 = 0.925799548625946\n",
      "test: loss =  0.0742, R2 = 0.925809383392334\n",
      "epoch: 137, loss =  0.0742, R2 = 0.925809383392334\n",
      "test: loss =  0.0742, R2 = 0.9258192777633667\n",
      "epoch: 138, loss =  0.0742, R2 = 0.9258192777633667\n",
      "test: loss =  0.0742, R2 = 0.9258291125297546\n",
      "epoch: 139, loss =  0.0742, R2 = 0.9258291125297546\n",
      "test: loss =  0.0741, R2 = 0.9258389472961426\n",
      "epoch: 140, loss =  0.0741, R2 = 0.9258389472961426\n",
      "test: loss =  0.0741, R2 = 0.9258486032485962\n",
      "epoch: 141, loss =  0.0741, R2 = 0.9258486032485962\n",
      "test: loss =  0.0741, R2 = 0.9258581399917603\n",
      "epoch: 142, loss =  0.0741, R2 = 0.9258581399917603\n",
      "test: loss =  0.0741, R2 = 0.92586749792099\n",
      "epoch: 143, loss =  0.0741, R2 = 0.92586749792099\n",
      "test: loss =  0.0741, R2 = 0.9258765578269958\n",
      "epoch: 144, loss =  0.0741, R2 = 0.9258765578269958\n",
      "test: loss =  0.0741, R2 = 0.9258852005004883\n",
      "epoch: 145, loss =  0.0741, R2 = 0.9258852005004883\n",
      "test: loss =  0.0741, R2 = 0.9258933663368225\n",
      "epoch: 146, loss =  0.0741, R2 = 0.9258933663368225\n",
      "test: loss =  0.0741, R2 = 0.9259008765220642\n",
      "epoch: 147, loss =  0.0741, R2 = 0.9259008765220642\n",
      "test: loss =  0.0741, R2 = 0.9259074926376343\n",
      "epoch: 148, loss =  0.0741, R2 = 0.9259074926376343\n",
      "test: loss =  0.0741, R2 = 0.9259129166603088\n",
      "epoch: 149, loss =  0.0741, R2 = 0.9259129166603088\n",
      "test: loss =  0.0741, R2 = 0.9259166717529297\n",
      "epoch: 150, loss =  0.0741, R2 = 0.9259166717529297\n",
      "test: loss =  0.0741, R2 = 0.9259181618690491\n",
      "epoch: 151, loss =  0.0741, R2 = 0.9259181618690491\n",
      "test: loss =  0.0741, R2 = 0.9259164333343506\n",
      "epoch: 152, loss =  0.0741, R2 = 0.9259164333343506\n",
      "test: loss =  0.0741, R2 = 0.9259103536605835\n",
      "epoch: 153, loss =  0.0741, R2 = 0.9259103536605835\n",
      "test: loss =  0.0741, R2 = 0.9258979558944702\n",
      "epoch: 154, loss =  0.0741, R2 = 0.9258979558944702\n",
      "test: loss =  0.0741, R2 = 0.9258765578269958\n",
      "epoch: 155, loss =  0.0741, R2 = 0.9258765578269958\n",
      "test: loss =  0.0741, R2 = 0.9258424043655396\n",
      "epoch: 156, loss =  0.0741, R2 = 0.9258424043655396\n",
      "test: loss =  0.0742, R2 = 0.9257898926734924\n",
      "epoch: 157, loss =  0.0742, R2 = 0.9257898926734924\n",
      "test: loss =  0.0743, R2 = 0.9257111549377441\n",
      "epoch: 158, loss =  0.0743, R2 = 0.9257111549377441\n",
      "test: loss =  0.0744, R2 = 0.925594687461853\n",
      "epoch: 159, loss =  0.0744, R2 = 0.925594687461853\n",
      "test: loss =  0.0746, R2 = 0.9254252910614014\n",
      "epoch: 160, loss =  0.0746, R2 = 0.9254252910614014\n",
      "test: loss =  0.0748, R2 = 0.9251818060874939\n",
      "epoch: 161, loss =  0.0748, R2 = 0.9251818060874939\n",
      "test: loss =  0.0751, R2 = 0.9248395562171936\n",
      "epoch: 162, loss =  0.0751, R2 = 0.9248395562171936\n",
      "test: loss =  0.0756, R2 = 0.9243707060813904\n",
      "epoch: 163, loss =  0.0756, R2 = 0.9243707060813904\n",
      "test: loss =  0.0762, R2 = 0.923757791519165\n",
      "epoch: 164, loss =  0.0762, R2 = 0.923757791519165\n",
      "test: loss =  0.0770, R2 = 0.9230077266693115\n",
      "epoch: 165, loss =  0.0770, R2 = 0.9230077266693115\n",
      "test: loss =  0.0778, R2 = 0.9221936464309692\n",
      "epoch: 166, loss =  0.0778, R2 = 0.9221936464309692\n",
      "test: loss =  0.0785, R2 = 0.9214751124382019\n",
      "epoch: 167, loss =  0.0785, R2 = 0.9214751124382019\n",
      "test: loss =  0.0789, R2 = 0.9211205840110779\n",
      "epoch: 168, loss =  0.0789, R2 = 0.9211205840110779\n",
      "test: loss =  0.0786, R2 = 0.9213939905166626\n",
      "epoch: 169, loss =  0.0786, R2 = 0.9213939905166626\n",
      "test: loss =  0.0776, R2 = 0.9223968982696533\n",
      "epoch: 170, loss =  0.0776, R2 = 0.9223968982696533\n",
      "test: loss =  0.0761, R2 = 0.9238687753677368\n",
      "epoch: 171, loss =  0.0761, R2 = 0.9238687753677368\n",
      "test: loss =  0.0747, R2 = 0.9252642393112183\n",
      "epoch: 172, loss =  0.0747, R2 = 0.9252642393112183\n",
      "test: loss =  0.0739, R2 = 0.9260531067848206\n",
      "epoch: 173, loss =  0.0739, R2 = 0.9260531067848206\n",
      "test: loss =  0.0739, R2 = 0.9260509610176086\n",
      "epoch: 174, loss =  0.0739, R2 = 0.9260509610176086\n",
      "test: loss =  0.0745, R2 = 0.9254873991012573\n",
      "epoch: 175, loss =  0.0745, R2 = 0.9254873991012573\n",
      "test: loss =  0.0752, R2 = 0.9248237609863281\n",
      "epoch: 176, loss =  0.0752, R2 = 0.9248237609863281\n",
      "test: loss =  0.0755, R2 = 0.9244921207427979\n",
      "epoch: 177, loss =  0.0755, R2 = 0.9244921207427979\n",
      "test: loss =  0.0753, R2 = 0.9246783256530762\n",
      "epoch: 178, loss =  0.0753, R2 = 0.9246783256530762\n",
      "test: loss =  0.0747, R2 = 0.9252520799636841\n",
      "epoch: 179, loss =  0.0747, R2 = 0.9252520799636841\n",
      "test: loss =  0.0741, R2 = 0.9258669018745422\n",
      "epoch: 180, loss =  0.0741, R2 = 0.9258669018745422\n",
      "test: loss =  0.0738, R2 = 0.9262014031410217\n",
      "epoch: 181, loss =  0.0738, R2 = 0.9262014031410217\n",
      "test: loss =  0.0738, R2 = 0.9261537790298462\n",
      "epoch: 182, loss =  0.0738, R2 = 0.9261537790298462\n",
      "test: loss =  0.0741, R2 = 0.925864577293396\n",
      "epoch: 183, loss =  0.0741, R2 = 0.925864577293396\n",
      "test: loss =  0.0744, R2 = 0.92558753490448\n",
      "epoch: 184, loss =  0.0744, R2 = 0.92558753490448\n",
      "test: loss =  0.0745, R2 = 0.9255186915397644\n",
      "epoch: 185, loss =  0.0745, R2 = 0.9255186915397644\n",
      "test: loss =  0.0743, R2 = 0.9256945848464966\n",
      "epoch: 186, loss =  0.0743, R2 = 0.9256945848464966\n",
      "test: loss =  0.0740, R2 = 0.925994336605072\n",
      "epoch: 187, loss =  0.0740, R2 = 0.925994336605072\n",
      "test: loss =  0.0737, R2 = 0.9262418746948242\n",
      "epoch: 188, loss =  0.0737, R2 = 0.9262418746948242\n",
      "test: loss =  0.0737, R2 = 0.9263224601745605\n",
      "epoch: 189, loss =  0.0737, R2 = 0.9263224601745605\n",
      "test: loss =  0.0737, R2 = 0.9262412190437317\n",
      "epoch: 190, loss =  0.0737, R2 = 0.9262412190437317\n",
      "test: loss =  0.0739, R2 = 0.9260963201522827\n",
      "epoch: 191, loss =  0.0739, R2 = 0.9260963201522827\n",
      "test: loss =  0.0740, R2 = 0.926002025604248\n",
      "epoch: 192, loss =  0.0740, R2 = 0.926002025604248\n",
      "test: loss =  0.0740, R2 = 0.9260210990905762\n",
      "epoch: 193, loss =  0.0740, R2 = 0.9260210990905762\n",
      "test: loss =  0.0738, R2 = 0.9261375665664673\n",
      "epoch: 194, loss =  0.0738, R2 = 0.9261375665664673\n",
      "test: loss =  0.0737, R2 = 0.9262825846672058\n",
      "epoch: 195, loss =  0.0737, R2 = 0.9262825846672058\n",
      "test: loss =  0.0736, R2 = 0.926382839679718\n",
      "epoch: 196, loss =  0.0736, R2 = 0.926382839679718\n",
      "test: loss =  0.0736, R2 = 0.9264041185379028\n",
      "epoch: 197, loss =  0.0736, R2 = 0.9264041185379028\n",
      "test: loss =  0.0736, R2 = 0.9263620972633362\n",
      "epoch: 198, loss =  0.0736, R2 = 0.9263620972633362\n",
      "test: loss =  0.0737, R2 = 0.9263030290603638\n",
      "epoch: 199, loss =  0.0737, R2 = 0.9263030290603638\n",
      "test: loss =  0.0737, R2 = 0.9262723326683044\n",
      "epoch: 200, loss =  0.0737, R2 = 0.9262723326683044\n",
      "test: loss =  0.0737, R2 = 0.9262906312942505\n",
      "epoch: 201, loss =  0.0737, R2 = 0.9262906312942505\n",
      "test: loss =  0.0736, R2 = 0.9263483285903931\n",
      "epoch: 202, loss =  0.0736, R2 = 0.9263483285903931\n",
      "test: loss =  0.0736, R2 = 0.92641681432724\n",
      "epoch: 203, loss =  0.0736, R2 = 0.92641681432724\n",
      "test: loss =  0.0735, R2 = 0.9264674186706543\n",
      "epoch: 204, loss =  0.0735, R2 = 0.9264674186706543\n",
      "test: loss =  0.0735, R2 = 0.9264863133430481\n",
      "epoch: 205, loss =  0.0735, R2 = 0.9264863133430481\n",
      "test: loss =  0.0735, R2 = 0.92647784948349\n",
      "epoch: 206, loss =  0.0735, R2 = 0.92647784948349\n",
      "test: loss =  0.0735, R2 = 0.9264588356018066\n",
      "epoch: 207, loss =  0.0735, R2 = 0.9264588356018066\n",
      "test: loss =  0.0735, R2 = 0.9264472723007202\n",
      "epoch: 208, loss =  0.0735, R2 = 0.9264472723007202\n",
      "test: loss =  0.0735, R2 = 0.9264538288116455\n",
      "epoch: 209, loss =  0.0735, R2 = 0.9264538288116455\n",
      "test: loss =  0.0735, R2 = 0.9264782667160034\n",
      "epoch: 210, loss =  0.0735, R2 = 0.9264782667160034\n",
      "test: loss =  0.0735, R2 = 0.9265119433403015\n",
      "epoch: 211, loss =  0.0735, R2 = 0.9265119433403015\n",
      "test: loss =  0.0734, R2 = 0.9265438914299011\n",
      "epoch: 212, loss =  0.0734, R2 = 0.9265438914299011\n",
      "test: loss =  0.0734, R2 = 0.9265658855438232\n",
      "epoch: 213, loss =  0.0734, R2 = 0.9265658855438232\n",
      "test: loss =  0.0734, R2 = 0.926575779914856\n",
      "epoch: 214, loss =  0.0734, R2 = 0.926575779914856\n",
      "test: loss =  0.0734, R2 = 0.9265768527984619\n",
      "epoch: 215, loss =  0.0734, R2 = 0.9265768527984619\n",
      "test: loss =  0.0734, R2 = 0.926575243473053\n",
      "epoch: 216, loss =  0.0734, R2 = 0.926575243473053\n",
      "test: loss =  0.0734, R2 = 0.9265769124031067\n",
      "epoch: 217, loss =  0.0734, R2 = 0.9265769124031067\n",
      "test: loss =  0.0734, R2 = 0.9265850186347961\n",
      "epoch: 218, loss =  0.0734, R2 = 0.9265850186347961\n",
      "test: loss =  0.0734, R2 = 0.9265996217727661\n",
      "epoch: 219, loss =  0.0734, R2 = 0.9265996217727661\n",
      "test: loss =  0.0734, R2 = 0.926618218421936\n",
      "epoch: 220, loss =  0.0734, R2 = 0.926618218421936\n",
      "test: loss =  0.0733, R2 = 0.9266373515129089\n",
      "epoch: 221, loss =  0.0733, R2 = 0.9266373515129089\n",
      "test: loss =  0.0733, R2 = 0.9266539812088013\n",
      "epoch: 222, loss =  0.0733, R2 = 0.9266539812088013\n",
      "test: loss =  0.0733, R2 = 0.9266666173934937\n",
      "epoch: 223, loss =  0.0733, R2 = 0.9266666173934937\n",
      "test: loss =  0.0733, R2 = 0.92667555809021\n",
      "epoch: 224, loss =  0.0733, R2 = 0.92667555809021\n",
      "test: loss =  0.0733, R2 = 0.9266821146011353\n",
      "epoch: 225, loss =  0.0733, R2 = 0.9266821146011353\n",
      "test: loss =  0.0733, R2 = 0.9266881346702576\n",
      "epoch: 226, loss =  0.0733, R2 = 0.9266881346702576\n",
      "test: loss =  0.0733, R2 = 0.9266951680183411\n",
      "epoch: 227, loss =  0.0733, R2 = 0.9266951680183411\n",
      "test: loss =  0.0733, R2 = 0.9267041087150574\n",
      "epoch: 228, loss =  0.0733, R2 = 0.9267041087150574\n",
      "test: loss =  0.0733, R2 = 0.926715075969696\n",
      "epoch: 229, loss =  0.0733, R2 = 0.926715075969696\n",
      "test: loss =  0.0733, R2 = 0.9267274737358093\n",
      "epoch: 230, loss =  0.0733, R2 = 0.9267274737358093\n",
      "test: loss =  0.0732, R2 = 0.9267405271530151\n",
      "epoch: 231, loss =  0.0732, R2 = 0.9267405271530151\n",
      "test: loss =  0.0732, R2 = 0.9267533421516418\n",
      "epoch: 232, loss =  0.0732, R2 = 0.9267533421516418\n",
      "test: loss =  0.0732, R2 = 0.9267653822898865\n",
      "epoch: 233, loss =  0.0732, R2 = 0.9267653822898865\n",
      "test: loss =  0.0732, R2 = 0.9267762899398804\n",
      "epoch: 234, loss =  0.0732, R2 = 0.9267762899398804\n",
      "test: loss =  0.0732, R2 = 0.9267861843109131\n",
      "epoch: 235, loss =  0.0732, R2 = 0.9267861843109131\n",
      "test: loss =  0.0732, R2 = 0.9267953634262085\n",
      "epoch: 236, loss =  0.0732, R2 = 0.9267953634262085\n",
      "test: loss =  0.0732, R2 = 0.9268041253089905\n",
      "epoch: 237, loss =  0.0732, R2 = 0.9268041253089905\n",
      "test: loss =  0.0732, R2 = 0.9268128275871277\n",
      "epoch: 238, loss =  0.0732, R2 = 0.9268128275871277\n",
      "test: loss =  0.0732, R2 = 0.9268217086791992\n",
      "epoch: 239, loss =  0.0732, R2 = 0.9268217086791992\n",
      "test: loss =  0.0731, R2 = 0.9268309473991394\n",
      "epoch: 240, loss =  0.0731, R2 = 0.9268309473991394\n",
      "test: loss =  0.0731, R2 = 0.9268404245376587\n",
      "epoch: 241, loss =  0.0731, R2 = 0.9268404245376587\n",
      "test: loss =  0.0731, R2 = 0.9268500804901123\n",
      "epoch: 242, loss =  0.0731, R2 = 0.9268500804901123\n",
      "test: loss =  0.0731, R2 = 0.9268596172332764\n",
      "epoch: 243, loss =  0.0731, R2 = 0.9268596172332764\n",
      "test: loss =  0.0731, R2 = 0.9268687963485718\n",
      "epoch: 244, loss =  0.0731, R2 = 0.9268687963485718\n",
      "test: loss =  0.0731, R2 = 0.9268771409988403\n",
      "epoch: 245, loss =  0.0731, R2 = 0.9268771409988403\n",
      "test: loss =  0.0731, R2 = 0.926884114742279\n",
      "epoch: 246, loss =  0.0731, R2 = 0.926884114742279\n",
      "test: loss =  0.0731, R2 = 0.9268890023231506\n",
      "epoch: 247, loss =  0.0731, R2 = 0.9268890023231506\n",
      "test: loss =  0.0731, R2 = 0.9268907904624939\n",
      "epoch: 248, loss =  0.0731, R2 = 0.9268907904624939\n",
      "test: loss =  0.0731, R2 = 0.9268876314163208\n",
      "epoch: 249, loss =  0.0731, R2 = 0.9268876314163208\n",
      "test: loss =  0.0731, R2 = 0.926876962184906\n",
      "epoch: 250, loss =  0.0731, R2 = 0.926876962184906\n",
      "test: loss =  0.0731, R2 = 0.9268543124198914\n",
      "epoch: 251, loss =  0.0731, R2 = 0.9268543124198914\n",
      "test: loss =  0.0732, R2 = 0.926813006401062\n",
      "epoch: 252, loss =  0.0732, R2 = 0.926813006401062\n",
      "test: loss =  0.0732, R2 = 0.9267420768737793\n",
      "epoch: 253, loss =  0.0732, R2 = 0.9267420768737793\n",
      "test: loss =  0.0734, R2 = 0.9266251921653748\n",
      "epoch: 254, loss =  0.0734, R2 = 0.9266251921653748\n",
      "test: loss =  0.0735, R2 = 0.9264383316040039\n",
      "epoch: 255, loss =  0.0735, R2 = 0.9264383316040039\n",
      "test: loss =  0.0738, R2 = 0.9261497259140015\n",
      "epoch: 256, loss =  0.0738, R2 = 0.9261497259140015\n",
      "test: loss =  0.0743, R2 = 0.9257245063781738\n",
      "epoch: 257, loss =  0.0743, R2 = 0.9257245063781738\n",
      "test: loss =  0.0748, R2 = 0.9251450896263123\n",
      "epoch: 258, loss =  0.0748, R2 = 0.9251450896263123\n",
      "test: loss =  0.0755, R2 = 0.9244503378868103\n",
      "epoch: 259, loss =  0.0755, R2 = 0.9244503378868103\n",
      "test: loss =  0.0762, R2 = 0.9237995147705078\n",
      "epoch: 260, loss =  0.0762, R2 = 0.9237995147705078\n",
      "test: loss =  0.0765, R2 = 0.9234834313392639\n",
      "epoch: 261, loss =  0.0765, R2 = 0.9234834313392639\n",
      "test: loss =  0.0762, R2 = 0.923799991607666\n",
      "epoch: 262, loss =  0.0762, R2 = 0.923799991607666\n",
      "test: loss =  0.0752, R2 = 0.9247472882270813\n",
      "epoch: 263, loss =  0.0752, R2 = 0.9247472882270813\n",
      "test: loss =  0.0741, R2 = 0.9258692264556885\n",
      "epoch: 264, loss =  0.0741, R2 = 0.9258692264556885\n",
      "test: loss =  0.0734, R2 = 0.9265332221984863\n",
      "epoch: 265, loss =  0.0734, R2 = 0.9265332221984863\n",
      "test: loss =  0.0735, R2 = 0.9264562129974365\n",
      "epoch: 266, loss =  0.0735, R2 = 0.9264562129974365\n",
      "test: loss =  0.0741, R2 = 0.9259136915206909\n",
      "epoch: 267, loss =  0.0741, R2 = 0.9259136915206909\n",
      "test: loss =  0.0745, R2 = 0.9254878759384155\n",
      "epoch: 268, loss =  0.0745, R2 = 0.9254878759384155\n",
      "test: loss =  0.0744, R2 = 0.9256001114845276\n",
      "epoch: 269, loss =  0.0744, R2 = 0.9256001114845276\n",
      "test: loss =  0.0738, R2 = 0.9261738061904907\n",
      "epoch: 270, loss =  0.0738, R2 = 0.9261738061904907\n",
      "test: loss =  0.0732, R2 = 0.9267353415489197\n",
      "epoch: 271, loss =  0.0732, R2 = 0.9267353415489197\n",
      "test: loss =  0.0731, R2 = 0.926899790763855\n",
      "epoch: 272, loss =  0.0731, R2 = 0.926899790763855\n",
      "test: loss =  0.0733, R2 = 0.9267141222953796\n",
      "epoch: 273, loss =  0.0733, R2 = 0.9267141222953796\n",
      "test: loss =  0.0735, R2 = 0.9265234470367432\n",
      "epoch: 274, loss =  0.0735, R2 = 0.9265234470367432\n",
      "test: loss =  0.0734, R2 = 0.9265779256820679\n",
      "epoch: 275, loss =  0.0734, R2 = 0.9265779256820679\n",
      "test: loss =  0.0732, R2 = 0.926811695098877\n",
      "epoch: 276, loss =  0.0732, R2 = 0.926811695098877\n",
      "test: loss =  0.0730, R2 = 0.9269838333129883\n",
      "epoch: 277, loss =  0.0730, R2 = 0.9269838333129883\n",
      "test: loss =  0.0730, R2 = 0.9269717931747437\n",
      "epoch: 278, loss =  0.0730, R2 = 0.9269717931747437\n",
      "test: loss =  0.0731, R2 = 0.9268758296966553\n",
      "epoch: 279, loss =  0.0731, R2 = 0.9268758296966553\n",
      "test: loss =  0.0731, R2 = 0.9268618822097778\n",
      "epoch: 280, loss =  0.0731, R2 = 0.9268618822097778\n",
      "test: loss =  0.0730, R2 = 0.9269644021987915\n",
      "epoch: 281, loss =  0.0730, R2 = 0.9269644021987915\n",
      "test: loss =  0.0729, R2 = 0.9270757436752319\n",
      "epoch: 282, loss =  0.0729, R2 = 0.9270757436752319\n",
      "test: loss =  0.0729, R2 = 0.927100658416748\n",
      "epoch: 283, loss =  0.0729, R2 = 0.927100658416748\n",
      "test: loss =  0.0729, R2 = 0.9270649552345276\n",
      "epoch: 284, loss =  0.0729, R2 = 0.9270649552345276\n",
      "test: loss =  0.0729, R2 = 0.9270632266998291\n",
      "epoch: 285, loss =  0.0729, R2 = 0.9270632266998291\n",
      "test: loss =  0.0728, R2 = 0.9271335005760193\n",
      "epoch: 286, loss =  0.0728, R2 = 0.9271335005760193\n",
      "test: loss =  0.0728, R2 = 0.9272159337997437\n",
      "epoch: 287, loss =  0.0728, R2 = 0.9272159337997437\n",
      "test: loss =  0.0727, R2 = 0.9272361993789673\n",
      "epoch: 288, loss =  0.0727, R2 = 0.9272361993789673\n",
      "test: loss =  0.0728, R2 = 0.9271975159645081\n",
      "epoch: 289, loss =  0.0728, R2 = 0.9271975159645081\n",
      "test: loss =  0.0728, R2 = 0.9271705150604248\n",
      "epoch: 290, loss =  0.0728, R2 = 0.9271705150604248\n",
      "test: loss =  0.0728, R2 = 0.92720627784729\n",
      "epoch: 291, loss =  0.0728, R2 = 0.92720627784729\n",
      "test: loss =  0.0727, R2 = 0.9272801876068115\n",
      "epoch: 292, loss =  0.0727, R2 = 0.9272801876068115\n",
      "test: loss =  0.0727, R2 = 0.9273266196250916\n",
      "epoch: 293, loss =  0.0727, R2 = 0.9273266196250916\n",
      "test: loss =  0.0727, R2 = 0.9273169040679932\n",
      "epoch: 294, loss =  0.0727, R2 = 0.9273169040679932\n",
      "test: loss =  0.0727, R2 = 0.9272860288619995\n",
      "epoch: 295, loss =  0.0727, R2 = 0.9272860288619995\n",
      "test: loss =  0.0727, R2 = 0.9272869229316711\n",
      "epoch: 296, loss =  0.0727, R2 = 0.9272869229316711\n",
      "test: loss =  0.0726, R2 = 0.9273317456245422\n",
      "epoch: 297, loss =  0.0726, R2 = 0.9273317456245422\n",
      "test: loss =  0.0726, R2 = 0.9273847937583923\n",
      "epoch: 298, loss =  0.0726, R2 = 0.9273847937583923\n",
      "test: loss =  0.0726, R2 = 0.927406370639801\n",
      "epoch: 299, loss =  0.0726, R2 = 0.927406370639801\n",
      "test: loss =  0.0726, R2 = 0.9273943901062012\n",
      "epoch: 300, loss =  0.0726, R2 = 0.9273943901062012\n",
      "test: loss =  0.0726, R2 = 0.9273796081542969\n",
      "epoch: 301, loss =  0.0726, R2 = 0.9273796081542969\n",
      "test: loss =  0.0726, R2 = 0.9273890256881714\n",
      "epoch: 302, loss =  0.0726, R2 = 0.9273890256881714\n",
      "test: loss =  0.0726, R2 = 0.9274200797080994\n",
      "epoch: 303, loss =  0.0726, R2 = 0.9274200797080994\n",
      "test: loss =  0.0725, R2 = 0.9274493455886841\n",
      "epoch: 304, loss =  0.0725, R2 = 0.9274493455886841\n",
      "test: loss =  0.0725, R2 = 0.9274598956108093\n",
      "epoch: 305, loss =  0.0725, R2 = 0.9274598956108093\n",
      "test: loss =  0.0725, R2 = 0.9274563789367676\n",
      "epoch: 306, loss =  0.0725, R2 = 0.9274563789367676\n",
      "test: loss =  0.0725, R2 = 0.9274556040763855\n",
      "epoch: 307, loss =  0.0725, R2 = 0.9274556040763855\n",
      "test: loss =  0.0725, R2 = 0.9274675846099854\n",
      "epoch: 308, loss =  0.0725, R2 = 0.9274675846099854\n",
      "test: loss =  0.0725, R2 = 0.9274874925613403\n",
      "epoch: 309, loss =  0.0725, R2 = 0.9274874925613403\n",
      "test: loss =  0.0725, R2 = 0.9275039434432983\n",
      "epoch: 310, loss =  0.0725, R2 = 0.9275039434432983\n",
      "test: loss =  0.0725, R2 = 0.9275118112564087\n",
      "epoch: 311, loss =  0.0725, R2 = 0.9275118112564087\n",
      "test: loss =  0.0725, R2 = 0.9275153875350952\n",
      "epoch: 312, loss =  0.0725, R2 = 0.9275153875350952\n",
      "test: loss =  0.0725, R2 = 0.9275216460227966\n",
      "epoch: 313, loss =  0.0725, R2 = 0.9275216460227966\n",
      "test: loss =  0.0724, R2 = 0.9275327920913696\n",
      "epoch: 314, loss =  0.0724, R2 = 0.9275327920913696\n",
      "test: loss =  0.0724, R2 = 0.9275446534156799\n",
      "epoch: 315, loss =  0.0724, R2 = 0.9275446534156799\n",
      "test: loss =  0.0724, R2 = 0.9275522828102112\n",
      "epoch: 316, loss =  0.0724, R2 = 0.9275522828102112\n",
      "test: loss =  0.0724, R2 = 0.9275545477867126\n",
      "epoch: 317, loss =  0.0724, R2 = 0.9275545477867126\n",
      "test: loss =  0.0724, R2 = 0.9275539517402649\n",
      "epoch: 318, loss =  0.0724, R2 = 0.9275539517402649\n",
      "test: loss =  0.0724, R2 = 0.9275524616241455\n",
      "epoch: 319, loss =  0.0724, R2 = 0.9275524616241455\n",
      "test: loss =  0.0724, R2 = 0.9275481104850769\n",
      "epoch: 320, loss =  0.0724, R2 = 0.9275481104850769\n",
      "test: loss =  0.0724, R2 = 0.927535355091095\n",
      "epoch: 321, loss =  0.0724, R2 = 0.927535355091095\n",
      "test: loss =  0.0725, R2 = 0.9275074005126953\n",
      "epoch: 322, loss =  0.0725, R2 = 0.9275074005126953\n",
      "test: loss =  0.0725, R2 = 0.9274578094482422\n",
      "epoch: 323, loss =  0.0725, R2 = 0.9274578094482422\n",
      "test: loss =  0.0726, R2 = 0.9273784756660461\n",
      "epoch: 324, loss =  0.0726, R2 = 0.9273784756660461\n",
      "test: loss =  0.0727, R2 = 0.9272555112838745\n",
      "epoch: 325, loss =  0.0727, R2 = 0.9272555112838745\n",
      "test: loss =  0.0729, R2 = 0.9270647764205933\n",
      "epoch: 326, loss =  0.0729, R2 = 0.9270647764205933\n",
      "test: loss =  0.0732, R2 = 0.9267697334289551\n",
      "epoch: 327, loss =  0.0732, R2 = 0.9267697334289551\n",
      "test: loss =  0.0737, R2 = 0.9263191819190979\n",
      "epoch: 328, loss =  0.0737, R2 = 0.9263191819190979\n",
      "test: loss =  0.0743, R2 = 0.9256521463394165\n",
      "epoch: 329, loss =  0.0743, R2 = 0.9256521463394165\n",
      "test: loss =  0.0753, R2 = 0.924707293510437\n",
      "epoch: 330, loss =  0.0753, R2 = 0.924707293510437\n",
      "test: loss =  0.0765, R2 = 0.9234671592712402\n",
      "epoch: 331, loss =  0.0765, R2 = 0.9234671592712402\n",
      "test: loss =  0.0779, R2 = 0.9220287203788757\n",
      "epoch: 332, loss =  0.0779, R2 = 0.9220287203788757\n",
      "test: loss =  0.0793, R2 = 0.9207273125648499\n",
      "epoch: 333, loss =  0.0793, R2 = 0.9207273125648499\n",
      "test: loss =  0.0798, R2 = 0.9201494455337524\n",
      "epoch: 334, loss =  0.0798, R2 = 0.9201494455337524\n",
      "test: loss =  0.0791, R2 = 0.9209140539169312\n",
      "epoch: 335, loss =  0.0791, R2 = 0.9209140539169312\n",
      "test: loss =  0.0769, R2 = 0.9230670928955078\n",
      "epoch: 336, loss =  0.0769, R2 = 0.9230670928955078\n",
      "test: loss =  0.0743, R2 = 0.9257180690765381\n",
      "epoch: 337, loss =  0.0743, R2 = 0.9257180690765381\n",
      "test: loss =  0.0725, R2 = 0.9274970293045044\n",
      "epoch: 338, loss =  0.0725, R2 = 0.9274970293045044\n",
      "test: loss =  0.0723, R2 = 0.927636444568634\n",
      "epoch: 339, loss =  0.0723, R2 = 0.927636444568634\n",
      "test: loss =  0.0735, R2 = 0.9265137314796448\n",
      "epoch: 340, loss =  0.0735, R2 = 0.9265137314796448\n",
      "test: loss =  0.0747, R2 = 0.9252532720565796\n",
      "epoch: 341, loss =  0.0747, R2 = 0.9252532720565796\n",
      "test: loss =  0.0751, R2 = 0.9248949885368347\n",
      "epoch: 342, loss =  0.0751, R2 = 0.9248949885368347\n",
      "test: loss =  0.0743, R2 = 0.9256922006607056\n",
      "epoch: 343, loss =  0.0743, R2 = 0.9256922006607056\n",
      "test: loss =  0.0730, R2 = 0.9269811511039734\n",
      "epoch: 344, loss =  0.0730, R2 = 0.9269811511039734\n",
      "test: loss =  0.0722, R2 = 0.9277820587158203\n",
      "epoch: 345, loss =  0.0722, R2 = 0.9277820587158203\n",
      "test: loss =  0.0723, R2 = 0.9276511073112488\n",
      "epoch: 346, loss =  0.0723, R2 = 0.9276511073112488\n",
      "test: loss =  0.0730, R2 = 0.9269593358039856\n",
      "epoch: 347, loss =  0.0730, R2 = 0.9269593358039856\n",
      "test: loss =  0.0735, R2 = 0.9264495372772217\n",
      "epoch: 348, loss =  0.0735, R2 = 0.9264495372772217\n",
      "test: loss =  0.0734, R2 = 0.9265804290771484\n",
      "epoch: 349, loss =  0.0734, R2 = 0.9265804290771484\n",
      "test: loss =  0.0728, R2 = 0.9271999001502991\n",
      "epoch: 350, loss =  0.0728, R2 = 0.9271999001502991\n",
      "test: loss =  0.0722, R2 = 0.9277672171592712\n",
      "epoch: 351, loss =  0.0722, R2 = 0.9277672171592712\n",
      "test: loss =  0.0721, R2 = 0.9278753399848938\n",
      "epoch: 352, loss =  0.0721, R2 = 0.9278753399848938\n",
      "test: loss =  0.0724, R2 = 0.9275736808776855\n",
      "epoch: 353, loss =  0.0724, R2 = 0.9275736808776855\n",
      "test: loss =  0.0727, R2 = 0.9272392392158508\n",
      "epoch: 354, loss =  0.0727, R2 = 0.9272392392158508\n",
      "test: loss =  0.0728, R2 = 0.9272010326385498\n",
      "epoch: 355, loss =  0.0728, R2 = 0.9272010326385498\n",
      "test: loss =  0.0725, R2 = 0.927477240562439\n",
      "epoch: 356, loss =  0.0725, R2 = 0.927477240562439\n",
      "test: loss =  0.0722, R2 = 0.9278159141540527\n",
      "epoch: 357, loss =  0.0722, R2 = 0.9278159141540527\n",
      "test: loss =  0.0720, R2 = 0.9279569983482361\n",
      "epoch: 358, loss =  0.0720, R2 = 0.9279569983482361\n",
      "test: loss =  0.0721, R2 = 0.9278522729873657\n",
      "epoch: 359, loss =  0.0721, R2 = 0.9278522729873657\n",
      "test: loss =  0.0723, R2 = 0.9276636838912964\n",
      "epoch: 360, loss =  0.0723, R2 = 0.9276636838912964\n",
      "test: loss =  0.0724, R2 = 0.9275886416435242\n",
      "epoch: 361, loss =  0.0724, R2 = 0.9275886416435242\n",
      "test: loss =  0.0723, R2 = 0.9276917576789856\n",
      "epoch: 362, loss =  0.0723, R2 = 0.9276917576789856\n",
      "test: loss =  0.0721, R2 = 0.9278788566589355\n",
      "epoch: 363, loss =  0.0721, R2 = 0.9278788566589355\n",
      "test: loss =  0.0720, R2 = 0.9280039668083191\n",
      "epoch: 364, loss =  0.0720, R2 = 0.9280039668083191\n",
      "test: loss =  0.0720, R2 = 0.9279983043670654\n",
      "epoch: 365, loss =  0.0720, R2 = 0.9279983043670654\n",
      "test: loss =  0.0721, R2 = 0.9279099702835083\n",
      "epoch: 366, loss =  0.0721, R2 = 0.9279099702835083\n",
      "test: loss =  0.0721, R2 = 0.9278416633605957\n",
      "epoch: 367, loss =  0.0721, R2 = 0.9278416633605957\n",
      "test: loss =  0.0721, R2 = 0.927859365940094\n",
      "epoch: 368, loss =  0.0721, R2 = 0.927859365940094\n",
      "test: loss =  0.0720, R2 = 0.9279481172561646\n",
      "epoch: 369, loss =  0.0720, R2 = 0.9279481172561646\n",
      "test: loss =  0.0719, R2 = 0.9280403256416321\n",
      "epoch: 370, loss =  0.0719, R2 = 0.9280403256416321\n",
      "test: loss =  0.0719, R2 = 0.9280784130096436\n",
      "epoch: 371, loss =  0.0719, R2 = 0.9280784130096436\n",
      "test: loss =  0.0719, R2 = 0.9280567169189453\n",
      "epoch: 372, loss =  0.0719, R2 = 0.9280567169189453\n",
      "test: loss =  0.0720, R2 = 0.9280151128768921\n",
      "epoch: 373, loss =  0.0720, R2 = 0.9280151128768921\n",
      "test: loss =  0.0720, R2 = 0.9279997944831848\n",
      "epoch: 374, loss =  0.0720, R2 = 0.9279997944831848\n",
      "test: loss =  0.0720, R2 = 0.9280279874801636\n",
      "epoch: 375, loss =  0.0720, R2 = 0.9280279874801636\n",
      "test: loss =  0.0719, R2 = 0.9280813336372375\n",
      "epoch: 376, loss =  0.0719, R2 = 0.9280813336372375\n",
      "test: loss =  0.0719, R2 = 0.9281265735626221\n",
      "epoch: 377, loss =  0.0719, R2 = 0.9281265735626221\n",
      "test: loss =  0.0718, R2 = 0.9281427264213562\n",
      "epoch: 378, loss =  0.0718, R2 = 0.9281427264213562\n",
      "test: loss =  0.0718, R2 = 0.9281331896781921\n",
      "epoch: 379, loss =  0.0718, R2 = 0.9281331896781921\n",
      "test: loss =  0.0719, R2 = 0.9281184077262878\n",
      "epoch: 380, loss =  0.0719, R2 = 0.9281184077262878\n",
      "test: loss =  0.0719, R2 = 0.928118109703064\n",
      "epoch: 381, loss =  0.0719, R2 = 0.928118109703064\n",
      "test: loss =  0.0718, R2 = 0.928138017654419\n",
      "epoch: 382, loss =  0.0718, R2 = 0.928138017654419\n",
      "test: loss =  0.0718, R2 = 0.9281688332557678\n",
      "epoch: 383, loss =  0.0718, R2 = 0.9281688332557678\n",
      "test: loss =  0.0718, R2 = 0.9281957149505615\n",
      "epoch: 384, loss =  0.0718, R2 = 0.9281957149505615\n",
      "test: loss =  0.0718, R2 = 0.9282092452049255\n",
      "epoch: 385, loss =  0.0718, R2 = 0.9282092452049255\n",
      "test: loss =  0.0718, R2 = 0.9282104969024658\n",
      "epoch: 386, loss =  0.0718, R2 = 0.9282104969024658\n",
      "test: loss =  0.0718, R2 = 0.9282081723213196\n",
      "epoch: 387, loss =  0.0718, R2 = 0.9282081723213196\n",
      "test: loss =  0.0718, R2 = 0.9282112717628479\n",
      "epoch: 388, loss =  0.0718, R2 = 0.9282112717628479\n",
      "test: loss =  0.0718, R2 = 0.9282235503196716\n",
      "epoch: 389, loss =  0.0718, R2 = 0.9282235503196716\n",
      "test: loss =  0.0717, R2 = 0.9282422065734863\n",
      "epoch: 390, loss =  0.0717, R2 = 0.9282422065734863\n",
      "test: loss =  0.0717, R2 = 0.9282611012458801\n",
      "epoch: 391, loss =  0.0717, R2 = 0.9282611012458801\n",
      "test: loss =  0.0717, R2 = 0.9282750487327576\n",
      "epoch: 392, loss =  0.0717, R2 = 0.9282750487327576\n",
      "test: loss =  0.0717, R2 = 0.9282829165458679\n",
      "epoch: 393, loss =  0.0717, R2 = 0.9282829165458679\n",
      "test: loss =  0.0717, R2 = 0.9282872676849365\n",
      "epoch: 394, loss =  0.0717, R2 = 0.9282872676849365\n",
      "test: loss =  0.0717, R2 = 0.9282920360565186\n",
      "epoch: 395, loss =  0.0717, R2 = 0.9282920360565186\n",
      "test: loss =  0.0717, R2 = 0.9283002018928528\n",
      "epoch: 396, loss =  0.0717, R2 = 0.9283002018928528\n",
      "test: loss =  0.0717, R2 = 0.9283121228218079\n",
      "epoch: 397, loss =  0.0717, R2 = 0.9283121228218079\n",
      "test: loss =  0.0717, R2 = 0.9283260107040405\n",
      "epoch: 398, loss =  0.0717, R2 = 0.9283260107040405\n",
      "test: loss =  0.0716, R2 = 0.9283393025398254\n",
      "epoch: 399, loss =  0.0716, R2 = 0.9283393025398254\n",
      "test: loss =  0.0716, R2 = 0.9283503890037537\n",
      "epoch: 400, loss =  0.0716, R2 = 0.9283503890037537\n",
      "test: loss =  0.0716, R2 = 0.9283589720726013\n",
      "epoch: 401, loss =  0.0716, R2 = 0.9283589720726013\n",
      "test: loss =  0.0716, R2 = 0.9283661842346191\n",
      "epoch: 402, loss =  0.0716, R2 = 0.9283661842346191\n",
      "test: loss =  0.0716, R2 = 0.9283735752105713\n",
      "epoch: 403, loss =  0.0716, R2 = 0.9283735752105713\n",
      "test: loss =  0.0716, R2 = 0.9283822774887085\n",
      "epoch: 404, loss =  0.0716, R2 = 0.9283822774887085\n",
      "test: loss =  0.0716, R2 = 0.9283925294876099\n",
      "epoch: 405, loss =  0.0716, R2 = 0.9283925294876099\n",
      "test: loss =  0.0716, R2 = 0.9284037351608276\n",
      "epoch: 406, loss =  0.0716, R2 = 0.9284037351608276\n",
      "test: loss =  0.0716, R2 = 0.9284150004386902\n",
      "epoch: 407, loss =  0.0716, R2 = 0.9284150004386902\n",
      "test: loss =  0.0716, R2 = 0.9284255504608154\n",
      "epoch: 408, loss =  0.0716, R2 = 0.9284255504608154\n",
      "test: loss =  0.0715, R2 = 0.9284350872039795\n",
      "epoch: 409, loss =  0.0715, R2 = 0.9284350872039795\n",
      "test: loss =  0.0715, R2 = 0.9284439086914062\n",
      "epoch: 410, loss =  0.0715, R2 = 0.9284439086914062\n",
      "test: loss =  0.0715, R2 = 0.9284523725509644\n",
      "epoch: 411, loss =  0.0715, R2 = 0.9284523725509644\n",
      "test: loss =  0.0715, R2 = 0.9284611940383911\n",
      "epoch: 412, loss =  0.0715, R2 = 0.9284611940383911\n",
      "test: loss =  0.0715, R2 = 0.9284704327583313\n",
      "epoch: 413, loss =  0.0715, R2 = 0.9284704327583313\n",
      "test: loss =  0.0715, R2 = 0.9284802675247192\n",
      "epoch: 414, loss =  0.0715, R2 = 0.9284802675247192\n",
      "test: loss =  0.0715, R2 = 0.928490400314331\n",
      "epoch: 415, loss =  0.0715, R2 = 0.928490400314331\n",
      "test: loss =  0.0715, R2 = 0.9285005927085876\n",
      "epoch: 416, loss =  0.0715, R2 = 0.9285005927085876\n",
      "test: loss =  0.0715, R2 = 0.9285105466842651\n",
      "epoch: 417, loss =  0.0715, R2 = 0.9285105466842651\n",
      "test: loss =  0.0715, R2 = 0.928520143032074\n",
      "epoch: 418, loss =  0.0715, R2 = 0.928520143032074\n",
      "test: loss =  0.0715, R2 = 0.9285294413566589\n",
      "epoch: 419, loss =  0.0715, R2 = 0.9285294413566589\n",
      "test: loss =  0.0714, R2 = 0.9285385608673096\n",
      "epoch: 420, loss =  0.0714, R2 = 0.9285385608673096\n",
      "test: loss =  0.0714, R2 = 0.9285476803779602\n",
      "epoch: 421, loss =  0.0714, R2 = 0.9285476803779602\n",
      "test: loss =  0.0714, R2 = 0.9285569190979004\n",
      "epoch: 422, loss =  0.0714, R2 = 0.9285569190979004\n",
      "test: loss =  0.0714, R2 = 0.9285662770271301\n",
      "epoch: 423, loss =  0.0714, R2 = 0.9285662770271301\n",
      "test: loss =  0.0714, R2 = 0.9285757541656494\n",
      "epoch: 424, loss =  0.0714, R2 = 0.9285757541656494\n",
      "test: loss =  0.0714, R2 = 0.9285852909088135\n",
      "epoch: 425, loss =  0.0714, R2 = 0.9285852909088135\n",
      "test: loss =  0.0714, R2 = 0.928594708442688\n",
      "epoch: 426, loss =  0.0714, R2 = 0.928594708442688\n",
      "test: loss =  0.0714, R2 = 0.9286038875579834\n",
      "epoch: 427, loss =  0.0714, R2 = 0.9286038875579834\n",
      "test: loss =  0.0714, R2 = 0.9286127686500549\n",
      "epoch: 428, loss =  0.0714, R2 = 0.9286127686500549\n",
      "test: loss =  0.0714, R2 = 0.9286211133003235\n",
      "epoch: 429, loss =  0.0714, R2 = 0.9286211133003235\n",
      "test: loss =  0.0714, R2 = 0.9286287426948547\n",
      "epoch: 430, loss =  0.0714, R2 = 0.9286287426948547\n",
      "test: loss =  0.0713, R2 = 0.9286352396011353\n",
      "epoch: 431, loss =  0.0713, R2 = 0.9286352396011353\n",
      "test: loss =  0.0713, R2 = 0.9286400675773621\n",
      "epoch: 432, loss =  0.0713, R2 = 0.9286400675773621\n",
      "test: loss =  0.0713, R2 = 0.9286420941352844\n",
      "epoch: 433, loss =  0.0713, R2 = 0.9286420941352844\n",
      "test: loss =  0.0713, R2 = 0.9286395907402039\n",
      "epoch: 434, loss =  0.0713, R2 = 0.9286395907402039\n",
      "test: loss =  0.0714, R2 = 0.9286296367645264\n",
      "epoch: 435, loss =  0.0714, R2 = 0.9286296367645264\n",
      "test: loss =  0.0714, R2 = 0.9286071062088013\n",
      "epoch: 436, loss =  0.0714, R2 = 0.9286071062088013\n",
      "test: loss =  0.0714, R2 = 0.9285638332366943\n",
      "epoch: 437, loss =  0.0714, R2 = 0.9285638332366943\n",
      "test: loss =  0.0715, R2 = 0.9284859895706177\n",
      "epoch: 438, loss =  0.0715, R2 = 0.9284859895706177\n",
      "test: loss =  0.0716, R2 = 0.9283515214920044\n",
      "epoch: 439, loss =  0.0716, R2 = 0.9283515214920044\n",
      "test: loss =  0.0719, R2 = 0.9281252026557922\n",
      "epoch: 440, loss =  0.0719, R2 = 0.9281252026557922\n",
      "test: loss =  0.0722, R2 = 0.9277571439743042\n",
      "epoch: 441, loss =  0.0722, R2 = 0.9277571439743042\n",
      "test: loss =  0.0728, R2 = 0.9271828532218933\n",
      "epoch: 442, loss =  0.0728, R2 = 0.9271828532218933\n",
      "test: loss =  0.0736, R2 = 0.9263560175895691\n",
      "epoch: 443, loss =  0.0736, R2 = 0.9263560175895691\n",
      "test: loss =  0.0747, R2 = 0.9253075122833252\n",
      "epoch: 444, loss =  0.0747, R2 = 0.9253075122833252\n",
      "test: loss =  0.0757, R2 = 0.9242959022521973\n",
      "epoch: 445, loss =  0.0757, R2 = 0.9242959022521973\n",
      "test: loss =  0.0761, R2 = 0.9238336682319641\n",
      "epoch: 446, loss =  0.0761, R2 = 0.9238336682319641\n",
      "test: loss =  0.0755, R2 = 0.9244934320449829\n",
      "epoch: 447, loss =  0.0755, R2 = 0.9244934320449829\n",
      "test: loss =  0.0738, R2 = 0.9261903762817383\n",
      "epoch: 448, loss =  0.0738, R2 = 0.9261903762817383\n",
      "test: loss =  0.0720, R2 = 0.9279652833938599\n",
      "epoch: 449, loss =  0.0720, R2 = 0.9279652833938599\n",
      "test: loss =  0.0713, R2 = 0.9286996126174927\n",
      "epoch: 450, loss =  0.0713, R2 = 0.9286996126174927\n",
      "test: loss =  0.0718, R2 = 0.9281913042068481\n",
      "epoch: 451, loss =  0.0718, R2 = 0.9281913042068481\n",
      "test: loss =  0.0727, R2 = 0.927232027053833\n",
      "epoch: 452, loss =  0.0727, R2 = 0.927232027053833\n",
      "test: loss =  0.0731, R2 = 0.926832377910614\n",
      "epoch: 453, loss =  0.0731, R2 = 0.926832377910614\n",
      "test: loss =  0.0726, R2 = 0.9273728728294373\n",
      "epoch: 454, loss =  0.0726, R2 = 0.9273728728294373\n",
      "test: loss =  0.0717, R2 = 0.9283002018928528\n",
      "epoch: 455, loss =  0.0717, R2 = 0.9283002018928528\n",
      "test: loss =  0.0712, R2 = 0.9287580251693726\n",
      "epoch: 456, loss =  0.0712, R2 = 0.9287580251693726\n",
      "test: loss =  0.0715, R2 = 0.9284813404083252\n",
      "epoch: 457, loss =  0.0715, R2 = 0.9284813404083252\n",
      "test: loss =  0.0720, R2 = 0.9279687404632568\n",
      "epoch: 458, loss =  0.0720, R2 = 0.9279687404632568\n",
      "test: loss =  0.0721, R2 = 0.9278576970100403\n",
      "epoch: 459, loss =  0.0721, R2 = 0.9278576970100403\n",
      "test: loss =  0.0717, R2 = 0.9282634854316711\n",
      "epoch: 460, loss =  0.0717, R2 = 0.9282634854316711\n",
      "test: loss =  0.0712, R2 = 0.9287417531013489\n",
      "epoch: 461, loss =  0.0712, R2 = 0.9287417531013489\n",
      "test: loss =  0.0711, R2 = 0.928843080997467\n",
      "epoch: 462, loss =  0.0711, R2 = 0.928843080997467\n",
      "test: loss =  0.0714, R2 = 0.9285926222801208\n",
      "epoch: 463, loss =  0.0714, R2 = 0.9285926222801208\n",
      "test: loss =  0.0716, R2 = 0.9283753037452698\n",
      "epoch: 464, loss =  0.0716, R2 = 0.9283753037452698\n",
      "test: loss =  0.0715, R2 = 0.9284637570381165\n",
      "epoch: 465, loss =  0.0715, R2 = 0.9284637570381165\n",
      "test: loss =  0.0712, R2 = 0.9287496209144592\n",
      "epoch: 466, loss =  0.0712, R2 = 0.9287496209144592\n",
      "test: loss =  0.0711, R2 = 0.928925633430481\n",
      "epoch: 467, loss =  0.0711, R2 = 0.928925633430481\n",
      "test: loss =  0.0711, R2 = 0.9288563132286072\n",
      "epoch: 468, loss =  0.0711, R2 = 0.9288563132286072\n",
      "test: loss =  0.0713, R2 = 0.9286943078041077\n",
      "epoch: 469, loss =  0.0713, R2 = 0.9286943078041077\n",
      "test: loss =  0.0713, R2 = 0.9286644458770752\n",
      "epoch: 470, loss =  0.0713, R2 = 0.9286644458770752\n",
      "test: loss =  0.0712, R2 = 0.9288104772567749\n",
      "epoch: 471, loss =  0.0712, R2 = 0.9288104772567749\n",
      "test: loss =  0.0710, R2 = 0.9289772510528564\n",
      "epoch: 472, loss =  0.0710, R2 = 0.9289772510528564\n",
      "test: loss =  0.0710, R2 = 0.9290112853050232\n",
      "epoch: 473, loss =  0.0710, R2 = 0.9290112853050232\n",
      "test: loss =  0.0711, R2 = 0.9289237260818481\n",
      "epoch: 474, loss =  0.0711, R2 = 0.9289237260818481\n",
      "test: loss =  0.0711, R2 = 0.9288487434387207\n",
      "epoch: 475, loss =  0.0711, R2 = 0.9288487434387207\n",
      "test: loss =  0.0711, R2 = 0.9288817644119263\n",
      "epoch: 476, loss =  0.0711, R2 = 0.9288817644119263\n",
      "test: loss =  0.0710, R2 = 0.928987979888916\n",
      "epoch: 477, loss =  0.0710, R2 = 0.928987979888916\n",
      "test: loss =  0.0709, R2 = 0.929063081741333\n",
      "epoch: 478, loss =  0.0709, R2 = 0.929063081741333\n",
      "test: loss =  0.0709, R2 = 0.9290540218353271\n",
      "epoch: 479, loss =  0.0709, R2 = 0.9290540218353271\n",
      "test: loss =  0.0710, R2 = 0.9290032982826233\n",
      "epoch: 480, loss =  0.0710, R2 = 0.9290032982826233\n",
      "test: loss =  0.0710, R2 = 0.9289865493774414\n",
      "epoch: 481, loss =  0.0710, R2 = 0.9289865493774414\n",
      "test: loss =  0.0710, R2 = 0.9290297031402588\n",
      "epoch: 482, loss =  0.0710, R2 = 0.9290297031402588\n",
      "test: loss =  0.0709, R2 = 0.9290930032730103\n",
      "epoch: 483, loss =  0.0709, R2 = 0.9290930032730103\n",
      "test: loss =  0.0709, R2 = 0.9291239976882935\n",
      "epoch: 484, loss =  0.0709, R2 = 0.9291239976882935\n",
      "test: loss =  0.0709, R2 = 0.9291123747825623\n",
      "epoch: 485, loss =  0.0709, R2 = 0.9291123747825623\n",
      "test: loss =  0.0709, R2 = 0.9290914535522461\n",
      "epoch: 486, loss =  0.0709, R2 = 0.9290914535522461\n",
      "test: loss =  0.0709, R2 = 0.929095983505249\n",
      "epoch: 487, loss =  0.0709, R2 = 0.929095983505249\n",
      "test: loss =  0.0709, R2 = 0.9291281700134277\n",
      "epoch: 488, loss =  0.0709, R2 = 0.9291281700134277\n",
      "test: loss =  0.0708, R2 = 0.9291622042655945\n",
      "epoch: 489, loss =  0.0708, R2 = 0.9291622042655945\n",
      "test: loss =  0.0708, R2 = 0.929175615310669\n",
      "epoch: 490, loss =  0.0708, R2 = 0.929175615310669\n",
      "test: loss =  0.0708, R2 = 0.9291703701019287\n",
      "epoch: 491, loss =  0.0708, R2 = 0.9291703701019287\n",
      "test: loss =  0.0708, R2 = 0.9291660785675049\n",
      "epoch: 492, loss =  0.0708, R2 = 0.9291660785675049\n",
      "test: loss =  0.0708, R2 = 0.9291775822639465\n",
      "epoch: 493, loss =  0.0708, R2 = 0.9291775822639465\n",
      "test: loss =  0.0708, R2 = 0.9292016625404358\n",
      "epoch: 494, loss =  0.0708, R2 = 0.9292016625404358\n",
      "test: loss =  0.0708, R2 = 0.9292236566543579\n",
      "epoch: 495, loss =  0.0708, R2 = 0.9292236566543579\n",
      "test: loss =  0.0707, R2 = 0.9292334318161011\n",
      "epoch: 496, loss =  0.0707, R2 = 0.9292334318161011\n",
      "test: loss =  0.0707, R2 = 0.929233968257904\n",
      "epoch: 497, loss =  0.0707, R2 = 0.929233968257904\n",
      "test: loss =  0.0707, R2 = 0.9292359948158264\n",
      "epoch: 498, loss =  0.0707, R2 = 0.9292359948158264\n",
      "test: loss =  0.0707, R2 = 0.9292468428611755\n",
      "epoch: 499, loss =  0.0707, R2 = 0.9292468428611755\n",
      "test: loss =  0.0707, R2 = 0.9292643070220947\n",
      "epoch: 500, loss =  0.0707, R2 = 0.9292643070220947\n",
      "test: loss =  0.0707, R2 = 0.9292808175086975\n",
      "epoch: 501, loss =  0.0707, R2 = 0.9292808175086975\n",
      "test: loss =  0.0707, R2 = 0.9292908310890198\n",
      "epoch: 502, loss =  0.0707, R2 = 0.9292908310890198\n",
      "test: loss =  0.0707, R2 = 0.9292954206466675\n",
      "epoch: 503, loss =  0.0707, R2 = 0.9292954206466675\n",
      "test: loss =  0.0707, R2 = 0.9293001890182495\n",
      "epoch: 504, loss =  0.0707, R2 = 0.9293001890182495\n",
      "test: loss =  0.0707, R2 = 0.9293091297149658\n",
      "epoch: 505, loss =  0.0707, R2 = 0.9293091297149658\n",
      "test: loss =  0.0707, R2 = 0.9293220639228821\n",
      "epoch: 506, loss =  0.0707, R2 = 0.9293220639228821\n",
      "test: loss =  0.0706, R2 = 0.929335355758667\n",
      "epoch: 507, loss =  0.0706, R2 = 0.929335355758667\n",
      "test: loss =  0.0706, R2 = 0.9293458461761475\n",
      "epoch: 508, loss =  0.0706, R2 = 0.9293458461761475\n",
      "test: loss =  0.0706, R2 = 0.9293532967567444\n",
      "epoch: 509, loss =  0.0706, R2 = 0.9293532967567444\n",
      "test: loss =  0.0706, R2 = 0.929360032081604\n",
      "epoch: 510, loss =  0.0706, R2 = 0.929360032081604\n",
      "test: loss =  0.0706, R2 = 0.9293684363365173\n",
      "epoch: 511, loss =  0.0706, R2 = 0.9293684363365173\n",
      "test: loss =  0.0706, R2 = 0.9293789863586426\n",
      "epoch: 512, loss =  0.0706, R2 = 0.9293789863586426\n",
      "test: loss =  0.0706, R2 = 0.9293903112411499\n",
      "epoch: 513, loss =  0.0706, R2 = 0.9293903112411499\n",
      "test: loss =  0.0706, R2 = 0.9294006824493408\n",
      "epoch: 514, loss =  0.0706, R2 = 0.9294006824493408\n",
      "test: loss =  0.0706, R2 = 0.929409384727478\n",
      "epoch: 515, loss =  0.0706, R2 = 0.929409384727478\n",
      "test: loss =  0.0706, R2 = 0.9294171333312988\n",
      "epoch: 516, loss =  0.0706, R2 = 0.9294171333312988\n",
      "test: loss =  0.0706, R2 = 0.9294252991676331\n",
      "epoch: 517, loss =  0.0706, R2 = 0.9294252991676331\n",
      "test: loss =  0.0705, R2 = 0.9294344782829285\n",
      "epoch: 518, loss =  0.0705, R2 = 0.9294344782829285\n",
      "test: loss =  0.0705, R2 = 0.929444432258606\n",
      "epoch: 519, loss =  0.0705, R2 = 0.929444432258606\n",
      "test: loss =  0.0705, R2 = 0.9294542074203491\n",
      "epoch: 520, loss =  0.0705, R2 = 0.9294542074203491\n",
      "test: loss =  0.0705, R2 = 0.9294631481170654\n",
      "epoch: 521, loss =  0.0705, R2 = 0.9294631481170654\n",
      "test: loss =  0.0705, R2 = 0.9294710159301758\n",
      "epoch: 522, loss =  0.0705, R2 = 0.9294710159301758\n",
      "test: loss =  0.0705, R2 = 0.9294782876968384\n",
      "epoch: 523, loss =  0.0705, R2 = 0.9294782876968384\n",
      "test: loss =  0.0705, R2 = 0.9294853806495667\n",
      "epoch: 524, loss =  0.0705, R2 = 0.9294853806495667\n",
      "test: loss =  0.0705, R2 = 0.9294922351837158\n",
      "epoch: 525, loss =  0.0705, R2 = 0.9294922351837158\n",
      "test: loss =  0.0705, R2 = 0.9294984340667725\n",
      "epoch: 526, loss =  0.0705, R2 = 0.9294984340667725\n",
      "test: loss =  0.0705, R2 = 0.9295030236244202\n",
      "epoch: 527, loss =  0.0705, R2 = 0.9295030236244202\n",
      "test: loss =  0.0705, R2 = 0.9295049905776978\n",
      "epoch: 528, loss =  0.0705, R2 = 0.9295049905776978\n",
      "test: loss =  0.0705, R2 = 0.9295031428337097\n",
      "epoch: 529, loss =  0.0705, R2 = 0.9295031428337097\n",
      "test: loss =  0.0705, R2 = 0.9294959902763367\n",
      "epoch: 530, loss =  0.0705, R2 = 0.9294959902763367\n",
      "test: loss =  0.0705, R2 = 0.9294811487197876\n",
      "epoch: 531, loss =  0.0705, R2 = 0.9294811487197876\n",
      "test: loss =  0.0705, R2 = 0.9294545650482178\n",
      "epoch: 532, loss =  0.0705, R2 = 0.9294545650482178\n",
      "test: loss =  0.0706, R2 = 0.929409921169281\n",
      "epoch: 533, loss =  0.0706, R2 = 0.929409921169281\n",
      "test: loss =  0.0706, R2 = 0.9293372631072998\n",
      "epoch: 534, loss =  0.0706, R2 = 0.9293372631072998\n",
      "test: loss =  0.0708, R2 = 0.9292212724685669\n",
      "epoch: 535, loss =  0.0708, R2 = 0.9292212724685669\n",
      "test: loss =  0.0709, R2 = 0.9290394186973572\n",
      "epoch: 536, loss =  0.0709, R2 = 0.9290394186973572\n",
      "test: loss =  0.0712, R2 = 0.9287580251693726\n",
      "epoch: 537, loss =  0.0712, R2 = 0.9287580251693726\n",
      "test: loss =  0.0716, R2 = 0.9283303022384644\n",
      "epoch: 538, loss =  0.0716, R2 = 0.9283303022384644\n",
      "test: loss =  0.0723, R2 = 0.9276953339576721\n",
      "epoch: 539, loss =  0.0723, R2 = 0.9276953339576721\n",
      "test: loss =  0.0732, R2 = 0.9267914295196533\n",
      "epoch: 540, loss =  0.0732, R2 = 0.9267914295196533\n",
      "test: loss =  0.0744, R2 = 0.9255849719047546\n",
      "epoch: 541, loss =  0.0744, R2 = 0.9255849719047546\n",
      "test: loss =  0.0758, R2 = 0.9241556525230408\n",
      "epoch: 542, loss =  0.0758, R2 = 0.9241556525230408\n",
      "test: loss =  0.0772, R2 = 0.9227883815765381\n",
      "epoch: 543, loss =  0.0772, R2 = 0.9227883815765381\n",
      "test: loss =  0.0779, R2 = 0.922055721282959\n",
      "epoch: 544, loss =  0.0779, R2 = 0.922055721282959\n",
      "test: loss =  0.0774, R2 = 0.9225863218307495\n",
      "epoch: 545, loss =  0.0774, R2 = 0.9225863218307495\n",
      "test: loss =  0.0754, R2 = 0.924567461013794\n",
      "epoch: 546, loss =  0.0754, R2 = 0.924567461013794\n",
      "test: loss =  0.0727, R2 = 0.9272326231002808\n",
      "epoch: 547, loss =  0.0727, R2 = 0.9272326231002808\n",
      "test: loss =  0.0708, R2 = 0.9292282462120056\n",
      "epoch: 548, loss =  0.0708, R2 = 0.9292282462120056\n",
      "test: loss =  0.0703, R2 = 0.9296362996101379\n",
      "epoch: 549, loss =  0.0703, R2 = 0.9296362996101379\n",
      "test: loss =  0.0713, R2 = 0.9286607503890991\n",
      "epoch: 550, loss =  0.0713, R2 = 0.9286607503890991\n",
      "test: loss =  0.0726, R2 = 0.927354097366333\n",
      "epoch: 551, loss =  0.0726, R2 = 0.927354097366333\n",
      "test: loss =  0.0732, R2 = 0.9268115758895874\n",
      "epoch: 552, loss =  0.0732, R2 = 0.9268115758895874\n",
      "test: loss =  0.0725, R2 = 0.9274375438690186\n",
      "epoch: 553, loss =  0.0725, R2 = 0.9274375438690186\n",
      "test: loss =  0.0713, R2 = 0.9286949634552002\n",
      "epoch: 554, loss =  0.0713, R2 = 0.9286949634552002\n",
      "test: loss =  0.0704, R2 = 0.9296128153800964\n",
      "epoch: 555, loss =  0.0704, R2 = 0.9296128153800964\n",
      "test: loss =  0.0703, R2 = 0.9296324253082275\n",
      "epoch: 556, loss =  0.0703, R2 = 0.9296324253082275\n",
      "test: loss =  0.0710, R2 = 0.9290040135383606\n",
      "epoch: 557, loss =  0.0710, R2 = 0.9290040135383606\n",
      "test: loss =  0.0715, R2 = 0.9284404516220093\n",
      "epoch: 558, loss =  0.0715, R2 = 0.9284404516220093\n",
      "test: loss =  0.0715, R2 = 0.9284647703170776\n",
      "epoch: 559, loss =  0.0715, R2 = 0.9284647703170776\n",
      "test: loss =  0.0710, R2 = 0.9290211200714111\n",
      "epoch: 560, loss =  0.0710, R2 = 0.9290211200714111\n",
      "test: loss =  0.0704, R2 = 0.9296110272407532\n",
      "epoch: 561, loss =  0.0704, R2 = 0.9296110272407532\n",
      "test: loss =  0.0702, R2 = 0.9297923445701599\n",
      "epoch: 562, loss =  0.0702, R2 = 0.9297923445701599\n",
      "test: loss =  0.0704, R2 = 0.9295454025268555\n",
      "epoch: 563, loss =  0.0704, R2 = 0.9295454025268555\n",
      "test: loss =  0.0708, R2 = 0.9292086958885193\n",
      "epoch: 564, loss =  0.0708, R2 = 0.9292086958885193\n",
      "test: loss =  0.0709, R2 = 0.9291268587112427\n",
      "epoch: 565, loss =  0.0709, R2 = 0.9291268587112427\n",
      "test: loss =  0.0706, R2 = 0.929362416267395\n",
      "epoch: 566, loss =  0.0706, R2 = 0.929362416267395\n",
      "test: loss =  0.0703, R2 = 0.9296959638595581\n",
      "epoch: 567, loss =  0.0703, R2 = 0.9296959638595581\n",
      "test: loss =  0.0701, R2 = 0.9298630952835083\n",
      "epoch: 568, loss =  0.0701, R2 = 0.9298630952835083\n",
      "test: loss =  0.0702, R2 = 0.9297873377799988\n",
      "epoch: 569, loss =  0.0702, R2 = 0.9297873377799988\n",
      "test: loss =  0.0704, R2 = 0.9296069145202637\n",
      "epoch: 570, loss =  0.0704, R2 = 0.9296069145202637\n",
      "test: loss =  0.0705, R2 = 0.9295178055763245\n",
      "epoch: 571, loss =  0.0705, R2 = 0.9295178055763245\n",
      "test: loss =  0.0704, R2 = 0.9296019673347473\n",
      "epoch: 572, loss =  0.0704, R2 = 0.9296019673347473\n",
      "test: loss =  0.0702, R2 = 0.9297816753387451\n",
      "epoch: 573, loss =  0.0702, R2 = 0.9297816753387451\n",
      "test: loss =  0.0701, R2 = 0.9299144744873047\n",
      "epoch: 574, loss =  0.0701, R2 = 0.9299144744873047\n",
      "test: loss =  0.0701, R2 = 0.9299215078353882\n",
      "epoch: 575, loss =  0.0701, R2 = 0.9299215078353882\n",
      "test: loss =  0.0701, R2 = 0.929839015007019\n",
      "epoch: 576, loss =  0.0701, R2 = 0.929839015007019\n",
      "test: loss =  0.0702, R2 = 0.9297663569450378\n",
      "epoch: 577, loss =  0.0702, R2 = 0.9297663569450378\n",
      "test: loss =  0.0702, R2 = 0.9297751188278198\n",
      "epoch: 578, loss =  0.0702, R2 = 0.9297751188278198\n",
      "test: loss =  0.0701, R2 = 0.9298588037490845\n",
      "epoch: 579, loss =  0.0701, R2 = 0.9298588037490845\n",
      "test: loss =  0.0700, R2 = 0.9299532175064087\n",
      "epoch: 580, loss =  0.0700, R2 = 0.9299532175064087\n",
      "test: loss =  0.0700, R2 = 0.929997444152832\n",
      "epoch: 581, loss =  0.0700, R2 = 0.929997444152832\n",
      "test: loss =  0.0700, R2 = 0.9299800992012024\n",
      "epoch: 582, loss =  0.0700, R2 = 0.9299800992012024\n",
      "test: loss =  0.0700, R2 = 0.9299376010894775\n",
      "epoch: 583, loss =  0.0700, R2 = 0.9299376010894775\n",
      "test: loss =  0.0701, R2 = 0.9299175143241882\n",
      "epoch: 584, loss =  0.0701, R2 = 0.9299175143241882\n",
      "test: loss =  0.0700, R2 = 0.9299410581588745\n",
      "epoch: 585, loss =  0.0700, R2 = 0.9299410581588745\n",
      "test: loss =  0.0700, R2 = 0.929993212223053\n",
      "epoch: 586, loss =  0.0700, R2 = 0.929993212223053\n",
      "test: loss =  0.0699, R2 = 0.9300408363342285\n",
      "epoch: 587, loss =  0.0699, R2 = 0.9300408363342285\n",
      "test: loss =  0.0699, R2 = 0.9300604462623596\n",
      "epoch: 588, loss =  0.0699, R2 = 0.9300604462623596\n",
      "test: loss =  0.0699, R2 = 0.930052638053894\n",
      "epoch: 589, loss =  0.0699, R2 = 0.930052638053894\n",
      "test: loss =  0.0699, R2 = 0.9300368428230286\n",
      "epoch: 590, loss =  0.0699, R2 = 0.9300368428230286\n",
      "test: loss =  0.0699, R2 = 0.9300337433815002\n",
      "epoch: 591, loss =  0.0699, R2 = 0.9300337433815002\n",
      "test: loss =  0.0699, R2 = 0.9300509691238403\n",
      "epoch: 592, loss =  0.0699, R2 = 0.9300509691238403\n",
      "test: loss =  0.0699, R2 = 0.9300806522369385\n",
      "epoch: 593, loss =  0.0699, R2 = 0.9300806522369385\n",
      "test: loss =  0.0699, R2 = 0.9301081895828247\n",
      "epoch: 594, loss =  0.0699, R2 = 0.9301081895828247\n",
      "test: loss =  0.0699, R2 = 0.9301231503486633\n",
      "epoch: 595, loss =  0.0699, R2 = 0.9301231503486633\n",
      "test: loss =  0.0699, R2 = 0.9301254153251648\n",
      "epoch: 596, loss =  0.0699, R2 = 0.9301254153251648\n",
      "test: loss =  0.0699, R2 = 0.9301229119300842\n",
      "epoch: 597, loss =  0.0699, R2 = 0.9301229119300842\n",
      "test: loss =  0.0699, R2 = 0.9301248788833618\n",
      "epoch: 598, loss =  0.0699, R2 = 0.9301248788833618\n",
      "test: loss =  0.0698, R2 = 0.9301357269287109\n",
      "epoch: 599, loss =  0.0698, R2 = 0.9301357269287109\n",
      "test: loss =  0.0698, R2 = 0.9301533699035645\n",
      "epoch: 600, loss =  0.0698, R2 = 0.9301533699035645\n",
      "test: loss =  0.0698, R2 = 0.9301718473434448\n",
      "epoch: 601, loss =  0.0698, R2 = 0.9301718473434448\n",
      "test: loss =  0.0698, R2 = 0.9301859140396118\n",
      "epoch: 602, loss =  0.0698, R2 = 0.9301859140396118\n",
      "test: loss =  0.0698, R2 = 0.9301939606666565\n",
      "epoch: 603, loss =  0.0698, R2 = 0.9301939606666565\n",
      "test: loss =  0.0698, R2 = 0.9301982522010803\n",
      "epoch: 604, loss =  0.0698, R2 = 0.9301982522010803\n",
      "test: loss =  0.0698, R2 = 0.9302026629447937\n",
      "epoch: 605, loss =  0.0698, R2 = 0.9302026629447937\n",
      "test: loss =  0.0698, R2 = 0.9302101731300354\n",
      "epoch: 606, loss =  0.0698, R2 = 0.9302101731300354\n",
      "test: loss =  0.0698, R2 = 0.930221438407898\n",
      "epoch: 607, loss =  0.0698, R2 = 0.930221438407898\n",
      "test: loss =  0.0697, R2 = 0.9302347898483276\n",
      "epoch: 608, loss =  0.0697, R2 = 0.9302347898483276\n",
      "test: loss =  0.0697, R2 = 0.9302477836608887\n",
      "epoch: 609, loss =  0.0697, R2 = 0.9302477836608887\n",
      "test: loss =  0.0697, R2 = 0.9302586317062378\n",
      "epoch: 610, loss =  0.0697, R2 = 0.9302586317062378\n",
      "test: loss =  0.0697, R2 = 0.9302669763565063\n",
      "epoch: 611, loss =  0.0697, R2 = 0.9302669763565063\n",
      "test: loss =  0.0697, R2 = 0.9302738308906555\n",
      "epoch: 612, loss =  0.0697, R2 = 0.9302738308906555\n",
      "test: loss =  0.0697, R2 = 0.9302808046340942\n",
      "epoch: 613, loss =  0.0697, R2 = 0.9302808046340942\n",
      "test: loss =  0.0697, R2 = 0.9302890300750732\n",
      "epoch: 614, loss =  0.0697, R2 = 0.9302890300750732\n",
      "test: loss =  0.0697, R2 = 0.9302987456321716\n",
      "epoch: 615, loss =  0.0697, R2 = 0.9302987456321716\n",
      "test: loss =  0.0697, R2 = 0.930309534072876\n",
      "epoch: 616, loss =  0.0697, R2 = 0.930309534072876\n",
      "test: loss =  0.0697, R2 = 0.9303204417228699\n",
      "epoch: 617, loss =  0.0697, R2 = 0.9303204417228699\n",
      "test: loss =  0.0697, R2 = 0.9303306937217712\n",
      "epoch: 618, loss =  0.0697, R2 = 0.9303306937217712\n",
      "test: loss =  0.0696, R2 = 0.9303399920463562\n",
      "epoch: 619, loss =  0.0696, R2 = 0.9303399920463562\n",
      "test: loss =  0.0696, R2 = 0.9303484559059143\n",
      "epoch: 620, loss =  0.0696, R2 = 0.9303484559059143\n",
      "test: loss =  0.0696, R2 = 0.9303565621376038\n",
      "epoch: 621, loss =  0.0696, R2 = 0.9303565621376038\n",
      "test: loss =  0.0696, R2 = 0.9303649067878723\n",
      "epoch: 622, loss =  0.0696, R2 = 0.9303649067878723\n",
      "test: loss =  0.0696, R2 = 0.9303737282752991\n",
      "epoch: 623, loss =  0.0696, R2 = 0.9303737282752991\n",
      "test: loss =  0.0696, R2 = 0.9303831458091736\n",
      "epoch: 624, loss =  0.0696, R2 = 0.9303831458091736\n",
      "test: loss =  0.0696, R2 = 0.930392861366272\n",
      "epoch: 625, loss =  0.0696, R2 = 0.930392861366272\n",
      "test: loss =  0.0696, R2 = 0.9304026365280151\n",
      "epoch: 626, loss =  0.0696, R2 = 0.9304026365280151\n",
      "test: loss =  0.0696, R2 = 0.930412232875824\n",
      "epoch: 627, loss =  0.0696, R2 = 0.930412232875824\n",
      "test: loss =  0.0696, R2 = 0.9304214715957642\n",
      "epoch: 628, loss =  0.0696, R2 = 0.9304214715957642\n",
      "test: loss =  0.0696, R2 = 0.9304304122924805\n",
      "epoch: 629, loss =  0.0696, R2 = 0.9304304122924805\n",
      "test: loss =  0.0695, R2 = 0.9304391741752625\n",
      "epoch: 630, loss =  0.0695, R2 = 0.9304391741752625\n",
      "test: loss =  0.0695, R2 = 0.9304479360580444\n",
      "epoch: 631, loss =  0.0695, R2 = 0.9304479360580444\n",
      "test: loss =  0.0695, R2 = 0.9304568767547607\n",
      "epoch: 632, loss =  0.0695, R2 = 0.9304568767547607\n",
      "test: loss =  0.0695, R2 = 0.9304659366607666\n",
      "epoch: 633, loss =  0.0695, R2 = 0.9304659366607666\n",
      "test: loss =  0.0695, R2 = 0.930475115776062\n",
      "epoch: 634, loss =  0.0695, R2 = 0.930475115776062\n",
      "test: loss =  0.0695, R2 = 0.9304844737052917\n",
      "epoch: 635, loss =  0.0695, R2 = 0.9304844737052917\n",
      "test: loss =  0.0695, R2 = 0.9304938316345215\n",
      "epoch: 636, loss =  0.0695, R2 = 0.9304938316345215\n",
      "test: loss =  0.0695, R2 = 0.9305031299591064\n",
      "epoch: 637, loss =  0.0695, R2 = 0.9305031299591064\n",
      "test: loss =  0.0695, R2 = 0.9305123686790466\n",
      "epoch: 638, loss =  0.0695, R2 = 0.9305123686790466\n",
      "test: loss =  0.0695, R2 = 0.9305214881896973\n",
      "epoch: 639, loss =  0.0695, R2 = 0.9305214881896973\n",
      "test: loss =  0.0695, R2 = 0.9305305480957031\n",
      "epoch: 640, loss =  0.0695, R2 = 0.9305305480957031\n",
      "test: loss =  0.0694, R2 = 0.9305395483970642\n",
      "epoch: 641, loss =  0.0694, R2 = 0.9305395483970642\n",
      "test: loss =  0.0694, R2 = 0.9305485486984253\n",
      "epoch: 642, loss =  0.0694, R2 = 0.9305485486984253\n",
      "test: loss =  0.0694, R2 = 0.9305576086044312\n",
      "epoch: 643, loss =  0.0694, R2 = 0.9305576086044312\n",
      "test: loss =  0.0694, R2 = 0.930566668510437\n",
      "epoch: 644, loss =  0.0694, R2 = 0.930566668510437\n",
      "test: loss =  0.0694, R2 = 0.9305758476257324\n",
      "epoch: 645, loss =  0.0694, R2 = 0.9305758476257324\n",
      "test: loss =  0.0694, R2 = 0.9305849671363831\n",
      "epoch: 646, loss =  0.0694, R2 = 0.9305849671363831\n",
      "test: loss =  0.0694, R2 = 0.9305942058563232\n",
      "epoch: 647, loss =  0.0694, R2 = 0.9305942058563232\n",
      "test: loss =  0.0694, R2 = 0.9306033849716187\n",
      "epoch: 648, loss =  0.0694, R2 = 0.9306033849716187\n",
      "test: loss =  0.0694, R2 = 0.9306125640869141\n",
      "epoch: 649, loss =  0.0694, R2 = 0.9306125640869141\n",
      "test: loss =  0.0694, R2 = 0.9306217432022095\n",
      "epoch: 650, loss =  0.0694, R2 = 0.9306217432022095\n",
      "test: loss =  0.0693, R2 = 0.9306309223175049\n",
      "epoch: 651, loss =  0.0693, R2 = 0.9306309223175049\n",
      "test: loss =  0.0693, R2 = 0.9306401014328003\n",
      "epoch: 652, loss =  0.0693, R2 = 0.9306401014328003\n",
      "test: loss =  0.0693, R2 = 0.9306492209434509\n",
      "epoch: 653, loss =  0.0693, R2 = 0.9306492209434509\n",
      "test: loss =  0.0693, R2 = 0.9306583404541016\n",
      "epoch: 654, loss =  0.0693, R2 = 0.9306583404541016\n",
      "test: loss =  0.0693, R2 = 0.9306674599647522\n",
      "epoch: 655, loss =  0.0693, R2 = 0.9306674599647522\n",
      "test: loss =  0.0693, R2 = 0.9306765794754028\n",
      "epoch: 656, loss =  0.0693, R2 = 0.9306765794754028\n",
      "test: loss =  0.0693, R2 = 0.9306856989860535\n",
      "epoch: 657, loss =  0.0693, R2 = 0.9306856989860535\n",
      "test: loss =  0.0693, R2 = 0.9306948184967041\n",
      "epoch: 658, loss =  0.0693, R2 = 0.9306948184967041\n",
      "test: loss =  0.0693, R2 = 0.9307039380073547\n",
      "epoch: 659, loss =  0.0693, R2 = 0.9307039380073547\n",
      "test: loss =  0.0693, R2 = 0.9307130575180054\n",
      "epoch: 660, loss =  0.0693, R2 = 0.9307130575180054\n",
      "test: loss =  0.0693, R2 = 0.930722177028656\n",
      "epoch: 661, loss =  0.0693, R2 = 0.930722177028656\n",
      "test: loss =  0.0692, R2 = 0.9307312965393066\n",
      "epoch: 662, loss =  0.0692, R2 = 0.9307312965393066\n",
      "test: loss =  0.0692, R2 = 0.9307403564453125\n",
      "epoch: 663, loss =  0.0692, R2 = 0.9307403564453125\n",
      "test: loss =  0.0692, R2 = 0.9307493567466736\n",
      "epoch: 664, loss =  0.0692, R2 = 0.9307493567466736\n",
      "test: loss =  0.0692, R2 = 0.9307582974433899\n",
      "epoch: 665, loss =  0.0692, R2 = 0.9307582974433899\n",
      "test: loss =  0.0692, R2 = 0.9307670593261719\n",
      "epoch: 666, loss =  0.0692, R2 = 0.9307670593261719\n",
      "test: loss =  0.0692, R2 = 0.9307756423950195\n",
      "epoch: 667, loss =  0.0692, R2 = 0.9307756423950195\n",
      "test: loss =  0.0692, R2 = 0.9307839274406433\n",
      "epoch: 668, loss =  0.0692, R2 = 0.9307839274406433\n",
      "test: loss =  0.0692, R2 = 0.9307917356491089\n",
      "epoch: 669, loss =  0.0692, R2 = 0.9307917356491089\n",
      "test: loss =  0.0692, R2 = 0.9307987689971924\n",
      "epoch: 670, loss =  0.0692, R2 = 0.9307987689971924\n",
      "test: loss =  0.0692, R2 = 0.9308046102523804\n",
      "epoch: 671, loss =  0.0692, R2 = 0.9308046102523804\n",
      "test: loss =  0.0692, R2 = 0.930808424949646\n",
      "epoch: 672, loss =  0.0692, R2 = 0.930808424949646\n",
      "test: loss =  0.0692, R2 = 0.9308092594146729\n",
      "epoch: 673, loss =  0.0692, R2 = 0.9308092594146729\n",
      "test: loss =  0.0692, R2 = 0.930804967880249\n",
      "epoch: 674, loss =  0.0692, R2 = 0.930804967880249\n",
      "test: loss =  0.0692, R2 = 0.9307925701141357\n",
      "epoch: 675, loss =  0.0692, R2 = 0.9307925701141357\n",
      "test: loss =  0.0692, R2 = 0.9307669401168823\n",
      "epoch: 676, loss =  0.0692, R2 = 0.9307669401168823\n",
      "test: loss =  0.0693, R2 = 0.9307196140289307\n",
      "epoch: 677, loss =  0.0693, R2 = 0.9307196140289307\n",
      "test: loss =  0.0693, R2 = 0.9306371212005615\n",
      "epoch: 678, loss =  0.0693, R2 = 0.9306371212005615\n",
      "test: loss =  0.0695, R2 = 0.9304978251457214\n",
      "epoch: 679, loss =  0.0695, R2 = 0.9304978251457214\n",
      "test: loss =  0.0697, R2 = 0.9302689433097839\n",
      "epoch: 680, loss =  0.0697, R2 = 0.9302689433097839\n",
      "test: loss =  0.0701, R2 = 0.9299042820930481\n",
      "epoch: 681, loss =  0.0701, R2 = 0.9299042820930481\n",
      "test: loss =  0.0706, R2 = 0.9293491244316101\n",
      "epoch: 682, loss =  0.0706, R2 = 0.9293491244316101\n",
      "test: loss =  0.0714, R2 = 0.9285650253295898\n",
      "epoch: 683, loss =  0.0714, R2 = 0.9285650253295898\n",
      "test: loss =  0.0724, R2 = 0.9275938272476196\n",
      "epoch: 684, loss =  0.0724, R2 = 0.9275938272476196\n",
      "test: loss =  0.0733, R2 = 0.9266606569290161\n",
      "epoch: 685, loss =  0.0733, R2 = 0.9266606569290161\n",
      "test: loss =  0.0738, R2 = 0.9262193441390991\n",
      "epoch: 686, loss =  0.0738, R2 = 0.9262193441390991\n",
      "test: loss =  0.0732, R2 = 0.926735520362854\n",
      "epoch: 687, loss =  0.0732, R2 = 0.926735520362854\n",
      "test: loss =  0.0718, R2 = 0.9281608462333679\n",
      "epoch: 688, loss =  0.0718, R2 = 0.9281608462333679\n",
      "test: loss =  0.0703, R2 = 0.9296919703483582\n",
      "epoch: 689, loss =  0.0703, R2 = 0.9296919703483582\n",
      "test: loss =  0.0696, R2 = 0.9303609728813171\n",
      "epoch: 690, loss =  0.0696, R2 = 0.9303609728813171\n",
      "test: loss =  0.0701, R2 = 0.929920494556427\n",
      "epoch: 691, loss =  0.0701, R2 = 0.929920494556427\n",
      "test: loss =  0.0710, R2 = 0.9290096759796143\n",
      "epoch: 692, loss =  0.0710, R2 = 0.9290096759796143\n",
      "test: loss =  0.0714, R2 = 0.9285691976547241\n",
      "epoch: 693, loss =  0.0714, R2 = 0.9285691976547241\n",
      "test: loss =  0.0709, R2 = 0.9290597438812256\n",
      "epoch: 694, loss =  0.0709, R2 = 0.9290597438812256\n",
      "test: loss =  0.0699, R2 = 0.9300556182861328\n",
      "epoch: 695, loss =  0.0699, R2 = 0.9300556182861328\n",
      "test: loss =  0.0693, R2 = 0.9306963086128235\n",
      "epoch: 696, loss =  0.0693, R2 = 0.9306963086128235\n",
      "test: loss =  0.0694, R2 = 0.9305921196937561\n",
      "epoch: 697, loss =  0.0694, R2 = 0.9305921196937561\n",
      "test: loss =  0.0698, R2 = 0.930143415927887\n",
      "epoch: 698, loss =  0.0698, R2 = 0.930143415927887\n",
      "test: loss =  0.0700, R2 = 0.930001974105835\n",
      "epoch: 699, loss =  0.0700, R2 = 0.930001974105835\n",
      "test: loss =  0.0696, R2 = 0.93037348985672\n",
      "epoch: 700, loss =  0.0696, R2 = 0.93037348985672\n",
      "test: loss =  0.0691, R2 = 0.930873692035675\n",
      "epoch: 701, loss =  0.0691, R2 = 0.930873692035675\n",
      "test: loss =  0.0690, R2 = 0.9310283064842224\n",
      "epoch: 702, loss =  0.0690, R2 = 0.9310283064842224\n",
      "test: loss =  0.0692, R2 = 0.9307964444160461\n",
      "epoch: 703, loss =  0.0692, R2 = 0.9307964444160461\n",
      "test: loss =  0.0694, R2 = 0.9305352568626404\n",
      "epoch: 704, loss =  0.0694, R2 = 0.9305352568626404\n",
      "test: loss =  0.0694, R2 = 0.9305538535118103\n",
      "epoch: 705, loss =  0.0694, R2 = 0.9305538535118103\n",
      "test: loss =  0.0692, R2 = 0.9308007955551147\n",
      "epoch: 706, loss =  0.0692, R2 = 0.9308007955551147\n",
      "test: loss =  0.0690, R2 = 0.9309905767440796\n",
      "epoch: 707, loss =  0.0690, R2 = 0.9309905767440796\n",
      "test: loss =  0.0690, R2 = 0.9309578537940979\n",
      "epoch: 708, loss =  0.0690, R2 = 0.9309578537940979\n",
      "test: loss =  0.0692, R2 = 0.9308131337165833\n",
      "epoch: 709, loss =  0.0692, R2 = 0.9308131337165833\n",
      "test: loss =  0.0692, R2 = 0.9307680130004883\n",
      "epoch: 710, loss =  0.0692, R2 = 0.9307680130004883\n",
      "test: loss =  0.0691, R2 = 0.9308873414993286\n",
      "epoch: 711, loss =  0.0691, R2 = 0.9308873414993286\n",
      "test: loss =  0.0689, R2 = 0.9310471415519714\n",
      "epoch: 712, loss =  0.0689, R2 = 0.9310471415519714\n",
      "test: loss =  0.0689, R2 = 0.9311050772666931\n",
      "epoch: 713, loss =  0.0689, R2 = 0.9311050772666931\n",
      "test: loss =  0.0689, R2 = 0.931057333946228\n",
      "epoch: 714, loss =  0.0689, R2 = 0.931057333946228\n",
      "test: loss =  0.0690, R2 = 0.9310129284858704\n",
      "epoch: 715, loss =  0.0690, R2 = 0.9310129284858704\n",
      "test: loss =  0.0689, R2 = 0.9310520887374878\n",
      "epoch: 716, loss =  0.0689, R2 = 0.9310520887374878\n",
      "test: loss =  0.0688, R2 = 0.9311450123786926\n",
      "epoch: 717, loss =  0.0688, R2 = 0.9311450123786926\n",
      "test: loss =  0.0688, R2 = 0.9312060475349426\n",
      "epoch: 718, loss =  0.0688, R2 = 0.9312060475349426\n",
      "test: loss =  0.0688, R2 = 0.9311975240707397\n",
      "epoch: 719, loss =  0.0688, R2 = 0.9311975240707397\n",
      "test: loss =  0.0688, R2 = 0.9311612248420715\n",
      "epoch: 720, loss =  0.0688, R2 = 0.9311612248420715\n",
      "test: loss =  0.0688, R2 = 0.9311583638191223\n",
      "epoch: 721, loss =  0.0688, R2 = 0.9311583638191223\n",
      "test: loss =  0.0688, R2 = 0.9312012195587158\n",
      "epoch: 722, loss =  0.0688, R2 = 0.9312012195587158\n",
      "test: loss =  0.0687, R2 = 0.9312496185302734\n",
      "epoch: 723, loss =  0.0687, R2 = 0.9312496185302734\n",
      "test: loss =  0.0687, R2 = 0.9312638640403748\n",
      "epoch: 724, loss =  0.0687, R2 = 0.9312638640403748\n",
      "test: loss =  0.0687, R2 = 0.9312463998794556\n",
      "epoch: 725, loss =  0.0687, R2 = 0.9312463998794556\n",
      "test: loss =  0.0687, R2 = 0.9312317371368408\n",
      "epoch: 726, loss =  0.0687, R2 = 0.9312317371368408\n",
      "test: loss =  0.0687, R2 = 0.9312445521354675\n",
      "epoch: 727, loss =  0.0687, R2 = 0.9312445521354675\n",
      "test: loss =  0.0687, R2 = 0.9312763810157776\n",
      "epoch: 728, loss =  0.0687, R2 = 0.9312763810157776\n",
      "test: loss =  0.0687, R2 = 0.931300163269043\n",
      "epoch: 729, loss =  0.0687, R2 = 0.931300163269043\n",
      "test: loss =  0.0687, R2 = 0.9313012957572937\n",
      "epoch: 730, loss =  0.0687, R2 = 0.9313012957572937\n",
      "test: loss =  0.0687, R2 = 0.9312899112701416\n",
      "epoch: 731, loss =  0.0687, R2 = 0.9312899112701416\n",
      "test: loss =  0.0687, R2 = 0.9312860369682312\n",
      "epoch: 732, loss =  0.0687, R2 = 0.9312860369682312\n",
      "test: loss =  0.0687, R2 = 0.9312977194786072\n",
      "epoch: 733, loss =  0.0687, R2 = 0.9312977194786072\n",
      "test: loss =  0.0687, R2 = 0.9313148260116577\n",
      "epoch: 734, loss =  0.0687, R2 = 0.9313148260116577\n",
      "test: loss =  0.0687, R2 = 0.9313218593597412\n",
      "epoch: 735, loss =  0.0687, R2 = 0.9313218593597412\n",
      "test: loss =  0.0687, R2 = 0.9313133358955383\n",
      "epoch: 736, loss =  0.0687, R2 = 0.9313133358955383\n",
      "test: loss =  0.0687, R2 = 0.9312963485717773\n",
      "epoch: 737, loss =  0.0687, R2 = 0.9312963485717773\n",
      "test: loss =  0.0687, R2 = 0.9312806129455566\n",
      "epoch: 738, loss =  0.0687, R2 = 0.9312806129455566\n",
      "test: loss =  0.0687, R2 = 0.9312674403190613\n",
      "epoch: 739, loss =  0.0687, R2 = 0.9312674403190613\n",
      "test: loss =  0.0687, R2 = 0.9312478303909302\n",
      "epoch: 740, loss =  0.0687, R2 = 0.9312478303909302\n",
      "test: loss =  0.0688, R2 = 0.9312101602554321\n",
      "epoch: 741, loss =  0.0688, R2 = 0.9312101602554321\n",
      "test: loss =  0.0688, R2 = 0.9311474561691284\n",
      "epoch: 742, loss =  0.0688, R2 = 0.9311474561691284\n",
      "test: loss =  0.0689, R2 = 0.9310579299926758\n",
      "epoch: 743, loss =  0.0689, R2 = 0.9310579299926758\n",
      "test: loss =  0.0690, R2 = 0.9309391975402832\n",
      "epoch: 744, loss =  0.0690, R2 = 0.9309391975402832\n",
      "test: loss =  0.0692, R2 = 0.9307820200920105\n",
      "epoch: 745, loss =  0.0692, R2 = 0.9307820200920105\n",
      "test: loss =  0.0694, R2 = 0.930570662021637\n",
      "epoch: 746, loss =  0.0694, R2 = 0.930570662021637\n",
      "test: loss =  0.0697, R2 = 0.9302862286567688\n",
      "epoch: 747, loss =  0.0697, R2 = 0.9302862286567688\n",
      "test: loss =  0.0701, R2 = 0.9299175143241882\n",
      "epoch: 748, loss =  0.0701, R2 = 0.9299175143241882\n",
      "test: loss =  0.0705, R2 = 0.9294645190238953\n",
      "epoch: 749, loss =  0.0705, R2 = 0.9294645190238953\n",
      "test: loss =  0.0710, R2 = 0.9289543628692627\n",
      "epoch: 750, loss =  0.0710, R2 = 0.9289543628692627\n",
      "test: loss =  0.0715, R2 = 0.9284423589706421\n",
      "epoch: 751, loss =  0.0715, R2 = 0.9284423589706421\n",
      "test: loss =  0.0719, R2 = 0.9280351400375366\n",
      "epoch: 752, loss =  0.0719, R2 = 0.9280351400375366\n",
      "test: loss =  0.0721, R2 = 0.9278641939163208\n",
      "epoch: 753, loss =  0.0721, R2 = 0.9278641939163208\n",
      "test: loss =  0.0719, R2 = 0.9280645847320557\n",
      "epoch: 754, loss =  0.0719, R2 = 0.9280645847320557\n",
      "test: loss =  0.0713, R2 = 0.9286739826202393\n",
      "epoch: 755, loss =  0.0713, R2 = 0.9286739826202393\n",
      "test: loss =  0.0704, R2 = 0.9295858144760132\n",
      "epoch: 756, loss =  0.0704, R2 = 0.9295858144760132\n",
      "test: loss =  0.0694, R2 = 0.9305422306060791\n",
      "epoch: 757, loss =  0.0694, R2 = 0.9305422306060791\n",
      "test: loss =  0.0687, R2 = 0.9312576055526733\n",
      "epoch: 758, loss =  0.0687, R2 = 0.9312576055526733\n",
      "test: loss =  0.0684, R2 = 0.9315564632415771\n",
      "epoch: 759, loss =  0.0684, R2 = 0.9315564632415771\n",
      "test: loss =  0.0685, R2 = 0.9314485192298889\n",
      "epoch: 760, loss =  0.0685, R2 = 0.9314485192298889\n",
      "test: loss =  0.0689, R2 = 0.9310957789421082\n",
      "epoch: 761, loss =  0.0689, R2 = 0.9310957789421082\n",
      "test: loss =  0.0693, R2 = 0.9307186007499695\n",
      "epoch: 762, loss =  0.0693, R2 = 0.9307186007499695\n",
      "test: loss =  0.0695, R2 = 0.9305030107498169\n",
      "epoch: 763, loss =  0.0695, R2 = 0.9305030107498169\n",
      "test: loss =  0.0694, R2 = 0.9305355548858643\n",
      "epoch: 764, loss =  0.0694, R2 = 0.9305355548858643\n",
      "test: loss =  0.0692, R2 = 0.9307874441146851\n",
      "epoch: 765, loss =  0.0692, R2 = 0.9307874441146851\n",
      "test: loss =  0.0688, R2 = 0.9311397075653076\n",
      "epoch: 766, loss =  0.0688, R2 = 0.9311397075653076\n",
      "test: loss =  0.0685, R2 = 0.9314494729042053\n",
      "epoch: 767, loss =  0.0685, R2 = 0.9314494729042053\n",
      "test: loss =  0.0684, R2 = 0.9316160082817078\n",
      "epoch: 768, loss =  0.0684, R2 = 0.9316160082817078\n",
      "test: loss =  0.0684, R2 = 0.9316180348396301\n",
      "epoch: 769, loss =  0.0684, R2 = 0.9316180348396301\n",
      "test: loss =  0.0685, R2 = 0.9315060377120972\n",
      "epoch: 770, loss =  0.0685, R2 = 0.9315060377120972\n",
      "test: loss =  0.0686, R2 = 0.9313657879829407\n",
      "epoch: 771, loss =  0.0686, R2 = 0.9313657879829407\n",
      "test: loss =  0.0687, R2 = 0.9312753677368164\n",
      "epoch: 772, loss =  0.0687, R2 = 0.9312753677368164\n",
      "test: loss =  0.0687, R2 = 0.9312750697135925\n",
      "epoch: 773, loss =  0.0687, R2 = 0.9312750697135925\n",
      "test: loss =  0.0686, R2 = 0.9313598871231079\n",
      "epoch: 774, loss =  0.0686, R2 = 0.9313598871231079\n",
      "test: loss =  0.0685, R2 = 0.9314901232719421\n",
      "epoch: 775, loss =  0.0685, R2 = 0.9314901232719421\n",
      "test: loss =  0.0684, R2 = 0.9316158890724182\n",
      "epoch: 776, loss =  0.0684, R2 = 0.9316158890724182\n",
      "test: loss =  0.0683, R2 = 0.9316990971565247\n",
      "epoch: 777, loss =  0.0683, R2 = 0.9316990971565247\n",
      "test: loss =  0.0683, R2 = 0.9317262172698975\n",
      "epoch: 778, loss =  0.0683, R2 = 0.9317262172698975\n",
      "test: loss =  0.0683, R2 = 0.931707501411438\n",
      "epoch: 779, loss =  0.0683, R2 = 0.931707501411438\n",
      "test: loss =  0.0683, R2 = 0.9316672086715698\n",
      "epoch: 780, loss =  0.0683, R2 = 0.9316672086715698\n",
      "test: loss =  0.0683, R2 = 0.9316312670707703\n",
      "epoch: 781, loss =  0.0683, R2 = 0.9316312670707703\n",
      "test: loss =  0.0684, R2 = 0.9316179156303406\n",
      "epoch: 782, loss =  0.0684, R2 = 0.9316179156303406\n",
      "test: loss =  0.0683, R2 = 0.9316333532333374\n",
      "epoch: 783, loss =  0.0683, R2 = 0.9316333532333374\n",
      "test: loss =  0.0683, R2 = 0.9316722750663757\n",
      "epoch: 784, loss =  0.0683, R2 = 0.9316722750663757\n",
      "test: loss =  0.0683, R2 = 0.9317225217819214\n",
      "epoch: 785, loss =  0.0683, R2 = 0.9317225217819214\n",
      "test: loss =  0.0682, R2 = 0.9317704439163208\n",
      "epoch: 786, loss =  0.0682, R2 = 0.9317704439163208\n",
      "test: loss =  0.0682, R2 = 0.9318060278892517\n",
      "epoch: 787, loss =  0.0682, R2 = 0.9318060278892517\n",
      "test: loss =  0.0682, R2 = 0.9318253397941589\n",
      "epoch: 788, loss =  0.0682, R2 = 0.9318253397941589\n",
      "test: loss =  0.0682, R2 = 0.931830108165741\n",
      "epoch: 789, loss =  0.0682, R2 = 0.931830108165741\n",
      "test: loss =  0.0682, R2 = 0.9318259954452515\n",
      "epoch: 790, loss =  0.0682, R2 = 0.9318259954452515\n",
      "test: loss =  0.0682, R2 = 0.9318195581436157\n",
      "epoch: 791, loss =  0.0682, R2 = 0.9318195581436157\n",
      "test: loss =  0.0682, R2 = 0.9318165183067322\n",
      "epoch: 792, loss =  0.0682, R2 = 0.9318165183067322\n",
      "test: loss =  0.0682, R2 = 0.9318201541900635\n",
      "epoch: 793, loss =  0.0682, R2 = 0.9318201541900635\n",
      "test: loss =  0.0681, R2 = 0.9318312406539917\n",
      "epoch: 794, loss =  0.0681, R2 = 0.9318312406539917\n",
      "test: loss =  0.0681, R2 = 0.9318484663963318\n",
      "epoch: 795, loss =  0.0681, R2 = 0.9318484663963318\n",
      "test: loss =  0.0681, R2 = 0.9318692684173584\n",
      "epoch: 796, loss =  0.0681, R2 = 0.9318692684173584\n",
      "test: loss =  0.0681, R2 = 0.9318907260894775\n",
      "epoch: 797, loss =  0.0681, R2 = 0.9318907260894775\n",
      "test: loss =  0.0681, R2 = 0.9319102168083191\n",
      "epoch: 798, loss =  0.0681, R2 = 0.9319102168083191\n",
      "test: loss =  0.0681, R2 = 0.9319263100624084\n",
      "epoch: 799, loss =  0.0681, R2 = 0.9319263100624084\n",
      "test: loss =  0.0680, R2 = 0.9319383502006531\n",
      "epoch: 800, loss =  0.0680, R2 = 0.9319383502006531\n",
      "test: loss =  0.0680, R2 = 0.9319469332695007\n",
      "epoch: 801, loss =  0.0680, R2 = 0.9319469332695007\n",
      "test: loss =  0.0680, R2 = 0.931952953338623\n",
      "epoch: 802, loss =  0.0680, R2 = 0.931952953338623\n",
      "test: loss =  0.0680, R2 = 0.9319577217102051\n",
      "epoch: 803, loss =  0.0680, R2 = 0.9319577217102051\n",
      "test: loss =  0.0680, R2 = 0.9319623112678528\n",
      "epoch: 804, loss =  0.0680, R2 = 0.9319623112678528\n",
      "test: loss =  0.0680, R2 = 0.9319677948951721\n",
      "epoch: 805, loss =  0.0680, R2 = 0.9319677948951721\n",
      "test: loss =  0.0680, R2 = 0.9319745302200317\n",
      "epoch: 806, loss =  0.0680, R2 = 0.9319745302200317\n",
      "test: loss =  0.0680, R2 = 0.9319828152656555\n",
      "epoch: 807, loss =  0.0680, R2 = 0.9319828152656555\n",
      "test: loss =  0.0680, R2 = 0.9319925904273987\n",
      "epoch: 808, loss =  0.0680, R2 = 0.9319925904273987\n",
      "test: loss =  0.0680, R2 = 0.9320034980773926\n",
      "epoch: 809, loss =  0.0680, R2 = 0.9320034980773926\n",
      "test: loss =  0.0680, R2 = 0.9320151209831238\n",
      "epoch: 810, loss =  0.0680, R2 = 0.9320151209831238\n",
      "test: loss =  0.0680, R2 = 0.9320271015167236\n",
      "epoch: 811, loss =  0.0680, R2 = 0.9320271015167236\n",
      "test: loss =  0.0679, R2 = 0.9320390224456787\n",
      "epoch: 812, loss =  0.0679, R2 = 0.9320390224456787\n",
      "test: loss =  0.0679, R2 = 0.9320505261421204\n",
      "epoch: 813, loss =  0.0679, R2 = 0.9320505261421204\n",
      "test: loss =  0.0679, R2 = 0.9320616126060486\n",
      "epoch: 814, loss =  0.0679, R2 = 0.9320616126060486\n",
      "test: loss =  0.0679, R2 = 0.9320720434188843\n",
      "epoch: 815, loss =  0.0679, R2 = 0.9320720434188843\n",
      "test: loss =  0.0679, R2 = 0.932081937789917\n",
      "epoch: 816, loss =  0.0679, R2 = 0.932081937789917\n",
      "test: loss =  0.0679, R2 = 0.9320912957191467\n",
      "epoch: 817, loss =  0.0679, R2 = 0.9320912957191467\n",
      "test: loss =  0.0679, R2 = 0.932100236415863\n",
      "epoch: 818, loss =  0.0679, R2 = 0.932100236415863\n",
      "test: loss =  0.0679, R2 = 0.9321088194847107\n",
      "epoch: 819, loss =  0.0679, R2 = 0.9321088194847107\n",
      "test: loss =  0.0679, R2 = 0.9321171641349792\n",
      "epoch: 820, loss =  0.0679, R2 = 0.9321171641349792\n",
      "test: loss =  0.0679, R2 = 0.9321253895759583\n",
      "epoch: 821, loss =  0.0679, R2 = 0.9321253895759583\n",
      "test: loss =  0.0678, R2 = 0.9321334958076477\n",
      "epoch: 822, loss =  0.0678, R2 = 0.9321334958076477\n",
      "test: loss =  0.0678, R2 = 0.9321415424346924\n",
      "epoch: 823, loss =  0.0678, R2 = 0.9321415424346924\n",
      "test: loss =  0.0678, R2 = 0.9321496486663818\n",
      "epoch: 824, loss =  0.0678, R2 = 0.9321496486663818\n",
      "test: loss =  0.0678, R2 = 0.9321576356887817\n",
      "epoch: 825, loss =  0.0678, R2 = 0.9321576356887817\n",
      "test: loss =  0.0678, R2 = 0.9321656823158264\n",
      "epoch: 826, loss =  0.0678, R2 = 0.9321656823158264\n",
      "test: loss =  0.0678, R2 = 0.9321736097335815\n",
      "epoch: 827, loss =  0.0678, R2 = 0.9321736097335815\n",
      "test: loss =  0.0678, R2 = 0.9321814179420471\n",
      "epoch: 828, loss =  0.0678, R2 = 0.9321814179420471\n",
      "test: loss =  0.0678, R2 = 0.9321891069412231\n",
      "epoch: 829, loss =  0.0678, R2 = 0.9321891069412231\n",
      "test: loss =  0.0678, R2 = 0.9321966171264648\n",
      "epoch: 830, loss =  0.0678, R2 = 0.9321966171264648\n",
      "test: loss =  0.0678, R2 = 0.9322037696838379\n",
      "epoch: 831, loss =  0.0678, R2 = 0.9322037696838379\n",
      "test: loss =  0.0678, R2 = 0.9322104454040527\n",
      "epoch: 832, loss =  0.0678, R2 = 0.9322104454040527\n",
      "test: loss =  0.0678, R2 = 0.9322165250778198\n",
      "epoch: 833, loss =  0.0678, R2 = 0.9322165250778198\n",
      "test: loss =  0.0678, R2 = 0.9322217702865601\n",
      "epoch: 834, loss =  0.0678, R2 = 0.9322217702865601\n",
      "test: loss =  0.0678, R2 = 0.9322258234024048\n",
      "epoch: 835, loss =  0.0678, R2 = 0.9322258234024048\n",
      "test: loss =  0.0678, R2 = 0.9322282075881958\n",
      "epoch: 836, loss =  0.0678, R2 = 0.9322282075881958\n",
      "test: loss =  0.0678, R2 = 0.9322283267974854\n",
      "epoch: 837, loss =  0.0678, R2 = 0.9322283267974854\n",
      "test: loss =  0.0678, R2 = 0.9322251677513123\n",
      "epoch: 838, loss =  0.0678, R2 = 0.9322251677513123\n",
      "test: loss =  0.0678, R2 = 0.9322174787521362\n",
      "epoch: 839, loss =  0.0678, R2 = 0.9322174787521362\n",
      "test: loss =  0.0678, R2 = 0.9322032332420349\n",
      "epoch: 840, loss =  0.0678, R2 = 0.9322032332420349\n",
      "test: loss =  0.0678, R2 = 0.9321795701980591\n",
      "epoch: 841, loss =  0.0678, R2 = 0.9321795701980591\n",
      "test: loss =  0.0678, R2 = 0.932142436504364\n",
      "epoch: 842, loss =  0.0678, R2 = 0.932142436504364\n",
      "test: loss =  0.0679, R2 = 0.9320858120918274\n",
      "epoch: 843, loss =  0.0679, R2 = 0.9320858120918274\n",
      "test: loss =  0.0680, R2 = 0.932000994682312\n",
      "epoch: 844, loss =  0.0680, R2 = 0.932000994682312\n",
      "test: loss =  0.0681, R2 = 0.9318755269050598\n",
      "epoch: 845, loss =  0.0681, R2 = 0.9318755269050598\n",
      "test: loss =  0.0683, R2 = 0.9316922426223755\n",
      "epoch: 846, loss =  0.0683, R2 = 0.9316922426223755\n",
      "test: loss =  0.0686, R2 = 0.9314277172088623\n",
      "epoch: 847, loss =  0.0686, R2 = 0.9314277172088623\n",
      "test: loss =  0.0689, R2 = 0.9310532212257385\n",
      "epoch: 848, loss =  0.0689, R2 = 0.9310532212257385\n",
      "test: loss =  0.0694, R2 = 0.9305367469787598\n",
      "epoch: 849, loss =  0.0694, R2 = 0.9305367469787598\n",
      "test: loss =  0.0701, R2 = 0.9298552870750427\n",
      "epoch: 850, loss =  0.0701, R2 = 0.9298552870750427\n",
      "test: loss =  0.0710, R2 = 0.9290145635604858\n",
      "epoch: 851, loss =  0.0710, R2 = 0.9290145635604858\n",
      "test: loss =  0.0719, R2 = 0.9280913472175598\n",
      "epoch: 852, loss =  0.0719, R2 = 0.9280913472175598\n",
      "test: loss =  0.0727, R2 = 0.9272680878639221\n",
      "epoch: 853, loss =  0.0727, R2 = 0.9272680878639221\n",
      "test: loss =  0.0731, R2 = 0.9268491864204407\n",
      "epoch: 854, loss =  0.0731, R2 = 0.9268491864204407\n",
      "test: loss =  0.0728, R2 = 0.927145779132843\n",
      "epoch: 855, loss =  0.0728, R2 = 0.927145779132843\n",
      "test: loss =  0.0717, R2 = 0.9282737374305725\n",
      "epoch: 856, loss =  0.0717, R2 = 0.9282737374305725\n",
      "test: loss =  0.0701, R2 = 0.9299286007881165\n",
      "epoch: 857, loss =  0.0701, R2 = 0.9299286007881165\n",
      "test: loss =  0.0685, R2 = 0.931472659111023\n",
      "epoch: 858, loss =  0.0685, R2 = 0.931472659111023\n",
      "test: loss =  0.0677, R2 = 0.9322982430458069\n",
      "epoch: 859, loss =  0.0677, R2 = 0.9322982430458069\n",
      "test: loss =  0.0678, R2 = 0.9322178363800049\n",
      "epoch: 860, loss =  0.0678, R2 = 0.9322178363800049\n",
      "test: loss =  0.0685, R2 = 0.9315293431282043\n",
      "epoch: 861, loss =  0.0685, R2 = 0.9315293431282043\n",
      "test: loss =  0.0692, R2 = 0.9307886958122253\n",
      "epoch: 862, loss =  0.0692, R2 = 0.9307886958122253\n",
      "test: loss =  0.0695, R2 = 0.9304889440536499\n",
      "epoch: 863, loss =  0.0695, R2 = 0.9304889440536499\n",
      "test: loss =  0.0692, R2 = 0.9308071136474609\n",
      "epoch: 864, loss =  0.0692, R2 = 0.9308071136474609\n",
      "test: loss =  0.0685, R2 = 0.931530237197876\n",
      "epoch: 865, loss =  0.0685, R2 = 0.931530237197876\n",
      "test: loss =  0.0678, R2 = 0.9322181940078735\n",
      "epoch: 866, loss =  0.0678, R2 = 0.9322181940078735\n",
      "test: loss =  0.0675, R2 = 0.9325129985809326\n",
      "epoch: 867, loss =  0.0675, R2 = 0.9325129985809326\n",
      "test: loss =  0.0676, R2 = 0.9323563575744629\n",
      "epoch: 868, loss =  0.0676, R2 = 0.9323563575744629\n",
      "test: loss =  0.0680, R2 = 0.9319729208946228\n",
      "epoch: 869, loss =  0.0680, R2 = 0.9319729208946228\n",
      "test: loss =  0.0683, R2 = 0.9316791296005249\n",
      "epoch: 870, loss =  0.0683, R2 = 0.9316791296005249\n",
      "test: loss =  0.0683, R2 = 0.9316762089729309\n",
      "epoch: 871, loss =  0.0683, R2 = 0.9316762089729309\n",
      "test: loss =  0.0680, R2 = 0.9319484233856201\n",
      "epoch: 872, loss =  0.0680, R2 = 0.9319484233856201\n",
      "test: loss =  0.0677, R2 = 0.9323084354400635\n",
      "epoch: 873, loss =  0.0677, R2 = 0.9323084354400635\n",
      "test: loss =  0.0674, R2 = 0.932545006275177\n",
      "epoch: 874, loss =  0.0674, R2 = 0.932545006275177\n",
      "test: loss =  0.0674, R2 = 0.9325617551803589\n",
      "epoch: 875, loss =  0.0674, R2 = 0.9325617551803589\n",
      "test: loss =  0.0676, R2 = 0.9324155449867249\n",
      "epoch: 876, loss =  0.0676, R2 = 0.9324155449867249\n",
      "test: loss =  0.0677, R2 = 0.9322497844696045\n",
      "epoch: 877, loss =  0.0677, R2 = 0.9322497844696045\n",
      "test: loss =  0.0678, R2 = 0.9321897625923157\n",
      "epoch: 878, loss =  0.0678, R2 = 0.9321897625923157\n",
      "test: loss =  0.0677, R2 = 0.9322709441184998\n",
      "epoch: 879, loss =  0.0677, R2 = 0.9322709441184998\n",
      "test: loss =  0.0675, R2 = 0.9324353337287903\n",
      "epoch: 880, loss =  0.0675, R2 = 0.9324353337287903\n",
      "test: loss =  0.0674, R2 = 0.9325847625732422\n",
      "epoch: 881, loss =  0.0674, R2 = 0.9325847625732422\n",
      "test: loss =  0.0673, R2 = 0.9326481223106384\n",
      "epoch: 882, loss =  0.0673, R2 = 0.9326481223106384\n",
      "test: loss =  0.0674, R2 = 0.9326183795928955\n",
      "epoch: 883, loss =  0.0674, R2 = 0.9326183795928955\n",
      "test: loss =  0.0674, R2 = 0.9325435161590576\n",
      "epoch: 884, loss =  0.0674, R2 = 0.9325435161590576\n",
      "test: loss =  0.0675, R2 = 0.9324869513511658\n",
      "epoch: 885, loss =  0.0675, R2 = 0.9324869513511658\n",
      "test: loss =  0.0675, R2 = 0.9324883222579956\n",
      "epoch: 886, loss =  0.0675, R2 = 0.9324883222579956\n",
      "test: loss =  0.0674, R2 = 0.9325462579727173\n",
      "epoch: 887, loss =  0.0674, R2 = 0.9325462579727173\n",
      "test: loss =  0.0674, R2 = 0.9326276779174805\n",
      "epoch: 888, loss =  0.0674, R2 = 0.9326276779174805\n",
      "test: loss =  0.0673, R2 = 0.9326925873756409\n",
      "epoch: 889, loss =  0.0673, R2 = 0.9326925873756409\n",
      "test: loss =  0.0673, R2 = 0.9327179193496704\n",
      "epoch: 890, loss =  0.0673, R2 = 0.9327179193496704\n",
      "test: loss =  0.0673, R2 = 0.9327062368392944\n",
      "epoch: 891, loss =  0.0673, R2 = 0.9327062368392944\n",
      "test: loss =  0.0673, R2 = 0.932678759098053\n",
      "epoch: 892, loss =  0.0673, R2 = 0.932678759098053\n",
      "test: loss =  0.0673, R2 = 0.9326603412628174\n",
      "epoch: 893, loss =  0.0673, R2 = 0.9326603412628174\n",
      "test: loss =  0.0673, R2 = 0.9326655864715576\n",
      "epoch: 894, loss =  0.0673, R2 = 0.9326655864715576\n",
      "test: loss =  0.0673, R2 = 0.9326940774917603\n",
      "epoch: 895, loss =  0.0673, R2 = 0.9326940774917603\n",
      "test: loss =  0.0672, R2 = 0.932733416557312\n",
      "epoch: 896, loss =  0.0672, R2 = 0.932733416557312\n",
      "test: loss =  0.0672, R2 = 0.9327682852745056\n",
      "epoch: 897, loss =  0.0672, R2 = 0.9327682852745056\n",
      "test: loss =  0.0672, R2 = 0.932788610458374\n",
      "epoch: 898, loss =  0.0672, R2 = 0.932788610458374\n",
      "test: loss =  0.0672, R2 = 0.932793140411377\n",
      "epoch: 899, loss =  0.0672, R2 = 0.932793140411377\n",
      "test: loss =  0.0672, R2 = 0.932788074016571\n",
      "epoch: 900, loss =  0.0672, R2 = 0.932788074016571\n",
      "test: loss =  0.0672, R2 = 0.932782769203186\n",
      "epoch: 901, loss =  0.0672, R2 = 0.932782769203186\n",
      "test: loss =  0.0672, R2 = 0.932784378528595\n",
      "epoch: 902, loss =  0.0672, R2 = 0.932784378528595\n",
      "test: loss =  0.0672, R2 = 0.9327955842018127\n",
      "epoch: 903, loss =  0.0672, R2 = 0.9327955842018127\n",
      "test: loss =  0.0672, R2 = 0.9328141808509827\n",
      "epoch: 904, loss =  0.0672, R2 = 0.9328141808509827\n",
      "test: loss =  0.0671, R2 = 0.9328351616859436\n",
      "epoch: 905, loss =  0.0671, R2 = 0.9328351616859436\n",
      "test: loss =  0.0671, R2 = 0.9328535795211792\n",
      "epoch: 906, loss =  0.0671, R2 = 0.9328535795211792\n",
      "test: loss =  0.0671, R2 = 0.9328665137290955\n",
      "epoch: 907, loss =  0.0671, R2 = 0.9328665137290955\n",
      "test: loss =  0.0671, R2 = 0.9328737854957581\n",
      "epoch: 908, loss =  0.0671, R2 = 0.9328737854957581\n",
      "test: loss =  0.0671, R2 = 0.9328774809837341\n",
      "epoch: 909, loss =  0.0671, R2 = 0.9328774809837341\n",
      "test: loss =  0.0671, R2 = 0.9328805208206177\n",
      "epoch: 910, loss =  0.0671, R2 = 0.9328805208206177\n",
      "test: loss =  0.0671, R2 = 0.9328853487968445\n",
      "epoch: 911, loss =  0.0671, R2 = 0.9328853487968445\n",
      "test: loss =  0.0671, R2 = 0.9328932762145996\n",
      "epoch: 912, loss =  0.0671, R2 = 0.9328932762145996\n",
      "test: loss =  0.0671, R2 = 0.932904064655304\n",
      "epoch: 913, loss =  0.0671, R2 = 0.932904064655304\n",
      "test: loss =  0.0671, R2 = 0.9329164624214172\n",
      "epoch: 914, loss =  0.0671, R2 = 0.9329164624214172\n",
      "test: loss =  0.0671, R2 = 0.9329288005828857\n",
      "epoch: 915, loss =  0.0671, R2 = 0.9329288005828857\n",
      "test: loss =  0.0670, R2 = 0.932939887046814\n",
      "epoch: 916, loss =  0.0670, R2 = 0.932939887046814\n",
      "test: loss =  0.0670, R2 = 0.9329487681388855\n",
      "epoch: 917, loss =  0.0670, R2 = 0.9329487681388855\n",
      "test: loss =  0.0670, R2 = 0.9329555034637451\n",
      "epoch: 918, loss =  0.0670, R2 = 0.9329555034637451\n",
      "test: loss =  0.0670, R2 = 0.9329602718353271\n",
      "epoch: 919, loss =  0.0670, R2 = 0.9329602718353271\n",
      "test: loss =  0.0670, R2 = 0.9329636693000793\n",
      "epoch: 920, loss =  0.0670, R2 = 0.9329636693000793\n",
      "test: loss =  0.0670, R2 = 0.932965874671936\n",
      "epoch: 921, loss =  0.0670, R2 = 0.932965874671936\n",
      "test: loss =  0.0670, R2 = 0.9329667687416077\n",
      "epoch: 922, loss =  0.0670, R2 = 0.9329667687416077\n",
      "test: loss =  0.0670, R2 = 0.9329654574394226\n",
      "epoch: 923, loss =  0.0670, R2 = 0.9329654574394226\n",
      "test: loss =  0.0670, R2 = 0.9329603910446167\n",
      "epoch: 924, loss =  0.0670, R2 = 0.9329603910446167\n",
      "test: loss =  0.0670, R2 = 0.9329489469528198\n",
      "epoch: 925, loss =  0.0670, R2 = 0.9329489469528198\n",
      "test: loss =  0.0671, R2 = 0.9329276084899902\n",
      "epoch: 926, loss =  0.0671, R2 = 0.9329276084899902\n",
      "test: loss =  0.0671, R2 = 0.9328911304473877\n",
      "epoch: 927, loss =  0.0671, R2 = 0.9328911304473877\n",
      "test: loss =  0.0671, R2 = 0.9328320622444153\n",
      "epoch: 928, loss =  0.0671, R2 = 0.9328320622444153\n",
      "test: loss =  0.0672, R2 = 0.9327402710914612\n",
      "epoch: 929, loss =  0.0672, R2 = 0.9327402710914612\n",
      "test: loss =  0.0674, R2 = 0.9326013922691345\n",
      "epoch: 930, loss =  0.0674, R2 = 0.9326013922691345\n",
      "test: loss =  0.0676, R2 = 0.93239825963974\n",
      "epoch: 931, loss =  0.0676, R2 = 0.93239825963974\n",
      "test: loss =  0.0679, R2 = 0.9321112036705017\n",
      "epoch: 932, loss =  0.0679, R2 = 0.9321112036705017\n",
      "test: loss =  0.0683, R2 = 0.9317299127578735\n",
      "epoch: 933, loss =  0.0683, R2 = 0.9317299127578735\n",
      "test: loss =  0.0687, R2 = 0.9312633275985718\n",
      "epoch: 934, loss =  0.0687, R2 = 0.9312633275985718\n",
      "test: loss =  0.0692, R2 = 0.9307783246040344\n",
      "epoch: 935, loss =  0.0692, R2 = 0.9307783246040344\n",
      "test: loss =  0.0696, R2 = 0.9304030537605286\n",
      "epoch: 936, loss =  0.0696, R2 = 0.9304030537605286\n",
      "test: loss =  0.0696, R2 = 0.9303349256515503\n",
      "epoch: 937, loss =  0.0696, R2 = 0.9303349256515503\n",
      "test: loss =  0.0693, R2 = 0.9307045936584473\n",
      "epoch: 938, loss =  0.0693, R2 = 0.9307045936584473\n",
      "test: loss =  0.0685, R2 = 0.9314695596694946\n",
      "epoch: 939, loss =  0.0685, R2 = 0.9314695596694946\n",
      "test: loss =  0.0676, R2 = 0.9323316812515259\n",
      "epoch: 940, loss =  0.0676, R2 = 0.9323316812515259\n",
      "test: loss =  0.0671, R2 = 0.9329280853271484\n",
      "epoch: 941, loss =  0.0671, R2 = 0.9329280853271484\n",
      "test: loss =  0.0669, R2 = 0.9330624938011169\n",
      "epoch: 942, loss =  0.0669, R2 = 0.9330624938011169\n",
      "test: loss =  0.0672, R2 = 0.9328168630599976\n",
      "epoch: 943, loss =  0.0672, R2 = 0.9328168630599976\n",
      "test: loss =  0.0675, R2 = 0.9324568510055542\n",
      "epoch: 944, loss =  0.0675, R2 = 0.9324568510055542\n",
      "test: loss =  0.0677, R2 = 0.9322540760040283\n",
      "epoch: 945, loss =  0.0677, R2 = 0.9322540760040283\n",
      "test: loss =  0.0676, R2 = 0.9323391914367676\n",
      "epoch: 946, loss =  0.0676, R2 = 0.9323391914367676\n",
      "test: loss =  0.0673, R2 = 0.9326386451721191\n",
      "epoch: 947, loss =  0.0673, R2 = 0.9326386451721191\n",
      "test: loss =  0.0670, R2 = 0.932952344417572\n",
      "epoch: 948, loss =  0.0670, R2 = 0.932952344417572\n",
      "test: loss =  0.0669, R2 = 0.9331029653549194\n",
      "epoch: 949, loss =  0.0669, R2 = 0.9331029653549194\n",
      "test: loss =  0.0669, R2 = 0.9330562353134155\n",
      "epoch: 950, loss =  0.0669, R2 = 0.9330562353134155\n",
      "test: loss =  0.0671, R2 = 0.932915449142456\n",
      "epoch: 951, loss =  0.0671, R2 = 0.932915449142456\n",
      "test: loss =  0.0672, R2 = 0.9328248500823975\n",
      "epoch: 952, loss =  0.0672, R2 = 0.9328248500823975\n",
      "test: loss =  0.0671, R2 = 0.9328653216362\n",
      "epoch: 953, loss =  0.0671, R2 = 0.9328653216362\n",
      "test: loss =  0.0670, R2 = 0.9330098628997803\n",
      "epoch: 954, loss =  0.0670, R2 = 0.9330098628997803\n",
      "test: loss =  0.0668, R2 = 0.9331611394882202\n",
      "epoch: 955, loss =  0.0668, R2 = 0.9331611394882202\n",
      "test: loss =  0.0667, R2 = 0.9332324266433716\n",
      "epoch: 956, loss =  0.0667, R2 = 0.9332324266433716\n",
      "test: loss =  0.0668, R2 = 0.9332072734832764\n",
      "epoch: 957, loss =  0.0668, R2 = 0.9332072734832764\n",
      "test: loss =  0.0668, R2 = 0.9331378936767578\n",
      "epoch: 958, loss =  0.0668, R2 = 0.9331378936767578\n",
      "test: loss =  0.0669, R2 = 0.9330962896347046\n",
      "epoch: 959, loss =  0.0669, R2 = 0.9330962896347046\n",
      "test: loss =  0.0669, R2 = 0.9331218600273132\n",
      "epoch: 960, loss =  0.0669, R2 = 0.9331218600273132\n",
      "test: loss =  0.0668, R2 = 0.9332000017166138\n",
      "epoch: 961, loss =  0.0668, R2 = 0.9332000017166138\n",
      "test: loss =  0.0667, R2 = 0.9332815408706665\n",
      "epoch: 962, loss =  0.0667, R2 = 0.9332815408706665\n",
      "test: loss =  0.0667, R2 = 0.9333224892616272\n",
      "epoch: 963, loss =  0.0667, R2 = 0.9333224892616272\n",
      "test: loss =  0.0667, R2 = 0.9333131909370422\n",
      "epoch: 964, loss =  0.0667, R2 = 0.9333131909370422\n",
      "test: loss =  0.0667, R2 = 0.9332785606384277\n",
      "epoch: 965, loss =  0.0667, R2 = 0.9332785606384277\n",
      "test: loss =  0.0667, R2 = 0.933255672454834\n",
      "epoch: 966, loss =  0.0667, R2 = 0.933255672454834\n",
      "test: loss =  0.0667, R2 = 0.9332677125930786\n",
      "epoch: 967, loss =  0.0667, R2 = 0.9332677125930786\n",
      "test: loss =  0.0667, R2 = 0.9333115220069885\n",
      "epoch: 968, loss =  0.0667, R2 = 0.9333115220069885\n",
      "test: loss =  0.0666, R2 = 0.9333645105361938\n",
      "epoch: 969, loss =  0.0666, R2 = 0.9333645105361938\n",
      "test: loss =  0.0666, R2 = 0.933401882648468\n",
      "epoch: 970, loss =  0.0666, R2 = 0.933401882648468\n",
      "test: loss =  0.0666, R2 = 0.9334123730659485\n",
      "epoch: 971, loss =  0.0666, R2 = 0.9334123730659485\n",
      "test: loss =  0.0666, R2 = 0.9334023594856262\n",
      "epoch: 972, loss =  0.0666, R2 = 0.9334023594856262\n",
      "test: loss =  0.0666, R2 = 0.933388352394104\n",
      "epoch: 973, loss =  0.0666, R2 = 0.933388352394104\n",
      "test: loss =  0.0666, R2 = 0.9333854913711548\n",
      "epoch: 974, loss =  0.0666, R2 = 0.9333854913711548\n",
      "test: loss =  0.0666, R2 = 0.9333990812301636\n",
      "epoch: 975, loss =  0.0666, R2 = 0.9333990812301636\n",
      "test: loss =  0.0666, R2 = 0.9334238767623901\n",
      "epoch: 976, loss =  0.0666, R2 = 0.9334238767623901\n",
      "test: loss =  0.0665, R2 = 0.9334493279457092\n",
      "epoch: 977, loss =  0.0665, R2 = 0.9334493279457092\n",
      "test: loss =  0.0665, R2 = 0.9334670305252075\n",
      "epoch: 978, loss =  0.0665, R2 = 0.9334670305252075\n",
      "test: loss =  0.0665, R2 = 0.9334746599197388\n",
      "epoch: 979, loss =  0.0665, R2 = 0.9334746599197388\n",
      "test: loss =  0.0665, R2 = 0.9334758520126343\n",
      "epoch: 980, loss =  0.0665, R2 = 0.9334758520126343\n",
      "test: loss =  0.0665, R2 = 0.9334770441055298\n",
      "epoch: 981, loss =  0.0665, R2 = 0.9334770441055298\n",
      "test: loss =  0.0665, R2 = 0.9334830045700073\n",
      "epoch: 982, loss =  0.0665, R2 = 0.9334830045700073\n",
      "test: loss =  0.0665, R2 = 0.9334948062896729\n",
      "epoch: 983, loss =  0.0665, R2 = 0.9334948062896729\n",
      "test: loss =  0.0665, R2 = 0.9335100650787354\n",
      "epoch: 984, loss =  0.0665, R2 = 0.9335100650787354\n",
      "test: loss =  0.0665, R2 = 0.9335249066352844\n",
      "epoch: 985, loss =  0.0665, R2 = 0.9335249066352844\n",
      "test: loss =  0.0664, R2 = 0.9335364103317261\n",
      "epoch: 986, loss =  0.0664, R2 = 0.9335364103317261\n",
      "test: loss =  0.0664, R2 = 0.9335440397262573\n",
      "epoch: 987, loss =  0.0664, R2 = 0.9335440397262573\n",
      "test: loss =  0.0664, R2 = 0.9335490465164185\n",
      "epoch: 988, loss =  0.0664, R2 = 0.9335490465164185\n",
      "test: loss =  0.0664, R2 = 0.93355393409729\n",
      "epoch: 989, loss =  0.0664, R2 = 0.93355393409729\n",
      "test: loss =  0.0664, R2 = 0.9335605502128601\n",
      "epoch: 990, loss =  0.0664, R2 = 0.9335605502128601\n",
      "test: loss =  0.0664, R2 = 0.9335695505142212\n",
      "epoch: 991, loss =  0.0664, R2 = 0.9335695505142212\n",
      "test: loss =  0.0664, R2 = 0.9335802793502808\n",
      "epoch: 992, loss =  0.0664, R2 = 0.9335802793502808\n",
      "test: loss =  0.0664, R2 = 0.9335911273956299\n",
      "epoch: 993, loss =  0.0664, R2 = 0.9335911273956299\n",
      "test: loss =  0.0664, R2 = 0.9336007833480835\n",
      "epoch: 994, loss =  0.0664, R2 = 0.9336007833480835\n",
      "test: loss =  0.0664, R2 = 0.9336082339286804\n",
      "epoch: 995, loss =  0.0664, R2 = 0.9336082339286804\n",
      "test: loss =  0.0664, R2 = 0.9336134195327759\n",
      "epoch: 996, loss =  0.0664, R2 = 0.9336134195327759\n",
      "test: loss =  0.0664, R2 = 0.9336166381835938\n",
      "epoch: 997, loss =  0.0664, R2 = 0.9336166381835938\n",
      "test: loss =  0.0664, R2 = 0.933618426322937\n",
      "epoch: 998, loss =  0.0664, R2 = 0.933618426322937\n",
      "test: loss =  0.0664, R2 = 0.9336187839508057\n",
      "epoch: 999, loss =  0.0664, R2 = 0.9336187839508057\n",
      "test: loss =  0.0664, R2 = 0.9336169958114624\n",
      "epoch: 1000, loss =  0.0664, R2 = 0.9336169958114624\n",
      "test: loss =  0.0664, R2 = 0.9336117506027222\n",
      "epoch: 1001, loss =  0.0664, R2 = 0.9336117506027222\n",
      "test: loss =  0.0664, R2 = 0.9336007833480835\n",
      "epoch: 1002, loss =  0.0664, R2 = 0.9336007833480835\n",
      "test: loss =  0.0664, R2 = 0.933580756187439\n",
      "epoch: 1003, loss =  0.0664, R2 = 0.933580756187439\n",
      "test: loss =  0.0664, R2 = 0.9335473775863647\n",
      "epoch: 1004, loss =  0.0664, R2 = 0.9335473775863647\n",
      "test: loss =  0.0665, R2 = 0.9334943890571594\n",
      "epoch: 1005, loss =  0.0665, R2 = 0.9334943890571594\n",
      "test: loss =  0.0666, R2 = 0.9334129691123962\n",
      "epoch: 1006, loss =  0.0666, R2 = 0.9334129691123962\n",
      "test: loss =  0.0667, R2 = 0.9332900047302246\n",
      "epoch: 1007, loss =  0.0667, R2 = 0.9332900047302246\n",
      "test: loss =  0.0669, R2 = 0.9331069588661194\n",
      "epoch: 1008, loss =  0.0669, R2 = 0.9331069588661194\n",
      "test: loss =  0.0671, R2 = 0.9328376054763794\n",
      "epoch: 1009, loss =  0.0671, R2 = 0.9328376054763794\n",
      "test: loss =  0.0675, R2 = 0.9324479103088379\n",
      "epoch: 1010, loss =  0.0675, R2 = 0.9324479103088379\n",
      "test: loss =  0.0681, R2 = 0.9318971037864685\n",
      "epoch: 1011, loss =  0.0681, R2 = 0.9318971037864685\n",
      "test: loss =  0.0688, R2 = 0.9311505556106567\n",
      "epoch: 1012, loss =  0.0688, R2 = 0.9311505556106567\n",
      "test: loss =  0.0698, R2 = 0.9302002787590027\n",
      "epoch: 1013, loss =  0.0698, R2 = 0.9302002787590027\n",
      "test: loss =  0.0709, R2 = 0.9291192293167114\n",
      "epoch: 1014, loss =  0.0709, R2 = 0.9291192293167114\n",
      "test: loss =  0.0719, R2 = 0.9281096458435059\n",
      "epoch: 1015, loss =  0.0719, R2 = 0.9281096458435059\n",
      "test: loss =  0.0724, R2 = 0.9275424480438232\n",
      "epoch: 1016, loss =  0.0724, R2 = 0.9275424480438232\n",
      "test: loss =  0.0722, R2 = 0.9278247356414795\n",
      "epoch: 1017, loss =  0.0722, R2 = 0.9278247356414795\n",
      "test: loss =  0.0708, R2 = 0.929134726524353\n",
      "epoch: 1018, loss =  0.0708, R2 = 0.929134726524353\n",
      "test: loss =  0.0689, R2 = 0.9311035871505737\n",
      "epoch: 1019, loss =  0.0689, R2 = 0.9311035871505737\n",
      "test: loss =  0.0671, R2 = 0.9329120516777039\n",
      "epoch: 1020, loss =  0.0671, R2 = 0.9329120516777039\n",
      "test: loss =  0.0662, R2 = 0.9337958097457886\n",
      "epoch: 1021, loss =  0.0662, R2 = 0.9337958097457886\n",
      "test: loss =  0.0664, R2 = 0.9335652589797974\n",
      "epoch: 1022, loss =  0.0664, R2 = 0.9335652589797974\n",
      "test: loss =  0.0673, R2 = 0.9326614141464233\n",
      "epoch: 1023, loss =  0.0673, R2 = 0.9326614141464233\n",
      "test: loss =  0.0682, R2 = 0.9318134784698486\n",
      "epoch: 1024, loss =  0.0682, R2 = 0.9318134784698486\n",
      "test: loss =  0.0684, R2 = 0.9316016435623169\n",
      "epoch: 1025, loss =  0.0684, R2 = 0.9316016435623169\n",
      "test: loss =  0.0678, R2 = 0.9321399331092834\n",
      "epoch: 1026, loss =  0.0678, R2 = 0.9321399331092834\n",
      "test: loss =  0.0669, R2 = 0.9330470561981201\n",
      "epoch: 1027, loss =  0.0669, R2 = 0.9330470561981201\n",
      "test: loss =  0.0662, R2 = 0.9337417483329773\n",
      "epoch: 1028, loss =  0.0662, R2 = 0.9337417483329773\n",
      "test: loss =  0.0661, R2 = 0.9338709115982056\n",
      "epoch: 1029, loss =  0.0661, R2 = 0.9338709115982056\n",
      "test: loss =  0.0665, R2 = 0.9335149526596069\n",
      "epoch: 1030, loss =  0.0665, R2 = 0.9335149526596069\n",
      "test: loss =  0.0669, R2 = 0.9330539703369141\n",
      "epoch: 1031, loss =  0.0669, R2 = 0.9330539703369141\n",
      "test: loss =  0.0671, R2 = 0.9328621029853821\n",
      "epoch: 1032, loss =  0.0671, R2 = 0.9328621029853821\n",
      "test: loss =  0.0669, R2 = 0.9330636858940125\n",
      "epoch: 1033, loss =  0.0669, R2 = 0.9330636858940125\n",
      "test: loss =  0.0665, R2 = 0.93349289894104\n",
      "epoch: 1034, loss =  0.0665, R2 = 0.93349289894104\n",
      "test: loss =  0.0661, R2 = 0.9338539242744446\n",
      "epoch: 1035, loss =  0.0661, R2 = 0.9338539242744446\n",
      "test: loss =  0.0660, R2 = 0.9339492917060852\n",
      "epoch: 1036, loss =  0.0660, R2 = 0.9339492917060852\n",
      "test: loss =  0.0662, R2 = 0.9337967038154602\n",
      "epoch: 1037, loss =  0.0662, R2 = 0.9337967038154602\n",
      "test: loss =  0.0664, R2 = 0.9335742592811584\n",
      "epoch: 1038, loss =  0.0664, R2 = 0.9335742592811584\n",
      "test: loss =  0.0665, R2 = 0.9334693551063538\n",
      "epoch: 1039, loss =  0.0665, R2 = 0.9334693551063538\n",
      "test: loss =  0.0664, R2 = 0.9335532188415527\n",
      "epoch: 1040, loss =  0.0664, R2 = 0.9335532188415527\n",
      "test: loss =  0.0662, R2 = 0.9337570071220398\n",
      "epoch: 1041, loss =  0.0662, R2 = 0.9337570071220398\n",
      "test: loss =  0.0660, R2 = 0.9339439868927002\n",
      "epoch: 1042, loss =  0.0660, R2 = 0.9339439868927002\n",
      "test: loss =  0.0660, R2 = 0.934013843536377\n",
      "epoch: 1043, loss =  0.0660, R2 = 0.934013843536377\n",
      "test: loss =  0.0660, R2 = 0.9339612722396851\n",
      "epoch: 1044, loss =  0.0660, R2 = 0.9339612722396851\n",
      "test: loss =  0.0661, R2 = 0.9338597655296326\n",
      "epoch: 1045, loss =  0.0661, R2 = 0.9338597655296326\n",
      "test: loss =  0.0662, R2 = 0.9337980151176453\n",
      "epoch: 1046, loss =  0.0662, R2 = 0.9337980151176453\n",
      "test: loss =  0.0662, R2 = 0.9338209629058838\n",
      "epoch: 1047, loss =  0.0662, R2 = 0.9338209629058838\n",
      "test: loss =  0.0661, R2 = 0.9339109063148499\n",
      "epoch: 1048, loss =  0.0661, R2 = 0.9339109063148499\n",
      "test: loss =  0.0660, R2 = 0.9340112209320068\n",
      "epoch: 1049, loss =  0.0660, R2 = 0.9340112209320068\n",
      "test: loss =  0.0659, R2 = 0.9340697526931763\n",
      "epoch: 1050, loss =  0.0659, R2 = 0.9340697526931763\n",
      "test: loss =  0.0659, R2 = 0.934069812297821\n",
      "epoch: 1051, loss =  0.0659, R2 = 0.934069812297821\n",
      "test: loss =  0.0659, R2 = 0.9340327382087708\n",
      "epoch: 1052, loss =  0.0659, R2 = 0.9340327382087708\n",
      "test: loss =  0.0660, R2 = 0.9339970350265503\n",
      "epoch: 1053, loss =  0.0660, R2 = 0.9339970350265503\n",
      "test: loss =  0.0660, R2 = 0.9339919686317444\n",
      "epoch: 1054, loss =  0.0660, R2 = 0.9339919686317444\n",
      "test: loss =  0.0660, R2 = 0.9340227842330933\n",
      "epoch: 1055, loss =  0.0660, R2 = 0.9340227842330933\n",
      "test: loss =  0.0659, R2 = 0.9340727925300598\n",
      "epoch: 1056, loss =  0.0659, R2 = 0.9340727925300598\n",
      "test: loss =  0.0659, R2 = 0.9341176748275757\n",
      "epoch: 1057, loss =  0.0659, R2 = 0.9341176748275757\n",
      "test: loss =  0.0658, R2 = 0.9341406226158142\n",
      "epoch: 1058, loss =  0.0658, R2 = 0.9341406226158142\n",
      "test: loss =  0.0658, R2 = 0.9341403245925903\n",
      "epoch: 1059, loss =  0.0658, R2 = 0.9341403245925903\n",
      "test: loss =  0.0659, R2 = 0.9341280460357666\n",
      "epoch: 1060, loss =  0.0659, R2 = 0.9341280460357666\n",
      "test: loss =  0.0659, R2 = 0.934118926525116\n",
      "epoch: 1061, loss =  0.0659, R2 = 0.934118926525116\n",
      "test: loss =  0.0659, R2 = 0.9341230392456055\n",
      "epoch: 1062, loss =  0.0659, R2 = 0.9341230392456055\n",
      "test: loss =  0.0658, R2 = 0.9341413974761963\n",
      "epoch: 1063, loss =  0.0658, R2 = 0.9341413974761963\n",
      "test: loss =  0.0658, R2 = 0.9341673851013184\n",
      "epoch: 1064, loss =  0.0658, R2 = 0.9341673851013184\n",
      "test: loss =  0.0658, R2 = 0.9341918230056763\n",
      "epoch: 1065, loss =  0.0658, R2 = 0.9341918230056763\n",
      "test: loss =  0.0658, R2 = 0.9342081546783447\n",
      "epoch: 1066, loss =  0.0658, R2 = 0.9342081546783447\n",
      "test: loss =  0.0658, R2 = 0.9342150688171387\n",
      "epoch: 1067, loss =  0.0658, R2 = 0.9342150688171387\n",
      "test: loss =  0.0658, R2 = 0.9342159628868103\n",
      "epoch: 1068, loss =  0.0658, R2 = 0.9342159628868103\n",
      "test: loss =  0.0658, R2 = 0.9342162609100342\n",
      "epoch: 1069, loss =  0.0658, R2 = 0.9342162609100342\n",
      "test: loss =  0.0658, R2 = 0.9342204332351685\n",
      "epoch: 1070, loss =  0.0658, R2 = 0.9342204332351685\n",
      "test: loss =  0.0658, R2 = 0.9342302083969116\n",
      "epoch: 1071, loss =  0.0658, R2 = 0.9342302083969116\n",
      "test: loss =  0.0657, R2 = 0.9342443346977234\n",
      "epoch: 1072, loss =  0.0657, R2 = 0.9342443346977234\n",
      "test: loss =  0.0657, R2 = 0.9342599511146545\n",
      "epoch: 1073, loss =  0.0657, R2 = 0.9342599511146545\n",
      "test: loss =  0.0657, R2 = 0.9342740178108215\n",
      "epoch: 1074, loss =  0.0657, R2 = 0.9342740178108215\n",
      "test: loss =  0.0657, R2 = 0.9342849254608154\n",
      "epoch: 1075, loss =  0.0657, R2 = 0.9342849254608154\n",
      "test: loss =  0.0657, R2 = 0.9342926144599915\n",
      "epoch: 1076, loss =  0.0657, R2 = 0.9342926144599915\n",
      "test: loss =  0.0657, R2 = 0.9342982769012451\n",
      "epoch: 1077, loss =  0.0657, R2 = 0.9342982769012451\n",
      "test: loss =  0.0657, R2 = 0.9343036413192749\n",
      "epoch: 1078, loss =  0.0657, R2 = 0.9343036413192749\n",
      "test: loss =  0.0657, R2 = 0.934310257434845\n",
      "epoch: 1079, loss =  0.0657, R2 = 0.934310257434845\n",
      "test: loss =  0.0657, R2 = 0.9343187212944031\n",
      "epoch: 1080, loss =  0.0657, R2 = 0.9343187212944031\n",
      "test: loss =  0.0657, R2 = 0.9343287944793701\n",
      "epoch: 1081, loss =  0.0657, R2 = 0.9343287944793701\n",
      "test: loss =  0.0656, R2 = 0.9343397617340088\n",
      "epoch: 1082, loss =  0.0656, R2 = 0.9343397617340088\n",
      "test: loss =  0.0656, R2 = 0.9343507289886475\n",
      "epoch: 1083, loss =  0.0656, R2 = 0.9343507289886475\n",
      "test: loss =  0.0656, R2 = 0.9343608617782593\n",
      "epoch: 1084, loss =  0.0656, R2 = 0.9343608617782593\n",
      "test: loss =  0.0656, R2 = 0.9343699812889099\n",
      "epoch: 1085, loss =  0.0656, R2 = 0.9343699812889099\n",
      "test: loss =  0.0656, R2 = 0.9343780875205994\n",
      "epoch: 1086, loss =  0.0656, R2 = 0.9343780875205994\n",
      "test: loss =  0.0656, R2 = 0.9343856573104858\n",
      "epoch: 1087, loss =  0.0656, R2 = 0.9343856573104858\n",
      "test: loss =  0.0656, R2 = 0.9343931078910828\n",
      "epoch: 1088, loss =  0.0656, R2 = 0.9343931078910828\n",
      "test: loss =  0.0656, R2 = 0.9344007968902588\n",
      "epoch: 1089, loss =  0.0656, R2 = 0.9344007968902588\n",
      "test: loss =  0.0656, R2 = 0.9344090819358826\n",
      "epoch: 1090, loss =  0.0656, R2 = 0.9344090819358826\n",
      "test: loss =  0.0656, R2 = 0.9344178438186646\n",
      "epoch: 1091, loss =  0.0656, R2 = 0.9344178438186646\n",
      "test: loss =  0.0656, R2 = 0.93442702293396\n",
      "epoch: 1092, loss =  0.0656, R2 = 0.93442702293396\n",
      "test: loss =  0.0655, R2 = 0.9344363808631897\n",
      "epoch: 1093, loss =  0.0655, R2 = 0.9344363808631897\n",
      "test: loss =  0.0655, R2 = 0.9344456195831299\n",
      "epoch: 1094, loss =  0.0655, R2 = 0.9344456195831299\n",
      "test: loss =  0.0655, R2 = 0.9344547390937805\n",
      "epoch: 1095, loss =  0.0655, R2 = 0.9344547390937805\n",
      "test: loss =  0.0655, R2 = 0.9344635605812073\n",
      "epoch: 1096, loss =  0.0655, R2 = 0.9344635605812073\n",
      "test: loss =  0.0655, R2 = 0.9344720840454102\n",
      "epoch: 1097, loss =  0.0655, R2 = 0.9344720840454102\n",
      "test: loss =  0.0655, R2 = 0.9344803690910339\n",
      "epoch: 1098, loss =  0.0655, R2 = 0.9344803690910339\n",
      "test: loss =  0.0655, R2 = 0.9344885945320129\n",
      "epoch: 1099, loss =  0.0655, R2 = 0.9344885945320129\n",
      "test: loss =  0.0655, R2 = 0.9344968199729919\n",
      "epoch: 1100, loss =  0.0655, R2 = 0.9344968199729919\n",
      "test: loss =  0.0655, R2 = 0.9345051050186157\n",
      "epoch: 1101, loss =  0.0655, R2 = 0.9345051050186157\n",
      "test: loss =  0.0655, R2 = 0.9345134496688843\n",
      "epoch: 1102, loss =  0.0655, R2 = 0.9345134496688843\n",
      "test: loss =  0.0655, R2 = 0.9345219135284424\n",
      "epoch: 1103, loss =  0.0655, R2 = 0.9345219135284424\n",
      "test: loss =  0.0655, R2 = 0.9345305562019348\n",
      "epoch: 1104, loss =  0.0655, R2 = 0.9345305562019348\n",
      "test: loss =  0.0654, R2 = 0.9345391988754272\n",
      "epoch: 1105, loss =  0.0654, R2 = 0.9345391988754272\n",
      "test: loss =  0.0654, R2 = 0.9345479011535645\n",
      "epoch: 1106, loss =  0.0654, R2 = 0.9345479011535645\n",
      "test: loss =  0.0654, R2 = 0.9345566034317017\n",
      "epoch: 1107, loss =  0.0654, R2 = 0.9345566034317017\n",
      "test: loss =  0.0654, R2 = 0.9345652461051941\n",
      "epoch: 1108, loss =  0.0654, R2 = 0.9345652461051941\n",
      "test: loss =  0.0654, R2 = 0.9345738291740417\n",
      "epoch: 1109, loss =  0.0654, R2 = 0.9345738291740417\n",
      "test: loss =  0.0654, R2 = 0.9345822930335999\n",
      "epoch: 1110, loss =  0.0654, R2 = 0.9345822930335999\n",
      "test: loss =  0.0654, R2 = 0.9345905780792236\n",
      "epoch: 1111, loss =  0.0654, R2 = 0.9345905780792236\n",
      "test: loss =  0.0654, R2 = 0.9345986843109131\n",
      "epoch: 1112, loss =  0.0654, R2 = 0.9345986843109131\n",
      "test: loss =  0.0654, R2 = 0.9346063733100891\n",
      "epoch: 1113, loss =  0.0654, R2 = 0.9346063733100891\n",
      "test: loss =  0.0654, R2 = 0.9346136450767517\n",
      "epoch: 1114, loss =  0.0654, R2 = 0.9346136450767517\n",
      "test: loss =  0.0654, R2 = 0.9346200227737427\n",
      "epoch: 1115, loss =  0.0654, R2 = 0.9346200227737427\n",
      "test: loss =  0.0654, R2 = 0.9346250891685486\n",
      "epoch: 1116, loss =  0.0654, R2 = 0.9346250891685486\n",
      "test: loss =  0.0654, R2 = 0.934627890586853\n",
      "epoch: 1117, loss =  0.0654, R2 = 0.934627890586853\n",
      "test: loss =  0.0654, R2 = 0.9346269965171814\n",
      "epoch: 1118, loss =  0.0654, R2 = 0.9346269965171814\n",
      "test: loss =  0.0654, R2 = 0.9346197843551636\n",
      "epoch: 1119, loss =  0.0654, R2 = 0.9346197843551636\n",
      "test: loss =  0.0654, R2 = 0.934601902961731\n",
      "epoch: 1120, loss =  0.0654, R2 = 0.934601902961731\n",
      "test: loss =  0.0654, R2 = 0.9345659017562866\n",
      "epoch: 1121, loss =  0.0654, R2 = 0.9345659017562866\n",
      "test: loss =  0.0655, R2 = 0.9344991445541382\n",
      "epoch: 1122, loss =  0.0655, R2 = 0.9344991445541382\n",
      "test: loss =  0.0656, R2 = 0.9343804121017456\n",
      "epoch: 1123, loss =  0.0656, R2 = 0.9343804121017456\n",
      "test: loss =  0.0658, R2 = 0.9341753125190735\n",
      "epoch: 1124, loss =  0.0658, R2 = 0.9341753125190735\n",
      "test: loss =  0.0662, R2 = 0.9338306188583374\n",
      "epoch: 1125, loss =  0.0662, R2 = 0.9338306188583374\n",
      "test: loss =  0.0667, R2 = 0.9332754015922546\n",
      "epoch: 1126, loss =  0.0667, R2 = 0.9332754015922546\n",
      "test: loss =  0.0675, R2 = 0.9324371814727783\n",
      "epoch: 1127, loss =  0.0675, R2 = 0.9324371814727783\n",
      "test: loss =  0.0687, R2 = 0.9313182234764099\n",
      "epoch: 1128, loss =  0.0687, R2 = 0.9313182234764099\n",
      "test: loss =  0.0699, R2 = 0.930126965045929\n",
      "epoch: 1129, loss =  0.0699, R2 = 0.930126965045929\n",
      "test: loss =  0.0706, R2 = 0.9294280409812927\n",
      "epoch: 1130, loss =  0.0706, R2 = 0.9294280409812927\n",
      "test: loss =  0.0701, R2 = 0.9299041032791138\n",
      "epoch: 1131, loss =  0.0701, R2 = 0.9299041032791138\n",
      "test: loss =  0.0683, R2 = 0.931656002998352\n",
      "epoch: 1132, loss =  0.0683, R2 = 0.931656002998352\n",
      "test: loss =  0.0663, R2 = 0.9336521029472351\n",
      "epoch: 1133, loss =  0.0663, R2 = 0.9336521029472351\n",
      "test: loss =  0.0654, R2 = 0.9345484972000122\n",
      "epoch: 1134, loss =  0.0654, R2 = 0.9345484972000122\n",
      "test: loss =  0.0660, R2 = 0.9340203404426575\n",
      "epoch: 1135, loss =  0.0660, R2 = 0.9340203404426575\n",
      "test: loss =  0.0670, R2 = 0.9329841136932373\n",
      "epoch: 1136, loss =  0.0670, R2 = 0.9329841136932373\n",
      "test: loss =  0.0674, R2 = 0.9326307773590088\n",
      "epoch: 1137, loss =  0.0674, R2 = 0.9326307773590088\n",
      "test: loss =  0.0667, R2 = 0.9333062171936035\n",
      "epoch: 1138, loss =  0.0667, R2 = 0.9333062171936035\n",
      "test: loss =  0.0657, R2 = 0.9342607855796814\n",
      "epoch: 1139, loss =  0.0657, R2 = 0.9342607855796814\n",
      "test: loss =  0.0654, R2 = 0.9345684051513672\n",
      "epoch: 1140, loss =  0.0654, R2 = 0.9345684051513672\n",
      "test: loss =  0.0658, R2 = 0.9341695308685303\n",
      "epoch: 1141, loss =  0.0658, R2 = 0.9341695308685303\n",
      "test: loss =  0.0662, R2 = 0.9337798357009888\n",
      "epoch: 1142, loss =  0.0662, R2 = 0.9337798357009888\n",
      "test: loss =  0.0660, R2 = 0.9339456558227539\n",
      "epoch: 1143, loss =  0.0660, R2 = 0.9339456558227539\n",
      "test: loss =  0.0655, R2 = 0.9344437122344971\n",
      "epoch: 1144, loss =  0.0655, R2 = 0.9344437122344971\n",
      "test: loss =  0.0653, R2 = 0.9346832633018494\n",
      "epoch: 1145, loss =  0.0653, R2 = 0.9346832633018494\n",
      "test: loss =  0.0655, R2 = 0.9344971179962158\n",
      "epoch: 1146, loss =  0.0655, R2 = 0.9344971179962158\n",
      "test: loss =  0.0657, R2 = 0.9342721700668335\n",
      "epoch: 1147, loss =  0.0657, R2 = 0.9342721700668335\n",
      "test: loss =  0.0656, R2 = 0.9343729019165039\n",
      "epoch: 1148, loss =  0.0656, R2 = 0.9343729019165039\n",
      "test: loss =  0.0653, R2 = 0.9346779584884644\n",
      "epoch: 1149, loss =  0.0653, R2 = 0.9346779584884644\n",
      "test: loss =  0.0652, R2 = 0.9348105788230896\n",
      "epoch: 1150, loss =  0.0652, R2 = 0.9348105788230896\n",
      "test: loss =  0.0653, R2 = 0.9346708059310913\n",
      "epoch: 1151, loss =  0.0653, R2 = 0.9346708059310913\n",
      "test: loss =  0.0655, R2 = 0.9345252513885498\n",
      "epoch: 1152, loss =  0.0655, R2 = 0.9345252513885498\n",
      "test: loss =  0.0654, R2 = 0.9346120357513428\n",
      "epoch: 1153, loss =  0.0654, R2 = 0.9346120357513428\n",
      "test: loss =  0.0651, R2 = 0.9348320960998535\n",
      "epoch: 1154, loss =  0.0651, R2 = 0.9348320960998535\n",
      "test: loss =  0.0651, R2 = 0.934921383857727\n",
      "epoch: 1155, loss =  0.0651, R2 = 0.934921383857727\n",
      "test: loss =  0.0652, R2 = 0.9348177313804626\n",
      "epoch: 1156, loss =  0.0652, R2 = 0.9348177313804626\n",
      "test: loss =  0.0653, R2 = 0.9347119927406311\n",
      "epoch: 1157, loss =  0.0653, R2 = 0.9347119927406311\n",
      "test: loss =  0.0652, R2 = 0.9347687363624573\n",
      "epoch: 1158, loss =  0.0652, R2 = 0.9347687363624573\n",
      "test: loss =  0.0651, R2 = 0.9349215626716614\n",
      "epoch: 1159, loss =  0.0651, R2 = 0.9349215626716614\n",
      "test: loss =  0.0650, R2 = 0.9349915981292725\n",
      "epoch: 1160, loss =  0.0650, R2 = 0.9349915981292725\n",
      "test: loss =  0.0651, R2 = 0.9349307417869568\n",
      "epoch: 1161, loss =  0.0651, R2 = 0.9349307417869568\n",
      "test: loss =  0.0651, R2 = 0.9348591566085815\n",
      "epoch: 1162, loss =  0.0651, R2 = 0.9348591566085815\n",
      "test: loss =  0.0651, R2 = 0.9348869919776917\n",
      "epoch: 1163, loss =  0.0651, R2 = 0.9348869919776917\n",
      "test: loss =  0.0650, R2 = 0.9349813461303711\n",
      "epoch: 1164, loss =  0.0650, R2 = 0.9349813461303711\n",
      "test: loss =  0.0649, R2 = 0.9350336194038391\n",
      "epoch: 1165, loss =  0.0649, R2 = 0.9350336194038391\n",
      "test: loss =  0.0650, R2 = 0.9350073337554932\n",
      "epoch: 1166, loss =  0.0650, R2 = 0.9350073337554932\n",
      "test: loss =  0.0650, R2 = 0.9349679946899414\n",
      "epoch: 1167, loss =  0.0650, R2 = 0.9349679946899414\n",
      "test: loss =  0.0650, R2 = 0.9349820017814636\n",
      "epoch: 1168, loss =  0.0650, R2 = 0.9349820017814636\n",
      "test: loss =  0.0649, R2 = 0.9350361227989197\n",
      "epoch: 1169, loss =  0.0649, R2 = 0.9350361227989197\n",
      "test: loss =  0.0649, R2 = 0.9350706934928894\n",
      "epoch: 1170, loss =  0.0649, R2 = 0.9350706934928894\n",
      "test: loss =  0.0649, R2 = 0.9350622892379761\n",
      "epoch: 1171, loss =  0.0649, R2 = 0.9350622892379761\n",
      "test: loss =  0.0649, R2 = 0.9350447654724121\n",
      "epoch: 1172, loss =  0.0649, R2 = 0.9350447654724121\n",
      "test: loss =  0.0649, R2 = 0.9350556135177612\n",
      "epoch: 1173, loss =  0.0649, R2 = 0.9350556135177612\n",
      "test: loss =  0.0649, R2 = 0.9350891709327698\n",
      "epoch: 1174, loss =  0.0649, R2 = 0.9350891709327698\n",
      "test: loss =  0.0649, R2 = 0.9351125955581665\n",
      "epoch: 1175, loss =  0.0649, R2 = 0.9351125955581665\n",
      "test: loss =  0.0649, R2 = 0.9351111650466919\n",
      "epoch: 1176, loss =  0.0649, R2 = 0.9351111650466919\n",
      "test: loss =  0.0649, R2 = 0.935103178024292\n",
      "epoch: 1177, loss =  0.0649, R2 = 0.935103178024292\n",
      "test: loss =  0.0649, R2 = 0.935111403465271\n",
      "epoch: 1178, loss =  0.0649, R2 = 0.935111403465271\n",
      "test: loss =  0.0648, R2 = 0.935134768486023\n",
      "epoch: 1179, loss =  0.0648, R2 = 0.935134768486023\n",
      "test: loss =  0.0648, R2 = 0.9351538419723511\n",
      "epoch: 1180, loss =  0.0648, R2 = 0.9351538419723511\n",
      "test: loss =  0.0648, R2 = 0.9351570010185242\n",
      "epoch: 1181, loss =  0.0648, R2 = 0.9351570010185242\n",
      "test: loss =  0.0648, R2 = 0.9351526498794556\n",
      "epoch: 1182, loss =  0.0648, R2 = 0.9351526498794556\n",
      "test: loss =  0.0648, R2 = 0.9351557493209839\n",
      "epoch: 1183, loss =  0.0648, R2 = 0.9351557493209839\n",
      "test: loss =  0.0648, R2 = 0.9351690411567688\n",
      "epoch: 1184, loss =  0.0648, R2 = 0.9351690411567688\n",
      "test: loss =  0.0648, R2 = 0.9351821541786194\n",
      "epoch: 1185, loss =  0.0648, R2 = 0.9351821541786194\n",
      "test: loss =  0.0648, R2 = 0.9351850748062134\n",
      "epoch: 1186, loss =  0.0648, R2 = 0.9351850748062134\n",
      "test: loss =  0.0648, R2 = 0.9351783990859985\n",
      "epoch: 1187, loss =  0.0648, R2 = 0.9351783990859985\n",
      "test: loss =  0.0648, R2 = 0.9351693987846375\n",
      "epoch: 1188, loss =  0.0648, R2 = 0.9351693987846375\n",
      "test: loss =  0.0648, R2 = 0.9351605772972107\n",
      "epoch: 1189, loss =  0.0648, R2 = 0.9351605772972107\n",
      "test: loss =  0.0648, R2 = 0.9351452589035034\n",
      "epoch: 1190, loss =  0.0648, R2 = 0.9351452589035034\n",
      "test: loss =  0.0649, R2 = 0.9351124167442322\n",
      "epoch: 1191, loss =  0.0649, R2 = 0.9351124167442322\n",
      "test: loss =  0.0649, R2 = 0.935052752494812\n",
      "epoch: 1192, loss =  0.0649, R2 = 0.935052752494812\n",
      "test: loss =  0.0650, R2 = 0.9349581003189087\n",
      "epoch: 1193, loss =  0.0650, R2 = 0.9349581003189087\n",
      "test: loss =  0.0652, R2 = 0.9348142147064209\n",
      "epoch: 1194, loss =  0.0652, R2 = 0.9348142147064209\n",
      "test: loss =  0.0654, R2 = 0.9345934391021729\n",
      "epoch: 1195, loss =  0.0654, R2 = 0.9345934391021729\n",
      "test: loss =  0.0657, R2 = 0.934253990650177\n",
      "epoch: 1196, loss =  0.0657, R2 = 0.934253990650177\n",
      "test: loss =  0.0662, R2 = 0.9337401390075684\n",
      "epoch: 1197, loss =  0.0662, R2 = 0.9337401390075684\n",
      "test: loss =  0.0670, R2 = 0.9329926371574402\n",
      "epoch: 1198, loss =  0.0670, R2 = 0.9329926371574402\n",
      "test: loss =  0.0680, R2 = 0.9319589138031006\n",
      "epoch: 1199, loss =  0.0680, R2 = 0.9319589138031006\n",
      "test: loss =  0.0693, R2 = 0.930653989315033\n",
      "epoch: 1200, loss =  0.0693, R2 = 0.930653989315033\n",
      "test: loss =  0.0708, R2 = 0.929226279258728\n",
      "epoch: 1201, loss =  0.0708, R2 = 0.929226279258728\n",
      "test: loss =  0.0719, R2 = 0.9280913472175598\n",
      "epoch: 1202, loss =  0.0719, R2 = 0.9280913472175598\n",
      "test: loss =  0.0721, R2 = 0.9278451800346375\n",
      "epoch: 1203, loss =  0.0721, R2 = 0.9278451800346375\n",
      "test: loss =  0.0710, R2 = 0.9289847016334534\n",
      "epoch: 1204, loss =  0.0710, R2 = 0.9289847016334534\n",
      "test: loss =  0.0687, R2 = 0.9312832951545715\n",
      "epoch: 1205, loss =  0.0687, R2 = 0.9312832951545715\n",
      "test: loss =  0.0663, R2 = 0.933699369430542\n",
      "epoch: 1206, loss =  0.0663, R2 = 0.933699369430542\n",
      "test: loss =  0.0650, R2 = 0.9350290894508362\n",
      "epoch: 1207, loss =  0.0650, R2 = 0.9350290894508362\n",
      "test: loss =  0.0651, R2 = 0.9348540306091309\n",
      "epoch: 1208, loss =  0.0651, R2 = 0.9348540306091309\n",
      "test: loss =  0.0662, R2 = 0.9337711334228516\n",
      "epoch: 1209, loss =  0.0662, R2 = 0.9337711334228516\n",
      "test: loss =  0.0671, R2 = 0.9328432679176331\n",
      "epoch: 1210, loss =  0.0671, R2 = 0.9328432679176331\n",
      "test: loss =  0.0671, R2 = 0.9328339099884033\n",
      "epoch: 1211, loss =  0.0671, R2 = 0.9328339099884033\n",
      "test: loss =  0.0663, R2 = 0.9336914420127869\n",
      "epoch: 1212, loss =  0.0663, R2 = 0.9336914420127869\n",
      "test: loss =  0.0653, R2 = 0.9346954822540283\n",
      "epoch: 1213, loss =  0.0653, R2 = 0.9346954822540283\n",
      "test: loss =  0.0649, R2 = 0.9351230263710022\n",
      "epoch: 1214, loss =  0.0649, R2 = 0.9351230263710022\n",
      "test: loss =  0.0651, R2 = 0.9348537921905518\n",
      "epoch: 1215, loss =  0.0651, R2 = 0.9348537921905518\n",
      "test: loss =  0.0656, R2 = 0.9343585968017578\n",
      "epoch: 1216, loss =  0.0656, R2 = 0.9343585968017578\n",
      "test: loss =  0.0658, R2 = 0.9341945648193359\n",
      "epoch: 1217, loss =  0.0658, R2 = 0.9341945648193359\n",
      "test: loss =  0.0655, R2 = 0.9345231652259827\n",
      "epoch: 1218, loss =  0.0655, R2 = 0.9345231652259827\n",
      "test: loss =  0.0650, R2 = 0.9350289106369019\n",
      "epoch: 1219, loss =  0.0650, R2 = 0.9350289106369019\n",
      "test: loss =  0.0647, R2 = 0.9352876543998718\n",
      "epoch: 1220, loss =  0.0647, R2 = 0.9352876543998718\n",
      "test: loss =  0.0648, R2 = 0.9351673722267151\n",
      "epoch: 1221, loss =  0.0648, R2 = 0.9351673722267151\n",
      "test: loss =  0.0651, R2 = 0.9348965287208557\n",
      "epoch: 1222, loss =  0.0651, R2 = 0.9348965287208557\n",
      "test: loss =  0.0652, R2 = 0.9347996711730957\n",
      "epoch: 1223, loss =  0.0652, R2 = 0.9347996711730957\n",
      "test: loss =  0.0650, R2 = 0.9349884986877441\n",
      "epoch: 1224, loss =  0.0650, R2 = 0.9349884986877441\n",
      "test: loss =  0.0647, R2 = 0.9352943897247314\n",
      "epoch: 1225, loss =  0.0647, R2 = 0.9352943897247314\n",
      "test: loss =  0.0645, R2 = 0.9354618191719055\n",
      "epoch: 1226, loss =  0.0645, R2 = 0.9354618191719055\n",
      "test: loss =  0.0646, R2 = 0.9353957772254944\n",
      "epoch: 1227, loss =  0.0646, R2 = 0.9353957772254944\n",
      "test: loss =  0.0648, R2 = 0.9352203011512756\n",
      "epoch: 1228, loss =  0.0648, R2 = 0.9352203011512756\n",
      "test: loss =  0.0648, R2 = 0.9351335167884827\n",
      "epoch: 1229, loss =  0.0648, R2 = 0.9351335167884827\n",
      "test: loss =  0.0648, R2 = 0.9352253675460815\n",
      "epoch: 1230, loss =  0.0648, R2 = 0.9352253675460815\n",
      "test: loss =  0.0646, R2 = 0.9354152679443359\n",
      "epoch: 1231, loss =  0.0646, R2 = 0.9354152679443359\n",
      "test: loss =  0.0644, R2 = 0.9355497360229492\n",
      "epoch: 1232, loss =  0.0644, R2 = 0.9355497360229492\n",
      "test: loss =  0.0644, R2 = 0.9355459809303284\n",
      "epoch: 1233, loss =  0.0644, R2 = 0.9355459809303284\n",
      "test: loss =  0.0645, R2 = 0.9354498982429504\n",
      "epoch: 1234, loss =  0.0645, R2 = 0.9354498982429504\n",
      "test: loss =  0.0646, R2 = 0.9353740811347961\n",
      "epoch: 1235, loss =  0.0646, R2 = 0.9353740811347961\n",
      "test: loss =  0.0646, R2 = 0.9353927373886108\n",
      "epoch: 1236, loss =  0.0646, R2 = 0.9353927373886108\n",
      "test: loss =  0.0645, R2 = 0.9354889392852783\n",
      "epoch: 1237, loss =  0.0645, R2 = 0.9354889392852783\n",
      "test: loss =  0.0644, R2 = 0.9355850219726562\n",
      "epoch: 1238, loss =  0.0644, R2 = 0.9355850219726562\n",
      "test: loss =  0.0644, R2 = 0.9356179237365723\n",
      "epoch: 1239, loss =  0.0644, R2 = 0.9356179237365723\n",
      "test: loss =  0.0644, R2 = 0.935587465763092\n",
      "epoch: 1240, loss =  0.0644, R2 = 0.935587465763092\n",
      "test: loss =  0.0644, R2 = 0.9355431199073792\n",
      "epoch: 1241, loss =  0.0644, R2 = 0.9355431199073792\n",
      "test: loss =  0.0644, R2 = 0.9355342388153076\n",
      "epoch: 1242, loss =  0.0644, R2 = 0.9355342388153076\n",
      "test: loss =  0.0644, R2 = 0.9355710744857788\n",
      "epoch: 1243, loss =  0.0644, R2 = 0.9355710744857788\n",
      "test: loss =  0.0644, R2 = 0.9356253743171692\n",
      "epoch: 1244, loss =  0.0644, R2 = 0.9356253743171692\n",
      "test: loss =  0.0643, R2 = 0.9356608390808105\n",
      "epoch: 1245, loss =  0.0643, R2 = 0.9356608390808105\n",
      "test: loss =  0.0643, R2 = 0.9356632828712463\n",
      "epoch: 1246, loss =  0.0643, R2 = 0.9356632828712463\n",
      "test: loss =  0.0643, R2 = 0.9356471300125122\n",
      "epoch: 1247, loss =  0.0643, R2 = 0.9356471300125122\n",
      "test: loss =  0.0643, R2 = 0.9356377720832825\n",
      "epoch: 1248, loss =  0.0643, R2 = 0.9356377720832825\n",
      "test: loss =  0.0643, R2 = 0.9356500506401062\n",
      "epoch: 1249, loss =  0.0643, R2 = 0.9356500506401062\n",
      "test: loss =  0.0643, R2 = 0.9356788396835327\n",
      "epoch: 1250, loss =  0.0643, R2 = 0.9356788396835327\n",
      "test: loss =  0.0643, R2 = 0.9357072710990906\n",
      "epoch: 1251, loss =  0.0643, R2 = 0.9357072710990906\n",
      "test: loss =  0.0643, R2 = 0.935721755027771\n",
      "epoch: 1252, loss =  0.0643, R2 = 0.935721755027771\n",
      "test: loss =  0.0643, R2 = 0.9357213377952576\n",
      "epoch: 1253, loss =  0.0643, R2 = 0.9357213377952576\n",
      "test: loss =  0.0643, R2 = 0.9357160925865173\n",
      "epoch: 1254, loss =  0.0643, R2 = 0.9357160925865173\n",
      "test: loss =  0.0643, R2 = 0.9357176423072815\n",
      "epoch: 1255, loss =  0.0643, R2 = 0.9357176423072815\n",
      "test: loss =  0.0643, R2 = 0.935730516910553\n",
      "epoch: 1256, loss =  0.0643, R2 = 0.935730516910553\n",
      "test: loss =  0.0642, R2 = 0.935750424861908\n",
      "epoch: 1257, loss =  0.0642, R2 = 0.935750424861908\n",
      "test: loss =  0.0642, R2 = 0.9357689619064331\n",
      "epoch: 1258, loss =  0.0642, R2 = 0.9357689619064331\n",
      "test: loss =  0.0642, R2 = 0.9357801675796509\n",
      "epoch: 1259, loss =  0.0642, R2 = 0.9357801675796509\n",
      "test: loss =  0.0642, R2 = 0.9357840418815613\n",
      "epoch: 1260, loss =  0.0642, R2 = 0.9357840418815613\n",
      "test: loss =  0.0642, R2 = 0.9357852935791016\n",
      "epoch: 1261, loss =  0.0642, R2 = 0.9357852935791016\n",
      "test: loss =  0.0642, R2 = 0.935789167881012\n",
      "epoch: 1262, loss =  0.0642, R2 = 0.935789167881012\n",
      "test: loss =  0.0642, R2 = 0.9357982277870178\n",
      "epoch: 1263, loss =  0.0642, R2 = 0.9357982277870178\n",
      "test: loss =  0.0642, R2 = 0.9358110427856445\n",
      "epoch: 1264, loss =  0.0642, R2 = 0.9358110427856445\n",
      "test: loss =  0.0642, R2 = 0.9358242750167847\n",
      "epoch: 1265, loss =  0.0642, R2 = 0.9358242750167847\n",
      "test: loss =  0.0641, R2 = 0.9358350038528442\n",
      "epoch: 1266, loss =  0.0641, R2 = 0.9358350038528442\n",
      "test: loss =  0.0641, R2 = 0.9358424544334412\n",
      "epoch: 1267, loss =  0.0641, R2 = 0.9358424544334412\n",
      "test: loss =  0.0641, R2 = 0.9358479380607605\n",
      "epoch: 1268, loss =  0.0641, R2 = 0.9358479380607605\n",
      "test: loss =  0.0641, R2 = 0.9358537197113037\n",
      "epoch: 1269, loss =  0.0641, R2 = 0.9358537197113037\n",
      "test: loss =  0.0641, R2 = 0.935861349105835\n",
      "epoch: 1270, loss =  0.0641, R2 = 0.935861349105835\n",
      "test: loss =  0.0641, R2 = 0.9358708262443542\n",
      "epoch: 1271, loss =  0.0641, R2 = 0.9358708262443542\n",
      "test: loss =  0.0641, R2 = 0.9358811378479004\n",
      "epoch: 1272, loss =  0.0641, R2 = 0.9358811378479004\n",
      "test: loss =  0.0641, R2 = 0.9358909130096436\n",
      "epoch: 1273, loss =  0.0641, R2 = 0.9358909130096436\n",
      "test: loss =  0.0641, R2 = 0.9358993768692017\n",
      "epoch: 1274, loss =  0.0641, R2 = 0.9358993768692017\n",
      "test: loss =  0.0641, R2 = 0.9359066486358643\n",
      "epoch: 1275, loss =  0.0641, R2 = 0.9359066486358643\n",
      "test: loss =  0.0641, R2 = 0.9359133839607239\n",
      "epoch: 1276, loss =  0.0641, R2 = 0.9359133839607239\n",
      "test: loss =  0.0641, R2 = 0.9359204769134521\n",
      "epoch: 1277, loss =  0.0641, R2 = 0.9359204769134521\n",
      "test: loss =  0.0641, R2 = 0.935928463935852\n",
      "epoch: 1278, loss =  0.0641, R2 = 0.935928463935852\n",
      "test: loss =  0.0640, R2 = 0.935937225818634\n",
      "epoch: 1279, loss =  0.0640, R2 = 0.935937225818634\n",
      "test: loss =  0.0640, R2 = 0.9359463453292847\n",
      "epoch: 1280, loss =  0.0640, R2 = 0.9359463453292847\n",
      "test: loss =  0.0640, R2 = 0.935955286026001\n",
      "epoch: 1281, loss =  0.0640, R2 = 0.935955286026001\n",
      "test: loss =  0.0640, R2 = 0.9359637498855591\n",
      "epoch: 1282, loss =  0.0640, R2 = 0.9359637498855591\n",
      "test: loss =  0.0640, R2 = 0.9359716773033142\n",
      "epoch: 1283, loss =  0.0640, R2 = 0.9359716773033142\n",
      "test: loss =  0.0640, R2 = 0.9359792470932007\n",
      "epoch: 1284, loss =  0.0640, R2 = 0.9359792470932007\n",
      "test: loss =  0.0640, R2 = 0.9359867572784424\n",
      "epoch: 1285, loss =  0.0640, R2 = 0.9359867572784424\n",
      "test: loss =  0.0640, R2 = 0.935994565486908\n",
      "epoch: 1286, loss =  0.0640, R2 = 0.935994565486908\n",
      "test: loss =  0.0640, R2 = 0.9360026717185974\n",
      "epoch: 1287, loss =  0.0640, R2 = 0.9360026717185974\n",
      "test: loss =  0.0640, R2 = 0.9360110759735107\n",
      "epoch: 1288, loss =  0.0640, R2 = 0.9360110759735107\n",
      "test: loss =  0.0640, R2 = 0.9360194802284241\n",
      "epoch: 1289, loss =  0.0640, R2 = 0.9360194802284241\n",
      "test: loss =  0.0640, R2 = 0.9360278248786926\n",
      "epoch: 1290, loss =  0.0640, R2 = 0.9360278248786926\n",
      "test: loss =  0.0639, R2 = 0.9360359311103821\n",
      "epoch: 1291, loss =  0.0639, R2 = 0.9360359311103821\n",
      "test: loss =  0.0639, R2 = 0.936043918132782\n",
      "epoch: 1292, loss =  0.0639, R2 = 0.936043918132782\n",
      "test: loss =  0.0639, R2 = 0.9360517859458923\n",
      "epoch: 1293, loss =  0.0639, R2 = 0.9360517859458923\n",
      "test: loss =  0.0639, R2 = 0.9360595941543579\n",
      "epoch: 1294, loss =  0.0639, R2 = 0.9360595941543579\n",
      "test: loss =  0.0639, R2 = 0.9360675811767578\n",
      "epoch: 1295, loss =  0.0639, R2 = 0.9360675811767578\n",
      "test: loss =  0.0639, R2 = 0.9360755681991577\n",
      "epoch: 1296, loss =  0.0639, R2 = 0.9360755681991577\n",
      "test: loss =  0.0639, R2 = 0.9360837340354919\n",
      "epoch: 1297, loss =  0.0639, R2 = 0.9360837340354919\n",
      "test: loss =  0.0639, R2 = 0.9360918998718262\n",
      "epoch: 1298, loss =  0.0639, R2 = 0.9360918998718262\n",
      "test: loss =  0.0639, R2 = 0.9361001253128052\n",
      "epoch: 1299, loss =  0.0639, R2 = 0.9361001253128052\n",
      "test: loss =  0.0639, R2 = 0.9361082911491394\n",
      "epoch: 1300, loss =  0.0639, R2 = 0.9361082911491394\n",
      "test: loss =  0.0639, R2 = 0.9361163973808289\n",
      "epoch: 1301, loss =  0.0639, R2 = 0.9361163973808289\n",
      "test: loss =  0.0639, R2 = 0.9361244440078735\n",
      "epoch: 1302, loss =  0.0639, R2 = 0.9361244440078735\n",
      "test: loss =  0.0638, R2 = 0.9361324310302734\n",
      "epoch: 1303, loss =  0.0638, R2 = 0.9361324310302734\n",
      "test: loss =  0.0638, R2 = 0.9361404180526733\n",
      "epoch: 1304, loss =  0.0638, R2 = 0.9361404180526733\n",
      "test: loss =  0.0638, R2 = 0.9361484050750732\n",
      "epoch: 1305, loss =  0.0638, R2 = 0.9361484050750732\n",
      "test: loss =  0.0638, R2 = 0.9361564517021179\n",
      "epoch: 1306, loss =  0.0638, R2 = 0.9361564517021179\n",
      "test: loss =  0.0638, R2 = 0.9361644387245178\n",
      "epoch: 1307, loss =  0.0638, R2 = 0.9361644387245178\n",
      "test: loss =  0.0638, R2 = 0.9361724853515625\n",
      "epoch: 1308, loss =  0.0638, R2 = 0.9361724853515625\n",
      "test: loss =  0.0638, R2 = 0.9361805319786072\n",
      "epoch: 1309, loss =  0.0638, R2 = 0.9361805319786072\n",
      "test: loss =  0.0638, R2 = 0.9361885786056519\n",
      "epoch: 1310, loss =  0.0638, R2 = 0.9361885786056519\n",
      "test: loss =  0.0638, R2 = 0.9361965656280518\n",
      "epoch: 1311, loss =  0.0638, R2 = 0.9361965656280518\n",
      "test: loss =  0.0638, R2 = 0.9362044334411621\n",
      "epoch: 1312, loss =  0.0638, R2 = 0.9362044334411621\n",
      "test: loss =  0.0638, R2 = 0.9362121820449829\n",
      "epoch: 1313, loss =  0.0638, R2 = 0.9362121820449829\n",
      "test: loss =  0.0638, R2 = 0.9362198710441589\n",
      "epoch: 1314, loss =  0.0638, R2 = 0.9362198710441589\n",
      "test: loss =  0.0638, R2 = 0.9362273216247559\n",
      "epoch: 1315, loss =  0.0638, R2 = 0.9362273216247559\n",
      "test: loss =  0.0637, R2 = 0.9362345337867737\n",
      "epoch: 1316, loss =  0.0637, R2 = 0.9362345337867737\n",
      "test: loss =  0.0637, R2 = 0.9362413883209229\n",
      "epoch: 1317, loss =  0.0637, R2 = 0.9362413883209229\n",
      "test: loss =  0.0637, R2 = 0.9362476468086243\n",
      "epoch: 1318, loss =  0.0637, R2 = 0.9362476468086243\n",
      "test: loss =  0.0637, R2 = 0.9362530708312988\n",
      "epoch: 1319, loss =  0.0637, R2 = 0.9362530708312988\n",
      "test: loss =  0.0637, R2 = 0.9362573027610779\n",
      "epoch: 1320, loss =  0.0637, R2 = 0.9362573027610779\n",
      "test: loss =  0.0637, R2 = 0.9362595677375793\n",
      "epoch: 1321, loss =  0.0637, R2 = 0.9362595677375793\n",
      "test: loss =  0.0637, R2 = 0.9362589716911316\n",
      "epoch: 1322, loss =  0.0637, R2 = 0.9362589716911316\n",
      "test: loss =  0.0637, R2 = 0.9362539052963257\n",
      "epoch: 1323, loss =  0.0637, R2 = 0.9362539052963257\n",
      "test: loss =  0.0637, R2 = 0.936241865158081\n",
      "epoch: 1324, loss =  0.0637, R2 = 0.936241865158081\n",
      "test: loss =  0.0638, R2 = 0.9362191557884216\n",
      "epoch: 1325, loss =  0.0638, R2 = 0.9362191557884216\n",
      "test: loss =  0.0638, R2 = 0.9361795783042908\n",
      "epoch: 1326, loss =  0.0638, R2 = 0.9361795783042908\n",
      "test: loss =  0.0639, R2 = 0.936113715171814\n",
      "epoch: 1327, loss =  0.0639, R2 = 0.936113715171814\n",
      "test: loss =  0.0640, R2 = 0.9360068440437317\n",
      "epoch: 1328, loss =  0.0640, R2 = 0.9360068440437317\n",
      "test: loss =  0.0641, R2 = 0.9358364939689636\n",
      "epoch: 1329, loss =  0.0641, R2 = 0.9358364939689636\n",
      "test: loss =  0.0644, R2 = 0.9355692267417908\n",
      "epoch: 1330, loss =  0.0644, R2 = 0.9355692267417908\n",
      "test: loss =  0.0648, R2 = 0.9351591467857361\n",
      "epoch: 1331, loss =  0.0648, R2 = 0.9351591467857361\n",
      "test: loss =  0.0654, R2 = 0.9345496892929077\n",
      "epoch: 1332, loss =  0.0654, R2 = 0.9345496892929077\n",
      "test: loss =  0.0663, R2 = 0.9336912631988525\n",
      "epoch: 1333, loss =  0.0663, R2 = 0.9336912631988525\n",
      "test: loss =  0.0674, R2 = 0.9325830340385437\n",
      "epoch: 1334, loss =  0.0674, R2 = 0.9325830340385437\n",
      "test: loss =  0.0686, R2 = 0.9313574433326721\n",
      "epoch: 1335, loss =  0.0686, R2 = 0.9313574433326721\n",
      "test: loss =  0.0696, R2 = 0.9303538799285889\n",
      "epoch: 1336, loss =  0.0696, R2 = 0.9303538799285889\n",
      "test: loss =  0.0699, R2 = 0.9300816655158997\n",
      "epoch: 1337, loss =  0.0699, R2 = 0.9300816655158997\n",
      "test: loss =  0.0691, R2 = 0.9308953285217285\n",
      "epoch: 1338, loss =  0.0691, R2 = 0.9308953285217285\n",
      "test: loss =  0.0674, R2 = 0.9325763583183289\n",
      "epoch: 1339, loss =  0.0674, R2 = 0.9325763583183289\n",
      "test: loss =  0.0657, R2 = 0.9343022704124451\n",
      "epoch: 1340, loss =  0.0657, R2 = 0.9343022704124451\n",
      "test: loss =  0.0647, R2 = 0.9352594614028931\n",
      "epoch: 1341, loss =  0.0647, R2 = 0.9352594614028931\n",
      "test: loss =  0.0647, R2 = 0.9352784752845764\n",
      "epoch: 1342, loss =  0.0647, R2 = 0.9352784752845764\n",
      "test: loss =  0.0651, R2 = 0.9348734617233276\n",
      "epoch: 1343, loss =  0.0651, R2 = 0.9348734617233276\n",
      "test: loss =  0.0653, R2 = 0.9347066283226013\n",
      "epoch: 1344, loss =  0.0653, R2 = 0.9347066283226013\n",
      "test: loss =  0.0650, R2 = 0.9350136518478394\n",
      "epoch: 1345, loss =  0.0650, R2 = 0.9350136518478394\n",
      "test: loss =  0.0645, R2 = 0.93547123670578\n",
      "epoch: 1346, loss =  0.0645, R2 = 0.93547123670578\n",
      "test: loss =  0.0644, R2 = 0.9356317520141602\n",
      "epoch: 1347, loss =  0.0644, R2 = 0.9356317520141602\n",
      "test: loss =  0.0646, R2 = 0.9354292750358582\n",
      "epoch: 1348, loss =  0.0646, R2 = 0.9354292750358582\n",
      "test: loss =  0.0648, R2 = 0.9352189302444458\n",
      "epoch: 1349, loss =  0.0648, R2 = 0.9352189302444458\n",
      "test: loss =  0.0646, R2 = 0.9353782534599304\n",
      "epoch: 1350, loss =  0.0646, R2 = 0.9353782534599304\n",
      "test: loss =  0.0641, R2 = 0.9358798861503601\n",
      "epoch: 1351, loss =  0.0641, R2 = 0.9358798861503601\n",
      "test: loss =  0.0637, R2 = 0.9363194704055786\n",
      "epoch: 1352, loss =  0.0637, R2 = 0.9363194704055786\n",
      "test: loss =  0.0636, R2 = 0.9363536834716797\n",
      "epoch: 1353, loss =  0.0636, R2 = 0.9363536834716797\n",
      "test: loss =  0.0639, R2 = 0.9360522627830505\n",
      "epoch: 1354, loss =  0.0639, R2 = 0.9360522627830505\n",
      "test: loss =  0.0642, R2 = 0.9357844591140747\n",
      "epoch: 1355, loss =  0.0642, R2 = 0.9357844591140747\n",
      "test: loss =  0.0642, R2 = 0.9358290433883667\n",
      "epoch: 1356, loss =  0.0642, R2 = 0.9358290433883667\n",
      "test: loss =  0.0639, R2 = 0.9361294507980347\n",
      "epoch: 1357, loss =  0.0639, R2 = 0.9361294507980347\n",
      "test: loss =  0.0636, R2 = 0.9363999366760254\n",
      "epoch: 1358, loss =  0.0636, R2 = 0.9363999366760254\n",
      "test: loss =  0.0635, R2 = 0.9364487528800964\n",
      "epoch: 1359, loss =  0.0635, R2 = 0.9364487528800964\n",
      "test: loss =  0.0636, R2 = 0.9363399744033813\n",
      "epoch: 1360, loss =  0.0636, R2 = 0.9363399744033813\n",
      "test: loss =  0.0637, R2 = 0.9362660050392151\n",
      "epoch: 1361, loss =  0.0637, R2 = 0.9362660050392151\n",
      "test: loss =  0.0637, R2 = 0.9363230466842651\n",
      "epoch: 1362, loss =  0.0637, R2 = 0.9363230466842651\n",
      "test: loss =  0.0635, R2 = 0.9364367723464966\n",
      "epoch: 1363, loss =  0.0635, R2 = 0.9364367723464966\n",
      "test: loss =  0.0635, R2 = 0.9364840984344482\n",
      "epoch: 1364, loss =  0.0635, R2 = 0.9364840984344482\n",
      "test: loss =  0.0635, R2 = 0.9364414811134338\n",
      "epoch: 1365, loss =  0.0635, R2 = 0.9364414811134338\n",
      "test: loss =  0.0636, R2 = 0.9363963603973389\n",
      "epoch: 1366, loss =  0.0636, R2 = 0.9363963603973389\n",
      "test: loss =  0.0635, R2 = 0.9364340305328369\n",
      "epoch: 1367, loss =  0.0635, R2 = 0.9364340305328369\n",
      "test: loss =  0.0634, R2 = 0.9365423321723938\n",
      "epoch: 1368, loss =  0.0634, R2 = 0.9365423321723938\n",
      "test: loss =  0.0634, R2 = 0.9366323947906494\n",
      "epoch: 1369, loss =  0.0634, R2 = 0.9366323947906494\n",
      "test: loss =  0.0633, R2 = 0.9366377592086792\n",
      "epoch: 1370, loss =  0.0633, R2 = 0.9366377592086792\n",
      "test: loss =  0.0634, R2 = 0.936578094959259\n",
      "epoch: 1371, loss =  0.0634, R2 = 0.936578094959259\n",
      "test: loss =  0.0635, R2 = 0.9365288019180298\n",
      "epoch: 1372, loss =  0.0635, R2 = 0.9365288019180298\n",
      "test: loss =  0.0634, R2 = 0.9365423917770386\n",
      "epoch: 1373, loss =  0.0634, R2 = 0.9365423917770386\n",
      "test: loss =  0.0634, R2 = 0.9366055130958557\n",
      "epoch: 1374, loss =  0.0634, R2 = 0.9366055130958557\n",
      "test: loss =  0.0633, R2 = 0.9366648197174072\n",
      "epoch: 1375, loss =  0.0633, R2 = 0.9366648197174072\n",
      "test: loss =  0.0633, R2 = 0.9366840124130249\n",
      "epoch: 1376, loss =  0.0633, R2 = 0.9366840124130249\n",
      "test: loss =  0.0633, R2 = 0.9366719722747803\n",
      "epoch: 1377, loss =  0.0633, R2 = 0.9366719722747803\n",
      "test: loss =  0.0633, R2 = 0.9366625547409058\n",
      "epoch: 1378, loss =  0.0633, R2 = 0.9366625547409058\n",
      "test: loss =  0.0633, R2 = 0.9366764426231384\n",
      "epoch: 1379, loss =  0.0633, R2 = 0.9366764426231384\n",
      "test: loss =  0.0633, R2 = 0.9367051720619202\n",
      "epoch: 1380, loss =  0.0633, R2 = 0.9367051720619202\n",
      "test: loss =  0.0633, R2 = 0.9367260932922363\n",
      "epoch: 1381, loss =  0.0633, R2 = 0.9367260932922363\n",
      "test: loss =  0.0633, R2 = 0.936728298664093\n",
      "epoch: 1382, loss =  0.0633, R2 = 0.936728298664093\n",
      "test: loss =  0.0633, R2 = 0.9367210865020752\n",
      "epoch: 1383, loss =  0.0633, R2 = 0.9367210865020752\n",
      "test: loss =  0.0633, R2 = 0.9367218017578125\n",
      "epoch: 1384, loss =  0.0633, R2 = 0.9367218017578125\n",
      "test: loss =  0.0632, R2 = 0.9367384314537048\n",
      "epoch: 1385, loss =  0.0632, R2 = 0.9367384314537048\n",
      "test: loss =  0.0632, R2 = 0.9367634654045105\n",
      "epoch: 1386, loss =  0.0632, R2 = 0.9367634654045105\n",
      "test: loss =  0.0632, R2 = 0.9367836713790894\n",
      "epoch: 1387, loss =  0.0632, R2 = 0.9367836713790894\n",
      "test: loss =  0.0632, R2 = 0.9367923140525818\n",
      "epoch: 1388, loss =  0.0632, R2 = 0.9367923140525818\n",
      "test: loss =  0.0632, R2 = 0.9367935061454773\n",
      "epoch: 1389, loss =  0.0632, R2 = 0.9367935061454773\n",
      "test: loss =  0.0632, R2 = 0.9367961883544922\n",
      "epoch: 1390, loss =  0.0632, R2 = 0.9367961883544922\n",
      "test: loss =  0.0632, R2 = 0.936805248260498\n",
      "epoch: 1391, loss =  0.0632, R2 = 0.936805248260498\n",
      "test: loss =  0.0632, R2 = 0.9368184208869934\n",
      "epoch: 1392, loss =  0.0632, R2 = 0.9368184208869934\n",
      "test: loss =  0.0632, R2 = 0.9368299841880798\n",
      "epoch: 1393, loss =  0.0632, R2 = 0.9368299841880798\n",
      "test: loss =  0.0631, R2 = 0.9368367195129395\n",
      "epoch: 1394, loss =  0.0631, R2 = 0.9368367195129395\n",
      "test: loss =  0.0631, R2 = 0.9368405342102051\n",
      "epoch: 1395, loss =  0.0631, R2 = 0.9368405342102051\n",
      "test: loss =  0.0631, R2 = 0.9368458390235901\n",
      "epoch: 1396, loss =  0.0631, R2 = 0.9368458390235901\n",
      "test: loss =  0.0631, R2 = 0.9368553757667542\n",
      "epoch: 1397, loss =  0.0631, R2 = 0.9368553757667542\n",
      "test: loss =  0.0631, R2 = 0.9368682503700256\n",
      "epoch: 1398, loss =  0.0631, R2 = 0.9368682503700256\n",
      "test: loss =  0.0631, R2 = 0.9368810057640076\n",
      "epoch: 1399, loss =  0.0631, R2 = 0.9368810057640076\n",
      "test: loss =  0.0631, R2 = 0.9368909001350403\n",
      "epoch: 1400, loss =  0.0631, R2 = 0.9368909001350403\n",
      "test: loss =  0.0631, R2 = 0.9368976950645447\n",
      "epoch: 1401, loss =  0.0631, R2 = 0.9368976950645447\n",
      "test: loss =  0.0631, R2 = 0.9369034171104431\n",
      "epoch: 1402, loss =  0.0631, R2 = 0.9369034171104431\n",
      "test: loss =  0.0631, R2 = 0.9369102120399475\n",
      "epoch: 1403, loss =  0.0631, R2 = 0.9369102120399475\n",
      "test: loss =  0.0631, R2 = 0.9369187951087952\n",
      "epoch: 1404, loss =  0.0631, R2 = 0.9369187951087952\n",
      "test: loss =  0.0631, R2 = 0.9369282722473145\n",
      "epoch: 1405, loss =  0.0631, R2 = 0.9369282722473145\n",
      "test: loss =  0.0630, R2 = 0.9369372129440308\n",
      "epoch: 1406, loss =  0.0630, R2 = 0.9369372129440308\n",
      "test: loss =  0.0630, R2 = 0.936944842338562\n",
      "epoch: 1407, loss =  0.0630, R2 = 0.936944842338562\n",
      "test: loss =  0.0630, R2 = 0.9369515180587769\n",
      "epoch: 1408, loss =  0.0630, R2 = 0.9369515180587769\n",
      "test: loss =  0.0630, R2 = 0.936958372592926\n",
      "epoch: 1409, loss =  0.0630, R2 = 0.936958372592926\n",
      "test: loss =  0.0630, R2 = 0.936966061592102\n",
      "epoch: 1410, loss =  0.0630, R2 = 0.936966061592102\n",
      "test: loss =  0.0630, R2 = 0.9369745850563049\n",
      "epoch: 1411, loss =  0.0630, R2 = 0.9369745850563049\n",
      "test: loss =  0.0630, R2 = 0.9369834065437317\n",
      "epoch: 1412, loss =  0.0630, R2 = 0.9369834065437317\n",
      "test: loss =  0.0630, R2 = 0.9369918704032898\n",
      "epoch: 1413, loss =  0.0630, R2 = 0.9369918704032898\n",
      "test: loss =  0.0630, R2 = 0.9369997978210449\n",
      "epoch: 1414, loss =  0.0630, R2 = 0.9369997978210449\n",
      "test: loss =  0.0630, R2 = 0.9370073676109314\n",
      "epoch: 1415, loss =  0.0630, R2 = 0.9370073676109314\n",
      "test: loss =  0.0630, R2 = 0.9370150566101074\n",
      "epoch: 1416, loss =  0.0630, R2 = 0.9370150566101074\n",
      "test: loss =  0.0630, R2 = 0.9370231032371521\n",
      "epoch: 1417, loss =  0.0630, R2 = 0.9370231032371521\n",
      "test: loss =  0.0630, R2 = 0.9370315074920654\n",
      "epoch: 1418, loss =  0.0630, R2 = 0.9370315074920654\n",
      "test: loss =  0.0629, R2 = 0.9370400309562683\n",
      "epoch: 1419, loss =  0.0629, R2 = 0.9370400309562683\n",
      "test: loss =  0.0629, R2 = 0.9370482563972473\n",
      "epoch: 1420, loss =  0.0629, R2 = 0.9370482563972473\n",
      "test: loss =  0.0629, R2 = 0.9370561838150024\n",
      "epoch: 1421, loss =  0.0629, R2 = 0.9370561838150024\n",
      "test: loss =  0.0629, R2 = 0.9370639324188232\n",
      "epoch: 1422, loss =  0.0629, R2 = 0.9370639324188232\n",
      "test: loss =  0.0629, R2 = 0.9370716214179993\n",
      "epoch: 1423, loss =  0.0629, R2 = 0.9370716214179993\n",
      "test: loss =  0.0629, R2 = 0.9370794296264648\n",
      "epoch: 1424, loss =  0.0629, R2 = 0.9370794296264648\n",
      "test: loss =  0.0629, R2 = 0.9370874762535095\n",
      "epoch: 1425, loss =  0.0629, R2 = 0.9370874762535095\n",
      "test: loss =  0.0629, R2 = 0.937095582485199\n",
      "epoch: 1426, loss =  0.0629, R2 = 0.937095582485199\n",
      "test: loss =  0.0629, R2 = 0.9371036291122437\n",
      "epoch: 1427, loss =  0.0629, R2 = 0.9371036291122437\n",
      "test: loss =  0.0629, R2 = 0.9371115565299988\n",
      "epoch: 1428, loss =  0.0629, R2 = 0.9371115565299988\n",
      "test: loss =  0.0629, R2 = 0.9371193051338196\n",
      "epoch: 1429, loss =  0.0629, R2 = 0.9371193051338196\n",
      "test: loss =  0.0629, R2 = 0.9371269345283508\n",
      "epoch: 1430, loss =  0.0629, R2 = 0.9371269345283508\n",
      "test: loss =  0.0628, R2 = 0.9371345639228821\n",
      "epoch: 1431, loss =  0.0628, R2 = 0.9371345639228821\n",
      "test: loss =  0.0628, R2 = 0.9371421337127686\n",
      "epoch: 1432, loss =  0.0628, R2 = 0.9371421337127686\n",
      "test: loss =  0.0628, R2 = 0.937149703502655\n",
      "epoch: 1433, loss =  0.0628, R2 = 0.937149703502655\n",
      "test: loss =  0.0628, R2 = 0.9371570944786072\n",
      "epoch: 1434, loss =  0.0628, R2 = 0.9371570944786072\n",
      "test: loss =  0.0628, R2 = 0.9371641874313354\n",
      "epoch: 1435, loss =  0.0628, R2 = 0.9371641874313354\n",
      "test: loss =  0.0628, R2 = 0.9371709227561951\n",
      "epoch: 1436, loss =  0.0628, R2 = 0.9371709227561951\n",
      "test: loss =  0.0628, R2 = 0.9371770620346069\n",
      "epoch: 1437, loss =  0.0628, R2 = 0.9371770620346069\n",
      "test: loss =  0.0628, R2 = 0.9371823668479919\n",
      "epoch: 1438, loss =  0.0628, R2 = 0.9371823668479919\n",
      "test: loss =  0.0628, R2 = 0.937186598777771\n",
      "epoch: 1439, loss =  0.0628, R2 = 0.937186598777771\n",
      "test: loss =  0.0628, R2 = 0.9371891021728516\n",
      "epoch: 1440, loss =  0.0628, R2 = 0.9371891021728516\n",
      "test: loss =  0.0628, R2 = 0.9371891021728516\n",
      "epoch: 1441, loss =  0.0628, R2 = 0.9371891021728516\n",
      "test: loss =  0.0628, R2 = 0.9371851682662964\n",
      "epoch: 1442, loss =  0.0628, R2 = 0.9371851682662964\n",
      "test: loss =  0.0628, R2 = 0.9371752738952637\n",
      "epoch: 1443, loss =  0.0628, R2 = 0.9371752738952637\n",
      "test: loss =  0.0628, R2 = 0.9371562004089355\n",
      "epoch: 1444, loss =  0.0628, R2 = 0.9371562004089355\n",
      "test: loss =  0.0629, R2 = 0.9371227622032166\n",
      "epoch: 1445, loss =  0.0629, R2 = 0.9371227622032166\n",
      "test: loss =  0.0629, R2 = 0.9370672106742859\n",
      "epoch: 1446, loss =  0.0629, R2 = 0.9370672106742859\n",
      "test: loss =  0.0630, R2 = 0.9369770884513855\n",
      "epoch: 1447, loss =  0.0630, R2 = 0.9369770884513855\n",
      "test: loss =  0.0631, R2 = 0.9368333220481873\n",
      "epoch: 1448, loss =  0.0631, R2 = 0.9368333220481873\n",
      "test: loss =  0.0634, R2 = 0.9366062879562378\n",
      "epoch: 1449, loss =  0.0634, R2 = 0.9366062879562378\n",
      "test: loss =  0.0637, R2 = 0.9362530708312988\n",
      "epoch: 1450, loss =  0.0637, R2 = 0.9362530708312988\n",
      "test: loss =  0.0643, R2 = 0.9357120990753174\n",
      "epoch: 1451, loss =  0.0643, R2 = 0.9357120990753174\n",
      "test: loss =  0.0651, R2 = 0.9349099397659302\n",
      "epoch: 1452, loss =  0.0651, R2 = 0.9349099397659302\n",
      "test: loss =  0.0662, R2 = 0.9337739944458008\n",
      "epoch: 1453, loss =  0.0662, R2 = 0.9337739944458008\n",
      "test: loss =  0.0677, R2 = 0.9323025941848755\n",
      "epoch: 1454, loss =  0.0677, R2 = 0.9323025941848755\n",
      "test: loss =  0.0693, R2 = 0.9306529760360718\n",
      "epoch: 1455, loss =  0.0693, R2 = 0.9306529760360718\n",
      "test: loss =  0.0707, R2 = 0.9293092489242554\n",
      "epoch: 1456, loss =  0.0707, R2 = 0.9293092489242554\n",
      "test: loss =  0.0710, R2 = 0.9289971590042114\n",
      "epoch: 1457, loss =  0.0710, R2 = 0.9289971590042114\n",
      "test: loss =  0.0696, R2 = 0.9303327202796936\n",
      "epoch: 1458, loss =  0.0696, R2 = 0.9303327202796936\n",
      "test: loss =  0.0670, R2 = 0.9330211281776428\n",
      "epoch: 1459, loss =  0.0670, R2 = 0.9330211281776428\n",
      "test: loss =  0.0642, R2 = 0.9357537627220154\n",
      "epoch: 1460, loss =  0.0642, R2 = 0.9357537627220154\n",
      "test: loss =  0.0629, R2 = 0.9370899796485901\n",
      "epoch: 1461, loss =  0.0629, R2 = 0.9370899796485901\n",
      "test: loss =  0.0633, R2 = 0.9366481900215149\n",
      "epoch: 1462, loss =  0.0633, R2 = 0.9366481900215149\n",
      "test: loss =  0.0647, R2 = 0.9352916479110718\n",
      "epoch: 1463, loss =  0.0647, R2 = 0.9352916479110718\n",
      "test: loss =  0.0656, R2 = 0.9343423843383789\n",
      "epoch: 1464, loss =  0.0656, R2 = 0.9343423843383789\n",
      "test: loss =  0.0654, R2 = 0.9346010684967041\n",
      "epoch: 1465, loss =  0.0654, R2 = 0.9346010684967041\n",
      "test: loss =  0.0642, R2 = 0.9357842803001404\n",
      "epoch: 1466, loss =  0.0642, R2 = 0.9357842803001404\n",
      "test: loss =  0.0631, R2 = 0.9368841052055359\n",
      "epoch: 1467, loss =  0.0631, R2 = 0.9368841052055359\n",
      "test: loss =  0.0629, R2 = 0.9371224641799927\n",
      "epoch: 1468, loss =  0.0629, R2 = 0.9371224641799927\n",
      "test: loss =  0.0634, R2 = 0.9365935325622559\n",
      "epoch: 1469, loss =  0.0634, R2 = 0.9365935325622559\n",
      "test: loss =  0.0640, R2 = 0.9360185265541077\n",
      "epoch: 1470, loss =  0.0640, R2 = 0.9360185265541077\n",
      "test: loss =  0.0640, R2 = 0.9360057711601257\n",
      "epoch: 1471, loss =  0.0640, R2 = 0.9360057711601257\n",
      "test: loss =  0.0634, R2 = 0.9365476369857788\n",
      "epoch: 1472, loss =  0.0634, R2 = 0.9365476369857788\n",
      "test: loss =  0.0629, R2 = 0.9371300339698792\n",
      "epoch: 1473, loss =  0.0629, R2 = 0.9371300339698792\n",
      "test: loss =  0.0627, R2 = 0.9372952580451965\n",
      "epoch: 1474, loss =  0.0627, R2 = 0.9372952580451965\n",
      "test: loss =  0.0629, R2 = 0.9370447397232056\n",
      "epoch: 1475, loss =  0.0629, R2 = 0.9370447397232056\n",
      "test: loss =  0.0632, R2 = 0.9367436766624451\n",
      "epoch: 1476, loss =  0.0632, R2 = 0.9367436766624451\n",
      "test: loss =  0.0633, R2 = 0.9367223978042603\n",
      "epoch: 1477, loss =  0.0633, R2 = 0.9367223978042603\n",
      "test: loss =  0.0630, R2 = 0.9369902014732361\n",
      "epoch: 1478, loss =  0.0630, R2 = 0.9369902014732361\n",
      "test: loss =  0.0627, R2 = 0.9372955560684204\n",
      "epoch: 1479, loss =  0.0627, R2 = 0.9372955560684204\n",
      "test: loss =  0.0626, R2 = 0.9374046325683594\n",
      "epoch: 1480, loss =  0.0626, R2 = 0.9374046325683594\n",
      "test: loss =  0.0627, R2 = 0.937305212020874\n",
      "epoch: 1481, loss =  0.0627, R2 = 0.937305212020874\n",
      "test: loss =  0.0628, R2 = 0.9371653199195862\n",
      "epoch: 1482, loss =  0.0628, R2 = 0.9371653199195862\n",
      "test: loss =  0.0628, R2 = 0.9371432662010193\n",
      "epoch: 1483, loss =  0.0628, R2 = 0.9371432662010193\n",
      "test: loss =  0.0627, R2 = 0.9372552037239075\n",
      "epoch: 1484, loss =  0.0627, R2 = 0.9372552037239075\n",
      "test: loss =  0.0626, R2 = 0.937396764755249\n",
      "epoch: 1485, loss =  0.0626, R2 = 0.937396764755249\n",
      "test: loss =  0.0625, R2 = 0.9374647736549377\n",
      "epoch: 1486, loss =  0.0625, R2 = 0.9374647736549377\n",
      "test: loss =  0.0625, R2 = 0.9374451637268066\n",
      "epoch: 1487, loss =  0.0625, R2 = 0.9374451637268066\n",
      "test: loss =  0.0626, R2 = 0.9374003410339355\n",
      "epoch: 1488, loss =  0.0626, R2 = 0.9374003410339355\n",
      "test: loss =  0.0626, R2 = 0.937394380569458\n",
      "epoch: 1489, loss =  0.0626, R2 = 0.937394380569458\n",
      "test: loss =  0.0625, R2 = 0.9374380707740784\n",
      "epoch: 1490, loss =  0.0625, R2 = 0.9374380707740784\n",
      "test: loss =  0.0625, R2 = 0.9374959468841553\n",
      "epoch: 1491, loss =  0.0625, R2 = 0.9374959468841553\n",
      "test: loss =  0.0625, R2 = 0.9375301003456116\n",
      "epoch: 1492, loss =  0.0625, R2 = 0.9375301003456116\n",
      "test: loss =  0.0624, R2 = 0.9375337958335876\n",
      "epoch: 1493, loss =  0.0624, R2 = 0.9375337958335876\n",
      "test: loss =  0.0625, R2 = 0.9375275373458862\n",
      "epoch: 1494, loss =  0.0625, R2 = 0.9375275373458862\n",
      "test: loss =  0.0624, R2 = 0.9375333786010742\n",
      "epoch: 1495, loss =  0.0624, R2 = 0.9375333786010742\n",
      "test: loss =  0.0624, R2 = 0.9375554919242859\n",
      "epoch: 1496, loss =  0.0624, R2 = 0.9375554919242859\n",
      "test: loss =  0.0624, R2 = 0.9375820755958557\n",
      "epoch: 1497, loss =  0.0624, R2 = 0.9375820755958557\n",
      "test: loss =  0.0624, R2 = 0.9376006722450256\n",
      "epoch: 1498, loss =  0.0624, R2 = 0.9376006722450256\n",
      "test: loss =  0.0624, R2 = 0.9376093149185181\n",
      "epoch: 1499, loss =  0.0624, R2 = 0.9376093149185181\n",
      "test: loss =  0.0624, R2 = 0.9376147985458374\n",
      "epoch: 1500, loss =  0.0624, R2 = 0.9376147985458374\n",
      "test: loss =  0.0624, R2 = 0.9376241564750671\n",
      "epoch: 1501, loss =  0.0624, R2 = 0.9376241564750671\n",
      "test: loss =  0.0623, R2 = 0.9376378059387207\n",
      "epoch: 1502, loss =  0.0623, R2 = 0.9376378059387207\n",
      "test: loss =  0.0623, R2 = 0.9376516342163086\n",
      "epoch: 1503, loss =  0.0623, R2 = 0.9376516342163086\n",
      "test: loss =  0.0623, R2 = 0.9376621842384338\n",
      "epoch: 1504, loss =  0.0623, R2 = 0.9376621842384338\n",
      "test: loss =  0.0623, R2 = 0.937670111656189\n",
      "epoch: 1505, loss =  0.0623, R2 = 0.937670111656189\n",
      "test: loss =  0.0623, R2 = 0.9376787543296814\n",
      "epoch: 1506, loss =  0.0623, R2 = 0.9376787543296814\n",
      "test: loss =  0.0623, R2 = 0.9376904368400574\n",
      "epoch: 1507, loss =  0.0623, R2 = 0.9376904368400574\n",
      "test: loss =  0.0623, R2 = 0.9377038478851318\n",
      "epoch: 1508, loss =  0.0623, R2 = 0.9377038478851318\n",
      "test: loss =  0.0623, R2 = 0.9377157092094421\n",
      "epoch: 1509, loss =  0.0623, R2 = 0.9377157092094421\n",
      "test: loss =  0.0623, R2 = 0.9377237558364868\n",
      "epoch: 1510, loss =  0.0623, R2 = 0.9377237558364868\n",
      "test: loss =  0.0623, R2 = 0.9377288818359375\n",
      "epoch: 1511, loss =  0.0623, R2 = 0.9377288818359375\n",
      "test: loss =  0.0622, R2 = 0.9377343654632568\n",
      "epoch: 1512, loss =  0.0622, R2 = 0.9377343654632568\n",
      "test: loss =  0.0622, R2 = 0.9377428293228149\n",
      "epoch: 1513, loss =  0.0622, R2 = 0.9377428293228149\n",
      "test: loss =  0.0622, R2 = 0.9377545118331909\n",
      "epoch: 1514, loss =  0.0622, R2 = 0.9377545118331909\n",
      "test: loss =  0.0622, R2 = 0.9377670288085938\n",
      "epoch: 1515, loss =  0.0622, R2 = 0.9377670288085938\n",
      "test: loss =  0.0622, R2 = 0.9377776384353638\n",
      "epoch: 1516, loss =  0.0622, R2 = 0.9377776384353638\n",
      "test: loss =  0.0622, R2 = 0.9377851486206055\n",
      "epoch: 1517, loss =  0.0622, R2 = 0.9377851486206055\n",
      "test: loss =  0.0622, R2 = 0.9377906322479248\n",
      "epoch: 1518, loss =  0.0622, R2 = 0.9377906322479248\n",
      "test: loss =  0.0622, R2 = 0.9377963542938232\n",
      "epoch: 1519, loss =  0.0622, R2 = 0.9377963542938232\n",
      "test: loss =  0.0622, R2 = 0.9378041625022888\n",
      "epoch: 1520, loss =  0.0622, R2 = 0.9378041625022888\n",
      "test: loss =  0.0622, R2 = 0.9378141164779663\n",
      "epoch: 1521, loss =  0.0622, R2 = 0.9378141164779663\n",
      "test: loss =  0.0622, R2 = 0.9378248453140259\n",
      "epoch: 1522, loss =  0.0622, R2 = 0.9378248453140259\n",
      "test: loss =  0.0621, R2 = 0.937834620475769\n",
      "epoch: 1523, loss =  0.0621, R2 = 0.937834620475769\n",
      "test: loss =  0.0621, R2 = 0.9378424882888794\n",
      "epoch: 1524, loss =  0.0621, R2 = 0.9378424882888794\n",
      "test: loss =  0.0621, R2 = 0.9378488063812256\n",
      "epoch: 1525, loss =  0.0621, R2 = 0.9378488063812256\n",
      "test: loss =  0.0621, R2 = 0.9378548264503479\n",
      "epoch: 1526, loss =  0.0621, R2 = 0.9378548264503479\n",
      "test: loss =  0.0621, R2 = 0.9378618001937866\n",
      "epoch: 1527, loss =  0.0621, R2 = 0.9378618001937866\n",
      "test: loss =  0.0621, R2 = 0.9378702044487\n",
      "epoch: 1528, loss =  0.0621, R2 = 0.9378702044487\n",
      "test: loss =  0.0621, R2 = 0.9378796219825745\n",
      "epoch: 1529, loss =  0.0621, R2 = 0.9378796219825745\n",
      "test: loss =  0.0621, R2 = 0.937889039516449\n",
      "epoch: 1530, loss =  0.0621, R2 = 0.937889039516449\n",
      "test: loss =  0.0621, R2 = 0.9378976821899414\n",
      "epoch: 1531, loss =  0.0621, R2 = 0.9378976821899414\n",
      "test: loss =  0.0621, R2 = 0.9379053115844727\n",
      "epoch: 1532, loss =  0.0621, R2 = 0.9379053115844727\n",
      "test: loss =  0.0621, R2 = 0.9379122853279114\n",
      "epoch: 1533, loss =  0.0621, R2 = 0.9379122853279114\n",
      "test: loss =  0.0621, R2 = 0.9379191398620605\n",
      "epoch: 1534, loss =  0.0621, R2 = 0.9379191398620605\n",
      "test: loss =  0.0621, R2 = 0.9379264712333679\n",
      "epoch: 1535, loss =  0.0621, R2 = 0.9379264712333679\n",
      "test: loss =  0.0620, R2 = 0.9379345178604126\n",
      "epoch: 1536, loss =  0.0620, R2 = 0.9379345178604126\n",
      "test: loss =  0.0620, R2 = 0.9379429817199707\n",
      "epoch: 1537, loss =  0.0620, R2 = 0.9379429817199707\n",
      "test: loss =  0.0620, R2 = 0.9379515647888184\n",
      "epoch: 1538, loss =  0.0620, R2 = 0.9379515647888184\n",
      "test: loss =  0.0620, R2 = 0.9379598498344421\n",
      "epoch: 1539, loss =  0.0620, R2 = 0.9379598498344421\n",
      "test: loss =  0.0620, R2 = 0.9379676580429077\n",
      "epoch: 1540, loss =  0.0620, R2 = 0.9379676580429077\n",
      "test: loss =  0.0620, R2 = 0.9379751086235046\n",
      "epoch: 1541, loss =  0.0620, R2 = 0.9379751086235046\n",
      "test: loss =  0.0620, R2 = 0.9379823803901672\n",
      "epoch: 1542, loss =  0.0620, R2 = 0.9379823803901672\n",
      "test: loss =  0.0620, R2 = 0.9379898309707642\n",
      "epoch: 1543, loss =  0.0620, R2 = 0.9379898309707642\n",
      "test: loss =  0.0620, R2 = 0.9379975199699402\n",
      "epoch: 1544, loss =  0.0620, R2 = 0.9379975199699402\n",
      "test: loss =  0.0620, R2 = 0.9380054473876953\n",
      "epoch: 1545, loss =  0.0620, R2 = 0.9380054473876953\n",
      "test: loss =  0.0620, R2 = 0.9380135536193848\n",
      "epoch: 1546, loss =  0.0620, R2 = 0.9380135536193848\n",
      "test: loss =  0.0620, R2 = 0.9380216002464294\n",
      "epoch: 1547, loss =  0.0620, R2 = 0.9380216002464294\n",
      "test: loss =  0.0620, R2 = 0.9380295276641846\n",
      "epoch: 1548, loss =  0.0620, R2 = 0.9380295276641846\n",
      "test: loss =  0.0619, R2 = 0.9380373358726501\n",
      "epoch: 1549, loss =  0.0619, R2 = 0.9380373358726501\n",
      "test: loss =  0.0619, R2 = 0.9380450248718262\n",
      "epoch: 1550, loss =  0.0619, R2 = 0.9380450248718262\n",
      "test: loss =  0.0619, R2 = 0.9380525946617126\n",
      "epoch: 1551, loss =  0.0619, R2 = 0.9380525946617126\n",
      "test: loss =  0.0619, R2 = 0.9380602240562439\n",
      "epoch: 1552, loss =  0.0619, R2 = 0.9380602240562439\n",
      "test: loss =  0.0619, R2 = 0.9380679130554199\n",
      "epoch: 1553, loss =  0.0619, R2 = 0.9380679130554199\n",
      "test: loss =  0.0619, R2 = 0.9380756616592407\n",
      "epoch: 1554, loss =  0.0619, R2 = 0.9380756616592407\n",
      "test: loss =  0.0619, R2 = 0.9380834698677063\n",
      "epoch: 1555, loss =  0.0619, R2 = 0.9380834698677063\n",
      "test: loss =  0.0619, R2 = 0.9380912780761719\n",
      "epoch: 1556, loss =  0.0619, R2 = 0.9380912780761719\n",
      "test: loss =  0.0619, R2 = 0.9380990862846375\n",
      "epoch: 1557, loss =  0.0619, R2 = 0.9380990862846375\n",
      "test: loss =  0.0619, R2 = 0.9381067752838135\n",
      "epoch: 1558, loss =  0.0619, R2 = 0.9381067752838135\n",
      "test: loss =  0.0619, R2 = 0.9381143450737\n",
      "epoch: 1559, loss =  0.0619, R2 = 0.9381143450737\n",
      "test: loss =  0.0619, R2 = 0.9381218552589417\n",
      "epoch: 1560, loss =  0.0619, R2 = 0.9381218552589417\n",
      "test: loss =  0.0619, R2 = 0.9381292462348938\n",
      "epoch: 1561, loss =  0.0619, R2 = 0.9381292462348938\n",
      "test: loss =  0.0618, R2 = 0.9381365180015564\n",
      "epoch: 1562, loss =  0.0618, R2 = 0.9381365180015564\n",
      "test: loss =  0.0618, R2 = 0.9381435513496399\n",
      "epoch: 1563, loss =  0.0618, R2 = 0.9381435513496399\n",
      "test: loss =  0.0618, R2 = 0.9381502866744995\n",
      "epoch: 1564, loss =  0.0618, R2 = 0.9381502866744995\n",
      "test: loss =  0.0618, R2 = 0.9381566047668457\n",
      "epoch: 1565, loss =  0.0618, R2 = 0.9381566047668457\n",
      "test: loss =  0.0618, R2 = 0.9381622076034546\n",
      "epoch: 1566, loss =  0.0618, R2 = 0.9381622076034546\n",
      "test: loss =  0.0618, R2 = 0.9381667971611023\n",
      "epoch: 1567, loss =  0.0618, R2 = 0.9381667971611023\n",
      "test: loss =  0.0618, R2 = 0.9381697773933411\n",
      "epoch: 1568, loss =  0.0618, R2 = 0.9381697773933411\n",
      "test: loss =  0.0618, R2 = 0.9381704330444336\n",
      "epoch: 1569, loss =  0.0618, R2 = 0.9381704330444336\n",
      "test: loss =  0.0618, R2 = 0.9381674528121948\n",
      "epoch: 1570, loss =  0.0618, R2 = 0.9381674528121948\n",
      "test: loss =  0.0618, R2 = 0.9381589293479919\n",
      "epoch: 1571, loss =  0.0618, R2 = 0.9381589293479919\n",
      "test: loss =  0.0618, R2 = 0.9381418228149414\n",
      "epoch: 1572, loss =  0.0618, R2 = 0.9381418228149414\n",
      "test: loss =  0.0619, R2 = 0.9381113648414612\n",
      "epoch: 1573, loss =  0.0619, R2 = 0.9381113648414612\n",
      "test: loss =  0.0619, R2 = 0.9380602240562439\n",
      "epoch: 1574, loss =  0.0619, R2 = 0.9380602240562439\n",
      "test: loss =  0.0620, R2 = 0.937977135181427\n",
      "epoch: 1575, loss =  0.0620, R2 = 0.937977135181427\n",
      "test: loss =  0.0621, R2 = 0.9378451108932495\n",
      "epoch: 1576, loss =  0.0621, R2 = 0.9378451108932495\n",
      "test: loss =  0.0623, R2 = 0.9376397132873535\n",
      "epoch: 1577, loss =  0.0623, R2 = 0.9376397132873535\n",
      "test: loss =  0.0627, R2 = 0.9373278617858887\n",
      "epoch: 1578, loss =  0.0627, R2 = 0.9373278617858887\n",
      "test: loss =  0.0631, R2 = 0.9368721842765808\n",
      "epoch: 1579, loss =  0.0631, R2 = 0.9368721842765808\n",
      "test: loss =  0.0637, R2 = 0.9362441897392273\n",
      "epoch: 1580, loss =  0.0637, R2 = 0.9362441897392273\n",
      "test: loss =  0.0645, R2 = 0.9354622960090637\n",
      "epoch: 1581, loss =  0.0645, R2 = 0.9354622960090637\n",
      "test: loss =  0.0653, R2 = 0.9346446990966797\n",
      "epoch: 1582, loss =  0.0653, R2 = 0.9346446990966797\n",
      "test: loss =  0.0659, R2 = 0.9340641498565674\n",
      "epoch: 1583, loss =  0.0659, R2 = 0.9340641498565674\n",
      "test: loss =  0.0659, R2 = 0.9340692758560181\n",
      "epoch: 1584, loss =  0.0659, R2 = 0.9340692758560181\n",
      "test: loss =  0.0651, R2 = 0.9348502159118652\n",
      "epoch: 1585, loss =  0.0651, R2 = 0.9348502159118652\n",
      "test: loss =  0.0639, R2 = 0.9361319541931152\n",
      "epoch: 1586, loss =  0.0639, R2 = 0.9361319541931152\n",
      "test: loss =  0.0627, R2 = 0.9372661113739014\n",
      "epoch: 1587, loss =  0.0627, R2 = 0.9372661113739014\n",
      "test: loss =  0.0623, R2 = 0.9377129077911377\n",
      "epoch: 1588, loss =  0.0623, R2 = 0.9377129077911377\n",
      "test: loss =  0.0625, R2 = 0.9374650120735168\n",
      "epoch: 1589, loss =  0.0625, R2 = 0.9374650120735168\n",
      "test: loss =  0.0630, R2 = 0.9369915723800659\n",
      "epoch: 1590, loss =  0.0630, R2 = 0.9369915723800659\n",
      "test: loss =  0.0631, R2 = 0.9368375539779663\n",
      "epoch: 1591, loss =  0.0631, R2 = 0.9368375539779663\n",
      "test: loss =  0.0628, R2 = 0.9372127056121826\n",
      "epoch: 1592, loss =  0.0628, R2 = 0.9372127056121826\n",
      "test: loss =  0.0621, R2 = 0.9378417730331421\n",
      "epoch: 1593, loss =  0.0621, R2 = 0.9378417730331421\n",
      "test: loss =  0.0617, R2 = 0.9382368326187134\n",
      "epoch: 1594, loss =  0.0617, R2 = 0.9382368326187134\n",
      "test: loss =  0.0618, R2 = 0.9381483197212219\n",
      "epoch: 1595, loss =  0.0618, R2 = 0.9381483197212219\n",
      "test: loss =  0.0622, R2 = 0.93775475025177\n",
      "epoch: 1596, loss =  0.0622, R2 = 0.93775475025177\n",
      "test: loss =  0.0625, R2 = 0.9374561309814453\n",
      "epoch: 1597, loss =  0.0625, R2 = 0.9374561309814453\n",
      "test: loss =  0.0625, R2 = 0.9375200271606445\n",
      "epoch: 1598, loss =  0.0625, R2 = 0.9375200271606445\n",
      "test: loss =  0.0621, R2 = 0.9378820061683655\n",
      "epoch: 1599, loss =  0.0621, R2 = 0.9378820061683655\n",
      "test: loss =  0.0617, R2 = 0.9382424354553223\n",
      "epoch: 1600, loss =  0.0617, R2 = 0.9382424354553223\n",
      "test: loss =  0.0616, R2 = 0.9383583664894104\n",
      "epoch: 1601, loss =  0.0616, R2 = 0.9383583664894104\n",
      "test: loss =  0.0617, R2 = 0.938238799571991\n",
      "epoch: 1602, loss =  0.0617, R2 = 0.938238799571991\n",
      "test: loss =  0.0619, R2 = 0.9380831718444824\n",
      "epoch: 1603, loss =  0.0619, R2 = 0.9380831718444824\n",
      "test: loss =  0.0619, R2 = 0.9380710124969482\n",
      "epoch: 1604, loss =  0.0619, R2 = 0.9380710124969482\n",
      "test: loss =  0.0618, R2 = 0.9382122159004211\n",
      "epoch: 1605, loss =  0.0618, R2 = 0.9382122159004211\n",
      "test: loss =  0.0616, R2 = 0.9383728504180908\n",
      "epoch: 1606, loss =  0.0616, R2 = 0.9383728504180908\n",
      "test: loss =  0.0616, R2 = 0.9384217262268066\n",
      "epoch: 1607, loss =  0.0616, R2 = 0.9384217262268066\n",
      "test: loss =  0.0616, R2 = 0.938348114490509\n",
      "epoch: 1608, loss =  0.0616, R2 = 0.938348114490509\n",
      "test: loss =  0.0617, R2 = 0.9382500052452087\n",
      "epoch: 1609, loss =  0.0617, R2 = 0.9382500052452087\n",
      "test: loss =  0.0618, R2 = 0.9382308721542358\n",
      "epoch: 1610, loss =  0.0618, R2 = 0.9382308721542358\n",
      "test: loss =  0.0617, R2 = 0.9383115172386169\n",
      "epoch: 1611, loss =  0.0617, R2 = 0.9383115172386169\n",
      "test: loss =  0.0616, R2 = 0.938427209854126\n",
      "epoch: 1612, loss =  0.0616, R2 = 0.938427209854126\n",
      "test: loss =  0.0615, R2 = 0.9384975433349609\n",
      "epoch: 1613, loss =  0.0615, R2 = 0.9384975433349609\n",
      "test: loss =  0.0615, R2 = 0.9384952783584595\n",
      "epoch: 1614, loss =  0.0615, R2 = 0.9384952783584595\n",
      "test: loss =  0.0615, R2 = 0.9384568929672241\n",
      "epoch: 1615, loss =  0.0615, R2 = 0.9384568929672241\n",
      "test: loss =  0.0615, R2 = 0.9384382963180542\n",
      "epoch: 1616, loss =  0.0615, R2 = 0.9384382963180542\n",
      "test: loss =  0.0615, R2 = 0.9384645819664001\n",
      "epoch: 1617, loss =  0.0615, R2 = 0.9384645819664001\n",
      "test: loss =  0.0615, R2 = 0.9385163187980652\n",
      "epoch: 1618, loss =  0.0615, R2 = 0.9385163187980652\n",
      "test: loss =  0.0614, R2 = 0.938555121421814\n",
      "epoch: 1619, loss =  0.0614, R2 = 0.938555121421814\n",
      "test: loss =  0.0614, R2 = 0.9385594129562378\n",
      "epoch: 1620, loss =  0.0614, R2 = 0.9385594129562378\n",
      "test: loss =  0.0614, R2 = 0.9385390281677246\n",
      "epoch: 1621, loss =  0.0614, R2 = 0.9385390281677246\n",
      "test: loss =  0.0615, R2 = 0.9385213851928711\n",
      "epoch: 1622, loss =  0.0615, R2 = 0.9385213851928711\n",
      "test: loss =  0.0615, R2 = 0.9385266900062561\n",
      "epoch: 1623, loss =  0.0615, R2 = 0.9385266900062561\n",
      "test: loss =  0.0614, R2 = 0.9385538101196289\n",
      "epoch: 1624, loss =  0.0614, R2 = 0.9385538101196289\n",
      "test: loss =  0.0614, R2 = 0.938585638999939\n",
      "epoch: 1625, loss =  0.0614, R2 = 0.938585638999939\n",
      "test: loss =  0.0614, R2 = 0.9386054277420044\n",
      "epoch: 1626, loss =  0.0614, R2 = 0.9386054277420044\n",
      "test: loss =  0.0614, R2 = 0.9386094212532043\n",
      "epoch: 1627, loss =  0.0614, R2 = 0.9386094212532043\n",
      "test: loss =  0.0614, R2 = 0.9386066794395447\n",
      "epoch: 1628, loss =  0.0614, R2 = 0.9386066794395447\n",
      "test: loss =  0.0614, R2 = 0.9386093020439148\n",
      "epoch: 1629, loss =  0.0614, R2 = 0.9386093020439148\n",
      "test: loss =  0.0614, R2 = 0.9386228322982788\n",
      "epoch: 1630, loss =  0.0614, R2 = 0.9386228322982788\n",
      "test: loss =  0.0613, R2 = 0.9386434555053711\n",
      "epoch: 1631, loss =  0.0613, R2 = 0.9386434555053711\n",
      "test: loss =  0.0613, R2 = 0.9386625289916992\n",
      "epoch: 1632, loss =  0.0613, R2 = 0.9386625289916992\n",
      "test: loss =  0.0613, R2 = 0.9386742115020752\n",
      "epoch: 1633, loss =  0.0613, R2 = 0.9386742115020752\n",
      "test: loss =  0.0613, R2 = 0.9386788010597229\n",
      "epoch: 1634, loss =  0.0613, R2 = 0.9386788010597229\n",
      "test: loss =  0.0613, R2 = 0.9386811852455139\n",
      "epoch: 1635, loss =  0.0613, R2 = 0.9386811852455139\n",
      "test: loss =  0.0613, R2 = 0.9386865496635437\n",
      "epoch: 1636, loss =  0.0613, R2 = 0.9386865496635437\n",
      "test: loss =  0.0613, R2 = 0.9386968016624451\n",
      "epoch: 1637, loss =  0.0613, R2 = 0.9386968016624451\n",
      "test: loss =  0.0613, R2 = 0.9387098550796509\n",
      "epoch: 1638, loss =  0.0613, R2 = 0.9387098550796509\n",
      "test: loss =  0.0613, R2 = 0.9387220144271851\n",
      "epoch: 1639, loss =  0.0613, R2 = 0.9387220144271851\n",
      "test: loss =  0.0613, R2 = 0.9387308955192566\n",
      "epoch: 1640, loss =  0.0613, R2 = 0.9387308955192566\n",
      "test: loss =  0.0612, R2 = 0.9387367367744446\n",
      "epoch: 1641, loss =  0.0612, R2 = 0.9387367367744446\n",
      "test: loss =  0.0612, R2 = 0.9387415051460266\n",
      "epoch: 1642, loss =  0.0612, R2 = 0.9387415051460266\n",
      "test: loss =  0.0612, R2 = 0.9387475252151489\n",
      "epoch: 1643, loss =  0.0612, R2 = 0.9387475252151489\n",
      "test: loss =  0.0612, R2 = 0.9387556910514832\n",
      "epoch: 1644, loss =  0.0612, R2 = 0.9387556910514832\n",
      "test: loss =  0.0612, R2 = 0.938765287399292\n",
      "epoch: 1645, loss =  0.0612, R2 = 0.938765287399292\n",
      "test: loss =  0.0612, R2 = 0.938774824142456\n",
      "epoch: 1646, loss =  0.0612, R2 = 0.938774824142456\n",
      "test: loss =  0.0612, R2 = 0.9387831091880798\n",
      "epoch: 1647, loss =  0.0612, R2 = 0.9387831091880798\n",
      "test: loss =  0.0612, R2 = 0.9387897849082947\n",
      "epoch: 1648, loss =  0.0612, R2 = 0.9387897849082947\n",
      "test: loss =  0.0612, R2 = 0.9387956261634827\n",
      "epoch: 1649, loss =  0.0612, R2 = 0.9387956261634827\n",
      "test: loss =  0.0612, R2 = 0.9388014078140259\n",
      "epoch: 1650, loss =  0.0612, R2 = 0.9388014078140259\n",
      "test: loss =  0.0612, R2 = 0.9388077259063721\n",
      "epoch: 1651, loss =  0.0612, R2 = 0.9388077259063721\n",
      "test: loss =  0.0612, R2 = 0.9388143420219421\n",
      "epoch: 1652, loss =  0.0612, R2 = 0.9388143420219421\n",
      "test: loss =  0.0612, R2 = 0.9388206005096436\n",
      "epoch: 1653, loss =  0.0612, R2 = 0.9388206005096436\n",
      "test: loss =  0.0612, R2 = 0.9388256669044495\n",
      "epoch: 1654, loss =  0.0612, R2 = 0.9388256669044495\n",
      "test: loss =  0.0612, R2 = 0.938828706741333\n",
      "epoch: 1655, loss =  0.0612, R2 = 0.938828706741333\n",
      "test: loss =  0.0612, R2 = 0.9388291835784912\n",
      "epoch: 1656, loss =  0.0612, R2 = 0.9388291835784912\n",
      "test: loss =  0.0612, R2 = 0.9388263821601868\n",
      "epoch: 1657, loss =  0.0612, R2 = 0.9388263821601868\n",
      "test: loss =  0.0612, R2 = 0.9388192296028137\n",
      "epoch: 1658, loss =  0.0612, R2 = 0.9388192296028137\n",
      "test: loss =  0.0612, R2 = 0.9388055801391602\n",
      "epoch: 1659, loss =  0.0612, R2 = 0.9388055801391602\n",
      "test: loss =  0.0612, R2 = 0.9387820363044739\n",
      "epoch: 1660, loss =  0.0612, R2 = 0.9387820363044739\n",
      "test: loss =  0.0612, R2 = 0.9387431144714355\n",
      "epoch: 1661, loss =  0.0612, R2 = 0.9387431144714355\n",
      "test: loss =  0.0613, R2 = 0.9386805891990662\n",
      "epoch: 1662, loss =  0.0613, R2 = 0.9386805891990662\n",
      "test: loss =  0.0614, R2 = 0.9385820031166077\n",
      "epoch: 1663, loss =  0.0614, R2 = 0.9385820031166077\n",
      "test: loss =  0.0616, R2 = 0.9384291172027588\n",
      "epoch: 1664, loss =  0.0616, R2 = 0.9384291172027588\n",
      "test: loss =  0.0618, R2 = 0.9381944537162781\n",
      "epoch: 1665, loss =  0.0618, R2 = 0.9381944537162781\n",
      "test: loss =  0.0621, R2 = 0.9378393888473511\n",
      "epoch: 1666, loss =  0.0621, R2 = 0.9378393888473511\n",
      "test: loss =  0.0627, R2 = 0.9373111724853516\n",
      "epoch: 1667, loss =  0.0627, R2 = 0.9373111724853516\n",
      "test: loss =  0.0634, R2 = 0.9365496635437012\n",
      "epoch: 1668, loss =  0.0634, R2 = 0.9365496635437012\n",
      "test: loss =  0.0645, R2 = 0.9355024099349976\n",
      "epoch: 1669, loss =  0.0645, R2 = 0.9355024099349976\n",
      "test: loss =  0.0658, R2 = 0.9341809749603271\n",
      "epoch: 1670, loss =  0.0658, R2 = 0.9341809749603271\n",
      "test: loss =  0.0672, R2 = 0.9327353835105896\n",
      "epoch: 1671, loss =  0.0672, R2 = 0.9327353835105896\n",
      "test: loss =  0.0684, R2 = 0.9315714240074158\n",
      "epoch: 1672, loss =  0.0684, R2 = 0.9315714240074158\n",
      "test: loss =  0.0687, R2 = 0.9312900900840759\n",
      "epoch: 1673, loss =  0.0687, R2 = 0.9312900900840759\n",
      "test: loss =  0.0676, R2 = 0.93240886926651\n",
      "epoch: 1674, loss =  0.0676, R2 = 0.93240886926651\n",
      "test: loss =  0.0652, R2 = 0.9347571730613708\n",
      "epoch: 1675, loss =  0.0652, R2 = 0.9347571730613708\n",
      "test: loss =  0.0627, R2 = 0.9373186826705933\n",
      "epoch: 1676, loss =  0.0627, R2 = 0.9373186826705933\n",
      "test: loss =  0.0612, R2 = 0.9388279318809509\n",
      "epoch: 1677, loss =  0.0612, R2 = 0.9388279318809509\n",
      "test: loss =  0.0612, R2 = 0.9387469291687012\n",
      "epoch: 1678, loss =  0.0612, R2 = 0.9387469291687012\n",
      "test: loss =  0.0624, R2 = 0.9375855922698975\n",
      "epoch: 1679, loss =  0.0624, R2 = 0.9375855922698975\n",
      "test: loss =  0.0636, R2 = 0.9364263415336609\n",
      "epoch: 1680, loss =  0.0636, R2 = 0.9364263415336609\n",
      "test: loss =  0.0638, R2 = 0.9361721277236938\n",
      "epoch: 1681, loss =  0.0638, R2 = 0.9361721277236938\n",
      "test: loss =  0.0630, R2 = 0.9369707703590393\n",
      "epoch: 1682, loss =  0.0630, R2 = 0.9369707703590393\n",
      "test: loss =  0.0618, R2 = 0.9381829500198364\n",
      "epoch: 1683, loss =  0.0618, R2 = 0.9381829500198364\n",
      "test: loss =  0.0610, R2 = 0.9389394521713257\n",
      "epoch: 1684, loss =  0.0610, R2 = 0.9389394521713257\n",
      "test: loss =  0.0611, R2 = 0.9388567209243774\n",
      "epoch: 1685, loss =  0.0611, R2 = 0.9388567209243774\n",
      "test: loss =  0.0617, R2 = 0.9382498264312744\n",
      "epoch: 1686, loss =  0.0617, R2 = 0.9382498264312744\n",
      "test: loss =  0.0622, R2 = 0.937757670879364\n",
      "epoch: 1687, loss =  0.0622, R2 = 0.937757670879364\n",
      "test: loss =  0.0622, R2 = 0.9378032088279724\n",
      "epoch: 1688, loss =  0.0622, R2 = 0.9378032088279724\n",
      "test: loss =  0.0617, R2 = 0.938309371471405\n",
      "epoch: 1689, loss =  0.0617, R2 = 0.938309371471405\n",
      "test: loss =  0.0611, R2 = 0.9388500452041626\n",
      "epoch: 1690, loss =  0.0611, R2 = 0.9388500452041626\n",
      "test: loss =  0.0609, R2 = 0.9390496611595154\n",
      "epoch: 1691, loss =  0.0609, R2 = 0.9390496611595154\n",
      "test: loss =  0.0611, R2 = 0.9388716816902161\n",
      "epoch: 1692, loss =  0.0611, R2 = 0.9388716816902161\n",
      "test: loss =  0.0614, R2 = 0.9385756850242615\n",
      "epoch: 1693, loss =  0.0614, R2 = 0.9385756850242615\n",
      "test: loss =  0.0615, R2 = 0.938452959060669\n",
      "epoch: 1694, loss =  0.0615, R2 = 0.938452959060669\n",
      "test: loss =  0.0614, R2 = 0.9385979175567627\n",
      "epoch: 1695, loss =  0.0614, R2 = 0.9385979175567627\n",
      "test: loss =  0.0611, R2 = 0.9388759732246399\n",
      "epoch: 1696, loss =  0.0611, R2 = 0.9388759732246399\n",
      "test: loss =  0.0609, R2 = 0.9390764236450195\n",
      "epoch: 1697, loss =  0.0609, R2 = 0.9390764236450195\n",
      "test: loss =  0.0609, R2 = 0.9390912055969238\n",
      "epoch: 1698, loss =  0.0609, R2 = 0.9390912055969238\n",
      "test: loss =  0.0610, R2 = 0.9389744400978088\n",
      "epoch: 1699, loss =  0.0610, R2 = 0.9389744400978088\n",
      "test: loss =  0.0611, R2 = 0.938865065574646\n",
      "epoch: 1700, loss =  0.0611, R2 = 0.938865065574646\n",
      "test: loss =  0.0611, R2 = 0.9388636350631714\n",
      "epoch: 1701, loss =  0.0611, R2 = 0.9388636350631714\n",
      "test: loss =  0.0610, R2 = 0.9389669895172119\n",
      "epoch: 1702, loss =  0.0610, R2 = 0.9389669895172119\n",
      "test: loss =  0.0609, R2 = 0.9390944838523865\n",
      "epoch: 1703, loss =  0.0609, R2 = 0.9390944838523865\n",
      "test: loss =  0.0608, R2 = 0.939164936542511\n",
      "epoch: 1704, loss =  0.0608, R2 = 0.939164936542511\n",
      "test: loss =  0.0608, R2 = 0.9391558766365051\n",
      "epoch: 1705, loss =  0.0608, R2 = 0.9391558766365051\n",
      "test: loss =  0.0609, R2 = 0.9391059279441833\n",
      "epoch: 1706, loss =  0.0609, R2 = 0.9391059279441833\n",
      "test: loss =  0.0609, R2 = 0.9390727281570435\n",
      "epoch: 1707, loss =  0.0609, R2 = 0.9390727281570435\n",
      "test: loss =  0.0609, R2 = 0.9390878677368164\n",
      "epoch: 1708, loss =  0.0609, R2 = 0.9390878677368164\n",
      "test: loss =  0.0608, R2 = 0.9391415119171143\n",
      "epoch: 1709, loss =  0.0608, R2 = 0.9391415119171143\n",
      "test: loss =  0.0608, R2 = 0.9391991496086121\n",
      "epoch: 1710, loss =  0.0608, R2 = 0.9391991496086121\n",
      "test: loss =  0.0608, R2 = 0.939231276512146\n",
      "epoch: 1711, loss =  0.0608, R2 = 0.939231276512146\n",
      "test: loss =  0.0608, R2 = 0.9392319917678833\n",
      "epoch: 1712, loss =  0.0608, R2 = 0.9392319917678833\n",
      "test: loss =  0.0608, R2 = 0.9392170310020447\n",
      "epoch: 1713, loss =  0.0608, R2 = 0.9392170310020447\n",
      "test: loss =  0.0608, R2 = 0.9392082691192627\n",
      "epoch: 1714, loss =  0.0608, R2 = 0.9392082691192627\n",
      "test: loss =  0.0608, R2 = 0.9392176866531372\n",
      "epoch: 1715, loss =  0.0608, R2 = 0.9392176866531372\n",
      "test: loss =  0.0607, R2 = 0.9392427206039429\n",
      "epoch: 1716, loss =  0.0607, R2 = 0.9392427206039429\n",
      "test: loss =  0.0607, R2 = 0.9392713308334351\n",
      "epoch: 1717, loss =  0.0607, R2 = 0.9392713308334351\n",
      "test: loss =  0.0607, R2 = 0.9392920732498169\n",
      "epoch: 1718, loss =  0.0607, R2 = 0.9392920732498169\n",
      "test: loss =  0.0607, R2 = 0.9393009543418884\n",
      "epoch: 1719, loss =  0.0607, R2 = 0.9393009543418884\n",
      "test: loss =  0.0607, R2 = 0.9393019080162048\n",
      "epoch: 1720, loss =  0.0607, R2 = 0.9393019080162048\n",
      "test: loss =  0.0607, R2 = 0.9393022060394287\n",
      "epoch: 1721, loss =  0.0607, R2 = 0.9393022060394287\n",
      "test: loss =  0.0607, R2 = 0.9393075704574585\n",
      "epoch: 1722, loss =  0.0607, R2 = 0.9393075704574585\n",
      "test: loss =  0.0607, R2 = 0.9393191337585449\n",
      "epoch: 1723, loss =  0.0607, R2 = 0.9393191337585449\n",
      "test: loss =  0.0606, R2 = 0.9393343329429626\n",
      "epoch: 1724, loss =  0.0606, R2 = 0.9393343329429626\n",
      "test: loss =  0.0606, R2 = 0.9393490552902222\n",
      "epoch: 1725, loss =  0.0606, R2 = 0.9393490552902222\n",
      "test: loss =  0.0606, R2 = 0.9393605589866638\n",
      "epoch: 1726, loss =  0.0606, R2 = 0.9393605589866638\n",
      "test: loss =  0.0606, R2 = 0.9393683075904846\n",
      "epoch: 1727, loss =  0.0606, R2 = 0.9393683075904846\n",
      "test: loss =  0.0606, R2 = 0.9393738508224487\n",
      "epoch: 1728, loss =  0.0606, R2 = 0.9393738508224487\n",
      "test: loss =  0.0606, R2 = 0.9393792748451233\n",
      "epoch: 1729, loss =  0.0606, R2 = 0.9393792748451233\n",
      "test: loss =  0.0606, R2 = 0.9393861293792725\n",
      "epoch: 1730, loss =  0.0606, R2 = 0.9393861293792725\n",
      "test: loss =  0.0606, R2 = 0.9393948912620544\n",
      "epoch: 1731, loss =  0.0606, R2 = 0.9393948912620544\n",
      "test: loss =  0.0606, R2 = 0.9394049048423767\n",
      "epoch: 1732, loss =  0.0606, R2 = 0.9394049048423767\n",
      "test: loss =  0.0606, R2 = 0.9394151568412781\n",
      "epoch: 1733, loss =  0.0606, R2 = 0.9394151568412781\n",
      "test: loss =  0.0606, R2 = 0.9394246935844421\n",
      "epoch: 1734, loss =  0.0606, R2 = 0.9394246935844421\n",
      "test: loss =  0.0606, R2 = 0.939433217048645\n",
      "epoch: 1735, loss =  0.0606, R2 = 0.939433217048645\n",
      "test: loss =  0.0605, R2 = 0.9394407272338867\n",
      "epoch: 1736, loss =  0.0605, R2 = 0.9394407272338867\n",
      "test: loss =  0.0605, R2 = 0.9394476413726807\n",
      "epoch: 1737, loss =  0.0605, R2 = 0.9394476413726807\n",
      "test: loss =  0.0605, R2 = 0.9394545555114746\n",
      "epoch: 1738, loss =  0.0605, R2 = 0.9394545555114746\n",
      "test: loss =  0.0605, R2 = 0.939461886882782\n",
      "epoch: 1739, loss =  0.0605, R2 = 0.939461886882782\n",
      "test: loss =  0.0605, R2 = 0.9394697546958923\n",
      "epoch: 1740, loss =  0.0605, R2 = 0.9394697546958923\n",
      "test: loss =  0.0605, R2 = 0.9394780993461609\n",
      "epoch: 1741, loss =  0.0605, R2 = 0.9394780993461609\n",
      "test: loss =  0.0605, R2 = 0.9394867420196533\n",
      "epoch: 1742, loss =  0.0605, R2 = 0.9394867420196533\n",
      "test: loss =  0.0605, R2 = 0.9394952654838562\n",
      "epoch: 1743, loss =  0.0605, R2 = 0.9394952654838562\n",
      "test: loss =  0.0605, R2 = 0.9395034313201904\n",
      "epoch: 1744, loss =  0.0605, R2 = 0.9395034313201904\n",
      "test: loss =  0.0605, R2 = 0.9395111799240112\n",
      "epoch: 1745, loss =  0.0605, R2 = 0.9395111799240112\n",
      "test: loss =  0.0605, R2 = 0.9395185708999634\n",
      "epoch: 1746, loss =  0.0605, R2 = 0.9395185708999634\n",
      "test: loss =  0.0605, R2 = 0.9395257234573364\n",
      "epoch: 1747, loss =  0.0605, R2 = 0.9395257234573364\n",
      "test: loss =  0.0605, R2 = 0.9395329356193542\n",
      "epoch: 1748, loss =  0.0605, R2 = 0.9395329356193542\n",
      "test: loss =  0.0604, R2 = 0.9395403265953064\n",
      "epoch: 1749, loss =  0.0604, R2 = 0.9395403265953064\n",
      "test: loss =  0.0604, R2 = 0.9395480155944824\n",
      "epoch: 1750, loss =  0.0604, R2 = 0.9395480155944824\n",
      "test: loss =  0.0604, R2 = 0.9395560026168823\n",
      "epoch: 1751, loss =  0.0604, R2 = 0.9395560026168823\n",
      "test: loss =  0.0604, R2 = 0.939564049243927\n",
      "epoch: 1752, loss =  0.0604, R2 = 0.939564049243927\n",
      "test: loss =  0.0604, R2 = 0.9395720958709717\n",
      "epoch: 1753, loss =  0.0604, R2 = 0.9395720958709717\n",
      "test: loss =  0.0604, R2 = 0.9395800232887268\n",
      "epoch: 1754, loss =  0.0604, R2 = 0.9395800232887268\n",
      "test: loss =  0.0604, R2 = 0.9395876526832581\n",
      "epoch: 1755, loss =  0.0604, R2 = 0.9395876526832581\n",
      "test: loss =  0.0604, R2 = 0.9395951628684998\n",
      "epoch: 1756, loss =  0.0604, R2 = 0.9395951628684998\n",
      "test: loss =  0.0604, R2 = 0.9396024942398071\n",
      "epoch: 1757, loss =  0.0604, R2 = 0.9396024942398071\n",
      "test: loss =  0.0604, R2 = 0.9396098256111145\n",
      "epoch: 1758, loss =  0.0604, R2 = 0.9396098256111145\n",
      "test: loss =  0.0604, R2 = 0.9396172165870667\n",
      "epoch: 1759, loss =  0.0604, R2 = 0.9396172165870667\n",
      "test: loss =  0.0604, R2 = 0.9396247267723083\n",
      "epoch: 1760, loss =  0.0604, R2 = 0.9396247267723083\n",
      "test: loss =  0.0604, R2 = 0.9396324157714844\n",
      "epoch: 1761, loss =  0.0604, R2 = 0.9396324157714844\n",
      "test: loss =  0.0603, R2 = 0.9396401643753052\n",
      "epoch: 1762, loss =  0.0603, R2 = 0.9396401643753052\n",
      "test: loss =  0.0603, R2 = 0.9396479725837708\n",
      "epoch: 1763, loss =  0.0603, R2 = 0.9396479725837708\n",
      "test: loss =  0.0603, R2 = 0.9396557211875916\n",
      "epoch: 1764, loss =  0.0603, R2 = 0.9396557211875916\n",
      "test: loss =  0.0603, R2 = 0.9396634101867676\n",
      "epoch: 1765, loss =  0.0603, R2 = 0.9396634101867676\n",
      "test: loss =  0.0603, R2 = 0.939670979976654\n",
      "epoch: 1766, loss =  0.0603, R2 = 0.939670979976654\n",
      "test: loss =  0.0603, R2 = 0.939678430557251\n",
      "epoch: 1767, loss =  0.0603, R2 = 0.939678430557251\n",
      "test: loss =  0.0603, R2 = 0.9396857619285583\n",
      "epoch: 1768, loss =  0.0603, R2 = 0.9396857619285583\n",
      "test: loss =  0.0603, R2 = 0.939693033695221\n",
      "epoch: 1769, loss =  0.0603, R2 = 0.939693033695221\n",
      "test: loss =  0.0603, R2 = 0.9397001266479492\n",
      "epoch: 1770, loss =  0.0603, R2 = 0.9397001266479492\n",
      "test: loss =  0.0603, R2 = 0.9397070407867432\n",
      "epoch: 1771, loss =  0.0603, R2 = 0.9397070407867432\n",
      "test: loss =  0.0603, R2 = 0.9397136569023132\n",
      "epoch: 1772, loss =  0.0603, R2 = 0.9397136569023132\n",
      "test: loss =  0.0603, R2 = 0.9397197365760803\n",
      "epoch: 1773, loss =  0.0603, R2 = 0.9397197365760803\n",
      "test: loss =  0.0603, R2 = 0.9397249817848206\n",
      "epoch: 1774, loss =  0.0603, R2 = 0.9397249817848206\n",
      "test: loss =  0.0603, R2 = 0.9397287368774414\n",
      "epoch: 1775, loss =  0.0603, R2 = 0.9397287368774414\n",
      "test: loss =  0.0603, R2 = 0.9397300481796265\n",
      "epoch: 1776, loss =  0.0603, R2 = 0.9397300481796265\n",
      "test: loss =  0.0603, R2 = 0.9397273063659668\n",
      "epoch: 1777, loss =  0.0603, R2 = 0.9397273063659668\n",
      "test: loss =  0.0603, R2 = 0.9397178888320923\n",
      "epoch: 1778, loss =  0.0603, R2 = 0.9397178888320923\n",
      "test: loss =  0.0603, R2 = 0.9396971464157104\n",
      "epoch: 1779, loss =  0.0603, R2 = 0.9396971464157104\n",
      "test: loss =  0.0603, R2 = 0.9396575689315796\n",
      "epoch: 1780, loss =  0.0603, R2 = 0.9396575689315796\n",
      "test: loss =  0.0604, R2 = 0.9395865201950073\n",
      "epoch: 1781, loss =  0.0604, R2 = 0.9395865201950073\n",
      "test: loss =  0.0605, R2 = 0.939463198184967\n",
      "epoch: 1782, loss =  0.0605, R2 = 0.939463198184967\n",
      "test: loss =  0.0607, R2 = 0.9392551183700562\n",
      "epoch: 1783, loss =  0.0607, R2 = 0.9392551183700562\n",
      "test: loss =  0.0611, R2 = 0.9389140605926514\n",
      "epoch: 1784, loss =  0.0611, R2 = 0.9389140605926514\n",
      "test: loss =  0.0616, R2 = 0.9383804202079773\n",
      "epoch: 1785, loss =  0.0616, R2 = 0.9383804202079773\n",
      "test: loss =  0.0624, R2 = 0.9376055002212524\n",
      "epoch: 1786, loss =  0.0624, R2 = 0.9376055002212524\n",
      "test: loss =  0.0634, R2 = 0.9366270899772644\n",
      "epoch: 1787, loss =  0.0634, R2 = 0.9366270899772644\n",
      "test: loss =  0.0643, R2 = 0.9356836676597595\n",
      "epoch: 1788, loss =  0.0643, R2 = 0.9356836676597595\n",
      "test: loss =  0.0647, R2 = 0.9352951049804688\n",
      "epoch: 1789, loss =  0.0647, R2 = 0.9352951049804688\n",
      "test: loss =  0.0640, R2 = 0.9359744787216187\n",
      "epoch: 1790, loss =  0.0640, R2 = 0.9359744787216187\n",
      "test: loss =  0.0624, R2 = 0.9376211762428284\n",
      "epoch: 1791, loss =  0.0624, R2 = 0.9376211762428284\n",
      "test: loss =  0.0607, R2 = 0.9392454624176025\n",
      "epoch: 1792, loss =  0.0607, R2 = 0.9392454624176025\n",
      "test: loss =  0.0602, R2 = 0.9398053288459778\n",
      "epoch: 1793, loss =  0.0602, R2 = 0.9398053288459778\n",
      "test: loss =  0.0608, R2 = 0.939198911190033\n",
      "epoch: 1794, loss =  0.0608, R2 = 0.939198911190033\n",
      "test: loss =  0.0617, R2 = 0.9382662773132324\n",
      "epoch: 1795, loss =  0.0617, R2 = 0.9382662773132324\n",
      "test: loss =  0.0620, R2 = 0.9379839897155762\n",
      "epoch: 1796, loss =  0.0620, R2 = 0.9379839897155762\n",
      "test: loss =  0.0614, R2 = 0.9386219382286072\n",
      "epoch: 1797, loss =  0.0614, R2 = 0.9386219382286072\n",
      "test: loss =  0.0604, R2 = 0.9395370483398438\n",
      "epoch: 1798, loss =  0.0604, R2 = 0.9395370483398438\n",
      "test: loss =  0.0601, R2 = 0.9398807883262634\n",
      "epoch: 1799, loss =  0.0601, R2 = 0.9398807883262634\n",
      "test: loss =  0.0605, R2 = 0.939501166343689\n",
      "epoch: 1800, loss =  0.0605, R2 = 0.939501166343689\n",
      "test: loss =  0.0610, R2 = 0.9389870762825012\n",
      "epoch: 1801, loss =  0.0610, R2 = 0.9389870762825012\n",
      "test: loss =  0.0610, R2 = 0.9389569163322449\n",
      "epoch: 1802, loss =  0.0610, R2 = 0.9389569163322449\n",
      "test: loss =  0.0606, R2 = 0.9394283294677734\n",
      "epoch: 1803, loss =  0.0606, R2 = 0.9394283294677734\n",
      "test: loss =  0.0601, R2 = 0.9398818016052246\n",
      "epoch: 1804, loss =  0.0601, R2 = 0.9398818016052246\n",
      "test: loss =  0.0601, R2 = 0.9398978352546692\n",
      "epoch: 1805, loss =  0.0601, R2 = 0.9398978352546692\n",
      "test: loss =  0.0604, R2 = 0.9395906925201416\n",
      "epoch: 1806, loss =  0.0604, R2 = 0.9395906925201416\n",
      "test: loss =  0.0606, R2 = 0.9393903613090515\n",
      "epoch: 1807, loss =  0.0606, R2 = 0.9393903613090515\n",
      "test: loss =  0.0605, R2 = 0.9395317435264587\n",
      "epoch: 1808, loss =  0.0605, R2 = 0.9395317435264587\n",
      "test: loss =  0.0601, R2 = 0.9398390054702759\n",
      "epoch: 1809, loss =  0.0601, R2 = 0.9398390054702759\n",
      "test: loss =  0.0600, R2 = 0.9399870038032532\n",
      "epoch: 1810, loss =  0.0600, R2 = 0.9399870038032532\n",
      "test: loss =  0.0601, R2 = 0.939881443977356\n",
      "epoch: 1811, loss =  0.0601, R2 = 0.939881443977356\n",
      "test: loss =  0.0603, R2 = 0.9397167563438416\n",
      "epoch: 1812, loss =  0.0603, R2 = 0.9397167563438416\n",
      "test: loss =  0.0603, R2 = 0.939711332321167\n",
      "epoch: 1813, loss =  0.0603, R2 = 0.939711332321167\n",
      "test: loss =  0.0601, R2 = 0.9398683905601501\n",
      "epoch: 1814, loss =  0.0601, R2 = 0.9398683905601501\n",
      "test: loss =  0.0600, R2 = 0.9400128126144409\n",
      "epoch: 1815, loss =  0.0600, R2 = 0.9400128126144409\n",
      "test: loss =  0.0600, R2 = 0.9400136470794678\n",
      "epoch: 1816, loss =  0.0600, R2 = 0.9400136470794678\n",
      "test: loss =  0.0601, R2 = 0.9399171471595764\n",
      "epoch: 1817, loss =  0.0601, R2 = 0.9399171471595764\n",
      "test: loss =  0.0601, R2 = 0.939863383769989\n",
      "epoch: 1818, loss =  0.0601, R2 = 0.939863383769989\n",
      "test: loss =  0.0601, R2 = 0.9399195313453674\n",
      "epoch: 1819, loss =  0.0601, R2 = 0.9399195313453674\n",
      "test: loss =  0.0600, R2 = 0.9400231242179871\n",
      "epoch: 1820, loss =  0.0600, R2 = 0.9400231242179871\n",
      "test: loss =  0.0599, R2 = 0.940072774887085\n",
      "epoch: 1821, loss =  0.0599, R2 = 0.940072774887085\n",
      "test: loss =  0.0599, R2 = 0.9400426745414734\n",
      "epoch: 1822, loss =  0.0599, R2 = 0.9400426745414734\n",
      "test: loss =  0.0600, R2 = 0.9399945735931396\n",
      "epoch: 1823, loss =  0.0600, R2 = 0.9399945735931396\n",
      "test: loss =  0.0600, R2 = 0.9399960041046143\n",
      "epoch: 1824, loss =  0.0600, R2 = 0.9399960041046143\n",
      "test: loss =  0.0599, R2 = 0.9400493502616882\n",
      "epoch: 1825, loss =  0.0599, R2 = 0.9400493502616882\n",
      "test: loss =  0.0599, R2 = 0.9401021599769592\n",
      "epoch: 1826, loss =  0.0599, R2 = 0.9401021599769592\n",
      "test: loss =  0.0599, R2 = 0.9401122331619263\n",
      "epoch: 1827, loss =  0.0599, R2 = 0.9401122331619263\n",
      "test: loss =  0.0599, R2 = 0.9400888085365295\n",
      "epoch: 1828, loss =  0.0599, R2 = 0.9400888085365295\n",
      "test: loss =  0.0599, R2 = 0.9400730729103088\n",
      "epoch: 1829, loss =  0.0599, R2 = 0.9400730729103088\n",
      "test: loss =  0.0599, R2 = 0.9400901794433594\n",
      "epoch: 1830, loss =  0.0599, R2 = 0.9400901794433594\n",
      "test: loss =  0.0599, R2 = 0.9401273727416992\n",
      "epoch: 1831, loss =  0.0599, R2 = 0.9401273727416992\n",
      "test: loss =  0.0598, R2 = 0.9401540160179138\n",
      "epoch: 1832, loss =  0.0598, R2 = 0.9401540160179138\n",
      "test: loss =  0.0598, R2 = 0.9401557445526123\n",
      "epoch: 1833, loss =  0.0598, R2 = 0.9401557445526123\n",
      "test: loss =  0.0598, R2 = 0.9401450753211975\n",
      "epoch: 1834, loss =  0.0598, R2 = 0.9401450753211975\n",
      "test: loss =  0.0598, R2 = 0.9401439428329468\n",
      "epoch: 1835, loss =  0.0598, R2 = 0.9401439428329468\n",
      "test: loss =  0.0598, R2 = 0.9401604533195496\n",
      "epoch: 1836, loss =  0.0598, R2 = 0.9401604533195496\n",
      "test: loss =  0.0598, R2 = 0.9401838779449463\n",
      "epoch: 1837, loss =  0.0598, R2 = 0.9401838779449463\n",
      "test: loss =  0.0598, R2 = 0.9401987791061401\n",
      "epoch: 1838, loss =  0.0598, R2 = 0.9401987791061401\n",
      "test: loss =  0.0598, R2 = 0.9402008056640625\n",
      "epoch: 1839, loss =  0.0598, R2 = 0.9402008056640625\n",
      "test: loss =  0.0598, R2 = 0.9401983618736267\n",
      "epoch: 1840, loss =  0.0598, R2 = 0.9401983618736267\n",
      "test: loss =  0.0598, R2 = 0.9402024149894714\n",
      "epoch: 1841, loss =  0.0598, R2 = 0.9402024149894714\n",
      "test: loss =  0.0598, R2 = 0.9402156472206116\n",
      "epoch: 1842, loss =  0.0598, R2 = 0.9402156472206116\n",
      "test: loss =  0.0598, R2 = 0.9402316212654114\n",
      "epoch: 1843, loss =  0.0598, R2 = 0.9402316212654114\n",
      "test: loss =  0.0597, R2 = 0.94024258852005\n",
      "epoch: 1844, loss =  0.0597, R2 = 0.94024258852005\n",
      "test: loss =  0.0597, R2 = 0.9402468204498291\n",
      "epoch: 1845, loss =  0.0597, R2 = 0.9402468204498291\n",
      "test: loss =  0.0597, R2 = 0.9402487874031067\n",
      "epoch: 1846, loss =  0.0597, R2 = 0.9402487874031067\n",
      "test: loss =  0.0597, R2 = 0.9402540922164917\n",
      "epoch: 1847, loss =  0.0597, R2 = 0.9402540922164917\n",
      "test: loss =  0.0597, R2 = 0.9402639269828796\n",
      "epoch: 1848, loss =  0.0597, R2 = 0.9402639269828796\n",
      "test: loss =  0.0597, R2 = 0.9402751922607422\n",
      "epoch: 1849, loss =  0.0597, R2 = 0.9402751922607422\n",
      "test: loss =  0.0597, R2 = 0.9402837753295898\n",
      "epoch: 1850, loss =  0.0597, R2 = 0.9402837753295898\n",
      "test: loss =  0.0597, R2 = 0.9402883648872375\n",
      "epoch: 1851, loss =  0.0597, R2 = 0.9402883648872375\n",
      "test: loss =  0.0597, R2 = 0.9402907490730286\n",
      "epoch: 1852, loss =  0.0597, R2 = 0.9402907490730286\n",
      "test: loss =  0.0597, R2 = 0.9402934908866882\n",
      "epoch: 1853, loss =  0.0597, R2 = 0.9402934908866882\n",
      "test: loss =  0.0597, R2 = 0.9402971863746643\n",
      "epoch: 1854, loss =  0.0597, R2 = 0.9402971863746643\n",
      "test: loss =  0.0597, R2 = 0.9402997493743896\n",
      "epoch: 1855, loss =  0.0597, R2 = 0.9402997493743896\n",
      "test: loss =  0.0597, R2 = 0.9402977824211121\n",
      "epoch: 1856, loss =  0.0597, R2 = 0.9402977824211121\n",
      "test: loss =  0.0597, R2 = 0.9402883648872375\n",
      "epoch: 1857, loss =  0.0597, R2 = 0.9402883648872375\n",
      "test: loss =  0.0597, R2 = 0.9402688145637512\n",
      "epoch: 1858, loss =  0.0597, R2 = 0.9402688145637512\n",
      "test: loss =  0.0597, R2 = 0.9402356147766113\n",
      "epoch: 1859, loss =  0.0597, R2 = 0.9402356147766113\n",
      "test: loss =  0.0598, R2 = 0.9401815533638\n",
      "epoch: 1860, loss =  0.0598, R2 = 0.9401815533638\n",
      "test: loss =  0.0599, R2 = 0.9400935173034668\n",
      "epoch: 1861, loss =  0.0599, R2 = 0.9400935173034668\n",
      "test: loss =  0.0600, R2 = 0.9399504065513611\n",
      "epoch: 1862, loss =  0.0600, R2 = 0.9399504065513611\n",
      "test: loss =  0.0603, R2 = 0.939719557762146\n",
      "epoch: 1863, loss =  0.0603, R2 = 0.939719557762146\n",
      "test: loss =  0.0606, R2 = 0.9393530488014221\n",
      "epoch: 1864, loss =  0.0606, R2 = 0.9393530488014221\n",
      "test: loss =  0.0612, R2 = 0.9387814402580261\n",
      "epoch: 1865, loss =  0.0612, R2 = 0.9387814402580261\n",
      "test: loss =  0.0621, R2 = 0.9379146099090576\n",
      "epoch: 1866, loss =  0.0621, R2 = 0.9379146099090576\n",
      "test: loss =  0.0633, R2 = 0.9366559982299805\n",
      "epoch: 1867, loss =  0.0633, R2 = 0.9366559982299805\n",
      "test: loss =  0.0650, R2 = 0.9349659085273743\n",
      "epoch: 1868, loss =  0.0650, R2 = 0.9349659085273743\n",
      "test: loss =  0.0670, R2 = 0.9329867362976074\n",
      "epoch: 1869, loss =  0.0670, R2 = 0.9329867362976074\n",
      "test: loss =  0.0687, R2 = 0.9312365055084229\n",
      "epoch: 1870, loss =  0.0687, R2 = 0.9312365055084229\n",
      "test: loss =  0.0694, R2 = 0.930630087852478\n",
      "epoch: 1871, loss =  0.0694, R2 = 0.930630087852478\n",
      "test: loss =  0.0679, R2 = 0.9320331811904907\n",
      "epoch: 1872, loss =  0.0679, R2 = 0.9320331811904907\n",
      "test: loss =  0.0647, R2 = 0.9352576732635498\n",
      "epoch: 1873, loss =  0.0647, R2 = 0.9352576732635498\n",
      "test: loss =  0.0613, R2 = 0.9386774897575378\n",
      "epoch: 1874, loss =  0.0613, R2 = 0.9386774897575378\n",
      "test: loss =  0.0596, R2 = 0.9403455853462219\n",
      "epoch: 1875, loss =  0.0596, R2 = 0.9403455853462219\n",
      "test: loss =  0.0603, R2 = 0.9396848678588867\n",
      "epoch: 1876, loss =  0.0603, R2 = 0.9396848678588867\n",
      "test: loss =  0.0621, R2 = 0.9378430843353271\n",
      "epoch: 1877, loss =  0.0621, R2 = 0.9378430843353271\n",
      "test: loss =  0.0634, R2 = 0.9366313815116882\n",
      "epoch: 1878, loss =  0.0634, R2 = 0.9366313815116882\n",
      "test: loss =  0.0629, R2 = 0.9371129870414734\n",
      "epoch: 1879, loss =  0.0629, R2 = 0.9371129870414734\n",
      "test: loss =  0.0612, R2 = 0.9388063549995422\n",
      "epoch: 1880, loss =  0.0612, R2 = 0.9388063549995422\n",
      "test: loss =  0.0598, R2 = 0.9402153491973877\n",
      "epoch: 1881, loss =  0.0598, R2 = 0.9402153491973877\n",
      "test: loss =  0.0597, R2 = 0.9403019547462463\n",
      "epoch: 1882, loss =  0.0597, R2 = 0.9403019547462463\n",
      "test: loss =  0.0606, R2 = 0.9393782615661621\n",
      "epoch: 1883, loss =  0.0606, R2 = 0.9393782615661621\n",
      "test: loss =  0.0614, R2 = 0.9385947585105896\n",
      "epoch: 1884, loss =  0.0614, R2 = 0.9385947585105896\n",
      "test: loss =  0.0612, R2 = 0.9387651085853577\n",
      "epoch: 1885, loss =  0.0612, R2 = 0.9387651085853577\n",
      "test: loss =  0.0603, R2 = 0.9396706223487854\n",
      "epoch: 1886, loss =  0.0603, R2 = 0.9396706223487854\n",
      "test: loss =  0.0596, R2 = 0.9404197335243225\n",
      "epoch: 1887, loss =  0.0596, R2 = 0.9404197335243225\n",
      "test: loss =  0.0596, R2 = 0.9404155611991882\n",
      "epoch: 1888, loss =  0.0596, R2 = 0.9404155611991882\n",
      "test: loss =  0.0601, R2 = 0.93988037109375\n",
      "epoch: 1889, loss =  0.0601, R2 = 0.93988037109375\n",
      "test: loss =  0.0605, R2 = 0.9394933581352234\n",
      "epoch: 1890, loss =  0.0605, R2 = 0.9394933581352234\n",
      "test: loss =  0.0603, R2 = 0.9396641254425049\n",
      "epoch: 1891, loss =  0.0603, R2 = 0.9396641254425049\n",
      "test: loss =  0.0598, R2 = 0.9401900172233582\n",
      "epoch: 1892, loss =  0.0598, R2 = 0.9401900172233582\n",
      "test: loss =  0.0594, R2 = 0.9405562877655029\n",
      "epoch: 1893, loss =  0.0594, R2 = 0.9405562877655029\n",
      "test: loss =  0.0595, R2 = 0.9404909014701843\n",
      "epoch: 1894, loss =  0.0595, R2 = 0.9404909014701843\n",
      "test: loss =  0.0598, R2 = 0.940181314945221\n",
      "epoch: 1895, loss =  0.0598, R2 = 0.940181314945221\n",
      "test: loss =  0.0600, R2 = 0.9400076270103455\n",
      "epoch: 1896, loss =  0.0600, R2 = 0.9400076270103455\n",
      "test: loss =  0.0598, R2 = 0.9401472806930542\n",
      "epoch: 1897, loss =  0.0598, R2 = 0.9401472806930542\n",
      "test: loss =  0.0595, R2 = 0.9404450058937073\n",
      "epoch: 1898, loss =  0.0595, R2 = 0.9404450058937073\n",
      "test: loss =  0.0594, R2 = 0.9406225085258484\n",
      "epoch: 1899, loss =  0.0594, R2 = 0.9406225085258484\n",
      "test: loss =  0.0594, R2 = 0.9405652284622192\n",
      "epoch: 1900, loss =  0.0594, R2 = 0.9405652284622192\n",
      "test: loss =  0.0596, R2 = 0.9403979778289795\n",
      "epoch: 1901, loss =  0.0596, R2 = 0.9403979778289795\n",
      "test: loss =  0.0597, R2 = 0.9403213262557983\n",
      "epoch: 1902, loss =  0.0597, R2 = 0.9403213262557983\n",
      "test: loss =  0.0596, R2 = 0.940411388874054\n",
      "epoch: 1903, loss =  0.0596, R2 = 0.940411388874054\n",
      "test: loss =  0.0594, R2 = 0.9405747056007385\n",
      "epoch: 1904, loss =  0.0594, R2 = 0.9405747056007385\n",
      "test: loss =  0.0593, R2 = 0.9406673908233643\n",
      "epoch: 1905, loss =  0.0593, R2 = 0.9406673908233643\n",
      "test: loss =  0.0593, R2 = 0.9406368732452393\n",
      "epoch: 1906, loss =  0.0593, R2 = 0.9406368732452393\n",
      "test: loss =  0.0594, R2 = 0.9405515193939209\n",
      "epoch: 1907, loss =  0.0594, R2 = 0.9405515193939209\n",
      "test: loss =  0.0595, R2 = 0.9405142664909363\n",
      "epoch: 1908, loss =  0.0595, R2 = 0.9405142664909363\n",
      "test: loss =  0.0594, R2 = 0.9405640959739685\n",
      "epoch: 1909, loss =  0.0594, R2 = 0.9405640959739685\n",
      "test: loss =  0.0593, R2 = 0.9406540393829346\n",
      "epoch: 1910, loss =  0.0593, R2 = 0.9406540393829346\n",
      "test: loss =  0.0593, R2 = 0.9407103061676025\n",
      "epoch: 1911, loss =  0.0593, R2 = 0.9407103061676025\n",
      "test: loss =  0.0593, R2 = 0.9407023191452026\n",
      "epoch: 1912, loss =  0.0593, R2 = 0.9407023191452026\n",
      "test: loss =  0.0593, R2 = 0.940660834312439\n",
      "epoch: 1913, loss =  0.0593, R2 = 0.940660834312439\n",
      "test: loss =  0.0593, R2 = 0.9406388401985168\n",
      "epoch: 1914, loss =  0.0593, R2 = 0.9406388401985168\n",
      "test: loss =  0.0593, R2 = 0.9406619668006897\n",
      "epoch: 1915, loss =  0.0593, R2 = 0.9406619668006897\n",
      "test: loss =  0.0593, R2 = 0.9407120943069458\n",
      "epoch: 1916, loss =  0.0593, R2 = 0.9407120943069458\n",
      "test: loss =  0.0592, R2 = 0.9407516121864319\n",
      "epoch: 1917, loss =  0.0592, R2 = 0.9407516121864319\n",
      "test: loss =  0.0592, R2 = 0.9407588243484497\n",
      "epoch: 1918, loss =  0.0592, R2 = 0.9407588243484497\n",
      "test: loss =  0.0592, R2 = 0.9407426118850708\n",
      "epoch: 1919, loss =  0.0592, R2 = 0.9407426118850708\n",
      "test: loss =  0.0593, R2 = 0.9407289624214172\n",
      "epoch: 1920, loss =  0.0593, R2 = 0.9407289624214172\n",
      "test: loss =  0.0592, R2 = 0.9407363533973694\n",
      "epoch: 1921, loss =  0.0592, R2 = 0.9407363533973694\n",
      "test: loss =  0.0592, R2 = 0.9407625794410706\n",
      "epoch: 1922, loss =  0.0592, R2 = 0.9407625794410706\n",
      "test: loss =  0.0592, R2 = 0.9407906532287598\n",
      "epoch: 1923, loss =  0.0592, R2 = 0.9407906532287598\n",
      "test: loss =  0.0592, R2 = 0.9408054351806641\n",
      "epoch: 1924, loss =  0.0592, R2 = 0.9408054351806641\n",
      "test: loss =  0.0592, R2 = 0.9408050179481506\n",
      "epoch: 1925, loss =  0.0592, R2 = 0.9408050179481506\n",
      "test: loss =  0.0592, R2 = 0.9407994747161865\n",
      "epoch: 1926, loss =  0.0592, R2 = 0.9407994747161865\n",
      "test: loss =  0.0592, R2 = 0.9408006072044373\n",
      "epoch: 1927, loss =  0.0592, R2 = 0.9408006072044373\n",
      "test: loss =  0.0592, R2 = 0.9408128261566162\n",
      "epoch: 1928, loss =  0.0592, R2 = 0.9408128261566162\n",
      "test: loss =  0.0592, R2 = 0.9408310055732727\n",
      "epoch: 1929, loss =  0.0592, R2 = 0.9408310055732727\n",
      "test: loss =  0.0591, R2 = 0.9408466815948486\n",
      "epoch: 1930, loss =  0.0591, R2 = 0.9408466815948486\n",
      "test: loss =  0.0591, R2 = 0.9408547878265381\n",
      "epoch: 1931, loss =  0.0591, R2 = 0.9408547878265381\n",
      "test: loss =  0.0591, R2 = 0.9408566951751709\n",
      "epoch: 1932, loss =  0.0591, R2 = 0.9408566951751709\n",
      "test: loss =  0.0591, R2 = 0.940858006477356\n",
      "epoch: 1933, loss =  0.0591, R2 = 0.940858006477356\n",
      "test: loss =  0.0591, R2 = 0.9408634305000305\n",
      "epoch: 1934, loss =  0.0591, R2 = 0.9408634305000305\n",
      "test: loss =  0.0591, R2 = 0.940873920917511\n",
      "epoch: 1935, loss =  0.0591, R2 = 0.940873920917511\n",
      "test: loss =  0.0591, R2 = 0.9408866167068481\n",
      "epoch: 1936, loss =  0.0591, R2 = 0.9408866167068481\n",
      "test: loss =  0.0591, R2 = 0.9408977031707764\n",
      "epoch: 1937, loss =  0.0591, R2 = 0.9408977031707764\n",
      "test: loss =  0.0591, R2 = 0.9409050941467285\n",
      "epoch: 1938, loss =  0.0591, R2 = 0.9409050941467285\n",
      "test: loss =  0.0591, R2 = 0.940909743309021\n",
      "epoch: 1939, loss =  0.0591, R2 = 0.940909743309021\n",
      "test: loss =  0.0591, R2 = 0.9409140348434448\n",
      "epoch: 1940, loss =  0.0591, R2 = 0.9409140348434448\n",
      "test: loss =  0.0591, R2 = 0.9409201741218567\n",
      "epoch: 1941, loss =  0.0591, R2 = 0.9409201741218567\n",
      "test: loss =  0.0591, R2 = 0.9409286379814148\n",
      "epoch: 1942, loss =  0.0591, R2 = 0.9409286379814148\n",
      "test: loss =  0.0590, R2 = 0.9409383535385132\n",
      "epoch: 1943, loss =  0.0590, R2 = 0.9409383535385132\n",
      "test: loss =  0.0590, R2 = 0.9409476518630981\n",
      "epoch: 1944, loss =  0.0590, R2 = 0.9409476518630981\n",
      "test: loss =  0.0590, R2 = 0.9409553408622742\n",
      "epoch: 1945, loss =  0.0590, R2 = 0.9409553408622742\n",
      "test: loss =  0.0590, R2 = 0.9409616589546204\n",
      "epoch: 1946, loss =  0.0590, R2 = 0.9409616589546204\n",
      "test: loss =  0.0590, R2 = 0.9409674406051636\n",
      "epoch: 1947, loss =  0.0590, R2 = 0.9409674406051636\n",
      "test: loss =  0.0590, R2 = 0.9409737586975098\n",
      "epoch: 1948, loss =  0.0590, R2 = 0.9409737586975098\n",
      "test: loss =  0.0590, R2 = 0.9409810900688171\n",
      "epoch: 1949, loss =  0.0590, R2 = 0.9409810900688171\n",
      "test: loss =  0.0590, R2 = 0.9409892559051514\n",
      "epoch: 1950, loss =  0.0590, R2 = 0.9409892559051514\n",
      "test: loss =  0.0590, R2 = 0.9409976005554199\n",
      "epoch: 1951, loss =  0.0590, R2 = 0.9409976005554199\n",
      "test: loss =  0.0590, R2 = 0.9410054683685303\n",
      "epoch: 1952, loss =  0.0590, R2 = 0.9410054683685303\n",
      "test: loss =  0.0590, R2 = 0.9410126209259033\n",
      "epoch: 1953, loss =  0.0590, R2 = 0.9410126209259033\n",
      "test: loss =  0.0590, R2 = 0.9410192966461182\n",
      "epoch: 1954, loss =  0.0590, R2 = 0.9410192966461182\n",
      "test: loss =  0.0590, R2 = 0.9410259127616882\n",
      "epoch: 1955, loss =  0.0590, R2 = 0.9410259127616882\n",
      "test: loss =  0.0590, R2 = 0.9410327672958374\n",
      "epoch: 1956, loss =  0.0590, R2 = 0.9410327672958374\n",
      "test: loss =  0.0589, R2 = 0.9410400390625\n",
      "epoch: 1957, loss =  0.0589, R2 = 0.9410400390625\n",
      "test: loss =  0.0589, R2 = 0.9410476684570312\n",
      "epoch: 1958, loss =  0.0589, R2 = 0.9410476684570312\n",
      "test: loss =  0.0589, R2 = 0.9410553574562073\n",
      "epoch: 1959, loss =  0.0589, R2 = 0.9410553574562073\n",
      "test: loss =  0.0589, R2 = 0.941062867641449\n",
      "epoch: 1960, loss =  0.0589, R2 = 0.941062867641449\n",
      "test: loss =  0.0589, R2 = 0.9410701394081116\n",
      "epoch: 1961, loss =  0.0589, R2 = 0.9410701394081116\n",
      "test: loss =  0.0589, R2 = 0.9410771131515503\n",
      "epoch: 1962, loss =  0.0589, R2 = 0.9410771131515503\n",
      "test: loss =  0.0589, R2 = 0.9410840272903442\n",
      "epoch: 1963, loss =  0.0589, R2 = 0.9410840272903442\n",
      "test: loss =  0.0589, R2 = 0.9410910606384277\n",
      "epoch: 1964, loss =  0.0589, R2 = 0.9410910606384277\n",
      "test: loss =  0.0589, R2 = 0.9410982728004456\n",
      "epoch: 1965, loss =  0.0589, R2 = 0.9410982728004456\n",
      "test: loss =  0.0589, R2 = 0.9411056041717529\n",
      "epoch: 1966, loss =  0.0589, R2 = 0.9411056041717529\n",
      "test: loss =  0.0589, R2 = 0.9411129951477051\n",
      "epoch: 1967, loss =  0.0589, R2 = 0.9411129951477051\n",
      "test: loss =  0.0589, R2 = 0.9411203861236572\n",
      "epoch: 1968, loss =  0.0589, R2 = 0.9411203861236572\n",
      "test: loss =  0.0589, R2 = 0.9411276578903198\n",
      "epoch: 1969, loss =  0.0589, R2 = 0.9411276578903198\n",
      "test: loss =  0.0588, R2 = 0.9411348700523376\n",
      "epoch: 1970, loss =  0.0588, R2 = 0.9411348700523376\n",
      "test: loss =  0.0588, R2 = 0.9411420226097107\n",
      "epoch: 1971, loss =  0.0588, R2 = 0.9411420226097107\n",
      "test: loss =  0.0588, R2 = 0.941149115562439\n",
      "epoch: 1972, loss =  0.0588, R2 = 0.941149115562439\n",
      "test: loss =  0.0588, R2 = 0.941156268119812\n",
      "epoch: 1973, loss =  0.0588, R2 = 0.941156268119812\n",
      "test: loss =  0.0588, R2 = 0.9411634802818298\n",
      "epoch: 1974, loss =  0.0588, R2 = 0.9411634802818298\n",
      "test: loss =  0.0588, R2 = 0.9411707520484924\n",
      "epoch: 1975, loss =  0.0588, R2 = 0.9411707520484924\n",
      "test: loss =  0.0588, R2 = 0.941178023815155\n",
      "epoch: 1976, loss =  0.0588, R2 = 0.941178023815155\n",
      "test: loss =  0.0588, R2 = 0.9411852955818176\n",
      "epoch: 1977, loss =  0.0588, R2 = 0.9411852955818176\n",
      "test: loss =  0.0588, R2 = 0.9411925673484802\n",
      "epoch: 1978, loss =  0.0588, R2 = 0.9411925673484802\n",
      "test: loss =  0.0588, R2 = 0.9411998391151428\n",
      "epoch: 1979, loss =  0.0588, R2 = 0.9411998391151428\n",
      "test: loss =  0.0588, R2 = 0.9412069916725159\n",
      "epoch: 1980, loss =  0.0588, R2 = 0.9412069916725159\n",
      "test: loss =  0.0588, R2 = 0.9412142038345337\n",
      "epoch: 1981, loss =  0.0588, R2 = 0.9412142038345337\n",
      "test: loss =  0.0588, R2 = 0.9412213563919067\n",
      "epoch: 1982, loss =  0.0588, R2 = 0.9412213563919067\n",
      "test: loss =  0.0588, R2 = 0.941228449344635\n",
      "epoch: 1983, loss =  0.0588, R2 = 0.941228449344635\n",
      "test: loss =  0.0587, R2 = 0.9412356019020081\n",
      "epoch: 1984, loss =  0.0587, R2 = 0.9412356019020081\n",
      "test: loss =  0.0587, R2 = 0.9412426948547363\n",
      "epoch: 1985, loss =  0.0587, R2 = 0.9412426948547363\n",
      "test: loss =  0.0587, R2 = 0.9412497282028198\n",
      "epoch: 1986, loss =  0.0587, R2 = 0.9412497282028198\n",
      "test: loss =  0.0587, R2 = 0.9412567019462585\n",
      "epoch: 1987, loss =  0.0587, R2 = 0.9412567019462585\n",
      "test: loss =  0.0587, R2 = 0.9412634968757629\n",
      "epoch: 1988, loss =  0.0587, R2 = 0.9412634968757629\n",
      "test: loss =  0.0587, R2 = 0.9412700533866882\n",
      "epoch: 1989, loss =  0.0587, R2 = 0.9412700533866882\n",
      "test: loss =  0.0587, R2 = 0.9412762522697449\n",
      "epoch: 1990, loss =  0.0587, R2 = 0.9412762522697449\n",
      "test: loss =  0.0587, R2 = 0.9412819147109985\n",
      "epoch: 1991, loss =  0.0587, R2 = 0.9412819147109985\n",
      "test: loss =  0.0587, R2 = 0.9412867426872253\n",
      "epoch: 1992, loss =  0.0587, R2 = 0.9412867426872253\n",
      "test: loss =  0.0587, R2 = 0.9412902593612671\n",
      "epoch: 1993, loss =  0.0587, R2 = 0.9412902593612671\n",
      "test: loss =  0.0587, R2 = 0.9412916302680969\n",
      "epoch: 1994, loss =  0.0587, R2 = 0.9412916302680969\n",
      "test: loss =  0.0587, R2 = 0.9412897229194641\n",
      "epoch: 1995, loss =  0.0587, R2 = 0.9412897229194641\n",
      "test: loss =  0.0587, R2 = 0.9412825703620911\n",
      "epoch: 1996, loss =  0.0587, R2 = 0.9412825703620911\n",
      "test: loss =  0.0587, R2 = 0.9412668347358704\n",
      "epoch: 1997, loss =  0.0587, R2 = 0.9412668347358704\n",
      "test: loss =  0.0587, R2 = 0.9412374496459961\n",
      "epoch: 1998, loss =  0.0587, R2 = 0.9412374496459961\n",
      "test: loss =  0.0588, R2 = 0.9411858320236206\n",
      "epoch: 1999, loss =  0.0588, R2 = 0.9411858320236206\n",
      "test: loss =  0.0589, R2 = 0.9410983920097351\n",
      "epoch: 2000, loss =  0.0589, R2 = 0.9410983920097351\n",
      "test: loss =  0.0590, R2 = 0.9409536123275757\n"
     ]
    }
   ],
   "source": [
    "# 3) training loop\n",
    "epochs = 2000\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    # forward pass and loss\n",
    "    y_predicted = model(feature_train)\n",
    "    loss = criterion(y_predicted, train_target)\n",
    "    r2 = r2_score(y_predicted, train_target)\n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "    # update\n",
    "    optimizer.step()\n",
    "    \n",
    "    # init optimizer\n",
    "    optimizer.zero_grad()\n",
    "    print(f'epoch: {epoch+1}, loss = {loss.item(): .4f}, R2 = {r2}')\n",
    "    \n",
    "    model.eval()\n",
    "    y_predicted = model(feature_train)\n",
    "    loss = criterion(y_predicted, train_target)\n",
    "    r2 = r2_score(y_predicted, train_target)\n",
    "    \n",
    "    print(f'test: loss = {loss.item(): .4f}, R2 = {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f78ad47",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tqdm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/AL_main_new/linearregression model.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B140.115.59.235/home/AL_main_new/linearregression%20model.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m out_loss, num, tot_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, []\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B140.115.59.235/home/AL_main_new/linearregression%20model.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m y_out, y_tar \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor([]),torch\u001b[39m.\u001b[39mTensor([])\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B140.115.59.235/home/AL_main_new/linearregression%20model.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m data_loader \u001b[39m=\u001b[39m tqdm(data_loader)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B140.115.59.235/home/AL_main_new/linearregression%20model.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m step, (x, y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(data_loader):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B140.115.59.235/home/AL_main_new/linearregression%20model.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     x, y \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mcuda(), y\u001b[39m.\u001b[39mcuda()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tqdm' is not defined"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "out_loss, num, tot_loss = 0, 0, []\n",
    "y_out, y_tar = torch.Tensor([]),torch.Tensor([])\n",
    "data_loader = tqdm(data_loader)\n",
    "for step, (x, y) in enumerate(data_loader):\n",
    "\n",
    "    x, y = x.cuda(), y.cuda()\n",
    "    losses = model(x, y)\n",
    "    tot_loss.append(losses)\n",
    "        \n",
    "    pred = model.inference(x)\n",
    "    y_out = torch.cat((y_out, pred.cpu()), 0)\n",
    "    y_tar = torch.cat((y_tar, y.cpu()), 0)\n",
    "    #cor += (pred.argmax(-1) == y).sum().item()\n",
    "    out_loss += mse_loss(pred, y, reduction='sum')\n",
    "    num += x.size(0)\n",
    "    \n",
    "    data_loader.set_description(f'Train {epoch} | out_loss {torch.sqrt(out_loss/num)}')\n",
    "\n",
    "train_out = mse_loss(y_out, y_tar).item()\n",
    "train_r2 = r2_score(y_out, y_tar).item()\n",
    "train_loss = numpy.sum(tot_loss, axis=0)\n",
    "\n",
    "print(f'Train Epoch{epoch} out_loss {train_out}, R2 {train_r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad319671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(635, 125)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_target = data.columns[:6]\n",
    "col_feature1 = data.columns[6:33].to_list() # 27 cols\n",
    "col_feature2 = data.columns[33:43].to_list() # 10 cols\n",
    "col_feature3 = data.columns[43:103].to_list() # 60 cols\n",
    "col_feature4 = data.columns[103:].to_list() # 28 cols\n",
    "y = data[col_target]\n",
    "x = data[col_feature1 + col_feature2 + col_feature3 + col_feature4]\n",
    "x = x.fillna(0)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2f394a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 92, 100,  78,  78,  84,  68])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_train, clean_test, train_label, test_label = train_test_split(x, y, test_size=0.2)\n",
    "train_label.to_numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e22348e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensor_point5_i_value     0\n",
      "sensor_point6_i_value     0\n",
      "sensor_point7_i_value     0\n",
      "sensor_point8_i_value     0\n",
      "sensor_point9_i_value     0\n",
      "sensor_point10_i_value    0\n",
      "dtype: int64\n",
      "sensor_point5_i_value     0\n",
      "sensor_point6_i_value     0\n",
      "sensor_point7_i_value     0\n",
      "sensor_point8_i_value     0\n",
      "sensor_point9_i_value     0\n",
      "sensor_point10_i_value    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print((y == 0).sum())\n",
    "print((y.isna()).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57f55723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      22\n",
      "238     1\n",
      "286     3\n",
      "621     1\n",
      "dtype: int64 \n",
      "\n",
      "0    10\n",
      "dtype: int64 \n",
      "\n",
      "0      20\n",
      "16      2\n",
      "18      3\n",
      "21      5\n",
      "131     5\n",
      "162    10\n",
      "164     3\n",
      "167     1\n",
      "171     1\n",
      "573    10\n",
      "dtype: int64 \n",
      "\n",
      "0      15\n",
      "27      5\n",
      "65      5\n",
      "111     1\n",
      "117     1\n",
      "579     1\n",
      "dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for col in [col_feature1,col_feature2,col_feature3,col_feature4]:\n",
    "    print((x[col]==0).sum(axis=0).value_counts().sort_index(),\"\\n\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d93b4abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    321\n",
      "2     28\n",
      "3     14\n",
      "4     62\n",
      "5    210\n",
      "dtype: int64 \n",
      "\n",
      "0    635\n",
      "dtype: int64 \n",
      "\n",
      "0      62\n",
      "10    345\n",
      "11      4\n",
      "12      3\n",
      "15     33\n",
      "20     69\n",
      "25      3\n",
      "30    109\n",
      "38      2\n",
      "40      5\n",
      "dtype: int64 \n",
      "\n",
      "0     52\n",
      "1    380\n",
      "2     86\n",
      "3     25\n",
      "6      4\n",
      "7     88\n",
      "dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for col in [col_feature1,col_feature2,col_feature3,col_feature4]:\n",
    "    print((x[col]==0).sum(axis=1).value_counts().sort_index(),\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bbac891a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]/home/AL_main_new/utils.py:99: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  x = torch.tensor([x for x,y in batch], dtype=torch.float32)\n",
      "Train 0 | out_loss 79.63187408447266: 100%|███████| 8/8 [00:00<00:00, 20.38it/s]\n",
      "Train 1 | out_loss 79.25166320800781: 100%|██████| 8/8 [00:00<00:00, 695.60it/s]\n",
      "Train 2 | out_loss 78.55249786376953: 100%|██████| 8/8 [00:00<00:00, 633.23it/s]\n",
      "Train 3 | out_loss 78.12552642822266: 100%|██████| 8/8 [00:00<00:00, 704.66it/s]\n",
      "Train 4 | out_loss 77.69197082519531: 100%|██████| 8/8 [00:00<00:00, 641.07it/s]\n",
      "Train 5 | out_loss 77.02802276611328: 100%|██████| 8/8 [00:00<00:00, 704.56it/s]\n",
      "Train 6 | out_loss 76.23699951171875: 100%|██████| 8/8 [00:00<00:00, 699.82it/s]\n",
      "Train 7 | out_loss 75.69731140136719: 100%|██████| 8/8 [00:00<00:00, 703.52it/s]\n",
      "Train 8 | out_loss 75.17314147949219: 100%|██████| 8/8 [00:00<00:00, 696.66it/s]\n",
      "Train 9 | out_loss 74.6423110961914: 100%|███████| 8/8 [00:00<00:00, 709.26it/s]\n",
      "Train 10 | out_loss 73.97625732421875: 100%|█████| 8/8 [00:00<00:00, 704.51it/s]\n",
      "Train 11 | out_loss 73.28217315673828: 100%|█████| 8/8 [00:00<00:00, 705.46it/s]\n",
      "Train 12 | out_loss 72.73091125488281: 100%|█████| 8/8 [00:00<00:00, 720.39it/s]\n",
      "Train 13 | out_loss 72.18336486816406: 100%|█████| 8/8 [00:00<00:00, 718.48it/s]\n",
      "Train 14 | out_loss 71.63249969482422: 100%|█████| 8/8 [00:00<00:00, 714.71it/s]\n",
      "Train 15 | out_loss 71.08665466308594: 100%|█████| 8/8 [00:00<00:00, 714.14it/s]\n",
      "Train 16 | out_loss 70.54080963134766: 100%|█████| 8/8 [00:00<00:00, 718.23it/s]\n",
      "Train 17 | out_loss 69.99604797363281: 100%|█████| 8/8 [00:00<00:00, 708.99it/s]\n",
      "Train 18 | out_loss 69.44864654541016: 100%|█████| 8/8 [00:00<00:00, 718.56it/s]\n",
      "Train 19 | out_loss 68.90335083007812: 100%|█████| 8/8 [00:00<00:00, 712.14it/s]\n",
      "Train 20 | out_loss 68.35618591308594: 100%|█████| 8/8 [00:00<00:00, 723.37it/s]\n",
      "Train 21 | out_loss 67.81575775146484: 100%|█████| 8/8 [00:00<00:00, 464.24it/s]\n",
      "Train 22 | out_loss 67.27030181884766: 100%|█████| 8/8 [00:00<00:00, 534.80it/s]\n",
      "Train 23 | out_loss 66.72710418701172: 100%|█████| 8/8 [00:00<00:00, 678.24it/s]\n",
      "Train 24 | out_loss 66.18389129638672: 100%|█████| 8/8 [00:00<00:00, 690.93it/s]\n",
      "Train 25 | out_loss 65.64099884033203: 100%|█████| 8/8 [00:00<00:00, 692.46it/s]\n",
      "Train 26 | out_loss 65.10099792480469: 100%|█████| 8/8 [00:00<00:00, 646.40it/s]\n",
      "Train 27 | out_loss 64.55370330810547: 100%|█████| 8/8 [00:00<00:00, 707.12it/s]\n",
      "Train 28 | out_loss 64.01380920410156: 100%|█████| 8/8 [00:00<00:00, 631.96it/s]\n",
      "Train 29 | out_loss 63.47240447998047: 100%|█████| 8/8 [00:00<00:00, 686.48it/s]\n",
      "Train 30 | out_loss 62.92677688598633: 100%|█████| 8/8 [00:00<00:00, 708.66it/s]\n",
      "Train 31 | out_loss 62.3917236328125: 100%|██████| 8/8 [00:00<00:00, 708.33it/s]\n",
      "Train 32 | out_loss 61.84816360473633: 100%|█████| 8/8 [00:00<00:00, 709.28it/s]\n",
      "Train 33 | out_loss 61.30694580078125: 100%|█████| 8/8 [00:00<00:00, 706.23it/s]\n",
      "Train 34 | out_loss 60.76362991333008: 100%|█████| 8/8 [00:00<00:00, 710.67it/s]\n",
      "Train 35 | out_loss 60.227989196777344: 100%|████| 8/8 [00:00<00:00, 708.08it/s]\n",
      "Train 36 | out_loss 59.68289566040039: 100%|█████| 8/8 [00:00<00:00, 704.90it/s]\n",
      "Train 37 | out_loss 59.14426040649414: 100%|█████| 8/8 [00:00<00:00, 707.88it/s]\n",
      "Train 38 | out_loss 58.60676193237305: 100%|█████| 8/8 [00:00<00:00, 711.89it/s]\n",
      "Train 39 | out_loss 58.06916046142578: 100%|█████| 8/8 [00:00<00:00, 699.47it/s]\n",
      "Train 40 | out_loss 57.5296630859375: 100%|██████| 8/8 [00:00<00:00, 707.72it/s]\n",
      "Train 41 | out_loss 56.98972702026367: 100%|█████| 8/8 [00:00<00:00, 705.93it/s]\n",
      "Train 42 | out_loss 56.4539680480957: 100%|██████| 8/8 [00:00<00:00, 708.42it/s]\n",
      "Train 43 | out_loss 55.911231994628906: 100%|████| 8/8 [00:00<00:00, 702.78it/s]\n",
      "Train 44 | out_loss 55.37491989135742: 100%|█████| 8/8 [00:00<00:00, 703.62it/s]\n",
      "Train 45 | out_loss 54.842037200927734: 100%|████| 8/8 [00:00<00:00, 706.22it/s]\n",
      "Train 46 | out_loss 54.30598449707031: 100%|█████| 8/8 [00:00<00:00, 704.53it/s]\n",
      "Train 47 | out_loss 53.7697868347168: 100%|██████| 8/8 [00:00<00:00, 702.81it/s]\n",
      "Train 48 | out_loss 53.233177185058594: 100%|████| 8/8 [00:00<00:00, 707.03it/s]\n",
      "Train 49 | out_loss 52.700439453125: 100%|███████| 8/8 [00:00<00:00, 719.23it/s]\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]/home/AL_main_new/utils.py:99: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  x = torch.tensor([x for x,y in batch], dtype=torch.float32)\n",
      "Train 0 | out_loss 80.1839370727539: 100%|████████| 8/8 [00:00<00:00, 20.12it/s]\n",
      "Train 1 | out_loss 79.67149353027344: 100%|██████| 8/8 [00:00<00:00, 428.14it/s]\n",
      "Train 2 | out_loss 78.92416381835938: 100%|██████| 8/8 [00:00<00:00, 443.49it/s]\n",
      "Train 3 | out_loss 78.22335815429688: 100%|██████| 8/8 [00:00<00:00, 441.58it/s]\n",
      "Train 4 | out_loss 77.4666519165039: 100%|███████| 8/8 [00:00<00:00, 447.09it/s]\n",
      "Train 5 | out_loss 76.669921875: 100%|███████████| 8/8 [00:00<00:00, 445.53it/s]\n",
      "Train 6 | out_loss 75.89876556396484: 100%|██████| 8/8 [00:00<00:00, 451.51it/s]\n",
      "Train 7 | out_loss 75.08525848388672: 100%|██████| 8/8 [00:00<00:00, 448.61it/s]\n",
      "Train 8 | out_loss 74.25202941894531: 100%|██████| 8/8 [00:00<00:00, 440.42it/s]\n",
      "Train 9 | out_loss 73.59396362304688: 100%|██████| 8/8 [00:00<00:00, 448.85it/s]\n",
      "Train 10 | out_loss 72.81153869628906: 100%|█████| 8/8 [00:00<00:00, 444.61it/s]\n",
      "Train 11 | out_loss 71.83638000488281: 100%|█████| 8/8 [00:00<00:00, 451.74it/s]\n",
      "Train 12 | out_loss 70.8773193359375: 100%|██████| 8/8 [00:00<00:00, 438.42it/s]\n",
      "Train 13 | out_loss 70.07412719726562: 100%|█████| 8/8 [00:00<00:00, 452.14it/s]\n",
      "Train 14 | out_loss 69.27417755126953: 100%|█████| 8/8 [00:00<00:00, 445.17it/s]\n",
      "Train 15 | out_loss 68.60704040527344: 100%|█████| 8/8 [00:00<00:00, 440.95it/s]\n",
      "Train 16 | out_loss 67.88280487060547: 100%|█████| 8/8 [00:00<00:00, 448.79it/s]\n",
      "Train 17 | out_loss 67.0840072631836: 100%|██████| 8/8 [00:00<00:00, 442.89it/s]\n",
      "Train 18 | out_loss 66.30034637451172: 100%|█████| 8/8 [00:00<00:00, 451.92it/s]\n",
      "Train 19 | out_loss 65.51168823242188: 100%|█████| 8/8 [00:00<00:00, 445.73it/s]\n",
      "Train 20 | out_loss 64.74617004394531: 100%|█████| 8/8 [00:00<00:00, 448.23it/s]\n",
      "Train 21 | out_loss 63.93751907348633: 100%|█████| 8/8 [00:00<00:00, 449.68it/s]\n",
      "Train 22 | out_loss 63.1587028503418: 100%|██████| 8/8 [00:00<00:00, 442.76it/s]\n",
      "Train 23 | out_loss 62.37384033203125: 100%|█████| 8/8 [00:00<00:00, 447.77it/s]\n",
      "Train 24 | out_loss 61.589019775390625: 100%|████| 8/8 [00:00<00:00, 443.07it/s]\n",
      "Train 25 | out_loss 60.80298614501953: 100%|█████| 8/8 [00:00<00:00, 451.61it/s]\n",
      "Train 26 | out_loss 60.02083206176758: 100%|█████| 8/8 [00:00<00:00, 428.70it/s]\n",
      "Train 27 | out_loss 59.23554229736328: 100%|█████| 8/8 [00:00<00:00, 447.53it/s]\n",
      "Train 28 | out_loss 58.45291519165039: 100%|█████| 8/8 [00:00<00:00, 442.61it/s]\n",
      "Train 29 | out_loss 57.674530029296875: 100%|████| 8/8 [00:00<00:00, 442.05it/s]\n",
      "Train 30 | out_loss 56.89305114746094: 100%|█████| 8/8 [00:00<00:00, 448.06it/s]\n",
      "Train 31 | out_loss 56.114540100097656: 100%|████| 8/8 [00:00<00:00, 439.47it/s]\n",
      "Train 32 | out_loss 55.335609436035156: 100%|████| 8/8 [00:00<00:00, 451.61it/s]\n",
      "Train 33 | out_loss 54.55278778076172: 100%|█████| 8/8 [00:00<00:00, 439.74it/s]\n",
      "Train 34 | out_loss 53.77313995361328: 100%|█████| 8/8 [00:00<00:00, 442.83it/s]\n",
      "Train 35 | out_loss 52.997188568115234: 100%|████| 8/8 [00:00<00:00, 448.38it/s]\n",
      "Train 36 | out_loss 52.225120544433594: 100%|████| 8/8 [00:00<00:00, 447.61it/s]\n",
      "Train 37 | out_loss 51.45038604736328: 100%|█████| 8/8 [00:00<00:00, 453.63it/s]\n",
      "Train 38 | out_loss 50.671749114990234: 100%|████| 8/8 [00:00<00:00, 447.17it/s]\n",
      "Train 39 | out_loss 49.89935302734375: 100%|█████| 8/8 [00:00<00:00, 453.89it/s]\n",
      "Train 40 | out_loss 49.128543853759766: 100%|████| 8/8 [00:00<00:00, 447.31it/s]\n",
      "Train 41 | out_loss 48.356048583984375: 100%|████| 8/8 [00:00<00:00, 451.19it/s]\n",
      "Train 42 | out_loss 47.58466720581055: 100%|█████| 8/8 [00:00<00:00, 448.34it/s]\n",
      "Train 43 | out_loss 46.814754486083984: 100%|████| 8/8 [00:00<00:00, 449.02it/s]\n",
      "Train 44 | out_loss 46.049095153808594: 100%|████| 8/8 [00:00<00:00, 451.26it/s]\n",
      "Train 45 | out_loss 45.27891540527344: 100%|█████| 8/8 [00:00<00:00, 442.27it/s]\n",
      "Train 46 | out_loss 44.51369857788086: 100%|█████| 8/8 [00:00<00:00, 448.31it/s]\n",
      "Train 47 | out_loss 43.74678039550781: 100%|█████| 8/8 [00:00<00:00, 368.69it/s]\n",
      "Train 48 | out_loss 42.99005126953125: 100%|█████| 8/8 [00:00<00:00, 429.74it/s]\n",
      "Train 49 | out_loss 42.22278594970703: 100%|█████| 8/8 [00:00<00:00, 420.52it/s]\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]/home/AL_main_new/utils.py:99: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  x = torch.tensor([x for x,y in batch], dtype=torch.float32)\n",
      "Train 0 | out_loss 80.209716796875: 100%|█████████| 8/8 [00:00<00:00, 20.22it/s]\n",
      "Train 1 | out_loss 78.41635131835938: 100%|██████| 8/8 [00:00<00:00, 331.80it/s]\n",
      "Train 2 | out_loss 76.33370208740234: 100%|██████| 8/8 [00:00<00:00, 323.90it/s]\n",
      "Train 3 | out_loss 74.88867950439453: 100%|██████| 8/8 [00:00<00:00, 329.79it/s]\n",
      "Train 4 | out_loss 73.07209014892578: 100%|██████| 8/8 [00:00<00:00, 331.96it/s]\n",
      "Train 5 | out_loss 71.3323745727539: 100%|███████| 8/8 [00:00<00:00, 323.34it/s]\n",
      "Train 6 | out_loss 69.38390350341797: 100%|██████| 8/8 [00:00<00:00, 313.26it/s]\n",
      "Train 7 | out_loss 67.72940826416016: 100%|██████| 8/8 [00:00<00:00, 329.46it/s]\n",
      "Train 8 | out_loss 66.05982208251953: 100%|██████| 8/8 [00:00<00:00, 327.69it/s]\n",
      "Train 9 | out_loss 64.23028564453125: 100%|██████| 8/8 [00:00<00:00, 329.05it/s]\n",
      "Train 10 | out_loss 63.09996032714844: 100%|█████| 8/8 [00:00<00:00, 329.26it/s]\n",
      "Train 11 | out_loss 61.79642105102539: 100%|█████| 8/8 [00:00<00:00, 327.74it/s]\n",
      "Train 12 | out_loss 60.281192779541016: 100%|████| 8/8 [00:00<00:00, 328.97it/s]\n",
      "Train 13 | out_loss 58.84437561035156: 100%|█████| 8/8 [00:00<00:00, 315.77it/s]\n",
      "Train 14 | out_loss 57.55911636352539: 100%|█████| 8/8 [00:00<00:00, 329.86it/s]\n",
      "Train 15 | out_loss 55.30004119873047: 100%|█████| 8/8 [00:00<00:00, 325.78it/s]\n",
      "Train 16 | out_loss 53.536865234375: 100%|███████| 8/8 [00:00<00:00, 307.44it/s]\n",
      "Train 17 | out_loss 51.67790603637695: 100%|█████| 8/8 [00:00<00:00, 329.92it/s]\n",
      "Train 18 | out_loss 49.964290618896484: 100%|████| 8/8 [00:00<00:00, 319.16it/s]\n",
      "Train 19 | out_loss 48.405517578125: 100%|███████| 8/8 [00:00<00:00, 333.78it/s]\n",
      "Train 20 | out_loss 46.795196533203125: 100%|████| 8/8 [00:00<00:00, 321.71it/s]\n",
      "Train 21 | out_loss 45.20578384399414: 100%|█████| 8/8 [00:00<00:00, 329.82it/s]\n",
      "Train 22 | out_loss 43.82072448730469: 100%|█████| 8/8 [00:00<00:00, 331.69it/s]\n",
      "Train 23 | out_loss 42.397579193115234: 100%|████| 8/8 [00:00<00:00, 328.68it/s]\n",
      "Train 24 | out_loss 40.49637222290039: 100%|█████| 8/8 [00:00<00:00, 315.32it/s]\n",
      "Train 25 | out_loss 38.947078704833984: 100%|████| 8/8 [00:00<00:00, 304.85it/s]\n",
      "Train 26 | out_loss 37.39653396606445: 100%|█████| 8/8 [00:00<00:00, 275.86it/s]\n",
      "Train 27 | out_loss 35.851104736328125: 100%|████| 8/8 [00:00<00:00, 310.46it/s]\n",
      "Train 28 | out_loss 34.33168411254883: 100%|█████| 8/8 [00:00<00:00, 293.84it/s]\n",
      "Train 29 | out_loss 32.82575607299805: 100%|█████| 8/8 [00:00<00:00, 316.13it/s]\n",
      "Train 30 | out_loss 31.304658889770508: 100%|████| 8/8 [00:00<00:00, 323.88it/s]\n",
      "Train 31 | out_loss 29.852407455444336: 100%|████| 8/8 [00:00<00:00, 330.66it/s]\n",
      "Train 32 | out_loss 28.386613845825195: 100%|████| 8/8 [00:00<00:00, 330.03it/s]\n",
      "Train 33 | out_loss 26.939476013183594: 100%|████| 8/8 [00:00<00:00, 330.19it/s]\n",
      "Train 34 | out_loss 25.519277572631836: 100%|████| 8/8 [00:00<00:00, 332.97it/s]\n",
      "Train 35 | out_loss 24.14809799194336: 100%|█████| 8/8 [00:00<00:00, 326.50it/s]\n",
      "Train 36 | out_loss 22.798662185668945: 100%|████| 8/8 [00:00<00:00, 329.99it/s]\n",
      "Train 37 | out_loss 21.497499465942383: 100%|████| 8/8 [00:00<00:00, 324.04it/s]\n",
      "Train 38 | out_loss 20.252208709716797: 100%|████| 8/8 [00:00<00:00, 328.61it/s]\n",
      "Train 39 | out_loss 19.0393123626709: 100%|██████| 8/8 [00:00<00:00, 313.10it/s]\n",
      "Train 40 | out_loss 17.936738967895508: 100%|████| 8/8 [00:00<00:00, 285.65it/s]\n",
      "Train 41 | out_loss 16.878198623657227: 100%|████| 8/8 [00:00<00:00, 248.62it/s]\n",
      "Train 42 | out_loss 15.98067855834961: 100%|█████| 8/8 [00:00<00:00, 294.79it/s]\n",
      "Train 43 | out_loss 15.143105506896973: 100%|████| 8/8 [00:00<00:00, 325.39it/s]\n",
      "Train 44 | out_loss 14.48554801940918: 100%|█████| 8/8 [00:00<00:00, 329.37it/s]\n",
      "Train 45 | out_loss 13.999011993408203: 100%|████| 8/8 [00:00<00:00, 329.37it/s]\n",
      "Train 46 | out_loss 13.713170051574707: 100%|████| 8/8 [00:00<00:00, 329.91it/s]\n",
      "Train 47 | out_loss 13.566171646118164: 100%|████| 8/8 [00:00<00:00, 328.11it/s]\n",
      "Train 48 | out_loss 13.668951988220215: 100%|████| 8/8 [00:00<00:00, 333.09it/s]\n",
      "Train 49 | out_loss 13.953893661499023: 100%|████| 8/8 [00:00<00:00, 336.34it/s]\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]/home/AL_main_new/utils.py:99: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  x = torch.tensor([x for x,y in batch], dtype=torch.float32)\n",
      "Train 0 | out_loss 80.70198822021484: 100%|███████| 8/8 [00:00<00:00, 20.20it/s]\n",
      "Train 1 | out_loss 79.37141418457031: 100%|██████| 8/8 [00:00<00:00, 261.25it/s]\n",
      "Train 2 | out_loss 76.00825500488281: 100%|██████| 8/8 [00:00<00:00, 249.21it/s]\n",
      "Train 3 | out_loss 74.83383178710938: 100%|██████| 8/8 [00:00<00:00, 261.77it/s]\n",
      "Train 4 | out_loss 73.23185729980469: 100%|██████| 8/8 [00:00<00:00, 254.42it/s]\n",
      "Train 5 | out_loss 70.96790313720703: 100%|██████| 8/8 [00:00<00:00, 261.21it/s]\n",
      "Train 6 | out_loss 69.69562530517578: 100%|██████| 8/8 [00:00<00:00, 257.83it/s]\n",
      "Train 7 | out_loss 68.27301025390625: 100%|██████| 8/8 [00:00<00:00, 265.61it/s]\n",
      "Train 8 | out_loss 66.56065368652344: 100%|██████| 8/8 [00:00<00:00, 258.31it/s]\n",
      "Train 9 | out_loss 64.94966888427734: 100%|██████| 8/8 [00:00<00:00, 262.80it/s]\n",
      "Train 10 | out_loss 62.722476959228516: 100%|████| 8/8 [00:00<00:00, 263.97it/s]\n",
      "Train 11 | out_loss 60.365726470947266: 100%|████| 8/8 [00:00<00:00, 264.49it/s]\n",
      "Train 12 | out_loss 58.46863555908203: 100%|█████| 8/8 [00:00<00:00, 262.97it/s]\n",
      "Train 13 | out_loss 57.29116439819336: 100%|█████| 8/8 [00:00<00:00, 253.01it/s]\n",
      "Train 14 | out_loss 55.8953857421875: 100%|██████| 8/8 [00:00<00:00, 260.49it/s]\n",
      "Train 15 | out_loss 54.10263442993164: 100%|█████| 8/8 [00:00<00:00, 259.71it/s]\n",
      "Train 16 | out_loss 52.48109817504883: 100%|█████| 8/8 [00:00<00:00, 241.39it/s]\n",
      "Train 17 | out_loss 50.859710693359375: 100%|████| 8/8 [00:00<00:00, 261.19it/s]\n",
      "Train 18 | out_loss 49.17475128173828: 100%|█████| 8/8 [00:00<00:00, 260.84it/s]\n",
      "Train 19 | out_loss 47.44289779663086: 100%|█████| 8/8 [00:00<00:00, 263.43it/s]\n",
      "Train 20 | out_loss 46.02058792114258: 100%|█████| 8/8 [00:00<00:00, 259.60it/s]\n",
      "Train 21 | out_loss 43.9930534362793: 100%|██████| 8/8 [00:00<00:00, 258.02it/s]\n",
      "Train 22 | out_loss 42.46625900268555: 100%|█████| 8/8 [00:00<00:00, 251.89it/s]\n",
      "Train 23 | out_loss 40.730491638183594: 100%|████| 8/8 [00:00<00:00, 246.78it/s]\n",
      "Train 24 | out_loss 39.2940673828125: 100%|██████| 8/8 [00:00<00:00, 253.93it/s]\n",
      "Train 25 | out_loss 38.9849853515625: 100%|██████| 8/8 [00:00<00:00, 264.94it/s]\n",
      "Train 26 | out_loss 37.07102966308594: 100%|█████| 8/8 [00:00<00:00, 261.55it/s]\n",
      "Train 27 | out_loss 36.00474548339844: 100%|█████| 8/8 [00:00<00:00, 236.57it/s]\n",
      "Train 28 | out_loss 34.03815460205078: 100%|█████| 8/8 [00:00<00:00, 257.86it/s]\n",
      "Train 29 | out_loss 32.811214447021484: 100%|████| 8/8 [00:00<00:00, 258.10it/s]\n",
      "Train 30 | out_loss 31.94162940979004: 100%|█████| 8/8 [00:00<00:00, 238.77it/s]\n",
      "Train 31 | out_loss 30.253469467163086: 100%|████| 8/8 [00:00<00:00, 240.10it/s]\n",
      "Train 32 | out_loss 30.13692855834961: 100%|█████| 8/8 [00:00<00:00, 241.46it/s]\n",
      "Train 33 | out_loss 29.37782859802246: 100%|█████| 8/8 [00:00<00:00, 251.41it/s]\n",
      "Train 34 | out_loss 27.908700942993164: 100%|████| 8/8 [00:00<00:00, 246.52it/s]\n",
      "Train 35 | out_loss 26.49312973022461: 100%|█████| 8/8 [00:00<00:00, 226.42it/s]\n",
      "Train 36 | out_loss 25.005998611450195: 100%|████| 8/8 [00:00<00:00, 262.06it/s]\n",
      "Train 37 | out_loss 21.362024307250977: 100%|████| 8/8 [00:00<00:00, 261.04it/s]\n",
      "Train 38 | out_loss 21.59775161743164: 100%|█████| 8/8 [00:00<00:00, 263.73it/s]\n",
      "Train 39 | out_loss 20.24393081665039: 100%|█████| 8/8 [00:00<00:00, 261.35it/s]\n",
      "Train 40 | out_loss 19.445348739624023: 100%|████| 8/8 [00:00<00:00, 259.22it/s]\n",
      "Train 41 | out_loss 18.922348022460938: 100%|████| 8/8 [00:00<00:00, 265.79it/s]\n",
      "Train 42 | out_loss 18.063411712646484: 100%|████| 8/8 [00:00<00:00, 270.05it/s]\n",
      "Train 43 | out_loss 18.678688049316406: 100%|████| 8/8 [00:00<00:00, 267.07it/s]\n",
      "Train 44 | out_loss 18.636152267456055: 100%|████| 8/8 [00:00<00:00, 257.86it/s]\n",
      "Train 45 | out_loss 17.506397247314453: 100%|████| 8/8 [00:00<00:00, 258.13it/s]\n",
      "Train 46 | out_loss 16.118213653564453: 100%|████| 8/8 [00:00<00:00, 262.12it/s]\n",
      "Train 47 | out_loss 15.315125465393066: 100%|████| 8/8 [00:00<00:00, 256.18it/s]\n",
      "Train 48 | out_loss 14.668306350708008: 100%|████| 8/8 [00:00<00:00, 262.55it/s]\n",
      "Train 49 | out_loss 14.476733207702637: 100%|████| 8/8 [00:00<00:00, 256.22it/s]\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]/home/AL_main_new/utils.py:99: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  x = torch.tensor([x for x,y in batch], dtype=torch.float32)\n",
      "Train 0 | out_loss 80.37500762939453: 100%|███████| 8/8 [00:00<00:00, 19.10it/s]\n",
      "Train 1 | out_loss 78.81150817871094: 100%|██████| 8/8 [00:00<00:00, 199.20it/s]\n",
      "Train 2 | out_loss 74.7491455078125: 100%|███████| 8/8 [00:00<00:00, 204.01it/s]\n",
      "Train 3 | out_loss 74.28646850585938: 100%|██████| 8/8 [00:00<00:00, 201.45it/s]\n",
      "Train 4 | out_loss 71.90930938720703: 100%|██████| 8/8 [00:00<00:00, 201.03it/s]\n",
      "Train 5 | out_loss 69.00135040283203: 100%|██████| 8/8 [00:00<00:00, 118.95it/s]\n",
      "Train 6 | out_loss 67.6754150390625: 100%|███████| 8/8 [00:00<00:00, 196.80it/s]\n",
      "Train 7 | out_loss 66.5774154663086: 100%|███████| 8/8 [00:00<00:00, 202.85it/s]\n",
      "Train 8 | out_loss 64.30731964111328: 100%|██████| 8/8 [00:00<00:00, 207.16it/s]\n",
      "Train 9 | out_loss 64.85501861572266: 100%|██████| 8/8 [00:00<00:00, 206.53it/s]\n",
      "Train 10 | out_loss 63.77495193481445: 100%|█████| 8/8 [00:00<00:00, 204.28it/s]\n",
      "Train 11 | out_loss 63.24276351928711: 100%|█████| 8/8 [00:00<00:00, 206.29it/s]\n",
      "Train 12 | out_loss 64.66304779052734: 100%|█████| 8/8 [00:00<00:00, 204.98it/s]\n",
      "Train 13 | out_loss 63.305179595947266: 100%|████| 8/8 [00:00<00:00, 199.63it/s]\n",
      "Train 14 | out_loss 63.36079406738281: 100%|█████| 8/8 [00:00<00:00, 198.36it/s]\n",
      "Train 15 | out_loss 62.14793395996094: 100%|█████| 8/8 [00:00<00:00, 196.24it/s]\n",
      "Train 16 | out_loss 61.23788070678711: 100%|█████| 8/8 [00:00<00:00, 204.66it/s]\n",
      "Train 17 | out_loss 60.4580078125: 100%|█████████| 8/8 [00:00<00:00, 196.91it/s]\n",
      "Train 18 | out_loss 60.25556945800781: 100%|█████| 8/8 [00:00<00:00, 198.92it/s]\n",
      "Train 19 | out_loss 58.71955108642578: 100%|█████| 8/8 [00:00<00:00, 210.74it/s]\n",
      "Train 20 | out_loss 58.181617736816406: 100%|████| 8/8 [00:00<00:00, 211.67it/s]\n",
      "Train 21 | out_loss 57.202674865722656: 100%|████| 8/8 [00:00<00:00, 212.64it/s]\n",
      "Train 22 | out_loss 56.13126754760742: 100%|█████| 8/8 [00:00<00:00, 214.07it/s]\n",
      "Train 23 | out_loss 54.253841400146484: 100%|████| 8/8 [00:00<00:00, 212.98it/s]\n",
      "Train 24 | out_loss 54.26271057128906: 100%|█████| 8/8 [00:00<00:00, 214.74it/s]\n",
      "Train 25 | out_loss 52.986698150634766: 100%|████| 8/8 [00:00<00:00, 211.35it/s]\n",
      "Train 26 | out_loss 51.90414810180664: 100%|█████| 8/8 [00:00<00:00, 198.03it/s]\n",
      "Train 27 | out_loss 50.88888168334961: 100%|█████| 8/8 [00:00<00:00, 182.61it/s]\n",
      "Train 28 | out_loss 50.32403564453125: 100%|█████| 8/8 [00:00<00:00, 208.08it/s]\n",
      "Train 29 | out_loss 49.51617431640625: 100%|█████| 8/8 [00:00<00:00, 205.82it/s]\n",
      "Train 30 | out_loss 48.05425262451172: 100%|█████| 8/8 [00:00<00:00, 209.22it/s]\n",
      "Train 31 | out_loss 47.201263427734375: 100%|████| 8/8 [00:00<00:00, 194.49it/s]\n",
      "Train 32 | out_loss 46.14252471923828: 100%|█████| 8/8 [00:00<00:00, 174.83it/s]\n",
      "Train 33 | out_loss 45.170372009277344: 100%|████| 8/8 [00:00<00:00, 184.26it/s]\n",
      "Train 34 | out_loss 44.36216354370117: 100%|█████| 8/8 [00:00<00:00, 210.63it/s]\n",
      "Train 35 | out_loss 43.215763092041016: 100%|████| 8/8 [00:00<00:00, 210.97it/s]\n",
      "Train 36 | out_loss 41.94840621948242: 100%|█████| 8/8 [00:00<00:00, 182.48it/s]\n",
      "Train 37 | out_loss 41.321205139160156: 100%|████| 8/8 [00:00<00:00, 207.61it/s]\n",
      "Train 38 | out_loss 40.39054489135742: 100%|█████| 8/8 [00:00<00:00, 201.75it/s]\n",
      "Train 39 | out_loss 38.691898345947266: 100%|████| 8/8 [00:00<00:00, 207.63it/s]\n",
      "Train 40 | out_loss 38.087284088134766: 100%|████| 8/8 [00:00<00:00, 208.93it/s]\n",
      "Train 41 | out_loss 37.20523452758789: 100%|█████| 8/8 [00:00<00:00, 213.33it/s]\n",
      "Train 42 | out_loss 36.593990325927734: 100%|████| 8/8 [00:00<00:00, 207.69it/s]\n",
      "Train 43 | out_loss 35.34302520751953: 100%|█████| 8/8 [00:00<00:00, 211.15it/s]\n",
      "Train 44 | out_loss 34.38832473754883: 100%|█████| 8/8 [00:00<00:00, 171.12it/s]\n",
      "Train 45 | out_loss 33.6642951965332: 100%|██████| 8/8 [00:00<00:00, 166.46it/s]\n",
      "Train 46 | out_loss 32.989402770996094: 100%|████| 8/8 [00:00<00:00, 190.56it/s]\n",
      "Train 47 | out_loss 31.71337127685547: 100%|█████| 8/8 [00:00<00:00, 212.29it/s]\n",
      "Train 48 | out_loss 30.71175765991211: 100%|█████| 8/8 [00:00<00:00, 205.93it/s]\n",
      "Train 49 | out_loss 29.941850662231445: 100%|████| 8/8 [00:00<00:00, 216.46it/s]\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]/home/AL_main_new/utils.py:99: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  x = torch.tensor([x for x,y in batch], dtype=torch.float32)\n",
      "Train 0 | out_loss 80.66100311279297: 100%|███████| 8/8 [00:00<00:00, 19.41it/s]\n",
      "Train 1 | out_loss 80.04766845703125: 100%|██████| 8/8 [00:00<00:00, 159.56it/s]\n",
      "Train 2 | out_loss 76.83219146728516: 100%|██████| 8/8 [00:00<00:00, 184.47it/s]\n",
      "Train 3 | out_loss 76.01897430419922: 100%|██████| 8/8 [00:00<00:00, 186.30it/s]\n",
      "Train 4 | out_loss 76.9652099609375: 100%|███████| 8/8 [00:00<00:00, 177.57it/s]\n",
      "Train 5 | out_loss 76.62203979492188: 100%|██████| 8/8 [00:00<00:00, 179.41it/s]\n",
      "Train 6 | out_loss 72.59370422363281: 100%|██████| 8/8 [00:00<00:00, 184.34it/s]\n",
      "Train 7 | out_loss 73.07462310791016: 100%|██████| 8/8 [00:00<00:00, 183.33it/s]\n",
      "Train 8 | out_loss 71.96461486816406: 100%|██████| 8/8 [00:00<00:00, 185.63it/s]\n",
      "Train 9 | out_loss 70.82769012451172: 100%|██████| 8/8 [00:00<00:00, 179.19it/s]\n",
      "Train 10 | out_loss 68.93132019042969: 100%|█████| 8/8 [00:00<00:00, 181.68it/s]\n",
      "Train 11 | out_loss 69.43951416015625: 100%|█████| 8/8 [00:00<00:00, 186.13it/s]\n",
      "Train 12 | out_loss 68.33834838867188: 100%|█████| 8/8 [00:00<00:00, 176.25it/s]\n",
      "Train 13 | out_loss 66.50336456298828: 100%|█████| 8/8 [00:00<00:00, 170.95it/s]\n",
      "Train 14 | out_loss 64.53569793701172: 100%|█████| 8/8 [00:00<00:00, 185.77it/s]\n",
      "Train 15 | out_loss 64.85191345214844: 100%|█████| 8/8 [00:00<00:00, 183.41it/s]\n",
      "Train 16 | out_loss 63.683258056640625: 100%|████| 8/8 [00:00<00:00, 173.41it/s]\n",
      "Train 17 | out_loss 61.80734634399414: 100%|█████| 8/8 [00:00<00:00, 178.10it/s]\n",
      "Train 18 | out_loss 61.87030029296875: 100%|█████| 8/8 [00:00<00:00, 135.36it/s]\n",
      "Train 19 | out_loss 60.105064392089844: 100%|████| 8/8 [00:00<00:00, 178.08it/s]\n",
      "Train 20 | out_loss 58.656986236572266: 100%|████| 8/8 [00:00<00:00, 185.18it/s]\n",
      "Train 21 | out_loss 57.972740173339844: 100%|████| 8/8 [00:00<00:00, 186.24it/s]\n",
      "Train 22 | out_loss 57.290401458740234: 100%|████| 8/8 [00:00<00:00, 187.00it/s]\n",
      "Train 23 | out_loss 55.64370346069336: 100%|█████| 8/8 [00:00<00:00, 183.27it/s]\n",
      "Train 24 | out_loss 54.76457595825195: 100%|█████| 8/8 [00:00<00:00, 185.12it/s]\n",
      "Train 25 | out_loss 54.064029693603516: 100%|████| 8/8 [00:00<00:00, 175.89it/s]\n",
      "Train 26 | out_loss 52.72576141357422: 100%|█████| 8/8 [00:00<00:00, 183.29it/s]\n",
      "Train 27 | out_loss 52.17194366455078: 100%|█████| 8/8 [00:00<00:00, 183.01it/s]\n",
      "Train 28 | out_loss 50.56935501098633: 100%|█████| 8/8 [00:00<00:00, 185.58it/s]\n",
      "Train 29 | out_loss 50.13287353515625: 100%|█████| 8/8 [00:00<00:00, 183.77it/s]\n",
      "Train 30 | out_loss 48.821815490722656: 100%|████| 8/8 [00:00<00:00, 183.96it/s]\n",
      "Train 31 | out_loss 47.86432647705078: 100%|█████| 8/8 [00:00<00:00, 138.82it/s]\n",
      "Train 32 | out_loss 46.671722412109375: 100%|████| 8/8 [00:00<00:00, 165.68it/s]\n",
      "Train 33 | out_loss 46.059268951416016: 100%|████| 8/8 [00:00<00:00, 184.93it/s]\n",
      "Train 34 | out_loss 44.59551239013672: 100%|█████| 8/8 [00:00<00:00, 180.75it/s]\n",
      "Train 35 | out_loss 44.14033889770508: 100%|█████| 8/8 [00:00<00:00, 183.30it/s]\n",
      "Train 36 | out_loss 42.99190902709961: 100%|█████| 8/8 [00:00<00:00, 182.84it/s]\n",
      "Train 37 | out_loss 41.36393737792969: 100%|█████| 8/8 [00:00<00:00, 161.46it/s]\n",
      "Train 38 | out_loss 41.03105545043945: 100%|█████| 8/8 [00:00<00:00, 180.64it/s]\n",
      "Train 39 | out_loss 39.91022872924805: 100%|█████| 8/8 [00:00<00:00, 124.54it/s]\n",
      "Train 40 | out_loss 38.787437438964844: 100%|████| 8/8 [00:00<00:00, 161.21it/s]\n",
      "Train 41 | out_loss 37.313236236572266: 100%|████| 8/8 [00:00<00:00, 186.57it/s]\n",
      "Train 42 | out_loss 37.483985900878906: 100%|████| 8/8 [00:00<00:00, 183.61it/s]\n",
      "Train 43 | out_loss 35.931549072265625: 100%|████| 8/8 [00:00<00:00, 186.41it/s]\n",
      "Train 44 | out_loss 34.869056701660156: 100%|████| 8/8 [00:00<00:00, 186.29it/s]\n",
      "Train 45 | out_loss 34.22628402709961: 100%|█████| 8/8 [00:00<00:00, 187.49it/s]\n",
      "Train 46 | out_loss 32.9979133605957: 100%|██████| 8/8 [00:00<00:00, 138.48it/s]\n",
      "Train 47 | out_loss 32.45005416870117: 100%|█████| 8/8 [00:00<00:00, 185.26it/s]\n",
      "Train 48 | out_loss 31.098251342773438: 100%|████| 8/8 [00:00<00:00, 182.81it/s]\n",
      "Train 49 | out_loss 29.84471893310547: 100%|█████| 8/8 [00:00<00:00, 184.68it/s]\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]/home/AL_main_new/utils.py:99: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  x = torch.tensor([x for x,y in batch], dtype=torch.float32)\n",
      "Train 0 | out_loss 80.49638366699219: 100%|███████| 8/8 [00:00<00:00, 16.79it/s]\n",
      "Train 1 | out_loss 80.04119873046875: 100%|██████| 8/8 [00:00<00:00, 118.01it/s]\n",
      "Train 2 | out_loss 76.40736389160156: 100%|██████| 8/8 [00:00<00:00, 147.76it/s]\n",
      "Train 3 | out_loss 76.17723846435547: 100%|██████| 8/8 [00:00<00:00, 154.46it/s]\n",
      "Train 4 | out_loss 76.74988555908203: 100%|██████| 8/8 [00:00<00:00, 154.47it/s]\n",
      "Train 5 | out_loss 75.42984008789062: 100%|██████| 8/8 [00:00<00:00, 152.31it/s]\n",
      "Train 6 | out_loss 73.28553771972656: 100%|██████| 8/8 [00:00<00:00, 150.25it/s]\n",
      "Train 7 | out_loss 72.70917510986328: 100%|██████| 8/8 [00:00<00:00, 145.92it/s]\n",
      "Train 8 | out_loss 72.0929183959961: 100%|███████| 8/8 [00:00<00:00, 120.35it/s]\n",
      "Train 9 | out_loss 70.52079010009766: 100%|██████| 8/8 [00:00<00:00, 145.89it/s]\n",
      "Train 10 | out_loss 70.58097076416016: 100%|█████| 8/8 [00:00<00:00, 136.88it/s]\n",
      "Train 11 | out_loss 69.11646270751953: 100%|█████| 8/8 [00:00<00:00, 151.67it/s]\n",
      "Train 12 | out_loss 66.93928527832031: 100%|█████| 8/8 [00:00<00:00, 156.08it/s]\n",
      "Train 13 | out_loss 67.93284606933594: 100%|█████| 8/8 [00:00<00:00, 154.32it/s]\n",
      "Train 14 | out_loss 66.1832504272461: 100%|██████| 8/8 [00:00<00:00, 149.70it/s]\n",
      "Train 15 | out_loss 63.46704864501953: 100%|█████| 8/8 [00:00<00:00, 153.58it/s]\n",
      "Train 16 | out_loss 63.335628509521484: 100%|████| 8/8 [00:00<00:00, 156.67it/s]\n",
      "Train 17 | out_loss 63.478546142578125: 100%|████| 8/8 [00:00<00:00, 159.28it/s]\n",
      "Train 18 | out_loss 60.86749267578125: 100%|█████| 8/8 [00:00<00:00, 160.19it/s]\n",
      "Train 19 | out_loss 60.61685562133789: 100%|█████| 8/8 [00:00<00:00, 157.75it/s]\n",
      "Train 20 | out_loss 59.437198638916016: 100%|████| 8/8 [00:00<00:00, 141.37it/s]\n",
      "Train 21 | out_loss 57.191322326660156: 100%|████| 8/8 [00:00<00:00, 159.45it/s]\n",
      "Train 22 | out_loss 56.53840637207031: 100%|█████| 8/8 [00:00<00:00, 159.17it/s]\n",
      "Train 23 | out_loss 56.59222412109375: 100%|█████| 8/8 [00:00<00:00, 147.07it/s]\n",
      "Train 24 | out_loss 55.05754470825195: 100%|█████| 8/8 [00:00<00:00, 157.09it/s]\n",
      "Train 25 | out_loss 53.24253463745117: 100%|█████| 8/8 [00:00<00:00, 154.30it/s]\n",
      "Train 26 | out_loss 53.05616760253906: 100%|█████| 8/8 [00:00<00:00, 156.52it/s]\n",
      "Train 27 | out_loss 51.72322082519531: 100%|█████| 8/8 [00:00<00:00, 153.73it/s]\n",
      "Train 28 | out_loss 50.88424301147461: 100%|█████| 8/8 [00:00<00:00, 145.61it/s]\n",
      "Train 29 | out_loss 49.558128356933594: 100%|████| 8/8 [00:00<00:00, 153.51it/s]\n",
      "Train 30 | out_loss 48.816585540771484: 100%|████| 8/8 [00:00<00:00, 155.70it/s]\n",
      "Train 31 | out_loss 47.87942123413086: 100%|█████| 8/8 [00:00<00:00, 156.35it/s]\n",
      "Train 32 | out_loss 46.62918472290039: 100%|█████| 8/8 [00:00<00:00, 155.34it/s]\n",
      "Train 33 | out_loss 46.05249786376953: 100%|█████| 8/8 [00:00<00:00, 151.29it/s]\n",
      "Train 34 | out_loss 44.904022216796875: 100%|████| 8/8 [00:00<00:00, 140.08it/s]\n",
      "Train 35 | out_loss 43.66764831542969: 100%|█████| 8/8 [00:00<00:00, 156.54it/s]\n",
      "Train 36 | out_loss 42.97712707519531: 100%|█████| 8/8 [00:00<00:00, 157.61it/s]\n",
      "Train 37 | out_loss 41.55887222290039: 100%|█████| 8/8 [00:00<00:00, 153.37it/s]\n",
      "Train 38 | out_loss 41.14066696166992: 100%|█████| 8/8 [00:00<00:00, 157.38it/s]\n",
      "Train 39 | out_loss 39.571266174316406: 100%|████| 8/8 [00:00<00:00, 158.37it/s]\n",
      "Train 40 | out_loss 39.06513214111328: 100%|█████| 8/8 [00:00<00:00, 151.93it/s]\n",
      "Train 41 | out_loss 37.817745208740234: 100%|████| 8/8 [00:00<00:00, 155.76it/s]\n",
      "Train 42 | out_loss 36.9181022644043: 100%|██████| 8/8 [00:00<00:00, 152.06it/s]\n",
      "Train 43 | out_loss 35.98994827270508: 100%|█████| 8/8 [00:00<00:00, 151.90it/s]\n",
      "Train 44 | out_loss 35.015953063964844: 100%|████| 8/8 [00:00<00:00, 152.69it/s]\n",
      "Train 45 | out_loss 34.088470458984375: 100%|████| 8/8 [00:00<00:00, 159.12it/s]\n",
      "Train 46 | out_loss 33.0456428527832: 100%|██████| 8/8 [00:00<00:00, 157.98it/s]\n",
      "Train 47 | out_loss 32.16447830200195: 100%|█████| 8/8 [00:00<00:00, 154.97it/s]\n",
      "Train 48 | out_loss 31.2313232421875: 100%|██████| 8/8 [00:00<00:00, 157.47it/s]\n",
      "Train 49 | out_loss 30.243083953857422: 100%|████| 8/8 [00:00<00:00, 157.19it/s]\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]/home/AL_main_new/utils.py:99: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  x = torch.tensor([x for x,y in batch], dtype=torch.float32)\n",
      "Train 0 | out_loss 80.43840026855469: 100%|███████| 8/8 [00:00<00:00, 18.43it/s]\n",
      "Train 1 | out_loss 80.06683349609375: 100%|██████| 8/8 [00:00<00:00, 144.17it/s]\n",
      "Train 2 | out_loss 75.89752960205078: 100%|██████| 8/8 [00:00<00:00, 136.50it/s]\n",
      "Train 3 | out_loss 73.77307891845703: 100%|██████| 8/8 [00:00<00:00, 138.62it/s]\n",
      "Train 4 | out_loss 76.5927505493164: 100%|███████| 8/8 [00:00<00:00, 142.75it/s]\n",
      "Train 5 | out_loss 73.44481658935547: 100%|██████| 8/8 [00:00<00:00, 139.82it/s]\n",
      "Train 6 | out_loss 74.0082015991211: 100%|███████| 8/8 [00:00<00:00, 140.36it/s]\n",
      "Train 7 | out_loss 70.87808990478516: 100%|██████| 8/8 [00:00<00:00, 141.13it/s]\n",
      "Train 8 | out_loss 70.86625671386719: 100%|██████| 8/8 [00:00<00:00, 139.45it/s]\n",
      "Train 9 | out_loss 69.96812438964844: 100%|██████| 8/8 [00:00<00:00, 141.78it/s]\n",
      "Train 10 | out_loss 67.69815063476562: 100%|█████| 8/8 [00:00<00:00, 141.81it/s]\n",
      "Train 11 | out_loss 70.253662109375: 100%|███████| 8/8 [00:00<00:00, 142.75it/s]\n",
      "Train 12 | out_loss 68.1557846069336: 100%|██████| 8/8 [00:00<00:00, 142.84it/s]\n",
      "Train 13 | out_loss 64.00973510742188: 100%|█████| 8/8 [00:00<00:00, 142.88it/s]\n",
      "Train 14 | out_loss 62.50696563720703: 100%|█████| 8/8 [00:00<00:00, 140.74it/s]\n",
      "Train 15 | out_loss 68.0792465209961: 100%|██████| 8/8 [00:00<00:00, 141.18it/s]\n",
      "Train 16 | out_loss 62.74382019042969: 100%|█████| 8/8 [00:00<00:00, 141.24it/s]\n",
      "Train 17 | out_loss 60.00397872924805: 100%|█████| 8/8 [00:00<00:00, 131.87it/s]\n",
      "Train 18 | out_loss 59.98108673095703: 100%|█████| 8/8 [00:00<00:00, 136.44it/s]\n",
      "Train 19 | out_loss 59.88547134399414: 100%|█████| 8/8 [00:00<00:00, 140.00it/s]\n",
      "Train 20 | out_loss 58.59825134277344: 100%|█████| 8/8 [00:00<00:00, 109.79it/s]\n",
      "Train 21 | out_loss 56.43394088745117: 100%|█████| 8/8 [00:00<00:00, 136.97it/s]\n",
      "Train 22 | out_loss 56.95343017578125: 100%|█████| 8/8 [00:00<00:00, 140.01it/s]\n",
      "Train 23 | out_loss 55.99816131591797: 100%|█████| 8/8 [00:00<00:00, 140.32it/s]\n",
      "Train 24 | out_loss 53.85346221923828: 100%|█████| 8/8 [00:00<00:00, 135.83it/s]\n",
      "Train 25 | out_loss 53.15435028076172: 100%|█████| 8/8 [00:00<00:00, 135.86it/s]\n",
      "Train 26 | out_loss 52.35555648803711: 100%|█████| 8/8 [00:00<00:00, 141.91it/s]\n",
      "Train 27 | out_loss 50.99073791503906: 100%|█████| 8/8 [00:00<00:00, 141.77it/s]\n",
      "Train 28 | out_loss 50.244163513183594: 100%|████| 8/8 [00:00<00:00, 140.80it/s]\n",
      "Train 29 | out_loss 49.34428024291992: 100%|█████| 8/8 [00:00<00:00, 126.05it/s]\n",
      "Train 30 | out_loss 47.96443176269531: 100%|█████| 8/8 [00:00<00:00, 137.70it/s]\n",
      "Train 31 | out_loss 47.20622634887695: 100%|█████| 8/8 [00:00<00:00, 139.75it/s]\n",
      "Train 32 | out_loss 46.28666687011719: 100%|█████| 8/8 [00:00<00:00, 141.79it/s]\n",
      "Train 33 | out_loss 45.33556365966797: 100%|█████| 8/8 [00:00<00:00, 141.62it/s]\n",
      "Train 34 | out_loss 44.0860595703125: 100%|██████| 8/8 [00:00<00:00, 140.55it/s]\n",
      "Train 35 | out_loss 42.966468811035156: 100%|████| 8/8 [00:00<00:00, 139.56it/s]\n",
      "Train 36 | out_loss 42.547943115234375: 100%|████| 8/8 [00:00<00:00, 141.62it/s]\n",
      "Train 37 | out_loss 41.174407958984375: 100%|████| 8/8 [00:00<00:00, 141.84it/s]\n",
      "Train 38 | out_loss 40.04161071777344: 100%|█████| 8/8 [00:00<00:00, 141.58it/s]\n",
      "Train 39 | out_loss 39.454776763916016: 100%|████| 8/8 [00:00<00:00, 101.82it/s]\n",
      "Train 40 | out_loss 38.65217590332031: 100%|█████| 8/8 [00:00<00:00, 131.81it/s]\n",
      "Train 41 | out_loss 37.292667388916016: 100%|████| 8/8 [00:00<00:00, 122.88it/s]\n",
      "Train 42 | out_loss 36.50733184814453: 100%|█████| 8/8 [00:00<00:00, 125.54it/s]\n",
      "Train 43 | out_loss 35.220394134521484: 100%|████| 8/8 [00:00<00:00, 140.79it/s]\n",
      "Train 44 | out_loss 34.6325798034668: 100%|██████| 8/8 [00:00<00:00, 129.10it/s]\n",
      "Train 45 | out_loss 33.52259826660156: 100%|█████| 8/8 [00:00<00:00, 137.61it/s]\n",
      "Train 46 | out_loss 32.61160659790039: 100%|█████| 8/8 [00:00<00:00, 128.43it/s]\n",
      "Train 47 | out_loss 31.42249870300293: 100%|█████| 8/8 [00:00<00:00, 125.30it/s]\n",
      "Train 48 | out_loss 30.882131576538086: 100%|████| 8/8 [00:00<00:00, 137.67it/s]\n",
      "Train 49 | out_loss 29.71238136291504: 100%|█████| 8/8 [00:00<00:00, 140.58it/s]\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/AL_main_new/dis_train_al.py\", line 372, in <module>\n",
      "    main()\n",
      "  File \"/home/AL_main_new/dis_train_al.py\", line 325, in main\n",
      "    model = model.cuda()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 689, in cuda\n",
      "    return self._apply(lambda t: t.cuda(device))\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 579, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 579, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 579, in _apply\n",
      "    module._apply(fn)\n",
      "  [Previous line repeated 2 more times]\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 602, in _apply\n",
      "    param_applied = fn(param)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 689, in <lambda>\n",
      "    return self._apply(lambda t: t.cuda(device))\n",
      "KeyboardInterrupt\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]/home/AL_main_new/utils.py:99: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  x = torch.tensor([x for x,y in batch], dtype=torch.float32)\n",
      "Train 0 | out_loss 80.24359130859375: 100%|███████| 8/8 [00:00<00:00, 17.96it/s]\n",
      "Train 1 | out_loss 79.88316345214844: 100%|██████| 8/8 [00:00<00:00, 115.40it/s]\n",
      "Train 2 | out_loss 74.56044006347656: 100%|██████| 8/8 [00:00<00:00, 115.69it/s]\n",
      "Train 3 | out_loss 75.41255187988281: 100%|██████| 8/8 [00:00<00:00, 113.81it/s]\n",
      "Train 4 | out_loss 77.45304870605469: 100%|██████| 8/8 [00:00<00:00, 116.12it/s]\n",
      "Train 5 | out_loss 74.6749267578125: 100%|███████| 8/8 [00:00<00:00, 113.16it/s]\n",
      "Train 6 | out_loss 72.18120574951172: 100%|██████| 8/8 [00:00<00:00, 110.27it/s]\n",
      "Train 7 | out_loss 70.08329010009766: 100%|██████| 8/8 [00:00<00:00, 115.48it/s]\n",
      "Train 8 | out_loss 70.05838775634766: 100%|██████| 8/8 [00:00<00:00, 118.37it/s]\n",
      "Train 9 | out_loss 69.59609985351562: 100%|██████| 8/8 [00:00<00:00, 118.00it/s]\n",
      "Train 10 | out_loss 71.2622299194336: 100%|██████| 8/8 [00:00<00:00, 118.24it/s]\n",
      "Train 11 | out_loss 68.6736068725586: 100%|██████| 8/8 [00:00<00:00, 115.01it/s]\n",
      "Train 12 | out_loss 66.06277465820312: 100%|█████| 8/8 [00:00<00:00, 105.02it/s]\n",
      "Train 13 | out_loss 62.693443298339844: 100%|████| 8/8 [00:00<00:00, 106.75it/s]\n",
      "Train 14 | out_loss 64.92950439453125: 100%|██████| 8/8 [00:00<00:00, 91.77it/s]\n",
      "Train 15 | out_loss 62.49617385864258: 100%|█████| 8/8 [00:00<00:00, 110.71it/s]\n",
      "Train 16 | out_loss 62.999176025390625: 100%|████| 8/8 [00:00<00:00, 108.11it/s]\n",
      "Train 17 | out_loss 64.04316711425781: 100%|█████| 8/8 [00:00<00:00, 113.24it/s]\n",
      "Train 18 | out_loss 60.260093688964844: 100%|████| 8/8 [00:00<00:00, 116.76it/s]\n",
      "Train 19 | out_loss 57.43130111694336: 100%|█████| 8/8 [00:00<00:00, 107.74it/s]\n",
      "Train 20 | out_loss 57.15825653076172: 100%|█████| 8/8 [00:00<00:00, 113.22it/s]\n",
      "Train 21 | out_loss 59.28297805786133: 100%|█████| 8/8 [00:00<00:00, 114.91it/s]\n",
      "Train 22 | out_loss 61.188053131103516: 100%|████| 8/8 [00:00<00:00, 116.47it/s]\n",
      "Train 23 | out_loss 57.561241149902344: 100%|████| 8/8 [00:00<00:00, 116.58it/s]\n",
      "Train 24 | out_loss 52.083160400390625: 100%|████| 8/8 [00:00<00:00, 117.20it/s]\n",
      "Train 25 | out_loss 51.23246765136719: 100%|█████| 8/8 [00:00<00:00, 105.43it/s]\n",
      "Train 26 | out_loss 52.92039108276367: 100%|█████| 8/8 [00:00<00:00, 113.09it/s]\n",
      "Train 27 | out_loss 53.06496047973633: 100%|█████| 8/8 [00:00<00:00, 117.16it/s]\n",
      "Train 28 | out_loss 51.48799133300781: 100%|█████| 8/8 [00:00<00:00, 116.15it/s]\n",
      "Train 29 | out_loss 49.765201568603516: 100%|████| 8/8 [00:00<00:00, 117.96it/s]\n",
      "Train 30 | out_loss 48.504295349121094: 100%|████| 8/8 [00:00<00:00, 116.18it/s]\n",
      "Train 31 | out_loss 47.523651123046875: 100%|████| 8/8 [00:00<00:00, 114.75it/s]\n",
      "Train 32 | out_loss 46.82326889038086: 100%|█████| 8/8 [00:00<00:00, 114.54it/s]\n",
      "Train 33 | out_loss 46.12373733520508: 100%|█████| 8/8 [00:00<00:00, 109.43it/s]\n",
      "Train 34 | out_loss 44.91117477416992: 100%|█████| 8/8 [00:00<00:00, 106.40it/s]\n",
      "Train 35 | out_loss 43.690608978271484: 100%|████| 8/8 [00:00<00:00, 100.76it/s]\n",
      "Train 36 | out_loss 42.81014633178711: 100%|█████| 8/8 [00:00<00:00, 117.05it/s]\n",
      "Train 37 | out_loss 41.954315185546875: 100%|████| 8/8 [00:00<00:00, 116.91it/s]\n",
      "Train 38 | out_loss 40.88855743408203: 100%|█████| 8/8 [00:00<00:00, 116.78it/s]\n",
      "Train 39 | out_loss 39.87163162231445: 100%|█████| 8/8 [00:00<00:00, 115.06it/s]\n",
      "Train 40 | out_loss 38.933223724365234: 100%|████| 8/8 [00:00<00:00, 113.88it/s]\n",
      "Train 41 | out_loss 37.942832946777344: 100%|████| 8/8 [00:00<00:00, 116.44it/s]\n",
      "Train 42 | out_loss 36.99890899658203: 100%|█████| 8/8 [00:00<00:00, 111.95it/s]\n",
      "Train 43 | out_loss 36.015830993652344: 100%|█████| 8/8 [00:00<00:00, 93.29it/s]\n",
      "Train 44 | out_loss 35.035118103027344: 100%|████| 8/8 [00:00<00:00, 115.60it/s]\n",
      "Train 45 | out_loss 34.14024353027344: 100%|█████| 8/8 [00:00<00:00, 117.93it/s]\n",
      "Train 46 | out_loss 33.12535858154297: 100%|█████| 8/8 [00:00<00:00, 117.95it/s]\n",
      "Train 47 | out_loss 32.184024810791016: 100%|████| 8/8 [00:00<00:00, 116.65it/s]\n",
      "Train 48 | out_loss 31.242414474487305: 100%|████| 8/8 [00:00<00:00, 115.23it/s]\n",
      "Train 49 | out_loss 30.31960678100586: 100%|█████| 8/8 [00:00<00:00, 115.56it/s]\n"
     ]
    }
   ],
   "source": [
    "# LinearAL \n",
    "\n",
    "data = \"paint\"\n",
    "model =  \"linearal\"\n",
    "for layer in range(1,11):\n",
    "#for layer in [3]:\n",
    "    log = f\"result/{data}_{model}_l{layer}.log\"\n",
    "    !python3 dis_train_al.py --dataset {data} --model {model} --epoch 500 --num-layer {layer} --task regression  > {log}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6ae70865",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/AL_main_new/new dataset.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B140.115.59.235/home/AL_main_new/new%20dataset.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B140.115.59.235/home/AL_main_new/new%20dataset.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B140.115.59.235/home/AL_main_new/new%20dataset.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m args\u001b[39m.\u001b[39mtask \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mregression\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B140.115.59.235/home/AL_main_new/new%20dataset.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m args\u001b[39m.\u001b[39mfeature_dim \u001b[39m=\u001b[39m \u001b[39m40\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B140.115.59.235/home/AL_main_new/new%20dataset.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m df \u001b[39m=\u001b[39m load_dataset(\u001b[39m'\u001b[39m\u001b[39mLL0_296_ailerons\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "from mit_d3m import load_dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "args.task = \"regression\"\n",
    "args.feature_dim = 40\n",
    "df = load_dataset('LL0_296_ailerons')\n",
    "\n",
    "col_feature = df.X.columns[1:]\n",
    "#col_target= df.y.columns[:]\n",
    "\n",
    "y = df.y\n",
    "x = df.X[col_feature]\n",
    "x = x.fillna(0)\n",
    "feature_train, feature_test, train_target, test_target = train_test_split(x, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "833b414e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8800,)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target.to_numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7675b11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training\n",
      "  0%|                                                   | 0/138 [00:00<?, ?it/s]/home/AL_main_new/utils.py:99: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  x = torch.tensor([x for x,y in batch], dtype=torch.float32)\n",
      "Train 0 | out_loss 0.09604600071907043: 100%|█| 138/138 [00:00<00:00, 174.00it/s\n",
      "Train Epoch0 out_loss -54904.53125\n",
      "Test Epoch0 layer0 out_loss 0.3226132094860077\n",
      "Test Epoch0 layer1 out_loss 0.261636346578598\n",
      "Test Epoch0 layer2 out_loss 0.0825749933719635\n",
      "Train 1 | out_loss 0.0734870508313179: 100%|█| 138/138 [00:00<00:00, 332.80it/s]\n",
      "Train Epoch1 out_loss -32141.44140625\n",
      "Test Epoch1 layer0 out_loss 0.259891539812088\n",
      "Test Epoch1 layer1 out_loss 0.191255584359169\n",
      "Test Epoch1 layer2 out_loss 0.06840277463197708\n",
      "Train 2 | out_loss 0.06319232285022736: 100%|█| 138/138 [00:00<00:00, 334.77it/s\n",
      "Train Epoch2 out_loss -23766.646484375\n",
      "Test Epoch2 layer0 out_loss 0.34146255254745483\n",
      "Test Epoch2 layer1 out_loss 0.13012146949768066\n",
      "Test Epoch2 layer2 out_loss 0.05562148615717888\n",
      "Train 3 | out_loss 0.05656816065311432: 100%|█| 138/138 [00:00<00:00, 292.12it/s\n",
      "Train Epoch3 out_loss -19044.921875\n",
      "Test Epoch3 layer0 out_loss 0.32723450660705566\n",
      "Test Epoch3 layer1 out_loss 0.15748746693134308\n",
      "Test Epoch3 layer2 out_loss 0.056333377957344055\n",
      "Train 4 | out_loss 0.058965373784303665: 100%|█| 138/138 [00:00<00:00, 333.99it/\n",
      "Train Epoch4 out_loss -20693.3515625\n",
      "Test Epoch4 layer0 out_loss 0.23291215300559998\n",
      "Test Epoch4 layer1 out_loss 0.14360038936138153\n",
      "Test Epoch4 layer2 out_loss 0.05996169149875641\n",
      "Train 5 | out_loss 0.06477592140436172: 100%|█| 138/138 [00:00<00:00, 330.98it/s\n",
      "Train Epoch5 out_loss -24972.80859375\n",
      "Test Epoch5 layer0 out_loss 0.1989891529083252\n",
      "Test Epoch5 layer1 out_loss 0.17528913915157318\n",
      "Test Epoch5 layer2 out_loss 0.06678901612758636\n",
      "Train 6 | out_loss 0.06665418297052383: 100%|█| 138/138 [00:00<00:00, 335.56it/s\n",
      "Train Epoch6 out_loss -26442.09765625\n",
      "Test Epoch6 layer0 out_loss 0.25958555936813354\n",
      "Test Epoch6 layer1 out_loss 0.16691675782203674\n",
      "Test Epoch6 layer2 out_loss 0.06859374046325684\n",
      "Train 7 | out_loss 0.06661653518676758: 100%|█| 138/138 [00:00<00:00, 330.59it/s\n",
      "Train Epoch7 out_loss -26412.236328125\n",
      "Test Epoch7 layer0 out_loss 0.24077747762203217\n",
      "Test Epoch7 layer1 out_loss 0.12532556056976318\n",
      "Test Epoch7 layer2 out_loss 0.06274313479661942\n",
      "Train 8 | out_loss 0.05879414454102516: 100%|█| 138/138 [00:00<00:00, 333.37it/s\n",
      "Train Epoch8 out_loss -20573.345703125\n",
      "Test Epoch8 layer0 out_loss 0.20965445041656494\n",
      "Test Epoch8 layer1 out_loss 0.08589303493499756\n",
      "Test Epoch8 layer2 out_loss 0.053600385785102844\n",
      "Train 9 | out_loss 0.04935203492641449: 100%|█| 138/138 [00:00<00:00, 327.98it/s\n",
      "Train Epoch9 out_loss -14495.6640625\n",
      "Test Epoch9 layer0 out_loss 0.19961246848106384\n",
      "Test Epoch9 layer1 out_loss 0.06862387806177139\n",
      "Test Epoch9 layer2 out_loss 0.044294558465480804\n",
      "Train 10 | out_loss 0.04145931079983711: 100%|█| 138/138 [00:00<00:00, 327.83it/\n",
      "Train Epoch10 out_loss -10229.6220703125\n",
      "Test Epoch10 layer0 out_loss 0.19439083337783813\n",
      "Test Epoch10 layer1 out_loss 0.055396370589733124\n",
      "Test Epoch10 layer2 out_loss 0.03816363960504532\n",
      "Train 11 | out_loss 0.03598954156041145: 100%|█| 138/138 [00:00<00:00, 324.10it/\n",
      "Train Epoch11 out_loss -7708.22021484375\n",
      "Test Epoch11 layer0 out_loss 0.1954207867383957\n",
      "Test Epoch11 layer1 out_loss 0.044552069157361984\n",
      "Test Epoch11 layer2 out_loss 0.03370403125882149\n",
      "Train 12 | out_loss 0.033009979873895645: 100%|█| 138/138 [00:00<00:00, 327.10it\n",
      "Train Epoch12 out_loss -6484.57177734375\n",
      "Test Epoch12 layer0 out_loss 0.19915945827960968\n",
      "Test Epoch12 layer1 out_loss 0.04896198585629463\n",
      "Test Epoch12 layer2 out_loss 0.030901910737156868\n",
      "Train 13 | out_loss 0.03031942993402481: 100%|█| 138/138 [00:00<00:00, 326.53it/\n",
      "Train Epoch13 out_loss -5470.4189453125\n",
      "Test Epoch13 layer0 out_loss 0.20217952132225037\n",
      "Test Epoch13 layer1 out_loss 0.04390402138233185\n",
      "Test Epoch13 layer2 out_loss 0.029252953827381134\n",
      "Train 14 | out_loss 0.028342517092823982: 100%|█| 138/138 [00:00<00:00, 328.59it\n",
      "Train Epoch14 out_loss -4780.17919921875\n",
      "Test Epoch14 layer0 out_loss 0.20083770155906677\n",
      "Test Epoch14 layer1 out_loss 0.04242235794663429\n",
      "Test Epoch14 layer2 out_loss 0.0267756637185812\n",
      "Train 15 | out_loss 0.026281066238880157: 100%|█| 138/138 [00:00<00:00, 328.03it\n",
      "Train Epoch15 out_loss -4109.96484375\n",
      "Test Epoch15 layer0 out_loss 0.19978222250938416\n",
      "Test Epoch15 layer1 out_loss 0.04167638346552849\n",
      "Test Epoch15 layer2 out_loss 0.024747926741838455\n",
      "Train 16 | out_loss 0.024404438212513924: 100%|█| 138/138 [00:00<00:00, 319.88it\n",
      "Train Epoch16 out_loss -3543.8310546875\n",
      "Test Epoch16 layer0 out_loss 0.19769570231437683\n",
      "Test Epoch16 layer1 out_loss 0.04088999330997467\n",
      "Test Epoch16 layer2 out_loss 0.02304157055914402\n",
      "Train 17 | out_loss 0.022718843072652817: 100%|█| 138/138 [00:00<00:00, 322.56it\n",
      "Train Epoch17 out_loss -3071.064208984375\n",
      "Test Epoch17 layer0 out_loss 0.19135752320289612\n",
      "Test Epoch17 layer1 out_loss 0.04159956052899361\n",
      "Test Epoch17 layer2 out_loss 0.021511325612664223\n",
      "Train 18 | out_loss 0.021092673763632774: 100%|█| 138/138 [00:00<00:00, 320.10it\n",
      "Train Epoch18 out_loss -2647.01953125\n",
      "Test Epoch18 layer0 out_loss 0.19044850766658783\n",
      "Test Epoch18 layer1 out_loss 0.04013523831963539\n",
      "Test Epoch18 layer2 out_loss 0.019946927204728127\n",
      "Train 19 | out_loss 0.019490405917167664: 100%|█| 138/138 [00:00<00:00, 326.74it\n",
      "Train Epoch19 out_loss -2259.991943359375\n",
      "Test Epoch19 layer0 out_loss 0.18159028887748718\n",
      "Test Epoch19 layer1 out_loss 0.037707265466451645\n",
      "Test Epoch19 layer2 out_loss 0.018440138548612595\n",
      "Train 20 | out_loss 0.0180747639387846: 100%|█| 138/138 [00:00<00:00, 327.53it/s\n",
      "Train Epoch20 out_loss -1943.4786376953125\n",
      "Test Epoch20 layer0 out_loss 0.17604206502437592\n",
      "Test Epoch20 layer1 out_loss 0.036906421184539795\n",
      "Test Epoch20 layer2 out_loss 0.01719488576054573\n",
      "Train 21 | out_loss 0.016849378123879433: 100%|█| 138/138 [00:00<00:00, 329.64it\n",
      "Train Epoch21 out_loss -1688.7608642578125\n",
      "Test Epoch21 layer0 out_loss 0.1622474640607834\n",
      "Test Epoch21 layer1 out_loss 0.03498287871479988\n",
      "Test Epoch21 layer2 out_loss 0.016101809218525887\n",
      "Train 22 | out_loss 0.015835756435990334: 100%|█| 138/138 [00:00<00:00, 326.87it\n",
      "Train Epoch22 out_loss -1491.5718994140625\n",
      "Test Epoch22 layer0 out_loss 0.1604360193014145\n",
      "Test Epoch22 layer1 out_loss 0.03373132646083832\n",
      "Test Epoch22 layer2 out_loss 0.015226462855935097\n",
      "Train 23 | out_loss 0.015027128159999847: 100%|█| 138/138 [00:00<00:00, 322.38it\n",
      "Train Epoch23 out_loss -1343.03173828125\n",
      "Test Epoch23 layer0 out_loss 0.15551608800888062\n",
      "Test Epoch23 layer1 out_loss 0.03306618332862854\n",
      "Test Epoch23 layer2 out_loss 0.014548564329743385\n",
      "Train 24 | out_loss 0.014399304054677486: 100%|█| 138/138 [00:00<00:00, 324.39it\n",
      "Train Epoch24 out_loss -1233.0712890625\n",
      "Test Epoch24 layer0 out_loss 0.10881383717060089\n",
      "Test Epoch24 layer1 out_loss 0.032333921641111374\n",
      "Test Epoch24 layer2 out_loss 0.014082590118050575\n",
      "Train 25 | out_loss 0.013872340321540833: 100%|█| 138/138 [00:00<00:00, 325.39it\n",
      "Train Epoch25 out_loss -1144.400390625\n",
      "Test Epoch25 layer0 out_loss 0.10693833976984024\n",
      "Test Epoch25 layer1 out_loss 0.031928785145282745\n",
      "Test Epoch25 layer2 out_loss 0.01346407551318407\n",
      "Train 26 | out_loss 0.013252245262265205: 100%|█| 138/138 [00:00<00:00, 326.99it\n",
      "Train Epoch26 out_loss -1044.2894287109375\n",
      "Test Epoch26 layer0 out_loss 0.10394231975078583\n",
      "Test Epoch26 layer1 out_loss 0.03234267234802246\n",
      "Test Epoch26 layer2 out_loss 0.012902350164949894\n",
      "Train 27 | out_loss 0.012687437236309052: 100%|█| 138/138 [00:00<00:00, 325.28it\n",
      "Train Epoch27 out_loss -957.087646484375\n",
      "Test Epoch27 layer0 out_loss 0.10250382870435715\n",
      "Test Epoch27 layer1 out_loss 0.031243957579135895\n",
      "Test Epoch27 layer2 out_loss 0.012352971360087395\n",
      "Train 28 | out_loss 0.01218477450311184: 100%|█| 138/138 [00:00<00:00, 327.95it/\n",
      "Train Epoch28 out_loss -882.6755981445312\n",
      "Test Epoch28 layer0 out_loss 0.09840995073318481\n",
      "Test Epoch28 layer1 out_loss 0.03036608174443245\n",
      "Test Epoch28 layer2 out_loss 0.01188281923532486\n",
      "Train 29 | out_loss 0.01165946014225483: 100%|█| 138/138 [00:00<00:00, 318.27it/\n",
      "Train Epoch29 out_loss -808.1228637695312\n",
      "Test Epoch29 layer0 out_loss 0.09668073058128357\n",
      "Test Epoch29 layer1 out_loss 0.02954941987991333\n",
      "Test Epoch29 layer2 out_loss 0.011339117772877216\n",
      "Train 30 | out_loss 0.011184513568878174: 100%|█| 138/138 [00:00<00:00, 319.58it\n",
      "Train Epoch30 out_loss -743.5465698242188\n",
      "Test Epoch30 layer0 out_loss 0.0948183536529541\n",
      "Test Epoch30 layer1 out_loss 0.028555214405059814\n",
      "Test Epoch30 layer2 out_loss 0.010846524499356747\n",
      "Train 31 | out_loss 0.010699581354856491: 100%|█| 138/138 [00:00<00:00, 314.65it\n",
      "Train Epoch31 out_loss -680.38330078125\n",
      "Test Epoch31 layer0 out_loss 0.09191551804542542\n",
      "Test Epoch31 layer1 out_loss 0.027729257941246033\n",
      "Test Epoch31 layer2 out_loss 0.010461816564202309\n",
      "Train 32 | out_loss 0.010277317836880684: 100%|█| 138/138 [00:00<00:00, 322.23it\n",
      "Train Epoch32 out_loss -627.6622924804688\n",
      "Test Epoch32 layer0 out_loss 0.08975411206483841\n",
      "Test Epoch32 layer1 out_loss 0.0266818068921566\n",
      "Test Epoch32 layer2 out_loss 0.010047174990177155\n",
      "Train 33 | out_loss 0.009866401553153992: 100%|█| 138/138 [00:00<00:00, 324.32it\n",
      "Train Epoch33 out_loss -578.3955688476562\n",
      "Test Epoch33 layer0 out_loss 0.08435403555631638\n",
      "Test Epoch33 layer1 out_loss 0.02567128650844097\n",
      "Test Epoch33 layer2 out_loss 0.009636576287448406\n",
      "Train 34 | out_loss 0.009502631612122059: 100%|█| 138/138 [00:00<00:00, 321.15it\n",
      "Train Epoch34 out_loss -536.4590454101562\n",
      "Test Epoch34 layer0 out_loss 0.08063679188489914\n",
      "Test Epoch34 layer1 out_loss 0.025106826797127724\n",
      "Test Epoch34 layer2 out_loss 0.009213369339704514\n",
      "Train 35 | out_loss 0.009106959216296673: 100%|█| 138/138 [00:00<00:00, 320.18it\n",
      "Train Epoch35 out_loss -492.633544921875\n",
      "Test Epoch35 layer0 out_loss 0.07604766637086868\n",
      "Test Epoch35 layer1 out_loss 0.023601386696100235\n",
      "Test Epoch35 layer2 out_loss 0.008885873481631279\n",
      "Train 36 | out_loss 0.008796578273177147: 100%|█| 138/138 [00:00<00:00, 319.63it\n",
      "Train Epoch36 out_loss -459.55859375\n",
      "Test Epoch36 layer0 out_loss 0.07252395153045654\n",
      "Test Epoch36 layer1 out_loss 0.023079628124833107\n",
      "Test Epoch36 layer2 out_loss 0.008571485057473183\n",
      "Train 37 | out_loss 0.008466075174510479: 100%|█| 138/138 [00:00<00:00, 322.95it\n",
      "Train Epoch37 out_loss -425.60089111328125\n",
      "Test Epoch37 layer0 out_loss 0.07002755254507065\n",
      "Test Epoch37 layer1 out_loss 0.021976253017783165\n",
      "Test Epoch37 layer2 out_loss 0.008266552351415157\n",
      "Train 38 | out_loss 0.008138669654726982: 100%|█| 138/138 [00:00<00:00, 329.83it\n",
      "Train Epoch38 out_loss -393.2433776855469\n",
      "Test Epoch38 layer0 out_loss 0.06771605461835861\n",
      "Test Epoch38 layer1 out_loss 0.02114049158990383\n",
      "Test Epoch38 layer2 out_loss 0.007940614596009254\n",
      "Train 39 | out_loss 0.007840457372367382: 100%|█| 138/138 [00:00<00:00, 329.49it\n",
      "Train Epoch39 out_loss -364.8813781738281\n",
      "Test Epoch39 layer0 out_loss 0.06535002589225769\n",
      "Test Epoch39 layer1 out_loss 0.02057633176445961\n",
      "Test Epoch39 layer2 out_loss 0.007634059060364962\n",
      "Train 40 | out_loss 0.007563027087599039: 100%|█| 138/138 [00:00<00:00, 323.24it\n",
      "Train Epoch40 out_loss -339.4466552734375\n",
      "Test Epoch40 layer0 out_loss 0.06281089782714844\n",
      "Test Epoch40 layer1 out_loss 0.019643615931272507\n",
      "Test Epoch40 layer2 out_loss 0.007364656310528517\n",
      "Train 41 | out_loss 0.007266479544341564: 100%|█| 138/138 [00:00<00:00, 318.46it\n",
      "Train Epoch41 out_loss -313.2720642089844\n",
      "Test Epoch41 layer0 out_loss 0.06043514236807823\n",
      "Test Epoch41 layer1 out_loss 0.018993332982063293\n",
      "Test Epoch41 layer2 out_loss 0.007140324451029301\n",
      "Train 42 | out_loss 0.007024493534117937: 100%|█| 138/138 [00:00<00:00, 320.27it\n",
      "Train Epoch42 out_loss -292.68896484375\n",
      "Test Epoch42 layer0 out_loss 0.05855570361018181\n",
      "Test Epoch42 layer1 out_loss 0.018288807943463326\n",
      "Test Epoch42 layer2 out_loss 0.006858143489807844\n",
      "Train 43 | out_loss 0.006784218363463879: 100%|█| 138/138 [00:00<00:00, 314.31it\n",
      "Train Epoch43 out_loss -272.9411926269531\n",
      "Test Epoch43 layer0 out_loss 0.057096485048532486\n",
      "Test Epoch43 layer1 out_loss 0.018001848831772804\n",
      "Test Epoch43 layer2 out_loss 0.006617656908929348\n",
      "Train 44 | out_loss 0.006544626317918301: 100%|█| 138/138 [00:00<00:00, 319.12it\n",
      "Train Epoch44 out_loss -253.93382263183594\n",
      "Test Epoch44 layer0 out_loss 0.05612236633896828\n",
      "Test Epoch44 layer1 out_loss 0.01750359684228897\n",
      "Test Epoch44 layer2 out_loss 0.0063964081928133965\n",
      "Train 45 | out_loss 0.006318728905171156: 100%|█| 138/138 [00:00<00:00, 319.69it\n",
      "Train Epoch45 out_loss -236.6387939453125\n",
      "Test Epoch45 layer0 out_loss 0.05504263564944267\n",
      "Test Epoch45 layer1 out_loss 0.016841929405927658\n",
      "Test Epoch45 layer2 out_loss 0.006133414804935455\n",
      "Train 46 | out_loss 0.0061034816317260265: 100%|█| 138/138 [00:00<00:00, 320.01i\n",
      "Train Epoch46 out_loss -220.72425842285156\n",
      "Test Epoch46 layer0 out_loss 0.05340253561735153\n",
      "Test Epoch46 layer1 out_loss 0.016371583566069603\n",
      "Test Epoch46 layer2 out_loss 0.00593276834115386\n",
      "Train 47 | out_loss 0.0059085930697619915: 100%|█| 138/138 [00:00<00:00, 316.97i\n",
      "Train Epoch47 out_loss -206.79078674316406\n",
      "Test Epoch47 layer0 out_loss 0.05298859253525734\n",
      "Test Epoch47 layer1 out_loss 0.01584785245358944\n",
      "Test Epoch47 layer2 out_loss 0.005772165488451719\n",
      "Train 48 | out_loss 0.005727848038077354: 100%|█| 138/138 [00:00<00:00, 321.28it\n",
      "Train Epoch48 out_loss -194.27215576171875\n",
      "Test Epoch48 layer0 out_loss 0.051613226532936096\n",
      "Test Epoch48 layer1 out_loss 0.015485621057450771\n",
      "Test Epoch48 layer2 out_loss 0.0055649541318416595\n",
      "Train 49 | out_loss 0.005555198527872562: 100%|█| 138/138 [00:00<00:00, 322.15it\n",
      "Train Epoch49 out_loss -182.67784118652344\n",
      "Test Epoch49 layer0 out_loss 0.050602156668901443\n",
      "Test Epoch49 layer1 out_loss 0.015291412360966206\n",
      "Test Epoch49 layer2 out_loss 0.005371780600398779\n",
      "Train 50 | out_loss 0.005358218215405941: 100%|█| 138/138 [00:00<00:00, 320.91it\n",
      "Train Epoch50 out_loss -169.88279724121094\n",
      "Test Epoch50 layer0 out_loss 0.04975834861397743\n",
      "Test Epoch50 layer1 out_loss 0.014776958152651787\n",
      "Test Epoch50 layer2 out_loss 0.005252758506685495\n",
      "Train 51 | out_loss 0.0052160234190523624: 100%|█| 138/138 [00:00<00:00, 327.61i\n",
      "Train Epoch51 out_loss -160.93348693847656\n",
      "Test Epoch51 layer0 out_loss 0.04928283765912056\n",
      "Test Epoch51 layer1 out_loss 0.014754047617316246\n",
      "Test Epoch51 layer2 out_loss 0.00505302008241415\n",
      "Train 52 | out_loss 0.005039550364017487: 100%|█| 138/138 [00:00<00:00, 328.40it\n",
      "Train Epoch52 out_loss -150.16146850585938\n",
      "Test Epoch52 layer0 out_loss 0.04913366213440895\n",
      "Test Epoch52 layer1 out_loss 0.014225723221898079\n",
      "Test Epoch52 layer2 out_loss 0.0048948354087769985\n",
      "Train 53 | out_loss 0.0048720696941018105: 100%|█| 138/138 [00:00<00:00, 324.31i\n",
      "Train Epoch53 out_loss -140.2813720703125\n",
      "Test Epoch53 layer0 out_loss 0.04831686243414879\n",
      "Test Epoch53 layer1 out_loss 0.013582139275968075\n",
      "Test Epoch53 layer2 out_loss 0.004773698281496763\n",
      "Train 54 | out_loss 0.00473471824079752: 100%|█| 138/138 [00:00<00:00, 322.07it/\n",
      "Train Epoch54 out_loss -132.42774963378906\n",
      "Test Epoch54 layer0 out_loss 0.048387907445430756\n",
      "Test Epoch54 layer1 out_loss 0.013268524780869484\n",
      "Test Epoch54 layer2 out_loss 0.0046021644957363605\n",
      "Train 55 | out_loss 0.004578110761940479: 100%|█| 138/138 [00:00<00:00, 286.97it\n",
      "Train Epoch55 out_loss -123.74708557128906\n",
      "Test Epoch55 layer0 out_loss 0.047818832099437714\n",
      "Test Epoch55 layer1 out_loss 0.013282381929457188\n",
      "Test Epoch55 layer2 out_loss 0.004442985635250807\n",
      "Train 56 | out_loss 0.004433845169842243: 100%|█| 138/138 [00:00<00:00, 328.16it\n",
      "Train Epoch56 out_loss -116.0088119506836\n",
      "Test Epoch56 layer0 out_loss 0.04682609438896179\n",
      "Test Epoch56 layer1 out_loss 0.012834994122385979\n",
      "Test Epoch56 layer2 out_loss 0.004296041559427977\n",
      "Train 57 | out_loss 0.004300769418478012: 100%|█| 138/138 [00:00<00:00, 319.52it\n",
      "Train Epoch57 out_loss -109.0905532836914\n",
      "Test Epoch57 layer0 out_loss 0.04556513950228691\n",
      "Test Epoch57 layer1 out_loss 0.013179413974285126\n",
      "Test Epoch57 layer2 out_loss 0.004204827826470137\n",
      "Train 58 | out_loss 0.004164710640907288: 100%|█| 138/138 [00:00<00:00, 319.57it\n",
      "Train Epoch58 out_loss -102.23509979248047\n",
      "Test Epoch58 layer0 out_loss 0.046020880341529846\n",
      "Test Epoch58 layer1 out_loss 0.012430427595973015\n",
      "Test Epoch58 layer2 out_loss 0.004030491691082716\n",
      "Train 59 | out_loss 0.004033695440739393: 100%|█| 138/138 [00:00<00:00, 316.13it\n",
      "Train Epoch59 out_loss -95.8420181274414\n",
      "Test Epoch59 layer0 out_loss 0.04570484533905983\n",
      "Test Epoch59 layer1 out_loss 0.012152547016739845\n",
      "Test Epoch59 layer2 out_loss 0.003939858637750149\n",
      "Train 60 | out_loss 0.003929477650672197: 100%|█| 138/138 [00:00<00:00, 317.62it\n",
      "Train Epoch60 out_loss -90.90247344970703\n",
      "Test Epoch60 layer0 out_loss 0.04387802258133888\n",
      "Test Epoch60 layer1 out_loss 0.011811360716819763\n",
      "Test Epoch60 layer2 out_loss 0.003777752397581935\n",
      "Train 61 | out_loss 0.0038017178885638714: 100%|█| 138/138 [00:00<00:00, 320.36i\n",
      "Train Epoch61 out_loss -85.02349090576172\n",
      "Test Epoch61 layer0 out_loss 0.043261174112558365\n",
      "Test Epoch61 layer1 out_loss 0.011562498286366463\n",
      "Test Epoch61 layer2 out_loss 0.0036744887474924326\n",
      "Train 62 | out_loss 0.0036995154805481434: 100%|█| 138/138 [00:00<00:00, 308.77i\n",
      "Train Epoch62 out_loss -80.46055603027344\n",
      "Test Epoch62 layer0 out_loss 0.04407452791929245\n",
      "Test Epoch62 layer1 out_loss 0.011306602507829666\n",
      "Test Epoch62 layer2 out_loss 0.0035805420484393835\n",
      "Train 63 | out_loss 0.0035977282095700502: 100%|█| 138/138 [00:00<00:00, 318.38i\n",
      "Train Epoch63 out_loss -76.03971099853516\n",
      "Test Epoch63 layer0 out_loss 0.04334605485200882\n",
      "Test Epoch63 layer1 out_loss 0.01115401927381754\n",
      "Test Epoch63 layer2 out_loss 0.003497145604342222\n",
      "Train 64 | out_loss 0.0034986594691872597: 100%|█| 138/138 [00:00<00:00, 321.65i\n",
      "Train Epoch64 out_loss -71.85526275634766\n",
      "Test Epoch64 layer0 out_loss 0.042399413883686066\n",
      "Test Epoch64 layer1 out_loss 0.011004634201526642\n",
      "Test Epoch64 layer2 out_loss 0.003363036783412099\n",
      "Train 65 | out_loss 0.003405002411454916: 100%|█| 138/138 [00:00<00:00, 316.27it\n",
      "Train Epoch65 out_loss -68.00688934326172\n",
      "Test Epoch65 layer0 out_loss 0.042327988892793655\n",
      "Test Epoch65 layer1 out_loss 0.010914242826402187\n",
      "Test Epoch65 layer2 out_loss 0.003258367534726858\n",
      "Train 66 | out_loss 0.003308679908514023: 100%|█| 138/138 [00:00<00:00, 322.79it\n",
      "Train Epoch66 out_loss -64.15792846679688\n",
      "Test Epoch66 layer0 out_loss 0.04123833775520325\n",
      "Test Epoch66 layer1 out_loss 0.010403397493064404\n",
      "Test Epoch66 layer2 out_loss 0.003204221837222576\n",
      "Train 67 | out_loss 0.0032130922190845013: 100%|█| 138/138 [00:00<00:00, 319.05i\n",
      "Train Epoch67 out_loss -60.44750213623047\n",
      "Test Epoch67 layer0 out_loss 0.03910326585173607\n",
      "Test Epoch67 layer1 out_loss 0.010259381495416164\n",
      "Test Epoch67 layer2 out_loss 0.0031146558467298746\n",
      "Train 68 | out_loss 0.003139479085803032: 100%|█| 138/138 [00:00<00:00, 318.83it\n",
      "Train Epoch68 out_loss -57.664188385009766\n",
      "Test Epoch68 layer0 out_loss 0.039239250123500824\n",
      "Test Epoch68 layer1 out_loss 0.010248489677906036\n",
      "Test Epoch68 layer2 out_loss 0.003030680352821946\n",
      "Train 69 | out_loss 0.0030716601759195328: 100%|█| 138/138 [00:00<00:00, 321.22i\n",
      "Train Epoch69 out_loss -55.15702819824219\n",
      "Test Epoch69 layer0 out_loss 0.03883266821503639\n",
      "Test Epoch69 layer1 out_loss 0.00997911673039198\n",
      "Test Epoch69 layer2 out_loss 0.0029694517143070698\n",
      "Train 70 | out_loss 0.0029790436383336782: 100%|█| 138/138 [00:00<00:00, 323.12i\n",
      "Train Epoch70 out_loss -51.821537017822266\n",
      "Test Epoch70 layer0 out_loss 0.0377330556511879\n",
      "Test Epoch70 layer1 out_loss 0.009861853905022144\n",
      "Test Epoch70 layer2 out_loss 0.0028729590121656656\n",
      "Train 71 | out_loss 0.002912646858021617: 100%|█| 138/138 [00:00<00:00, 324.37it\n",
      "Train Epoch71 out_loss -49.49324417114258\n",
      "Test Epoch71 layer0 out_loss 0.036998651921749115\n",
      "Test Epoch71 layer1 out_loss 0.009586887434124947\n",
      "Test Epoch71 layer2 out_loss 0.00283124390989542\n",
      "Train 72 | out_loss 0.0028456845320761204: 100%|█| 138/138 [00:00<00:00, 322.22i\n",
      "Train Epoch72 out_loss -47.19828414916992\n",
      "Test Epoch72 layer0 out_loss 0.03750685229897499\n",
      "Test Epoch72 layer1 out_loss 0.009435474872589111\n",
      "Test Epoch72 layer2 out_loss 0.0027248593978583813\n",
      "Train 73 | out_loss 0.0027776744682341814: 100%|█| 138/138 [00:00<00:00, 314.37i\n",
      "Train Epoch73 out_loss -44.921939849853516\n",
      "Test Epoch73 layer0 out_loss 0.03900284692645073\n",
      "Test Epoch73 layer1 out_loss 0.009236232377588749\n",
      "Test Epoch73 layer2 out_loss 0.0026746289804577827\n",
      "Train 74 | out_loss 0.0027025348972529173: 100%|█| 138/138 [00:00<00:00, 314.31i\n",
      "Train Epoch74 out_loss -42.47109603881836\n",
      "Test Epoch74 layer0 out_loss 0.039944302290678024\n",
      "Test Epoch74 layer1 out_loss 0.009226380847394466\n",
      "Test Epoch74 layer2 out_loss 0.002615822944790125\n",
      "Train 75 | out_loss 0.0026512988843023777: 100%|█| 138/138 [00:00<00:00, 321.05i\n",
      "Train Epoch75 out_loss -40.83842086791992\n",
      "Test Epoch75 layer0 out_loss 0.03794580698013306\n",
      "Test Epoch75 layer1 out_loss 0.008916276507079601\n",
      "Test Epoch75 layer2 out_loss 0.0025463218335062265\n",
      "Train 76 | out_loss 0.002596331527456641: 100%|█| 138/138 [00:00<00:00, 317.14it\n",
      "Train Epoch76 out_loss -39.1215934753418\n",
      "Test Epoch76 layer0 out_loss 0.036424312740564346\n",
      "Test Epoch76 layer1 out_loss 0.008774248883128166\n",
      "Test Epoch76 layer2 out_loss 0.002488570287823677\n",
      "Train 77 | out_loss 0.0025400531012564898: 100%|█| 138/138 [00:00<00:00, 299.58i\n",
      "Train Epoch77 out_loss -37.401058197021484\n",
      "Test Epoch77 layer0 out_loss 0.03724803403019905\n",
      "Test Epoch77 layer1 out_loss 0.008650087751448154\n",
      "Test Epoch77 layer2 out_loss 0.0024649514816701412\n",
      "Train 78 | out_loss 0.0025045506190508604: 100%|█| 138/138 [00:00<00:00, 316.42i\n",
      "Train Epoch78 out_loss -36.335147857666016\n",
      "Test Epoch78 layer0 out_loss 0.03586576506495476\n",
      "Test Epoch78 layer1 out_loss 0.00855829194188118\n",
      "Test Epoch78 layer2 out_loss 0.002458089729771018\n",
      "Train 79 | out_loss 0.0024351419415324926: 100%|█| 138/138 [00:00<00:00, 315.08i\n",
      "Train Epoch79 out_loss -34.29440689086914\n",
      "Test Epoch79 layer0 out_loss 0.03538947179913521\n",
      "Test Epoch79 layer1 out_loss 0.008396608754992485\n",
      "Test Epoch79 layer2 out_loss 0.0023481380194425583\n",
      "Train 80 | out_loss 0.002387130632996559: 100%|█| 138/138 [00:00<00:00, 311.56it\n",
      "Train Epoch80 out_loss -32.91639709472656\n",
      "Test Epoch80 layer0 out_loss 0.036208443343639374\n",
      "Test Epoch80 layer1 out_loss 0.008228284306824207\n",
      "Test Epoch80 layer2 out_loss 0.002293677069246769\n",
      "Train 81 | out_loss 0.0023413049057126045: 100%|█| 138/138 [00:00<00:00, 323.97i\n",
      "Train Epoch81 out_loss -31.62676239013672\n",
      "Test Epoch81 layer0 out_loss 0.033924054354429245\n",
      "Test Epoch81 layer1 out_loss 0.008125620894134045\n",
      "Test Epoch81 layer2 out_loss 0.002286021364852786\n",
      "Train 82 | out_loss 0.002308111870661378: 100%|█| 138/138 [00:00<00:00, 310.79it\n",
      "Train Epoch82 out_loss -30.70819091796875\n",
      "Test Epoch82 layer0 out_loss 0.03401290625333786\n",
      "Test Epoch82 layer1 out_loss 0.008259644731879234\n",
      "Test Epoch82 layer2 out_loss 0.0022324128076434135\n",
      "Train 83 | out_loss 0.0022637860383838415: 100%|█| 138/138 [00:00<00:00, 321.89i\n",
      "Train Epoch83 out_loss -29.502002716064453\n",
      "Test Epoch83 layer0 out_loss 0.03303857520222664\n",
      "Test Epoch83 layer1 out_loss 0.007969526574015617\n",
      "Test Epoch83 layer2 out_loss 0.0021639468614012003\n",
      "Train 84 | out_loss 0.002213607309386134: 100%|█| 138/138 [00:00<00:00, 320.44it\n",
      "Train Epoch84 out_loss -28.164810180664062\n",
      "Test Epoch84 layer0 out_loss 0.03395819291472435\n",
      "Test Epoch84 layer1 out_loss 0.007975267246365547\n",
      "Test Epoch84 layer2 out_loss 0.002138015115633607\n",
      "Train 85 | out_loss 0.00218232162296772: 100%|█| 138/138 [00:00<00:00, 306.29it/\n",
      "Train Epoch85 out_loss -27.346214294433594\n",
      "Test Epoch85 layer0 out_loss 0.032673630863428116\n",
      "Test Epoch85 layer1 out_loss 0.007778738159686327\n",
      "Test Epoch85 layer2 out_loss 0.002107493346557021\n",
      "Train 86 | out_loss 0.0021441462449729443: 100%|█| 138/138 [00:00<00:00, 321.76i\n",
      "Train Epoch86 out_loss -26.363197326660156\n",
      "Test Epoch86 layer0 out_loss 0.03190561756491661\n",
      "Test Epoch86 layer1 out_loss 0.007590278051793575\n",
      "Test Epoch86 layer2 out_loss 0.0020597197581082582\n",
      "Train 87 | out_loss 0.0021027063485234976: 100%|█| 138/138 [00:00<00:00, 310.17i\n",
      "Train Epoch87 out_loss -25.31572723388672\n",
      "Test Epoch87 layer0 out_loss 0.03161326423287392\n",
      "Test Epoch87 layer1 out_loss 0.0074952831491827965\n",
      "Test Epoch87 layer2 out_loss 0.002060853410512209\n",
      "Train 88 | out_loss 0.0020810256246477365: 100%|█| 138/138 [00:00<00:00, 320.33i\n",
      "Train Epoch88 out_loss -24.77581787109375\n",
      "Test Epoch88 layer0 out_loss 0.03227747976779938\n",
      "Test Epoch88 layer1 out_loss 0.0073782033286988735\n",
      "Test Epoch88 layer2 out_loss 0.002009400399401784\n",
      "Train 89 | out_loss 0.0020356294699013233: 100%|█| 138/138 [00:00<00:00, 316.37i\n",
      "Train Epoch89 out_loss -23.66350746154785\n",
      "Test Epoch89 layer0 out_loss 0.031451575458049774\n",
      "Test Epoch89 layer1 out_loss 0.007207086309790611\n",
      "Test Epoch89 layer2 out_loss 0.0019935020245611668\n",
      "Train 90 | out_loss 0.0020086634904146194: 100%|█| 138/138 [00:00<00:00, 312.32i\n",
      "Train Epoch90 out_loss -23.014421463012695\n",
      "Test Epoch90 layer0 out_loss 0.030047280713915825\n",
      "Test Epoch90 layer1 out_loss 0.007127969525754452\n",
      "Test Epoch90 layer2 out_loss 0.0019820949528366327\n",
      "Train 91 | out_loss 0.001972438069060445: 100%|█| 138/138 [00:00<00:00, 318.60it\n",
      "Train Epoch91 out_loss -22.156051635742188\n",
      "Test Epoch91 layer0 out_loss 0.030272195115685463\n",
      "Test Epoch91 layer1 out_loss 0.007020304910838604\n",
      "Test Epoch91 layer2 out_loss 0.0018983156187459826\n",
      "Train 92 | out_loss 0.0019477891037240624: 100%|█| 138/138 [00:00<00:00, 321.20i\n",
      "Train Epoch92 out_loss -21.580923080444336\n",
      "Test Epoch92 layer0 out_loss 0.029370246455073357\n",
      "Test Epoch92 layer1 out_loss 0.0069158319383859634\n",
      "Test Epoch92 layer2 out_loss 0.0019343397580087185\n",
      "Train 93 | out_loss 0.0019134152680635452: 100%|█| 138/138 [00:00<00:00, 317.06i\n",
      "Train Epoch93 out_loss -20.790945053100586\n",
      "Test Epoch93 layer0 out_loss 0.029205286875367165\n",
      "Test Epoch93 layer1 out_loss 0.006897411309182644\n",
      "Test Epoch93 layer2 out_loss 0.0018606395460665226\n",
      "Train 94 | out_loss 0.0018860745476558805: 100%|█| 138/138 [00:00<00:00, 316.75i\n",
      "Train Epoch94 out_loss -20.172657012939453\n",
      "Test Epoch94 layer0 out_loss 0.029964348301291466\n",
      "Test Epoch94 layer1 out_loss 0.006740929093211889\n",
      "Test Epoch94 layer2 out_loss 0.0018204294610768557\n",
      "Train 95 | out_loss 0.0018632134888321161: 100%|█| 138/138 [00:00<00:00, 313.05i\n",
      "Train Epoch95 out_loss -19.66250991821289\n",
      "Test Epoch95 layer0 out_loss 0.02877959795296192\n",
      "Test Epoch95 layer1 out_loss 0.0066668870858848095\n",
      "Test Epoch95 layer2 out_loss 0.001796236028894782\n",
      "Train 96 | out_loss 0.0018373732455074787: 100%|█| 138/138 [00:00<00:00, 315.55i\n",
      "Train Epoch96 out_loss -19.093358993530273\n",
      "Test Epoch96 layer0 out_loss 0.029700463637709618\n",
      "Test Epoch96 layer1 out_loss 0.006651187315583229\n",
      "Test Epoch96 layer2 out_loss 0.001791211194358766\n",
      "Train 97 | out_loss 0.0018159006722271442: 100%|█| 138/138 [00:00<00:00, 319.67i\n",
      "Train Epoch97 out_loss -18.6264591217041\n",
      "Test Epoch97 layer0 out_loss 0.028902659192681313\n",
      "Test Epoch97 layer1 out_loss 0.00651009613648057\n",
      "Test Epoch97 layer2 out_loss 0.0017615940887480974\n",
      "Train 98 | out_loss 0.001786986947990954: 100%|█| 138/138 [00:00<00:00, 310.08it\n",
      "Train Epoch98 out_loss -18.006427764892578\n",
      "Test Epoch98 layer0 out_loss 0.02940601296722889\n",
      "Test Epoch98 layer1 out_loss 0.0063911969773471355\n",
      "Test Epoch98 layer2 out_loss 0.001724702538922429\n",
      "Train 99 | out_loss 0.0017644655890762806: 100%|█| 138/138 [00:00<00:00, 314.21i\n",
      "Train Epoch99 out_loss -17.530353546142578\n",
      "Test Epoch99 layer0 out_loss 0.029140636324882507\n",
      "Test Epoch99 layer1 out_loss 0.006337091792374849\n",
      "Test Epoch99 layer2 out_loss 0.0017064138082787395\n",
      "Train 100 | out_loss 0.0017435088520869613: 100%|█| 138/138 [00:00<00:00, 318.39\n",
      "Train Epoch100 out_loss -17.09282875061035\n",
      "Test Epoch100 layer0 out_loss 0.029436226934194565\n",
      "Test Epoch100 layer1 out_loss 0.006303389091044664\n",
      "Test Epoch100 layer2 out_loss 0.0016825421480461955\n",
      "Train 101 | out_loss 0.0017145201563835144: 100%|█| 138/138 [00:00<00:00, 312.31\n",
      "Train Epoch101 out_loss -16.496158599853516\n",
      "Test Epoch101 layer0 out_loss 0.029499422758817673\n",
      "Test Epoch101 layer1 out_loss 0.00617034500464797\n",
      "Test Epoch101 layer2 out_loss 0.0016633077757433057\n",
      "Train 102 | out_loss 0.0017035624478012323: 100%|█| 138/138 [00:00<00:00, 318.91\n",
      "Train Epoch102 out_loss -16.27324104309082\n",
      "Test Epoch102 layer0 out_loss 0.029170673340559006\n",
      "Test Epoch102 layer1 out_loss 0.006168697029352188\n",
      "Test Epoch102 layer2 out_loss 0.0016709021292626858\n",
      "Train 103 | out_loss 0.0016894475556910038: 100%|█| 138/138 [00:00<00:00, 307.75\n",
      "Train Epoch103 out_loss -15.988191604614258\n",
      "Test Epoch103 layer0 out_loss 0.029476581141352654\n",
      "Test Epoch103 layer1 out_loss 0.006063045002520084\n",
      "Test Epoch103 layer2 out_loss 0.0016672865021973848\n",
      "Train 104 | out_loss 0.0016627945005893707: 100%|█| 138/138 [00:00<00:00, 316.29\n",
      "Train Epoch104 out_loss -15.456415176391602\n",
      "Test Epoch104 layer0 out_loss 0.02947339415550232\n",
      "Test Epoch104 layer1 out_loss 0.0059442888014018536\n",
      "Test Epoch104 layer2 out_loss 0.001616580761037767\n",
      "Train 105 | out_loss 0.0016456512967124581: 100%|█| 138/138 [00:00<00:00, 318.71\n",
      "Train Epoch105 out_loss -15.118831634521484\n",
      "Test Epoch105 layer0 out_loss 0.028923122212290764\n",
      "Test Epoch105 layer1 out_loss 0.005934298504143953\n",
      "Test Epoch105 layer2 out_loss 0.0016165158012881875\n",
      "Train 106 | out_loss 0.0016327239573001862: 100%|█| 138/138 [00:00<00:00, 323.35\n",
      "Train Epoch106 out_loss -14.866572380065918\n",
      "Test Epoch106 layer0 out_loss 0.02909870818257332\n",
      "Test Epoch106 layer1 out_loss 0.005807566922158003\n",
      "Test Epoch106 layer2 out_loss 0.0015922596212476492\n",
      "Train 107 | out_loss 0.0016081553185358644: 100%|█| 138/138 [00:00<00:00, 319.84\n",
      "Train Epoch107 out_loss -14.392664909362793\n",
      "Test Epoch107 layer0 out_loss 0.029208602383732796\n",
      "Test Epoch107 layer1 out_loss 0.0057539562694728374\n",
      "Test Epoch107 layer2 out_loss 0.0016121375374495983\n",
      "Train 108 | out_loss 0.0015901461010798812: 100%|█| 138/138 [00:00<00:00, 312.03\n",
      "Train Epoch108 out_loss -14.049833297729492\n",
      "Test Epoch108 layer0 out_loss 0.028920698910951614\n",
      "Test Epoch108 layer1 out_loss 0.005807757843285799\n",
      "Test Epoch108 layer2 out_loss 0.0015460319118574262\n",
      "Train 109 | out_loss 0.0015868894988670945: 100%|█| 138/138 [00:00<00:00, 319.70\n",
      "Train Epoch109 out_loss -13.988260269165039\n",
      "Test Epoch109 layer0 out_loss 0.02912355400621891\n",
      "Test Epoch109 layer1 out_loss 0.005579737015068531\n",
      "Test Epoch109 layer2 out_loss 0.0015248720301315188\n",
      "Train 110 | out_loss 0.0015521810855716467: 100%|█| 138/138 [00:00<00:00, 315.38\n",
      "Train Epoch110 out_loss -13.339774131774902\n",
      "Test Epoch110 layer0 out_loss 0.02878763899207115\n",
      "Test Epoch110 layer1 out_loss 0.005522791296243668\n",
      "Test Epoch110 layer2 out_loss 0.0015220558270812035\n",
      "Train 111 | out_loss 0.0015412521315738559: 100%|█| 138/138 [00:00<00:00, 321.29\n",
      "Train Epoch111 out_loss -13.138567924499512\n",
      "Test Epoch111 layer0 out_loss 0.028575284406542778\n",
      "Test Epoch111 layer1 out_loss 0.005527079105377197\n",
      "Test Epoch111 layer2 out_loss 0.0015374163631349802\n",
      "Train 112 | out_loss 0.0015260449144989252: 100%|█| 138/138 [00:00<00:00, 316.26\n",
      "Train Epoch112 out_loss -12.860922813415527\n",
      "Test Epoch112 layer0 out_loss 0.028167827054858208\n",
      "Test Epoch112 layer1 out_loss 0.005392900668084621\n",
      "Test Epoch112 layer2 out_loss 0.001478746416978538\n",
      "Train 113 | out_loss 0.0015088572399690747: 100%|█| 138/138 [00:00<00:00, 314.85\n",
      "Train Epoch113 out_loss -12.550447463989258\n",
      "Test Epoch113 layer0 out_loss 0.02830800600349903\n",
      "Test Epoch113 layer1 out_loss 0.005375853274017572\n",
      "Test Epoch113 layer2 out_loss 0.0014802615623921156\n",
      "Train 114 | out_loss 0.001491735689342022: 100%|█| 138/138 [00:00<00:00, 315.57i\n",
      "Train Epoch114 out_loss -12.244671821594238\n",
      "Test Epoch114 layer0 out_loss 0.028302298858761787\n",
      "Test Epoch114 layer1 out_loss 0.005289486143738031\n",
      "Test Epoch114 layer2 out_loss 0.0014472398906946182\n",
      "Train 115 | out_loss 0.001492852228693664: 100%|█| 138/138 [00:00<00:00, 314.62i\n",
      "Train Epoch115 out_loss -12.264527320861816\n",
      "Test Epoch115 layer0 out_loss 0.027490373700857162\n",
      "Test Epoch115 layer1 out_loss 0.005235145799815655\n",
      "Test Epoch115 layer2 out_loss 0.0014911270700395107\n",
      "Train 116 | out_loss 0.0014665706548839808: 100%|█| 138/138 [00:00<00:00, 276.61\n",
      "Train Epoch116 out_loss -11.801587104797363\n",
      "Test Epoch116 layer0 out_loss 0.02682132087647915\n",
      "Test Epoch116 layer1 out_loss 0.005167356226593256\n",
      "Test Epoch116 layer2 out_loss 0.0014462873805314302\n",
      "Train 117 | out_loss 0.0014539393596351147: 100%|█| 138/138 [00:00<00:00, 310.46\n",
      "Train Epoch117 out_loss -11.582024574279785\n",
      "Test Epoch117 layer0 out_loss 0.02707108110189438\n",
      "Test Epoch117 layer1 out_loss 0.005288262851536274\n",
      "Test Epoch117 layer2 out_loss 0.0014293404528871179\n",
      "Train 118 | out_loss 0.0014289612881839275: 100%|█| 138/138 [00:00<00:00, 318.24\n",
      "Train Epoch118 out_loss -11.153426170349121\n",
      "Test Epoch118 layer0 out_loss 0.02661610208451748\n",
      "Test Epoch118 layer1 out_loss 0.005132587626576424\n",
      "Test Epoch118 layer2 out_loss 0.0014408123679459095\n",
      "Train 119 | out_loss 0.0014199248980730772: 100%|█| 138/138 [00:00<00:00, 319.91\n",
      "Train Epoch119 out_loss -11.000205039978027\n",
      "Test Epoch119 layer0 out_loss 0.02695058286190033\n",
      "Test Epoch119 layer1 out_loss 0.005068555008620024\n",
      "Test Epoch119 layer2 out_loss 0.0013691162457689643\n",
      "Train 120 | out_loss 0.0014036362990736961: 100%|█| 138/138 [00:00<00:00, 314.18\n",
      "Train Epoch120 out_loss -10.72645378112793\n",
      "Test Epoch120 layer0 out_loss 0.02611524611711502\n",
      "Test Epoch120 layer1 out_loss 0.004958372097462416\n",
      "Test Epoch120 layer2 out_loss 0.0013524085516110063\n",
      "Train 121 | out_loss 0.001389852026477456: 100%|█| 138/138 [00:00<00:00, 314.08i\n",
      "Train Epoch121 out_loss -10.497284889221191\n",
      "Test Epoch121 layer0 out_loss 0.02606501244008541\n",
      "Test Epoch121 layer1 out_loss 0.004960573744028807\n",
      "Test Epoch121 layer2 out_loss 0.0013530770083889365\n",
      "Train 122 | out_loss 0.0013832043623551726: 100%|█| 138/138 [00:00<00:00, 314.56\n",
      "Train Epoch122 out_loss -10.387547492980957\n",
      "Test Epoch122 layer0 out_loss 0.025893427431583405\n",
      "Test Epoch122 layer1 out_loss 0.004905981011688709\n",
      "Test Epoch122 layer2 out_loss 0.0013525402173399925\n",
      "Train 123 | out_loss 0.0013697908725589514: 100%|█| 138/138 [00:00<00:00, 309.96\n",
      "Train Epoch123 out_loss -10.167759895324707\n",
      "Test Epoch123 layer0 out_loss 0.02562023140490055\n",
      "Test Epoch123 layer1 out_loss 0.004808744881302118\n",
      "Test Epoch123 layer2 out_loss 0.001335386885330081\n",
      "Train 124 | out_loss 0.0013424884527921677: 100%|█| 138/138 [00:00<00:00, 313.86\n",
      "Train Epoch124 out_loss -9.727012634277344\n",
      "Test Epoch124 layer0 out_loss 0.02558884769678116\n",
      "Test Epoch124 layer1 out_loss 0.004818418528884649\n",
      "Test Epoch124 layer2 out_loss 0.0013102019438520074\n",
      "Train 125 | out_loss 0.0013325376203283668: 100%|█| 138/138 [00:00<00:00, 320.05\n",
      "Train Epoch125 out_loss -9.568575859069824\n",
      "Test Epoch125 layer0 out_loss 0.02557394467294216\n",
      "Test Epoch125 layer1 out_loss 0.004763542674481869\n",
      "Test Epoch125 layer2 out_loss 0.0012872460065409541\n",
      "Train 126 | out_loss 0.001328030601143837: 100%|█| 138/138 [00:00<00:00, 317.50i\n",
      "Train Epoch126 out_loss -9.497212409973145\n",
      "Test Epoch126 layer0 out_loss 0.024835655465722084\n",
      "Test Epoch126 layer1 out_loss 0.004702795762568712\n",
      "Test Epoch126 layer2 out_loss 0.0013029954861849546\n",
      "Train 127 | out_loss 0.001310517080128193: 100%|█| 138/138 [00:00<00:00, 315.60i\n",
      "Train Epoch127 out_loss -9.222159385681152\n",
      "Test Epoch127 layer0 out_loss 0.024941513314843178\n",
      "Test Epoch127 layer1 out_loss 0.004816772881895304\n",
      "Test Epoch127 layer2 out_loss 0.0012631189310923219\n",
      "Train 128 | out_loss 0.001298530725762248: 100%|█| 138/138 [00:00<00:00, 315.36i\n",
      "Train Epoch128 out_loss -9.036028861999512\n",
      "Test Epoch128 layer0 out_loss 0.02405092678964138\n",
      "Test Epoch128 layer1 out_loss 0.004624206107109785\n",
      "Test Epoch128 layer2 out_loss 0.001250716159120202\n",
      "Train 129 | out_loss 0.0012841862626373768: 100%|█| 138/138 [00:00<00:00, 318.88\n",
      "Train Epoch129 out_loss -8.815526962280273\n",
      "Test Epoch129 layer0 out_loss 0.02379034273326397\n",
      "Test Epoch129 layer1 out_loss 0.004577229730784893\n",
      "Test Epoch129 layer2 out_loss 0.001238257740624249\n",
      "Train 130 | out_loss 0.0012796181254088879: 100%|█| 138/138 [00:00<00:00, 319.92\n",
      "Train Epoch130 out_loss -8.745821952819824\n",
      "Test Epoch130 layer0 out_loss 0.02339121326804161\n",
      "Test Epoch130 layer1 out_loss 0.0045127226039767265\n",
      "Test Epoch130 layer2 out_loss 0.0012257994385436177\n",
      "Train 131 | out_loss 0.0012660648208111525: 100%|█| 138/138 [00:00<00:00, 312.03\n",
      "Train Epoch131 out_loss -8.540462493896484\n",
      "Test Epoch131 layer0 out_loss 0.023459242656826973\n",
      "Test Epoch131 layer1 out_loss 0.004537051077932119\n",
      "Test Epoch131 layer2 out_loss 0.0012409129412844777\n",
      "Train 132 | out_loss 0.0012574023567140102: 100%|█| 138/138 [00:00<00:00, 313.11\n",
      "Train Epoch132 out_loss -8.410364151000977\n",
      "Test Epoch132 layer0 out_loss 0.023202791810035706\n",
      "Test Epoch132 layer1 out_loss 0.004456165246665478\n",
      "Test Epoch132 layer2 out_loss 0.0012320250971242785\n",
      "Train 133 | out_loss 0.0012392103672027588: 100%|█| 138/138 [00:00<00:00, 319.96\n",
      "Train Epoch133 out_loss -8.140040397644043\n",
      "Test Epoch133 layer0 out_loss 0.02341843955218792\n",
      "Test Epoch133 layer1 out_loss 0.00447760708630085\n",
      "Test Epoch133 layer2 out_loss 0.0011931095505133271\n",
      "Train 134 | out_loss 0.0012308483710512519: 100%|█| 138/138 [00:00<00:00, 318.11\n",
      "Train Epoch134 out_loss -8.017098426818848\n",
      "Test Epoch134 layer0 out_loss 0.022735049948096275\n",
      "Test Epoch134 layer1 out_loss 0.004390707705169916\n",
      "Test Epoch134 layer2 out_loss 0.001187785528600216\n",
      "Train 135 | out_loss 0.0012093684636056423: 100%|█| 138/138 [00:00<00:00, 319.92\n",
      "Train Epoch135 out_loss -7.705120086669922\n",
      "Test Epoch135 layer0 out_loss 0.022743351757526398\n",
      "Test Epoch135 layer1 out_loss 0.004510956350713968\n",
      "Test Epoch135 layer2 out_loss 0.001260280259884894\n",
      "Train 136 | out_loss 0.0012052702950313687: 100%|█| 138/138 [00:00<00:00, 321.88\n",
      "Train Epoch136 out_loss -7.646221160888672\n",
      "Test Epoch136 layer0 out_loss 0.022354213520884514\n",
      "Test Epoch136 layer1 out_loss 0.00430954759940505\n",
      "Test Epoch136 layer2 out_loss 0.0011950660264119506\n",
      "Train 137 | out_loss 0.001195707474835217: 100%|█| 138/138 [00:00<00:00, 318.22i\n",
      "Train Epoch137 out_loss -7.509574890136719\n",
      "Test Epoch137 layer0 out_loss 0.022190051153302193\n",
      "Test Epoch137 layer1 out_loss 0.004294413607567549\n",
      "Test Epoch137 layer2 out_loss 0.0012976281577721238\n",
      "Train 138 | out_loss 0.0011910399189218879: 100%|█| 138/138 [00:00<00:00, 318.89\n",
      "Train Epoch138 out_loss -7.443267822265625\n",
      "Test Epoch138 layer0 out_loss 0.02249814197421074\n",
      "Test Epoch138 layer1 out_loss 0.004248248413205147\n",
      "Test Epoch138 layer2 out_loss 0.001159749343059957\n",
      "Train 139 | out_loss 0.0011758193140849471: 100%|█| 138/138 [00:00<00:00, 319.05\n",
      "Train Epoch139 out_loss -7.228843688964844\n",
      "Test Epoch139 layer0 out_loss 0.02158273756504059\n",
      "Test Epoch139 layer1 out_loss 0.004215007647871971\n",
      "Test Epoch139 layer2 out_loss 0.0011309743858873844\n",
      "Train 140 | out_loss 0.0011711807455867529: 100%|█| 138/138 [00:00<00:00, 318.10\n",
      "Train Epoch140 out_loss -7.164046287536621\n",
      "Test Epoch140 layer0 out_loss 0.02192682959139347\n",
      "Test Epoch140 layer1 out_loss 0.004207364749163389\n",
      "Test Epoch140 layer2 out_loss 0.0011810739524662495\n",
      "Train 141 | out_loss 0.0011471464531496167: 100%|█| 138/138 [00:00<00:00, 318.14\n",
      "Train Epoch141 out_loss -6.832413673400879\n",
      "Test Epoch141 layer0 out_loss 0.021768057718873024\n",
      "Test Epoch141 layer1 out_loss 0.004165809601545334\n",
      "Test Epoch141 layer2 out_loss 0.001133136684074998\n",
      "Train 142 | out_loss 0.0011406175326555967: 100%|█| 138/138 [00:00<00:00, 320.54\n",
      "Train Epoch142 out_loss -6.743504047393799\n",
      "Test Epoch142 layer0 out_loss 0.0218492578715086\n",
      "Test Epoch142 layer1 out_loss 0.004181466996669769\n",
      "Test Epoch142 layer2 out_loss 0.0011075243819504976\n",
      "Train 143 | out_loss 0.001133927027694881: 100%|█| 138/138 [00:00<00:00, 322.14i\n",
      "Train Epoch143 out_loss -6.652929782867432\n",
      "Test Epoch143 layer0 out_loss 0.021349357441067696\n",
      "Test Epoch143 layer1 out_loss 0.004172697197645903\n",
      "Test Epoch143 layer2 out_loss 0.0010913455625995994\n",
      "Train 144 | out_loss 0.001125073991715908: 100%|█| 138/138 [00:00<00:00, 320.90i\n",
      "Train Epoch144 out_loss -6.533900260925293\n",
      "Test Epoch144 layer0 out_loss 0.0213782899081707\n",
      "Test Epoch144 layer1 out_loss 0.004103022161871195\n",
      "Test Epoch144 layer2 out_loss 0.0011098758550360799\n",
      "Train 145 | out_loss 0.0011145442258566618: 100%|█| 138/138 [00:00<00:00, 324.69\n",
      "Train Epoch145 out_loss -6.393545150756836\n",
      "Test Epoch145 layer0 out_loss 0.021208271384239197\n",
      "Test Epoch145 layer1 out_loss 0.004152501467615366\n",
      "Test Epoch145 layer2 out_loss 0.0011290814727544785\n",
      "Train 146 | out_loss 0.0011024324921891093: 100%|█| 138/138 [00:00<00:00, 321.14\n",
      "Train Epoch146 out_loss -6.233722686767578\n",
      "Test Epoch146 layer0 out_loss 0.02110428363084793\n",
      "Test Epoch146 layer1 out_loss 0.003990634810179472\n",
      "Test Epoch146 layer2 out_loss 0.0010705175809562206\n",
      "Train 147 | out_loss 0.0010963848326355219: 100%|█| 138/138 [00:00<00:00, 318.92\n",
      "Train Epoch147 out_loss -6.154572486877441\n",
      "Test Epoch147 layer0 out_loss 0.020815791562199593\n",
      "Test Epoch147 layer1 out_loss 0.004034395329654217\n",
      "Test Epoch147 layer2 out_loss 0.0010630759643390775\n",
      "Train 148 | out_loss 0.001084033166989684: 100%|█| 138/138 [00:00<00:00, 317.26i\n",
      "Train Epoch148 out_loss -5.994282245635986\n",
      "Test Epoch148 layer0 out_loss 0.020513994619250298\n",
      "Test Epoch148 layer1 out_loss 0.003941103350371122\n",
      "Test Epoch148 layer2 out_loss 0.0010415799915790558\n",
      "Train 149 | out_loss 0.0010774416150525212: 100%|█| 138/138 [00:00<00:00, 322.98\n",
      "Train Epoch149 out_loss -5.909477710723877\n",
      "Test Epoch149 layer0 out_loss 0.020316073670983315\n",
      "Test Epoch149 layer1 out_loss 0.003919375594705343\n",
      "Test Epoch149 layer2 out_loss 0.001032192842103541\n",
      "Train 150 | out_loss 0.001066481345333159: 100%|█| 138/138 [00:00<00:00, 320.97i\n",
      "Train Epoch150 out_loss -5.769627094268799\n",
      "Test Epoch150 layer0 out_loss 0.019888674840331078\n",
      "Test Epoch150 layer1 out_loss 0.003958356566727161\n",
      "Test Epoch150 layer2 out_loss 0.001076837070286274\n",
      "Train 151 | out_loss 0.0010637654922902584: 100%|█| 138/138 [00:00<00:00, 323.90\n",
      "Train Epoch151 out_loss -5.735184669494629\n",
      "Test Epoch151 layer0 out_loss 0.019749436527490616\n",
      "Test Epoch151 layer1 out_loss 0.003869406646117568\n",
      "Test Epoch151 layer2 out_loss 0.0010216057999059558\n",
      "Train 152 | out_loss 0.0010477503528818488: 100%|█| 138/138 [00:00<00:00, 317.74\n",
      "Train Epoch152 out_loss -5.533915996551514\n",
      "Test Epoch152 layer0 out_loss 0.019568508490920067\n",
      "Test Epoch152 layer1 out_loss 0.003829249180853367\n",
      "Test Epoch152 layer2 out_loss 0.0010491968132555485\n",
      "Train 153 | out_loss 0.001055806060321629: 100%|█| 138/138 [00:00<00:00, 301.38i\n",
      "Train Epoch153 out_loss -5.634768009185791\n",
      "Test Epoch153 layer0 out_loss 0.019443070515990257\n",
      "Test Epoch153 layer1 out_loss 0.0040712361223995686\n",
      "Test Epoch153 layer2 out_loss 0.0010017777094617486\n",
      "Train 154 | out_loss 0.001041177660226822: 100%|█| 138/138 [00:00<00:00, 314.12i\n",
      "Train Epoch154 out_loss -5.4521965980529785\n",
      "Test Epoch154 layer0 out_loss 0.01960715278983116\n",
      "Test Epoch154 layer1 out_loss 0.0037959616165608168\n",
      "Test Epoch154 layer2 out_loss 0.0010034618899226189\n",
      "Train 155 | out_loss 0.0010268855839967728: 100%|█| 138/138 [00:00<00:00, 313.17\n",
      "Train Epoch155 out_loss -5.276270866394043\n",
      "Test Epoch155 layer0 out_loss 0.01914776675403118\n",
      "Test Epoch155 layer1 out_loss 0.0038333835545927286\n",
      "Test Epoch155 layer2 out_loss 0.0009952179389074445\n",
      "Train 156 | out_loss 0.001019287039525807: 100%|█| 138/138 [00:00<00:00, 314.71i\n",
      "Train Epoch156 out_loss -5.183732509613037\n",
      "Test Epoch156 layer0 out_loss 0.01908094249665737\n",
      "Test Epoch156 layer1 out_loss 0.003718686755746603\n",
      "Test Epoch156 layer2 out_loss 0.0009926120983436704\n",
      "Train 157 | out_loss 0.0010088197886943817: 100%|█| 138/138 [00:00<00:00, 309.47\n",
      "Train Epoch157 out_loss -5.057385444641113\n",
      "Test Epoch157 layer0 out_loss 0.018871081992983818\n",
      "Test Epoch157 layer1 out_loss 0.003699082648381591\n",
      "Test Epoch157 layer2 out_loss 0.0009996577864512801\n",
      "Train 158 | out_loss 0.0010053484002128243: 100%|█| 138/138 [00:00<00:00, 302.66\n",
      "Train Epoch158 out_loss -5.015768527984619\n",
      "Test Epoch158 layer0 out_loss 0.018471593037247658\n",
      "Test Epoch158 layer1 out_loss 0.0036597182042896748\n",
      "Test Epoch158 layer2 out_loss 0.0009841998107731342\n",
      "Train 159 | out_loss 0.0009900734294205904: 100%|█| 138/138 [00:00<00:00, 316.22\n",
      "Train Epoch159 out_loss -4.834348201751709\n",
      "Test Epoch159 layer0 out_loss 0.018403975293040276\n",
      "Test Epoch159 layer1 out_loss 0.0036407511215656996\n",
      "Test Epoch159 layer2 out_loss 0.0009550133254379034\n",
      "Train 160 | out_loss 0.0009806083980947733: 100%|█| 138/138 [00:00<00:00, 316.89\n",
      "Train Epoch160 out_loss -4.723332405090332\n",
      "Test Epoch160 layer0 out_loss 0.018347177654504776\n",
      "Test Epoch160 layer1 out_loss 0.0036787663120776415\n",
      "Test Epoch160 layer2 out_loss 0.000974987808149308\n",
      "Train 161 | out_loss 0.0009743833215907216: 100%|█| 138/138 [00:00<00:00, 319.62\n",
      "Train Epoch161 out_loss -4.6508989334106445\n",
      "Test Epoch161 layer0 out_loss 0.01790432631969452\n",
      "Test Epoch161 layer1 out_loss 0.0035936820786446333\n",
      "Test Epoch161 layer2 out_loss 0.0009464576723985374\n",
      "Train 162 | out_loss 0.0009738903609104455: 100%|█| 138/138 [00:00<00:00, 318.83\n",
      "Train Epoch162 out_loss -4.6451802253723145\n",
      "Test Epoch162 layer0 out_loss 0.017601411789655685\n",
      "Test Epoch162 layer1 out_loss 0.0035983517300337553\n",
      "Test Epoch162 layer2 out_loss 0.0009329579188488424\n",
      "Train 163 | out_loss 0.0009639085037633777: 100%|█| 138/138 [00:00<00:00, 307.53\n",
      "Train Epoch163 out_loss -4.530049800872803\n",
      "Test Epoch163 layer0 out_loss 0.017452403903007507\n",
      "Test Epoch163 layer1 out_loss 0.0036155139096081257\n",
      "Test Epoch163 layer2 out_loss 0.000935345480684191\n",
      "Train 164 | out_loss 0.0009507050854153931: 100%|█| 138/138 [00:00<00:00, 313.11\n",
      "Train Epoch164 out_loss -4.379593372344971\n",
      "Test Epoch164 layer0 out_loss 0.01719515025615692\n",
      "Test Epoch164 layer1 out_loss 0.0035273577086627483\n",
      "Test Epoch164 layer2 out_loss 0.0009473296231590211\n",
      "Train 165 | out_loss 0.0009469524375163019: 100%|█| 138/138 [00:00<00:00, 305.37\n",
      "Train Epoch165 out_loss -4.337204456329346\n",
      "Test Epoch165 layer0 out_loss 0.017489925026893616\n",
      "Test Epoch165 layer1 out_loss 0.0034965630620718002\n",
      "Test Epoch165 layer2 out_loss 0.0009252508752979338\n",
      "Train 166 | out_loss 0.0009390015038661659: 100%|█| 138/138 [00:00<00:00, 314.18\n",
      "Train Epoch166 out_loss -4.24796199798584\n",
      "Test Epoch166 layer0 out_loss 0.01678270474076271\n",
      "Test Epoch166 layer1 out_loss 0.0035122002009302378\n",
      "Test Epoch166 layer2 out_loss 0.0009157413151115179\n",
      "Train 167 | out_loss 0.0009300130768679082: 100%|█| 138/138 [00:00<00:00, 318.59\n",
      "Train Epoch167 out_loss -4.147968769073486\n",
      "Test Epoch167 layer0 out_loss 0.016567938029766083\n",
      "Test Epoch167 layer1 out_loss 0.0034466804936528206\n",
      "Test Epoch167 layer2 out_loss 0.0009439511341042817\n",
      "Train 168 | out_loss 0.000923674029763788: 100%|█| 138/138 [00:00<00:00, 316.45i\n",
      "Train Epoch168 out_loss -4.078023910522461\n",
      "Test Epoch168 layer0 out_loss 0.01653047278523445\n",
      "Test Epoch168 layer1 out_loss 0.0035030199214816093\n",
      "Test Epoch168 layer2 out_loss 0.0008956891833804548\n",
      "Train 169 | out_loss 0.0009149290854111314: 100%|█| 138/138 [00:00<00:00, 320.85\n",
      "Train Epoch169 out_loss -3.9823312759399414\n",
      "Test Epoch169 layer0 out_loss 0.01642196625471115\n",
      "Test Epoch169 layer1 out_loss 0.003395560896024108\n",
      "Test Epoch169 layer2 out_loss 0.0008857808425091207\n",
      "Train 170 | out_loss 0.000913062016479671: 100%|█| 138/138 [00:00<00:00, 319.52i\n",
      "Train Epoch170 out_loss -3.9620118141174316\n",
      "Test Epoch170 layer0 out_loss 0.01607345975935459\n",
      "Test Epoch170 layer1 out_loss 0.0035016608890146017\n",
      "Test Epoch170 layer2 out_loss 0.0009079298470169306\n",
      "Train 171 | out_loss 0.0009024034952744842: 100%|█| 138/138 [00:00<00:00, 315.01\n",
      "Train Epoch171 out_loss -3.8468456268310547\n",
      "Test Epoch171 layer0 out_loss 0.01594981551170349\n",
      "Test Epoch171 layer1 out_loss 0.0033479873090982437\n",
      "Test Epoch171 layer2 out_loss 0.00086104596266523\n",
      "Train 172 | out_loss 0.0008966311579570174: 100%|█| 138/138 [00:00<00:00, 314.32\n",
      "Train Epoch172 out_loss -3.7850356101989746\n",
      "Test Epoch172 layer0 out_loss 0.015907425433397293\n",
      "Test Epoch172 layer1 out_loss 0.003360832342877984\n",
      "Test Epoch172 layer2 out_loss 0.0009257554193027318\n",
      "Train 173 | out_loss 0.0008876647334545851: 100%|█| 138/138 [00:00<00:00, 316.28\n",
      "Train Epoch173 out_loss -3.6898159980773926\n",
      "Test Epoch173 layer0 out_loss 0.015545559115707874\n",
      "Test Epoch173 layer1 out_loss 0.0033103807363659143\n",
      "Test Epoch173 layer2 out_loss 0.0008435560157522559\n",
      "Train 174 | out_loss 0.0008875967469066381: 100%|█| 138/138 [00:00<00:00, 315.50\n",
      "Train Epoch174 out_loss -3.689096450805664\n",
      "Test Epoch174 layer0 out_loss 0.015361347235739231\n",
      "Test Epoch174 layer1 out_loss 0.0032995345536619425\n",
      "Test Epoch174 layer2 out_loss 0.0008573358063586056\n",
      "Train 175 | out_loss 0.0008798815542832017: 100%|█| 138/138 [00:00<00:00, 316.51\n",
      "Train Epoch175 out_loss -3.607933521270752\n",
      "Test Epoch175 layer0 out_loss 0.015172774903476238\n",
      "Test Epoch175 layer1 out_loss 0.003271836321800947\n",
      "Test Epoch175 layer2 out_loss 0.0008768396219238639\n",
      "Train 176 | out_loss 0.0008706485969014466: 100%|█| 138/138 [00:00<00:00, 313.63\n",
      "Train Epoch176 out_loss -3.511735439300537\n",
      "Test Epoch176 layer0 out_loss 0.014970931224524975\n",
      "Test Epoch176 layer1 out_loss 0.003275561612099409\n",
      "Test Epoch176 layer2 out_loss 0.0008480158285237849\n",
      "Train 177 | out_loss 0.0008589619537815452: 100%|█| 138/138 [00:00<00:00, 317.25\n",
      "Train Epoch177 out_loss -3.3914241790771484\n",
      "Test Epoch177 layer0 out_loss 0.014902726747095585\n",
      "Test Epoch177 layer1 out_loss 0.0032954555936157703\n",
      "Test Epoch177 layer2 out_loss 0.0008323639049194753\n",
      "Train 178 | out_loss 0.0008566654287278652: 100%|█| 138/138 [00:00<00:00, 317.77\n",
      "Train Epoch178 out_loss -3.367980480194092\n",
      "Test Epoch178 layer0 out_loss 0.01505272462964058\n",
      "Test Epoch178 layer1 out_loss 0.003302148310467601\n",
      "Test Epoch178 layer2 out_loss 0.0008326007518917322\n",
      "Train 179 | out_loss 0.0008497560047544539: 100%|█| 138/138 [00:00<00:00, 301.47\n",
      "Train Epoch179 out_loss -3.2978034019470215\n",
      "Test Epoch179 layer0 out_loss 0.014688238501548767\n",
      "Test Epoch179 layer1 out_loss 0.003189271781593561\n",
      "Test Epoch179 layer2 out_loss 0.0008930783369578421\n",
      "Train 180 | out_loss 0.0008426454733125865: 100%|█| 138/138 [00:00<00:00, 313.45\n",
      "Train Epoch180 out_loss -3.2261757850646973\n",
      "Test Epoch180 layer0 out_loss 0.01432304922491312\n",
      "Test Epoch180 layer1 out_loss 0.003163360757753253\n",
      "Test Epoch180 layer2 out_loss 0.0008868264849297702\n",
      "Train 181 | out_loss 0.0008329296833835542: 100%|█| 138/138 [00:00<00:00, 312.72\n",
      "Train Epoch181 out_loss -3.129281997680664\n",
      "Test Epoch181 layer0 out_loss 0.014112270437180996\n",
      "Test Epoch181 layer1 out_loss 0.003110775025561452\n",
      "Test Epoch181 layer2 out_loss 0.0007957206689752638\n",
      "Train 182 | out_loss 0.0008329970296472311: 100%|█| 138/138 [00:00<00:00, 315.08\n",
      "Train Epoch182 out_loss -3.129948139190674\n",
      "Test Epoch182 layer0 out_loss 0.013888676650822163\n",
      "Test Epoch182 layer1 out_loss 0.00309220002964139\n",
      "Test Epoch182 layer2 out_loss 0.0007852910202927887\n",
      "Train 183 | out_loss 0.0008258700836449862: 100%|█| 138/138 [00:00<00:00, 318.74\n",
      "Train Epoch183 out_loss -3.0595803260803223\n",
      "Test Epoch183 layer0 out_loss 0.01372764352709055\n",
      "Test Epoch183 layer1 out_loss 0.0030812090262770653\n",
      "Test Epoch183 layer2 out_loss 0.0007901799981482327\n",
      "Train 184 | out_loss 0.0008111590286716819: 100%|█| 138/138 [00:00<00:00, 311.55\n",
      "Train Epoch184 out_loss -2.9162418842315674\n",
      "Test Epoch184 layer0 out_loss 0.013654178939759731\n",
      "Test Epoch184 layer1 out_loss 0.0030573077965527773\n",
      "Test Epoch184 layer2 out_loss 0.0008165154722519219\n",
      "Train 185 | out_loss 0.0008056969963945448: 100%|█| 138/138 [00:00<00:00, 309.90\n",
      "Train Epoch185 out_loss -2.8636772632598877\n",
      "Test Epoch185 layer0 out_loss 0.013496657833456993\n",
      "Test Epoch185 layer1 out_loss 0.0031061943154782057\n",
      "Test Epoch185 layer2 out_loss 0.0008602967136539519\n",
      "Train 186 | out_loss 0.0008030906901694834: 100%|█| 138/138 [00:00<00:00, 312.92\n",
      "Train Epoch186 out_loss -2.8387250900268555\n",
      "Test Epoch186 layer0 out_loss 0.01325999479740858\n",
      "Test Epoch186 layer1 out_loss 0.00307457079179585\n",
      "Test Epoch186 layer2 out_loss 0.0008328569820150733\n",
      "Train 187 | out_loss 0.0007949511054903269: 100%|█| 138/138 [00:00<00:00, 318.33\n",
      "Train Epoch187 out_loss -2.7613015174865723\n",
      "Test Epoch187 layer0 out_loss 0.013098981231451035\n",
      "Test Epoch187 layer1 out_loss 0.003061346709728241\n",
      "Test Epoch187 layer2 out_loss 0.000782809394877404\n",
      "Train 188 | out_loss 0.0007928663399070501: 100%|█| 138/138 [00:00<00:00, 316.39\n",
      "Train Epoch188 out_loss -2.7416040897369385\n",
      "Test Epoch188 layer0 out_loss 0.012959721498191357\n",
      "Test Epoch188 layer1 out_loss 0.0029920628294348717\n",
      "Test Epoch188 layer2 out_loss 0.0008242460899055004\n",
      "Train 189 | out_loss 0.0007946766563691199: 100%|█| 138/138 [00:00<00:00, 308.19\n",
      "Train Epoch189 out_loss -2.7587082386016846\n",
      "Test Epoch189 layer0 out_loss 0.012754571624100208\n",
      "Test Epoch189 layer1 out_loss 0.0029717390425503254\n",
      "Test Epoch189 layer2 out_loss 0.0007861963240429759\n",
      "Train 190 | out_loss 0.000774899497628212: 100%|█| 138/138 [00:00<00:00, 315.38i\n",
      "Train Epoch190 out_loss -2.5739517211914062\n",
      "Test Epoch190 layer0 out_loss 0.012516764923930168\n",
      "Test Epoch190 layer1 out_loss 0.0029426743276417255\n",
      "Test Epoch190 layer2 out_loss 0.000771857041399926\n",
      "Train 191 | out_loss 0.0007715658284723759: 100%|█| 138/138 [00:00<00:00, 313.15\n",
      "Train Epoch191 out_loss -2.5432653427124023\n",
      "Test Epoch191 layer0 out_loss 0.012411759234964848\n",
      "Test Epoch191 layer1 out_loss 0.0030305946711450815\n",
      "Test Epoch191 layer2 out_loss 0.0007381688337773085\n",
      "Train 192 | out_loss 0.0007681971183046699: 100%|█| 138/138 [00:00<00:00, 309.77\n",
      "Train Epoch192 out_loss -2.512392044067383\n",
      "Test Epoch192 layer0 out_loss 0.01230765599757433\n",
      "Test Epoch192 layer1 out_loss 0.0029130668845027685\n",
      "Test Epoch192 layer2 out_loss 0.0007286797626875341\n",
      "Train 193 | out_loss 0.0007600393728353083: 100%|█| 138/138 [00:00<00:00, 308.64\n",
      "Train Epoch193 out_loss -2.4381930828094482\n",
      "Test Epoch193 layer0 out_loss 0.012243231758475304\n",
      "Test Epoch193 layer1 out_loss 0.0030319674406200647\n",
      "Test Epoch193 layer2 out_loss 0.0007286169566214085\n",
      "Train 194 | out_loss 0.000755352433770895: 100%|█| 138/138 [00:00<00:00, 317.94i\n",
      "Train Epoch194 out_loss -2.395918369293213\n",
      "Test Epoch194 layer0 out_loss 0.011936787515878677\n",
      "Test Epoch194 layer1 out_loss 0.002856905572116375\n",
      "Test Epoch194 layer2 out_loss 0.0007304995669983327\n",
      "Train 195 | out_loss 0.0007486128015443683: 100%|█| 138/138 [00:00<00:00, 312.43\n",
      "Train Epoch195 out_loss -2.3355839252471924\n",
      "Test Epoch195 layer0 out_loss 0.01190109457820654\n",
      "Test Epoch195 layer1 out_loss 0.0028476337902247906\n",
      "Test Epoch195 layer2 out_loss 0.0007114394102245569\n",
      "Train 196 | out_loss 0.0007432359852828085: 100%|█| 138/138 [00:00<00:00, 271.40\n",
      "Train Epoch196 out_loss -2.2878429889678955\n",
      "Test Epoch196 layer0 out_loss 0.011873496696352959\n",
      "Test Epoch196 layer1 out_loss 0.002846501301974058\n",
      "Test Epoch196 layer2 out_loss 0.0007189831230789423\n",
      "Train 197 | out_loss 0.000741237192414701: 100%|█| 138/138 [00:00<00:00, 303.15i\n",
      "Train Epoch197 out_loss -2.2701852321624756\n",
      "Test Epoch197 layer0 out_loss 0.011577686294913292\n",
      "Test Epoch197 layer1 out_loss 0.002798296045511961\n",
      "Test Epoch197 layer2 out_loss 0.0007052026339806616\n",
      "Train 198 | out_loss 0.0007377046276815236: 100%|█| 138/138 [00:00<00:00, 313.58\n",
      "Train Epoch198 out_loss -2.2390880584716797\n",
      "Test Epoch198 layer0 out_loss 0.011318379081785679\n",
      "Test Epoch198 layer1 out_loss 0.0027756388299167156\n",
      "Test Epoch198 layer2 out_loss 0.000745428551454097\n",
      "Train 199 | out_loss 0.0007272323127835989: 100%|█| 138/138 [00:00<00:00, 310.60\n",
      "Train Epoch199 out_loss -2.1477739810943604\n",
      "Test Epoch199 layer0 out_loss 0.01127389445900917\n",
      "Test Epoch199 layer1 out_loss 0.0027905490715056658\n",
      "Test Epoch199 layer2 out_loss 0.0007140777888707817\n",
      "Train 200 | out_loss 0.0007253932417370379: 100%|█| 138/138 [00:00<00:00, 312.58\n",
      "Train Epoch200 out_loss -2.131876230239868\n",
      "Test Epoch200 layer0 out_loss 0.011255292221903801\n",
      "Test Epoch200 layer1 out_loss 0.00273237400688231\n",
      "Test Epoch200 layer2 out_loss 0.0007129556615836918\n",
      "Train 201 | out_loss 0.0007114656618796289: 100%|█| 138/138 [00:00<00:00, 310.28\n",
      "Train Epoch201 out_loss -2.0127665996551514\n",
      "Test Epoch201 layer0 out_loss 0.010934275574982166\n",
      "Test Epoch201 layer1 out_loss 0.0027657996397465467\n",
      "Test Epoch201 layer2 out_loss 0.0007047528633847833\n",
      "Train 202 | out_loss 0.0007223566644825041: 100%|█| 138/138 [00:00<00:00, 311.36\n",
      "Train Epoch202 out_loss -2.1057119369506836\n",
      "Test Epoch202 layer0 out_loss 0.01086688693612814\n",
      "Test Epoch202 layer1 out_loss 0.0027059291023761034\n",
      "Test Epoch202 layer2 out_loss 0.0007493983721360564\n",
      "Train 203 | out_loss 0.0007124855765141547: 100%|█| 138/138 [00:00<00:00, 313.43\n",
      "Train Epoch203 out_loss -2.0214123725891113\n",
      "Test Epoch203 layer0 out_loss 0.010818217881023884\n",
      "Test Epoch203 layer1 out_loss 0.0027149738743901253\n",
      "Test Epoch203 layer2 out_loss 0.0007245044107548892\n",
      "Train 204 | out_loss 0.0007038896437734365: 100%|█| 138/138 [00:00<00:00, 315.40\n",
      "Train Epoch204 out_loss -1.9489455223083496\n",
      "Test Epoch204 layer0 out_loss 0.010590607300400734\n",
      "Test Epoch204 layer1 out_loss 0.0026630365755409002\n",
      "Test Epoch204 layer2 out_loss 0.0006963421474210918\n",
      "Train 205 | out_loss 0.0006927041104063392: 100%|█| 138/138 [00:00<00:00, 314.28\n",
      "Train Epoch205 out_loss -1.8559679985046387\n",
      "Test Epoch205 layer0 out_loss 0.010462800972163677\n",
      "Test Epoch205 layer1 out_loss 0.0027036310639232397\n",
      "Test Epoch205 layer2 out_loss 0.0006742995465174317\n",
      "Train 206 | out_loss 0.0006910352967679501: 100%|█| 138/138 [00:00<00:00, 312.26\n",
      "Train Epoch206 out_loss -1.8422226905822754\n",
      "Test Epoch206 layer0 out_loss 0.010476300492882729\n",
      "Test Epoch206 layer1 out_loss 0.0027067740447819233\n",
      "Test Epoch206 layer2 out_loss 0.0006681812810711563\n",
      "Train 207 | out_loss 0.000689181440975517: 100%|█| 138/138 [00:00<00:00, 313.83i\n",
      "Train Epoch207 out_loss -1.826993465423584\n",
      "Test Epoch207 layer0 out_loss 0.010817334987223148\n",
      "Test Epoch207 layer1 out_loss 0.0026493147015571594\n",
      "Test Epoch207 layer2 out_loss 0.0006617690087296069\n",
      "Train 208 | out_loss 0.0006874795071780682: 100%|█| 138/138 [00:00<00:00, 307.39\n",
      "Train Epoch208 out_loss -1.8130455017089844\n",
      "Test Epoch208 layer0 out_loss 0.010469735600054264\n",
      "Test Epoch208 layer1 out_loss 0.0026246237102895975\n",
      "Test Epoch208 layer2 out_loss 0.0006494307890534401\n",
      "Train 209 | out_loss 0.0006894340622238815: 100%|█| 138/138 [00:00<00:00, 312.62\n",
      "Train Epoch209 out_loss -1.8290693759918213\n",
      "Test Epoch209 layer0 out_loss 0.01035922672599554\n",
      "Test Epoch209 layer1 out_loss 0.00260625290684402\n",
      "Test Epoch209 layer2 out_loss 0.0006417585536837578\n",
      "Train 210 | out_loss 0.0006727881846018136: 100%|█| 138/138 [00:00<00:00, 320.63\n",
      "Train Epoch210 out_loss -1.6941032409667969\n",
      "Test Epoch210 layer0 out_loss 0.010166948661208153\n",
      "Test Epoch210 layer1 out_loss 0.0025577181950211525\n",
      "Test Epoch210 layer2 out_loss 0.0006600389024242759\n",
      "Train 211 | out_loss 0.0006788669852539897: 100%|█| 138/138 [00:00<00:00, 318.31\n",
      "Train Epoch211 out_loss -1.743006944656372\n",
      "Test Epoch211 layer0 out_loss 0.010086792521178722\n",
      "Test Epoch211 layer1 out_loss 0.002536621643230319\n",
      "Test Epoch211 layer2 out_loss 0.0006395767559297383\n",
      "Train 212 | out_loss 0.0006636579637415707: 100%|█| 138/138 [00:00<00:00, 310.82\n",
      "Train Epoch212 out_loss -1.6214795112609863\n",
      "Test Epoch212 layer0 out_loss 0.01004994660615921\n",
      "Test Epoch212 layer1 out_loss 0.0025415527634322643\n",
      "Test Epoch212 layer2 out_loss 0.0006590117700397968\n",
      "Train 213 | out_loss 0.0006631645956076682: 100%|█| 138/138 [00:00<00:00, 309.88\n",
      "Train Epoch213 out_loss -1.6175832748413086\n",
      "Test Epoch213 layer0 out_loss 0.009936848655343056\n",
      "Test Epoch213 layer1 out_loss 0.002606844762340188\n",
      "Test Epoch213 layer2 out_loss 0.0006598877371288836\n",
      "Train 214 | out_loss 0.0006587589741684496: 100%|█| 138/138 [00:00<00:00, 314.90\n",
      "Train Epoch214 out_loss -1.5829226970672607\n",
      "Test Epoch214 layer0 out_loss 0.009910926222801208\n",
      "Test Epoch214 layer1 out_loss 0.0025294218212366104\n",
      "Test Epoch214 layer2 out_loss 0.0006204670644365251\n",
      "Train 215 | out_loss 0.0006493738619610667: 100%|█| 138/138 [00:00<00:00, 316.40\n",
      "Train Epoch215 out_loss -1.50984787940979\n",
      "Test Epoch215 layer0 out_loss 0.009802468121051788\n",
      "Test Epoch215 layer1 out_loss 0.0024967954959720373\n",
      "Test Epoch215 layer2 out_loss 0.0006602774374186993\n",
      "Train 216 | out_loss 0.0006477781571447849: 100%|█| 138/138 [00:00<00:00, 317.31\n",
      "Train Epoch216 out_loss -1.4975261688232422\n",
      "Test Epoch216 layer0 out_loss 0.00950681883841753\n",
      "Test Epoch216 layer1 out_loss 0.002504125004634261\n",
      "Test Epoch216 layer2 out_loss 0.0006474832771345973\n",
      "Train 217 | out_loss 0.0006466852501034737: 100%|█| 138/138 [00:00<00:00, 319.71\n",
      "Train Epoch217 out_loss -1.4891066551208496\n",
      "Test Epoch217 layer0 out_loss 0.009770471602678299\n",
      "Test Epoch217 layer1 out_loss 0.0024488659109920263\n",
      "Test Epoch217 layer2 out_loss 0.0006161116762086749\n",
      "Train 218 | out_loss 0.000641818915028125: 100%|█| 138/138 [00:00<00:00, 318.96i\n",
      "Train Epoch218 out_loss -1.451784610748291\n",
      "Test Epoch218 layer0 out_loss 0.0092994449660182\n",
      "Test Epoch218 layer1 out_loss 0.0025207879953086376\n",
      "Test Epoch218 layer2 out_loss 0.0006066365167498589\n",
      "Train 219 | out_loss 0.0006371230119839311: 100%|█| 138/138 [00:00<00:00, 319.29\n",
      "Train Epoch219 out_loss -1.4160418510437012\n",
      "Test Epoch219 layer0 out_loss 0.009331521578133106\n",
      "Test Epoch219 layer1 out_loss 0.002493432490155101\n",
      "Test Epoch219 layer2 out_loss 0.0006174767040647566\n",
      "Train 220 | out_loss 0.0006384065491147339: 100%|█| 138/138 [00:00<00:00, 319.26\n",
      "Train Epoch220 out_loss -1.4257831573486328\n",
      "Test Epoch220 layer0 out_loss 0.009121737442910671\n",
      "Test Epoch220 layer1 out_loss 0.002412105444818735\n",
      "Test Epoch220 layer2 out_loss 0.0006373421638272703\n",
      "Train 221 | out_loss 0.0006266227574087679: 100%|█| 138/138 [00:00<00:00, 313.82\n",
      "Train Epoch221 out_loss -1.3370623588562012\n",
      "Test Epoch221 layer0 out_loss 0.009075230918824673\n",
      "Test Epoch221 layer1 out_loss 0.0023975945077836514\n",
      "Test Epoch221 layer2 out_loss 0.0007578050717711449\n",
      "Train 222 | out_loss 0.0006208663689903915: 100%|█| 138/138 [00:00<00:00, 316.11\n",
      "Train Epoch222 out_loss -1.294321060180664\n",
      "Test Epoch222 layer0 out_loss 0.00892437994480133\n",
      "Test Epoch222 layer1 out_loss 0.002386931562796235\n",
      "Test Epoch222 layer2 out_loss 0.0005887128063477576\n",
      "Train 223 | out_loss 0.0006151476409286261: 100%|█| 138/138 [00:00<00:00, 316.41\n",
      "Train Epoch223 out_loss -1.2522504329681396\n",
      "Test Epoch223 layer0 out_loss 0.008807720616459846\n",
      "Test Epoch223 layer1 out_loss 0.002370374044403434\n",
      "Test Epoch223 layer2 out_loss 0.000603844178840518\n",
      "Train 224 | out_loss 0.0006154461880214512: 100%|█| 138/138 [00:00<00:00, 315.13\n",
      "Train Epoch224 out_loss -1.2544379234313965\n",
      "Test Epoch224 layer0 out_loss 0.008725345134735107\n",
      "Test Epoch224 layer1 out_loss 0.0023495606146752834\n",
      "Test Epoch224 layer2 out_loss 0.0006113343988545239\n",
      "Train 225 | out_loss 0.0006060987943783402: 100%|█| 138/138 [00:00<00:00, 313.72\n",
      "Train Epoch225 out_loss -1.1864726543426514\n",
      "Test Epoch225 layer0 out_loss 0.008823261596262455\n",
      "Test Epoch225 layer1 out_loss 0.002340238308534026\n",
      "Test Epoch225 layer2 out_loss 0.0005811633891426027\n",
      "Train 226 | out_loss 0.0006104362546466291: 100%|█| 138/138 [00:00<00:00, 318.65\n",
      "Train Epoch226 out_loss -1.217881679534912\n",
      "Test Epoch226 layer0 out_loss 0.008796555921435356\n",
      "Test Epoch226 layer1 out_loss 0.0023735957220196724\n",
      "Test Epoch226 layer2 out_loss 0.0006540660979226232\n",
      "Train 227 | out_loss 0.0005978649714961648: 100%|█| 138/138 [00:00<00:00, 307.99\n",
      "Train Epoch227 out_loss -1.1274731159210205\n",
      "Test Epoch227 layer0 out_loss 0.008457979187369347\n",
      "Test Epoch227 layer1 out_loss 0.0023765091318637133\n",
      "Test Epoch227 layer2 out_loss 0.0005727204843424261\n",
      "Train 228 | out_loss 0.000604093074798584: 100%|█| 138/138 [00:00<00:00, 320.32i\n",
      "Train Epoch228 out_loss -1.1720302104949951\n",
      "Test Epoch228 layer0 out_loss 0.00833395030349493\n",
      "Test Epoch228 layer1 out_loss 0.002363039879128337\n",
      "Test Epoch228 layer2 out_loss 0.0005846342537552118\n",
      "Train 229 | out_loss 0.0005943806027062237: 100%|█| 138/138 [00:00<00:00, 306.76\n",
      "Train Epoch229 out_loss -1.1027462482452393\n",
      "Test Epoch229 layer0 out_loss 0.0084223048761487\n",
      "Test Epoch229 layer1 out_loss 0.0022572327870875597\n",
      "Test Epoch229 layer2 out_loss 0.0005878262454643846\n",
      "Train 230 | out_loss 0.0005927675520069897: 100%|█| 138/138 [00:00<00:00, 312.19\n",
      "Train Epoch230 out_loss -1.09134840965271\n",
      "Test Epoch230 layer0 out_loss 0.008212060667574406\n",
      "Test Epoch230 layer1 out_loss 0.002252290491014719\n",
      "Test Epoch230 layer2 out_loss 0.0005670911050401628\n",
      "Train 231 | out_loss 0.0005903665442019701: 100%|█| 138/138 [00:00<00:00, 309.70\n",
      "Train Epoch231 out_loss -1.074439525604248\n",
      "Test Epoch231 layer0 out_loss 0.008104673586785793\n",
      "Test Epoch231 layer1 out_loss 0.0022625711280852556\n",
      "Test Epoch231 layer2 out_loss 0.0005941514391452074\n",
      "Train 232 | out_loss 0.0005865878192707896: 100%|█| 138/138 [00:00<00:00, 309.07\n",
      "Train Epoch232 out_loss -1.0479698181152344\n",
      "Test Epoch232 layer0 out_loss 0.008080655708909035\n",
      "Test Epoch232 layer1 out_loss 0.002243003575131297\n",
      "Test Epoch232 layer2 out_loss 0.0005602907622233033\n",
      "Train 233 | out_loss 0.0005920220864936709: 100%|█| 138/138 [00:00<00:00, 313.78\n",
      "Train Epoch233 out_loss -1.086094856262207\n",
      "Test Epoch233 layer0 out_loss 0.0081325126811862\n",
      "Test Epoch233 layer1 out_loss 0.0022359113208949566\n",
      "Test Epoch233 layer2 out_loss 0.0005559602868743241\n",
      "Train 234 | out_loss 0.0005810211296193302: 100%|█| 138/138 [00:00<00:00, 314.98\n",
      "Train Epoch234 out_loss -1.009284257888794\n",
      "Test Epoch234 layer0 out_loss 0.007823813706636429\n",
      "Test Epoch234 layer1 out_loss 0.0022255394142121077\n",
      "Test Epoch234 layer2 out_loss 0.00055536167928949\n",
      "Train 235 | out_loss 0.000583167071454227: 100%|█| 138/138 [00:00<00:00, 308.98i\n",
      "Train Epoch235 out_loss -1.0241553783416748\n",
      "Test Epoch235 layer0 out_loss 0.00798095390200615\n",
      "Test Epoch235 layer1 out_loss 0.002205156721174717\n",
      "Test Epoch235 layer2 out_loss 0.0005481366533786058\n",
      "Train 236 | out_loss 0.0005791616858914495: 100%|█| 138/138 [00:00<00:00, 314.20\n",
      "Train Epoch236 out_loss -0.9964437484741211\n",
      "Test Epoch236 layer0 out_loss 0.007834160700440407\n",
      "Test Epoch236 layer1 out_loss 0.0021674761082977057\n",
      "Test Epoch236 layer2 out_loss 0.0005670126411132514\n",
      "Train 237 | out_loss 0.0005682185874320567: 100%|█| 138/138 [00:00<00:00, 321.46\n",
      "Train Epoch237 out_loss -0.9217123985290527\n",
      "Test Epoch237 layer0 out_loss 0.00780493812635541\n",
      "Test Epoch237 layer1 out_loss 0.0021586923394352198\n",
      "Test Epoch237 layer2 out_loss 0.0005351423751562834\n",
      "Train 238 | out_loss 0.0005667089717462659: 100%|█| 138/138 [00:00<00:00, 313.50\n",
      "Train Epoch238 out_loss -0.911514163017273\n",
      "Test Epoch238 layer0 out_loss 0.007907448336482048\n",
      "Test Epoch238 layer1 out_loss 0.0023390543647110462\n",
      "Test Epoch238 layer2 out_loss 0.0005908136372454464\n",
      "Train 239 | out_loss 0.0005640896270051599: 100%|█| 138/138 [00:00<00:00, 314.47\n",
      "Train Epoch239 out_loss -0.893885612487793\n",
      "Test Epoch239 layer0 out_loss 0.007580706384032965\n",
      "Test Epoch239 layer1 out_loss 0.002177091781049967\n",
      "Test Epoch239 layer2 out_loss 0.0005363596137613058\n",
      "Train 240 | out_loss 0.0005641156458295882: 100%|█| 138/138 [00:00<00:00, 305.91\n",
      "Train Epoch240 out_loss -0.8940621614456177\n",
      "Test Epoch240 layer0 out_loss 0.00737058836966753\n",
      "Test Epoch240 layer1 out_loss 0.0021063671447336674\n",
      "Test Epoch240 layer2 out_loss 0.0005431133904494345\n",
      "Train 241 | out_loss 0.0005586508777923882: 100%|█| 138/138 [00:00<00:00, 313.99\n",
      "Train Epoch241 out_loss -0.857540488243103\n",
      "Test Epoch241 layer0 out_loss 0.007303904742002487\n",
      "Test Epoch241 layer1 out_loss 0.00210972991771996\n",
      "Test Epoch241 layer2 out_loss 0.0005747557152062654\n",
      "Train 242 | out_loss 0.00056230160407722: 100%|█| 138/138 [00:00<00:00, 318.02it\n",
      "Train Epoch242 out_loss -0.8818973302841187\n",
      "Test Epoch242 layer0 out_loss 0.00769140524789691\n",
      "Test Epoch242 layer1 out_loss 0.002093491842970252\n",
      "Test Epoch242 layer2 out_loss 0.0005397365894168615\n",
      "Train 243 | out_loss 0.0005593081586994231: 100%|█| 138/138 [00:00<00:00, 312.43\n",
      "Train Epoch243 out_loss -0.8619165420532227\n",
      "Test Epoch243 layer0 out_loss 0.007396703120321035\n",
      "Test Epoch243 layer1 out_loss 0.0021607079543173313\n",
      "Test Epoch243 layer2 out_loss 0.0005357236950658262\n",
      "Train 244 | out_loss 0.0005561684374697506: 100%|█| 138/138 [00:00<00:00, 294.18\n",
      "Train Epoch244 out_loss -0.8410691022872925\n",
      "Test Epoch244 layer0 out_loss 0.0073739574290812016\n",
      "Test Epoch244 layer1 out_loss 0.0020861653611063957\n",
      "Test Epoch244 layer2 out_loss 0.0005727055831812322\n",
      "Train 245 | out_loss 0.0005539082339964807: 100%|█| 138/138 [00:00<00:00, 311.25\n",
      "Train Epoch245 out_loss -0.8261364698410034\n",
      "Test Epoch245 layer0 out_loss 0.007314779330044985\n",
      "Test Epoch245 layer1 out_loss 0.002041085623204708\n",
      "Test Epoch245 layer2 out_loss 0.0005536594544537365\n",
      "Train 246 | out_loss 0.0005509880720637739: 100%|█| 138/138 [00:00<00:00, 307.18\n",
      "Train Epoch246 out_loss -0.8069322109222412\n",
      "Test Epoch246 layer0 out_loss 0.007001684978604317\n",
      "Test Epoch246 layer1 out_loss 0.0020330785773694515\n",
      "Test Epoch246 layer2 out_loss 0.000516773434355855\n",
      "Train 247 | out_loss 0.0005458826199173927: 100%|█| 138/138 [00:00<00:00, 313.90\n",
      "Train Epoch247 out_loss -0.773604154586792\n",
      "Test Epoch247 layer0 out_loss 0.007109431084245443\n",
      "Test Epoch247 layer1 out_loss 0.0020237527787685394\n",
      "Test Epoch247 layer2 out_loss 0.0005212550167925656\n",
      "Train 248 | out_loss 0.0005421966779977083: 100%|█| 138/138 [00:00<00:00, 317.72\n",
      "Train Epoch248 out_loss -0.7497321367263794\n",
      "Test Epoch248 layer0 out_loss 0.007307886146008968\n",
      "Test Epoch248 layer1 out_loss 0.0020198847632855177\n",
      "Test Epoch248 layer2 out_loss 0.0005260179750621319\n",
      "Train 249 | out_loss 0.0005462504923343658: 100%|█| 138/138 [00:00<00:00, 318.13\n",
      "Train Epoch249 out_loss -0.7759941816329956\n",
      "Test Epoch249 layer0 out_loss 0.0069675990380346775\n",
      "Test Epoch249 layer1 out_loss 0.002080281963571906\n",
      "Test Epoch249 layer2 out_loss 0.0005721082561649382\n",
      "Train 250 | out_loss 0.0005404922412708402: 100%|█| 138/138 [00:00<00:00, 304.41\n",
      "Train Epoch250 out_loss -0.7387468814849854\n",
      "Test Epoch250 layer0 out_loss 0.007013858761638403\n",
      "Test Epoch250 layer1 out_loss 0.002012452110648155\n",
      "Test Epoch250 layer2 out_loss 0.0005956134991720319\n",
      "Train 251 | out_loss 0.0005387829733081162: 100%|█| 138/138 [00:00<00:00, 313.47\n",
      "Train Epoch251 out_loss -0.7277669906616211\n",
      "Test Epoch251 layer0 out_loss 0.006804156117141247\n",
      "Test Epoch251 layer1 out_loss 0.0019852553959935904\n",
      "Test Epoch251 layer2 out_loss 0.0005824991385452449\n",
      "Train 252 | out_loss 0.0005318204639479518: 100%|█| 138/138 [00:00<00:00, 310.92\n",
      "Train Epoch252 out_loss -0.6834039688110352\n",
      "Test Epoch252 layer0 out_loss 0.006668284069746733\n",
      "Test Epoch252 layer1 out_loss 0.0020048890728503466\n",
      "Test Epoch252 layer2 out_loss 0.0005283295758999884\n",
      "Train 253 | out_loss 0.000537427666131407: 100%|█| 138/138 [00:00<00:00, 313.16i\n",
      "Train Epoch253 out_loss -0.7190847396850586\n",
      "Test Epoch253 layer0 out_loss 0.006656155455857515\n",
      "Test Epoch253 layer1 out_loss 0.0019896149169653654\n",
      "Test Epoch253 layer2 out_loss 0.0005273104761727154\n",
      "Train 254 | out_loss 0.0005268535460345447: 100%|█| 138/138 [00:00<00:00, 320.14\n",
      "Train Epoch254 out_loss -0.6521047353744507\n",
      "Test Epoch254 layer0 out_loss 0.006515295244753361\n",
      "Test Epoch254 layer1 out_loss 0.0019359397701919079\n",
      "Test Epoch254 layer2 out_loss 0.0005062940181232989\n",
      "Train 255 | out_loss 0.0005257284501567483: 100%|█| 138/138 [00:00<00:00, 306.07\n",
      "Train Epoch255 out_loss -0.6450564861297607\n",
      "Test Epoch255 layer0 out_loss 0.006654398050159216\n",
      "Test Epoch255 layer1 out_loss 0.0019201645627617836\n",
      "Test Epoch255 layer2 out_loss 0.0005183835164643824\n",
      "Train 256 | out_loss 0.0005255764699541032: 100%|█| 138/138 [00:00<00:00, 318.65\n",
      "Train Epoch256 out_loss -0.644102931022644\n",
      "Test Epoch256 layer0 out_loss 0.00671194726601243\n",
      "Test Epoch256 layer1 out_loss 0.0019138352945446968\n",
      "Test Epoch256 layer2 out_loss 0.0005026384023949504\n",
      "Train 257 | out_loss 0.0005179301369935274: 100%|█| 138/138 [00:00<00:00, 312.95\n",
      "Train Epoch257 out_loss -0.5966129302978516\n",
      "Test Epoch257 layer0 out_loss 0.006646817084401846\n",
      "Test Epoch257 layer1 out_loss 0.0019125667167827487\n",
      "Test Epoch257 layer2 out_loss 0.0004942078376188874\n",
      "Train 258 | out_loss 0.000528433418367058: 100%|█| 138/138 [00:00<00:00, 311.92i\n",
      "Train Epoch258 out_loss -0.6620273590087891\n",
      "Test Epoch258 layer0 out_loss 0.00662704324349761\n",
      "Test Epoch258 layer1 out_loss 0.001967904856428504\n",
      "Test Epoch258 layer2 out_loss 0.0005945676239207387\n",
      "Train 259 | out_loss 0.0005221644532866776: 100%|█| 138/138 [00:00<00:00, 310.17\n",
      "Train Epoch259 out_loss -0.6228266954421997\n",
      "Test Epoch259 layer0 out_loss 0.0069184498861432076\n",
      "Test Epoch259 layer1 out_loss 0.0018667898839339614\n",
      "Test Epoch259 layer2 out_loss 0.0004907992552034557\n",
      "Train 260 | out_loss 0.0005188201903365552: 100%|█| 138/138 [00:00<00:00, 308.17\n",
      "Train Epoch260 out_loss -0.6021066904067993\n",
      "Test Epoch260 layer0 out_loss 0.006288014817982912\n",
      "Test Epoch260 layer1 out_loss 0.0018787364242598414\n",
      "Test Epoch260 layer2 out_loss 0.0005528662586584687\n",
      "Train 261 | out_loss 0.0005201934254728258: 100%|█| 138/138 [00:00<00:00, 309.39\n",
      "Train Epoch261 out_loss -0.6105990409851074\n",
      "Test Epoch261 layer0 out_loss 0.006476627662777901\n",
      "Test Epoch261 layer1 out_loss 0.0019047880778089166\n",
      "Test Epoch261 layer2 out_loss 0.0004984987317584455\n",
      "Train 262 | out_loss 0.0005135351093485951: 100%|█| 138/138 [00:00<00:00, 314.75\n",
      "Train Epoch262 out_loss -0.5696312189102173\n",
      "Test Epoch262 layer0 out_loss 0.006297474727034569\n",
      "Test Epoch262 layer1 out_loss 0.0018671939615160227\n",
      "Test Epoch262 layer2 out_loss 0.0004949761787429452\n",
      "Train 263 | out_loss 0.0005155402468517423: 100%|█| 138/138 [00:00<00:00, 311.43\n",
      "Train Epoch263 out_loss -0.5819144248962402\n",
      "Test Epoch263 layer0 out_loss 0.006600393448024988\n",
      "Test Epoch263 layer1 out_loss 0.0018329343292862177\n",
      "Test Epoch263 layer2 out_loss 0.0005009864689782262\n",
      "Train 264 | out_loss 0.0005247186636552215: 100%|█| 138/138 [00:00<00:00, 291.68\n",
      "Train Epoch264 out_loss -0.6387426853179932\n",
      "Test Epoch264 layer0 out_loss 0.006135378032922745\n",
      "Test Epoch264 layer1 out_loss 0.0018584560602903366\n",
      "Test Epoch264 layer2 out_loss 0.0005567538319155574\n",
      "Train 265 | out_loss 0.0005104258307255805: 100%|█| 138/138 [00:00<00:00, 305.56\n",
      "Train Epoch265 out_loss -0.5506831407546997\n",
      "Test Epoch265 layer0 out_loss 0.006289919372648001\n",
      "Test Epoch265 layer1 out_loss 0.0018505733460187912\n",
      "Test Epoch265 layer2 out_loss 0.0005356616456992924\n",
      "Train 266 | out_loss 0.0005161521839909256: 100%|█| 138/138 [00:00<00:00, 316.00\n",
      "Train Epoch266 out_loss -0.5856715440750122\n",
      "Test Epoch266 layer0 out_loss 0.006215229164808989\n",
      "Test Epoch266 layer1 out_loss 0.0018388323951512575\n",
      "Test Epoch266 layer2 out_loss 0.00048097060061991215\n",
      "Train 267 | out_loss 0.0005112046492286026: 100%|█| 138/138 [00:00<00:00, 313.41\n",
      "Train Epoch267 out_loss -0.5554195642471313\n",
      "Test Epoch267 layer0 out_loss 0.0061752209439873695\n",
      "Test Epoch267 layer1 out_loss 0.0018716284539550543\n",
      "Test Epoch267 layer2 out_loss 0.0005244538187980652\n",
      "Train 268 | out_loss 0.0005027948063798249: 100%|█| 138/138 [00:00<00:00, 314.25\n",
      "Train Epoch268 out_loss -0.5046628713607788\n",
      "Test Epoch268 layer0 out_loss 0.006342221517115831\n",
      "Test Epoch268 layer1 out_loss 0.0018071014201268554\n",
      "Test Epoch268 layer2 out_loss 0.0004960003425367177\n",
      "Train 269 | out_loss 0.0005050604231655598: 100%|█| 138/138 [00:00<00:00, 310.36\n",
      "Train Epoch269 out_loss -0.5182543992996216\n",
      "Test Epoch269 layer0 out_loss 0.006040831562131643\n",
      "Test Epoch269 layer1 out_loss 0.001760857179760933\n",
      "Test Epoch269 layer2 out_loss 0.0004849520046263933\n",
      "Train 270 | out_loss 0.0004963079700246453: 100%|█| 138/138 [00:00<00:00, 312.57\n",
      "Train Epoch270 out_loss -0.46608901023864746\n",
      "Test Epoch270 layer0 out_loss 0.005796030629426241\n",
      "Test Epoch270 layer1 out_loss 0.0017778252949938178\n",
      "Test Epoch270 layer2 out_loss 0.0004996583447791636\n",
      "Train 271 | out_loss 0.0005034926580265164: 100%|█| 138/138 [00:00<00:00, 315.94\n",
      "Train Epoch271 out_loss -0.5088440179824829\n",
      "Test Epoch271 layer0 out_loss 0.006040247622877359\n",
      "Test Epoch271 layer1 out_loss 0.0017577275866642594\n",
      "Test Epoch271 layer2 out_loss 0.00048117514234036207\n",
      "Train 272 | out_loss 0.0004956411430612206: 100%|█| 138/138 [00:00<00:00, 314.80\n",
      "Train Epoch272 out_loss -0.4621521234512329\n",
      "Test Epoch272 layer0 out_loss 0.005762515123933554\n",
      "Test Epoch272 layer1 out_loss 0.0018395200604572892\n",
      "Test Epoch272 layer2 out_loss 0.0004928517155349255\n",
      "Train 273 | out_loss 0.0005006794817745686: 100%|█| 138/138 [00:00<00:00, 314.28\n",
      "Train Epoch273 out_loss -0.4920283555984497\n",
      "Test Epoch273 layer0 out_loss 0.0060630072839558125\n",
      "Test Epoch273 layer1 out_loss 0.0017406608676537871\n",
      "Test Epoch273 layer2 out_loss 0.0004725902690552175\n",
      "Train 274 | out_loss 0.0005016433424316347: 100%|█| 138/138 [00:00<00:00, 306.18\n",
      "Train Epoch274 out_loss -0.49777936935424805\n",
      "Test Epoch274 layer0 out_loss 0.006140844896435738\n",
      "Test Epoch274 layer1 out_loss 0.0017134511144831777\n",
      "Test Epoch274 layer2 out_loss 0.000491723942104727\n",
      "Train 275 | out_loss 0.0004934185999445617: 100%|█| 138/138 [00:00<00:00, 315.24\n",
      "Train Epoch275 out_loss -0.4490680694580078\n",
      "Test Epoch275 layer0 out_loss 0.005900832824409008\n",
      "Test Epoch275 layer1 out_loss 0.0017439834773540497\n",
      "Test Epoch275 layer2 out_loss 0.00047645767335779965\n",
      "Train 276 | out_loss 0.0004951275186613202: 100%|█| 138/138 [00:00<00:00, 300.35\n",
      "Train Epoch276 out_loss -0.45912396907806396\n",
      "Test Epoch276 layer0 out_loss 0.006068672519177198\n",
      "Test Epoch276 layer1 out_loss 0.0017897466896101832\n",
      "Test Epoch276 layer2 out_loss 0.0005176377599127591\n",
      "Train 277 | out_loss 0.0004907737602479756: 100%|█| 138/138 [00:00<00:00, 317.13\n",
      "Train Epoch277 out_loss -0.43357419967651367\n",
      "Test Epoch277 layer0 out_loss 0.005649181082844734\n",
      "Test Epoch277 layer1 out_loss 0.0018677955958992243\n",
      "Test Epoch277 layer2 out_loss 0.0004735268885269761\n",
      "Train 278 | out_loss 0.0005025523714721203: 100%|█| 138/138 [00:00<00:00, 315.52\n",
      "Train Epoch278 out_loss -0.5032122135162354\n",
      "Test Epoch278 layer0 out_loss 0.005792382173240185\n",
      "Test Epoch278 layer1 out_loss 0.0016767267370596528\n",
      "Test Epoch278 layer2 out_loss 0.0005126151372678578\n",
      "Train 279 | out_loss 0.00048826876445673406: 100%|█| 138/138 [00:00<00:00, 310.4\n",
      "Train Epoch279 out_loss -0.418978214263916\n",
      "Test Epoch279 layer0 out_loss 0.00543923070654273\n",
      "Test Epoch279 layer1 out_loss 0.001671829610131681\n",
      "Test Epoch279 layer2 out_loss 0.00047226675087586045\n",
      "Train 280 | out_loss 0.0004923483356833458: 100%|█| 138/138 [00:00<00:00, 308.24\n",
      "Train Epoch280 out_loss -0.44278812408447266\n",
      "Test Epoch280 layer0 out_loss 0.0054307980462908745\n",
      "Test Epoch280 layer1 out_loss 0.0017186145996674895\n",
      "Test Epoch280 layer2 out_loss 0.00050462776562199\n",
      "Train 281 | out_loss 0.0004873160505667329: 100%|█| 138/138 [00:00<00:00, 309.56\n",
      "Train Epoch281 out_loss -0.41344594955444336\n",
      "Test Epoch281 layer0 out_loss 0.005386122036725283\n",
      "Test Epoch281 layer1 out_loss 0.0016478663310408592\n",
      "Test Epoch281 layer2 out_loss 0.0004700541903730482\n",
      "Train 282 | out_loss 0.0004908335977233946: 100%|█| 138/138 [00:00<00:00, 306.28\n",
      "Train Epoch282 out_loss -0.43392419815063477\n",
      "Test Epoch282 layer0 out_loss 0.005412064027041197\n",
      "Test Epoch282 layer1 out_loss 0.0016702262219041586\n",
      "Test Epoch282 layer2 out_loss 0.00046657081111334264\n",
      "Train 283 | out_loss 0.0004825979995075613: 100%|█| 138/138 [00:00<00:00, 306.96\n",
      "Train Epoch283 out_loss -0.3862088918685913\n",
      "Test Epoch283 layer0 out_loss 0.005327003076672554\n",
      "Test Epoch283 layer1 out_loss 0.001654727617278695\n",
      "Test Epoch283 layer2 out_loss 0.0004788439837284386\n",
      "Train 284 | out_loss 0.0004846674855798483: 100%|█| 138/138 [00:00<00:00, 314.06\n",
      "Train Epoch284 out_loss -0.3981229066848755\n",
      "Test Epoch284 layer0 out_loss 0.005606450606137514\n",
      "Test Epoch284 layer1 out_loss 0.0016271002823486924\n",
      "Test Epoch284 layer2 out_loss 0.0005382629460655153\n",
      "Train 285 | out_loss 0.00048367329873144627: 100%|█| 138/138 [00:00<00:00, 310.0\n",
      "Train Epoch285 out_loss -0.39239370822906494\n",
      "Test Epoch285 layer0 out_loss 0.005860616452991962\n",
      "Test Epoch285 layer1 out_loss 0.0016278040129691362\n",
      "Test Epoch285 layer2 out_loss 0.000468360201921314\n",
      "Train 286 | out_loss 0.0004775641136802733: 100%|█| 138/138 [00:00<00:00, 315.60\n",
      "Train Epoch286 out_loss -0.35744190216064453\n",
      "Test Epoch286 layer0 out_loss 0.00528327189385891\n",
      "Test Epoch286 layer1 out_loss 0.0017507591983303428\n",
      "Test Epoch286 layer2 out_loss 0.0004594932252075523\n",
      "Train 287 | out_loss 0.0004779498849529773: 100%|█| 138/138 [00:00<00:00, 300.38\n",
      "Train Epoch287 out_loss -0.3596365451812744\n",
      "Test Epoch287 layer0 out_loss 0.0052240765653550625\n",
      "Test Epoch287 layer1 out_loss 0.0016095180762931705\n",
      "Test Epoch287 layer2 out_loss 0.00045665563084185123\n",
      "Train 288 | out_loss 0.000489944068249315: 100%|█| 138/138 [00:00<00:00, 308.14i\n",
      "Train Epoch288 out_loss -0.4287310838699341\n",
      "Test Epoch288 layer0 out_loss 0.005229520611464977\n",
      "Test Epoch288 layer1 out_loss 0.001617702771909535\n",
      "Test Epoch288 layer2 out_loss 0.00048431247705593705\n",
      "Train 289 | out_loss 0.00047695281682536006: 100%|█| 138/138 [00:00<00:00, 306.8\n",
      "Train Epoch289 out_loss -0.35396814346313477\n",
      "Test Epoch289 layer0 out_loss 0.005188299342989922\n",
      "Test Epoch289 layer1 out_loss 0.0015749314334243536\n",
      "Test Epoch289 layer2 out_loss 0.0004568214062601328\n",
      "Train 290 | out_loss 0.00047512364108115435: 100%|█| 138/138 [00:00<00:00, 308.4\n",
      "Train Epoch290 out_loss -0.3436034917831421\n",
      "Test Epoch290 layer0 out_loss 0.0052224425598979\n",
      "Test Epoch290 layer1 out_loss 0.0016550907166674733\n",
      "Test Epoch290 layer2 out_loss 0.0004580648383125663\n",
      "Train 291 | out_loss 0.00047561226529069245: 100%|█| 138/138 [00:00<00:00, 313.7\n",
      "Train Epoch291 out_loss -0.3463679552078247\n",
      "Test Epoch291 layer0 out_loss 0.00518103176727891\n",
      "Test Epoch291 layer1 out_loss 0.001648364239372313\n",
      "Test Epoch291 layer2 out_loss 0.0004567438445519656\n",
      "Train 292 | out_loss 0.0004727244086097926: 100%|█| 138/138 [00:00<00:00, 270.00\n",
      "Train Epoch292 out_loss -0.33006906509399414\n",
      "Test Epoch292 layer0 out_loss 0.0051541514694690704\n",
      "Test Epoch292 layer1 out_loss 0.001568791689351201\n",
      "Test Epoch292 layer2 out_loss 0.0004770202504005283\n",
      "Train 293 | out_loss 0.00047575030475854874: 100%|█| 138/138 [00:00<00:00, 309.7\n",
      "Train Epoch293 out_loss -0.347149133682251\n",
      "Test Epoch293 layer0 out_loss 0.005010516848415136\n",
      "Test Epoch293 layer1 out_loss 0.0015509212389588356\n",
      "Test Epoch293 layer2 out_loss 0.00046207342529669404\n",
      "Train 294 | out_loss 0.00047702869051136076: 100%|█| 138/138 [00:00<00:00, 301.3\n",
      "Train Epoch294 out_loss -0.3543999195098877\n",
      "Test Epoch294 layer0 out_loss 0.005114274565130472\n",
      "Test Epoch294 layer1 out_loss 0.0016118602361530066\n",
      "Test Epoch294 layer2 out_loss 0.0004584742127917707\n",
      "Train 295 | out_loss 0.00047330884262919426: 100%|█| 138/138 [00:00<00:00, 312.0\n",
      "Train Epoch295 out_loss -0.3333582878112793\n",
      "Test Epoch295 layer0 out_loss 0.0050904112868011\n",
      "Test Epoch295 layer1 out_loss 0.0015626726672053337\n",
      "Test Epoch295 layer2 out_loss 0.00044939064537175\n",
      "Train 296 | out_loss 0.00047651820932514966: 100%|█| 138/138 [00:00<00:00, 314.1\n",
      "Train Epoch296 out_loss -0.3515009880065918\n",
      "Test Epoch296 layer0 out_loss 0.005330382846295834\n",
      "Test Epoch296 layer1 out_loss 0.0015473563689738512\n",
      "Test Epoch296 layer2 out_loss 0.0004623222630470991\n",
      "Train 297 | out_loss 0.00046929428935982287: 100%|█| 138/138 [00:00<00:00, 310.2\n",
      "Train Epoch297 out_loss -0.31083500385284424\n",
      "Test Epoch297 layer0 out_loss 0.004889931529760361\n",
      "Test Epoch297 layer1 out_loss 0.0015362250851467252\n",
      "Test Epoch297 layer2 out_loss 0.0004562813264783472\n",
      "Train 298 | out_loss 0.0004691397480200976: 100%|█| 138/138 [00:00<00:00, 310.12\n",
      "Train Epoch298 out_loss -0.3099721670150757\n",
      "Test Epoch298 layer0 out_loss 0.004984547384083271\n",
      "Test Epoch298 layer1 out_loss 0.0015076994895935059\n",
      "Test Epoch298 layer2 out_loss 0.00048199421144090593\n",
      "Train 299 | out_loss 0.00046988253598101437: 100%|█| 138/138 [00:00<00:00, 312.6\n",
      "Train Epoch299 out_loss -0.3141237497329712\n",
      "Test Epoch299 layer0 out_loss 0.004913513083010912\n",
      "Test Epoch299 layer1 out_loss 0.0016385233029723167\n",
      "Test Epoch299 layer2 out_loss 0.000475111766718328\n",
      "Best loss 0.00044939064537175 at L2\n",
      "Figure(640x480)\n",
      "Figure(640x480)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/AL_main_new/dis_train_al.py\", line 409, in <module>\n",
      "    main()\n",
      "  File \"/home/AL_main_new/dis_train_al.py\", line 401, in main\n",
      "    plotResult(model,'result/'+ path_name, args.task)\n",
      "  File \"/home/AL_main_new/dis_train_al.py\", line 48, in plotResult\n",
      "    plt.plot(history[\"train_out\"], \"k\", label='train_out' )\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/matplotlib/pyplot.py\", line 2757, in plot\n",
      "    return gca().plot(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/matplotlib/axes/_axes.py\", line 1632, in plot\n",
      "    lines = [*self._get_lines(*args, data=data, **kwargs)]\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/matplotlib/axes/_base.py\", line 312, in __call__\n",
      "    yield from self._plot_args(this, kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/matplotlib/axes/_base.py\", line 490, in _plot_args\n",
      "    x, y = index_of(xy[-1])\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/matplotlib/cbook/__init__.py\", line 1652, in index_of\n",
      "    y = _check_1d(y)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/matplotlib/cbook/__init__.py\", line 1304, in _check_1d\n",
      "    return np.atleast_1d(x)\n",
      "  File \"<__array_function__ internals>\", line 5, in atleast_1d\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/numpy/core/shape_base.py\", line 65, in atleast_1d\n",
      "    ary = asanyarray(ary)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/_tensor.py\", line 757, in __array__\n",
      "    return self.numpy()\n",
      "RuntimeError: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.\n"
     ]
    }
   ],
   "source": [
    "# LinearAL ailerons\n",
    "\n",
    "data = \"ailerons\"\n",
    "\n",
    "model =  \"linearal\"\n",
    "#for layer in range(1,11):\n",
    "for layer in [3]:\n",
    "    log = f\"result/{data}_{model}_l{layer}.log\"\n",
    "    !python3 dis_train_al.py --dataset {data} --model {model} --epoch 300 --num-layer {layer} --lr 0.00001 --task regression # > {log}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchmetrics\n",
      "  Downloading torchmetrics-0.9.3-py3-none-any.whl (419 kB)\n",
      "\u001b[K     |████████████████████████████████| 419 kB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.3.1 in /opt/conda/lib/python3.9/site-packages (from torchmetrics) (1.12.0+cu113)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /opt/conda/lib/python3.9/site-packages (from torchmetrics) (1.21.5)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from torchmetrics) (21.3)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch>=1.3.1->torchmetrics) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging->torchmetrics) (3.0.4)\n",
      "Installing collected packages: torchmetrics\n",
      "Successfully installed torchmetrics-0.9.3\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ea8a7f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0008725115898554677\n",
      "0.0004098966061174112\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "print(statistics.mean(y))\n",
    "print(statistics.stdev(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46d15db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
