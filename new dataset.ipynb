{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "06dd4036",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "df = pd.read_csv(\"criteo_small.csv\")\n",
    "\n",
    "col_target = [\"label\"]\n",
    "col_dense = [f\"I{i}\" for i in range(1,14)]\n",
    "col_sparse = [f\"C{i}\" for i in range(1,27)]\n",
    "\n",
    "df[col_sparse] = df[col_sparse].fillna('-1', )\n",
    "df[col_dense] = df[col_dense].fillna(0,)\n",
    "\n",
    "for feat in col_sparse:\n",
    "    lbe = LabelEncoder()\n",
    "    df[feat] = lbe.fit_transform(df[feat])\n",
    "    \n",
    "mms = MinMaxScaler(feature_range=(0,1))\n",
    "df[col_dense] = mms.fit_transform(df[col_dense])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0f28f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "house_dataset = fetch_california_housing()\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    house_dataset.data,\n",
    "    columns=house_dataset.feature_names\n",
    ")\n",
    "df.loc[:,\"Price\"] = house_dataset.target\n",
    "\n",
    "col_feature = house_dataset.feature_names\n",
    "col_target = [\"Price\"]\n",
    "\n",
    "y = df[col_target]\n",
    "x = df[col_feature]\n",
    "x = x.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f78ad47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.34476576,  0.98214266,  0.62855945, ...,  1.05254828,\n",
       "        -1.32783522,  2.12963148],\n",
       "       [ 2.33223796, -0.60701891,  0.32704136, ...,  1.04318455,\n",
       "        -1.32284391,  1.31415614],\n",
       "       [ 1.7826994 ,  1.85618152,  1.15562047, ...,  1.03850269,\n",
       "        -1.33282653,  1.25869341],\n",
       "       ...,\n",
       "       [-1.14259331, -0.92485123, -0.09031802, ...,  1.77823747,\n",
       "        -0.8237132 , -0.99274649],\n",
       "       [-1.05458292, -0.84539315, -0.04021111, ...,  1.77823747,\n",
       "        -0.87362627, -1.05860847],\n",
       "       [-0.78012947, -1.00430931, -0.07044252, ...,  1.75014627,\n",
       "        -0.83369581, -1.01787803]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.344766</td>\n",
       "      <td>0.982143</td>\n",
       "      <td>0.628559</td>\n",
       "      <td>-0.153758</td>\n",
       "      <td>-0.974429</td>\n",
       "      <td>-0.049597</td>\n",
       "      <td>1.052548</td>\n",
       "      <td>-1.327835</td>\n",
       "      <td>2.129631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.332238</td>\n",
       "      <td>-0.607019</td>\n",
       "      <td>0.327041</td>\n",
       "      <td>-0.263336</td>\n",
       "      <td>0.861439</td>\n",
       "      <td>-0.092512</td>\n",
       "      <td>1.043185</td>\n",
       "      <td>-1.322844</td>\n",
       "      <td>1.314156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.782699</td>\n",
       "      <td>1.856182</td>\n",
       "      <td>1.155620</td>\n",
       "      <td>-0.049016</td>\n",
       "      <td>-0.820777</td>\n",
       "      <td>-0.025843</td>\n",
       "      <td>1.038503</td>\n",
       "      <td>-1.332827</td>\n",
       "      <td>1.258693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.932968</td>\n",
       "      <td>1.856182</td>\n",
       "      <td>0.156966</td>\n",
       "      <td>-0.049833</td>\n",
       "      <td>-0.766028</td>\n",
       "      <td>-0.050329</td>\n",
       "      <td>1.038503</td>\n",
       "      <td>-1.337818</td>\n",
       "      <td>1.165100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.012881</td>\n",
       "      <td>1.856182</td>\n",
       "      <td>0.344711</td>\n",
       "      <td>-0.032906</td>\n",
       "      <td>-0.759847</td>\n",
       "      <td>-0.085616</td>\n",
       "      <td>1.038503</td>\n",
       "      <td>-1.337818</td>\n",
       "      <td>1.172900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20635</th>\n",
       "      <td>-1.216128</td>\n",
       "      <td>-0.289187</td>\n",
       "      <td>-0.155023</td>\n",
       "      <td>0.077354</td>\n",
       "      <td>-0.512592</td>\n",
       "      <td>-0.049110</td>\n",
       "      <td>1.801647</td>\n",
       "      <td>-0.758826</td>\n",
       "      <td>-1.115804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20636</th>\n",
       "      <td>-0.691593</td>\n",
       "      <td>-0.845393</td>\n",
       "      <td>0.276881</td>\n",
       "      <td>0.462365</td>\n",
       "      <td>-0.944405</td>\n",
       "      <td>0.005021</td>\n",
       "      <td>1.806329</td>\n",
       "      <td>-0.818722</td>\n",
       "      <td>-1.124470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20637</th>\n",
       "      <td>-1.142593</td>\n",
       "      <td>-0.924851</td>\n",
       "      <td>-0.090318</td>\n",
       "      <td>0.049414</td>\n",
       "      <td>-0.369537</td>\n",
       "      <td>-0.071735</td>\n",
       "      <td>1.778237</td>\n",
       "      <td>-0.823713</td>\n",
       "      <td>-0.992746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20638</th>\n",
       "      <td>-1.054583</td>\n",
       "      <td>-0.845393</td>\n",
       "      <td>-0.040211</td>\n",
       "      <td>0.158778</td>\n",
       "      <td>-0.604429</td>\n",
       "      <td>-0.091225</td>\n",
       "      <td>1.778237</td>\n",
       "      <td>-0.873626</td>\n",
       "      <td>-1.058608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20639</th>\n",
       "      <td>-0.780129</td>\n",
       "      <td>-1.004309</td>\n",
       "      <td>-0.070443</td>\n",
       "      <td>0.138403</td>\n",
       "      <td>-0.033977</td>\n",
       "      <td>-0.043682</td>\n",
       "      <td>1.750146</td>\n",
       "      <td>-0.833696</td>\n",
       "      <td>-1.017878</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20640 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  \\\n",
       "0      2.344766  0.982143  0.628559  -0.153758   -0.974429 -0.049597   \n",
       "1      2.332238 -0.607019  0.327041  -0.263336    0.861439 -0.092512   \n",
       "2      1.782699  1.856182  1.155620  -0.049016   -0.820777 -0.025843   \n",
       "3      0.932968  1.856182  0.156966  -0.049833   -0.766028 -0.050329   \n",
       "4     -0.012881  1.856182  0.344711  -0.032906   -0.759847 -0.085616   \n",
       "...         ...       ...       ...        ...         ...       ...   \n",
       "20635 -1.216128 -0.289187 -0.155023   0.077354   -0.512592 -0.049110   \n",
       "20636 -0.691593 -0.845393  0.276881   0.462365   -0.944405  0.005021   \n",
       "20637 -1.142593 -0.924851 -0.090318   0.049414   -0.369537 -0.071735   \n",
       "20638 -1.054583 -0.845393 -0.040211   0.158778   -0.604429 -0.091225   \n",
       "20639 -0.780129 -1.004309 -0.070443   0.138403   -0.033977 -0.043682   \n",
       "\n",
       "       Latitude  Longitude     Price  \n",
       "0      1.052548  -1.327835  2.129631  \n",
       "1      1.043185  -1.322844  1.314156  \n",
       "2      1.038503  -1.332827  1.258693  \n",
       "3      1.038503  -1.337818  1.165100  \n",
       "4      1.038503  -1.337818  1.172900  \n",
       "...         ...        ...       ...  \n",
       "20635  1.801647  -0.758826 -1.115804  \n",
       "20636  1.806329  -0.818722 -1.124470  \n",
       "20637  1.778237  -0.823713 -0.992746  \n",
       "20638  1.778237  -0.873626 -1.058608  \n",
       "20639  1.750146  -0.833696 -1.017878  \n",
       "\n",
       "[20640 rows x 9 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "da3ef581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "      <td>4.526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "      <td>3.585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "      <td>3.521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20635</th>\n",
       "      <td>1.5603</td>\n",
       "      <td>25.0</td>\n",
       "      <td>5.045455</td>\n",
       "      <td>1.133333</td>\n",
       "      <td>845.0</td>\n",
       "      <td>2.560606</td>\n",
       "      <td>39.48</td>\n",
       "      <td>-121.09</td>\n",
       "      <td>0.781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20636</th>\n",
       "      <td>2.5568</td>\n",
       "      <td>18.0</td>\n",
       "      <td>6.114035</td>\n",
       "      <td>1.315789</td>\n",
       "      <td>356.0</td>\n",
       "      <td>3.122807</td>\n",
       "      <td>39.49</td>\n",
       "      <td>-121.21</td>\n",
       "      <td>0.771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20637</th>\n",
       "      <td>1.7000</td>\n",
       "      <td>17.0</td>\n",
       "      <td>5.205543</td>\n",
       "      <td>1.120092</td>\n",
       "      <td>1007.0</td>\n",
       "      <td>2.325635</td>\n",
       "      <td>39.43</td>\n",
       "      <td>-121.22</td>\n",
       "      <td>0.923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20638</th>\n",
       "      <td>1.8672</td>\n",
       "      <td>18.0</td>\n",
       "      <td>5.329513</td>\n",
       "      <td>1.171920</td>\n",
       "      <td>741.0</td>\n",
       "      <td>2.123209</td>\n",
       "      <td>39.43</td>\n",
       "      <td>-121.32</td>\n",
       "      <td>0.847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20639</th>\n",
       "      <td>2.3886</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5.254717</td>\n",
       "      <td>1.162264</td>\n",
       "      <td>1387.0</td>\n",
       "      <td>2.616981</td>\n",
       "      <td>39.37</td>\n",
       "      <td>-121.24</td>\n",
       "      <td>0.894</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20640 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0      8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
       "1      8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       "2      7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       "3      5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       "4      3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
       "...       ...       ...       ...        ...         ...       ...       ...   \n",
       "20635  1.5603      25.0  5.045455   1.133333       845.0  2.560606     39.48   \n",
       "20636  2.5568      18.0  6.114035   1.315789       356.0  3.122807     39.49   \n",
       "20637  1.7000      17.0  5.205543   1.120092      1007.0  2.325635     39.43   \n",
       "20638  1.8672      18.0  5.329513   1.171920       741.0  2.123209     39.43   \n",
       "20639  2.3886      16.0  5.254717   1.162264      1387.0  2.616981     39.37   \n",
       "\n",
       "       Longitude  Price  \n",
       "0        -122.23  4.526  \n",
       "1        -122.22  3.585  \n",
       "2        -122.24  3.521  \n",
       "3        -122.25  3.413  \n",
       "4        -122.25  3.422  \n",
       "...          ...    ...  \n",
       "20635    -121.09  0.781  \n",
       "20636    -121.21  0.771  \n",
       "20637    -121.22  0.923  \n",
       "20638    -121.32  0.847  \n",
       "20639    -121.24  0.894  \n",
       "\n",
       "[20640 rows x 9 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad319671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(635, 125)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_target = data.columns[:6]\n",
    "col_feature1 = data.columns[6:33].to_list() # 27 cols\n",
    "col_feature2 = data.columns[33:43].to_list() # 10 cols\n",
    "col_feature3 = data.columns[43:103].to_list() # 60 cols\n",
    "col_feature4 = data.columns[103:].to_list() # 28 cols\n",
    "y = data[col_target]\n",
    "x = data[col_feature1 + col_feature2 + col_feature3 + col_feature4]\n",
    "x = x.fillna(0)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2f394a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 92, 100,  78,  78,  84,  68])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_train, clean_test, train_label, test_label = train_test_split(x, y, test_size=0.2)\n",
    "train_label.to_numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e22348e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensor_point5_i_value     0\n",
      "sensor_point6_i_value     0\n",
      "sensor_point7_i_value     0\n",
      "sensor_point8_i_value     0\n",
      "sensor_point9_i_value     0\n",
      "sensor_point10_i_value    0\n",
      "dtype: int64\n",
      "sensor_point5_i_value     0\n",
      "sensor_point6_i_value     0\n",
      "sensor_point7_i_value     0\n",
      "sensor_point8_i_value     0\n",
      "sensor_point9_i_value     0\n",
      "sensor_point10_i_value    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print((y == 0).sum())\n",
    "print((y.isna()).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57f55723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      22\n",
      "238     1\n",
      "286     3\n",
      "621     1\n",
      "dtype: int64 \n",
      "\n",
      "0    10\n",
      "dtype: int64 \n",
      "\n",
      "0      20\n",
      "16      2\n",
      "18      3\n",
      "21      5\n",
      "131     5\n",
      "162    10\n",
      "164     3\n",
      "167     1\n",
      "171     1\n",
      "573    10\n",
      "dtype: int64 \n",
      "\n",
      "0      15\n",
      "27      5\n",
      "65      5\n",
      "111     1\n",
      "117     1\n",
      "579     1\n",
      "dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for col in [col_feature1,col_feature2,col_feature3,col_feature4]:\n",
    "    print((x[col]==0).sum(axis=0).value_counts().sort_index(),\"\\n\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d93b4abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    321\n",
      "2     28\n",
      "3     14\n",
      "4     62\n",
      "5    210\n",
      "dtype: int64 \n",
      "\n",
      "0    635\n",
      "dtype: int64 \n",
      "\n",
      "0      62\n",
      "10    345\n",
      "11      4\n",
      "12      3\n",
      "15     33\n",
      "20     69\n",
      "25      3\n",
      "30    109\n",
      "38      2\n",
      "40      5\n",
      "dtype: int64 \n",
      "\n",
      "0     52\n",
      "1    380\n",
      "2     86\n",
      "3     25\n",
      "6      4\n",
      "7     88\n",
      "dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for col in [col_feature1,col_feature2,col_feature3,col_feature4]:\n",
    "    print((x[col]==0).sum(axis=1).value_counts().sort_index(),\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbac891a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/258 [00:00<?, ?it/s]/home/AL_main_new/utils.py:99: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  x = torch.tensor([x for x,y in batch], dtype=torch.float32)\n",
      "Train 0 | out_loss 0.6323755383491516: 100%|█| 258/258 [00:00<00:00, 411.25it/s]\n",
      "Train 1 | out_loss 0.5671249628067017: 100%|█| 258/258 [00:00<00:00, 1004.02it/s\n",
      "Train 2 | out_loss 0.5580160021781921: 100%|█| 258/258 [00:00<00:00, 1004.73it/s\n",
      "Train 3 | out_loss 0.5522083044052124: 100%|█| 258/258 [00:00<00:00, 1010.93it/s\n",
      "Train 4 | out_loss 0.5481408834457397: 100%|█| 258/258 [00:00<00:00, 1015.23it/s\n",
      "Train 5 | out_loss 0.5422919988632202: 100%|█| 258/258 [00:00<00:00, 1001.37it/s\n",
      "Train 6 | out_loss 0.5391968488693237: 100%|█| 258/258 [00:00<00:00, 1005.05it/s\n",
      "Train 7 | out_loss 0.5350698828697205: 100%|█| 258/258 [00:00<00:00, 1016.85it/s\n",
      "Train 8 | out_loss 0.5324639678001404: 100%|█| 258/258 [00:00<00:00, 958.89it/s]\n",
      "Train 9 | out_loss 0.5281893610954285: 100%|█| 258/258 [00:00<00:00, 1004.02it/s\n",
      "Train 10 | out_loss 0.5257931351661682: 100%|█| 258/258 [00:00<00:00, 1010.67it/\n",
      "Train 11 | out_loss 0.522997260093689: 100%|█| 258/258 [00:00<00:00, 990.47it/s]\n",
      "Train 12 | out_loss 0.520327091217041: 100%|█| 258/258 [00:00<00:00, 1004.73it/s\n",
      "Train 13 | out_loss 0.5175157785415649: 100%|█| 258/258 [00:00<00:00, 1011.28it/\n",
      "Train 14 | out_loss 0.5153937339782715: 100%|█| 258/258 [00:00<00:00, 1020.84it/\n",
      "Train 15 | out_loss 0.5142051577568054: 100%|█| 258/258 [00:00<00:00, 1017.41it/\n",
      "Train 16 | out_loss 0.5109066367149353: 100%|█| 258/258 [00:00<00:00, 1027.02it/\n",
      "Train 17 | out_loss 0.5078006386756897: 100%|█| 258/258 [00:00<00:00, 1028.09it/\n",
      "Train 18 | out_loss 0.5068603157997131: 100%|█| 258/258 [00:00<00:00, 1009.02it/\n",
      "Train 19 | out_loss 0.5042381882667542: 100%|█| 258/258 [00:00<00:00, 1023.05it/\n",
      "Train 20 | out_loss 0.5029906034469604: 100%|█| 258/258 [00:00<00:00, 1022.39it/\n",
      "Train 21 | out_loss 0.5006752014160156: 100%|█| 258/258 [00:00<00:00, 1020.61it/\n",
      "Train 22 | out_loss 0.5008803009986877: 100%|█| 258/258 [00:00<00:00, 997.21it/s\n",
      "Train 23 | out_loss 0.4992542862892151: 100%|█| 258/258 [00:00<00:00, 1015.54it/\n",
      "Train 24 | out_loss 0.49723461270332336: 100%|█| 258/258 [00:00<00:00, 1019.98it\n",
      "Train 25 | out_loss 0.49567246437072754: 100%|█| 258/258 [00:00<00:00, 1020.78it\n",
      "Train 26 | out_loss 0.4946315884590149: 100%|█| 258/258 [00:00<00:00, 1023.29it/\n",
      "Train 27 | out_loss 0.49475300312042236: 100%|█| 258/258 [00:00<00:00, 1014.73it\n",
      "Train 28 | out_loss 0.49307897686958313: 100%|█| 258/258 [00:00<00:00, 1009.17it\n",
      "Train 29 | out_loss 0.4928206205368042: 100%|█| 258/258 [00:00<00:00, 1025.08it/\n",
      "Train 30 | out_loss 0.4907675087451935: 100%|█| 258/258 [00:00<00:00, 1012.33it/\n",
      "Train 31 | out_loss 0.4898958206176758: 100%|█| 258/258 [00:00<00:00, 995.60it/s\n",
      "Train 32 | out_loss 0.49073946475982666: 100%|█| 258/258 [00:00<00:00, 1013.23it\n",
      "Train 33 | out_loss 0.4894426465034485: 100%|█| 258/258 [00:00<00:00, 1011.29it/\n",
      "Train 34 | out_loss 0.48837608098983765: 100%|█| 258/258 [00:00<00:00, 1000.24it\n",
      "Train 35 | out_loss 0.4881318211555481: 100%|█| 258/258 [00:00<00:00, 1004.93it/\n",
      "Train 36 | out_loss 0.4869599938392639: 100%|█| 258/258 [00:00<00:00, 1006.46it/\n",
      "Train 37 | out_loss 0.4861539304256439: 100%|█| 258/258 [00:00<00:00, 1020.17it/\n",
      "Train 38 | out_loss 0.4859641194343567: 100%|█| 258/258 [00:00<00:00, 1012.15it/\n",
      "Train 39 | out_loss 0.4861600399017334: 100%|█| 258/258 [00:00<00:00, 1008.62it/\n",
      "Train 40 | out_loss 0.4866246283054352: 100%|█| 258/258 [00:00<00:00, 1006.22it/\n",
      "Train 41 | out_loss 0.48479971289634705: 100%|█| 258/258 [00:00<00:00, 1011.21it\n",
      "Train 42 | out_loss 0.48462042212486267: 100%|█| 258/258 [00:00<00:00, 1005.13it\n",
      "Train 43 | out_loss 0.48320114612579346: 100%|█| 258/258 [00:00<00:00, 1003.31it\n",
      "Train 44 | out_loss 0.48433032631874084: 100%|█| 258/258 [00:00<00:00, 973.22it/\n",
      "Train 45 | out_loss 0.4831724762916565: 100%|█| 258/258 [00:00<00:00, 1015.10it/\n",
      "Train 46 | out_loss 0.48357999324798584: 100%|█| 258/258 [00:00<00:00, 1008.85it\n",
      "Train 47 | out_loss 0.48165014386177063: 100%|█| 258/258 [00:00<00:00, 1021.05it\n",
      "Train 48 | out_loss 0.4813777506351471: 100%|█| 258/258 [00:00<00:00, 1021.71it/\n",
      "Train 49 | out_loss 0.48185858130455017: 100%|█| 258/258 [00:00<00:00, 985.88it/\n",
      "Train 50 | out_loss 0.4811112880706787: 100%|█| 258/258 [00:00<00:00, 1014.51it/\n",
      "Train 51 | out_loss 0.4805995523929596: 100%|█| 258/258 [00:00<00:00, 1017.83it/\n",
      "Train 52 | out_loss 0.48023495078086853: 100%|█| 258/258 [00:00<00:00, 1012.75it\n",
      "Train 53 | out_loss 0.48023521900177: 100%|██| 258/258 [00:00<00:00, 968.15it/s]\n",
      "Train 54 | out_loss 0.48054417967796326: 100%|█| 258/258 [00:00<00:00, 990.30it/\n",
      "Train 55 | out_loss 0.4799560010433197: 100%|█| 258/258 [00:00<00:00, 1003.37it/\n",
      "Train 56 | out_loss 0.48025181889533997: 100%|█| 258/258 [00:00<00:00, 999.24it/\n",
      "Train 57 | out_loss 0.4797191917896271: 100%|█| 258/258 [00:00<00:00, 968.25it/s\n",
      "Train 58 | out_loss 0.4788621664047241: 100%|█| 258/258 [00:00<00:00, 984.61it/s\n",
      "Train 59 | out_loss 0.4776220917701721: 100%|█| 258/258 [00:00<00:00, 1014.70it/\n",
      "Train 60 | out_loss 0.4782424569129944: 100%|█| 258/258 [00:00<00:00, 1000.41it/\n",
      "Train 61 | out_loss 0.4773295521736145: 100%|█| 258/258 [00:00<00:00, 990.96it/s\n",
      "Train 62 | out_loss 0.47691941261291504: 100%|█| 258/258 [00:00<00:00, 1018.86it\n",
      "Train 63 | out_loss 0.4770888686180115: 100%|█| 258/258 [00:00<00:00, 1008.73it/\n",
      "Train 64 | out_loss 0.4764072895050049: 100%|█| 258/258 [00:00<00:00, 1026.14it/\n",
      "Train 65 | out_loss 0.47589537501335144: 100%|█| 258/258 [00:00<00:00, 984.32it/\n",
      "Train 66 | out_loss 0.47462835907936096: 100%|█| 258/258 [00:00<00:00, 985.31it/\n",
      "Train 67 | out_loss 0.4759296774864197: 100%|█| 258/258 [00:00<00:00, 964.74it/s\n",
      "Train 68 | out_loss 0.4756377935409546: 100%|█| 258/258 [00:00<00:00, 990.92it/s\n",
      "Train 69 | out_loss 0.47492051124572754: 100%|█| 258/258 [00:00<00:00, 990.80it/\n",
      "Train 70 | out_loss 0.475872278213501: 100%|█| 258/258 [00:00<00:00, 999.81it/s]\n",
      "Train 71 | out_loss 0.47451868653297424: 100%|█| 258/258 [00:00<00:00, 992.58it/\n",
      "Train 72 | out_loss 0.4746500849723816: 100%|█| 258/258 [00:00<00:00, 972.19it/s\n",
      "Train 73 | out_loss 0.4744105041027069: 100%|█| 258/258 [00:00<00:00, 897.69it/s\n",
      "Train 74 | out_loss 0.4730626046657562: 100%|█| 258/258 [00:00<00:00, 969.86it/s\n",
      "Train 75 | out_loss 0.47355958819389343: 100%|█| 258/258 [00:00<00:00, 1002.00it\n",
      "Train 76 | out_loss 0.4746741056442261: 100%|█| 258/258 [00:00<00:00, 1003.86it/\n",
      "Train 77 | out_loss 0.47228458523750305: 100%|█| 258/258 [00:00<00:00, 993.93it/\n",
      "Train 78 | out_loss 0.4723304510116577: 100%|█| 258/258 [00:00<00:00, 980.44it/s\n",
      "Train 79 | out_loss 0.472404420375824: 100%|█| 258/258 [00:00<00:00, 988.00it/s]\n",
      "Train 80 | out_loss 0.47218212485313416: 100%|█| 258/258 [00:00<00:00, 1011.74it\n",
      "Train 81 | out_loss 0.47273677587509155: 100%|█| 258/258 [00:00<00:00, 1013.67it\n",
      "Train 82 | out_loss 0.47124341130256653: 100%|█| 258/258 [00:00<00:00, 1005.49it\n",
      "Train 83 | out_loss 0.4731186330318451: 100%|█| 258/258 [00:00<00:00, 1006.34it/\n",
      "Train 84 | out_loss 0.47074824571609497: 100%|█| 258/258 [00:00<00:00, 1009.26it\n",
      "Train 85 | out_loss 0.4706641733646393: 100%|█| 258/258 [00:00<00:00, 1001.66it/\n",
      "Train 86 | out_loss 0.4708195626735687: 100%|█| 258/258 [00:00<00:00, 946.23it/s\n",
      "Train 87 | out_loss 0.47177577018737793: 100%|█| 258/258 [00:00<00:00, 974.62it/\n",
      "Train 88 | out_loss 0.4688793122768402: 100%|█| 258/258 [00:00<00:00, 968.87it/s\n",
      "Train 89 | out_loss 0.4698632061481476: 100%|█| 258/258 [00:00<00:00, 970.19it/s\n",
      "Train 90 | out_loss 0.4687083661556244: 100%|█| 258/258 [00:00<00:00, 982.00it/s\n",
      "Train 91 | out_loss 0.46982452273368835: 100%|█| 258/258 [00:00<00:00, 1001.42it\n",
      "Train 92 | out_loss 0.4704270362854004: 100%|█| 258/258 [00:00<00:00, 983.69it/s\n",
      "Train 93 | out_loss 0.4687505066394806: 100%|█| 258/258 [00:00<00:00, 999.94it/s\n",
      "Train 94 | out_loss 0.4695351719856262: 100%|█| 258/258 [00:00<00:00, 1003.11it/\n",
      "Train 95 | out_loss 0.46862223744392395: 100%|█| 258/258 [00:00<00:00, 1004.48it\n",
      "Train 96 | out_loss 0.46957525610923767: 100%|█| 258/258 [00:00<00:00, 997.78it/\n",
      "Train 97 | out_loss 0.46818679571151733: 100%|█| 258/258 [00:00<00:00, 996.72it/\n",
      "Train 98 | out_loss 0.468757688999176: 100%|█| 258/258 [00:00<00:00, 1010.41it/s\n",
      "Train 99 | out_loss 0.46794429421424866: 100%|█| 258/258 [00:00<00:00, 982.03it/\n",
      "  0%|                                                   | 0/258 [00:00<?, ?it/s]/home/AL_main_new/utils.py:99: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  x = torch.tensor([x for x,y in batch], dtype=torch.float32)\n",
      "Train 0 | out_loss 0.6275599598884583: 100%|█| 258/258 [00:00<00:00, 315.85it/s]\n",
      "Train 1 | out_loss 0.5512330532073975: 100%|█| 258/258 [00:00<00:00, 581.74it/s]\n",
      "Train 2 | out_loss 0.535333514213562: 100%|██| 258/258 [00:00<00:00, 586.07it/s]\n",
      "Train 3 | out_loss 0.5277308821678162: 100%|█| 258/258 [00:00<00:00, 584.77it/s]\n",
      "Train 4 | out_loss 0.5206342339515686: 100%|█| 258/258 [00:00<00:00, 596.82it/s]\n",
      "Train 5 | out_loss 0.5170708298683167: 100%|█| 258/258 [00:00<00:00, 601.41it/s]\n",
      "Train 6 | out_loss 0.5144744515419006: 100%|█| 258/258 [00:00<00:00, 597.37it/s]\n",
      "Train 7 | out_loss 0.5098408460617065: 100%|█| 258/258 [00:00<00:00, 600.83it/s]\n",
      "Train 8 | out_loss 0.5078296065330505: 100%|█| 258/258 [00:00<00:00, 594.19it/s]\n",
      "Train 9 | out_loss 0.5035132765769958: 100%|█| 258/258 [00:00<00:00, 595.26it/s]\n",
      "Train 10 | out_loss 0.49999260902404785: 100%|█| 258/258 [00:00<00:00, 591.74it/\n",
      "Train 11 | out_loss 0.49716460704803467: 100%|█| 258/258 [00:00<00:00, 593.54it/\n",
      "Train 12 | out_loss 0.49464157223701477: 100%|█| 258/258 [00:00<00:00, 597.84it/\n",
      "Train 13 | out_loss 0.49048835039138794: 100%|█| 258/258 [00:00<00:00, 598.15it/\n",
      "Train 14 | out_loss 0.488267719745636: 100%|█| 258/258 [00:00<00:00, 594.96it/s]\n",
      "Train 15 | out_loss 0.48574531078338623: 100%|█| 258/258 [00:00<00:00, 582.28it/\n",
      "Train 16 | out_loss 0.4829632639884949: 100%|█| 258/258 [00:00<00:00, 589.70it/s\n",
      "Train 17 | out_loss 0.4799424409866333: 100%|█| 258/258 [00:00<00:00, 591.49it/s\n",
      "Train 18 | out_loss 0.4777870178222656: 100%|█| 258/258 [00:00<00:00, 592.41it/s\n",
      "Train 19 | out_loss 0.47479838132858276: 100%|█| 258/258 [00:00<00:00, 577.53it/\n",
      "Train 20 | out_loss 0.4727237820625305: 100%|█| 258/258 [00:00<00:00, 593.79it/s\n",
      "Train 21 | out_loss 0.4724618196487427: 100%|█| 258/258 [00:00<00:00, 596.31it/s\n",
      "Train 22 | out_loss 0.4702925384044647: 100%|█| 258/258 [00:00<00:00, 603.57it/s\n",
      "Train 23 | out_loss 0.4670208990573883: 100%|█| 258/258 [00:00<00:00, 605.48it/s\n",
      "Train 24 | out_loss 0.4674423635005951: 100%|█| 258/258 [00:00<00:00, 593.45it/s\n",
      "Train 25 | out_loss 0.4655991792678833: 100%|█| 258/258 [00:00<00:00, 596.82it/s\n",
      "Train 26 | out_loss 0.4656536877155304: 100%|█| 258/258 [00:00<00:00, 589.16it/s\n",
      "Train 27 | out_loss 0.46422600746154785: 100%|█| 258/258 [00:00<00:00, 599.77it/\n",
      "Train 28 | out_loss 0.46311384439468384: 100%|█| 258/258 [00:00<00:00, 604.79it/\n",
      "Train 29 | out_loss 0.46117833256721497: 100%|█| 258/258 [00:00<00:00, 590.32it/\n",
      "Train 30 | out_loss 0.46096527576446533: 100%|█| 258/258 [00:00<00:00, 593.87it/\n",
      "Train 31 | out_loss 0.4603119194507599: 100%|█| 258/258 [00:00<00:00, 599.03it/s\n",
      "Train 32 | out_loss 0.46002042293548584: 100%|█| 258/258 [00:00<00:00, 598.79it/\n",
      "Train 33 | out_loss 0.45892542600631714: 100%|█| 258/258 [00:00<00:00, 587.79it/\n",
      "Train 34 | out_loss 0.4580058753490448: 100%|█| 258/258 [00:00<00:00, 589.16it/s\n",
      "Train 35 | out_loss 0.4566047489643097: 100%|█| 258/258 [00:00<00:00, 605.93it/s\n",
      "Train 36 | out_loss 0.4568360447883606: 100%|█| 258/258 [00:00<00:00, 603.37it/s\n",
      "Train 37 | out_loss 0.45642921328544617: 100%|█| 258/258 [00:00<00:00, 599.05it/\n",
      "Train 38 | out_loss 0.45475369691848755: 100%|█| 258/258 [00:00<00:00, 595.09it/\n",
      "Train 39 | out_loss 0.4552426040172577: 100%|█| 258/258 [00:00<00:00, 596.22it/s\n",
      "Train 40 | out_loss 0.4550164043903351: 100%|█| 258/258 [00:00<00:00, 601.21it/s\n",
      "Train 41 | out_loss 0.4540100693702698: 100%|█| 258/258 [00:00<00:00, 583.74it/s\n",
      "Train 42 | out_loss 0.45306912064552307: 100%|█| 258/258 [00:00<00:00, 579.54it/\n",
      "Train 43 | out_loss 0.4527760446071625: 100%|█| 258/258 [00:00<00:00, 583.15it/s\n",
      "Train 44 | out_loss 0.45182573795318604: 100%|█| 258/258 [00:00<00:00, 598.56it/\n",
      "Train 45 | out_loss 0.45106613636016846: 100%|█| 258/258 [00:00<00:00, 602.90it/\n",
      "Train 46 | out_loss 0.45135754346847534: 100%|█| 258/258 [00:00<00:00, 594.10it/\n",
      "Train 47 | out_loss 0.44935035705566406: 100%|█| 258/258 [00:00<00:00, 601.44it/\n",
      "Train 48 | out_loss 0.4500734210014343: 100%|█| 258/258 [00:00<00:00, 595.92it/s\n",
      "Train 49 | out_loss 0.44976863265037537: 100%|█| 258/258 [00:00<00:00, 597.32it/\n",
      "Train 50 | out_loss 0.4480200409889221: 100%|█| 258/258 [00:00<00:00, 598.91it/s\n",
      "Train 51 | out_loss 0.44839024543762207: 100%|█| 258/258 [00:00<00:00, 574.59it/\n",
      "Train 52 | out_loss 0.447960764169693: 100%|█| 258/258 [00:00<00:00, 602.21it/s]\n",
      "Train 53 | out_loss 0.44688430428504944: 100%|█| 258/258 [00:00<00:00, 594.18it/\n",
      "Train 54 | out_loss 0.44559651613235474: 100%|█| 258/258 [00:00<00:00, 591.22it/\n",
      "Train 55 | out_loss 0.4459192156791687: 100%|█| 258/258 [00:00<00:00, 584.63it/s\n",
      "Train 56 | out_loss 0.4460444748401642: 100%|█| 258/258 [00:00<00:00, 590.02it/s\n",
      "Train 57 | out_loss 0.44506746530532837: 100%|█| 258/258 [00:00<00:00, 583.09it/\n",
      "Train 58 | out_loss 0.4450863003730774: 100%|█| 258/258 [00:00<00:00, 604.94it/s\n",
      "Train 59 | out_loss 0.4435666799545288: 100%|█| 258/258 [00:00<00:00, 594.05it/s\n",
      "Train 60 | out_loss 0.4436962306499481: 100%|█| 258/258 [00:00<00:00, 602.36it/s\n",
      "Train 61 | out_loss 0.4428323209285736: 100%|█| 258/258 [00:00<00:00, 562.05it/s\n",
      "Train 62 | out_loss 0.44257378578186035: 100%|█| 258/258 [00:00<00:00, 595.01it/\n",
      "Train 63 | out_loss 0.4421963691711426: 100%|█| 258/258 [00:00<00:00, 595.13it/s\n",
      "Train 64 | out_loss 0.44169870018959045: 100%|█| 258/258 [00:00<00:00, 580.36it/\n",
      "Train 65 | out_loss 0.4410724937915802: 100%|█| 258/258 [00:00<00:00, 600.40it/s\n",
      "Train 66 | out_loss 0.440467894077301: 100%|█| 258/258 [00:00<00:00, 588.78it/s]\n",
      "Train 67 | out_loss 0.4396525025367737: 100%|█| 258/258 [00:00<00:00, 585.67it/s\n",
      "Train 68 | out_loss 0.43964654207229614: 100%|█| 258/258 [00:00<00:00, 589.70it/\n",
      "Train 69 | out_loss 0.4393593966960907: 100%|█| 258/258 [00:00<00:00, 595.31it/s\n",
      "Train 70 | out_loss 0.4405492842197418: 100%|█| 258/258 [00:00<00:00, 598.40it/s\n",
      "Train 71 | out_loss 0.4396668076515198: 100%|█| 258/258 [00:00<00:00, 596.67it/s\n",
      "Train 72 | out_loss 0.4377143681049347: 100%|█| 258/258 [00:00<00:00, 590.94it/s\n",
      "Train 73 | out_loss 0.4374440014362335: 100%|█| 258/258 [00:00<00:00, 580.10it/s\n",
      "Train 74 | out_loss 0.43788251280784607: 100%|█| 258/258 [00:00<00:00, 596.82it/\n",
      "Train 75 | out_loss 0.4370313882827759: 100%|█| 258/258 [00:00<00:00, 600.31it/s\n",
      "Train 76 | out_loss 0.43680667877197266: 100%|█| 258/258 [00:00<00:00, 590.63it/\n",
      "Train 77 | out_loss 0.4363536834716797: 100%|█| 258/258 [00:00<00:00, 595.84it/s\n",
      "Train 78 | out_loss 0.43642252683639526: 100%|█| 258/258 [00:00<00:00, 593.74it/\n",
      "Train 79 | out_loss 0.43608325719833374: 100%|█| 258/258 [00:00<00:00, 598.82it/\n",
      "Train 80 | out_loss 0.4352794289588928: 100%|█| 258/258 [00:00<00:00, 590.99it/s\n",
      "Train 81 | out_loss 0.43416035175323486: 100%|█| 258/258 [00:00<00:00, 596.33it/\n",
      "Train 82 | out_loss 0.4350353479385376: 100%|█| 258/258 [00:00<00:00, 602.51it/s\n",
      "Train 83 | out_loss 0.4346635937690735: 100%|█| 258/258 [00:00<00:00, 597.46it/s\n",
      "Train 84 | out_loss 0.43483206629753113: 100%|█| 258/258 [00:00<00:00, 560.02it/\n",
      "Train 85 | out_loss 0.4332524538040161: 100%|█| 258/258 [00:00<00:00, 595.20it/s\n",
      "Train 86 | out_loss 0.4343801438808441: 100%|█| 258/258 [00:00<00:00, 593.83it/s\n",
      "Train 87 | out_loss 0.4338086247444153: 100%|█| 258/258 [00:00<00:00, 592.16it/s\n",
      "Train 88 | out_loss 0.433622270822525: 100%|█| 258/258 [00:00<00:00, 588.56it/s]\n",
      "Train 89 | out_loss 0.4321456551551819: 100%|█| 258/258 [00:00<00:00, 590.01it/s\n",
      "Train 90 | out_loss 0.4325369894504547: 100%|█| 258/258 [00:00<00:00, 598.03it/s\n",
      "Train 91 | out_loss 0.43153223395347595: 100%|█| 258/258 [00:00<00:00, 596.36it/\n",
      "Train 92 | out_loss 0.43203046917915344: 100%|█| 258/258 [00:00<00:00, 598.62it/\n",
      "Train 93 | out_loss 0.43208977580070496: 100%|█| 258/258 [00:00<00:00, 586.37it/\n",
      "Train 94 | out_loss 0.43103814125061035: 100%|█| 258/258 [00:00<00:00, 596.97it/\n",
      "Train 95 | out_loss 0.4312487542629242: 100%|█| 258/258 [00:00<00:00, 597.39it/s\n",
      "Train 96 | out_loss 0.431023508310318: 100%|█| 258/258 [00:00<00:00, 592.64it/s]\n",
      "Train 97 | out_loss 0.4303833544254303: 100%|█| 258/258 [00:00<00:00, 581.50it/s\n",
      "Train 98 | out_loss 0.43054482340812683: 100%|█| 258/258 [00:00<00:00, 583.84it/\n",
      "Train 99 | out_loss 0.4312768578529358: 100%|█| 258/258 [00:00<00:00, 578.43it/s\n",
      "  0%|                                                   | 0/258 [00:00<?, ?it/s]/home/AL_main_new/utils.py:99: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  x = torch.tensor([x for x,y in batch], dtype=torch.float32)\n",
      "Train 0 | out_loss 0.6318169236183167: 100%|█| 258/258 [00:00<00:00, 261.64it/s]\n",
      "Train 1 | out_loss 0.5474315285682678: 100%|█| 258/258 [00:00<00:00, 431.01it/s]\n",
      "Train 2 | out_loss 0.5304464101791382: 100%|█| 258/258 [00:00<00:00, 430.19it/s]\n",
      "Train 3 | out_loss 0.5217760801315308: 100%|█| 258/258 [00:00<00:00, 435.08it/s]\n",
      "Train 4 | out_loss 0.515630304813385: 100%|██| 258/258 [00:00<00:00, 430.66it/s]\n",
      "Train 5 | out_loss 0.511347770690918: 100%|██| 258/258 [00:00<00:00, 431.87it/s]\n",
      "Train 6 | out_loss 0.5081198811531067: 100%|█| 258/258 [00:00<00:00, 431.05it/s]\n",
      "Train 7 | out_loss 0.5035065412521362: 100%|█| 258/258 [00:00<00:00, 428.97it/s]\n",
      "Train 8 | out_loss 0.4996524751186371: 100%|█| 258/258 [00:00<00:00, 430.21it/s]\n",
      "Train 9 | out_loss 0.49777764081954956: 100%|█| 258/258 [00:00<00:00, 438.12it/s\n",
      "Train 10 | out_loss 0.4925631582736969: 100%|█| 258/258 [00:00<00:00, 426.78it/s\n",
      "Train 11 | out_loss 0.4895453453063965: 100%|█| 258/258 [00:00<00:00, 426.44it/s\n",
      "Train 12 | out_loss 0.48552781343460083: 100%|█| 258/258 [00:00<00:00, 437.00it/\n",
      "Train 13 | out_loss 0.4819096624851227: 100%|█| 258/258 [00:00<00:00, 428.38it/s\n",
      "Train 14 | out_loss 0.4794614911079407: 100%|█| 258/258 [00:00<00:00, 415.17it/s\n",
      "Train 15 | out_loss 0.4757622480392456: 100%|█| 258/258 [00:00<00:00, 431.59it/s\n",
      "Train 16 | out_loss 0.47264736890792847: 100%|█| 258/258 [00:00<00:00, 428.58it/\n",
      "Train 17 | out_loss 0.47223421931266785: 100%|█| 258/258 [00:00<00:00, 428.24it/\n",
      "Train 18 | out_loss 0.4692717492580414: 100%|█| 258/258 [00:00<00:00, 424.16it/s\n",
      "Train 19 | out_loss 0.4675050973892212: 100%|█| 258/258 [00:00<00:00, 429.26it/s\n",
      "Train 20 | out_loss 0.4653041362762451: 100%|█| 258/258 [00:00<00:00, 430.28it/s\n",
      "Train 21 | out_loss 0.4640762209892273: 100%|█| 258/258 [00:00<00:00, 426.25it/s\n",
      "Train 22 | out_loss 0.4621504247188568: 100%|█| 258/258 [00:00<00:00, 426.62it/s\n",
      "Train 23 | out_loss 0.4603011906147003: 100%|█| 258/258 [00:00<00:00, 426.91it/s\n",
      "Train 24 | out_loss 0.46011683344841003: 100%|█| 258/258 [00:00<00:00, 401.04it/\n",
      "Train 25 | out_loss 0.4609604477882385: 100%|█| 258/258 [00:00<00:00, 395.04it/s\n",
      "Train 26 | out_loss 0.4583775997161865: 100%|█| 258/258 [00:00<00:00, 428.77it/s\n",
      "Train 27 | out_loss 0.4573730230331421: 100%|█| 258/258 [00:00<00:00, 429.53it/s\n",
      "Train 28 | out_loss 0.4565679132938385: 100%|█| 258/258 [00:00<00:00, 413.27it/s\n",
      "Train 29 | out_loss 0.4556850492954254: 100%|█| 258/258 [00:00<00:00, 409.75it/s\n",
      "Train 30 | out_loss 0.454985648393631: 100%|█| 258/258 [00:00<00:00, 423.19it/s]\n",
      "Train 31 | out_loss 0.4530731439590454: 100%|█| 258/258 [00:00<00:00, 418.31it/s\n",
      "Train 32 | out_loss 0.4536117613315582: 100%|█| 258/258 [00:00<00:00, 424.61it/s\n",
      "Train 33 | out_loss 0.45166993141174316: 100%|█| 258/258 [00:00<00:00, 430.79it/\n",
      "Train 34 | out_loss 0.45026829838752747: 100%|█| 258/258 [00:00<00:00, 425.27it/\n",
      "Train 35 | out_loss 0.4513389766216278: 100%|█| 258/258 [00:00<00:00, 431.41it/s\n",
      "Train 36 | out_loss 0.44895419478416443: 100%|█| 258/258 [00:00<00:00, 424.80it/\n",
      "Train 37 | out_loss 0.45108070969581604: 100%|█| 258/258 [00:00<00:00, 434.21it/\n",
      "Train 38 | out_loss 0.448074072599411: 100%|█| 258/258 [00:00<00:00, 431.13it/s]\n",
      "Train 39 | out_loss 0.4473794400691986: 100%|█| 258/258 [00:00<00:00, 427.99it/s\n",
      "Train 40 | out_loss 0.4479694068431854: 100%|█| 258/258 [00:00<00:00, 426.19it/s\n",
      "Train 41 | out_loss 0.44679906964302063: 100%|█| 258/258 [00:00<00:00, 417.35it/\n",
      "Train 42 | out_loss 0.4461134672164917: 100%|█| 258/258 [00:00<00:00, 429.94it/s\n",
      "Train 43 | out_loss 0.4459080100059509: 100%|█| 258/258 [00:00<00:00, 431.69it/s\n",
      "Train 44 | out_loss 0.4443034827709198: 100%|█| 258/258 [00:00<00:00, 406.86it/s\n",
      "Train 45 | out_loss 0.4452393651008606: 100%|█| 258/258 [00:00<00:00, 409.51it/s\n",
      "Train 46 | out_loss 0.44348233938217163: 100%|█| 258/258 [00:00<00:00, 419.55it/\n",
      "Train 47 | out_loss 0.4423534572124481: 100%|█| 258/258 [00:00<00:00, 426.67it/s\n",
      "Train 48 | out_loss 0.44227567315101624: 100%|█| 258/258 [00:00<00:00, 421.75it/\n",
      "Train 49 | out_loss 0.44227269291877747: 100%|█| 258/258 [00:00<00:00, 429.74it/\n",
      "Train 50 | out_loss 0.4414309859275818: 100%|█| 258/258 [00:00<00:00, 436.27it/s\n",
      "Train 51 | out_loss 0.4408789575099945: 100%|█| 258/258 [00:00<00:00, 426.82it/s\n",
      "Train 52 | out_loss 0.44022974371910095: 100%|█| 258/258 [00:00<00:00, 424.78it/\n",
      "Train 53 | out_loss 0.43923062086105347: 100%|█| 258/258 [00:00<00:00, 419.45it/\n",
      "Train 54 | out_loss 0.43876945972442627: 100%|█| 258/258 [00:00<00:00, 428.94it/\n",
      "Train 55 | out_loss 0.43876519799232483: 100%|█| 258/258 [00:00<00:00, 427.10it/\n",
      "Train 56 | out_loss 0.43701446056365967: 100%|█| 258/258 [00:00<00:00, 416.50it/\n",
      "Train 57 | out_loss 0.4383854269981384: 100%|█| 258/258 [00:00<00:00, 421.94it/s\n",
      "Train 58 | out_loss 0.43736499547958374: 100%|█| 258/258 [00:00<00:00, 419.60it/\n",
      "Train 59 | out_loss 0.43597906827926636: 100%|█| 258/258 [00:00<00:00, 424.89it/\n",
      "Train 60 | out_loss 0.43439844250679016: 100%|█| 258/258 [00:00<00:00, 429.39it/\n",
      "Train 61 | out_loss 0.4332004189491272: 100%|█| 258/258 [00:00<00:00, 424.71it/s\n",
      "Train 62 | out_loss 0.4343337118625641: 100%|█| 258/258 [00:00<00:00, 424.91it/s\n",
      "Train 63 | out_loss 0.43359676003456116: 100%|█| 258/258 [00:00<00:00, 427.96it/\n",
      "Train 64 | out_loss 0.43309012055397034: 100%|█| 258/258 [00:00<00:00, 429.59it/\n",
      "Train 65 | out_loss 0.43303385376930237: 100%|█| 258/258 [00:00<00:00, 421.23it/\n",
      "Train 66 | out_loss 0.43245673179626465: 100%|█| 258/258 [00:00<00:00, 423.63it/\n",
      "Train 67 | out_loss 0.4320775866508484: 100%|█| 258/258 [00:00<00:00, 421.93it/s\n",
      "Train 68 | out_loss 0.43064671754837036: 100%|█| 258/258 [00:00<00:00, 430.25it/\n",
      "Train 69 | out_loss 0.4304329454898834: 100%|█| 258/258 [00:00<00:00, 418.93it/s\n",
      "Train 70 | out_loss 0.42939451336860657: 100%|█| 258/258 [00:00<00:00, 427.18it/\n",
      "Train 71 | out_loss 0.42997023463249207: 100%|█| 258/258 [00:00<00:00, 416.30it/\n",
      "Train 72 | out_loss 0.42849189043045044: 100%|█| 258/258 [00:00<00:00, 428.24it/\n",
      "Train 73 | out_loss 0.42851489782333374: 100%|█| 258/258 [00:00<00:00, 419.63it/\n",
      "Train 74 | out_loss 0.4267558455467224: 100%|█| 258/258 [00:00<00:00, 424.85it/s\n",
      "Train 75 | out_loss 0.42725667357444763: 100%|█| 258/258 [00:00<00:00, 421.49it/\n",
      "Train 76 | out_loss 0.4269702732563019: 100%|█| 258/258 [00:00<00:00, 421.80it/s\n",
      "Train 77 | out_loss 0.42754054069519043: 100%|█| 258/258 [00:00<00:00, 420.33it/\n",
      "Train 78 | out_loss 0.4259825348854065: 100%|█| 258/258 [00:00<00:00, 424.53it/s\n",
      "Train 79 | out_loss 0.4249458611011505: 100%|█| 258/258 [00:00<00:00, 425.34it/s\n",
      "Train 80 | out_loss 0.42524316906929016: 100%|█| 258/258 [00:00<00:00, 424.41it/\n",
      "Train 81 | out_loss 0.4250624477863312: 100%|█| 258/258 [00:00<00:00, 421.13it/s\n",
      "Train 82 | out_loss 0.4237958788871765: 100%|█| 258/258 [00:00<00:00, 416.68it/s\n",
      "Train 83 | out_loss 0.4244483411312103: 100%|█| 258/258 [00:00<00:00, 424.59it/s\n",
      "Train 84 | out_loss 0.4234519302845001: 100%|█| 258/258 [00:00<00:00, 402.61it/s\n",
      "Train 85 | out_loss 0.42292335629463196: 100%|█| 258/258 [00:00<00:00, 416.59it/\n",
      "Train 86 | out_loss 0.4235341548919678: 100%|█| 258/258 [00:00<00:00, 420.21it/s\n",
      "Train 87 | out_loss 0.4223591387271881: 100%|█| 258/258 [00:00<00:00, 418.22it/s\n",
      "Train 88 | out_loss 0.42240941524505615: 100%|█| 258/258 [00:00<00:00, 428.03it/\n",
      "Train 89 | out_loss 0.4219801127910614: 100%|█| 258/258 [00:00<00:00, 420.80it/s\n",
      "Train 90 | out_loss 0.4203043282032013: 100%|█| 258/258 [00:00<00:00, 429.07it/s\n",
      "Train 91 | out_loss 0.42207157611846924: 100%|█| 258/258 [00:00<00:00, 426.97it/\n",
      "Train 92 | out_loss 0.42208990454673767: 100%|█| 258/258 [00:00<00:00, 410.07it/\n",
      "Train 93 | out_loss 0.42040175199508667: 100%|█| 258/258 [00:00<00:00, 422.44it/\n",
      "Train 94 | out_loss 0.41990798711776733: 100%|█| 258/258 [00:00<00:00, 416.59it/\n",
      "Train 95 | out_loss 0.42012423276901245: 100%|█| 258/258 [00:00<00:00, 420.86it/\n",
      "Train 96 | out_loss 0.41977840662002563: 100%|█| 258/258 [00:00<00:00, 424.08it/\n",
      "Train 97 | out_loss 0.4194245934486389: 100%|█| 258/258 [00:00<00:00, 398.46it/s\n",
      "Train 98 | out_loss 0.41812634468078613: 100%|█| 258/258 [00:00<00:00, 411.09it/\n",
      "Train 99 | out_loss 0.41860729455947876: 100%|█| 258/258 [00:00<00:00, 417.78it/\n",
      "  0%|                                                   | 0/258 [00:00<?, ?it/s]/home/AL_main_new/utils.py:99: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  x = torch.tensor([x for x,y in batch], dtype=torch.float32)\n",
      "Train 0 | out_loss 0.6385809183120728: 100%|█| 258/258 [00:01<00:00, 219.57it/s]\n",
      "Train 1 | out_loss 0.5417067408561707: 100%|█| 258/258 [00:00<00:00, 325.68it/s]\n",
      "Train 2 | out_loss 0.5260849595069885: 100%|█| 258/258 [00:00<00:00, 320.24it/s]\n",
      "Train 3 | out_loss 0.5181676149368286: 100%|█| 258/258 [00:00<00:00, 321.30it/s]\n",
      "Train 4 | out_loss 0.5121442079544067: 100%|█| 258/258 [00:00<00:00, 322.04it/s]\n",
      "Train 5 | out_loss 0.508991539478302: 100%|██| 258/258 [00:00<00:00, 328.61it/s]\n",
      "Train 6 | out_loss 0.5029098987579346: 100%|█| 258/258 [00:00<00:00, 330.54it/s]\n",
      "Train 7 | out_loss 0.5015316605567932: 100%|█| 258/258 [00:00<00:00, 337.09it/s]\n",
      "Train 8 | out_loss 0.49964991211891174: 100%|█| 258/258 [00:00<00:00, 342.10it/s\n",
      "Train 9 | out_loss 0.49514588713645935: 100%|█| 258/258 [00:00<00:00, 332.87it/s\n",
      "Train 10 | out_loss 0.49153825640678406: 100%|█| 258/258 [00:00<00:00, 336.48it/\n",
      "Train 11 | out_loss 0.48797208070755005: 100%|█| 258/258 [00:00<00:00, 331.92it/\n",
      "Train 12 | out_loss 0.4854765236377716: 100%|█| 258/258 [00:00<00:00, 337.73it/s\n",
      "Train 13 | out_loss 0.4816850423812866: 100%|█| 258/258 [00:00<00:00, 335.35it/s\n",
      "Train 14 | out_loss 0.4798663258552551: 100%|█| 258/258 [00:00<00:00, 331.63it/s\n",
      "Train 15 | out_loss 0.4779443144798279: 100%|█| 258/258 [00:00<00:00, 337.13it/s\n",
      "Train 16 | out_loss 0.47397178411483765: 100%|█| 258/258 [00:00<00:00, 337.32it/\n",
      "Train 17 | out_loss 0.47181418538093567: 100%|█| 258/258 [00:00<00:00, 334.71it/\n",
      "Train 18 | out_loss 0.4671665132045746: 100%|█| 258/258 [00:00<00:00, 331.75it/s\n",
      "Train 19 | out_loss 0.46450236439704895: 100%|█| 258/258 [00:00<00:00, 333.76it/\n",
      "Train 20 | out_loss 0.4629223048686981: 100%|█| 258/258 [00:00<00:00, 334.07it/s\n",
      "Train 21 | out_loss 0.45936062932014465: 100%|█| 258/258 [00:00<00:00, 332.50it/\n",
      "Train 22 | out_loss 0.45889613032341003: 100%|█| 258/258 [00:00<00:00, 333.43it/\n",
      "Train 23 | out_loss 0.4568251669406891: 100%|█| 258/258 [00:00<00:00, 336.76it/s\n",
      "Train 24 | out_loss 0.45538681745529175: 100%|█| 258/258 [00:00<00:00, 330.12it/\n",
      "Train 25 | out_loss 0.45307278633117676: 100%|█| 258/258 [00:00<00:00, 329.16it/\n",
      "Train 26 | out_loss 0.45276471972465515: 100%|█| 258/258 [00:00<00:00, 333.32it/\n",
      "Train 27 | out_loss 0.45063477754592896: 100%|█| 258/258 [00:00<00:00, 328.32it/\n",
      "Train 28 | out_loss 0.45014575123786926: 100%|█| 258/258 [00:00<00:00, 334.88it/\n",
      "Train 29 | out_loss 0.4483787715435028: 100%|█| 258/258 [00:00<00:00, 335.92it/s\n",
      "Train 30 | out_loss 0.44786444306373596: 100%|█| 258/258 [00:00<00:00, 316.06it/\n",
      "Train 31 | out_loss 0.4465978741645813: 100%|█| 258/258 [00:00<00:00, 336.56it/s\n",
      "Train 32 | out_loss 0.4444253444671631: 100%|█| 258/258 [00:00<00:00, 339.91it/s\n",
      "Train 33 | out_loss 0.4426279664039612: 100%|█| 258/258 [00:00<00:00, 334.98it/s\n",
      "Train 34 | out_loss 0.4443527162075043: 100%|█| 258/258 [00:00<00:00, 329.81it/s\n",
      "Train 35 | out_loss 0.44241681694984436: 100%|█| 258/258 [00:00<00:00, 330.16it/\n",
      "Train 36 | out_loss 0.44231104850769043: 100%|█| 258/258 [00:00<00:00, 336.29it/\n",
      "Train 37 | out_loss 0.4405880570411682: 100%|█| 258/258 [00:00<00:00, 315.85it/s\n",
      "Train 38 | out_loss 0.44036564230918884: 100%|█| 258/258 [00:00<00:00, 332.39it/\n",
      "Train 39 | out_loss 0.4393245577812195: 100%|█| 258/258 [00:00<00:00, 338.37it/s\n",
      "Train 40 | out_loss 0.4391660690307617: 100%|█| 258/258 [00:00<00:00, 338.03it/s\n",
      "Train 41 | out_loss 0.43771520256996155: 100%|█| 258/258 [00:00<00:00, 333.96it/\n",
      "Train 42 | out_loss 0.4368327558040619: 100%|█| 258/258 [00:00<00:00, 330.51it/s\n",
      "Train 43 | out_loss 0.43706458806991577: 100%|█| 258/258 [00:00<00:00, 333.26it/\n",
      "Train 44 | out_loss 0.4357461929321289: 100%|█| 258/258 [00:00<00:00, 338.47it/s\n",
      "Train 45 | out_loss 0.4363507032394409: 100%|█| 258/258 [00:00<00:00, 330.61it/s\n",
      "Train 46 | out_loss 0.4353129267692566: 100%|█| 258/258 [00:00<00:00, 318.54it/s\n",
      "Train 47 | out_loss 0.4358515441417694: 100%|█| 258/258 [00:00<00:00, 326.64it/s\n",
      "Train 48 | out_loss 0.4349795877933502: 100%|█| 258/258 [00:00<00:00, 310.20it/s\n",
      "Train 49 | out_loss 0.43450355529785156: 100%|█| 258/258 [00:00<00:00, 332.45it/\n",
      "Train 50 | out_loss 0.4342401921749115: 100%|█| 258/258 [00:00<00:00, 337.67it/s\n",
      "Train 51 | out_loss 0.43361636996269226: 100%|█| 258/258 [00:00<00:00, 332.78it/\n",
      "Train 52 | out_loss 0.4335751533508301: 100%|█| 258/258 [00:00<00:00, 331.74it/s\n",
      "Train 53 | out_loss 0.43224307894706726: 100%|█| 258/258 [00:00<00:00, 338.77it/\n",
      "Train 54 | out_loss 0.43335625529289246: 100%|█| 258/258 [00:00<00:00, 327.77it/\n",
      "Train 55 | out_loss 0.4307321012020111: 100%|█| 258/258 [00:00<00:00, 332.44it/s\n",
      "Train 56 | out_loss 0.4307767152786255: 100%|█| 258/258 [00:00<00:00, 329.03it/s\n",
      "Train 57 | out_loss 0.43091216683387756: 100%|█| 258/258 [00:00<00:00, 330.68it/\n",
      "Train 58 | out_loss 0.4304094910621643: 100%|█| 258/258 [00:00<00:00, 328.14it/s\n",
      "Train 59 | out_loss 0.4308546781539917: 100%|█| 258/258 [00:00<00:00, 320.12it/s\n",
      "Train 60 | out_loss 0.4298900067806244: 100%|█| 258/258 [00:00<00:00, 329.04it/s\n",
      "Train 61 | out_loss 0.4282170832157135: 100%|█| 258/258 [00:00<00:00, 325.55it/s\n",
      "Train 62 | out_loss 0.42909908294677734: 100%|█| 258/258 [00:00<00:00, 326.26it/\n",
      "Train 63 | out_loss 0.4287137985229492: 100%|█| 258/258 [00:00<00:00, 334.85it/s\n",
      "Train 64 | out_loss 0.4284451901912689: 100%|█| 258/258 [00:00<00:00, 337.93it/s\n",
      "Train 65 | out_loss 0.42860516905784607: 100%|█| 258/258 [00:00<00:00, 327.26it/\n",
      "Train 66 | out_loss 0.4263443350791931: 100%|█| 258/258 [00:00<00:00, 327.25it/s\n",
      "Train 67 | out_loss 0.42683330178260803: 100%|█| 258/258 [00:00<00:00, 331.28it/\n",
      "Train 68 | out_loss 0.42678070068359375: 100%|█| 258/258 [00:00<00:00, 320.50it/\n",
      "Train 69 | out_loss 0.4264558255672455: 100%|█| 258/258 [00:00<00:00, 325.55it/s\n",
      "Train 70 | out_loss 0.42647936940193176: 100%|█| 258/258 [00:00<00:00, 329.44it/\n",
      "Train 71 | out_loss 0.42572250962257385: 100%|█| 258/258 [00:00<00:00, 324.88it/\n",
      "Train 72 | out_loss 0.4275859594345093: 100%|█| 258/258 [00:00<00:00, 329.39it/s\n",
      "Train 73 | out_loss 0.42563939094543457: 100%|█| 258/258 [00:00<00:00, 321.62it/\n",
      "Train 74 | out_loss 0.4250697195529938: 100%|█| 258/258 [00:00<00:00, 323.89it/s\n",
      "Train 75 | out_loss 0.42434945702552795: 100%|█| 258/258 [00:00<00:00, 339.76it/\n",
      "Train 76 | out_loss 0.42412394285202026: 100%|█| 258/258 [00:00<00:00, 332.85it/\n",
      "Train 77 | out_loss 0.42461857199668884: 100%|█| 258/258 [00:00<00:00, 327.49it/\n",
      "Train 78 | out_loss 0.42384785413742065: 100%|█| 258/258 [00:00<00:00, 329.81it/\n",
      "Train 79 | out_loss 0.42311909794807434: 100%|█| 258/258 [00:00<00:00, 329.20it/\n",
      "Train 80 | out_loss 0.42363253235816956: 100%|█| 258/258 [00:00<00:00, 328.14it/\n",
      "Train 81 | out_loss 0.4229939877986908: 100%|█| 258/258 [00:00<00:00, 326.04it/s\n",
      "Train 82 | out_loss 0.422554612159729: 100%|█| 258/258 [00:00<00:00, 337.17it/s]\n",
      "Train 83 | out_loss 0.4232890009880066: 100%|█| 258/258 [00:00<00:00, 335.41it/s\n",
      "Train 84 | out_loss 0.4215674102306366: 100%|█| 258/258 [00:00<00:00, 329.25it/s\n",
      "Train 85 | out_loss 0.42192620038986206: 100%|█| 258/258 [00:00<00:00, 332.79it/\n",
      "Train 86 | out_loss 0.42159304022789: 100%|██| 258/258 [00:00<00:00, 327.94it/s]\n",
      "Train 87 | out_loss 0.42192861437797546: 100%|█| 258/258 [00:00<00:00, 331.66it/\n",
      "Train 88 | out_loss 0.4207085967063904: 100%|█| 258/258 [00:00<00:00, 331.71it/s\n",
      "Train 89 | out_loss 0.4220186471939087: 100%|█| 258/258 [00:00<00:00, 335.58it/s\n",
      "Train 90 | out_loss 0.4196174144744873: 100%|█| 258/258 [00:00<00:00, 325.03it/s\n",
      "Train 91 | out_loss 0.42106863856315613: 100%|█| 258/258 [00:00<00:00, 315.57it/\n",
      "Train 92 | out_loss 0.4188750684261322: 100%|█| 258/258 [00:00<00:00, 327.15it/s\n",
      "Train 93 | out_loss 0.4201149642467499: 100%|█| 258/258 [00:00<00:00, 333.69it/s\n",
      "Train 94 | out_loss 0.4198065400123596: 100%|█| 258/258 [00:00<00:00, 338.88it/s\n",
      "Train 95 | out_loss 0.4191477298736572: 100%|█| 258/258 [00:00<00:00, 331.14it/s\n",
      "Train 96 | out_loss 0.4195554852485657: 100%|█| 258/258 [00:00<00:00, 325.32it/s\n",
      "Train 97 | out_loss 0.41720128059387207: 100%|█| 258/258 [00:00<00:00, 327.26it/\n",
      "Train 98 | out_loss 0.41827937960624695: 100%|█| 258/258 [00:00<00:00, 335.45it/\n",
      "Train 99 | out_loss 0.4195212423801422: 100%|█| 258/258 [00:00<00:00, 332.74it/s\n",
      "  0%|                                                   | 0/258 [00:00<?, ?it/s]/home/AL_main_new/utils.py:99: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  x = torch.tensor([x for x,y in batch], dtype=torch.float32)\n",
      "Train 0 | out_loss 0.6502485871315002: 100%|█| 258/258 [00:01<00:00, 196.16it/s]\n",
      "Train 1 | out_loss 0.5527934432029724: 100%|█| 258/258 [00:00<00:00, 275.35it/s]\n",
      "Train 2 | out_loss 0.5358134508132935: 100%|█| 258/258 [00:00<00:00, 277.77it/s]\n",
      "Train 3 | out_loss 0.5309338569641113: 100%|█| 258/258 [00:00<00:00, 278.15it/s]\n",
      "Train 4 | out_loss 0.5241693258285522: 100%|█| 258/258 [00:00<00:00, 269.22it/s]\n",
      "Train 5 | out_loss 0.5166015625: 100%|███████| 258/258 [00:00<00:00, 279.08it/s]\n",
      "Train 6 | out_loss 0.5103679895401001: 100%|█| 258/258 [00:00<00:00, 274.20it/s]\n",
      "Train 7 | out_loss 0.5095618963241577: 100%|█| 258/258 [00:00<00:00, 277.02it/s]\n",
      "Train 8 | out_loss 0.504879355430603: 100%|██| 258/258 [00:00<00:00, 281.94it/s]\n",
      "Train 9 | out_loss 0.5016789436340332: 100%|█| 258/258 [00:00<00:00, 276.42it/s]\n",
      "Train 10 | out_loss 0.49806931614875793: 100%|█| 258/258 [00:00<00:00, 273.35it/\n",
      "Train 11 | out_loss 0.49757757782936096: 100%|█| 258/258 [00:00<00:00, 279.43it/\n",
      "Train 12 | out_loss 0.491934210062027: 100%|█| 258/258 [00:00<00:00, 275.11it/s]\n",
      "Train 13 | out_loss 0.49284106492996216: 100%|█| 258/258 [00:00<00:00, 263.04it/\n",
      "Train 14 | out_loss 0.4868561029434204: 100%|█| 258/258 [00:00<00:00, 279.10it/s\n",
      "Train 15 | out_loss 0.48503702878952026: 100%|█| 258/258 [00:00<00:00, 274.40it/\n",
      "Train 16 | out_loss 0.48399630188941956: 100%|█| 258/258 [00:00<00:00, 281.07it/\n",
      "Train 17 | out_loss 0.47952765226364136: 100%|█| 258/258 [00:00<00:00, 280.35it/\n",
      "Train 18 | out_loss 0.4780634641647339: 100%|█| 258/258 [00:00<00:00, 272.40it/s\n",
      "Train 19 | out_loss 0.47404804825782776: 100%|█| 258/258 [00:00<00:00, 277.52it/\n",
      "Train 20 | out_loss 0.47143691778182983: 100%|█| 258/258 [00:00<00:00, 273.23it/\n",
      "Train 21 | out_loss 0.469360888004303: 100%|█| 258/258 [00:00<00:00, 279.42it/s]\n",
      "Train 22 | out_loss 0.47245675325393677: 100%|█| 258/258 [00:00<00:00, 276.55it/\n",
      "Train 23 | out_loss 0.4708907902240753: 100%|█| 258/258 [00:00<00:00, 277.01it/s\n",
      "Train 24 | out_loss 0.46887287497520447: 100%|█| 258/258 [00:00<00:00, 265.47it/\n",
      "Train 25 | out_loss 0.46664202213287354: 100%|█| 258/258 [00:00<00:00, 275.23it/\n",
      "Train 26 | out_loss 0.4653250575065613: 100%|█| 258/258 [00:00<00:00, 279.27it/s\n",
      "Train 27 | out_loss 0.4634581208229065: 100%|█| 258/258 [00:00<00:00, 274.01it/s\n",
      "Train 28 | out_loss 0.461628258228302: 100%|█| 258/258 [00:00<00:00, 272.66it/s]\n",
      "Train 29 | out_loss 0.46053555607795715: 100%|█| 258/258 [00:00<00:00, 273.73it/\n",
      "Train 30 | out_loss 0.4614582061767578: 100%|█| 258/258 [00:00<00:00, 278.53it/s\n",
      "Train 31 | out_loss 0.4592892527580261: 100%|█| 258/258 [00:00<00:00, 273.79it/s\n",
      "Train 32 | out_loss 0.4594726860523224: 100%|█| 258/258 [00:00<00:00, 271.99it/s\n",
      "Train 33 | out_loss 0.4580020010471344: 100%|█| 258/258 [00:00<00:00, 272.94it/s\n",
      "Train 34 | out_loss 0.4579490125179291: 100%|█| 258/258 [00:00<00:00, 277.15it/s\n",
      "Train 35 | out_loss 0.4565613567829132: 100%|█| 258/258 [00:00<00:00, 266.18it/s\n",
      "Train 36 | out_loss 0.4575866162776947: 100%|█| 258/258 [00:00<00:00, 276.80it/s\n",
      "Train 37 | out_loss 0.45562317967414856: 100%|█| 258/258 [00:00<00:00, 278.31it/\n",
      "Train 38 | out_loss 0.4541366994380951: 100%|█| 258/258 [00:00<00:00, 278.42it/s\n",
      "Train 39 | out_loss 0.45346495509147644: 100%|█| 258/258 [00:00<00:00, 276.24it/\n",
      "Train 40 | out_loss 0.4533674120903015: 100%|█| 258/258 [00:00<00:00, 275.31it/s\n",
      "Train 41 | out_loss 0.4518570899963379: 100%|█| 258/258 [00:00<00:00, 275.55it/s\n",
      "Train 42 | out_loss 0.4517083466053009: 100%|█| 258/258 [00:00<00:00, 275.16it/s\n",
      "Train 43 | out_loss 0.4504498839378357: 100%|█| 258/258 [00:00<00:00, 271.11it/s\n",
      "Train 44 | out_loss 0.4495987594127655: 100%|█| 258/258 [00:00<00:00, 270.48it/s\n",
      "Train 45 | out_loss 0.449896901845932: 100%|█| 258/258 [00:00<00:00, 273.14it/s]\n",
      "Train 46 | out_loss 0.4501153230667114: 100%|█| 258/258 [00:00<00:00, 270.57it/s\n",
      "Train 47 | out_loss 0.44836631417274475: 100%|█| 258/258 [00:00<00:00, 272.81it/\n",
      "Train 48 | out_loss 0.4470278024673462: 100%|█| 258/258 [00:00<00:00, 276.06it/s\n",
      "Train 49 | out_loss 0.44707250595092773: 100%|█| 258/258 [00:00<00:00, 274.00it/\n",
      "Train 50 | out_loss 0.4459163248538971: 100%|█| 258/258 [00:00<00:00, 275.08it/s\n",
      "Train 51 | out_loss 0.445436030626297: 100%|█| 258/258 [00:00<00:00, 277.58it/s]\n",
      "Train 52 | out_loss 0.44483280181884766: 100%|█| 258/258 [00:00<00:00, 275.47it/\n",
      "Train 53 | out_loss 0.44362136721611023: 100%|█| 258/258 [00:00<00:00, 264.36it/\n",
      "Train 54 | out_loss 0.4436626732349396: 100%|█| 258/258 [00:00<00:00, 278.39it/s\n",
      "Train 55 | out_loss 0.4432641267776489: 100%|█| 258/258 [00:00<00:00, 277.57it/s\n",
      "Train 56 | out_loss 0.4421730935573578: 100%|█| 258/258 [00:00<00:00, 271.75it/s\n",
      "Train 57 | out_loss 0.44299453496932983: 100%|█| 258/258 [00:00<00:00, 277.57it/\n",
      "Train 58 | out_loss 0.44235581159591675: 100%|█| 258/258 [00:00<00:00, 276.55it/\n",
      "Train 59 | out_loss 0.4411044418811798: 100%|█| 258/258 [00:00<00:00, 275.73it/s\n",
      "Train 60 | out_loss 0.4422723948955536: 100%|█| 258/258 [00:00<00:00, 272.88it/s\n",
      "Train 61 | out_loss 0.44060733914375305: 100%|█| 258/258 [00:00<00:00, 274.22it/\n",
      "Train 62 | out_loss 0.4411696493625641: 100%|█| 258/258 [00:00<00:00, 270.80it/s\n",
      "Train 63 | out_loss 0.4407613277435303: 100%|█| 258/258 [00:00<00:00, 277.25it/s\n",
      "Train 64 | out_loss 0.4391864240169525: 100%|█| 258/258 [00:00<00:00, 272.34it/s\n",
      "Train 65 | out_loss 0.4398688077926636: 100%|█| 258/258 [00:00<00:00, 274.66it/s\n",
      "Train 66 | out_loss 0.4395870566368103: 100%|█| 258/258 [00:00<00:00, 276.41it/s\n",
      "Train 67 | out_loss 0.4387909173965454: 100%|█| 258/258 [00:00<00:00, 273.31it/s\n",
      "Train 68 | out_loss 0.4377896189689636: 100%|█| 258/258 [00:00<00:00, 274.94it/s\n",
      "Train 69 | out_loss 0.43606749176979065: 100%|█| 258/258 [00:00<00:00, 276.67it/\n",
      "Train 70 | out_loss 0.43558648228645325: 100%|█| 258/258 [00:00<00:00, 275.48it/\n",
      "Train 71 | out_loss 0.4421626329421997: 100%|█| 258/258 [00:00<00:00, 271.21it/s\n",
      "Train 72 | out_loss 0.43885165452957153: 100%|█| 258/258 [00:00<00:00, 278.31it/\n",
      "Train 73 | out_loss 0.4357251226902008: 100%|█| 258/258 [00:00<00:00, 270.88it/s\n",
      "Train 74 | out_loss 0.43539193272590637: 100%|█| 258/258 [00:00<00:00, 275.37it/\n",
      "Train 75 | out_loss 0.43495893478393555: 100%|█| 258/258 [00:00<00:00, 270.01it/\n",
      "Train 76 | out_loss 0.43596428632736206: 100%|█| 258/258 [00:00<00:00, 265.43it/\n",
      "Train 77 | out_loss 0.4332400858402252: 100%|█| 258/258 [00:00<00:00, 273.99it/s\n",
      "Train 78 | out_loss 0.4326723515987396: 100%|█| 258/258 [00:00<00:00, 275.00it/s\n",
      "Train 79 | out_loss 0.4325426518917084: 100%|█| 258/258 [00:00<00:00, 276.42it/s\n",
      "Train 80 | out_loss 0.4326311945915222: 100%|█| 258/258 [00:00<00:00, 280.73it/s\n",
      "Train 81 | out_loss 0.4329737424850464: 100%|█| 258/258 [00:00<00:00, 265.79it/s\n",
      "Train 82 | out_loss 0.43248867988586426: 100%|█| 258/258 [00:00<00:00, 274.61it/\n",
      "Train 83 | out_loss 0.4319140911102295: 100%|█| 258/258 [00:00<00:00, 273.65it/s\n",
      "Train 84 | out_loss 0.43308547139167786: 100%|█| 258/258 [00:00<00:00, 275.28it/\n",
      "Train 85 | out_loss 0.4327410161495209: 100%|█| 258/258 [00:00<00:00, 277.78it/s\n",
      "Train 86 | out_loss 0.4313005805015564: 100%|█| 258/258 [00:00<00:00, 276.37it/s\n",
      "Train 87 | out_loss 0.43282976746559143: 100%|█| 258/258 [00:00<00:00, 270.70it/\n",
      "Train 88 | out_loss 0.4309953451156616: 100%|█| 258/258 [00:00<00:00, 260.41it/s\n",
      "Train 89 | out_loss 0.4314306080341339: 100%|█| 258/258 [00:00<00:00, 274.11it/s\n",
      "Train 90 | out_loss 0.42935892939567566: 100%|█| 258/258 [00:00<00:00, 263.80it/\n",
      "Train 91 | out_loss 0.4293120801448822: 100%|█| 258/258 [00:00<00:00, 273.04it/s\n",
      "Train 92 | out_loss 0.4285550117492676: 100%|█| 258/258 [00:00<00:00, 273.97it/s\n",
      "Train 93 | out_loss 0.42953377962112427: 100%|█| 258/258 [00:00<00:00, 274.08it/\n",
      "Train 94 | out_loss 0.4280777871608734: 100%|█| 258/258 [00:00<00:00, 271.40it/s\n",
      "Train 95 | out_loss 0.42839691042900085: 100%|█| 258/258 [00:00<00:00, 274.18it/\n",
      "Train 96 | out_loss 0.428326815366745: 100%|█| 258/258 [00:00<00:00, 270.20it/s]\n",
      "Train 97 | out_loss 0.4269871711730957: 100%|█| 258/258 [00:00<00:00, 273.01it/s\n",
      "Train 98 | out_loss 0.42785972356796265: 100%|█| 258/258 [00:00<00:00, 268.19it/\n",
      "Train 99 | out_loss 0.4274269938468933: 100%|█| 258/258 [00:00<00:00, 269.01it/s\n",
      "  0%|                                                   | 0/258 [00:00<?, ?it/s]/home/AL_main_new/utils.py:99: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  x = torch.tensor([x for x,y in batch], dtype=torch.float32)\n",
      "Train 0 | out_loss 0.6609001755714417: 100%|█| 258/258 [00:01<00:00, 174.45it/s]\n",
      "Train 1 | out_loss 0.563581645488739: 100%|██| 258/258 [00:01<00:00, 232.78it/s]\n",
      "Train 2 | out_loss 0.5479578375816345: 100%|█| 258/258 [00:01<00:00, 236.45it/s]\n",
      "Train 3 | out_loss 0.541900634765625: 100%|██| 258/258 [00:01<00:00, 232.07it/s]\n",
      "Train 4 | out_loss 0.5341299772262573: 100%|█| 258/258 [00:01<00:00, 238.34it/s]\n",
      "Train 5 | out_loss 0.5237227082252502: 100%|█| 258/258 [00:01<00:00, 232.44it/s]\n",
      "Train 6 | out_loss 0.5191580653190613: 100%|█| 258/258 [00:01<00:00, 233.17it/s]\n",
      "Train 7 | out_loss 0.518815815448761: 100%|██| 258/258 [00:01<00:00, 235.45it/s]\n",
      "Train 8 | out_loss 0.5146114230155945: 100%|█| 258/258 [00:01<00:00, 233.79it/s]\n",
      "Train 9 | out_loss 0.5115974545478821: 100%|█| 258/258 [00:01<00:00, 233.14it/s]\n",
      "Train 10 | out_loss 0.5183370113372803: 100%|█| 258/258 [00:01<00:00, 235.62it/s\n",
      "Train 11 | out_loss 0.5084018111228943: 100%|█| 258/258 [00:01<00:00, 232.75it/s\n",
      "Train 12 | out_loss 0.5068238377571106: 100%|█| 258/258 [00:01<00:00, 233.89it/s\n",
      "Train 13 | out_loss 0.5054025053977966: 100%|█| 258/258 [00:01<00:00, 232.79it/s\n",
      "Train 14 | out_loss 0.49900856614112854: 100%|█| 258/258 [00:01<00:00, 236.25it/\n",
      "Train 15 | out_loss 0.4925347566604614: 100%|█| 258/258 [00:01<00:00, 236.31it/s\n",
      "Train 16 | out_loss 0.49153462052345276: 100%|█| 258/258 [00:01<00:00, 232.99it/\n",
      "Train 17 | out_loss 0.4872155785560608: 100%|█| 258/258 [00:01<00:00, 231.64it/s\n",
      "Train 18 | out_loss 0.4857807755470276: 100%|█| 258/258 [00:01<00:00, 237.17it/s\n",
      "Train 19 | out_loss 0.48200443387031555: 100%|█| 258/258 [00:01<00:00, 231.80it/\n",
      "Train 20 | out_loss 0.4802700877189636: 100%|█| 258/258 [00:01<00:00, 234.72it/s\n",
      "Train 21 | out_loss 0.47539111971855164: 100%|█| 258/258 [00:01<00:00, 231.97it/\n",
      "Train 22 | out_loss 0.4743571877479553: 100%|█| 258/258 [00:01<00:00, 234.51it/s\n",
      "Train 23 | out_loss 0.47142601013183594: 100%|█| 258/258 [00:01<00:00, 235.37it/\n",
      "Train 24 | out_loss 0.46835678815841675: 100%|█| 258/258 [00:01<00:00, 234.66it/\n",
      "Train 25 | out_loss 0.4652523994445801: 100%|█| 258/258 [00:01<00:00, 232.67it/s\n",
      "Train 26 | out_loss 0.4661554992198944: 100%|█| 258/258 [00:01<00:00, 235.68it/s\n",
      "Train 27 | out_loss 0.46600592136383057: 100%|█| 258/258 [00:01<00:00, 233.57it/\n",
      "Train 28 | out_loss 0.4637333154678345: 100%|█| 258/258 [00:01<00:00, 229.13it/s\n",
      "Train 29 | out_loss 0.4665212035179138: 100%|█| 258/258 [00:01<00:00, 236.35it/s\n",
      "Train 30 | out_loss 0.4614007771015167: 100%|█| 258/258 [00:01<00:00, 235.06it/s\n",
      "Train 31 | out_loss 0.4645479619503021: 100%|█| 258/258 [00:01<00:00, 233.89it/s\n",
      "Train 32 | out_loss 0.46104133129119873: 100%|█| 258/258 [00:01<00:00, 231.94it/\n",
      "Train 33 | out_loss 0.4585016071796417: 100%|█| 258/258 [00:01<00:00, 233.67it/s\n",
      "Train 34 | out_loss 0.45607608556747437: 100%|█| 258/258 [00:01<00:00, 233.11it/\n",
      "Train 35 | out_loss 0.4554310142993927: 100%|█| 258/258 [00:01<00:00, 233.19it/s\n",
      "Train 36 | out_loss 0.4532333016395569: 100%|█| 258/258 [00:01<00:00, 234.24it/s\n",
      "Train 37 | out_loss 0.4538240134716034: 100%|█| 258/258 [00:01<00:00, 234.07it/s\n",
      "Train 38 | out_loss 0.4521605670452118: 100%|█| 258/258 [00:01<00:00, 234.24it/s\n",
      "Train 39 | out_loss 0.4520668089389801: 100%|█| 258/258 [00:01<00:00, 235.30it/s\n",
      "Train 40 | out_loss 0.449402391910553: 100%|█| 258/258 [00:01<00:00, 224.23it/s]\n",
      "Train 41 | out_loss 0.4463438093662262: 100%|█| 258/258 [00:01<00:00, 233.74it/s\n",
      "Train 42 | out_loss 0.445935994386673: 100%|█| 258/258 [00:01<00:00, 231.55it/s]\n",
      "Train 43 | out_loss 0.4475597143173218: 100%|█| 258/258 [00:01<00:00, 229.80it/s\n",
      "Train 44 | out_loss 0.44396278262138367: 100%|█| 258/258 [00:01<00:00, 233.55it/\n",
      "Train 45 | out_loss 0.44568413496017456: 100%|█| 258/258 [00:01<00:00, 237.32it/\n",
      "Train 46 | out_loss 0.4448610246181488: 100%|█| 258/258 [00:01<00:00, 230.60it/s\n",
      "Train 47 | out_loss 0.4442085325717926: 100%|█| 258/258 [00:01<00:00, 232.68it/s\n",
      "Train 48 | out_loss 0.44572338461875916: 100%|█| 258/258 [00:01<00:00, 233.36it/\n",
      "Train 49 | out_loss 0.441995769739151: 100%|█| 258/258 [00:01<00:00, 219.03it/s]\n",
      "Train 50 | out_loss 0.439900666475296: 100%|█| 258/258 [00:01<00:00, 231.39it/s]\n",
      "Train 51 | out_loss 0.4414900243282318: 100%|█| 258/258 [00:01<00:00, 232.38it/s\n",
      "Train 52 | out_loss 0.4396544098854065: 100%|█| 258/258 [00:01<00:00, 231.79it/s\n",
      "Train 53 | out_loss 0.43945515155792236: 100%|█| 258/258 [00:01<00:00, 230.24it/\n",
      "Train 54 | out_loss 0.4394204318523407: 100%|█| 258/258 [00:01<00:00, 232.39it/s\n",
      "Train 55 | out_loss 0.43819212913513184: 100%|█| 258/258 [00:01<00:00, 234.85it/\n",
      "Train 56 | out_loss 0.4371647238731384: 100%|█| 258/258 [00:01<00:00, 231.20it/s\n",
      "Train 57 | out_loss 0.437041312456131: 100%|█| 258/258 [00:01<00:00, 234.98it/s]\n",
      "Train 58 | out_loss 0.4372485876083374: 100%|█| 258/258 [00:01<00:00, 234.65it/s\n",
      "Train 59 | out_loss 0.4358343780040741: 100%|█| 258/258 [00:01<00:00, 233.59it/s\n",
      "Train 60 | out_loss 0.44118964672088623: 100%|█| 258/258 [00:01<00:00, 234.46it/\n",
      "Train 61 | out_loss 0.4407954216003418: 100%|█| 258/258 [00:01<00:00, 228.58it/s\n",
      "Train 62 | out_loss 0.43695348501205444: 100%|█| 258/258 [00:01<00:00, 228.64it/\n",
      "Train 63 | out_loss 0.43528446555137634: 100%|█| 258/258 [00:01<00:00, 231.24it/\n",
      "Train 64 | out_loss 0.4355309307575226: 100%|█| 258/258 [00:01<00:00, 232.08it/s\n",
      "Train 65 | out_loss 0.4355355501174927: 100%|█| 258/258 [00:01<00:00, 231.93it/s\n",
      "Train 66 | out_loss 0.43468818068504333: 100%|█| 258/258 [00:01<00:00, 231.95it/\n",
      "Train 67 | out_loss 0.43267822265625: 100%|██| 258/258 [00:01<00:00, 231.13it/s]\n",
      "Train 68 | out_loss 0.43432027101516724: 100%|█| 258/258 [00:01<00:00, 228.79it/\n",
      "Train 69 | out_loss 0.4318956434726715: 100%|█| 258/258 [00:01<00:00, 232.97it/s\n",
      "Train 70 | out_loss 0.4346575140953064: 100%|█| 258/258 [00:01<00:00, 232.63it/s\n",
      "Train 71 | out_loss 0.4409564137458801: 100%|█| 258/258 [00:01<00:00, 232.26it/s\n",
      "Train 72 | out_loss 0.4335763156414032: 100%|█| 258/258 [00:01<00:00, 232.84it/s\n",
      "Train 73 | out_loss 0.43060922622680664: 100%|█| 258/258 [00:01<00:00, 220.91it/\n",
      "Train 74 | out_loss 0.43913164734840393: 100%|█| 258/258 [00:01<00:00, 228.90it/\n",
      "Train 75 | out_loss 0.43344566226005554: 100%|█| 258/258 [00:01<00:00, 233.08it/\n",
      "Train 76 | out_loss 0.4327816367149353: 100%|█| 258/258 [00:01<00:00, 234.34it/s\n",
      "Train 77 | out_loss 0.44267022609710693: 100%|█| 258/258 [00:01<00:00, 229.92it/\n",
      "Train 78 | out_loss 0.4359872043132782: 100%|█| 258/258 [00:01<00:00, 234.88it/s\n",
      "Train 79 | out_loss 0.433220773935318: 100%|█| 258/258 [00:01<00:00, 231.27it/s]\n",
      "Train 80 | out_loss 0.4294990003108978: 100%|█| 258/258 [00:01<00:00, 234.61it/s\n",
      "Train 81 | out_loss 0.4285479784011841: 100%|█| 258/258 [00:01<00:00, 233.09it/s\n",
      "Train 82 | out_loss 0.4289390742778778: 100%|█| 258/258 [00:01<00:00, 234.48it/s\n",
      "Train 83 | out_loss 0.42907100915908813: 100%|█| 258/258 [00:01<00:00, 231.01it/\n",
      "Train 84 | out_loss 0.4290755093097687: 100%|█| 258/258 [00:01<00:00, 232.40it/s\n",
      "Train 85 | out_loss 0.42722487449645996: 100%|█| 258/258 [00:01<00:00, 227.53it/\n",
      "Train 86 | out_loss 0.4295159876346588: 100%|█| 258/258 [00:01<00:00, 231.90it/s\n",
      "Train 87 | out_loss 0.4329903721809387: 100%|█| 258/258 [00:01<00:00, 234.93it/s\n",
      "Train 88 | out_loss 0.4282037913799286: 100%|█| 258/258 [00:01<00:00, 224.74it/s\n",
      "Train 89 | out_loss 0.4283934533596039: 100%|█| 258/258 [00:01<00:00, 232.82it/s\n",
      "Train 90 | out_loss 0.4714065194129944: 100%|█| 258/258 [00:01<00:00, 231.28it/s\n",
      "Train 91 | out_loss 0.43184152245521545: 100%|█| 258/258 [00:01<00:00, 232.89it/\n",
      "Train 92 | out_loss 0.42747196555137634: 100%|█| 258/258 [00:01<00:00, 233.83it/\n",
      "Train 93 | out_loss 0.428012490272522: 100%|█| 258/258 [00:01<00:00, 225.37it/s]\n",
      "Train 94 | out_loss 0.42722538113594055: 100%|█| 258/258 [00:01<00:00, 233.40it/\n",
      "Train 95 | out_loss 0.4285711348056793: 100%|█| 258/258 [00:01<00:00, 233.13it/s\n",
      "Train 96 | out_loss 0.42698192596435547: 100%|█| 258/258 [00:01<00:00, 232.53it/\n",
      "Train 97 | out_loss 0.4255058467388153: 100%|█| 258/258 [00:01<00:00, 232.58it/s\n",
      "Train 98 | out_loss 0.42811843752861023: 100%|█| 258/258 [00:01<00:00, 228.86it/\n",
      "Train 99 | out_loss 0.4263136386871338: 100%|█| 258/258 [00:01<00:00, 224.74it/s\n",
      "  0%|                                                   | 0/258 [00:00<?, ?it/s]/home/AL_main_new/utils.py:99: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  x = torch.tensor([x for x,y in batch], dtype=torch.float32)\n",
      "Train 0 | out_loss 0.693138599395752: 100%|██| 258/258 [00:01<00:00, 154.32it/s]\n",
      "Train 1 | out_loss 0.5734400749206543: 100%|█| 258/258 [00:01<00:00, 200.18it/s]\n",
      "Train 2 | out_loss 0.548487663269043: 100%|██| 258/258 [00:01<00:00, 201.70it/s]\n",
      "Train 3 | out_loss 0.5376628041267395: 100%|█| 258/258 [00:01<00:00, 202.65it/s]\n",
      "Train 4 | out_loss 0.5301933884620667: 100%|█| 258/258 [00:01<00:00, 201.54it/s]\n",
      "Train 5 | out_loss 0.5254131555557251: 100%|█| 258/258 [00:01<00:00, 202.34it/s]\n",
      "Train 6 | out_loss 0.5227841734886169: 100%|█| 258/258 [00:01<00:00, 200.13it/s]\n",
      "Train 7 | out_loss 0.5268009305000305: 100%|█| 258/258 [00:01<00:00, 204.38it/s]\n",
      "Train 8 | out_loss 0.5144466757774353: 100%|█| 258/258 [00:01<00:00, 195.05it/s]\n",
      "Train 9 | out_loss 0.5046791434288025: 100%|█| 258/258 [00:01<00:00, 201.15it/s]\n",
      "Train 10 | out_loss 0.5031518340110779: 100%|█| 258/258 [00:01<00:00, 203.13it/s\n",
      "Train 11 | out_loss 0.503692090511322: 100%|█| 258/258 [00:01<00:00, 204.25it/s]\n",
      "Train 12 | out_loss 0.49692124128341675: 100%|█| 258/258 [00:01<00:00, 201.88it/\n",
      "Train 13 | out_loss 0.4942900538444519: 100%|█| 258/258 [00:01<00:00, 199.77it/s\n",
      "Train 14 | out_loss 0.49122336506843567: 100%|█| 258/258 [00:01<00:00, 202.75it/\n",
      "Train 15 | out_loss 0.48894238471984863: 100%|█| 258/258 [00:01<00:00, 204.75it/\n",
      "Train 16 | out_loss 0.48572662472724915: 100%|█| 258/258 [00:01<00:00, 200.99it/\n",
      "Train 17 | out_loss 0.4818401634693146: 100%|█| 258/258 [00:01<00:00, 202.63it/s\n",
      "Train 18 | out_loss 0.4773668348789215: 100%|█| 258/258 [00:01<00:00, 204.54it/s\n",
      "Train 19 | out_loss 0.4767857491970062: 100%|█| 258/258 [00:01<00:00, 202.86it/s\n",
      "Train 20 | out_loss 0.4766108989715576: 100%|█| 258/258 [00:01<00:00, 199.97it/s\n",
      "Train 21 | out_loss 0.4734957814216614: 100%|█| 258/258 [00:01<00:00, 203.76it/s\n",
      "Train 22 | out_loss 0.46997958421707153: 100%|█| 258/258 [00:01<00:00, 199.38it/\n",
      "Train 23 | out_loss 0.46767115592956543: 100%|█| 258/258 [00:01<00:00, 198.41it/\n",
      "Train 24 | out_loss 0.4654211699962616: 100%|█| 258/258 [00:01<00:00, 199.85it/s\n",
      "Train 25 | out_loss 0.4663303792476654: 100%|█| 258/258 [00:01<00:00, 203.25it/s\n",
      "Train 26 | out_loss 0.4646848440170288: 100%|█| 258/258 [00:01<00:00, 203.84it/s\n",
      "Train 27 | out_loss 0.4616043269634247: 100%|█| 258/258 [00:01<00:00, 204.91it/s\n",
      "Train 28 | out_loss 0.46280544996261597: 100%|█| 258/258 [00:01<00:00, 203.97it/\n",
      "Train 29 | out_loss 0.45914793014526367: 100%|█| 258/258 [00:01<00:00, 200.28it/\n",
      "Train 30 | out_loss 0.45719921588897705: 100%|█| 258/258 [00:01<00:00, 202.39it/\n",
      "Train 31 | out_loss 0.4550468623638153: 100%|█| 258/258 [00:01<00:00, 203.48it/s\n",
      "Train 32 | out_loss 0.4555944502353668: 100%|█| 258/258 [00:01<00:00, 199.99it/s\n",
      "Train 33 | out_loss 0.45340561866760254: 100%|█| 258/258 [00:01<00:00, 195.00it/\n",
      "Train 34 | out_loss 0.4559379816055298: 100%|█| 258/258 [00:01<00:00, 201.40it/s\n",
      "Train 35 | out_loss 0.4554496109485626: 100%|█| 258/258 [00:01<00:00, 200.98it/s\n",
      "Train 36 | out_loss 0.4530324637889862: 100%|█| 258/258 [00:01<00:00, 201.43it/s\n",
      "Train 37 | out_loss 0.4512445330619812: 100%|█| 258/258 [00:01<00:00, 197.91it/s\n",
      "Train 38 | out_loss 0.4509177505970001: 100%|█| 258/258 [00:01<00:00, 203.16it/s\n",
      "Train 39 | out_loss 0.4529646933078766: 100%|█| 258/258 [00:01<00:00, 199.81it/s\n",
      "Train 40 | out_loss 0.4490501880645752: 100%|█| 258/258 [00:01<00:00, 200.89it/s\n",
      "Train 41 | out_loss 0.4491954445838928: 100%|█| 258/258 [00:01<00:00, 198.85it/s\n",
      "Train 42 | out_loss 0.4517644941806793: 100%|█| 258/258 [00:01<00:00, 201.07it/s\n",
      "Train 43 | out_loss 0.4500901997089386: 100%|█| 258/258 [00:01<00:00, 202.58it/s\n",
      "Train 44 | out_loss 0.4474979043006897: 100%|█| 258/258 [00:01<00:00, 199.02it/s\n",
      "Train 45 | out_loss 0.4454641044139862: 100%|█| 258/258 [00:01<00:00, 201.99it/s\n",
      "Train 46 | out_loss 0.4443894326686859: 100%|█| 258/258 [00:01<00:00, 198.11it/s\n",
      "Train 47 | out_loss 0.44471317529678345: 100%|█| 258/258 [00:01<00:00, 201.32it/\n",
      "Train 48 | out_loss 0.443290114402771: 100%|█| 258/258 [00:01<00:00, 205.88it/s]\n",
      "Train 49 | out_loss 0.443332701921463: 100%|█| 258/258 [00:01<00:00, 200.40it/s]\n",
      "Train 50 | out_loss 0.4443915784358978: 100%|█| 258/258 [00:01<00:00, 198.25it/s\n",
      "Train 51 | out_loss 0.43971115350723267: 100%|█| 258/258 [00:01<00:00, 202.35it/\n",
      "Train 52 | out_loss 0.44491347670555115: 100%|█| 258/258 [00:01<00:00, 201.67it/\n",
      "Train 53 | out_loss 0.4454819858074188: 100%|█| 258/258 [00:01<00:00, 196.92it/s\n",
      "Train 54 | out_loss 0.44214776158332825: 100%|█| 258/258 [00:01<00:00, 203.49it/\n",
      "Train 55 | out_loss 0.44115111231803894: 100%|█| 258/258 [00:01<00:00, 202.20it/\n",
      "Train 56 | out_loss 0.442531019449234: 100%|█| 258/258 [00:01<00:00, 201.83it/s]\n",
      "Train 57 | out_loss 0.44450974464416504: 100%|█| 258/258 [00:01<00:00, 197.59it/\n",
      "Train 58 | out_loss 0.43889281153678894: 100%|█| 258/258 [00:01<00:00, 193.09it/\n",
      "Train 59 | out_loss 0.4405246376991272: 100%|█| 258/258 [00:01<00:00, 200.44it/s\n",
      "Train 60 | out_loss 0.44056418538093567: 100%|█| 258/258 [00:01<00:00, 202.09it/\n",
      "Train 61 | out_loss 0.4434051513671875: 100%|█| 258/258 [00:01<00:00, 201.42it/s\n",
      "Train 62 | out_loss 0.5088846683502197: 100%|█| 258/258 [00:01<00:00, 200.55it/s\n",
      "Train 63 | out_loss 0.44063058495521545: 100%|█| 258/258 [00:01<00:00, 202.14it/\n",
      "Train 64 | out_loss 0.4440774917602539: 100%|█| 258/258 [00:01<00:00, 197.77it/s\n",
      "Train 65 | out_loss 0.43689340353012085: 100%|█| 258/258 [00:01<00:00, 201.67it/\n",
      "Train 66 | out_loss 0.43735823035240173: 100%|█| 258/258 [00:01<00:00, 197.85it/\n",
      "Train 67 | out_loss 0.4411317706108093: 100%|█| 258/258 [00:01<00:00, 200.77it/s\n",
      "Train 68 | out_loss 0.4379236102104187: 100%|█| 258/258 [00:01<00:00, 202.72it/s\n",
      "Train 69 | out_loss 0.435861736536026: 100%|█| 258/258 [00:01<00:00, 199.35it/s]\n",
      "Train 70 | out_loss 0.43568265438079834: 100%|█| 258/258 [00:01<00:00, 203.64it/\n",
      "Train 71 | out_loss 0.4411362111568451: 100%|█| 258/258 [00:01<00:00, 202.43it/s\n",
      "Train 72 | out_loss 0.43465161323547363: 100%|█| 258/258 [00:01<00:00, 197.51it/\n",
      "Train 73 | out_loss 0.43653956055641174: 100%|█| 258/258 [00:01<00:00, 200.49it/\n",
      "Train 74 | out_loss 0.4336045980453491: 100%|█| 258/258 [00:01<00:00, 198.94it/s\n",
      "Train 75 | out_loss 0.43254587054252625: 100%|█| 258/258 [00:01<00:00, 200.71it/\n",
      "Train 76 | out_loss 0.6096491813659668: 100%|█| 258/258 [00:01<00:00, 204.58it/s\n",
      "Train 77 | out_loss 0.4363725781440735: 100%|█| 258/258 [00:01<00:00, 198.22it/s\n",
      "Train 78 | out_loss 0.4372554123401642: 100%|█| 258/258 [00:01<00:00, 203.10it/s\n",
      "Train 79 | out_loss 0.43540632724761963: 100%|█| 258/258 [00:01<00:00, 201.26it/\n",
      "Train 80 | out_loss 0.43292003870010376: 100%|█| 258/258 [00:01<00:00, 199.94it/\n",
      "Train 81 | out_loss 0.4332456588745117: 100%|█| 258/258 [00:01<00:00, 200.01it/s\n",
      "Train 82 | out_loss 0.4310343563556671: 100%|█| 258/258 [00:01<00:00, 201.69it/s\n",
      "Train 83 | out_loss 0.4323008060455322: 100%|█| 258/258 [00:01<00:00, 193.13it/s\n",
      "Train 84 | out_loss 0.42955243587493896: 100%|█| 258/258 [00:01<00:00, 197.26it/\n",
      "Train 85 | out_loss 0.43272483348846436: 100%|█| 258/258 [00:01<00:00, 201.96it/\n",
      "Train 86 | out_loss 0.43192625045776367: 100%|█| 258/258 [00:01<00:00, 200.00it/\n",
      "Train 87 | out_loss 0.4320131540298462: 100%|█| 258/258 [00:01<00:00, 200.56it/s\n",
      "Train 88 | out_loss 0.43218347430229187: 100%|█| 258/258 [00:01<00:00, 194.87it/\n",
      "Train 89 | out_loss 0.4822008013725281: 100%|█| 258/258 [00:01<00:00, 200.55it/s\n",
      "Train 90 | out_loss 0.43277451395988464: 100%|█| 258/258 [00:01<00:00, 197.19it/\n",
      "Train 91 | out_loss 0.4298396706581116: 100%|█| 258/258 [00:01<00:00, 200.66it/s\n",
      "Train 92 | out_loss 0.432358056306839: 100%|█| 258/258 [00:01<00:00, 199.45it/s]\n",
      "Train 93 | out_loss 0.42985212802886963: 100%|█| 258/258 [00:01<00:00, 203.61it/\n",
      "Train 94 | out_loss 0.4301055669784546: 100%|█| 258/258 [00:01<00:00, 204.19it/s\n",
      "Train 95 | out_loss 0.42881226539611816: 100%|█| 258/258 [00:01<00:00, 201.10it/\n",
      "Train 96 | out_loss 0.4288683533668518: 100%|█| 258/258 [00:01<00:00, 200.83it/s\n",
      "Train 97 | out_loss 0.43034130334854126: 100%|█| 258/258 [00:01<00:00, 198.34it/\n",
      "Train 98 | out_loss 0.4300776422023773: 100%|█| 258/258 [00:01<00:00, 202.60it/s\n",
      "Train 99 | out_loss 0.4303561747074127: 100%|█| 258/258 [00:01<00:00, 204.20it/s\n",
      "  0%|                                                   | 0/258 [00:00<?, ?it/s]/home/AL_main_new/utils.py:99: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  x = torch.tensor([x for x,y in batch], dtype=torch.float32)\n",
      "Train 0 | out_loss 0.7126606702804565: 100%|█| 258/258 [00:01<00:00, 141.29it/s]\n",
      "Train 1 | out_loss 0.5716829299926758: 100%|█| 258/258 [00:01<00:00, 177.60it/s]\n",
      "Train 2 | out_loss 0.5746170878410339: 100%|█| 258/258 [00:01<00:00, 177.91it/s]\n",
      "Train 3 | out_loss 0.5562019348144531: 100%|█| 258/258 [00:01<00:00, 180.37it/s]\n",
      "Train 4 | out_loss 0.5367169380187988: 100%|█| 258/258 [00:01<00:00, 177.98it/s]\n",
      "Train 5 | out_loss 0.544211745262146: 100%|██| 258/258 [00:01<00:00, 176.67it/s]\n",
      "Train 6 | out_loss 0.5329007506370544: 100%|█| 258/258 [00:01<00:00, 178.31it/s]\n",
      "Train 7 | out_loss 0.5262420177459717: 100%|█| 258/258 [00:01<00:00, 179.22it/s]\n",
      "Train 8 | out_loss 0.5205345749855042: 100%|█| 258/258 [00:01<00:00, 177.37it/s]\n",
      "Train 9 | out_loss 0.5182147026062012: 100%|█| 258/258 [00:01<00:00, 179.69it/s]\n",
      "Train 10 | out_loss 0.5163235664367676: 100%|█| 258/258 [00:01<00:00, 178.51it/s\n",
      "Train 11 | out_loss 0.5092920064926147: 100%|█| 258/258 [00:01<00:00, 178.82it/s\n",
      "Train 12 | out_loss 0.5026640892028809: 100%|█| 258/258 [00:01<00:00, 178.71it/s\n",
      "Train 13 | out_loss 0.5048243999481201: 100%|█| 258/258 [00:01<00:00, 175.88it/s\n",
      "Train 14 | out_loss 0.5023496150970459: 100%|█| 258/258 [00:01<00:00, 175.86it/s\n",
      "Train 15 | out_loss 0.4955560266971588: 100%|█| 258/258 [00:01<00:00, 177.73it/s\n",
      "Train 16 | out_loss 0.49155500531196594: 100%|█| 258/258 [00:01<00:00, 176.42it/\n",
      "Train 17 | out_loss 0.49486181139945984: 100%|█| 258/258 [00:01<00:00, 178.00it/\n",
      "Train 18 | out_loss 0.4869224429130554: 100%|█| 258/258 [00:01<00:00, 178.05it/s\n",
      "Train 19 | out_loss 0.4901493489742279: 100%|█| 258/258 [00:01<00:00, 178.37it/s\n",
      "Train 20 | out_loss 0.49205878376960754: 100%|█| 258/258 [00:01<00:00, 178.54it/\n",
      "Train 21 | out_loss 0.4816705882549286: 100%|█| 258/258 [00:01<00:00, 180.77it/s\n",
      "Train 22 | out_loss 0.4800820052623749: 100%|█| 258/258 [00:01<00:00, 178.67it/s\n",
      "Train 23 | out_loss 0.4789266288280487: 100%|█| 258/258 [00:01<00:00, 177.54it/s\n",
      "Train 24 | out_loss 0.47887521982192993: 100%|█| 258/258 [00:01<00:00, 178.29it/\n",
      "Train 25 | out_loss 0.47073686122894287: 100%|█| 258/258 [00:01<00:00, 179.96it/\n",
      "Train 26 | out_loss 0.4720977544784546: 100%|█| 258/258 [00:01<00:00, 179.61it/s\n",
      "Train 27 | out_loss 0.4716814160346985: 100%|█| 258/258 [00:01<00:00, 173.32it/s\n",
      "Train 28 | out_loss 0.4674800634384155: 100%|█| 258/258 [00:01<00:00, 179.23it/s\n",
      "Train 29 | out_loss 0.4692344665527344: 100%|█| 258/258 [00:01<00:00, 176.02it/s\n",
      "Train 30 | out_loss 0.4653027355670929: 100%|█| 258/258 [00:01<00:00, 180.86it/s\n",
      "Train 31 | out_loss 0.46024855971336365: 100%|█| 258/258 [00:01<00:00, 180.83it/\n",
      "Train 32 | out_loss 0.45971983671188354: 100%|█| 258/258 [00:01<00:00, 179.33it/\n",
      "Train 33 | out_loss 0.46158578991889954: 100%|█| 258/258 [00:01<00:00, 181.48it/\n",
      "Train 34 | out_loss 0.46027716994285583: 100%|█| 258/258 [00:01<00:00, 175.03it/\n",
      "Train 35 | out_loss 0.460453063249588: 100%|█| 258/258 [00:01<00:00, 179.60it/s]\n",
      "Train 36 | out_loss 0.4565288722515106: 100%|█| 258/258 [00:01<00:00, 181.16it/s\n",
      "Train 37 | out_loss 0.45977213978767395: 100%|█| 258/258 [00:01<00:00, 176.60it/\n",
      "Train 38 | out_loss 0.46142125129699707: 100%|█| 258/258 [00:01<00:00, 178.72it/\n",
      "Train 39 | out_loss 0.45732995867729187: 100%|█| 258/258 [00:01<00:00, 176.10it/\n",
      "Train 40 | out_loss 0.4563586115837097: 100%|█| 258/258 [00:01<00:00, 178.09it/s\n",
      "Train 41 | out_loss 0.45400527119636536: 100%|█| 258/258 [00:01<00:00, 179.12it/\n",
      "Train 42 | out_loss 0.45141056180000305: 100%|█| 258/258 [00:01<00:00, 178.43it/\n",
      "Train 43 | out_loss 0.45409339666366577: 100%|█| 258/258 [00:01<00:00, 174.72it/\n",
      "Train 44 | out_loss 0.458055317401886: 100%|█| 258/258 [00:01<00:00, 176.14it/s]\n",
      "Train 45 | out_loss 0.45834997296333313: 100%|█| 258/258 [00:01<00:00, 176.77it/\n",
      "Train 46 | out_loss 0.4656293988227844: 100%|█| 258/258 [00:01<00:00, 177.71it/s\n",
      "Train 47 | out_loss 0.45497429370880127: 100%|█| 258/258 [00:01<00:00, 178.91it/\n",
      "Train 48 | out_loss 0.45156416296958923: 100%|█| 258/258 [00:01<00:00, 178.10it/\n",
      "Train 49 | out_loss 0.5472598075866699: 100%|█| 258/258 [00:01<00:00, 176.01it/s\n",
      "Train 50 | out_loss 0.463301420211792: 100%|█| 258/258 [00:01<00:00, 179.80it/s]\n",
      "Train 51 | out_loss 0.45429378747940063: 100%|█| 258/258 [00:01<00:00, 170.01it/\n",
      "Train 52 | out_loss 0.4502272605895996: 100%|█| 258/258 [00:01<00:00, 178.29it/s\n",
      "Train 53 | out_loss 0.47735095024108887: 100%|█| 258/258 [00:01<00:00, 178.64it/\n",
      "Train 54 | out_loss 0.48177722096443176: 100%|█| 258/258 [00:01<00:00, 178.82it/\n",
      "Train 55 | out_loss 0.45329612493515015: 100%|█| 258/258 [00:01<00:00, 180.88it/\n",
      "Train 56 | out_loss 0.4490441083908081: 100%|█| 258/258 [00:01<00:00, 176.88it/s\n",
      "Train 57 | out_loss 0.4490811228752136: 100%|█| 258/258 [00:01<00:00, 175.43it/s\n",
      "Train 58 | out_loss 0.4453170597553253: 100%|█| 258/258 [00:01<00:00, 177.71it/s\n",
      "Train 59 | out_loss 0.4501054286956787: 100%|█| 258/258 [00:01<00:00, 179.82it/s\n",
      "Train 60 | out_loss 0.4464159607887268: 100%|█| 258/258 [00:01<00:00, 179.99it/s\n",
      "Train 61 | out_loss 0.6454161405563354: 100%|█| 258/258 [00:01<00:00, 179.97it/s\n",
      "Train 62 | out_loss 0.45088544487953186: 100%|█| 258/258 [00:01<00:00, 178.15it/\n",
      "Train 63 | out_loss 0.45355522632598877: 100%|█| 258/258 [00:01<00:00, 181.72it/\n",
      "Train 64 | out_loss 0.44740238785743713: 100%|█| 258/258 [00:01<00:00, 179.44it/\n",
      "Train 65 | out_loss 0.44550037384033203: 100%|█| 258/258 [00:01<00:00, 181.13it/\n",
      "Train 66 | out_loss 0.4420771300792694: 100%|█| 258/258 [00:01<00:00, 180.31it/s\n",
      "Train 67 | out_loss 0.44685834646224976: 100%|█| 258/258 [00:01<00:00, 177.04it/\n",
      "Train 68 | out_loss 0.44611066579818726: 100%|█| 258/258 [00:01<00:00, 178.38it/\n",
      "Train 69 | out_loss 0.4440605938434601: 100%|█| 258/258 [00:01<00:00, 180.01it/s\n",
      "Train 70 | out_loss 0.5083483457565308: 100%|█| 258/258 [00:01<00:00, 174.95it/s\n",
      "Train 71 | out_loss 0.44140103459358215: 100%|█| 258/258 [00:01<00:00, 172.61it/\n",
      "Train 72 | out_loss 0.4484761655330658: 100%|█| 258/258 [00:01<00:00, 178.40it/s\n",
      "Train 73 | out_loss 0.48767977952957153: 100%|█| 258/258 [00:01<00:00, 176.61it/\n",
      "Train 74 | out_loss 0.44674593210220337: 100%|█| 258/258 [00:01<00:00, 177.59it/\n",
      "Train 75 | out_loss 0.4488404393196106: 100%|█| 258/258 [00:01<00:00, 175.64it/s\n",
      "Train 76 | out_loss 0.44925376772880554: 100%|█| 258/258 [00:01<00:00, 180.15it/\n",
      "Train 77 | out_loss 0.4531538188457489: 100%|█| 258/258 [00:01<00:00, 179.54it/s\n",
      "Train 78 | out_loss 0.46226948499679565: 100%|█| 258/258 [00:01<00:00, 168.61it/\n",
      "Train 79 | out_loss 0.4731002449989319: 100%|█| 258/258 [00:01<00:00, 177.42it/s\n",
      "Train 80 | out_loss 0.6906682252883911: 100%|█| 258/258 [00:01<00:00, 174.47it/s\n",
      "Train 81 | out_loss 0.4619147777557373: 100%|█| 258/258 [00:01<00:00, 173.74it/s\n",
      "Train 82 | out_loss 0.44959700107574463: 100%|█| 258/258 [00:01<00:00, 176.36it/\n",
      "Train 83 | out_loss 0.45610103011131287: 100%|█| 258/258 [00:01<00:00, 175.45it/\n",
      "Train 84 | out_loss 0.4494001269340515: 100%|█| 258/258 [00:01<00:00, 173.98it/s\n",
      "Train 85 | out_loss 0.44903695583343506: 100%|█| 258/258 [00:01<00:00, 177.40it/\n",
      "Train 86 | out_loss 0.45625177025794983: 100%|█| 258/258 [00:01<00:00, 174.09it/\n",
      "Train 87 | out_loss 0.4485916793346405: 100%|█| 258/258 [00:01<00:00, 177.50it/s\n",
      "Train 88 | out_loss 0.49168577790260315: 100%|█| 258/258 [00:01<00:00, 178.22it/\n",
      "Train 89 | out_loss 0.445891410112381: 100%|█| 258/258 [00:01<00:00, 173.67it/s]\n",
      "Train 90 | out_loss 0.8746054172515869: 100%|█| 258/258 [00:01<00:00, 179.69it/s\n",
      "Train 91 | out_loss 0.4592404365539551: 100%|█| 258/258 [00:01<00:00, 176.01it/s\n",
      "Train 92 | out_loss 0.44414833188056946: 100%|█| 258/258 [00:01<00:00, 178.64it/\n",
      "Train 93 | out_loss 0.445082426071167: 100%|█| 258/258 [00:01<00:00, 177.15it/s]\n",
      "Train 94 | out_loss 0.45043444633483887: 100%|█| 258/258 [00:01<00:00, 176.77it/\n",
      "Train 95 | out_loss 0.4405982494354248: 100%|█| 258/258 [00:01<00:00, 173.46it/s\n",
      "Train 96 | out_loss 0.4418160319328308: 100%|█| 258/258 [00:01<00:00, 175.91it/s\n",
      "Train 97 | out_loss 0.4469049572944641: 100%|█| 258/258 [00:01<00:00, 174.18it/s\n",
      "Train 98 | out_loss 0.44555696845054626: 100%|█| 258/258 [00:01<00:00, 179.02it/\n",
      "Train 99 | out_loss 0.4482778012752533: 100%|█| 258/258 [00:01<00:00, 179.49it/s\n",
      "  0%|                                                   | 0/258 [00:00<?, ?it/s]/home/AL_main_new/utils.py:99: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  x = torch.tensor([x for x,y in batch], dtype=torch.float32)\n",
      "Train 0 | out_loss 0.7404147386550903: 100%|█| 258/258 [00:01<00:00, 129.72it/s]\n",
      "Train 1 | out_loss 0.5836005806922913: 100%|█| 258/258 [00:01<00:00, 161.23it/s]\n",
      "Train 2 | out_loss 0.5839998126029968: 100%|█| 258/258 [00:01<00:00, 161.44it/s]\n",
      "Train 3 | out_loss 0.571479856967926: 100%|██| 258/258 [00:01<00:00, 161.30it/s]\n",
      "Train 4 | out_loss 0.554517924785614: 100%|██| 258/258 [00:01<00:00, 161.07it/s]\n",
      "Train 5 | out_loss 0.5575969219207764: 100%|█| 258/258 [00:01<00:00, 159.19it/s]\n",
      "Train 6 | out_loss 0.5359325408935547: 100%|█| 258/258 [00:01<00:00, 153.95it/s]\n",
      "Train 7 | out_loss 0.5320274829864502: 100%|█| 258/258 [00:01<00:00, 160.73it/s]\n",
      "Train 8 | out_loss 0.5313127040863037: 100%|█| 258/258 [00:01<00:00, 161.76it/s]\n",
      "Train 9 | out_loss 0.537257969379425: 100%|██| 258/258 [00:01<00:00, 159.37it/s]\n",
      "Train 10 | out_loss 0.5193153023719788: 100%|█| 258/258 [00:01<00:00, 161.09it/s\n",
      "Train 11 | out_loss 0.5168582797050476: 100%|█| 258/258 [00:01<00:00, 158.03it/s\n",
      "Train 12 | out_loss 0.5132154822349548: 100%|█| 258/258 [00:01<00:00, 160.02it/s\n",
      "Train 13 | out_loss 0.5074964761734009: 100%|█| 258/258 [00:01<00:00, 161.48it/s\n",
      "Train 14 | out_loss 0.5017606616020203: 100%|█| 258/258 [00:01<00:00, 159.99it/s\n",
      "Train 15 | out_loss 0.5014201998710632: 100%|█| 258/258 [00:01<00:00, 156.59it/s\n",
      "Train 16 | out_loss 0.5016058087348938: 100%|█| 258/258 [00:01<00:00, 155.22it/s\n",
      "Train 17 | out_loss 0.5025993585586548: 100%|█| 258/258 [00:01<00:00, 159.37it/s\n",
      "Train 18 | out_loss 0.49146726727485657: 100%|█| 258/258 [00:01<00:00, 158.35it/\n",
      "Train 19 | out_loss 0.4909633994102478: 100%|█| 258/258 [00:01<00:00, 159.12it/s\n",
      "Train 20 | out_loss 0.4919600486755371: 100%|█| 258/258 [00:01<00:00, 160.85it/s\n",
      "Train 21 | out_loss 0.4846642017364502: 100%|█| 258/258 [00:01<00:00, 161.91it/s\n",
      "Train 22 | out_loss 0.48518359661102295: 100%|█| 258/258 [00:01<00:00, 160.85it/\n",
      "Train 23 | out_loss 0.4781073331832886: 100%|█| 258/258 [00:01<00:00, 158.38it/s\n",
      "Train 24 | out_loss 0.4747384488582611: 100%|█| 258/258 [00:01<00:00, 161.54it/s\n",
      "Train 25 | out_loss 0.48118823766708374: 100%|█| 258/258 [00:01<00:00, 160.29it/\n",
      "Train 26 | out_loss 0.4742445647716522: 100%|█| 258/258 [00:01<00:00, 160.14it/s\n",
      "Train 27 | out_loss 0.47874462604522705: 100%|█| 258/258 [00:01<00:00, 159.92it/\n",
      "Train 28 | out_loss 0.47668275237083435: 100%|█| 258/258 [00:01<00:00, 160.13it/\n",
      "Train 29 | out_loss 0.4737260341644287: 100%|█| 258/258 [00:01<00:00, 158.41it/s\n",
      "Train 30 | out_loss 0.4769023358821869: 100%|█| 258/258 [00:01<00:00, 159.98it/s\n",
      "Train 31 | out_loss 0.46843481063842773: 100%|█| 258/258 [00:01<00:00, 156.16it/\n",
      "Train 32 | out_loss 0.46626052260398865: 100%|█| 258/258 [00:01<00:00, 158.85it/\n",
      "Train 33 | out_loss 0.46979832649230957: 100%|█| 258/258 [00:01<00:00, 160.06it/\n",
      "Train 34 | out_loss 0.4640803635120392: 100%|█| 258/258 [00:01<00:00, 160.34it/s\n",
      "Train 35 | out_loss 0.46767547726631165: 100%|█| 258/258 [00:01<00:00, 158.75it/\n",
      "Train 36 | out_loss 0.47511547803878784: 100%|█| 258/258 [00:01<00:00, 158.81it/\n",
      "Train 37 | out_loss 0.47162771224975586: 100%|█| 258/258 [00:01<00:00, 161.38it/\n",
      "Train 38 | out_loss 0.4658840000629425: 100%|█| 258/258 [00:01<00:00, 160.47it/s\n",
      "Train 39 | out_loss 0.46415629982948303: 100%|█| 258/258 [00:01<00:00, 154.65it/\n",
      "Train 40 | out_loss 0.4677598774433136: 100%|█| 258/258 [00:01<00:00, 160.55it/s\n",
      "Train 41 | out_loss 0.46626150608062744: 100%|█| 258/258 [00:01<00:00, 157.29it/\n",
      "Train 42 | out_loss 0.46373066306114197: 100%|█| 258/258 [00:01<00:00, 160.12it/\n",
      "Train 43 | out_loss 0.4642612338066101: 100%|█| 258/258 [00:01<00:00, 154.95it/s\n",
      "Train 44 | out_loss 0.4727040231227875: 100%|█| 258/258 [00:01<00:00, 157.22it/s\n",
      "Train 45 | out_loss 0.46325618028640747: 100%|█| 258/258 [00:01<00:00, 160.12it/\n",
      "Train 46 | out_loss 0.4663075804710388: 100%|█| 258/258 [00:01<00:00, 160.24it/s\n",
      "Train 47 | out_loss 0.4670466482639313: 100%|█| 258/258 [00:01<00:00, 157.84it/s\n",
      "Train 48 | out_loss 0.46597015857696533: 100%|█| 258/258 [00:01<00:00, 158.65it/\n",
      "Train 49 | out_loss 0.4670613408088684: 100%|█| 258/258 [00:01<00:00, 158.04it/s\n",
      "Train 50 | out_loss 0.5129945874214172: 100%|█| 258/258 [00:01<00:00, 157.11it/s\n",
      "Train 51 | out_loss 0.47842973470687866: 100%|█| 258/258 [00:01<00:00, 157.57it/\n",
      "Train 52 | out_loss 0.46688616275787354: 100%|█| 258/258 [00:01<00:00, 158.52it/\n",
      "Train 53 | out_loss 0.46126699447631836: 100%|█| 258/258 [00:01<00:00, 157.62it/\n",
      "Train 54 | out_loss 0.4819541573524475: 100%|█| 258/258 [00:01<00:00, 159.98it/s\n",
      "Train 55 | out_loss 0.4685511291027069: 100%|█| 258/258 [00:01<00:00, 156.47it/s\n",
      "Train 56 | out_loss 0.47736474871635437: 100%|█| 258/258 [00:01<00:00, 159.81it/\n",
      "Train 57 | out_loss 0.4745124280452728: 100%|█| 258/258 [00:01<00:00, 155.88it/s\n",
      "Train 58 | out_loss 0.5074571967124939: 100%|█| 258/258 [00:01<00:00, 160.00it/s\n",
      "Train 59 | out_loss 0.48093119263648987: 100%|█| 258/258 [00:01<00:00, 157.37it/\n",
      "Train 60 | out_loss 0.6941579580307007: 100%|█| 258/258 [00:01<00:00, 158.41it/s\n",
      "Train 61 | out_loss 0.4682333469390869: 100%|█| 258/258 [00:01<00:00, 158.12it/s\n",
      "Train 62 | out_loss 0.45783260464668274: 100%|█| 258/258 [00:01<00:00, 159.49it/\n",
      "Train 63 | out_loss 0.4680668115615845: 100%|█| 258/258 [00:01<00:00, 156.38it/s\n",
      "Train 64 | out_loss 0.47818902134895325: 100%|█| 258/258 [00:01<00:00, 158.41it/\n",
      "Train 65 | out_loss 0.47663572430610657: 100%|█| 258/258 [00:01<00:00, 159.88it/\n",
      "Train 66 | out_loss 0.6612474322319031: 100%|█| 258/258 [00:01<00:00, 157.69it/s\n",
      "Train 67 | out_loss 0.4621760845184326: 100%|█| 258/258 [00:01<00:00, 160.50it/s\n",
      "Train 68 | out_loss 0.45915043354034424: 100%|█| 258/258 [00:01<00:00, 161.25it/\n",
      "Train 69 | out_loss 0.4600113034248352: 100%|█| 258/258 [00:01<00:00, 159.99it/s\n",
      "Train 70 | out_loss 0.456228643655777: 100%|█| 258/258 [00:01<00:00, 161.05it/s]\n",
      "Train 71 | out_loss 0.4626823663711548: 100%|█| 258/258 [00:01<00:00, 160.08it/s\n",
      "Train 72 | out_loss 0.45204198360443115: 100%|█| 258/258 [00:01<00:00, 159.76it/\n",
      "Train 73 | out_loss 0.4584393799304962: 100%|█| 258/258 [00:01<00:00, 153.55it/s\n",
      "Train 74 | out_loss 0.4476684331893921: 100%|█| 258/258 [00:01<00:00, 159.03it/s\n",
      "Train 75 | out_loss 0.45174553990364075: 100%|█| 258/258 [00:01<00:00, 158.08it/\n",
      "Train 76 | out_loss 0.45625656843185425: 100%|█| 258/258 [00:01<00:00, 157.93it/\n",
      "Train 77 | out_loss 0.48728153109550476: 100%|█| 258/258 [00:01<00:00, 159.51it/\n",
      "Train 78 | out_loss 0.4630199074745178: 100%|█| 258/258 [00:01<00:00, 158.09it/s\n",
      "Train 79 | out_loss 1.0303000211715698: 100%|█| 258/258 [00:01<00:00, 157.53it/s\n",
      "Train 80 | out_loss 0.46197399497032166: 100%|█| 258/258 [00:01<00:00, 158.88it/\n",
      "Train 81 | out_loss 0.4510813057422638: 100%|█| 258/258 [00:01<00:00, 157.55it/s\n",
      "Train 82 | out_loss 0.4479656517505646: 100%|█| 258/258 [00:01<00:00, 159.04it/s\n",
      "Train 83 | out_loss 0.4463541805744171: 100%|█| 258/258 [00:01<00:00, 157.64it/s\n",
      "Train 84 | out_loss 0.4486255943775177: 100%|█| 258/258 [00:01<00:00, 154.83it/s\n",
      "Train 85 | out_loss 0.4536258280277252: 100%|█| 258/258 [00:01<00:00, 152.25it/s\n",
      "Train 86 | out_loss 0.4479198455810547: 100%|█| 258/258 [00:01<00:00, 159.12it/s\n",
      "Train 87 | out_loss 0.45433947443962097: 100%|█| 258/258 [00:01<00:00, 157.94it/\n",
      "Train 88 | out_loss 0.44795575737953186: 100%|█| 258/258 [00:01<00:00, 157.94it/\n",
      "Train 89 | out_loss 0.45160603523254395: 100%|█| 258/258 [00:01<00:00, 156.45it/\n",
      "Train 90 | out_loss 0.4449261724948883: 100%|█| 258/258 [00:01<00:00, 152.90it/s\n",
      "Train 91 | out_loss 0.461650013923645: 100%|█| 258/258 [00:01<00:00, 159.61it/s]\n",
      "Train 92 | out_loss 0.45645153522491455: 100%|█| 258/258 [00:01<00:00, 158.99it/\n",
      "Train 93 | out_loss 0.4468669593334198: 100%|█| 258/258 [00:01<00:00, 155.92it/s\n",
      "Train 94 | out_loss 0.4429760277271271: 100%|█| 258/258 [00:01<00:00, 156.47it/s\n",
      "Train 95 | out_loss 0.4513344168663025: 100%|█| 258/258 [00:01<00:00, 159.58it/s\n",
      "Train 96 | out_loss 0.9325043559074402: 100%|█| 258/258 [00:01<00:00, 153.97it/s\n",
      "Train 97 | out_loss 0.45355817675590515: 100%|█| 258/258 [00:01<00:00, 160.24it/\n",
      "Train 98 | out_loss 0.448659211397171: 100%|█| 258/258 [00:01<00:00, 158.86it/s]\n",
      "Train 99 | out_loss 0.44588711857795715: 100%|█| 258/258 [00:01<00:00, 159.22it/\n",
      "  0%|                                                   | 0/258 [00:00<?, ?it/s]/home/AL_main_new/utils.py:99: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  x = torch.tensor([x for x,y in batch], dtype=torch.float32)\n",
      "Train 0 | out_loss 0.767744243144989: 100%|██| 258/258 [00:02<00:00, 119.96it/s]\n",
      "Train 1 | out_loss 0.5986772179603577: 100%|█| 258/258 [00:01<00:00, 145.53it/s]\n",
      "Train 2 | out_loss 0.5867964625358582: 100%|█| 258/258 [00:01<00:00, 147.15it/s]\n",
      "Train 3 | out_loss 0.5749761462211609: 100%|█| 258/258 [00:01<00:00, 146.64it/s]\n",
      "Train 4 | out_loss 0.5471799969673157: 100%|█| 258/258 [00:01<00:00, 141.21it/s]\n",
      "Train 5 | out_loss 0.5586017966270447: 100%|█| 258/258 [00:01<00:00, 146.46it/s]\n",
      "Train 6 | out_loss 0.5410900115966797: 100%|█| 258/258 [00:01<00:00, 147.35it/s]\n",
      "Train 7 | out_loss 0.5371960401535034: 100%|█| 258/258 [00:01<00:00, 144.56it/s]\n",
      "Train 8 | out_loss 0.5349209308624268: 100%|█| 258/258 [00:01<00:00, 145.62it/s]\n",
      "Train 9 | out_loss 0.5286186337471008: 100%|█| 258/258 [00:01<00:00, 143.44it/s]\n",
      "Train 10 | out_loss 0.5208320021629333: 100%|█| 258/258 [00:01<00:00, 146.98it/s\n",
      "Train 11 | out_loss 0.5200412273406982: 100%|█| 258/258 [00:01<00:00, 145.54it/s\n",
      "Train 12 | out_loss 0.5098780393600464: 100%|█| 258/258 [00:01<00:00, 143.58it/s\n",
      "Train 13 | out_loss 0.5091063976287842: 100%|█| 258/258 [00:01<00:00, 142.77it/s\n",
      "Train 14 | out_loss 0.49765416979789734: 100%|█| 258/258 [00:01<00:00, 146.08it/\n",
      "Train 15 | out_loss 0.4953502118587494: 100%|█| 258/258 [00:01<00:00, 144.42it/s\n",
      "Train 16 | out_loss 0.4890812635421753: 100%|█| 258/258 [00:01<00:00, 146.86it/s\n",
      "Train 17 | out_loss 0.4829547107219696: 100%|█| 258/258 [00:01<00:00, 146.22it/s\n",
      "Train 18 | out_loss 0.48245152831077576: 100%|█| 258/258 [00:01<00:00, 142.63it/\n",
      "Train 19 | out_loss 0.494108647108078: 100%|█| 258/258 [00:01<00:00, 144.91it/s]\n",
      "Train 20 | out_loss 0.48255762457847595: 100%|█| 258/258 [00:01<00:00, 146.46it/\n",
      "Train 21 | out_loss 0.4786337614059448: 100%|█| 258/258 [00:01<00:00, 146.53it/s\n",
      "Train 22 | out_loss 0.4790499806404114: 100%|█| 258/258 [00:01<00:00, 148.15it/s\n",
      "Train 23 | out_loss 0.4880744218826294: 100%|█| 258/258 [00:01<00:00, 138.67it/s\n",
      "Train 24 | out_loss 0.4731252193450928: 100%|█| 258/258 [00:01<00:00, 145.56it/s\n",
      "Train 25 | out_loss 0.47258347272872925: 100%|█| 258/258 [00:01<00:00, 145.08it/\n",
      "Train 26 | out_loss 0.48317617177963257: 100%|█| 258/258 [00:01<00:00, 147.14it/\n",
      "Train 27 | out_loss 0.46975505352020264: 100%|█| 258/258 [00:01<00:00, 143.77it/\n",
      "Train 28 | out_loss 0.4639660120010376: 100%|█| 258/258 [00:01<00:00, 142.71it/s\n",
      "Train 29 | out_loss 0.4692082405090332: 100%|█| 258/258 [00:01<00:00, 139.56it/s\n",
      "Train 30 | out_loss 0.469957172870636: 100%|█| 258/258 [00:01<00:00, 144.08it/s]\n",
      "Train 31 | out_loss 0.4669310748577118: 100%|█| 258/258 [00:01<00:00, 145.80it/s\n",
      "Train 32 | out_loss 0.4633665084838867: 100%|█| 258/258 [00:01<00:00, 143.90it/s\n",
      "Train 33 | out_loss 0.46678876876831055: 100%|█| 258/258 [00:01<00:00, 142.11it/\n",
      "Train 34 | out_loss 0.46647512912750244: 100%|█| 258/258 [00:01<00:00, 142.23it/\n",
      "Train 35 | out_loss 0.4688325524330139: 100%|█| 258/258 [00:01<00:00, 146.17it/s\n",
      "Train 36 | out_loss 0.46084120869636536: 100%|█| 258/258 [00:01<00:00, 143.19it/\n",
      "Train 37 | out_loss 0.46002396941185: 100%|██| 258/258 [00:01<00:00, 143.40it/s]\n",
      "Train 38 | out_loss 0.46619537472724915: 100%|█| 258/258 [00:01<00:00, 146.52it/\n",
      "Train 39 | out_loss 0.4795531630516052: 100%|█| 258/258 [00:01<00:00, 145.55it/s\n",
      "Train 40 | out_loss 0.470436155796051: 100%|█| 258/258 [00:01<00:00, 145.10it/s]\n",
      "Train 41 | out_loss 0.4585733711719513: 100%|█| 258/258 [00:01<00:00, 144.13it/s\n",
      "Train 42 | out_loss 0.46722373366355896: 100%|█| 258/258 [00:01<00:00, 140.10it/\n",
      "Train 43 | out_loss 0.46558499336242676: 100%|█| 258/258 [00:01<00:00, 144.00it/\n",
      "Train 44 | out_loss 0.47175851464271545: 100%|█| 258/258 [00:01<00:00, 143.84it/\n",
      "Train 45 | out_loss 0.4713464081287384: 100%|█| 258/258 [00:01<00:00, 145.09it/s\n",
      "Train 46 | out_loss 0.468058317899704: 100%|█| 258/258 [00:01<00:00, 144.59it/s]\n",
      "Train 47 | out_loss 0.4737892746925354: 100%|█| 258/258 [00:01<00:00, 145.10it/s\n",
      "Train 48 | out_loss 0.5308058857917786: 100%|█| 258/258 [00:01<00:00, 143.93it/s\n",
      "Train 49 | out_loss 0.5013020634651184: 100%|█| 258/258 [00:01<00:00, 142.70it/s\n",
      "Train 50 | out_loss 0.7898299098014832: 100%|█| 258/258 [00:01<00:00, 144.36it/s\n",
      "Train 51 | out_loss 0.47522011399269104: 100%|█| 258/258 [00:01<00:00, 143.57it/\n",
      "Train 52 | out_loss 0.48120173811912537: 100%|█| 258/258 [00:01<00:00, 144.49it/\n",
      "Train 53 | out_loss 0.46534231305122375: 100%|█| 258/258 [00:01<00:00, 144.45it/\n",
      "Train 54 | out_loss 0.6095175743103027: 100%|█| 258/258 [00:01<00:00, 144.09it/s\n",
      "Train 55 | out_loss 0.46440649032592773: 100%|█| 258/258 [00:01<00:00, 145.74it/\n",
      "Train 56 | out_loss 0.4636293053627014: 100%|█| 258/258 [00:01<00:00, 141.62it/s\n",
      "Train 57 | out_loss 0.46520841121673584: 100%|█| 258/258 [00:01<00:00, 144.27it/\n",
      "Train 58 | out_loss 0.4757814407348633: 100%|█| 258/258 [00:01<00:00, 146.32it/s\n",
      "Train 59 | out_loss 0.4771530330181122: 100%|█| 258/258 [00:01<00:00, 146.76it/s\n",
      "Train 60 | out_loss 1.0069268941879272: 100%|█| 258/258 [00:01<00:00, 146.40it/s\n",
      "Train 61 | out_loss 0.4744756519794464: 100%|█| 258/258 [00:01<00:00, 144.28it/s\n",
      "Train 62 | out_loss 0.4595038592815399: 100%|█| 258/258 [00:01<00:00, 143.68it/s\n",
      "Train 63 | out_loss 0.4559110999107361: 100%|█| 258/258 [00:01<00:00, 145.47it/s\n",
      "Train 64 | out_loss 0.48724937438964844: 100%|█| 258/258 [00:01<00:00, 146.15it/\n",
      "Train 65 | out_loss 0.49840086698532104: 100%|█| 258/258 [00:01<00:00, 144.80it/\n",
      "Train 66 | out_loss 0.4583536684513092: 100%|█| 258/258 [00:01<00:00, 145.03it/s\n",
      "Train 67 | out_loss 0.4753628075122833: 100%|█| 258/258 [00:01<00:00, 144.55it/s\n",
      "Train 68 | out_loss 0.48365452885627747: 100%|█| 258/258 [00:01<00:00, 140.77it/\n",
      "Train 69 | out_loss 0.4687981605529785: 100%|█| 258/258 [00:01<00:00, 140.33it/s\n",
      "Train 70 | out_loss 0.483833372592926: 100%|█| 258/258 [00:01<00:00, 144.87it/s]\n",
      "Train 71 | out_loss 1.2191048860549927: 100%|█| 258/258 [00:01<00:00, 142.78it/s\n",
      "Train 72 | out_loss 0.44296887516975403: 100%|█| 258/258 [00:01<00:00, 144.62it/\n",
      "Train 73 | out_loss 0.4418734312057495: 100%|█| 258/258 [00:01<00:00, 146.34it/s\n",
      "Train 74 | out_loss 0.4425491988658905: 100%|█| 258/258 [00:01<00:00, 144.30it/s\n",
      "Train 75 | out_loss 0.44443705677986145: 100%|█| 258/258 [00:01<00:00, 145.73it/\n",
      "Train 76 | out_loss 0.5265675783157349: 100%|█| 258/258 [00:01<00:00, 143.04it/s\n",
      "Train 77 | out_loss 0.4598352313041687: 100%|█| 258/258 [00:01<00:00, 145.14it/s\n",
      "Train 78 | out_loss 0.6724621653556824: 100%|█| 258/258 [00:01<00:00, 143.71it/s\n",
      "Train 79 | out_loss 0.45817944407463074: 100%|█| 258/258 [00:01<00:00, 145.66it/\n",
      "Train 80 | out_loss 0.4716896712779999: 100%|█| 258/258 [00:01<00:00, 139.19it/s\n",
      "Train 81 | out_loss 0.45897287130355835: 100%|█| 258/258 [00:01<00:00, 141.63it/\n",
      "Train 82 | out_loss 0.4597758948802948: 100%|█| 258/258 [00:01<00:00, 145.54it/s\n",
      "Train 83 | out_loss 0.45244356989860535: 100%|█| 258/258 [00:01<00:00, 143.66it/\n",
      "Train 84 | out_loss 1.2118085622787476: 100%|█| 258/258 [00:01<00:00, 139.12it/s\n",
      "Train 85 | out_loss 0.4890061616897583: 100%|█| 258/258 [00:01<00:00, 141.90it/s\n",
      "Train 86 | out_loss 0.4936126172542572: 100%|█| 258/258 [00:01<00:00, 145.18it/s\n",
      "Train 87 | out_loss 0.4695182740688324: 100%|█| 258/258 [00:01<00:00, 143.28it/s\n",
      "Train 88 | out_loss 0.47050127387046814: 100%|█| 258/258 [00:01<00:00, 141.96it/\n",
      "Train 89 | out_loss 0.45466378331184387: 100%|█| 258/258 [00:01<00:00, 143.84it/\n",
      "Train 90 | out_loss 0.45109859108924866: 100%|█| 258/258 [00:01<00:00, 145.28it/\n",
      "Train 91 | out_loss 0.45801565051078796: 100%|█| 258/258 [00:01<00:00, 143.07it/\n",
      "Train 92 | out_loss 0.4583328664302826: 100%|█| 258/258 [00:01<00:00, 138.98it/s\n",
      "Train 93 | out_loss 0.46161285042762756: 100%|█| 258/258 [00:01<00:00, 140.16it/\n",
      "Train 94 | out_loss 0.477092444896698: 100%|█| 258/258 [00:01<00:00, 145.56it/s]\n",
      "Train 95 | out_loss 0.45931315422058105: 100%|█| 258/258 [00:01<00:00, 143.87it/\n",
      "Train 96 | out_loss 0.45175501704216003: 100%|█| 258/258 [00:01<00:00, 144.53it/\n",
      "Train 97 | out_loss 0.47175705432891846: 100%|█| 258/258 [00:01<00:00, 143.94it/\n",
      "Train 98 | out_loss 0.4447093904018402: 100%|█| 258/258 [00:01<00:00, 145.74it/s\n",
      "Train 99 | out_loss 0.44950810074806213: 100%|█| 258/258 [00:01<00:00, 145.13it/\n"
     ]
    }
   ],
   "source": [
    "# LinearAL \n",
    "\n",
    "data = \"ca_housing\"\n",
    "model =  \"linearal\"\n",
    "for layer in range(1,11):\n",
    "#for layer in [5]:\n",
    "    log = f\"result/{data}_{model}_l{layer}.log\"\n",
    "    !python3 dis_train_al.py --dataset {data} --model {model} --epoch 100 --num-layer {layer} --task regression  > {log}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6ae70865",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/AL_main_new/new dataset.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B140.115.59.235/home/AL_main_new/new%20dataset.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B140.115.59.235/home/AL_main_new/new%20dataset.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B140.115.59.235/home/AL_main_new/new%20dataset.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m args\u001b[39m.\u001b[39mtask \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mregression\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B140.115.59.235/home/AL_main_new/new%20dataset.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m args\u001b[39m.\u001b[39mfeature_dim \u001b[39m=\u001b[39m \u001b[39m40\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B140.115.59.235/home/AL_main_new/new%20dataset.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m df \u001b[39m=\u001b[39m load_dataset(\u001b[39m'\u001b[39m\u001b[39mLL0_296_ailerons\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "from mit_d3m import load_dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "args.task = \"regression\"\n",
    "args.feature_dim = 40\n",
    "df = load_dataset('LL0_296_ailerons')\n",
    "\n",
    "col_feature = df.X.columns[1:]\n",
    "#col_target= df.y.columns[:]\n",
    "\n",
    "y = df.y\n",
    "x = df.X[col_feature]\n",
    "x = x.fillna(0)\n",
    "feature_train, feature_test, train_target, test_target = train_test_split(x, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "833b414e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8800,)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target.to_numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7675b11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training\n",
      "  0%|                                                   | 0/125 [00:00<?, ?it/s]/home/AL_main_new/utils.py:99: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  x = torch.tensor([x for x,y in batch], dtype=torch.float32)\n",
      "Train 0 | out_loss 0.44186654686927795: 100%|█| 125/125 [00:00<00:00, 150.58it/s\n",
      "Train Epoch0 out_loss 0.19524604082107544, R2 -0.14529931545257568\n",
      "Test Epoch0 layer0 out_loss 0.18002794682979584, R2 -0.052554965019226074\n",
      "Test Epoch0 layer1 out_loss 0.1784631460905075, R2 -0.043406128883361816\n",
      "Test Epoch0 layer2 out_loss 0.1811799854040146, R2 -0.059290528297424316\n",
      "Test Epoch0 layer3 out_loss 0.18376043438911438, R2 -0.07437741756439209\n",
      "Test Epoch0 layer4 out_loss 0.18444551527500153, R2 -0.07838273048400879\n",
      "Train 1 | out_loss 0.4164918065071106: 100%|█| 125/125 [00:00<00:00, 268.66it/s]\n",
      "Train Epoch1 out_loss 0.17346543073654175, R2 -0.017535805702209473\n",
      "Test Epoch1 layer0 out_loss 0.17081891000270844, R2 0.0012868046760559082\n",
      "Test Epoch1 layer1 out_loss 0.17120790481567383, R2 -0.0009875297546386719\n",
      "Test Epoch1 layer2 out_loss 0.17183788120746613, R2 -0.00467073917388916\n",
      "Test Epoch1 layer3 out_loss 0.1721288561820984, R2 -0.006371974945068359\n",
      "Test Epoch1 layer4 out_loss 0.17247378826141357, R2 -0.008388638496398926\n",
      "Train 2 | out_loss 0.4132488965988159: 100%|█| 125/125 [00:00<00:00, 272.60it/s]\n",
      "Train Epoch2 out_loss 0.1707746386528015, R2 -0.0017517805099487305\n",
      "Test Epoch2 layer0 out_loss 0.16990621387958527, R2 0.006622970104217529\n",
      "Test Epoch2 layer1 out_loss 0.17030492424964905, R2 0.004291832447052002\n",
      "Test Epoch2 layer2 out_loss 0.17076623439788818, R2 0.0015947818756103516\n",
      "Test Epoch2 layer3 out_loss 0.17067107558250427, R2 0.0021510720252990723\n",
      "Test Epoch2 layer4 out_loss 0.1708938330411911, R2 0.0008487701416015625\n",
      "Train 3 | out_loss 0.4129578173160553: 100%|█| 125/125 [00:00<00:00, 273.77it/s]\n",
      "Train Epoch3 out_loss 0.1705341786146164, R2 -0.0003412961959838867\n",
      "Test Epoch3 layer0 out_loss 0.1694541871547699, R2 0.009265780448913574\n",
      "Test Epoch3 layer1 out_loss 0.170181542634964, R2 0.005013227462768555\n",
      "Test Epoch3 layer2 out_loss 0.17106495797634125, R2 -0.00015175342559814453\n",
      "Test Epoch3 layer3 out_loss 0.1711731106042862, R2 -0.0007840394973754883\n",
      "Test Epoch3 layer4 out_loss 0.17142745852470398, R2 -0.0022711753845214844\n",
      "Train 4 | out_loss 0.413339763879776: 100%|██| 125/125 [00:00<00:00, 274.55it/s]\n",
      "Train Epoch4 out_loss 0.1708497256040573, R2 -0.002192258834838867\n",
      "Test Epoch4 layer0 out_loss 0.16891014575958252, R2 0.01244664192199707\n",
      "Test Epoch4 layer1 out_loss 0.16973409056663513, R2 0.007629334926605225\n",
      "Test Epoch4 layer2 out_loss 0.17103223502635956, R2 3.9517879486083984e-05\n",
      "Test Epoch4 layer3 out_loss 0.17113801836967468, R2 -0.0005788803100585938\n",
      "Test Epoch4 layer4 out_loss 0.17131827771663666, R2 -0.0016328096389770508\n",
      "Train 5 | out_loss 0.4129824638366699: 100%|█| 125/125 [00:00<00:00, 252.05it/s]\n",
      "Train Epoch5 out_loss 0.17055444419384003, R2 -0.0004601478576660156\n",
      "Test Epoch5 layer0 out_loss 0.16847173869609833, R2 0.015009820461273193\n",
      "Test Epoch5 layer1 out_loss 0.16908004879951477, R2 0.01145327091217041\n",
      "Test Epoch5 layer2 out_loss 0.17056967318058014, R2 0.002744019031524658\n",
      "Test Epoch5 layer3 out_loss 0.17081795632839203, R2 0.0012923479080200195\n",
      "Test Epoch5 layer4 out_loss 0.17103280127048492, R2 3.62396240234375e-05\n",
      "Train 6 | out_loss 0.4129266142845154: 100%|█| 125/125 [00:00<00:00, 277.97it/s]\n",
      "Train Epoch6 out_loss 0.17050841450691223, R2 -0.0001901388168334961\n",
      "Test Epoch6 layer0 out_loss 0.16802608966827393, R2 0.017615318298339844\n",
      "Test Epoch6 layer1 out_loss 0.16852571070194244, R2 0.014694273471832275\n",
      "Test Epoch6 layer2 out_loss 0.17030224204063416, R2 0.00430750846862793\n",
      "Test Epoch6 layer3 out_loss 0.1706707924604416, R2 0.0021528005599975586\n",
      "Test Epoch6 layer4 out_loss 0.17094756662845612, R2 0.0005345344543457031\n",
      "Train 7 | out_loss 0.41293269395828247: 100%|█| 125/125 [00:00<00:00, 274.53it/s\n",
      "Train Epoch7 out_loss 0.17051339149475098, R2 -0.0002193450927734375\n",
      "Test Epoch7 layer0 out_loss 0.1675797700881958, R2 0.020224809646606445\n",
      "Test Epoch7 layer1 out_loss 0.16791954636573792, R2 0.018238306045532227\n",
      "Test Epoch7 layer2 out_loss 0.1700836569070816, R2 0.00558549165725708\n",
      "Test Epoch7 layer3 out_loss 0.1708071380853653, R2 0.001355588436126709\n",
      "Test Epoch7 layer4 out_loss 0.17108991742134094, R2 -0.0002976655960083008\n",
      "Train 8 | out_loss 0.4129439890384674: 100%|█| 125/125 [00:00<00:00, 274.78it/s]\n",
      "Train Epoch8 out_loss 0.17052271962165833, R2 -0.0002740621566772461\n",
      "Test Epoch8 layer0 out_loss 0.16712641716003418, R2 0.02287536859512329\n",
      "Test Epoch8 layer1 out_loss 0.1670684963464737, R2 0.02321404218673706\n",
      "Test Epoch8 layer2 out_loss 0.16945087909698486, R2 0.009285151958465576\n",
      "Test Epoch8 layer3 out_loss 0.17043502628803253, R2 0.003531217575073242\n",
      "Test Epoch8 layer4 out_loss 0.17091722786426544, R2 0.000711977481842041\n",
      "Train 9 | out_loss 0.4127190113067627: 100%|█| 125/125 [00:00<00:00, 273.99it/s]\n",
      "Train Epoch9 out_loss 0.17033696174621582, R2 0.0008155703544616699\n",
      "Test Epoch9 layer0 out_loss 0.16673430800437927, R2 0.025167882442474365\n",
      "Test Epoch9 layer1 out_loss 0.1663234531879425, R2 0.027570009231567383\n",
      "Test Epoch9 layer2 out_loss 0.1688525378704071, R2 0.012783408164978027\n",
      "Test Epoch9 layer3 out_loss 0.17021675407886505, R2 0.004807412624359131\n",
      "Test Epoch9 layer4 out_loss 0.17099426686763763, R2 0.00026154518127441406\n",
      "Train 10 | out_loss 0.4125986397266388: 100%|█| 125/125 [00:00<00:00, 273.59it/s\n",
      "Train Epoch10 out_loss 0.17023761570453644, R2 0.0013983845710754395\n",
      "Test Epoch10 layer0 out_loss 0.16620615124702454, R2 0.028255879878997803\n",
      "Test Epoch10 layer1 out_loss 0.16532033681869507, R2 0.03343480825424194\n",
      "Test Epoch10 layer2 out_loss 0.16780200600624084, R2 0.018925487995147705\n",
      "Test Epoch10 layer3 out_loss 0.1696806699037552, R2 0.007941663265228271\n",
      "Test Epoch10 layer4 out_loss 0.17081016302108765, R2 0.0013379454612731934\n",
      "Train 11 | out_loss 0.4121149480342865: 100%|█| 125/125 [00:00<00:00, 274.19it/s\n",
      "Train Epoch11 out_loss 0.16983874142169952, R2 0.003738105297088623\n",
      "Test Epoch11 layer0 out_loss 0.1657034307718277, R2 0.03119504451751709\n",
      "Test Epoch11 layer1 out_loss 0.16494494676589966, R2 0.03562963008880615\n",
      "Test Epoch11 layer2 out_loss 0.16752442717552185, R2 0.020548343658447266\n",
      "Test Epoch11 layer3 out_loss 0.1695793867111206, R2 0.008533775806427002\n",
      "Test Epoch11 layer4 out_loss 0.17055396735668182, R2 0.0028358101844787598\n",
      "Train 12 | out_loss 0.41117626428604126: 100%|█| 125/125 [00:00<00:00, 274.81it/\n",
      "Train Epoch12 out_loss 0.1690659373998642, R2 0.008271336555480957\n",
      "Test Epoch12 layer0 out_loss 0.16522695124149323, R2 0.03398090600967407\n",
      "Test Epoch12 layer1 out_loss 0.16385146975517273, R2 0.042022764682769775\n",
      "Test Epoch12 layer2 out_loss 0.16539135575294495, R2 0.03301960229873657\n",
      "Test Epoch12 layer3 out_loss 0.16655844449996948, R2 0.02619612216949463\n",
      "Test Epoch12 layer4 out_loss 0.1687655746936798, R2 0.01329183578491211\n",
      "Train 13 | out_loss 0.40939658880233765: 100%|█| 125/125 [00:00<00:00, 268.02it/\n",
      "Train Epoch13 out_loss 0.16760556399822235, R2 0.016837775707244873\n",
      "Test Epoch13 layer0 out_loss 0.1647605150938034, R2 0.036707937717437744\n",
      "Test Epoch13 layer1 out_loss 0.1629031002521515, R2 0.04756748676300049\n",
      "Test Epoch13 layer2 out_loss 0.16356715559959412, R2 0.043685078620910645\n",
      "Test Epoch13 layer3 out_loss 0.16391433775424957, R2 0.04165518283843994\n",
      "Test Epoch13 layer4 out_loss 0.16596077382564545, R2 0.02969050407409668\n",
      "Train 14 | out_loss 0.4061358869075775: 100%|█| 125/125 [00:00<00:00, 277.33it/s\n",
      "Train Epoch14 out_loss 0.1649462878704071, R2 0.03243684768676758\n",
      "Test Epoch14 layer0 out_loss 0.1643950343132019, R2 0.0388447642326355\n",
      "Test Epoch14 layer1 out_loss 0.1624908596277237, R2 0.04997771978378296\n",
      "Test Epoch14 layer2 out_loss 0.16350699961185455, R2 0.04403674602508545\n",
      "Test Epoch14 layer3 out_loss 0.16480156779289246, R2 0.03646785020828247\n",
      "Test Epoch14 layer4 out_loss 0.1648680567741394, R2 0.03607916831970215\n",
      "Train 15 | out_loss 0.40519723296165466: 100%|█| 125/125 [00:00<00:00, 272.85it/\n",
      "Train Epoch15 out_loss 0.1641847938299179, R2 0.0369037389755249\n",
      "Test Epoch15 layer0 out_loss 0.1638999581336975, R2 0.04173922538757324\n",
      "Test Epoch15 layer1 out_loss 0.16213127970695496, R2 0.05208003520965576\n",
      "Test Epoch15 layer2 out_loss 0.16213977336883545, R2 0.05203044414520264\n",
      "Test Epoch15 layer3 out_loss 0.16197696328163147, R2 0.052982330322265625\n",
      "Test Epoch15 layer4 out_loss 0.1622534990310669, R2 0.05136549472808838\n",
      "Train 16 | out_loss 0.4031573534011841: 100%|█| 125/125 [00:00<00:00, 268.83it/s\n",
      "Train Epoch16 out_loss 0.1625359058380127, R2 0.04657602310180664\n",
      "Test Epoch16 layer0 out_loss 0.1635390669107437, R2 0.043849289417266846\n",
      "Test Epoch16 layer1 out_loss 0.1618937849998474, R2 0.05346858501434326\n",
      "Test Epoch16 layer2 out_loss 0.16189080476760864, R2 0.053485989570617676\n",
      "Test Epoch16 layer3 out_loss 0.161955326795578, R2 0.053108811378479004\n",
      "Test Epoch16 layer4 out_loss 0.16194592416286469, R2 0.053163766860961914\n",
      "Train 17 | out_loss 0.4039100706577301: 100%|█| 125/125 [00:00<00:00, 261.07it/s\n",
      "Train Epoch17 out_loss 0.16314329206943512, R2 0.04301309585571289\n",
      "Test Epoch17 layer0 out_loss 0.16320142149925232, R2 0.04582339525222778\n",
      "Test Epoch17 layer1 out_loss 0.16181185841560364, R2 0.0539475679397583\n",
      "Test Epoch17 layer2 out_loss 0.16194424033164978, R2 0.053173601627349854\n",
      "Test Epoch17 layer3 out_loss 0.16189150512218475, R2 0.053481876850128174\n",
      "Test Epoch17 layer4 out_loss 0.1617134064435959, R2 0.05452316999435425\n",
      "Train 18 | out_loss 0.4028063416481018: 100%|█| 125/125 [00:00<00:00, 271.23it/s\n",
      "Train Epoch18 out_loss 0.16225293278694153, R2 0.048235952854156494\n",
      "Test Epoch18 layer0 out_loss 0.1630275994539261, R2 0.04683959484100342\n",
      "Test Epoch18 layer1 out_loss 0.16162431240081787, R2 0.05504411458969116\n",
      "Test Epoch18 layer2 out_loss 0.16160385310649872, R2 0.05516374111175537\n",
      "Test Epoch18 layer3 out_loss 0.16177551448345184, R2 0.054160118103027344\n",
      "Test Epoch18 layer4 out_loss 0.1618354618549347, R2 0.05380958318710327\n",
      "Train 19 | out_loss 0.40233904123306274: 100%|█| 125/125 [00:00<00:00, 271.86it/\n",
      "Train Epoch19 out_loss 0.16187670826911926, R2 0.05044281482696533\n",
      "Test Epoch19 layer0 out_loss 0.16264083981513977, R2 0.04910087585449219\n",
      "Test Epoch19 layer1 out_loss 0.16177280247211456, R2 0.0541759729385376\n",
      "Test Epoch19 layer2 out_loss 0.1624218374490738, R2 0.050381243228912354\n",
      "Test Epoch19 layer3 out_loss 0.1632385402917862, R2 0.04560631513595581\n",
      "Test Epoch19 layer4 out_loss 0.16336935758590698, R2 0.04484152793884277\n",
      "Train 20 | out_loss 0.4031536281108856: 100%|█| 125/125 [00:00<00:00, 276.98it/s\n",
      "Train Epoch20 out_loss 0.16253286600112915, R2 0.04659384489059448\n",
      "Test Epoch20 layer0 out_loss 0.16246289014816284, R2 0.05014127492904663\n",
      "Test Epoch20 layer1 out_loss 0.1617361158132553, R2 0.05439043045043945\n",
      "Test Epoch20 layer2 out_loss 0.16179198026657104, R2 0.05406385660171509\n",
      "Test Epoch20 layer3 out_loss 0.1619470715522766, R2 0.05315709114074707\n",
      "Test Epoch20 layer4 out_loss 0.16199450194835663, R2 0.0528796911239624\n",
      "Train 21 | out_loss 0.4025764763355255: 100%|█| 125/125 [00:00<00:00, 274.09it/s\n",
      "Train Epoch21 out_loss 0.16206781566143036, R2 0.04932183027267456\n",
      "Test Epoch21 layer0 out_loss 0.16220994293689728, R2 0.05162012577056885\n",
      "Test Epoch21 layer1 out_loss 0.16145408153533936, R2 0.05603933334350586\n",
      "Test Epoch21 layer2 out_loss 0.16132478415966034, R2 0.056795358657836914\n",
      "Test Epoch21 layer3 out_loss 0.16124531626701355, R2 0.057259976863861084\n",
      "Test Epoch21 layer4 out_loss 0.16112802922725677, R2 0.05794566869735718\n",
      "Train 22 | out_loss 0.40311765670776367: 100%|█| 125/125 [00:00<00:00, 268.74it/\n",
      "Train Epoch22 out_loss 0.16250383853912354, R2 0.046764075756073\n",
      "Test Epoch22 layer0 out_loss 0.1620364636182785, R2 0.052634358406066895\n",
      "Test Epoch22 layer1 out_loss 0.16135980188846588, R2 0.05659061670303345\n",
      "Test Epoch22 layer2 out_loss 0.16125522553920746, R2 0.057202041149139404\n",
      "Test Epoch22 layer3 out_loss 0.16118374466896057, R2 0.05761992931365967\n",
      "Test Epoch22 layer4 out_loss 0.16106179356575012, R2 0.05833292007446289\n",
      "Train 23 | out_loss 0.40315866470336914: 100%|█| 125/125 [00:00<00:00, 268.49it/\n",
      "Train Epoch23 out_loss 0.1625368893146515, R2 0.04657018184661865\n",
      "Test Epoch23 layer0 out_loss 0.16189880669116974, R2 0.05343925952911377\n",
      "Test Epoch23 layer1 out_loss 0.16138111054897308, R2 0.05646604299545288\n",
      "Test Epoch23 layer2 out_loss 0.16129064559936523, R2 0.05699491500854492\n",
      "Test Epoch23 layer3 out_loss 0.16118782758712769, R2 0.05759608745574951\n",
      "Test Epoch23 layer4 out_loss 0.1611069291830063, R2 0.058069050312042236\n",
      "Train 24 | out_loss 0.40303394198417664: 100%|█| 125/125 [00:00<00:00, 274.73it/\n",
      "Train Epoch24 out_loss 0.1624363362789154, R2 0.04716002941131592\n",
      "Test Epoch24 layer0 out_loss 0.16176842153072357, R2 0.054201602935791016\n",
      "Test Epoch24 layer1 out_loss 0.16132745146751404, R2 0.05677974224090576\n",
      "Test Epoch24 layer2 out_loss 0.16119800508022308, R2 0.05753660202026367\n",
      "Test Epoch24 layer3 out_loss 0.1610867977142334, R2 0.05818676948547363\n",
      "Test Epoch24 layer4 out_loss 0.16095642745494843, R2 0.05894899368286133\n",
      "Train 25 | out_loss 0.4022492468357086: 100%|█| 125/125 [00:00<00:00, 274.29it/s\n",
      "Train Epoch25 out_loss 0.1618044376373291, R2 0.05086672306060791\n",
      "Test Epoch25 layer0 out_loss 0.16170191764831543, R2 0.05459034442901611\n",
      "Test Epoch25 layer1 out_loss 0.1613481193780899, R2 0.056658923625946045\n",
      "Test Epoch25 layer2 out_loss 0.16115112602710724, R2 0.05781066417694092\n",
      "Test Epoch25 layer3 out_loss 0.16105706989765167, R2 0.05836057662963867\n",
      "Test Epoch25 layer4 out_loss 0.16090333461761475, R2 0.05925935506820679\n",
      "Train 26 | out_loss 0.40136539936065674: 100%|█| 125/125 [00:00<00:00, 277.93it/\n",
      "Train Epoch26 out_loss 0.16109418869018555, R2 0.05503302812576294\n",
      "Test Epoch26 layer0 out_loss 0.16162694990634918, R2 0.055028676986694336\n",
      "Test Epoch26 layer1 out_loss 0.1618565171957016, R2 0.053686439990997314\n",
      "Test Epoch26 layer2 out_loss 0.16242003440856934, R2 0.05039185285568237\n",
      "Test Epoch26 layer3 out_loss 0.16283108294010162, R2 0.04798859357833862\n",
      "Test Epoch26 layer4 out_loss 0.16286581754684448, R2 0.04778552055358887\n",
      "Train 27 | out_loss 0.4017179012298584: 100%|█| 125/125 [00:00<00:00, 266.77it/s\n",
      "Train Epoch27 out_loss 0.1613771915435791, R2 0.05337291955947876\n",
      "Test Epoch27 layer0 out_loss 0.1615711748600006, R2 0.05535471439361572\n",
      "Test Epoch27 layer1 out_loss 0.16124127805233002, R2 0.05728358030319214\n",
      "Test Epoch27 layer2 out_loss 0.16105255484580994, R2 0.05838698148727417\n",
      "Test Epoch27 layer3 out_loss 0.1609663963317871, R2 0.05889064073562622\n",
      "Test Epoch27 layer4 out_loss 0.16077035665512085, R2 0.06003689765930176\n",
      "Train 28 | out_loss 0.40173277258872986: 100%|█| 125/125 [00:00<00:00, 226.87it/\n",
      "Train Epoch28 out_loss 0.16138923168182373, R2 0.05330228805541992\n",
      "Test Epoch28 layer0 out_loss 0.1615392416715622, R2 0.05554145574569702\n",
      "Test Epoch28 layer1 out_loss 0.1614912748336792, R2 0.05582195520401001\n",
      "Test Epoch28 layer2 out_loss 0.16132274270057678, R2 0.05680727958679199\n",
      "Test Epoch28 layer3 out_loss 0.16121043264865875, R2 0.057463884353637695\n",
      "Test Epoch28 layer4 out_loss 0.16098694503307343, R2 0.058770596981048584\n",
      "Train 29 | out_loss 0.40160155296325684: 100%|█| 125/125 [00:00<00:00, 278.69it/\n",
      "Train Epoch29 out_loss 0.16128382086753845, R2 0.0539206862449646\n",
      "Test Epoch29 layer0 out_loss 0.16147130727767944, R2 0.055938661098480225\n",
      "Test Epoch29 layer1 out_loss 0.1612541824579239, R2 0.057208120822906494\n",
      "Test Epoch29 layer2 out_loss 0.16127566993236542, R2 0.057082533836364746\n",
      "Test Epoch29 layer3 out_loss 0.16161112487316132, R2 0.05512118339538574\n",
      "Test Epoch29 layer4 out_loss 0.161681666970253, R2 0.054708778858184814\n",
      "Train 30 | out_loss 0.40135815739631653: 100%|█| 125/125 [00:00<00:00, 268.97it/\n",
      "Train Epoch30 out_loss 0.16108837723731995, R2 0.05506712198257446\n",
      "Test Epoch30 layer0 out_loss 0.1613789051771164, R2 0.056478917598724365\n",
      "Test Epoch30 layer1 out_loss 0.16144604980945587, R2 0.05608636140823364\n",
      "Test Epoch30 layer2 out_loss 0.16159886121749878, R2 0.05519288778305054\n",
      "Test Epoch30 layer3 out_loss 0.1617719829082489, R2 0.054180681705474854\n",
      "Test Epoch30 layer4 out_loss 0.1616826206445694, R2 0.0547032356262207\n",
      "Train 31 | out_loss 0.4011436700820923: 100%|█| 125/125 [00:00<00:00, 280.76it/s\n",
      "Train Epoch31 out_loss 0.1609162837266922, R2 0.05607658624649048\n",
      "Test Epoch31 layer0 out_loss 0.16135938465595245, R2 0.05659300088882446\n",
      "Test Epoch31 layer1 out_loss 0.16141892969608307, R2 0.05624490976333618\n",
      "Test Epoch31 layer2 out_loss 0.16130179166793823, R2 0.05692976713180542\n",
      "Test Epoch31 layer3 out_loss 0.16112247109413147, R2 0.05797821283340454\n",
      "Test Epoch31 layer4 out_loss 0.16084018349647522, R2 0.05962860584259033\n",
      "Train 32 | out_loss 0.4014985263347626: 100%|█| 125/125 [00:00<00:00, 276.53it/s\n",
      "Train Epoch32 out_loss 0.16120107471942902, R2 0.05440598726272583\n",
      "Test Epoch32 layer0 out_loss 0.16131706535816193, R2 0.056840479373931885\n",
      "Test Epoch32 layer1 out_loss 0.1611764281988144, R2 0.0576627254486084\n",
      "Test Epoch32 layer2 out_loss 0.16097888350486755, R2 0.05881768465042114\n",
      "Test Epoch32 layer3 out_loss 0.1608588695526123, R2 0.05951935052871704\n",
      "Test Epoch32 layer4 out_loss 0.16062268614768982, R2 0.06090027093887329\n",
      "Train 33 | out_loss 0.4012033939361572: 100%|█| 125/125 [00:00<00:00, 274.30it/s\n",
      "Train Epoch33 out_loss 0.16096414625644684, R2 0.05579584836959839\n",
      "Test Epoch33 layer0 out_loss 0.16129976511001587, R2 0.05694162845611572\n",
      "Test Epoch33 layer1 out_loss 0.16190893948078156, R2 0.05338001251220703\n",
      "Test Epoch33 layer2 out_loss 0.1629628688097, R2 0.04721802473068237\n",
      "Test Epoch33 layer3 out_loss 0.16440799832344055, R2 0.038768887519836426\n",
      "Test Epoch33 layer4 out_loss 0.16504701972007751, R2 0.03503286838531494\n",
      "Train 34 | out_loss 0.40117964148521423: 100%|█| 125/125 [00:00<00:00, 277.37it/\n",
      "Train Epoch34 out_loss 0.1609450876712799, R2 0.05590766668319702\n",
      "Test Epoch34 layer0 out_loss 0.16139385104179382, R2 0.05639153718948364\n",
      "Test Epoch34 layer1 out_loss 0.16229386627674103, R2 0.05112946033477783\n",
      "Test Epoch34 layer2 out_loss 0.16301223635673523, R2 0.04692941904067993\n",
      "Test Epoch34 layer3 out_loss 0.16366513073444366, R2 0.043112218379974365\n",
      "Test Epoch34 layer4 out_loss 0.1638302057981491, R2 0.04214709997177124\n",
      "Train 35 | out_loss 0.40148305892944336: 100%|█| 125/125 [00:00<00:00, 275.29it/\n",
      "Train Epoch35 out_loss 0.16118863224983215, R2 0.05447906255722046\n",
      "Test Epoch35 layer0 out_loss 0.16126902401447296, R2 0.057121336460113525\n",
      "Test Epoch35 layer1 out_loss 0.16115964949131012, R2 0.057760775089263916\n",
      "Test Epoch35 layer2 out_loss 0.16115914285182953, R2 0.057763755321502686\n",
      "Test Epoch35 layer3 out_loss 0.16168604791164398, R2 0.054683148860931396\n",
      "Test Epoch35 layer4 out_loss 0.1617884635925293, R2 0.05408436059951782\n",
      "Train 36 | out_loss 0.40111589431762695: 100%|█| 125/125 [00:00<00:00, 272.36it/\n",
      "Train Epoch36 out_loss 0.16089396178722382, R2 0.05620753765106201\n",
      "Test Epoch36 layer0 out_loss 0.1612812876701355, R2 0.057049691677093506\n",
      "Test Epoch36 layer1 out_loss 0.161119744181633, R2 0.057994067668914795\n",
      "Test Epoch36 layer2 out_loss 0.16087733209133148, R2 0.05941140651702881\n",
      "Test Epoch36 layer3 out_loss 0.16074588894844055, R2 0.06017988920211792\n",
      "Test Epoch36 layer4 out_loss 0.16051562130451202, R2 0.06152617931365967\n",
      "Train 37 | out_loss 0.40099242329597473: 100%|█| 125/125 [00:00<00:00, 277.27it/\n",
      "Train Epoch37 out_loss 0.16079488396644592, R2 0.05678868293762207\n",
      "Test Epoch37 layer0 out_loss 0.16130639612674713, R2 0.05690282583236694\n",
      "Test Epoch37 layer1 out_loss 0.16140107810497284, R2 0.05634927749633789\n",
      "Test Epoch37 layer2 out_loss 0.16107097268104553, R2 0.05827927589416504\n",
      "Test Epoch37 layer3 out_loss 0.16081808507442474, R2 0.05975782871246338\n",
      "Test Epoch37 layer4 out_loss 0.16056104004383087, R2 0.06126070022583008\n",
      "Train 38 | out_loss 0.4005983769893646: 100%|█| 125/125 [00:00<00:00, 272.56it/s\n",
      "Train Epoch38 out_loss 0.16047900915145874, R2 0.058641672134399414\n",
      "Test Epoch38 layer0 out_loss 0.1612275093793869, R2 0.057364046573638916\n",
      "Test Epoch38 layer1 out_loss 0.1611834168434143, R2 0.057621896266937256\n",
      "Test Epoch38 layer2 out_loss 0.16112805902957916, R2 0.05794554948806763\n",
      "Test Epoch38 layer3 out_loss 0.1613805592060089, R2 0.05646926164627075\n",
      "Test Epoch38 layer4 out_loss 0.16140909492969513, R2 0.056302428245544434\n",
      "Train 39 | out_loss 0.400621235370636: 100%|█| 125/125 [00:00<00:00, 275.64it/s]\n",
      "Train Epoch39 out_loss 0.16049739718437195, R2 0.05853378772735596\n",
      "Test Epoch39 layer0 out_loss 0.16125820577144623, R2 0.057184576988220215\n",
      "Test Epoch39 layer1 out_loss 0.16123713552951813, R2 0.05730777978897095\n",
      "Test Epoch39 layer2 out_loss 0.16098091006278992, R2 0.05880582332611084\n",
      "Test Epoch39 layer3 out_loss 0.160817950963974, R2 0.05975860357284546\n",
      "Test Epoch39 layer4 out_loss 0.160577192902565, R2 0.061166226863861084\n",
      "Train 40 | out_loss 0.4010656177997589: 100%|█| 125/125 [00:00<00:00, 277.93it/s\n",
      "Train Epoch40 out_loss 0.16085363924503326, R2 0.05644404888153076\n",
      "Test Epoch40 layer0 out_loss 0.16123969852924347, R2 0.05729275941848755\n",
      "Test Epoch40 layer1 out_loss 0.16125693917274475, R2 0.05719196796417236\n",
      "Test Epoch40 layer2 out_loss 0.16107507050037384, R2 0.05825531482696533\n",
      "Test Epoch40 layer3 out_loss 0.1609703004360199, R2 0.058867812156677246\n",
      "Test Epoch40 layer4 out_loss 0.16077810525894165, R2 0.059991538524627686\n",
      "Train 41 | out_loss 0.4007757902145386: 100%|█| 125/125 [00:00<00:00, 277.19it/s\n",
      "Train Epoch41 out_loss 0.1606212705373764, R2 0.057807087898254395\n",
      "Test Epoch41 layer0 out_loss 0.1612042933702469, R2 0.05749976634979248\n",
      "Test Epoch41 layer1 out_loss 0.16115063428878784, R2 0.05781352519989014\n",
      "Test Epoch41 layer2 out_loss 0.16116854548454285, R2 0.057708799839019775\n",
      "Test Epoch41 layer3 out_loss 0.16161929070949554, R2 0.055073440074920654\n",
      "Test Epoch41 layer4 out_loss 0.1616230458021164, R2 0.05505150556564331\n",
      "Train 42 | out_loss 0.40073153376579285: 100%|█| 125/125 [00:00<00:00, 271.32it/\n",
      "Train Epoch42 out_loss 0.16058579087257385, R2 0.058015286922454834\n",
      "Test Epoch42 layer0 out_loss 0.16121792793273926, R2 0.05742007493972778\n",
      "Test Epoch42 layer1 out_loss 0.16209019720554352, R2 0.052320241928100586\n",
      "Test Epoch42 layer2 out_loss 0.1624777913093567, R2 0.050054073333740234\n",
      "Test Epoch42 layer3 out_loss 0.16276688873767853, R2 0.04836386442184448\n",
      "Test Epoch42 layer4 out_loss 0.16282805800437927, R2 0.04800623655319214\n",
      "Train 43 | out_loss 0.4007202684879303: 100%|█| 125/125 [00:00<00:00, 275.89it/s\n",
      "Train Epoch43 out_loss 0.160576730966568, R2 0.05806833505630493\n",
      "Test Epoch43 layer0 out_loss 0.16124874353408813, R2 0.05723989009857178\n",
      "Test Epoch43 layer1 out_loss 0.1616087555885315, R2 0.05513507127761841\n",
      "Test Epoch43 layer2 out_loss 0.16177351772785187, R2 0.05417180061340332\n",
      "Test Epoch43 layer3 out_loss 0.16211377084255219, R2 0.05218243598937988\n",
      "Test Epoch43 layer4 out_loss 0.16222958266735077, R2 0.051505327224731445\n",
      "Train 44 | out_loss 0.40067726373672485: 100%|█| 125/125 [00:00<00:00, 273.02it/\n",
      "Train Epoch44 out_loss 0.16054227948188782, R2 0.05827045440673828\n",
      "Test Epoch44 layer0 out_loss 0.16127917170524597, R2 0.057061970233917236\n",
      "Test Epoch44 layer1 out_loss 0.16163793206214905, R2 0.054964423179626465\n",
      "Test Epoch44 layer2 out_loss 0.16148526966571808, R2 0.05585700273513794\n",
      "Test Epoch44 layer3 out_loss 0.1613546758890152, R2 0.05662059783935547\n",
      "Test Epoch44 layer4 out_loss 0.16120122373104095, R2 0.05751776695251465\n",
      "Train 45 | out_loss 0.4007774293422699: 100%|█| 125/125 [00:00<00:00, 264.10it/s\n",
      "Train Epoch45 out_loss 0.1606225073337555, R2 0.0577998161315918\n",
      "Test Epoch45 layer0 out_loss 0.16118976473808289, R2 0.05758476257324219\n",
      "Test Epoch45 layer1 out_loss 0.16128051280975342, R2 0.05705416202545166\n",
      "Test Epoch45 layer2 out_loss 0.1611918956041336, R2 0.05757230520248413\n",
      "Test Epoch45 layer3 out_loss 0.16127407550811768, R2 0.05709177255630493\n",
      "Test Epoch45 layer4 out_loss 0.16117975115776062, R2 0.05764329433441162\n",
      "Train 46 | out_loss 0.40052178502082825: 100%|█| 125/125 [00:00<00:00, 274.23it/\n",
      "Train Epoch46 out_loss 0.16041772067546844, R2 0.05900108814239502\n",
      "Test Epoch46 layer0 out_loss 0.1611783355474472, R2 0.0576515793800354\n",
      "Test Epoch46 layer1 out_loss 0.16119469702243805, R2 0.05755585432052612\n",
      "Test Epoch46 layer2 out_loss 0.16089633107185364, R2 0.05930030345916748\n",
      "Test Epoch46 layer3 out_loss 0.16067571938037872, R2 0.060590147972106934\n",
      "Test Epoch46 layer4 out_loss 0.160442054271698, R2 0.06195634603500366\n",
      "Train 47 | out_loss 0.40061256289482117: 100%|█| 125/125 [00:00<00:00, 272.45it/\n",
      "Train Epoch47 out_loss 0.16049039363861084, R2 0.0585748553276062\n",
      "Test Epoch47 layer0 out_loss 0.16116587817668915, R2 0.05772441625595093\n",
      "Test Epoch47 layer1 out_loss 0.161044642329216, R2 0.0584331750869751\n",
      "Test Epoch47 layer2 out_loss 0.16074766218662262, R2 0.060169517993927\n",
      "Test Epoch47 layer3 out_loss 0.16057763993740082, R2 0.06116360425949097\n",
      "Test Epoch47 layer4 out_loss 0.16037356853485107, R2 0.062356770038604736\n",
      "Train 48 | out_loss 0.4002666473388672: 100%|█| 125/125 [00:00<00:00, 247.42it/s\n",
      "Train Epoch48 out_loss 0.1602133810520172, R2 0.0601997971534729\n",
      "Test Epoch48 layer0 out_loss 0.16123107075691223, R2 0.05734330415725708\n",
      "Test Epoch48 layer1 out_loss 0.16135162115097046, R2 0.05663841962814331\n",
      "Test Epoch48 layer2 out_loss 0.161020427942276, R2 0.058574795722961426\n",
      "Test Epoch48 layer3 out_loss 0.16081705689430237, R2 0.05976378917694092\n",
      "Test Epoch48 layer4 out_loss 0.16060684621334076, R2 0.06099289655685425\n",
      "Train 49 | out_loss 0.39992982149124146: 100%|█| 125/125 [00:00<00:00, 276.45it/\n",
      "Train Epoch49 out_loss 0.1599438637495041, R2 0.06178075075149536\n",
      "Test Epoch49 layer0 out_loss 0.16115514934062958, R2 0.05778712034225464\n",
      "Test Epoch49 layer1 out_loss 0.1613328903913498, R2 0.05674797296524048\n",
      "Test Epoch49 layer2 out_loss 0.16148807108402252, R2 0.05584067106246948\n",
      "Test Epoch49 layer3 out_loss 0.1619979292154312, R2 0.052859723567962646\n",
      "Test Epoch49 layer4 out_loss 0.16212798655033112, R2 0.05209928750991821\n",
      "Train 50 | out_loss 0.40084633231163025: 100%|█| 125/125 [00:00<00:00, 275.52it/\n",
      "Train Epoch50 out_loss 0.16067782044410706, R2 0.057475388050079346\n",
      "Test Epoch50 layer0 out_loss 0.16122278571128845, R2 0.0573917031288147\n",
      "Test Epoch50 layer1 out_loss 0.1613510102033615, R2 0.056641995906829834\n",
      "Test Epoch50 layer2 out_loss 0.16122901439666748, R2 0.05735522508621216\n",
      "Test Epoch50 layer3 out_loss 0.16121847927570343, R2 0.05741685628890991\n",
      "Test Epoch50 layer4 out_loss 0.16111920773983002, R2 0.057997286319732666\n",
      "Train 51 | out_loss 0.40014997124671936: 100%|█| 125/125 [00:00<00:00, 267.70it/\n",
      "Train Epoch51 out_loss 0.16011996567249298, R2 0.060747742652893066\n",
      "Test Epoch51 layer0 out_loss 0.16115057468414307, R2 0.05781388282775879\n",
      "Test Epoch51 layer1 out_loss 0.16103863716125488, R2 0.05846834182739258\n",
      "Test Epoch51 layer2 out_loss 0.16071295738220215, R2 0.06037247180938721\n",
      "Test Epoch51 layer3 out_loss 0.16055342555046082, R2 0.061305224895477295\n",
      "Test Epoch51 layer4 out_loss 0.16036269068717957, R2 0.06242036819458008\n",
      "Train 52 | out_loss 0.40014269948005676: 100%|█| 125/125 [00:00<00:00, 263.46it/\n",
      "Train Epoch52 out_loss 0.16011415421962738, R2 0.06078183650970459\n",
      "Test Epoch52 layer0 out_loss 0.16118237376213074, R2 0.057627975940704346\n",
      "Test Epoch52 layer1 out_loss 0.16102167963981628, R2 0.05856746435165405\n",
      "Test Epoch52 layer2 out_loss 0.16078022122383118, R2 0.059979140758514404\n",
      "Test Epoch52 layer3 out_loss 0.16074822843074799, R2 0.060166239738464355\n",
      "Test Epoch52 layer4 out_loss 0.16063156723976135, R2 0.060848355293273926\n",
      "Train 53 | out_loss 0.40021052956581116: 100%|█| 125/125 [00:00<00:00, 263.33it/\n",
      "Train Epoch53 out_loss 0.16016848385334015, R2 0.060463130474090576\n",
      "Test Epoch53 layer0 out_loss 0.16123510897159576, R2 0.05731964111328125\n",
      "Test Epoch53 layer1 out_loss 0.16155411303043365, R2 0.0554545521736145\n",
      "Test Epoch53 layer2 out_loss 0.16142308712005615, R2 0.0562206506729126\n",
      "Test Epoch53 layer3 out_loss 0.16145284473896027, R2 0.05604660511016846\n",
      "Test Epoch53 layer4 out_loss 0.16137050092220306, R2 0.05652803182601929\n",
      "Train 54 | out_loss 0.40049272775650024: 100%|█| 125/125 [00:00<00:00, 268.66it/\n",
      "Train Epoch54 out_loss 0.1603943258523941, R2 0.05913829803466797\n",
      "Test Epoch54 layer0 out_loss 0.1611320674419403, R2 0.05792206525802612\n",
      "Test Epoch54 layer1 out_loss 0.16117322444915771, R2 0.057681500911712646\n",
      "Test Epoch54 layer2 out_loss 0.16122274100780487, R2 0.05739188194274902\n",
      "Test Epoch54 layer3 out_loss 0.16147901117801666, R2 0.055893659591674805\n",
      "Test Epoch54 layer4 out_loss 0.1615346223115921, R2 0.05556851625442505\n",
      "Train 55 | out_loss 0.40057364106178284: 100%|█| 125/125 [00:00<00:00, 263.16it/\n",
      "Train Epoch55 out_loss 0.1604592502117157, R2 0.05875754356384277\n",
      "Test Epoch55 layer0 out_loss 0.1611281931400299, R2 0.05794477462768555\n",
      "Test Epoch55 layer1 out_loss 0.16100801527500153, R2 0.05864739418029785\n",
      "Test Epoch55 layer2 out_loss 0.16066381335258484, R2 0.06065976619720459\n",
      "Test Epoch55 layer3 out_loss 0.16046839952468872, R2 0.06180226802825928\n",
      "Test Epoch55 layer4 out_loss 0.16028186678886414, R2 0.06289291381835938\n",
      "Train 56 | out_loss 0.40037909150123596: 100%|█| 125/125 [00:00<00:00, 276.85it/\n",
      "Train Epoch56 out_loss 0.16030336916446686, R2 0.059671878814697266\n",
      "Test Epoch56 layer0 out_loss 0.161127969622612, R2 0.05794602632522583\n",
      "Test Epoch56 layer1 out_loss 0.16125284135341644, R2 0.057215988636016846\n",
      "Test Epoch56 layer2 out_loss 0.16140030324459076, R2 0.05635380744934082\n",
      "Test Epoch56 layer3 out_loss 0.16168364882469177, R2 0.05469715595245361\n",
      "Test Epoch56 layer4 out_loss 0.16165772080421448, R2 0.05484879016876221\n",
      "Train 57 | out_loss 0.4008195996284485: 100%|█| 125/125 [00:00<00:00, 277.20it/s\n",
      "Train Epoch57 out_loss 0.1606564223766327, R2 0.057600975036621094\n",
      "Test Epoch57 layer0 out_loss 0.16134876012802124, R2 0.056655168533325195\n",
      "Test Epoch57 layer1 out_loss 0.16137993335723877, R2 0.05647289752960205\n",
      "Test Epoch57 layer2 out_loss 0.16099627315998077, R2 0.058715999126434326\n",
      "Test Epoch57 layer3 out_loss 0.1607392281293869, R2 0.060218870639801025\n",
      "Test Epoch57 layer4 out_loss 0.16059115529060364, R2 0.0610845685005188\n",
      "Train 58 | out_loss 0.40083685517311096: 100%|█| 125/125 [00:00<00:00, 274.66it/\n",
      "Train Epoch58 out_loss 0.1606701910495758, R2 0.057520151138305664\n",
      "Test Epoch58 layer0 out_loss 0.16116277873516083, R2 0.057742536067962646\n",
      "Test Epoch58 layer1 out_loss 0.16163650155067444, R2 0.054972827434539795\n",
      "Test Epoch58 layer2 out_loss 0.16200406849384308, R2 0.052823781967163086\n",
      "Test Epoch58 layer3 out_loss 0.16259072721004486, R2 0.04939383268356323\n",
      "Test Epoch58 layer4 out_loss 0.1626291573047638, R2 0.049169182777404785\n",
      "Train 59 | out_loss 0.40003418922424316: 100%|█| 125/125 [00:00<00:00, 269.67it/\n",
      "Train Epoch59 out_loss 0.1600273698568344, R2 0.061290860176086426\n",
      "Test Epoch59 layer0 out_loss 0.16111662983894348, R2 0.05801236629486084\n",
      "Test Epoch59 layer1 out_loss 0.16122795641422272, R2 0.057361483573913574\n",
      "Test Epoch59 layer2 out_loss 0.16107425093650818, R2 0.05826014280319214\n",
      "Test Epoch59 layer3 out_loss 0.16111622750759125, R2 0.05801469087600708\n",
      "Test Epoch59 layer4 out_loss 0.16095857322216034, R2 0.05893641710281372\n",
      "Train 60 | out_loss 0.3998749256134033: 100%|█| 125/125 [00:00<00:00, 271.06it/s\n",
      "Train Epoch60 out_loss 0.15989996492862701, R2 0.06203824281692505\n",
      "Test Epoch60 layer0 out_loss 0.16110631823539734, R2 0.058072566986083984\n",
      "Test Epoch60 layer1 out_loss 0.16129568219184875, R2 0.05696547031402588\n",
      "Test Epoch60 layer2 out_loss 0.1611083745956421, R2 0.058060646057128906\n",
      "Test Epoch60 layer3 out_loss 0.16107547283172607, R2 0.05825299024581909\n",
      "Test Epoch60 layer4 out_loss 0.16096161305904388, R2 0.058918654918670654\n",
      "Train 61 | out_loss 0.4002511501312256: 100%|█| 125/125 [00:00<00:00, 264.23it/s\n",
      "Train Epoch61 out_loss 0.16020095348358154, R2 0.06027263402938843\n",
      "Test Epoch61 layer0 out_loss 0.16111542284488678, R2 0.058019399642944336\n",
      "Test Epoch61 layer1 out_loss 0.16100217401981354, R2 0.05868154764175415\n",
      "Test Epoch61 layer2 out_loss 0.1606271117925644, R2 0.06087440252304077\n",
      "Test Epoch61 layer3 out_loss 0.16038522124290466, R2 0.06228858232498169\n",
      "Test Epoch61 layer4 out_loss 0.16018977761268616, R2 0.06343132257461548\n",
      "Train 62 | out_loss 0.3998032212257385: 100%|█| 125/125 [00:00<00:00, 272.89it/s\n",
      "Train Epoch62 out_loss 0.1598425656557083, R2 0.062375009059906006\n",
      "Test Epoch62 layer0 out_loss 0.16112682223320007, R2 0.05795276165008545\n",
      "Test Epoch62 layer1 out_loss 0.1610497683286667, R2 0.058403193950653076\n",
      "Test Epoch62 layer2 out_loss 0.16063192486763, R2 0.06084620952606201\n",
      "Test Epoch62 layer3 out_loss 0.16037802398204803, R2 0.06233072280883789\n",
      "Test Epoch62 layer4 out_loss 0.16020476818084717, R2 0.0633435845375061\n",
      "Train 63 | out_loss 0.3998902440071106: 100%|█| 125/125 [00:00<00:00, 270.86it/s\n",
      "Train Epoch63 out_loss 0.15991216897964478, R2 0.061966657638549805\n",
      "Test Epoch63 layer0 out_loss 0.16128304600715637, R2 0.05703932046890259\n",
      "Test Epoch63 layer1 out_loss 0.16189031302928925, R2 0.053488850593566895\n",
      "Test Epoch63 layer2 out_loss 0.16169537603855133, R2 0.054628610610961914\n",
      "Test Epoch63 layer3 out_loss 0.16161197423934937, R2 0.05511629581451416\n",
      "Test Epoch63 layer4 out_loss 0.16146798431873322, R2 0.055958092212677\n",
      "Train 64 | out_loss 0.4007841944694519: 100%|█| 125/125 [00:00<00:00, 267.66it/s\n",
      "Train Epoch64 out_loss 0.16062799096107483, R2 0.05776768922805786\n",
      "Test Epoch64 layer0 out_loss 0.16124948859214783, R2 0.05723559856414795\n",
      "Test Epoch64 layer1 out_loss 0.1610298752784729, R2 0.05851954221725464\n",
      "Test Epoch64 layer2 out_loss 0.16060519218444824, R2 0.061002492904663086\n",
      "Test Epoch64 layer3 out_loss 0.16035805642604828, R2 0.06244736909866333\n",
      "Test Epoch64 layer4 out_loss 0.16017913818359375, R2 0.06349349021911621\n",
      "Train 65 | out_loss 0.3996426463127136: 100%|█| 125/125 [00:00<00:00, 272.76it/s\n",
      "Train Epoch65 out_loss 0.1597142368555069, R2 0.06312775611877441\n",
      "Test Epoch65 layer0 out_loss 0.1613253653049469, R2 0.05679196119308472\n",
      "Test Epoch65 layer1 out_loss 0.16094687581062317, R2 0.05900484323501587\n",
      "Test Epoch65 layer2 out_loss 0.1606324166059494, R2 0.06084334850311279\n",
      "Test Epoch65 layer3 out_loss 0.1606658697128296, R2 0.060647785663604736\n",
      "Test Epoch65 layer4 out_loss 0.16053670644760132, R2 0.06140291690826416\n",
      "Train 66 | out_loss 0.4001437723636627: 100%|█| 125/125 [00:00<00:00, 278.76it/s\n",
      "Train Epoch66 out_loss 0.16011501848697662, R2 0.06077677011489868\n",
      "Test Epoch66 layer0 out_loss 0.16110274195671082, R2 0.05809354782104492\n",
      "Test Epoch66 layer1 out_loss 0.1612062156200409, R2 0.05748850107192993\n",
      "Test Epoch66 layer2 out_loss 0.16076217591762543, R2 0.06008470058441162\n",
      "Test Epoch66 layer3 out_loss 0.1604696810245514, R2 0.061794817447662354\n",
      "Test Epoch66 layer4 out_loss 0.16037701070308685, R2 0.06233656406402588\n",
      "Train 67 | out_loss 0.39999204874038696: 100%|█| 125/125 [00:00<00:00, 263.04it/\n",
      "Train Epoch67 out_loss 0.15999361872673035, R2 0.06148886680603027\n",
      "Test Epoch67 layer0 out_loss 0.16112762689590454, R2 0.05794805288314819\n",
      "Test Epoch67 layer1 out_loss 0.16091634333133698, R2 0.059183359146118164\n",
      "Test Epoch67 layer2 out_loss 0.160578191280365, R2 0.061160385608673096\n",
      "Test Epoch67 layer3 out_loss 0.16046269237995148, R2 0.061835646629333496\n",
      "Test Epoch67 layer4 out_loss 0.16030612587928772, R2 0.06275105476379395\n",
      "Train 68 | out_loss 0.399768203496933: 100%|█| 125/125 [00:00<00:00, 270.29it/s]\n",
      "Train Epoch68 out_loss 0.15981464087963104, R2 0.06253880262374878\n",
      "Test Epoch68 layer0 out_loss 0.16123314201831818, R2 0.0573311448097229\n",
      "Test Epoch68 layer1 out_loss 0.16150493919849396, R2 0.05574202537536621\n",
      "Test Epoch68 layer2 out_loss 0.16106407344341278, R2 0.05831962823867798\n",
      "Test Epoch68 layer3 out_loss 0.16074138879776, R2 0.06020617485046387\n",
      "Test Epoch68 layer4 out_loss 0.16054022312164307, R2 0.06138235330581665\n",
      "Train 69 | out_loss 0.3997419476509094: 100%|█| 125/125 [00:00<00:00, 272.75it/s\n",
      "Train Epoch69 out_loss 0.1597936451435089, R2 0.06266194581985474\n",
      "Test Epoch69 layer0 out_loss 0.16108909249305725, R2 0.058173298835754395\n",
      "Test Epoch69 layer1 out_loss 0.16090817749500275, R2 0.05923110246658325\n",
      "Test Epoch69 layer2 out_loss 0.16053354740142822, R2 0.061421334743499756\n",
      "Test Epoch69 layer3 out_loss 0.1603417545557022, R2 0.06254273653030396\n",
      "Test Epoch69 layer4 out_loss 0.16015653312206268, R2 0.06362563371658325\n",
      "Train 70 | out_loss 0.39954251050949097: 100%|█| 125/125 [00:00<00:00, 264.45it/\n",
      "Train Epoch70 out_loss 0.15963418781757355, R2 0.06359732151031494\n",
      "Test Epoch70 layer0 out_loss 0.16109764575958252, R2 0.05812335014343262\n",
      "Test Epoch70 layer1 out_loss 0.16197693347930908, R2 0.05298250913619995\n",
      "Test Epoch70 layer2 out_loss 0.16190427541732788, R2 0.053407251834869385\n",
      "Test Epoch70 layer3 out_loss 0.16176845133304596, R2 0.05420142412185669\n",
      "Test Epoch70 layer4 out_loss 0.16173946857452393, R2 0.05437082052230835\n",
      "Train 71 | out_loss 0.40006232261657715: 100%|█| 125/125 [00:00<00:00, 271.41it/\n",
      "Train Epoch71 out_loss 0.1600499004125595, R2 0.06115877628326416\n",
      "Test Epoch71 layer0 out_loss 0.16105258464813232, R2 0.058386802673339844\n",
      "Test Epoch71 layer1 out_loss 0.16089938580989838, R2 0.05928248167037964\n",
      "Test Epoch71 layer2 out_loss 0.1605033427476883, R2 0.061598002910614014\n",
      "Test Epoch71 layer3 out_loss 0.16025109589099884, R2 0.06307274103164673\n",
      "Test Epoch71 layer4 out_loss 0.16007551550865173, R2 0.06409931182861328\n",
      "Train 72 | out_loss 0.3994652032852173: 100%|█| 125/125 [00:00<00:00, 257.88it/s\n",
      "Train Epoch72 out_loss 0.15957249701023102, R2 0.06395918130874634\n",
      "Test Epoch72 layer0 out_loss 0.16124410927295685, R2 0.05726701021194458\n",
      "Test Epoch72 layer1 out_loss 0.16169415414333344, R2 0.05463576316833496\n",
      "Test Epoch72 layer2 out_loss 0.16139300167560577, R2 0.056396484375\n",
      "Test Epoch72 layer3 out_loss 0.16120390594005585, R2 0.05750209093093872\n",
      "Test Epoch72 layer4 out_loss 0.160984069108963, R2 0.05878734588623047\n",
      "Train 73 | out_loss 0.3997819721698761: 100%|█| 125/125 [00:00<00:00, 267.11it/s\n",
      "Train Epoch73 out_loss 0.1598256230354309, R2 0.06247431039810181\n",
      "Test Epoch73 layer0 out_loss 0.16107875108718872, R2 0.058233797550201416\n",
      "Test Epoch73 layer1 out_loss 0.1608862429857254, R2 0.05935931205749512\n",
      "Test Epoch73 layer2 out_loss 0.1604863852262497, R2 0.06169712543487549\n",
      "Test Epoch73 layer3 out_loss 0.16024744510650635, R2 0.06309407949447632\n",
      "Test Epoch73 layer4 out_loss 0.1600896418094635, R2 0.06401669979095459\n",
      "Train 74 | out_loss 0.39942699670791626: 100%|█| 125/125 [00:00<00:00, 275.76it/\n",
      "Train Epoch74 out_loss 0.15954191982746124, R2 0.06413853168487549\n",
      "Test Epoch74 layer0 out_loss 0.16108861565589905, R2 0.05817610025405884\n",
      "Test Epoch74 layer1 out_loss 0.16193276643753052, R2 0.05324065685272217\n",
      "Test Epoch74 layer2 out_loss 0.16208040714263916, R2 0.05237746238708496\n",
      "Test Epoch74 layer3 out_loss 0.1621979922056198, R2 0.051689982414245605\n",
      "Test Epoch74 layer4 out_loss 0.16227614879608154, R2 0.05123305320739746\n",
      "Train 75 | out_loss 0.3996583819389343: 100%|█| 125/125 [00:00<00:00, 272.84it/s\n",
      "Train Epoch75 out_loss 0.15972685813903809, R2 0.06305372714996338\n",
      "Test Epoch75 layer0 out_loss 0.16105473041534424, R2 0.058374226093292236\n",
      "Test Epoch75 layer1 out_loss 0.16087166965007782, R2 0.05944448709487915\n",
      "Test Epoch75 layer2 out_loss 0.16046564280986786, R2 0.06181842088699341\n",
      "Test Epoch75 layer3 out_loss 0.16020534932613373, R2 0.06334024667739868\n",
      "Test Epoch75 layer4 out_loss 0.16003738343715668, R2 0.06432229280471802\n",
      "Train 76 | out_loss 0.3993915915489197: 100%|█| 125/125 [00:00<00:00, 272.36it/s\n",
      "Train Epoch76 out_loss 0.15951359272003174, R2 0.0643046498298645\n",
      "Test Epoch76 layer0 out_loss 0.16113410890102386, R2 0.057910144329071045\n",
      "Test Epoch76 layer1 out_loss 0.16089573502540588, R2 0.05930382013320923\n",
      "Test Epoch76 layer2 out_loss 0.16046780347824097, R2 0.061805784702301025\n",
      "Test Epoch76 layer3 out_loss 0.16019025444984436, R2 0.06342846155166626\n",
      "Test Epoch76 layer4 out_loss 0.16000592708587646, R2 0.06450623273849487\n",
      "Train 77 | out_loss 0.3997265100479126: 100%|█| 125/125 [00:00<00:00, 270.14it/s\n",
      "Train Epoch77 out_loss 0.1597813069820404, R2 0.06273424625396729\n",
      "Test Epoch77 layer0 out_loss 0.16110754013061523, R2 0.05806547403335571\n",
      "Test Epoch77 layer1 out_loss 0.1611376404762268, R2 0.05788952112197876\n",
      "Test Epoch77 layer2 out_loss 0.16088594496250153, R2 0.05936110019683838\n",
      "Test Epoch77 layer3 out_loss 0.16070547699928284, R2 0.060416221618652344\n",
      "Test Epoch77 layer4 out_loss 0.16058051586151123, R2 0.06114673614501953\n",
      "Train 78 | out_loss 0.3992652893066406: 100%|█| 125/125 [00:00<00:00, 260.76it/s\n",
      "Train Epoch78 out_loss 0.15941278636455536, R2 0.06489604711532593\n",
      "Test Epoch78 layer0 out_loss 0.16115747392177582, R2 0.05777353048324585\n",
      "Test Epoch78 layer1 out_loss 0.16143062710762024, R2 0.05617648363113403\n",
      "Test Epoch78 layer2 out_loss 0.16123418509960175, R2 0.057325005531311035\n",
      "Test Epoch78 layer3 out_loss 0.16111215949058533, R2 0.058038532733917236\n",
      "Test Epoch78 layer4 out_loss 0.1609565019607544, R2 0.0589485764503479\n",
      "Train 79 | out_loss 0.39954349398612976: 100%|█| 125/125 [00:00<00:00, 270.22it/\n",
      "Train Epoch79 out_loss 0.15963497757911682, R2 0.06359261274337769\n",
      "Test Epoch79 layer0 out_loss 0.16103941202163696, R2 0.05846381187438965\n",
      "Test Epoch79 layer1 out_loss 0.16081608831882477, R2 0.059769511222839355\n",
      "Test Epoch79 layer2 out_loss 0.16040004789829254, R2 0.062201857566833496\n",
      "Test Epoch79 layer3 out_loss 0.16010677814483643, R2 0.06391656398773193\n",
      "Test Epoch79 layer4 out_loss 0.15993058681488037, R2 0.06494665145874023\n",
      "Train 80 | out_loss 0.39944878220558167: 100%|█| 125/125 [00:00<00:00, 273.03it/\n",
      "Train Epoch80 out_loss 0.1595592498779297, R2 0.06403684616088867\n",
      "Test Epoch80 layer0 out_loss 0.1613144874572754, R2 0.05685555934906006\n",
      "Test Epoch80 layer1 out_loss 0.16168765723705292, R2 0.05467379093170166\n",
      "Test Epoch80 layer2 out_loss 0.1613607108592987, R2 0.05658525228500366\n",
      "Test Epoch80 layer3 out_loss 0.16116556525230408, R2 0.057726263999938965\n",
      "Test Epoch80 layer4 out_loss 0.16096466779708862, R2 0.05890083312988281\n",
      "Train 81 | out_loss 0.399410605430603: 100%|█| 125/125 [00:00<00:00, 269.62it/s]\n",
      "Train Epoch81 out_loss 0.15952877700328827, R2 0.06421560049057007\n",
      "Test Epoch81 layer0 out_loss 0.16103634238243103, R2 0.058481693267822266\n",
      "Test Epoch81 layer1 out_loss 0.1610429883003235, R2 0.058442890644073486\n",
      "Test Epoch81 layer2 out_loss 0.16102494299411774, R2 0.05854839086532593\n",
      "Test Epoch81 layer3 out_loss 0.16114600002765656, R2 0.05784064531326294\n",
      "Test Epoch81 layer4 out_loss 0.16100050508975983, R2 0.05869126319885254\n",
      "Train 82 | out_loss 0.39974579215049744: 100%|█| 125/125 [00:00<00:00, 271.60it/\n",
      "Train Epoch82 out_loss 0.15979668498039246, R2 0.06264406442642212\n",
      "Test Epoch82 layer0 out_loss 0.1610078513622284, R2 0.05864828824996948\n",
      "Test Epoch82 layer1 out_loss 0.1608552783727646, R2 0.05954033136367798\n",
      "Test Epoch82 layer2 out_loss 0.16048449277877808, R2 0.061708152294158936\n",
      "Test Epoch82 layer3 out_loss 0.16022899746894836, R2 0.06320196390151978\n",
      "Test Epoch82 layer4 out_loss 0.16010026633739471, R2 0.06395465135574341\n",
      "Train 83 | out_loss 0.39943379163742065: 100%|█| 125/125 [00:00<00:00, 268.46it/\n",
      "Train Epoch83 out_loss 0.1595473289489746, R2 0.0641067624092102\n",
      "Test Epoch83 layer0 out_loss 0.16099944710731506, R2 0.05869746208190918\n",
      "Test Epoch83 layer1 out_loss 0.16089265048503876, R2 0.059321820735931396\n",
      "Test Epoch83 layer2 out_loss 0.16051292419433594, R2 0.061541974544525146\n",
      "Test Epoch83 layer3 out_loss 0.16020967066287994, R2 0.06331497430801392\n",
      "Test Epoch83 layer4 out_loss 0.16005398333072662, R2 0.0642251968383789\n",
      "Train 84 | out_loss 0.3991023004055023: 100%|█| 125/125 [00:00<00:00, 273.17it/s\n",
      "Train Epoch84 out_loss 0.1592826247215271, R2 0.0656595230102539\n",
      "Test Epoch84 layer0 out_loss 0.16146200895309448, R2 0.055993080139160156\n",
      "Test Epoch84 layer1 out_loss 0.1612294316291809, R2 0.05735284090042114\n",
      "Test Epoch84 layer2 out_loss 0.16063272953033447, R2 0.060841500759124756\n",
      "Test Epoch84 layer3 out_loss 0.1602807343006134, R2 0.06289952993392944\n",
      "Test Epoch84 layer4 out_loss 0.16009685397148132, R2 0.06397461891174316\n",
      "Train 85 | out_loss 0.4000847637653351: 100%|█| 125/125 [00:00<00:00, 275.66it/s\n",
      "Train Epoch85 out_loss 0.1600678414106369, R2 0.06105351448059082\n",
      "Test Epoch85 layer0 out_loss 0.1611110121011734, R2 0.05804520845413208\n",
      "Test Epoch85 layer1 out_loss 0.16206274926662445, R2 0.05248075723648071\n",
      "Test Epoch85 layer2 out_loss 0.16250701248645782, R2 0.049883246421813965\n",
      "Test Epoch85 layer3 out_loss 0.16278119385242462, R2 0.048280298709869385\n",
      "Test Epoch85 layer4 out_loss 0.1626797765493393, R2 0.048873186111450195\n",
      "Train 86 | out_loss 0.3992758095264435: 100%|█| 125/125 [00:00<00:00, 270.08it/s\n",
      "Train Epoch86 out_loss 0.1594211310148239, R2 0.06484705209732056\n",
      "Test Epoch86 layer0 out_loss 0.1610076129436493, R2 0.05864971876144409\n",
      "Test Epoch86 layer1 out_loss 0.16085781157016754, R2 0.059525489807128906\n",
      "Test Epoch86 layer2 out_loss 0.16047325730323792, R2 0.06177389621734619\n",
      "Test Epoch86 layer3 out_loss 0.1602298617362976, R2 0.06319695711135864\n",
      "Test Epoch86 layer4 out_loss 0.16002987325191498, R2 0.0643661618232727\n",
      "Train 87 | out_loss 0.39945802092552185: 100%|█| 125/125 [00:00<00:00, 270.11it/\n",
      "Train Epoch87 out_loss 0.1595667451620102, R2 0.06399291753768921\n",
      "Test Epoch87 layer0 out_loss 0.1610039621591568, R2 0.05867105722427368\n",
      "Test Epoch87 layer1 out_loss 0.16078625619411469, R2 0.05994391441345215\n",
      "Test Epoch87 layer2 out_loss 0.16041234135627747, R2 0.06213003396987915\n",
      "Test Epoch87 layer3 out_loss 0.16013848781585693, R2 0.0637311339378357\n",
      "Test Epoch87 layer4 out_loss 0.15994492173194885, R2 0.06486290693283081\n",
      "Train 88 | out_loss 0.3994314968585968: 100%|█| 125/125 [00:00<00:00, 271.57it/s\n",
      "Train Epoch88 out_loss 0.15954546630382538, R2 0.0641176700592041\n",
      "Test Epoch88 layer0 out_loss 0.16097454726696014, R2 0.058843016624450684\n",
      "Test Epoch88 layer1 out_loss 0.16092169284820557, R2 0.059152066707611084\n",
      "Test Epoch88 layer2 out_loss 0.1607389748096466, R2 0.06022036075592041\n",
      "Test Epoch88 layer3 out_loss 0.16053780913352966, R2 0.06139647960662842\n",
      "Test Epoch88 layer4 out_loss 0.16040995717048645, R2 0.06214398145675659\n",
      "Train 89 | out_loss 0.3994942605495453: 100%|█| 125/125 [00:00<00:00, 262.43it/s\n",
      "Train Epoch89 out_loss 0.15959566831588745, R2 0.06382322311401367\n",
      "Test Epoch89 layer0 out_loss 0.16097243130207062, R2 0.058855414390563965\n",
      "Test Epoch89 layer1 out_loss 0.16088594496250153, R2 0.05936110019683838\n",
      "Test Epoch89 layer2 out_loss 0.1606341153383255, R2 0.06083345413208008\n",
      "Test Epoch89 layer3 out_loss 0.1604735404253006, R2 0.06177222728729248\n",
      "Test Epoch89 layer4 out_loss 0.1603119671344757, R2 0.06271690130233765\n",
      "Train 90 | out_loss 0.39945167303085327: 100%|█| 125/125 [00:00<00:00, 263.28it/\n",
      "Train Epoch90 out_loss 0.15956170856952667, R2 0.0640224814414978\n",
      "Test Epoch90 layer0 out_loss 0.1609629988670349, R2 0.0589105486869812\n",
      "Test Epoch90 layer1 out_loss 0.16072580218315125, R2 0.060297369956970215\n",
      "Test Epoch90 layer2 out_loss 0.16029809415340424, R2 0.06279796361923218\n",
      "Test Epoch90 layer3 out_loss 0.1599663645029068, R2 0.06473743915557861\n",
      "Test Epoch90 layer4 out_loss 0.15975689888000488, R2 0.06596219539642334\n",
      "Train 91 | out_loss 0.39927738904953003: 100%|█| 125/125 [00:00<00:00, 261.86it/\n",
      "Train Epoch91 out_loss 0.15942244231700897, R2 0.06483936309814453\n",
      "Test Epoch91 layer0 out_loss 0.1609569787979126, R2 0.05894571542739868\n",
      "Test Epoch91 layer1 out_loss 0.1608111709356308, R2 0.059798240661621094\n",
      "Test Epoch91 layer2 out_loss 0.160425066947937, R2 0.06205564737319946\n",
      "Test Epoch91 layer3 out_loss 0.16008859872817993, R2 0.06402289867401123\n",
      "Test Epoch91 layer4 out_loss 0.15990176796913147, R2 0.06511521339416504\n",
      "Train 92 | out_loss 0.3991963565349579: 100%|█| 125/125 [00:00<00:00, 260.81it/s\n",
      "Train Epoch92 out_loss 0.15935774147510529, R2 0.06521886587142944\n",
      "Test Epoch92 layer0 out_loss 0.1609698235988617, R2 0.058870673179626465\n",
      "Test Epoch92 layer1 out_loss 0.16072030365467072, R2 0.06032949686050415\n",
      "Test Epoch92 layer2 out_loss 0.16026906669139862, R2 0.06296765804290771\n",
      "Test Epoch92 layer3 out_loss 0.15995503962039948, R2 0.06480377912521362\n",
      "Test Epoch92 layer4 out_loss 0.15974575281143188, R2 0.06602728366851807\n",
      "Train 93 | out_loss 0.3992727994918823: 100%|█| 125/125 [00:00<00:00, 258.96it/s\n",
      "Train Epoch93 out_loss 0.1594187319278717, R2 0.06486111879348755\n",
      "Test Epoch93 layer0 out_loss 0.1609485149383545, R2 0.05899524688720703\n",
      "Test Epoch93 layer1 out_loss 0.16077615320682526, R2 0.06000298261642456\n",
      "Test Epoch93 layer2 out_loss 0.16037532687187195, R2 0.062346458435058594\n",
      "Test Epoch93 layer3 out_loss 0.16007675230503082, R2 0.06409209966659546\n",
      "Test Epoch93 layer4 out_loss 0.15984682738780975, R2 0.06543636322021484\n",
      "Train 94 | out_loss 0.3991117775440216: 100%|█| 125/125 [00:00<00:00, 272.59it/s\n",
      "Train Epoch94 out_loss 0.15929023921489716, R2 0.06561487913131714\n",
      "Test Epoch94 layer0 out_loss 0.16095565259456635, R2 0.05895346403121948\n",
      "Test Epoch94 layer1 out_loss 0.16081923246383667, R2 0.05975109338760376\n",
      "Test Epoch94 layer2 out_loss 0.1603509783744812, R2 0.06248879432678223\n",
      "Test Epoch94 layer3 out_loss 0.1599598526954651, R2 0.06477558612823486\n",
      "Test Epoch94 layer4 out_loss 0.15973447263240814, R2 0.0660933256149292\n",
      "Train 95 | out_loss 0.39940595626831055: 100%|█| 125/125 [00:00<00:00, 268.57it/\n",
      "Train Epoch95 out_loss 0.159525066614151, R2 0.06423735618591309\n",
      "Test Epoch95 layer0 out_loss 0.16097596287727356, R2 0.058834731578826904\n",
      "Test Epoch95 layer1 out_loss 0.16086266934871674, R2 0.05949711799621582\n",
      "Test Epoch95 layer2 out_loss 0.16048207879066467, R2 0.0617222785949707\n",
      "Test Epoch95 layer3 out_loss 0.1602436602115631, R2 0.06311619281768799\n",
      "Test Epoch95 layer4 out_loss 0.16002009809017181, R2 0.0644233226776123\n",
      "Train 96 | out_loss 0.3992367386817932: 100%|█| 125/125 [00:00<00:00, 268.48it/s\n",
      "Train Epoch96 out_loss 0.1593899130821228, R2 0.06503021717071533\n",
      "Test Epoch96 layer0 out_loss 0.16095957159996033, R2 0.05893063545227051\n",
      "Test Epoch96 layer1 out_loss 0.1606994867324829, R2 0.06045114994049072\n",
      "Test Epoch96 layer2 out_loss 0.16031812131404877, R2 0.06268095970153809\n",
      "Test Epoch96 layer3 out_loss 0.16004236042499542, R2 0.06429320573806763\n",
      "Test Epoch96 layer4 out_loss 0.1598297655582428, R2 0.06553608179092407\n",
      "Train 97 | out_loss 0.3989563286304474: 100%|█| 125/125 [00:00<00:00, 275.09it/s\n",
      "Train Epoch97 out_loss 0.1591661423444748, R2 0.06634283065795898\n",
      "Test Epoch97 layer0 out_loss 0.16096730530261993, R2 0.058885395526885986\n",
      "Test Epoch97 layer1 out_loss 0.1609145551919937, R2 0.05919378995895386\n",
      "Test Epoch97 layer2 out_loss 0.16060581803321838, R2 0.06099885702133179\n",
      "Test Epoch97 layer3 out_loss 0.16035251319408417, R2 0.06247985363006592\n",
      "Test Epoch97 layer4 out_loss 0.1600928157567978, R2 0.06399816274642944\n",
      "Train 98 | out_loss 0.39921364188194275: 100%|█| 125/125 [00:00<00:00, 269.98it/\n",
      "Train Epoch98 out_loss 0.1593714952468872, R2 0.06513822078704834\n",
      "Test Epoch98 layer0 out_loss 0.16105104982852936, R2 0.05839574337005615\n",
      "Test Epoch98 layer1 out_loss 0.16124026477336884, R2 0.0572894811630249\n",
      "Test Epoch98 layer2 out_loss 0.16096743941307068, R2 0.058884620666503906\n",
      "Test Epoch98 layer3 out_loss 0.16070088744163513, R2 0.06044304370880127\n",
      "Test Epoch98 layer4 out_loss 0.16052120923995972, R2 0.061493515968322754\n",
      "Train 99 | out_loss 0.3988935649394989: 100%|█| 125/125 [00:00<00:00, 268.03it/s\n",
      "Train Epoch99 out_loss 0.15911607444286346, R2 0.06663650274276733\n",
      "Test Epoch99 layer0 out_loss 0.1610153317451477, R2 0.058604538440704346\n",
      "Test Epoch99 layer1 out_loss 0.1611635386943817, R2 0.05773806571960449\n",
      "Test Epoch99 layer2 out_loss 0.1608608216047287, R2 0.059507906436920166\n",
      "Test Epoch99 layer3 out_loss 0.16055987775325775, R2 0.0612674355506897\n",
      "Test Epoch99 layer4 out_loss 0.16037695109844208, R2 0.06233692169189453\n",
      "Train 100 | out_loss 0.3990676999092102: 100%|█| 125/125 [00:00<00:00, 275.04it/\n",
      "Train Epoch100 out_loss 0.1592550128698349, R2 0.06582146883010864\n",
      "Test Epoch100 layer0 out_loss 0.16101118922233582, R2 0.058628857135772705\n",
      "Test Epoch100 layer1 out_loss 0.1607024222612381, R2 0.060434043407440186\n",
      "Test Epoch100 layer2 out_loss 0.16020840406417847, R2 0.06332236528396606\n",
      "Test Epoch100 layer3 out_loss 0.15983447432518005, R2 0.06550860404968262\n",
      "Test Epoch100 layer4 out_loss 0.1595899909734726, R2 0.06693798303604126\n",
      "Train 101 | out_loss 0.399029940366745: 100%|█| 125/125 [00:00<00:00, 272.38it/s\n",
      "Train Epoch101 out_loss 0.15922486782073975, R2 0.06599831581115723\n",
      "Test Epoch101 layer0 out_loss 0.16092202067375183, R2 0.059150099754333496\n",
      "Test Epoch101 layer1 out_loss 0.16088657081127167, R2 0.059357404708862305\n",
      "Test Epoch101 layer2 out_loss 0.16060754656791687, R2 0.060988783836364746\n",
      "Test Epoch101 layer3 out_loss 0.16038468480110168, R2 0.06229168176651001\n",
      "Test Epoch101 layer4 out_loss 0.1602315604686737, R2 0.06318694353103638\n",
      "Train 102 | out_loss 0.39923056960105896: 100%|█| 125/125 [00:00<00:00, 269.40it\n",
      "Train Epoch102 out_loss 0.15938502550125122, R2 0.06505882740020752\n",
      "Test Epoch102 layer0 out_loss 0.16091448068618774, R2 0.05919426679611206\n",
      "Test Epoch102 layer1 out_loss 0.1609572023153305, R2 0.0589444637298584\n",
      "Test Epoch102 layer2 out_loss 0.16075381636619568, R2 0.06013357639312744\n",
      "Test Epoch102 layer3 out_loss 0.16070972383022308, R2 0.06039142608642578\n",
      "Test Epoch102 layer4 out_loss 0.1605520397424698, R2 0.06131333112716675\n",
      "Train 103 | out_loss 0.39891529083251953: 100%|█| 125/125 [00:00<00:00, 271.00it\n",
      "Train Epoch103 out_loss 0.1591334044933319, R2 0.06653481721878052\n",
      "Test Epoch103 layer0 out_loss 0.1609581708908081, R2 0.05893874168395996\n",
      "Test Epoch103 layer1 out_loss 0.16068635880947113, R2 0.0605279803276062\n",
      "Test Epoch103 layer2 out_loss 0.16019970178604126, R2 0.06337320804595947\n",
      "Test Epoch103 layer3 out_loss 0.15979479253292084, R2 0.06574058532714844\n",
      "Test Epoch103 layer4 out_loss 0.1595263034105301, R2 0.06731033325195312\n",
      "Train 104 | out_loss 0.3987576961517334: 100%|█| 125/125 [00:00<00:00, 277.12it/\n",
      "Train Epoch104 out_loss 0.15900768339633942, R2 0.06727224588394165\n",
      "Test Epoch104 layer0 out_loss 0.16092084348201752, R2 0.05915707349777222\n",
      "Test Epoch104 layer1 out_loss 0.1607050597667694, R2 0.06041860580444336\n",
      "Test Epoch104 layer2 out_loss 0.16028323769569397, R2 0.0628848671913147\n",
      "Test Epoch104 layer3 out_loss 0.15999279916286469, R2 0.0645829439163208\n",
      "Test Epoch104 layer4 out_loss 0.15972654521465302, R2 0.06613963842391968\n",
      "Train 105 | out_loss 0.39869171380996704: 100%|█| 125/125 [00:00<00:00, 265.63it\n",
      "Train Epoch105 out_loss 0.15895508229732513, R2 0.06758087873458862\n",
      "Test Epoch105 layer0 out_loss 0.16090795397758484, R2 0.059232354164123535\n",
      "Test Epoch105 layer1 out_loss 0.16061200201511383, R2 0.0609627366065979\n",
      "Test Epoch105 layer2 out_loss 0.16013921797275543, R2 0.06372684240341187\n",
      "Test Epoch105 layer3 out_loss 0.15973806381225586, R2 0.06607228517532349\n",
      "Test Epoch105 layer4 out_loss 0.15948046743869781, R2 0.06757831573486328\n",
      "Train 106 | out_loss 0.3991120159626007: 100%|█| 125/125 [00:00<00:00, 270.72it/\n",
      "Train Epoch106 out_loss 0.1592904031276703, R2 0.06561386585235596\n",
      "Test Epoch106 layer0 out_loss 0.16088929772377014, R2 0.0593414306640625\n",
      "Test Epoch106 layer1 out_loss 0.16060693562030792, R2 0.06099236011505127\n",
      "Test Epoch106 layer2 out_loss 0.16013659536838531, R2 0.06374222040176392\n",
      "Test Epoch106 layer3 out_loss 0.1597675234079361, R2 0.06590008735656738\n",
      "Test Epoch106 layer4 out_loss 0.15950119495391846, R2 0.06745719909667969\n",
      "Train 107 | out_loss 0.3987938463687897: 100%|█| 125/125 [00:00<00:00, 263.68it/\n",
      "Train Epoch107 out_loss 0.15903648734092712, R2 0.06710338592529297\n",
      "Test Epoch107 layer0 out_loss 0.16088995337486267, R2 0.059337615966796875\n",
      "Test Epoch107 layer1 out_loss 0.16146039962768555, R2 0.05600243806838989\n",
      "Test Epoch107 layer2 out_loss 0.1616058200597763, R2 0.05515223741531372\n",
      "Test Epoch107 layer3 out_loss 0.1613606959581375, R2 0.05658531188964844\n",
      "Test Epoch107 layer4 out_loss 0.16117027401924133, R2 0.057698726654052734\n",
      "Train 108 | out_loss 0.3991575241088867: 100%|█| 125/125 [00:00<00:00, 261.79it/\n",
      "Train Epoch108 out_loss 0.1593267172574997, R2 0.06540083885192871\n",
      "Test Epoch108 layer0 out_loss 0.1608853042125702, R2 0.05936485528945923\n",
      "Test Epoch108 layer1 out_loss 0.16059456765651703, R2 0.06106460094451904\n",
      "Test Epoch108 layer2 out_loss 0.16012749075889587, R2 0.06379544734954834\n",
      "Test Epoch108 layer3 out_loss 0.15971554815769196, R2 0.06620395183563232\n",
      "Test Epoch108 layer4 out_loss 0.15942905843257904, R2 0.06787890195846558\n",
      "Train 109 | out_loss 0.3987500071525574: 100%|█| 125/125 [00:00<00:00, 272.32it/\n",
      "Train Epoch109 out_loss 0.15900149941444397, R2 0.06730860471725464\n",
      "Test Epoch109 layer0 out_loss 0.1609063446521759, R2 0.05924183130264282\n",
      "Test Epoch109 layer1 out_loss 0.16069799661636353, R2 0.060459911823272705\n",
      "Test Epoch109 layer2 out_loss 0.16031880676746368, R2 0.06267690658569336\n",
      "Test Epoch109 layer3 out_loss 0.1600097119808197, R2 0.0644841194152832\n",
      "Test Epoch109 layer4 out_loss 0.15973390638828278, R2 0.06609660387039185\n",
      "Train 110 | out_loss 0.3987475335597992: 100%|█| 125/125 [00:00<00:00, 269.21it/\n",
      "Train Epoch110 out_loss 0.15899956226348877, R2 0.06731998920440674\n",
      "Test Epoch110 layer0 out_loss 0.1609792858362198, R2 0.0588153600692749\n",
      "Test Epoch110 layer1 out_loss 0.1609518676996231, R2 0.05897557735443115\n",
      "Test Epoch110 layer2 out_loss 0.16048207879066467, R2 0.0617222785949707\n",
      "Test Epoch110 layer3 out_loss 0.15998980402946472, R2 0.06460040807723999\n",
      "Test Epoch110 layer4 out_loss 0.15974946320056915, R2 0.0660056471824646\n",
      "Train 111 | out_loss 0.39891505241394043: 100%|█| 125/125 [00:00<00:00, 264.03it\n",
      "Train Epoch111 out_loss 0.1591332107782364, R2 0.06653600931167603\n",
      "Test Epoch111 layer0 out_loss 0.16090378165245056, R2 0.059256792068481445\n",
      "Test Epoch111 layer1 out_loss 0.1607344001531601, R2 0.060247063636779785\n",
      "Test Epoch111 layer2 out_loss 0.16036184132099152, R2 0.06242525577545166\n",
      "Test Epoch111 layer3 out_loss 0.16005125641822815, R2 0.06424117088317871\n",
      "Test Epoch111 layer4 out_loss 0.15979358553886414, R2 0.06574773788452148\n",
      "Train 112 | out_loss 0.39878910779953003: 100%|█| 125/125 [00:00<00:00, 264.91it\n",
      "Train Epoch112 out_loss 0.15903271734714508, R2 0.06712549924850464\n",
      "Test Epoch112 layer0 out_loss 0.16086703538894653, R2 0.05947160720825195\n",
      "Test Epoch112 layer1 out_loss 0.1605841964483261, R2 0.06112527847290039\n",
      "Test Epoch112 layer2 out_loss 0.16013552248477936, R2 0.06374847888946533\n",
      "Test Epoch112 layer3 out_loss 0.15973305702209473, R2 0.0661015510559082\n",
      "Test Epoch112 layer4 out_loss 0.15944992005825043, R2 0.06775695085525513\n",
      "Train 113 | out_loss 0.39875462651252747: 100%|█| 125/125 [00:00<00:00, 266.42it\n",
      "Train Epoch113 out_loss 0.15900523960590363, R2 0.0672866702079773\n",
      "Test Epoch113 layer0 out_loss 0.16086992621421814, R2 0.05945473909378052\n",
      "Test Epoch113 layer1 out_loss 0.1605837643146515, R2 0.06112778186798096\n",
      "Test Epoch113 layer2 out_loss 0.1601170003414154, R2 0.06385678052902222\n",
      "Test Epoch113 layer3 out_loss 0.15972760319709778, R2 0.06613343954086304\n",
      "Test Epoch113 layer4 out_loss 0.1594167947769165, R2 0.06795060634613037\n",
      "Train 114 | out_loss 0.3986060917377472: 100%|█| 125/125 [00:00<00:00, 265.92it/\n",
      "Train Epoch114 out_loss 0.15888674557209015, R2 0.06798171997070312\n",
      "Test Epoch114 layer0 out_loss 0.16092945635318756, R2 0.059106647968292236\n",
      "Test Epoch114 layer1 out_loss 0.16080136597156525, R2 0.05985558032989502\n",
      "Test Epoch114 layer2 out_loss 0.16031649708747864, R2 0.0626903772354126\n",
      "Test Epoch114 layer3 out_loss 0.1597878485918045, R2 0.06578123569488525\n",
      "Test Epoch114 layer4 out_loss 0.15949894487857819, R2 0.06747031211853027\n",
      "Train 115 | out_loss 0.398665189743042: 100%|█| 125/125 [00:00<00:00, 267.04it/s\n",
      "Train Epoch115 out_loss 0.15893396735191345, R2 0.06770479679107666\n",
      "Test Epoch115 layer0 out_loss 0.16090817749500275, R2 0.05923110246658325\n",
      "Test Epoch115 layer1 out_loss 0.16064220666885376, R2 0.06078606843948364\n",
      "Test Epoch115 layer2 out_loss 0.16014736890792847, R2 0.06367921829223633\n",
      "Test Epoch115 layer3 out_loss 0.15972919762134552, R2 0.0661240816116333\n",
      "Test Epoch115 layer4 out_loss 0.1594236195087433, R2 0.06791073083877563\n",
      "Train 116 | out_loss 0.39875248074531555: 100%|█| 125/125 [00:00<00:00, 258.06it\n",
      "Train Epoch116 out_loss 0.15900354087352753, R2 0.06729662418365479\n",
      "Test Epoch116 layer0 out_loss 0.16089893877506256, R2 0.059285104274749756\n",
      "Test Epoch116 layer1 out_loss 0.16054891049861908, R2 0.06133162975311279\n",
      "Test Epoch116 layer2 out_loss 0.16007405519485474, R2 0.06410789489746094\n",
      "Test Epoch116 layer3 out_loss 0.1597120612859726, R2 0.06622427701950073\n",
      "Test Epoch116 layer4 out_loss 0.15941467881202698, R2 0.06796300411224365\n",
      "Train 117 | out_loss 0.3986707031726837: 100%|█| 125/125 [00:00<00:00, 259.75it/\n",
      "Train Epoch117 out_loss 0.15893830358982086, R2 0.06767928600311279\n",
      "Test Epoch117 layer0 out_loss 0.16084787249565125, R2 0.05958366394042969\n",
      "Test Epoch117 layer1 out_loss 0.16055269539356232, R2 0.06130945682525635\n",
      "Test Epoch117 layer2 out_loss 0.16005901992321014, R2 0.06419575214385986\n",
      "Test Epoch117 layer3 out_loss 0.15957756340503693, R2 0.06701070070266724\n",
      "Test Epoch117 layer4 out_loss 0.15928219258785248, R2 0.06873756647109985\n",
      "Train 118 | out_loss 0.39844605326652527: 100%|█| 125/125 [00:00<00:00, 259.53it\n",
      "Train Epoch118 out_loss 0.158759206533432, R2 0.06872987747192383\n",
      "Test Epoch118 layer0 out_loss 0.1608419567346573, R2 0.05961829423904419\n",
      "Test Epoch118 layer1 out_loss 0.1605561524629593, R2 0.06128925085067749\n",
      "Test Epoch118 layer2 out_loss 0.16007700562477112, R2 0.06409060955047607\n",
      "Test Epoch118 layer3 out_loss 0.15964269638061523, R2 0.06662982702255249\n",
      "Test Epoch118 layer4 out_loss 0.15933018922805786, R2 0.06845700740814209\n",
      "Train 119 | out_loss 0.398651123046875: 100%|█| 125/125 [00:00<00:00, 262.99it/s\n",
      "Train Epoch119 out_loss 0.15892276167869568, R2 0.06777048110961914\n",
      "Test Epoch119 layer0 out_loss 0.16095422208309174, R2 0.05896186828613281\n",
      "Test Epoch119 layer1 out_loss 0.16064085066318512, R2 0.06079399585723877\n",
      "Test Epoch119 layer2 out_loss 0.16013148427009583, R2 0.06377208232879639\n",
      "Test Epoch119 layer3 out_loss 0.1596514731645584, R2 0.06657856702804565\n",
      "Test Epoch119 layer4 out_loss 0.15935370326042175, R2 0.06831949949264526\n",
      "Train 120 | out_loss 0.39865991473197937: 100%|█| 125/125 [00:00<00:00, 268.99it\n",
      "Train Epoch120 out_loss 0.1589297652244568, R2 0.06772935390472412\n",
      "Test Epoch120 layer0 out_loss 0.16097240149974823, R2 0.05885559320449829\n",
      "Test Epoch120 layer1 out_loss 0.1607220619916916, R2 0.06031924486160278\n",
      "Test Epoch120 layer2 out_loss 0.16027268767356873, R2 0.06294655799865723\n",
      "Test Epoch120 layer3 out_loss 0.15977203845977783, R2 0.06587368249893188\n",
      "Test Epoch120 layer4 out_loss 0.15948231518268585, R2 0.06756752729415894\n",
      "Train 121 | out_loss 0.39863255620002747: 100%|█| 125/125 [00:00<00:00, 266.61it\n",
      "Train Epoch121 out_loss 0.1589079648256302, R2 0.06785726547241211\n",
      "Test Epoch121 layer0 out_loss 0.1608325093984604, R2 0.0596734881401062\n",
      "Test Epoch121 layer1 out_loss 0.1605566442012787, R2 0.06128638982772827\n",
      "Test Epoch121 layer2 out_loss 0.1601378619670868, R2 0.06373482942581177\n",
      "Test Epoch121 layer3 out_loss 0.1596260517835617, R2 0.0667271614074707\n",
      "Test Epoch121 layer4 out_loss 0.15932001173496246, R2 0.06851649284362793\n",
      "Train 122 | out_loss 0.3983554542064667: 100%|█| 125/125 [00:00<00:00, 270.21it/\n",
      "Train Epoch122 out_loss 0.15868699550628662, R2 0.06915342807769775\n",
      "Test Epoch122 layer0 out_loss 0.16102446615695953, R2 0.05855119228363037\n",
      "Test Epoch122 layer1 out_loss 0.1606188863515854, R2 0.06092250347137451\n",
      "Test Epoch122 layer2 out_loss 0.16006287932395935, R2 0.06417322158813477\n",
      "Test Epoch122 layer3 out_loss 0.15953172743320465, R2 0.06727868318557739\n",
      "Test Epoch122 layer4 out_loss 0.1592320054769516, R2 0.0690310001373291\n",
      "Train 123 | out_loss 0.3986527621746063: 100%|█| 125/125 [00:00<00:00, 269.03it/\n",
      "Train Epoch123 out_loss 0.15892405807971954, R2 0.06776285171508789\n",
      "Test Epoch123 layer0 out_loss 0.1609949767589569, R2 0.0587235689163208\n",
      "Test Epoch123 layer1 out_loss 0.160847470164299, R2 0.05958598852157593\n",
      "Test Epoch123 layer2 out_loss 0.16034835577011108, R2 0.0625041127204895\n",
      "Test Epoch123 layer3 out_loss 0.15987959504127502, R2 0.06524479389190674\n",
      "Test Epoch123 layer4 out_loss 0.1595461666584015, R2 0.06719422340393066\n",
      "Train 124 | out_loss 0.3984889090061188: 100%|█| 125/125 [00:00<00:00, 263.03it/\n",
      "Train Epoch124 out_loss 0.15879346430301666, R2 0.06852895021438599\n",
      "Test Epoch124 layer0 out_loss 0.16085870563983917, R2 0.05952030420303345\n",
      "Test Epoch124 layer1 out_loss 0.1605764627456665, R2 0.06117051839828491\n",
      "Test Epoch124 layer2 out_loss 0.1600894182920456, R2 0.06401807069778442\n",
      "Test Epoch124 layer3 out_loss 0.15962918102741241, R2 0.06670886278152466\n",
      "Test Epoch124 layer4 out_loss 0.15929952263832092, R2 0.06863623857498169\n",
      "Train 125 | out_loss 0.3985169529914856: 100%|█| 125/125 [00:00<00:00, 270.94it/\n",
      "Train Epoch125 out_loss 0.15881575644016266, R2 0.06839817762374878\n",
      "Test Epoch125 layer0 out_loss 0.16083219647407532, R2 0.05967527627944946\n",
      "Test Epoch125 layer1 out_loss 0.16052117943763733, R2 0.06149369478225708\n",
      "Test Epoch125 layer2 out_loss 0.16002245247364044, R2 0.06440961360931396\n",
      "Test Epoch125 layer3 out_loss 0.1594678908586502, R2 0.06765186786651611\n",
      "Test Epoch125 layer4 out_loss 0.15917666256427765, R2 0.06935453414916992\n",
      "Train 126 | out_loss 0.3983767330646515: 100%|█| 125/125 [00:00<00:00, 268.41it/\n",
      "Train Epoch126 out_loss 0.15870408713817596, R2 0.06905317306518555\n",
      "Test Epoch126 layer0 out_loss 0.16087409853935242, R2 0.05943030118942261\n",
      "Test Epoch126 layer1 out_loss 0.16061820089817047, R2 0.06092649698257446\n",
      "Test Epoch126 layer2 out_loss 0.1601124405860901, R2 0.06388342380523682\n",
      "Test Epoch126 layer3 out_loss 0.15960052609443665, R2 0.06687641143798828\n",
      "Test Epoch126 layer4 out_loss 0.15931734442710876, R2 0.06853210926055908\n",
      "Train 127 | out_loss 0.39827924966812134: 100%|█| 125/125 [00:00<00:00, 271.71it\n",
      "Train Epoch127 out_loss 0.15862640738487244, R2 0.06950885057449341\n",
      "Test Epoch127 layer0 out_loss 0.16080930829048157, R2 0.059809088706970215\n",
      "Test Epoch127 layer1 out_loss 0.16061203181743622, R2 0.060962557792663574\n",
      "Test Epoch127 layer2 out_loss 0.1601995974779129, R2 0.063373863697052\n",
      "Test Epoch127 layer3 out_loss 0.15972763299942017, R2 0.06613326072692871\n",
      "Test Epoch127 layer4 out_loss 0.1595025658607483, R2 0.06744915246963501\n",
      "Train 128 | out_loss 0.39850732684135437: 100%|█| 125/125 [00:00<00:00, 262.91it\n",
      "Train Epoch128 out_loss 0.15880809724330902, R2 0.06844311952590942\n",
      "Test Epoch128 layer0 out_loss 0.16080500185489655, R2 0.05983424186706543\n",
      "Test Epoch128 layer1 out_loss 0.16048642992973328, R2 0.06169682741165161\n",
      "Test Epoch128 layer2 out_loss 0.1599787175655365, R2 0.06466531753540039\n",
      "Test Epoch128 layer3 out_loss 0.15948450565338135, R2 0.06755471229553223\n",
      "Test Epoch128 layer4 out_loss 0.1591862142086029, R2 0.06929874420166016\n",
      "Train 129 | out_loss 0.3986373245716095: 100%|█| 125/125 [00:00<00:00, 267.40it/\n",
      "Train Epoch129 out_loss 0.15891164541244507, R2 0.06783562898635864\n",
      "Test Epoch129 layer0 out_loss 0.1608079969882965, R2 0.05981677770614624\n",
      "Test Epoch129 layer1 out_loss 0.16043905913829803, R2 0.06197386980056763\n",
      "Test Epoch129 layer2 out_loss 0.1599196344614029, R2 0.06501072645187378\n",
      "Test Epoch129 layer3 out_loss 0.15941327810287476, R2 0.06797116994857788\n",
      "Test Epoch129 layer4 out_loss 0.159133180975914, R2 0.06960880756378174\n",
      "Train 130 | out_loss 0.3983985483646393: 100%|█| 125/125 [00:00<00:00, 272.38it/\n",
      "Train Epoch130 out_loss 0.15872138738632202, R2 0.06895166635513306\n",
      "Test Epoch130 layer0 out_loss 0.16081619262695312, R2 0.059768855571746826\n",
      "Test Epoch130 layer1 out_loss 0.16052208840847015, R2 0.061488330364227295\n",
      "Test Epoch130 layer2 out_loss 0.16007491946220398, R2 0.06410282850265503\n",
      "Test Epoch130 layer3 out_loss 0.1595522165298462, R2 0.06715887784957886\n",
      "Test Epoch130 layer4 out_loss 0.15925732254981995, R2 0.0688830018043518\n",
      "Train 131 | out_loss 0.3984794318675995: 100%|█| 125/125 [00:00<00:00, 271.35it/\n",
      "Train Epoch131 out_loss 0.1587858945131302, R2 0.06857335567474365\n",
      "Test Epoch131 layer0 out_loss 0.1609857827425003, R2 0.05877739191055298\n",
      "Test Epoch131 layer1 out_loss 0.16075856983661652, R2 0.060105741024017334\n",
      "Test Epoch131 layer2 out_loss 0.16032303869724274, R2 0.0626521110534668\n",
      "Test Epoch131 layer3 out_loss 0.1599416583776474, R2 0.06488192081451416\n",
      "Test Epoch131 layer4 out_loss 0.1596786379814148, R2 0.06641972064971924\n",
      "Train 132 | out_loss 0.398570716381073: 100%|█| 125/125 [00:00<00:00, 272.93it/s\n",
      "Train Epoch132 out_loss 0.15885862708091736, R2 0.06814664602279663\n",
      "Test Epoch132 layer0 out_loss 0.1608598679304123, R2 0.05951356887817383\n",
      "Test Epoch132 layer1 out_loss 0.1606065332889557, R2 0.060994625091552734\n",
      "Test Epoch132 layer2 out_loss 0.16011126339435577, R2 0.06389027833938599\n",
      "Test Epoch132 layer3 out_loss 0.15949924290180206, R2 0.06746864318847656\n",
      "Test Epoch132 layer4 out_loss 0.15923479199409485, R2 0.06901472806930542\n",
      "Train 133 | out_loss 0.3983156383037567: 100%|█| 125/125 [00:00<00:00, 271.05it/\n",
      "Train Epoch133 out_loss 0.1586553156375885, R2 0.06933927536010742\n",
      "Test Epoch133 layer0 out_loss 0.16083160042762756, R2 0.05967879295349121\n",
      "Test Epoch133 layer1 out_loss 0.16049054265022278, R2 0.061672866344451904\n",
      "Test Epoch133 layer2 out_loss 0.15994049608707428, R2 0.06488877534866333\n",
      "Test Epoch133 layer3 out_loss 0.15941262245178223, R2 0.0679749846458435\n",
      "Test Epoch133 layer4 out_loss 0.1591331511735916, R2 0.06960898637771606\n",
      "Train 134 | out_loss 0.39837056398391724: 100%|█| 125/125 [00:00<00:00, 266.07it\n",
      "Train Epoch134 out_loss 0.15869908034801483, R2 0.06908255815505981\n",
      "Test Epoch134 layer0 out_loss 0.1609790325164795, R2 0.05881679058074951\n",
      "Test Epoch134 layer1 out_loss 0.16073445975780487, R2 0.06024670600891113\n",
      "Test Epoch134 layer2 out_loss 0.16023072600364685, R2 0.06319189071655273\n",
      "Test Epoch134 layer3 out_loss 0.15966415405273438, R2 0.06650441884994507\n",
      "Test Epoch134 layer4 out_loss 0.15943770110607147, R2 0.06782841682434082\n",
      "Train 135 | out_loss 0.3983362913131714: 100%|█| 125/125 [00:00<00:00, 266.77it/\n",
      "Train Epoch135 out_loss 0.1586717814207077, R2 0.06924265623092651\n",
      "Test Epoch135 layer0 out_loss 0.16084593534469604, R2 0.05959498882293701\n",
      "Test Epoch135 layer1 out_loss 0.1606667935848236, R2 0.060642361640930176\n",
      "Test Epoch135 layer2 out_loss 0.16018950939178467, R2 0.06343281269073486\n",
      "Test Epoch135 layer3 out_loss 0.15971943736076355, R2 0.06618118286132812\n",
      "Test Epoch135 layer4 out_loss 0.1595047265291214, R2 0.06743651628494263\n",
      "Train 136 | out_loss 0.3982968330383301: 100%|█| 125/125 [00:00<00:00, 269.73it/\n",
      "Train Epoch136 out_loss 0.15864039957523346, R2 0.0694267749786377\n",
      "Test Epoch136 layer0 out_loss 0.16092310845851898, R2 0.059143781661987305\n",
      "Test Epoch136 layer1 out_loss 0.16073641180992126, R2 0.06023538112640381\n",
      "Test Epoch136 layer2 out_loss 0.16020633280277252, R2 0.06333452463150024\n",
      "Test Epoch136 layer3 out_loss 0.15977641940116882, R2 0.06584805250167847\n",
      "Test Epoch136 layer4 out_loss 0.15941482782363892, R2 0.06796211004257202\n",
      "Train 137 | out_loss 0.3984816372394562: 100%|█| 125/125 [00:00<00:00, 269.76it/\n",
      "Train Epoch137 out_loss 0.1587875932455063, R2 0.06856334209442139\n",
      "Test Epoch137 layer0 out_loss 0.1610710620880127, R2 0.05827873945236206\n",
      "Test Epoch137 layer1 out_loss 0.16072067618370056, R2 0.060327351093292236\n",
      "Test Epoch137 layer2 out_loss 0.16014915704727173, R2 0.06366878747940063\n",
      "Test Epoch137 layer3 out_loss 0.15947099030017853, R2 0.0676337480545044\n",
      "Test Epoch137 layer4 out_loss 0.15924383699893951, R2 0.06896185874938965\n",
      "Train 138 | out_loss 0.39827761054039: 100%|█| 125/125 [00:00<00:00, 265.57it/s]\n",
      "Train Epoch138 out_loss 0.158625066280365, R2 0.06951671838760376\n",
      "Test Epoch138 layer0 out_loss 0.16076767444610596, R2 0.060052573680877686\n",
      "Test Epoch138 layer1 out_loss 0.1604243814945221, R2 0.062059640884399414\n",
      "Test Epoch138 layer2 out_loss 0.15990658104419708, R2 0.06508702039718628\n",
      "Test Epoch138 layer3 out_loss 0.1593373417854309, R2 0.06841516494750977\n",
      "Test Epoch138 layer4 out_loss 0.15909485518932343, R2 0.06983292102813721\n",
      "Train 139 | out_loss 0.39814653992652893: 100%|█| 125/125 [00:00<00:00, 272.43it\n",
      "Train Epoch139 out_loss 0.1585206538438797, R2 0.0701291561126709\n",
      "Test Epoch139 layer0 out_loss 0.16096912324428558, R2 0.05887478590011597\n",
      "Test Epoch139 layer1 out_loss 0.16061019897460938, R2 0.06097322702407837\n",
      "Test Epoch139 layer2 out_loss 0.16006210446357727, R2 0.0641777515411377\n",
      "Test Epoch139 layer3 out_loss 0.1594439595937729, R2 0.06779181957244873\n",
      "Test Epoch139 layer4 out_loss 0.15920275449752808, R2 0.0692020058631897\n",
      "Train 140 | out_loss 0.398322194814682: 100%|█| 125/125 [00:00<00:00, 268.46it/s\n",
      "Train Epoch140 out_loss 0.15866056084632874, R2 0.06930845975875854\n",
      "Test Epoch140 layer0 out_loss 0.16076073050498962, R2 0.06009316444396973\n",
      "Test Epoch140 layer1 out_loss 0.16040754318237305, R2 0.062158048152923584\n",
      "Test Epoch140 layer2 out_loss 0.1598798930644989, R2 0.06524300575256348\n",
      "Test Epoch140 layer3 out_loss 0.15927758812904358, R2 0.06876450777053833\n",
      "Test Epoch140 layer4 out_loss 0.1590384691953659, R2 0.07016253471374512\n",
      "Train 141 | out_loss 0.3982694745063782: 100%|█| 125/125 [00:00<00:00, 271.85it/\n",
      "Train Epoch141 out_loss 0.15861858427524567, R2 0.06955468654632568\n",
      "Test Epoch141 layer0 out_loss 0.16085568070411682, R2 0.05953800678253174\n",
      "Test Epoch141 layer1 out_loss 0.16076678037643433, R2 0.060057759284973145\n",
      "Test Epoch141 layer2 out_loss 0.1603204309940338, R2 0.0626673698425293\n",
      "Test Epoch141 layer3 out_loss 0.15975050628185272, R2 0.06599956750869751\n",
      "Test Epoch141 layer4 out_loss 0.15941183269023895, R2 0.06797963380813599\n",
      "Train 142 | out_loss 0.3984628915786743: 100%|█| 125/125 [00:00<00:00, 275.00it/\n",
      "Train Epoch142 out_loss 0.15877270698547363, R2 0.06865066289901733\n",
      "Test Epoch142 layer0 out_loss 0.16079623997211456, R2 0.05988556146621704\n",
      "Test Epoch142 layer1 out_loss 0.16076025366783142, R2 0.060095906257629395\n",
      "Test Epoch142 layer2 out_loss 0.16046510636806488, R2 0.061821579933166504\n",
      "Test Epoch142 layer3 out_loss 0.1598481684923172, R2 0.06542849540710449\n",
      "Test Epoch142 layer4 out_loss 0.15965481102466583, R2 0.06655901670455933\n",
      "Train 143 | out_loss 0.3982999324798584: 100%|█| 125/125 [00:00<00:00, 261.18it/\n",
      "Train Epoch143 out_loss 0.15864284336566925, R2 0.0694124698638916\n",
      "Test Epoch143 layer0 out_loss 0.16076940298080444, R2 0.060042381286621094\n",
      "Test Epoch143 layer1 out_loss 0.16035592555999756, R2 0.06245988607406616\n",
      "Test Epoch143 layer2 out_loss 0.1598360240459442, R2 0.06549954414367676\n",
      "Test Epoch143 layer3 out_loss 0.1592012643814087, R2 0.06921076774597168\n",
      "Test Epoch143 layer4 out_loss 0.15899795293807983, R2 0.07039940357208252\n",
      "Train 144 | out_loss 0.3982047736644745: 100%|█| 125/125 [00:00<00:00, 248.09it/\n",
      "Train Epoch144 out_loss 0.15856698155403137, R2 0.06985741853713989\n",
      "Test Epoch144 layer0 out_loss 0.1609061062335968, R2 0.05924314260482788\n",
      "Test Epoch144 layer1 out_loss 0.16055667400360107, R2 0.061286211013793945\n",
      "Test Epoch144 layer2 out_loss 0.16001129150390625, R2 0.06447482109069824\n",
      "Test Epoch144 layer3 out_loss 0.15937639772891998, R2 0.06818675994873047\n",
      "Test Epoch144 layer4 out_loss 0.15914402902126312, R2 0.06954538822174072\n",
      "Train 145 | out_loss 0.3982052803039551: 100%|█| 125/125 [00:00<00:00, 273.45it/\n",
      "Train Epoch145 out_loss 0.15856745839118958, R2 0.06985461711883545\n",
      "Test Epoch145 layer0 out_loss 0.16074255108833313, R2 0.06019943952560425\n",
      "Test Epoch145 layer1 out_loss 0.16034658253192902, R2 0.06251448392868042\n",
      "Test Epoch145 layer2 out_loss 0.15979140996932983, R2 0.06576037406921387\n",
      "Test Epoch145 layer3 out_loss 0.1592353731393814, R2 0.069011390209198\n",
      "Test Epoch145 layer4 out_loss 0.15900960564613342, R2 0.07033127546310425\n",
      "Train 146 | out_loss 0.3981778919696808: 100%|█| 125/125 [00:00<00:00, 268.89it/\n",
      "Train Epoch146 out_loss 0.1585456281900406, R2 0.06998270750045776\n",
      "Test Epoch146 layer0 out_loss 0.1607721596956253, R2 0.060026347637176514\n",
      "Test Epoch146 layer1 out_loss 0.16041404008865356, R2 0.06212007999420166\n",
      "Test Epoch146 layer2 out_loss 0.15990972518920898, R2 0.06506860256195068\n",
      "Test Epoch146 layer3 out_loss 0.15929122269153595, R2 0.06868475675582886\n",
      "Test Epoch146 layer4 out_loss 0.15907427668571472, R2 0.06995320320129395\n",
      "Train 147 | out_loss 0.39827001094818115: 100%|█| 125/125 [00:00<00:00, 273.56it\n",
      "Train Epoch147 out_loss 0.1586189568042755, R2 0.06955254077911377\n",
      "Test Epoch147 layer0 out_loss 0.16096563637256622, R2 0.058895111083984375\n",
      "Test Epoch147 layer1 out_loss 0.16126959025859833, R2 0.057117998600006104\n",
      "Test Epoch147 layer2 out_loss 0.1608838438987732, R2 0.059373319149017334\n",
      "Test Epoch147 layer3 out_loss 0.16026929020881653, R2 0.06296640634536743\n",
      "Test Epoch147 layer4 out_loss 0.15996433794498444, R2 0.06474936008453369\n",
      "Train 148 | out_loss 0.398249089717865: 100%|█| 125/125 [00:00<00:00, 255.64it/s\n",
      "Train Epoch148 out_loss 0.15860237181186676, R2 0.06964981555938721\n",
      "Test Epoch148 layer0 out_loss 0.16099929809570312, R2 0.05869835615158081\n",
      "Test Epoch148 layer1 out_loss 0.1604590266942978, R2 0.06185704469680786\n",
      "Test Epoch148 layer2 out_loss 0.1598280668258667, R2 0.06554609537124634\n",
      "Test Epoch148 layer3 out_loss 0.1591874361038208, R2 0.06929159164428711\n",
      "Test Epoch148 layer4 out_loss 0.15901127457618713, R2 0.07032155990600586\n",
      "Train 149 | out_loss 0.3982006013393402: 100%|█| 125/125 [00:00<00:00, 271.43it/\n",
      "Train Epoch149 out_loss 0.1585637331008911, R2 0.06987649202346802\n",
      "Test Epoch149 layer0 out_loss 0.16074417531490326, R2 0.06018996238708496\n",
      "Test Epoch149 layer1 out_loss 0.16032204031944275, R2 0.06265801191329956\n",
      "Test Epoch149 layer2 out_loss 0.1597907543182373, R2 0.06576418876647949\n",
      "Test Epoch149 layer3 out_loss 0.1591983437538147, R2 0.06922781467437744\n",
      "Test Epoch149 layer4 out_loss 0.1590019315481186, R2 0.07037615776062012\n",
      "Train 150 | out_loss 0.39802277088165283: 100%|█| 125/125 [00:00<00:00, 273.48it\n",
      "Train Epoch150 out_loss 0.1584220975637436, R2 0.07070726156234741\n",
      "Test Epoch150 layer0 out_loss 0.1609678566455841, R2 0.058882176876068115\n",
      "Test Epoch150 layer1 out_loss 0.16053269803524017, R2 0.06142634153366089\n",
      "Test Epoch150 layer2 out_loss 0.15998630225658417, R2 0.0646209716796875\n",
      "Test Epoch150 layer3 out_loss 0.1593853235244751, R2 0.068134605884552\n",
      "Test Epoch150 layer4 out_loss 0.1591654270887375, R2 0.06942033767700195\n",
      "Train 151 | out_loss 0.3980095088481903: 100%|█| 125/125 [00:00<00:00, 276.18it/\n",
      "Train Epoch151 out_loss 0.15841153264045715, R2 0.0707693099975586\n",
      "Test Epoch151 layer0 out_loss 0.16074348986148834, R2 0.06019395589828491\n",
      "Test Epoch151 layer1 out_loss 0.16032801568508148, R2 0.0626230239868164\n",
      "Test Epoch151 layer2 out_loss 0.15979434549808502, R2 0.06574326753616333\n",
      "Test Epoch151 layer3 out_loss 0.15921419858932495, R2 0.06913512945175171\n",
      "Test Epoch151 layer4 out_loss 0.15900175273418427, R2 0.07037723064422607\n",
      "Train 152 | out_loss 0.39807575941085815: 100%|█| 125/125 [00:00<00:00, 271.35it\n",
      "Train Epoch152 out_loss 0.15846426784992218, R2 0.07045996189117432\n",
      "Test Epoch152 layer0 out_loss 0.16085197031497955, R2 0.059559643268585205\n",
      "Test Epoch152 layer1 out_loss 0.16052117943763733, R2 0.06149369478225708\n",
      "Test Epoch152 layer2 out_loss 0.15989848971366882, R2 0.06513434648513794\n",
      "Test Epoch152 layer3 out_loss 0.1592719405889511, R2 0.0687975287437439\n",
      "Test Epoch152 layer4 out_loss 0.15906991064548492, R2 0.06997871398925781\n",
      "Train 153 | out_loss 0.39834409952163696: 100%|█| 125/125 [00:00<00:00, 271.89it\n",
      "Train Epoch153 out_loss 0.15867803990840912, R2 0.06920599937438965\n",
      "Test Epoch153 layer0 out_loss 0.16071803867816925, R2 0.06034278869628906\n",
      "Test Epoch153 layer1 out_loss 0.1603311151266098, R2 0.06260496377944946\n",
      "Test Epoch153 layer2 out_loss 0.1598275601863861, R2 0.06554901599884033\n",
      "Test Epoch153 layer3 out_loss 0.1592588573694229, R2 0.0688740611076355\n",
      "Test Epoch153 layer4 out_loss 0.15913663804531097, R2 0.0695885419845581\n",
      "Train 154 | out_loss 0.39800217747688293: 100%|█| 125/125 [00:00<00:00, 267.05it\n",
      "Train Epoch154 out_loss 0.15840578079223633, R2 0.07080304622650146\n",
      "Test Epoch154 layer0 out_loss 0.1607164442539215, R2 0.06035202741622925\n",
      "Test Epoch154 layer1 out_loss 0.1602933555841446, R2 0.06282573938369751\n",
      "Test Epoch154 layer2 out_loss 0.15974493324756622, R2 0.06603211164474487\n",
      "Test Epoch154 layer3 out_loss 0.15914420783519745, R2 0.06954431533813477\n",
      "Test Epoch154 layer4 out_loss 0.15896691381931305, R2 0.07058089971542358\n",
      "Train 155 | out_loss 0.3982440233230591: 100%|█| 125/125 [00:00<00:00, 274.37it/\n",
      "Train Epoch155 out_loss 0.15859830379486084, R2 0.06967371702194214\n",
      "Test Epoch155 layer0 out_loss 0.16080285608768463, R2 0.05984681844711304\n",
      "Test Epoch155 layer1 out_loss 0.16046644747257233, R2 0.06181371212005615\n",
      "Test Epoch155 layer2 out_loss 0.15993322432041168, R2 0.06493121385574341\n",
      "Test Epoch155 layer3 out_loss 0.15928541123867035, R2 0.06871873140335083\n",
      "Test Epoch155 layer4 out_loss 0.15916603803634644, R2 0.06941676139831543\n",
      "Train 156 | out_loss 0.3980681598186493: 100%|█| 125/125 [00:00<00:00, 267.71it/\n",
      "Train Epoch156 out_loss 0.15845823287963867, R2 0.07049530744552612\n",
      "Test Epoch156 layer0 out_loss 0.16086216270923615, R2 0.05950009822845459\n",
      "Test Epoch156 layer1 out_loss 0.16075573861598969, R2 0.06012231111526489\n",
      "Test Epoch156 layer2 out_loss 0.16031716763973236, R2 0.0626865029335022\n",
      "Test Epoch156 layer3 out_loss 0.15966418385505676, R2 0.06650424003601074\n",
      "Test Epoch156 layer4 out_loss 0.1595410853624344, R2 0.06722390651702881\n",
      "Train 157 | out_loss 0.3981529176235199: 100%|█| 125/125 [00:00<00:00, 271.37it/\n",
      "Train Epoch157 out_loss 0.1585257202386856, R2 0.07009941339492798\n",
      "Test Epoch157 layer0 out_loss 0.16079594194889069, R2 0.05988729000091553\n",
      "Test Epoch157 layer1 out_loss 0.16026178002357483, R2 0.0630103349685669\n",
      "Test Epoch157 layer2 out_loss 0.15969596803188324, R2 0.06631839275360107\n",
      "Test Epoch157 layer3 out_loss 0.15907877683639526, R2 0.06992685794830322\n",
      "Test Epoch157 layer4 out_loss 0.15896160900592804, R2 0.07061189413070679\n",
      "Train 158 | out_loss 0.39844855666160583: 100%|█| 125/125 [00:00<00:00, 270.26it\n",
      "Train Epoch158 out_loss 0.15876123309135437, R2 0.06871801614761353\n",
      "Test Epoch158 layer0 out_loss 0.16069892048835754, R2 0.06045454740524292\n",
      "Test Epoch158 layer1 out_loss 0.16037674248218536, R2 0.062338173389434814\n",
      "Test Epoch158 layer2 out_loss 0.15985985100269318, R2 0.06536024808883667\n",
      "Test Epoch158 layer3 out_loss 0.15926754474639893, R2 0.06882321834564209\n",
      "Test Epoch158 layer4 out_loss 0.1591212898492813, R2 0.06967830657958984\n",
      "Train 159 | out_loss 0.3979898989200592: 100%|█| 125/125 [00:00<00:00, 270.12it/\n",
      "Train Epoch159 out_loss 0.1583959311246872, R2 0.07086080312728882\n",
      "Test Epoch159 layer0 out_loss 0.16081149876117706, R2 0.05979633331298828\n",
      "Test Epoch159 layer1 out_loss 0.16030126810073853, R2 0.06277942657470703\n",
      "Test Epoch159 layer2 out_loss 0.15979337692260742, R2 0.06574887037277222\n",
      "Test Epoch159 layer3 out_loss 0.15914809703826904, R2 0.06952154636383057\n",
      "Test Epoch159 layer4 out_loss 0.15899741649627686, R2 0.07040256261825562\n",
      "Train 160 | out_loss 0.3978331387042999: 100%|█| 125/125 [00:00<00:00, 275.41it/\n",
      "Train Epoch160 out_loss 0.1582712084054947, R2 0.07159239053726196\n",
      "Test Epoch160 layer0 out_loss 0.16069167852401733, R2 0.06049686670303345\n",
      "Test Epoch160 layer1 out_loss 0.1602928191423416, R2 0.06282883882522583\n",
      "Test Epoch160 layer2 out_loss 0.15974529087543488, R2 0.06603008508682251\n",
      "Test Epoch160 layer3 out_loss 0.15907466411590576, R2 0.06995093822479248\n",
      "Test Epoch160 layer4 out_loss 0.15898367762565613, R2 0.07048290967941284\n",
      "Train 161 | out_loss 0.398043155670166: 100%|█| 125/125 [00:00<00:00, 271.08it/s\n",
      "Train Epoch161 out_loss 0.1584383100271225, R2 0.07061225175857544\n",
      "Test Epoch161 layer0 out_loss 0.1607915610074997, R2 0.05991286039352417\n",
      "Test Epoch161 layer1 out_loss 0.16047154366970062, R2 0.06178390979766846\n",
      "Test Epoch161 layer2 out_loss 0.1598569005727768, R2 0.06537747383117676\n",
      "Test Epoch161 layer3 out_loss 0.15918274223804474, R2 0.06931906938552856\n",
      "Test Epoch161 layer4 out_loss 0.15904532372951508, R2 0.07012248039245605\n",
      "Train 162 | out_loss 0.3979420065879822: 100%|█| 125/125 [00:00<00:00, 270.10it/\n",
      "Train Epoch162 out_loss 0.15835785865783691, R2 0.07108408212661743\n",
      "Test Epoch162 layer0 out_loss 0.1607784777879715, R2 0.05998939275741577\n",
      "Test Epoch162 layer1 out_loss 0.16023284196853638, R2 0.06317949295043945\n",
      "Test Epoch162 layer2 out_loss 0.15967631340026855, R2 0.06643331050872803\n",
      "Test Epoch162 layer3 out_loss 0.1590384542942047, R2 0.07016265392303467\n",
      "Test Epoch162 layer4 out_loss 0.1589309275150299, R2 0.07079130411148071\n",
      "Train 163 | out_loss 0.3980138301849365: 100%|█| 125/125 [00:00<00:00, 263.01it/\n",
      "Train Epoch163 out_loss 0.1584150344133377, R2 0.07074874639511108\n",
      "Test Epoch163 layer0 out_loss 0.1608876883983612, R2 0.05935084819793701\n",
      "Test Epoch163 layer1 out_loss 0.1608608365058899, R2 0.05950784683227539\n",
      "Test Epoch163 layer2 out_loss 0.16035467386245728, R2 0.062467217445373535\n",
      "Test Epoch163 layer3 out_loss 0.15971757471561432, R2 0.06619209051132202\n",
      "Test Epoch163 layer4 out_loss 0.1596756875514984, R2 0.06643694639205933\n",
      "Train 164 | out_loss 0.3980156481266022: 100%|█| 125/125 [00:00<00:00, 269.31it/\n",
      "Train Epoch164 out_loss 0.15841642022132874, R2 0.07074058055877686\n",
      "Test Epoch164 layer0 out_loss 0.16068506240844727, R2 0.060535550117492676\n",
      "Test Epoch164 layer1 out_loss 0.16056986153125763, R2 0.061209142208099365\n",
      "Test Epoch164 layer2 out_loss 0.16001112759113312, R2 0.06447583436965942\n",
      "Test Epoch164 layer3 out_loss 0.1593955159187317, R2 0.06807500123977661\n",
      "Test Epoch164 layer4 out_loss 0.1592380404472351, R2 0.06899577379226685\n",
      "Train 165 | out_loss 0.39803776144981384: 100%|█| 125/125 [00:00<00:00, 272.66it\n",
      "Train Epoch165 out_loss 0.15843403339385986, R2 0.0706372857093811\n",
      "Test Epoch165 layer0 out_loss 0.16068615019321442, R2 0.06052917242050171\n",
      "Test Epoch165 layer1 out_loss 0.1604817509651184, R2 0.06172424554824829\n",
      "Test Epoch165 layer2 out_loss 0.16002115607261658, R2 0.06441718339920044\n",
      "Test Epoch165 layer3 out_loss 0.15932852029800415, R2 0.06846672296524048\n",
      "Test Epoch165 layer4 out_loss 0.15921905636787415, R2 0.06910675764083862\n",
      "Train 166 | out_loss 0.39796072244644165: 100%|█| 125/125 [00:00<00:00, 274.73it\n",
      "Train Epoch166 out_loss 0.15837275981903076, R2 0.07099676132202148\n",
      "Test Epoch166 layer0 out_loss 0.16071359813213348, R2 0.06036871671676636\n",
      "Test Epoch166 layer1 out_loss 0.16022098064422607, R2 0.06324887275695801\n",
      "Test Epoch166 layer2 out_loss 0.1596522182226181, R2 0.06657415628433228\n",
      "Test Epoch166 layer3 out_loss 0.15899094939231873, R2 0.07044035196304321\n",
      "Test Epoch166 layer4 out_loss 0.1589253544807434, R2 0.07082390785217285\n",
      "Train 167 | out_loss 0.39789503812789917: 100%|█| 125/125 [00:00<00:00, 271.90it\n",
      "Train Epoch167 out_loss 0.1583203673362732, R2 0.07130402326583862\n",
      "Test Epoch167 layer0 out_loss 0.16071976721286774, R2 0.06033259630203247\n",
      "Test Epoch167 layer1 out_loss 0.16023646295070648, R2 0.06315833330154419\n",
      "Test Epoch167 layer2 out_loss 0.15969112515449524, R2 0.06634670495986938\n",
      "Test Epoch167 layer3 out_loss 0.15898951888084412, R2 0.07044875621795654\n",
      "Test Epoch167 layer4 out_loss 0.15892499685287476, R2 0.07082593441009521\n",
      "Train 168 | out_loss 0.39787179231643677: 100%|█| 125/125 [00:00<00:00, 268.52it\n",
      "Train Epoch168 out_loss 0.15830197930335999, R2 0.07141190767288208\n",
      "Test Epoch168 layer0 out_loss 0.1607442945241928, R2 0.060189247131347656\n",
      "Test Epoch168 layer1 out_loss 0.16038407385349274, R2 0.06229525804519653\n",
      "Test Epoch168 layer2 out_loss 0.15983839333057404, R2 0.06548571586608887\n",
      "Test Epoch168 layer3 out_loss 0.15914861857891083, R2 0.06951850652694702\n",
      "Test Epoch168 layer4 out_loss 0.15905171632766724, R2 0.07008510828018188\n",
      "Train 169 | out_loss 0.3980315625667572: 100%|█| 125/125 [00:00<00:00, 269.95it/\n",
      "Train Epoch169 out_loss 0.1584291011095047, R2 0.07066619396209717\n",
      "Test Epoch169 layer0 out_loss 0.160854771733284, R2 0.05954337120056152\n",
      "Test Epoch169 layer1 out_loss 0.16069531440734863, R2 0.06047558784484863\n",
      "Test Epoch169 layer2 out_loss 0.16007393598556519, R2 0.06410861015319824\n",
      "Test Epoch169 layer3 out_loss 0.15927138924598694, R2 0.06880074739456177\n",
      "Test Epoch169 layer4 out_loss 0.1591721475124359, R2 0.0693809986114502\n",
      "Train 170 | out_loss 0.3978251516819: 100%|██| 125/125 [00:00<00:00, 271.98it/s]\n",
      "Train Epoch170 out_loss 0.15826478600502014, R2 0.07163012027740479\n",
      "Test Epoch170 layer0 out_loss 0.16066762804985046, R2 0.060637474060058594\n",
      "Test Epoch170 layer1 out_loss 0.16020692884922028, R2 0.0633310079574585\n",
      "Test Epoch170 layer2 out_loss 0.15966667234897614, R2 0.06648969650268555\n",
      "Test Epoch170 layer3 out_loss 0.15898893773555756, R2 0.07045215368270874\n",
      "Test Epoch170 layer4 out_loss 0.15892428159713745, R2 0.07083010673522949\n",
      "Train 171 | out_loss 0.397848516702652: 100%|█| 125/125 [00:00<00:00, 267.17it/s\n",
      "Train Epoch171 out_loss 0.15828338265419006, R2 0.07152098417282104\n",
      "Test Epoch171 layer0 out_loss 0.16066817939281464, R2 0.06063425540924072\n",
      "Test Epoch171 layer1 out_loss 0.16034387052059174, R2 0.06253033876419067\n",
      "Test Epoch171 layer2 out_loss 0.1598600149154663, R2 0.06535929441452026\n",
      "Test Epoch171 layer3 out_loss 0.1593063771724701, R2 0.06859618425369263\n",
      "Test Epoch171 layer4 out_loss 0.159108966588974, R2 0.06975036859512329\n",
      "Train 172 | out_loss 0.3979088068008423: 100%|█| 125/125 [00:00<00:00, 265.99it/\n",
      "Train Epoch172 out_loss 0.15833139419555664, R2 0.0712394118309021\n",
      "Test Epoch172 layer0 out_loss 0.1607128083705902, R2 0.06037330627441406\n",
      "Test Epoch172 layer1 out_loss 0.160185307264328, R2 0.0634574294090271\n",
      "Test Epoch172 layer2 out_loss 0.15962578356266022, R2 0.06672877073287964\n",
      "Test Epoch172 layer3 out_loss 0.15896464884281158, R2 0.0705941915512085\n",
      "Test Epoch172 layer4 out_loss 0.15890537202358246, R2 0.07094073295593262\n",
      "Train 173 | out_loss 0.3979758024215698: 100%|█| 125/125 [00:00<00:00, 263.33it/\n",
      "Train Epoch173 out_loss 0.1583847850561142, R2 0.07092618942260742\n",
      "Test Epoch173 layer0 out_loss 0.16073349118232727, R2 0.06025242805480957\n",
      "Test Epoch173 layer1 out_loss 0.16042840480804443, R2 0.062036097049713135\n",
      "Test Epoch173 layer2 out_loss 0.1598525196313858, R2 0.06540310382843018\n",
      "Test Epoch173 layer3 out_loss 0.15917479991912842, R2 0.06936544179916382\n",
      "Test Epoch173 layer4 out_loss 0.15911288559436798, R2 0.06972748041152954\n",
      "Train 174 | out_loss 0.39796778559684753: 100%|█| 125/125 [00:00<00:00, 269.95it\n",
      "Train Epoch174 out_loss 0.15837836265563965, R2 0.07096391916275024\n",
      "Test Epoch174 layer0 out_loss 0.16079263389110565, R2 0.059906601905822754\n",
      "Test Epoch174 layer1 out_loss 0.16041460633277893, R2 0.06211674213409424\n",
      "Test Epoch174 layer2 out_loss 0.15994365513324738, R2 0.06487029790878296\n",
      "Test Epoch174 layer3 out_loss 0.15948258340358734, R2 0.06756591796875\n",
      "Test Epoch174 layer4 out_loss 0.15921078622341156, R2 0.06915509700775146\n",
      "Train 175 | out_loss 0.397964209318161: 100%|█| 125/125 [00:00<00:00, 273.74it/s\n",
      "Train Epoch175 out_loss 0.15837545692920685, R2 0.07098090648651123\n",
      "Test Epoch175 layer0 out_loss 0.16066843271255493, R2 0.06063282489776611\n",
      "Test Epoch175 layer1 out_loss 0.16024838387966156, R2 0.06308865547180176\n",
      "Test Epoch175 layer2 out_loss 0.15972822904586792, R2 0.06612980365753174\n",
      "Test Epoch175 layer3 out_loss 0.15902358293533325, R2 0.07024955749511719\n",
      "Test Epoch175 layer4 out_loss 0.15903012454509735, R2 0.07021135091781616\n",
      "Train 176 | out_loss 0.3979808986186981: 100%|█| 125/125 [00:00<00:00, 270.67it/\n",
      "Train Epoch176 out_loss 0.15838876366615295, R2 0.07090282440185547\n",
      "Test Epoch176 layer0 out_loss 0.1606447547674179, R2 0.060771167278289795\n",
      "Test Epoch176 layer1 out_loss 0.16016598045825958, R2 0.06357038021087646\n",
      "Test Epoch176 layer2 out_loss 0.15960389375686646, R2 0.06685668230056763\n",
      "Test Epoch176 layer3 out_loss 0.15890812873840332, R2 0.07092463970184326\n",
      "Test Epoch176 layer4 out_loss 0.15888947248458862, R2 0.07103371620178223\n",
      "Train 177 | out_loss 0.39757245779037476: 100%|█| 125/125 [00:00<00:00, 268.78it\n",
      "Train Epoch177 out_loss 0.1580638885498047, R2 0.07280856370925903\n",
      "Test Epoch177 layer0 out_loss 0.16064588725566864, R2 0.06076455116271973\n",
      "Test Epoch177 layer1 out_loss 0.1602327823638916, R2 0.06317979097366333\n",
      "Test Epoch177 layer2 out_loss 0.15972571074962616, R2 0.06614452600479126\n",
      "Test Epoch177 layer3 out_loss 0.15907375514507294, R2 0.06995618343353271\n",
      "Test Epoch177 layer4 out_loss 0.15911473333835602, R2 0.0697166919708252\n",
      "Train 178 | out_loss 0.3978956639766693: 100%|█| 125/125 [00:00<00:00, 271.71it/\n",
      "Train Epoch178 out_loss 0.15832091867923737, R2 0.07130080461502075\n",
      "Test Epoch178 layer0 out_loss 0.16069857776165009, R2 0.06045651435852051\n",
      "Test Epoch178 layer1 out_loss 0.1603778600692749, R2 0.0623316764831543\n",
      "Test Epoch178 layer2 out_loss 0.1598617136478424, R2 0.06534940004348755\n",
      "Test Epoch178 layer3 out_loss 0.15920591354370117, R2 0.06918352842330933\n",
      "Test Epoch178 layer4 out_loss 0.1591320037841797, R2 0.06961566209793091\n",
      "Train 179 | out_loss 0.39797472953796387: 100%|█| 125/125 [00:00<00:00, 273.13it\n",
      "Train Epoch179 out_loss 0.15838387608528137, R2 0.07093149423599243\n",
      "Test Epoch179 layer0 out_loss 0.16098694503307343, R2 0.058770596981048584\n",
      "Test Epoch179 layer1 out_loss 0.16049891710281372, R2 0.06162387132644653\n",
      "Test Epoch179 layer2 out_loss 0.16001829504966736, R2 0.06443387269973755\n",
      "Test Epoch179 layer3 out_loss 0.15933097898960114, R2 0.06845235824584961\n",
      "Test Epoch179 layer4 out_loss 0.1593715250492096, R2 0.06821531057357788\n",
      "Train 180 | out_loss 0.3978767693042755: 100%|█| 125/125 [00:00<00:00, 274.89it/\n",
      "Train Epoch180 out_loss 0.15830589830875397, R2 0.07138895988464355\n",
      "Test Epoch180 layer0 out_loss 0.16066212952136993, R2 0.060669660568237305\n",
      "Test Epoch180 layer1 out_loss 0.1602087765932083, R2 0.06332021951675415\n",
      "Test Epoch180 layer2 out_loss 0.15964534878730774, R2 0.06661432981491089\n",
      "Test Epoch180 layer3 out_loss 0.15900641679763794, R2 0.07034993171691895\n",
      "Test Epoch180 layer4 out_loss 0.1589096188545227, R2 0.07091587781906128\n",
      "Train 181 | out_loss 0.3978315591812134: 100%|█| 125/125 [00:00<00:00, 270.87it/\n",
      "Train Epoch181 out_loss 0.15826991200447083, R2 0.07160001993179321\n",
      "Test Epoch181 layer0 out_loss 0.16066980361938477, R2 0.060624778270721436\n",
      "Test Epoch181 layer1 out_loss 0.1604297012090683, R2 0.06202852725982666\n",
      "Test Epoch181 layer2 out_loss 0.1598791927099228, R2 0.06524711847305298\n",
      "Test Epoch181 layer3 out_loss 0.15912747383117676, R2 0.06964218616485596\n",
      "Test Epoch181 layer4 out_loss 0.15910883247852325, R2 0.06975120306015015\n",
      "Train 182 | out_loss 0.39791738986968994: 100%|█| 125/125 [00:00<00:00, 269.89it\n",
      "Train Epoch182 out_loss 0.15833821892738342, R2 0.07119929790496826\n",
      "Test Epoch182 layer0 out_loss 0.16063356399536133, R2 0.06083667278289795\n",
      "Test Epoch182 layer1 out_loss 0.16017645597457886, R2 0.06350916624069214\n",
      "Test Epoch182 layer2 out_loss 0.15964694321155548, R2 0.06660503149032593\n",
      "Test Epoch182 layer3 out_loss 0.15907862782478333, R2 0.06992775201797485\n",
      "Test Epoch182 layer4 out_loss 0.15895497798919678, R2 0.07065075635910034\n",
      "Train 183 | out_loss 0.3977604806423187: 100%|█| 125/125 [00:00<00:00, 256.21it/\n",
      "Train Epoch183 out_loss 0.15821339190006256, R2 0.07193154096603394\n",
      "Test Epoch183 layer0 out_loss 0.16074883937835693, R2 0.06016266345977783\n",
      "Test Epoch183 layer1 out_loss 0.16070355474948883, R2 0.06042742729187012\n",
      "Test Epoch183 layer2 out_loss 0.16013158857822418, R2 0.06377148628234863\n",
      "Test Epoch183 layer3 out_loss 0.15951059758663177, R2 0.06740212440490723\n",
      "Test Epoch183 layer4 out_loss 0.15941603481769562, R2 0.06795507669448853\n",
      "Train 184 | out_loss 0.39762765169143677: 100%|█| 125/125 [00:00<00:00, 268.46it\n",
      "Train Epoch184 out_loss 0.15810777246952057, R2 0.07255113124847412\n",
      "Test Epoch184 layer0 out_loss 0.16080018877983093, R2 0.05986243486404419\n",
      "Test Epoch184 layer1 out_loss 0.16017411649227142, R2 0.0635228157043457\n",
      "Test Epoch184 layer2 out_loss 0.15962418913841248, R2 0.0667380690574646\n",
      "Test Epoch184 layer3 out_loss 0.15899252891540527, R2 0.0704311728477478\n",
      "Test Epoch184 layer4 out_loss 0.15907317399978638, R2 0.06995958089828491\n",
      "Train 185 | out_loss 0.3977872431278229: 100%|█| 125/125 [00:00<00:00, 270.41it/\n",
      "Train Epoch185 out_loss 0.15823468565940857, R2 0.07180666923522949\n",
      "Test Epoch185 layer0 out_loss 0.16062672436237335, R2 0.06087660789489746\n",
      "Test Epoch185 layer1 out_loss 0.16048911213874817, R2 0.06168121099472046\n",
      "Test Epoch185 layer2 out_loss 0.1598787158727646, R2 0.06524991989135742\n",
      "Test Epoch185 layer3 out_loss 0.15941154956817627, R2 0.0679813027381897\n",
      "Test Epoch185 layer4 out_loss 0.15925471484661102, R2 0.0688982605934143\n",
      "Train 186 | out_loss 0.39767953753471375: 100%|█| 125/125 [00:00<00:00, 267.22it\n",
      "Train Epoch186 out_loss 0.15814900398254395, R2 0.07230925559997559\n",
      "Test Epoch186 layer0 out_loss 0.1606120467185974, R2 0.06096243858337402\n",
      "Test Epoch186 layer1 out_loss 0.16012175381183624, R2 0.06382900476455688\n",
      "Test Epoch186 layer2 out_loss 0.15959584712982178, R2 0.06690382957458496\n",
      "Test Epoch186 layer3 out_loss 0.1589811146259308, R2 0.07049793004989624\n",
      "Test Epoch186 layer4 out_loss 0.15892426669597626, R2 0.07083022594451904\n",
      "Train 187 | out_loss 0.39779236912727356: 100%|█| 125/125 [00:00<00:00, 271.60it\n",
      "Train Epoch187 out_loss 0.1582387536764145, R2 0.07178276777267456\n",
      "Test Epoch187 layer0 out_loss 0.16064506769180298, R2 0.06076937913894653\n",
      "Test Epoch187 layer1 out_loss 0.16013407707214355, R2 0.06375694274902344\n",
      "Test Epoch187 layer2 out_loss 0.1596163958311081, R2 0.06678366661071777\n",
      "Test Epoch187 layer3 out_loss 0.1589949131011963, R2 0.07041716575622559\n",
      "Test Epoch187 layer4 out_loss 0.15897461771965027, R2 0.07053583860397339\n",
      "Train 188 | out_loss 0.39787670969963074: 100%|█| 125/125 [00:00<00:00, 264.23it\n",
      "Train Epoch188 out_loss 0.15830585360527039, R2 0.07138925790786743\n",
      "Test Epoch188 layer0 out_loss 0.16071555018424988, R2 0.06035733222961426\n",
      "Test Epoch188 layer1 out_loss 0.1603371500968933, R2 0.06256967782974243\n",
      "Test Epoch188 layer2 out_loss 0.15980125963687897, R2 0.06570285558700562\n",
      "Test Epoch188 layer3 out_loss 0.15914978086948395, R2 0.0695117712020874\n",
      "Test Epoch188 layer4 out_loss 0.15904441475868225, R2 0.07012778520584106\n",
      "Train 189 | out_loss 0.3978349566459656: 100%|█| 125/125 [00:00<00:00, 273.69it/\n",
      "Train Epoch189 out_loss 0.1582726538181305, R2 0.07158392667770386\n",
      "Test Epoch189 layer0 out_loss 0.16060970723628998, R2 0.06097608804702759\n",
      "Test Epoch189 layer1 out_loss 0.16009439527988434, R2 0.06398898363113403\n",
      "Test Epoch189 layer2 out_loss 0.15954074263572693, R2 0.06722593307495117\n",
      "Test Epoch189 layer3 out_loss 0.1589486449956894, R2 0.07068777084350586\n",
      "Test Epoch189 layer4 out_loss 0.1589164137840271, R2 0.07087618112564087\n",
      "Train 190 | out_loss 0.39775991439819336: 100%|█| 125/125 [00:00<00:00, 276.21it\n",
      "Train Epoch190 out_loss 0.15821294486522675, R2 0.07193416357040405\n",
      "Test Epoch190 layer0 out_loss 0.16060341894626617, R2 0.061012864112854004\n",
      "Test Epoch190 layer1 out_loss 0.1601976752281189, R2 0.06338506937026978\n",
      "Test Epoch190 layer2 out_loss 0.1596267968416214, R2 0.0667228102684021\n",
      "Test Epoch190 layer3 out_loss 0.15895608067512512, R2 0.07064419984817505\n",
      "Test Epoch190 layer4 out_loss 0.15897521376609802, R2 0.07053232192993164\n",
      "Train 191 | out_loss 0.39789965748786926: 100%|█| 125/125 [00:00<00:00, 255.92it\n",
      "Train Epoch191 out_loss 0.15832415223121643, R2 0.07128185033798218\n",
      "Test Epoch191 layer0 out_loss 0.16076751053333282, R2 0.060053467750549316\n",
      "Test Epoch191 layer1 out_loss 0.1604893058538437, R2 0.06168007850646973\n",
      "Test Epoch191 layer2 out_loss 0.1598680019378662, R2 0.06531262397766113\n",
      "Test Epoch191 layer3 out_loss 0.15907131135463715, R2 0.06997054815292358\n",
      "Test Epoch191 layer4 out_loss 0.15910790860652924, R2 0.06975656747817993\n",
      "Train 192 | out_loss 0.3976668119430542: 100%|█| 125/125 [00:00<00:00, 271.02it/\n",
      "Train Epoch192 out_loss 0.15813888609409332, R2 0.07236862182617188\n",
      "Test Epoch192 layer0 out_loss 0.16061994433403015, R2 0.060916244983673096\n",
      "Test Epoch192 layer1 out_loss 0.16008177399635315, R2 0.06406277418136597\n",
      "Test Epoch192 layer2 out_loss 0.15954641997814178, R2 0.06719279289245605\n",
      "Test Epoch192 layer3 out_loss 0.15899606049060822, R2 0.07041049003601074\n",
      "Test Epoch192 layer4 out_loss 0.15897181630134583, R2 0.07055222988128662\n",
      "Train 193 | out_loss 0.3978797197341919: 100%|█| 125/125 [00:00<00:00, 266.24it/\n",
      "Train Epoch193 out_loss 0.1583082228899002, R2 0.07137525081634521\n",
      "Test Epoch193 layer0 out_loss 0.16058999300003052, R2 0.06109136343002319\n",
      "Test Epoch193 layer1 out_loss 0.16016557812690735, R2 0.06357282400131226\n",
      "Test Epoch193 layer2 out_loss 0.1595807522535324, R2 0.06699204444885254\n",
      "Test Epoch193 layer3 out_loss 0.15909473598003387, R2 0.06983351707458496\n",
      "Test Epoch193 layer4 out_loss 0.15909938514232635, R2 0.06980639696121216\n",
      "Train 194 | out_loss 0.39778444170951843: 100%|█| 125/125 [00:00<00:00, 267.71it\n",
      "Train Epoch194 out_loss 0.1582324504852295, R2 0.07181978225708008\n",
      "Test Epoch194 layer0 out_loss 0.16058409214019775, R2 0.061125874519348145\n",
      "Test Epoch194 layer1 out_loss 0.16031689941883087, R2 0.06268805265426636\n",
      "Test Epoch194 layer2 out_loss 0.159805566072464, R2 0.06567758321762085\n",
      "Test Epoch194 layer3 out_loss 0.15930092334747314, R2 0.06862813234329224\n",
      "Test Epoch194 layer4 out_loss 0.15906819701194763, R2 0.06998878717422485\n",
      "Train 195 | out_loss 0.39797526597976685: 100%|█| 125/125 [00:00<00:00, 273.23it\n",
      "Train Epoch195 out_loss 0.158384308218956, R2 0.07092899084091187\n",
      "Test Epoch195 layer0 out_loss 0.16071490943431854, R2 0.06036108732223511\n",
      "Test Epoch195 layer1 out_loss 0.16044403612613678, R2 0.061944782733917236\n",
      "Test Epoch195 layer2 out_loss 0.16008955240249634, R2 0.06401723623275757\n",
      "Test Epoch195 layer3 out_loss 0.1592773199081421, R2 0.06876611709594727\n",
      "Test Epoch195 layer4 out_loss 0.15945987403392792, R2 0.06769877672195435\n",
      "Train 196 | out_loss 0.39769619703292847: 100%|█| 125/125 [00:00<00:00, 270.46it\n",
      "Train Epoch196 out_loss 0.15816232562065125, R2 0.07223111391067505\n",
      "Test Epoch196 layer0 out_loss 0.16063283383846283, R2 0.06084096431732178\n",
      "Test Epoch196 layer1 out_loss 0.16020788252353668, R2 0.06332540512084961\n",
      "Test Epoch196 layer2 out_loss 0.15964113175868988, R2 0.0666390061378479\n",
      "Test Epoch196 layer3 out_loss 0.15903295576572418, R2 0.0701947808265686\n",
      "Test Epoch196 layer4 out_loss 0.1590341329574585, R2 0.07018786668777466\n",
      "Train 197 | out_loss 0.39770904183387756: 100%|█| 125/125 [00:00<00:00, 269.18it\n",
      "Train Epoch197 out_loss 0.15817245841026306, R2 0.07217168807983398\n",
      "Test Epoch197 layer0 out_loss 0.1605866402387619, R2 0.0611109733581543\n",
      "Test Epoch197 layer1 out_loss 0.16003774106502533, R2 0.06432020664215088\n",
      "Test Epoch197 layer2 out_loss 0.15949757397174835, R2 0.06747835874557495\n",
      "Test Epoch197 layer3 out_loss 0.15885283052921295, R2 0.07124793529510498\n",
      "Test Epoch197 layer4 out_loss 0.15889471769332886, R2 0.0710030198097229\n",
      "Train 198 | out_loss 0.3980323076248169: 100%|█| 125/125 [00:00<00:00, 261.06it/\n",
      "Train Epoch198 out_loss 0.15842974185943604, R2 0.07066243886947632\n",
      "Test Epoch198 layer0 out_loss 0.16057530045509338, R2 0.06117725372314453\n",
      "Test Epoch198 layer1 out_loss 0.16002453863620758, R2 0.06439739465713501\n",
      "Test Epoch198 layer2 out_loss 0.15946221351623535, R2 0.067685067653656\n",
      "Test Epoch198 layer3 out_loss 0.15881678462028503, R2 0.07145863771438599\n",
      "Test Epoch198 layer4 out_loss 0.15891452133655548, R2 0.07088720798492432\n",
      "Train 199 | out_loss 0.3977188467979431: 100%|█| 125/125 [00:00<00:00, 271.62it/\n",
      "Train Epoch199 out_loss 0.15818028151988983, R2 0.07212579250335693\n",
      "Test Epoch199 layer0 out_loss 0.16060608625411987, R2 0.06099730730056763\n",
      "Test Epoch199 layer1 out_loss 0.16023913025856018, R2 0.06314271688461304\n",
      "Test Epoch199 layer2 out_loss 0.15967880189418793, R2 0.06641876697540283\n",
      "Test Epoch199 layer3 out_loss 0.15931540727615356, R2 0.06854337453842163\n",
      "Test Epoch199 layer4 out_loss 0.15923728048801422, R2 0.06900018453598022\n",
      "Train 200 | out_loss 0.39778995513916016: 100%|█| 125/125 [00:00<00:00, 266.76it\n",
      "Train Epoch200 out_loss 0.15823675692081451, R2 0.07179450988769531\n",
      "Test Epoch200 layer0 out_loss 0.16058145463466644, R2 0.06114131212234497\n",
      "Test Epoch200 layer1 out_loss 0.1600600630044937, R2 0.06418973207473755\n",
      "Test Epoch200 layer2 out_loss 0.15949085354804993, R2 0.06751757860183716\n",
      "Test Epoch200 layer3 out_loss 0.15888842940330505, R2 0.07103979587554932\n",
      "Test Epoch200 layer4 out_loss 0.15887516736984253, R2 0.07111728191375732\n",
      "Train 201 | out_loss 0.39773866534233093: 100%|█| 125/125 [00:00<00:00, 250.04it\n",
      "Train Epoch201 out_loss 0.15819603204727173, R2 0.07203340530395508\n",
      "Test Epoch201 layer0 out_loss 0.16057366132736206, R2 0.061186909675598145\n",
      "Test Epoch201 layer1 out_loss 0.1600254625082016, R2 0.06439197063446045\n",
      "Test Epoch201 layer2 out_loss 0.15950323641300201, R2 0.06744521856307983\n",
      "Test Epoch201 layer3 out_loss 0.15897636115550995, R2 0.0705256462097168\n",
      "Test Epoch201 layer4 out_loss 0.15893401205539703, R2 0.07077330350875854\n",
      "Train 202 | out_loss 0.39757809042930603: 100%|█| 125/125 [00:00<00:00, 268.47it\n",
      "Train Epoch202 out_loss 0.15806837379932404, R2 0.07278221845626831\n",
      "Test Epoch202 layer0 out_loss 0.16066572070121765, R2 0.06064862012863159\n",
      "Test Epoch202 layer1 out_loss 0.16005924344062805, R2 0.0641944408416748\n",
      "Test Epoch202 layer2 out_loss 0.15955302119255066, R2 0.0671541690826416\n",
      "Test Epoch202 layer3 out_loss 0.15924176573753357, R2 0.06897395849227905\n",
      "Test Epoch202 layer4 out_loss 0.15904077887535095, R2 0.07014906406402588\n",
      "Train 203 | out_loss 0.3978106379508972: 100%|█| 125/125 [00:00<00:00, 250.09it/\n",
      "Train Epoch203 out_loss 0.15825332701206207, R2 0.07169729471206665\n",
      "Test Epoch203 layer0 out_loss 0.16057251393795013, R2 0.06119358539581299\n",
      "Test Epoch203 layer1 out_loss 0.16021646559238434, R2 0.0632752776145935\n",
      "Test Epoch203 layer2 out_loss 0.15967173874378204, R2 0.06646007299423218\n",
      "Test Epoch203 layer3 out_loss 0.15896958112716675, R2 0.07056534290313721\n",
      "Test Epoch203 layer4 out_loss 0.15902364253997803, R2 0.07024925947189331\n",
      "Train 204 | out_loss 0.3977693021297455: 100%|█| 125/125 [00:00<00:00, 269.58it/\n",
      "Train Epoch204 out_loss 0.1582203358411789, R2 0.07189083099365234\n",
      "Test Epoch204 layer0 out_loss 0.16078908741474152, R2 0.05992734432220459\n",
      "Test Epoch204 layer1 out_loss 0.16014690697193146, R2 0.06368190050125122\n",
      "Test Epoch204 layer2 out_loss 0.15953920781612396, R2 0.06723487377166748\n",
      "Test Epoch204 layer3 out_loss 0.1589025855064392, R2 0.0709570050239563\n",
      "Test Epoch204 layer4 out_loss 0.158986434340477, R2 0.07046675682067871\n",
      "Train 205 | out_loss 0.3976494073867798: 100%|█| 125/125 [00:00<00:00, 269.08it/\n",
      "Train Epoch205 out_loss 0.15812504291534424, R2 0.07244980335235596\n",
      "Test Epoch205 layer0 out_loss 0.1606859713792801, R2 0.06053018569946289\n",
      "Test Epoch205 layer1 out_loss 0.1602640450000763, R2 0.06299710273742676\n",
      "Test Epoch205 layer2 out_loss 0.15968230366706848, R2 0.06639832258224487\n",
      "Test Epoch205 layer3 out_loss 0.15911135077476501, R2 0.06973648071289062\n",
      "Test Epoch205 layer4 out_loss 0.15911181271076202, R2 0.06973373889923096\n",
      "Train 206 | out_loss 0.3977048099040985: 100%|█| 125/125 [00:00<00:00, 271.31it/\n",
      "Train Epoch206 out_loss 0.15816907584667206, R2 0.07219147682189941\n",
      "Test Epoch206 layer0 out_loss 0.16055402159690857, R2 0.06130170822143555\n",
      "Test Epoch206 layer1 out_loss 0.1599748283624649, R2 0.06468802690505981\n",
      "Test Epoch206 layer2 out_loss 0.159407377243042, R2 0.06800568103790283\n",
      "Test Epoch206 layer3 out_loss 0.15877476334571838, R2 0.07170432806015015\n",
      "Test Epoch206 layer4 out_loss 0.1588575541973114, R2 0.0712202787399292\n",
      "Train 207 | out_loss 0.39770904183387756: 100%|█| 125/125 [00:00<00:00, 261.20it\n",
      "Train Epoch207 out_loss 0.15817245841026306, R2 0.07217168807983398\n",
      "Test Epoch207 layer0 out_loss 0.16059447824954987, R2 0.06106513738632202\n",
      "Test Epoch207 layer1 out_loss 0.16003185510635376, R2 0.0643545389175415\n",
      "Test Epoch207 layer2 out_loss 0.15948368608951569, R2 0.06755954027175903\n",
      "Test Epoch207 layer3 out_loss 0.15885581076145172, R2 0.07123053073883057\n",
      "Test Epoch207 layer4 out_loss 0.15890073776245117, R2 0.07096779346466064\n",
      "Train 208 | out_loss 0.39791160821914673: 100%|█| 125/125 [00:00<00:00, 257.77it\n",
      "Train Epoch208 out_loss 0.1583336442708969, R2 0.07122617959976196\n",
      "Test Epoch208 layer0 out_loss 0.1606246680021286, R2 0.06088864803314209\n",
      "Test Epoch208 layer1 out_loss 0.15995405614376068, R2 0.06480944156646729\n",
      "Test Epoch208 layer2 out_loss 0.1593884378671646, R2 0.06811636686325073\n",
      "Test Epoch208 layer3 out_loss 0.15877236425876617, R2 0.07171833515167236\n",
      "Test Epoch208 layer4 out_loss 0.15885858237743378, R2 0.07121425867080688\n",
      "Train 209 | out_loss 0.39758428931236267: 100%|█| 125/125 [00:00<00:00, 261.13it\n",
      "Train Epoch209 out_loss 0.15807324647903442, R2 0.0727536678314209\n",
      "Test Epoch209 layer0 out_loss 0.16056661307811737, R2 0.061228036880493164\n",
      "Test Epoch209 layer1 out_loss 0.1600157767534256, R2 0.06444859504699707\n",
      "Test Epoch209 layer2 out_loss 0.15947532653808594, R2 0.06760841608047485\n",
      "Test Epoch209 layer3 out_loss 0.15883125364780426, R2 0.07137405872344971\n",
      "Test Epoch209 layer4 out_loss 0.1589219868183136, R2 0.07084357738494873\n",
      "Train 210 | out_loss 0.39772841334342957: 100%|█| 125/125 [00:00<00:00, 258.29it\n",
      "Train Epoch210 out_loss 0.1581878662109375, R2 0.07208132743835449\n",
      "Test Epoch210 layer0 out_loss 0.1605842560529709, R2 0.06112492084503174\n",
      "Test Epoch210 layer1 out_loss 0.1600087434053421, R2 0.06448972225189209\n",
      "Test Epoch210 layer2 out_loss 0.1593954712152481, R2 0.06807523965835571\n",
      "Test Epoch210 layer3 out_loss 0.15888193249702454, R2 0.07107776403427124\n",
      "Test Epoch210 layer4 out_loss 0.1589222550392151, R2 0.07084202766418457\n",
      "Train 211 | out_loss 0.3976197838783264: 100%|█| 125/125 [00:00<00:00, 267.82it/\n",
      "Train Epoch211 out_loss 0.15810146927833557, R2 0.07258808612823486\n",
      "Test Epoch211 layer0 out_loss 0.1605624109506607, R2 0.0612526535987854\n",
      "Test Epoch211 layer1 out_loss 0.1599644124507904, R2 0.06474888324737549\n",
      "Test Epoch211 layer2 out_loss 0.15941408276557922, R2 0.0679665207862854\n",
      "Test Epoch211 layer3 out_loss 0.15877357125282288, R2 0.07171130180358887\n",
      "Test Epoch211 layer4 out_loss 0.15886618196964264, R2 0.07116985321044922\n",
      "Train 212 | out_loss 0.39778995513916016: 100%|█| 125/125 [00:00<00:00, 268.72it\n",
      "Train Epoch212 out_loss 0.1582368165254593, R2 0.07179415225982666\n",
      "Test Epoch212 layer0 out_loss 0.1605389565229416, R2 0.061389803886413574\n",
      "Test Epoch212 layer1 out_loss 0.16012781858444214, R2 0.06379354000091553\n",
      "Test Epoch212 layer2 out_loss 0.15947990119457245, R2 0.0675816535949707\n",
      "Test Epoch212 layer3 out_loss 0.1589292734861374, R2 0.07080096006393433\n",
      "Test Epoch212 layer4 out_loss 0.15890824794769287, R2 0.07092392444610596\n",
      "Train 213 | out_loss 0.3977010250091553: 100%|█| 125/125 [00:00<00:00, 271.39it/\n",
      "Train Epoch213 out_loss 0.15816611051559448, R2 0.0722089409828186\n",
      "Test Epoch213 layer0 out_loss 0.1605423241853714, R2 0.06137007474899292\n",
      "Test Epoch213 layer1 out_loss 0.15993821620941162, R2 0.06490206718444824\n",
      "Test Epoch213 layer2 out_loss 0.1593899428844452, R2 0.06810766458511353\n",
      "Test Epoch213 layer3 out_loss 0.15878881514072418, R2 0.07162213325500488\n",
      "Test Epoch213 layer4 out_loss 0.15888828039169312, R2 0.07104068994522095\n",
      "Train 214 | out_loss 0.39767521619796753: 100%|█| 125/125 [00:00<00:00, 272.91it\n",
      "Train Epoch214 out_loss 0.15814554691314697, R2 0.07232952117919922\n",
      "Test Epoch214 layer0 out_loss 0.1605510115623474, R2 0.06131929159164429\n",
      "Test Epoch214 layer1 out_loss 0.1605112999677658, R2 0.061551451683044434\n",
      "Test Epoch214 layer2 out_loss 0.15986695885658264, R2 0.06531870365142822\n",
      "Test Epoch214 layer3 out_loss 0.15979106724262238, R2 0.06576240062713623\n",
      "Test Epoch214 layer4 out_loss 0.15950441360473633, R2 0.06743836402893066\n",
      "Train 215 | out_loss 0.3976287245750427: 100%|█| 125/125 [00:00<00:00, 271.06it/\n",
      "Train Epoch215 out_loss 0.15810857713222504, R2 0.07254636287689209\n",
      "Test Epoch215 layer0 out_loss 0.16053834557533264, R2 0.06139332056045532\n",
      "Test Epoch215 layer1 out_loss 0.1599297970533371, R2 0.06495130062103271\n",
      "Test Epoch215 layer2 out_loss 0.15935415029525757, R2 0.06831681728363037\n",
      "Test Epoch215 layer3 out_loss 0.15876473486423492, R2 0.07176291942596436\n",
      "Test Epoch215 layer4 out_loss 0.15883837640285492, R2 0.07133239507675171\n",
      "Train 216 | out_loss 0.3976731598377228: 100%|█| 125/125 [00:00<00:00, 266.90it/\n",
      "Train Epoch216 out_loss 0.15814390778541565, R2 0.07233917713165283\n",
      "Test Epoch216 layer0 out_loss 0.16054554283618927, R2 0.061351239681243896\n",
      "Test Epoch216 layer1 out_loss 0.16004492342472076, R2 0.06427818536758423\n",
      "Test Epoch216 layer2 out_loss 0.15944203734397888, R2 0.06780308485031128\n",
      "Test Epoch216 layer3 out_loss 0.15886296331882477, R2 0.07118868827819824\n",
      "Test Epoch216 layer4 out_loss 0.15893088281154633, R2 0.07079160213470459\n",
      "Train 217 | out_loss 0.39772480726242065: 100%|█| 125/125 [00:00<00:00, 268.23it\n",
      "Train Epoch217 out_loss 0.15818502008914948, R2 0.0720980167388916\n",
      "Test Epoch217 layer0 out_loss 0.1606730818748474, R2 0.06060558557510376\n",
      "Test Epoch217 layer1 out_loss 0.1605912148952484, R2 0.061084210872650146\n",
      "Test Epoch217 layer2 out_loss 0.15986570715904236, R2 0.06532597541809082\n",
      "Test Epoch217 layer3 out_loss 0.15962323546409607, R2 0.06674367189407349\n",
      "Test Epoch217 layer4 out_loss 0.15935878455638885, R2 0.06828981637954712\n",
      "Train 218 | out_loss 0.3976258933544159: 100%|█| 125/125 [00:00<00:00, 275.94it/\n",
      "Train Epoch218 out_loss 0.15810631215572357, R2 0.07255971431732178\n",
      "Test Epoch218 layer0 out_loss 0.16052290797233582, R2 0.06148362159729004\n",
      "Test Epoch218 layer1 out_loss 0.16002069413661957, R2 0.06441986560821533\n",
      "Test Epoch218 layer2 out_loss 0.15941718220710754, R2 0.06794840097427368\n",
      "Test Epoch218 layer3 out_loss 0.15876413881778717, R2 0.0717664361000061\n",
      "Test Epoch218 layer4 out_loss 0.15889206528663635, R2 0.07101857662200928\n",
      "Train 219 | out_loss 0.3976561427116394: 100%|█| 125/125 [00:00<00:00, 271.02it/\n",
      "Train Epoch219 out_loss 0.15813037753105164, R2 0.07241857051849365\n",
      "Test Epoch219 layer0 out_loss 0.1605207324028015, R2 0.0614963173866272\n",
      "Test Epoch219 layer1 out_loss 0.1598980575799942, R2 0.0651368498802185\n",
      "Test Epoch219 layer2 out_loss 0.15930570662021637, R2 0.0686001181602478\n",
      "Test Epoch219 layer3 out_loss 0.15872272849082947, R2 0.07200855016708374\n",
      "Test Epoch219 layer4 out_loss 0.15884509682655334, R2 0.0712931752204895\n",
      "Train 220 | out_loss 0.39776065945625305: 100%|█| 125/125 [00:00<00:00, 235.91it\n",
      "Train Epoch220 out_loss 0.1582135558128357, R2 0.07193058729171753\n",
      "Test Epoch220 layer0 out_loss 0.1605241596698761, R2 0.061476290225982666\n",
      "Test Epoch220 layer1 out_loss 0.15988843142986298, R2 0.06519317626953125\n",
      "Test Epoch220 layer2 out_loss 0.1593414843082428, R2 0.06839090585708618\n",
      "Test Epoch220 layer3 out_loss 0.15881958603858948, R2 0.07144230604171753\n",
      "Test Epoch220 layer4 out_loss 0.1589813083410263, R2 0.07049673795700073\n",
      "Train 221 | out_loss 0.397635817527771: 100%|█| 125/125 [00:00<00:00, 262.83it/s\n",
      "Train Epoch221 out_loss 0.1581142246723175, R2 0.07251328229904175\n",
      "Test Epoch221 layer0 out_loss 0.16051295399665833, R2 0.06154179573059082\n",
      "Test Epoch221 layer1 out_loss 0.15988832712173462, R2 0.065193772315979\n",
      "Test Epoch221 layer2 out_loss 0.15931525826454163, R2 0.06854426860809326\n",
      "Test Epoch221 layer3 out_loss 0.1587066948413849, R2 0.07210230827331543\n",
      "Test Epoch221 layer4 out_loss 0.15882326662540436, R2 0.07142072916030884\n",
      "Train 222 | out_loss 0.3975256383419037: 100%|█| 125/125 [00:00<00:00, 268.24it/\n",
      "Train Epoch222 out_loss 0.15802662074565887, R2 0.07302713394165039\n",
      "Test Epoch222 layer0 out_loss 0.16050827503204346, R2 0.061569154262542725\n",
      "Test Epoch222 layer1 out_loss 0.1599077731370926, R2 0.06508004665374756\n",
      "Test Epoch222 layer2 out_loss 0.15930932760238647, R2 0.06857895851135254\n",
      "Test Epoch222 layer3 out_loss 0.15895992517471313, R2 0.07062172889709473\n",
      "Test Epoch222 layer4 out_loss 0.1590350717306137, R2 0.0701824426651001\n",
      "Train 223 | out_loss 0.3975253701210022: 100%|█| 125/125 [00:00<00:00, 266.85it/\n",
      "Train Epoch223 out_loss 0.15802642703056335, R2 0.07302826642990112\n",
      "Test Epoch223 layer0 out_loss 0.16056431829929352, R2 0.0612415075302124\n",
      "Test Epoch223 layer1 out_loss 0.160201758146286, R2 0.0633612871170044\n",
      "Test Epoch223 layer2 out_loss 0.15950903296470642, R2 0.06741136312484741\n",
      "Test Epoch223 layer3 out_loss 0.1589650958776474, R2 0.0705915093421936\n",
      "Test Epoch223 layer4 out_loss 0.15902841091156006, R2 0.07022136449813843\n",
      "Train 224 | out_loss 0.39754363894462585: 100%|█| 125/125 [00:00<00:00, 266.41it\n",
      "Train Epoch224 out_loss 0.15804089605808258, R2 0.07294344902038574\n",
      "Test Epoch224 layer0 out_loss 0.1605549305677414, R2 0.06129634380340576\n",
      "Test Epoch224 layer1 out_loss 0.16007544100284576, R2 0.06409978866577148\n",
      "Test Epoch224 layer2 out_loss 0.15946178138256073, R2 0.06768763065338135\n",
      "Test Epoch224 layer3 out_loss 0.15894532203674316, R2 0.07070708274841309\n",
      "Test Epoch224 layer4 out_loss 0.15896886587142944, R2 0.07056945562362671\n",
      "Train 225 | out_loss 0.39754459261894226: 100%|█| 125/125 [00:00<00:00, 270.34it\n",
      "Train Epoch225 out_loss 0.15804168581962585, R2 0.07293879985809326\n",
      "Test Epoch225 layer0 out_loss 0.16051806509494781, R2 0.06151193380355835\n",
      "Test Epoch225 layer1 out_loss 0.15990470349788666, R2 0.06509798765182495\n",
      "Test Epoch225 layer2 out_loss 0.15934385359287262, R2 0.06837707757949829\n",
      "Test Epoch225 layer3 out_loss 0.15901120007038116, R2 0.07032197713851929\n",
      "Test Epoch225 layer4 out_loss 0.15901051461696625, R2 0.07032597064971924\n",
      "Train 226 | out_loss 0.3974941670894623: 100%|█| 125/125 [00:00<00:00, 269.10it/\n",
      "Train Epoch226 out_loss 0.1580016016960144, R2 0.07317394018173218\n",
      "Test Epoch226 layer0 out_loss 0.160634845495224, R2 0.06082916259765625\n",
      "Test Epoch226 layer1 out_loss 0.16064013540744781, R2 0.06079822778701782\n",
      "Test Epoch226 layer2 out_loss 0.15996861457824707, R2 0.06472432613372803\n",
      "Test Epoch226 layer3 out_loss 0.15983407199382782, R2 0.06551092863082886\n",
      "Test Epoch226 layer4 out_loss 0.1596534550189972, R2 0.06656694412231445\n",
      "Train 227 | out_loss 0.3975170850753784: 100%|█| 125/125 [00:00<00:00, 273.49it/\n",
      "Train Epoch227 out_loss 0.15801982581615448, R2 0.0730670690536499\n",
      "Test Epoch227 layer0 out_loss 0.160541832447052, R2 0.06137293577194214\n",
      "Test Epoch227 layer1 out_loss 0.1598449945449829, R2 0.06544709205627441\n",
      "Test Epoch227 layer2 out_loss 0.15928296744823456, R2 0.06873303651809692\n",
      "Test Epoch227 layer3 out_loss 0.15896934270858765, R2 0.07056671380996704\n",
      "Test Epoch227 layer4 out_loss 0.15900243818759918, R2 0.07037317752838135\n",
      "Train 228 | out_loss 0.39776530861854553: 100%|█| 125/125 [00:00<00:00, 265.79it\n",
      "Train Epoch228 out_loss 0.15821722149848938, R2 0.07190907001495361\n",
      "Test Epoch228 layer0 out_loss 0.1605837196111679, R2 0.06112802028656006\n",
      "Test Epoch228 layer1 out_loss 0.1601352095603943, R2 0.06375032663345337\n",
      "Test Epoch228 layer2 out_loss 0.1594764143228531, R2 0.06760209798812866\n",
      "Test Epoch228 layer3 out_loss 0.15909439325332642, R2 0.0698356032371521\n",
      "Test Epoch228 layer4 out_loss 0.15915435552597046, R2 0.06948500871658325\n",
      "Train 229 | out_loss 0.39758044481277466: 100%|█| 125/125 [00:00<00:00, 255.64it\n",
      "Train Epoch229 out_loss 0.15807020664215088, R2 0.07277148962020874\n",
      "Test Epoch229 layer0 out_loss 0.16049720346927643, R2 0.0616338849067688\n",
      "Test Epoch229 layer1 out_loss 0.15992212295532227, R2 0.06499618291854858\n",
      "Test Epoch229 layer2 out_loss 0.1593468189239502, R2 0.06835973262786865\n",
      "Test Epoch229 layer3 out_loss 0.15891237556934357, R2 0.07089978456497192\n",
      "Test Epoch229 layer4 out_loss 0.15888607501983643, R2 0.07105350494384766\n",
      "Train 230 | out_loss 0.3976183831691742: 100%|█| 125/125 [00:00<00:00, 257.75it/\n",
      "Train Epoch230 out_loss 0.15810038149356842, R2 0.07259446382522583\n",
      "Test Epoch230 layer0 out_loss 0.16050076484680176, R2 0.06161308288574219\n",
      "Test Epoch230 layer1 out_loss 0.15984611213207245, R2 0.06544053554534912\n",
      "Test Epoch230 layer2 out_loss 0.1592349261045456, R2 0.06901395320892334\n",
      "Test Epoch230 layer3 out_loss 0.15877236425876617, R2 0.07171833515167236\n",
      "Test Epoch230 layer4 out_loss 0.15883108973503113, R2 0.07137507200241089\n",
      "Train 231 | out_loss 0.39745351672172546: 100%|█| 125/125 [00:00<00:00, 270.40it\n",
      "Train Epoch231 out_loss 0.15796923637390137, R2 0.0733637809753418\n",
      "Test Epoch231 layer0 out_loss 0.16048607230186462, R2 0.061698973178863525\n",
      "Test Epoch231 layer1 out_loss 0.1598547399044037, R2 0.06539016962051392\n",
      "Test Epoch231 layer2 out_loss 0.15923546254634857, R2 0.06901085376739502\n",
      "Test Epoch231 layer3 out_loss 0.15889179706573486, R2 0.07102006673812866\n",
      "Test Epoch231 layer4 out_loss 0.1589018553495407, R2 0.07096129655838013\n",
      "Train 232 | out_loss 0.39746227860450745: 100%|█| 125/125 [00:00<00:00, 266.18it\n",
      "Train Epoch232 out_loss 0.15797626972198486, R2 0.07332247495651245\n",
      "Test Epoch232 layer0 out_loss 0.16070450842380524, R2 0.06042182445526123\n",
      "Test Epoch232 layer1 out_loss 0.16041576862335205, R2 0.06211000680923462\n",
      "Test Epoch232 layer2 out_loss 0.15970993041992188, R2 0.06623679399490356\n",
      "Test Epoch232 layer3 out_loss 0.15953387320041656, R2 0.06726610660552979\n",
      "Test Epoch232 layer4 out_loss 0.1594104915857315, R2 0.06798750162124634\n",
      "Train 233 | out_loss 0.39765045046806335: 100%|█| 125/125 [00:00<00:00, 268.39it\n",
      "Train Epoch233 out_loss 0.15812590718269348, R2 0.07244479656219482\n",
      "Test Epoch233 layer0 out_loss 0.16049957275390625, R2 0.06162005662918091\n",
      "Test Epoch233 layer1 out_loss 0.16010355949401855, R2 0.06393533945083618\n",
      "Test Epoch233 layer2 out_loss 0.159429669380188, R2 0.06787532567977905\n",
      "Test Epoch233 layer3 out_loss 0.15902787446975708, R2 0.07022446393966675\n",
      "Test Epoch233 layer4 out_loss 0.15893538296222687, R2 0.07076525688171387\n",
      "Train 234 | out_loss 0.3975715637207031: 100%|█| 125/125 [00:00<00:00, 269.03it/\n",
      "Train Epoch234 out_loss 0.158063143491745, R2 0.07281291484832764\n",
      "Test Epoch234 layer0 out_loss 0.1605779081583023, R2 0.06116199493408203\n",
      "Test Epoch234 layer1 out_loss 0.16043128073215485, R2 0.06201934814453125\n",
      "Test Epoch234 layer2 out_loss 0.1597341001033783, R2 0.06609547138214111\n",
      "Test Epoch234 layer3 out_loss 0.1595001518726349, R2 0.06746327877044678\n",
      "Test Epoch234 layer4 out_loss 0.15929412841796875, R2 0.06866782903671265\n",
      "Train 235 | out_loss 0.39741507172584534: 100%|█| 125/125 [00:00<00:00, 271.13it\n",
      "Train Epoch235 out_loss 0.15793876349925995, R2 0.07354247570037842\n",
      "Test Epoch235 layer0 out_loss 0.16059288382530212, R2 0.06107449531555176\n",
      "Test Epoch235 layer1 out_loss 0.1603114753961563, R2 0.06271976232528687\n",
      "Test Epoch235 layer2 out_loss 0.15964339673519135, R2 0.06662571430206299\n",
      "Test Epoch235 layer3 out_loss 0.15919925272464752, R2 0.06922250986099243\n",
      "Test Epoch235 layer4 out_loss 0.15920618176460266, R2 0.06918203830718994\n",
      "Train 236 | out_loss 0.3976457417011261: 100%|█| 125/125 [00:00<00:00, 261.69it/\n",
      "Train Epoch236 out_loss 0.15812212228775024, R2 0.07246696949005127\n",
      "Test Epoch236 layer0 out_loss 0.16047608852386475, R2 0.06175732612609863\n",
      "Test Epoch236 layer1 out_loss 0.1597946435213089, R2 0.06574147939682007\n",
      "Test Epoch236 layer2 out_loss 0.15916851162910461, R2 0.06940221786499023\n",
      "Test Epoch236 layer3 out_loss 0.15863142907619476, R2 0.07254236936569214\n",
      "Test Epoch236 layer4 out_loss 0.15876594185829163, R2 0.07175588607788086\n",
      "Train 237 | out_loss 0.39748477935791016: 100%|█| 125/125 [00:00<00:00, 273.02it\n",
      "Train Epoch237 out_loss 0.1579941064119339, R2 0.07321786880493164\n",
      "Test Epoch237 layer0 out_loss 0.16048219799995422, R2 0.061721622943878174\n",
      "Test Epoch237 layer1 out_loss 0.15978671610355377, R2 0.06578785181045532\n",
      "Test Epoch237 layer2 out_loss 0.1591503918170929, R2 0.06950819492340088\n",
      "Test Epoch237 layer3 out_loss 0.15863026678562164, R2 0.07254916429519653\n",
      "Test Epoch237 layer4 out_loss 0.15878260135650635, R2 0.07165849208831787\n",
      "Train 238 | out_loss 0.3976074159145355: 100%|█| 125/125 [00:00<00:00, 265.57it/\n",
      "Train Epoch238 out_loss 0.15809160470962524, R2 0.07264602184295654\n",
      "Test Epoch238 layer0 out_loss 0.16053028404712677, R2 0.061440467834472656\n",
      "Test Epoch238 layer1 out_loss 0.1597917228937149, R2 0.0657585859298706\n",
      "Test Epoch238 layer2 out_loss 0.1592286080121994, R2 0.06905090808868408\n",
      "Test Epoch238 layer3 out_loss 0.15874993801116943, R2 0.07184946537017822\n",
      "Test Epoch238 layer4 out_loss 0.1588931828737259, R2 0.07101196050643921\n",
      "Train 239 | out_loss 0.39779049158096313: 100%|█| 125/125 [00:00<00:00, 256.32it\n",
      "Train Epoch239 out_loss 0.1582372486591339, R2 0.0717916488647461\n",
      "Test Epoch239 layer0 out_loss 0.1605340987443924, R2 0.061418116092681885\n",
      "Test Epoch239 layer1 out_loss 0.15988761186599731, R2 0.0651978850364685\n",
      "Test Epoch239 layer2 out_loss 0.1592373102903366, R2 0.0690000057220459\n",
      "Test Epoch239 layer3 out_loss 0.15863308310508728, R2 0.07253265380859375\n",
      "Test Epoch239 layer4 out_loss 0.15882174670696259, R2 0.07142966985702515\n",
      "Train 240 | out_loss 0.3974052667617798: 100%|█| 125/125 [00:00<00:00, 255.86it/\n",
      "Train Epoch240 out_loss 0.15793097019195557, R2 0.07358825206756592\n",
      "Test Epoch240 layer0 out_loss 0.1605314314365387, R2 0.06143373250961304\n",
      "Test Epoch240 layer1 out_loss 0.16002754867076874, R2 0.06437981128692627\n",
      "Test Epoch240 layer2 out_loss 0.1593361645936966, R2 0.06842201948165894\n",
      "Test Epoch240 layer3 out_loss 0.15892963111400604, R2 0.07079893350601196\n",
      "Test Epoch240 layer4 out_loss 0.1589946746826172, R2 0.0704185962677002\n",
      "Train 241 | out_loss 0.3976142108440399: 100%|█| 125/125 [00:00<00:00, 262.24it/\n",
      "Train Epoch241 out_loss 0.158097043633461, R2 0.07261407375335693\n",
      "Test Epoch241 layer0 out_loss 0.16047033667564392, R2 0.06179094314575195\n",
      "Test Epoch241 layer1 out_loss 0.1598997563123703, R2 0.06512695550918579\n",
      "Test Epoch241 layer2 out_loss 0.15925653278827667, R2 0.06888765096664429\n",
      "Test Epoch241 layer3 out_loss 0.15900857746601105, R2 0.07033735513687134\n",
      "Test Epoch241 layer4 out_loss 0.15902791917324066, R2 0.07022422552108765\n",
      "Train 242 | out_loss 0.39745813608169556: 100%|█| 125/125 [00:00<00:00, 254.44it\n",
      "Train Epoch242 out_loss 0.15797291696071625, R2 0.0733422040939331\n",
      "Test Epoch242 layer0 out_loss 0.16050945222377777, R2 0.061562299728393555\n",
      "Test Epoch242 layer1 out_loss 0.1598912626504898, R2 0.06517654657363892\n",
      "Test Epoch242 layer2 out_loss 0.15922679007053375, R2 0.0690615177154541\n",
      "Test Epoch242 layer3 out_loss 0.15872405469417572, R2 0.07200080156326294\n",
      "Test Epoch242 layer4 out_loss 0.15884847939014435, R2 0.0712733268737793\n",
      "Train 243 | out_loss 0.39770418405532837: 100%|█| 125/125 [00:00<00:00, 266.25it\n",
      "Train Epoch243 out_loss 0.15816862881183624, R2 0.0721941590309143\n",
      "Test Epoch243 layer0 out_loss 0.16046002507209778, R2 0.06185126304626465\n",
      "Test Epoch243 layer1 out_loss 0.16022413969039917, R2 0.06323039531707764\n",
      "Test Epoch243 layer2 out_loss 0.15946342051029205, R2 0.06767797470092773\n",
      "Test Epoch243 layer3 out_loss 0.15985427796840668, R2 0.06539279222488403\n",
      "Test Epoch243 layer4 out_loss 0.15927085280418396, R2 0.06880384683609009\n",
      "Train 244 | out_loss 0.397517591714859: 100%|█| 125/125 [00:00<00:00, 268.16it/s\n",
      "Train Epoch244 out_loss 0.1580202430486679, R2 0.07306450605392456\n",
      "Test Epoch244 layer0 out_loss 0.1605302095413208, R2 0.061440885066986084\n",
      "Test Epoch244 layer1 out_loss 0.15975722670555115, R2 0.06596022844314575\n",
      "Test Epoch244 layer2 out_loss 0.1591929793357849, R2 0.0692591667175293\n",
      "Test Epoch244 layer3 out_loss 0.15879596769809723, R2 0.07158035039901733\n",
      "Test Epoch244 layer4 out_loss 0.15887121856212616, R2 0.07114040851593018\n",
      "Train 245 | out_loss 0.3975064158439636: 100%|█| 125/125 [00:00<00:00, 264.86it/\n",
      "Train Epoch245 out_loss 0.15801139175891876, R2 0.07311642169952393\n",
      "Test Epoch245 layer0 out_loss 0.16045787930488586, R2 0.06186378002166748\n",
      "Test Epoch245 layer1 out_loss 0.15973438322544098, R2 0.06609374284744263\n",
      "Test Epoch245 layer2 out_loss 0.15907324850559235, R2 0.06995922327041626\n",
      "Test Epoch245 layer3 out_loss 0.15863440930843353, R2 0.07252490520477295\n",
      "Test Epoch245 layer4 out_loss 0.15879946947097778, R2 0.07155990600585938\n",
      "Train 246 | out_loss 0.3974592387676239: 100%|█| 125/125 [00:00<00:00, 267.09it/\n",
      "Train Epoch246 out_loss 0.15797385573387146, R2 0.073336660861969\n",
      "Test Epoch246 layer0 out_loss 0.16053888201713562, R2 0.061390221118927\n",
      "Test Epoch246 layer1 out_loss 0.15981321036815643, R2 0.0656328797340393\n",
      "Test Epoch246 layer2 out_loss 0.1591540277004242, R2 0.06948697566986084\n",
      "Test Epoch246 layer3 out_loss 0.15864558517932892, R2 0.07245957851409912\n",
      "Test Epoch246 layer4 out_loss 0.1588422954082489, R2 0.07130944728851318\n",
      "Train 247 | out_loss 0.39733558893203735: 100%|█| 125/125 [00:00<00:00, 259.18it\n",
      "Train Epoch247 out_loss 0.15787553787231445, R2 0.07391339540481567\n",
      "Test Epoch247 layer0 out_loss 0.16046710312366486, R2 0.06180989742279053\n",
      "Test Epoch247 layer1 out_loss 0.15974028408527374, R2 0.06605935096740723\n",
      "Test Epoch247 layer2 out_loss 0.15911561250686646, R2 0.06971150636672974\n",
      "Test Epoch247 layer3 out_loss 0.15860848128795624, R2 0.07267653942108154\n",
      "Test Epoch247 layer4 out_loss 0.15876591205596924, R2 0.07175606489181519\n",
      "Train 248 | out_loss 0.39723432064056396: 100%|█| 125/125 [00:00<00:00, 256.99it\n",
      "Train Epoch248 out_loss 0.15779511630535126, R2 0.07438510656356812\n",
      "Test Epoch248 layer0 out_loss 0.16071830689907074, R2 0.06034117937088013\n",
      "Test Epoch248 layer1 out_loss 0.1600368618965149, R2 0.06432533264160156\n",
      "Test Epoch248 layer2 out_loss 0.15945115685462952, R2 0.0677497386932373\n",
      "Test Epoch248 layer3 out_loss 0.15899041295051575, R2 0.07044351100921631\n",
      "Test Epoch248 layer4 out_loss 0.1591176688671112, R2 0.06969946622848511\n",
      "Train 249 | out_loss 0.39743316173553467: 100%|█| 125/125 [00:00<00:00, 265.42it\n",
      "Train Epoch249 out_loss 0.1579531580209732, R2 0.07345807552337646\n",
      "Test Epoch249 layer0 out_loss 0.16044162213802338, R2 0.06195884943008423\n",
      "Test Epoch249 layer1 out_loss 0.15973429381847382, R2 0.0660942792892456\n",
      "Test Epoch249 layer2 out_loss 0.15904252231121063, R2 0.07013881206512451\n",
      "Test Epoch249 layer3 out_loss 0.15856921672821045, R2 0.07290607690811157\n",
      "Test Epoch249 layer4 out_loss 0.1587338149547577, R2 0.07194375991821289\n",
      "Train 250 | out_loss 0.3973967134952545: 100%|█| 125/125 [00:00<00:00, 268.93it/\n",
      "Train Epoch250 out_loss 0.1579241156578064, R2 0.07362842559814453\n",
      "Test Epoch250 layer0 out_loss 0.160503089427948, R2 0.0615994930267334\n",
      "Test Epoch250 layer1 out_loss 0.15978743135929108, R2 0.06578367948532104\n",
      "Test Epoch250 layer2 out_loss 0.15909703075885773, R2 0.06982016563415527\n",
      "Test Epoch250 layer3 out_loss 0.15861082077026367, R2 0.0726628303527832\n",
      "Test Epoch250 layer4 out_loss 0.15881676971912384, R2 0.07145869731903076\n",
      "Train 251 | out_loss 0.3974013328552246: 100%|█| 125/125 [00:00<00:00, 269.26it/\n",
      "Train Epoch251 out_loss 0.15792781114578247, R2 0.07360678911209106\n",
      "Test Epoch251 layer0 out_loss 0.16048231720924377, R2 0.06172090768814087\n",
      "Test Epoch251 layer1 out_loss 0.1598844975233078, R2 0.06521618366241455\n",
      "Test Epoch251 layer2 out_loss 0.15920059382915497, R2 0.06921470165252686\n",
      "Test Epoch251 layer3 out_loss 0.1589685082435608, R2 0.07057160139083862\n",
      "Test Epoch251 layer4 out_loss 0.15884250402450562, R2 0.07130831480026245\n",
      "Train 252 | out_loss 0.3975318372249603: 100%|█| 125/125 [00:00<00:00, 266.65it/\n",
      "Train Epoch252 out_loss 0.15803153812885284, R2 0.0729982852935791\n",
      "Test Epoch252 layer0 out_loss 0.1604922115802765, R2 0.061663031578063965\n",
      "Test Epoch252 layer1 out_loss 0.15980948507785797, R2 0.06565475463867188\n",
      "Test Epoch252 layer2 out_loss 0.15908382833003998, R2 0.0698973536491394\n",
      "Test Epoch252 layer3 out_loss 0.15865033864974976, R2 0.07243174314498901\n",
      "Test Epoch252 layer4 out_loss 0.15877072513103485, R2 0.07172799110412598\n",
      "Train 253 | out_loss 0.39727354049682617: 100%|█| 125/125 [00:00<00:00, 270.47it\n",
      "Train Epoch253 out_loss 0.1578262597322464, R2 0.07420241832733154\n",
      "Test Epoch253 layer0 out_loss 0.16044582426548004, R2 0.06193423271179199\n",
      "Test Epoch253 layer1 out_loss 0.16010482609272003, R2 0.06392794847488403\n",
      "Test Epoch253 layer2 out_loss 0.15940704941749573, R2 0.06800764799118042\n",
      "Test Epoch253 layer3 out_loss 0.1593049317598343, R2 0.06860464811325073\n",
      "Test Epoch253 layer4 out_loss 0.1590215116739273, R2 0.07026165723800659\n",
      "Train 254 | out_loss 0.39755669236183167: 100%|█| 125/125 [00:00<00:00, 263.37it\n",
      "Train Epoch254 out_loss 0.1580512672662735, R2 0.07288259267807007\n",
      "Test Epoch254 layer0 out_loss 0.16044291853904724, R2 0.061951279640197754\n",
      "Test Epoch254 layer1 out_loss 0.1600358635187149, R2 0.06433117389678955\n",
      "Test Epoch254 layer2 out_loss 0.15921248495578766, R2 0.06914520263671875\n",
      "Test Epoch254 layer3 out_loss 0.15891553461551666, R2 0.07088136672973633\n",
      "Test Epoch254 layer4 out_loss 0.15894149243831635, R2 0.07072949409484863\n",
      "Train 255 | out_loss 0.39743825793266296: 100%|█| 125/125 [00:00<00:00, 265.09it\n",
      "Train Epoch255 out_loss 0.15795713663101196, R2 0.07343471050262451\n",
      "Test Epoch255 layer0 out_loss 0.16044661402702332, R2 0.06192970275878906\n",
      "Test Epoch255 layer1 out_loss 0.1596747189760208, R2 0.06644266843795776\n",
      "Test Epoch255 layer2 out_loss 0.15898695588111877, R2 0.07046371698379517\n",
      "Test Epoch255 layer3 out_loss 0.15857595205307007, R2 0.07286667823791504\n",
      "Test Epoch255 layer4 out_loss 0.1587228924036026, R2 0.07200765609741211\n",
      "Train 256 | out_loss 0.3973615765571594: 100%|█| 125/125 [00:00<00:00, 265.81it/\n",
      "Train Epoch256 out_loss 0.1578962355852127, R2 0.0737919807434082\n",
      "Test Epoch256 layer0 out_loss 0.16041871905326843, R2 0.06209278106689453\n",
      "Test Epoch256 layer1 out_loss 0.1600140780210495, R2 0.06445848941802979\n",
      "Test Epoch256 layer2 out_loss 0.15921467542648315, R2 0.06913232803344727\n",
      "Test Epoch256 layer3 out_loss 0.15907496213912964, R2 0.06994915008544922\n",
      "Test Epoch256 layer4 out_loss 0.15910868346691132, R2 0.06975197792053223\n",
      "Train 257 | out_loss 0.3974529206752777: 100%|█| 125/125 [00:00<00:00, 268.26it/\n",
      "Train Epoch257 out_loss 0.15796880424022675, R2 0.07336628437042236\n",
      "Test Epoch257 layer0 out_loss 0.16042374074459076, R2 0.062063395977020264\n",
      "Test Epoch257 layer1 out_loss 0.15966925024986267, R2 0.06647461652755737\n",
      "Test Epoch257 layer2 out_loss 0.15896563231945038, R2 0.07058835029602051\n",
      "Test Epoch257 layer3 out_loss 0.158650740981102, R2 0.07242941856384277\n",
      "Test Epoch257 layer4 out_loss 0.15873099863529205, R2 0.0719602108001709\n",
      "Train 258 | out_loss 0.3975178599357605: 100%|█| 125/125 [00:00<00:00, 249.67it/\n",
      "Train Epoch258 out_loss 0.15802042186260223, R2 0.07306355237960815\n",
      "Test Epoch258 layer0 out_loss 0.1605101227760315, R2 0.06155836582183838\n",
      "Test Epoch258 layer1 out_loss 0.1598111093044281, R2 0.06564521789550781\n",
      "Test Epoch258 layer2 out_loss 0.15898728370666504, R2 0.07046175003051758\n",
      "Test Epoch258 layer3 out_loss 0.1586979478597641, R2 0.07215344905853271\n",
      "Test Epoch258 layer4 out_loss 0.15886591374874115, R2 0.0711713433265686\n",
      "Train 259 | out_loss 0.3974204957485199: 100%|█| 125/125 [00:00<00:00, 263.06it/\n",
      "Train Epoch259 out_loss 0.1579430103302002, R2 0.07351762056350708\n",
      "Test Epoch259 layer0 out_loss 0.1604088395833969, R2 0.06215047836303711\n",
      "Test Epoch259 layer1 out_loss 0.159809410572052, R2 0.06565511226654053\n",
      "Test Epoch259 layer2 out_loss 0.15904249250888824, R2 0.07013899087905884\n",
      "Test Epoch259 layer3 out_loss 0.15869593620300293, R2 0.07216519117355347\n",
      "Test Epoch259 layer4 out_loss 0.158760666847229, R2 0.07178676128387451\n",
      "Train 260 | out_loss 0.39739683270454407: 100%|█| 125/125 [00:00<00:00, 267.48it\n",
      "Train Epoch260 out_loss 0.15792423486709595, R2 0.07362771034240723\n",
      "Test Epoch260 layer0 out_loss 0.1604124903678894, R2 0.06212913990020752\n",
      "Test Epoch260 layer1 out_loss 0.1596556454896927, R2 0.06655418872833252\n",
      "Test Epoch260 layer2 out_loss 0.15891726315021515, R2 0.07087117433547974\n",
      "Test Epoch260 layer3 out_loss 0.1585785448551178, R2 0.07285147905349731\n",
      "Test Epoch260 layer4 out_loss 0.15870696306228638, R2 0.0721006989479065\n",
      "Train 261 | out_loss 0.3973262310028076: 100%|█| 125/125 [00:00<00:00, 266.43it/\n",
      "Train Epoch261 out_loss 0.1578681319952011, R2 0.07395684719085693\n",
      "Test Epoch261 layer0 out_loss 0.16041472554206848, R2 0.062116146087646484\n",
      "Test Epoch261 layer1 out_loss 0.15970271825790405, R2 0.06627899408340454\n",
      "Test Epoch261 layer2 out_loss 0.15897323191165924, R2 0.07054394483566284\n",
      "Test Epoch261 layer3 out_loss 0.15860150754451752, R2 0.07271730899810791\n",
      "Test Epoch261 layer4 out_loss 0.15871067345142365, R2 0.07207900285720825\n",
      "Train 262 | out_loss 0.3972180485725403: 100%|█| 125/125 [00:00<00:00, 268.23it/\n",
      "Train Epoch262 out_loss 0.1577821522951126, R2 0.07446122169494629\n",
      "Test Epoch262 layer0 out_loss 0.16041447222232819, R2 0.062117576599121094\n",
      "Test Epoch262 layer1 out_loss 0.1597891002893448, R2 0.06577396392822266\n",
      "Test Epoch262 layer2 out_loss 0.15895886719226837, R2 0.07062798738479614\n",
      "Test Epoch262 layer3 out_loss 0.15863268077373505, R2 0.07253509759902954\n",
      "Test Epoch262 layer4 out_loss 0.1587592512369156, R2 0.07179504632949829\n",
      "Train 263 | out_loss 0.39719775319099426: 100%|█| 125/125 [00:00<00:00, 261.29it\n",
      "Train Epoch263 out_loss 0.15776605904102325, R2 0.07455563545227051\n",
      "Test Epoch263 layer0 out_loss 0.16042426228523254, R2 0.06206035614013672\n",
      "Test Epoch263 layer1 out_loss 0.15988540649414062, R2 0.06521081924438477\n",
      "Test Epoch263 layer2 out_loss 0.1590615063905716, R2 0.07002782821655273\n",
      "Test Epoch263 layer3 out_loss 0.15896767377853394, R2 0.0705764889717102\n",
      "Test Epoch263 layer4 out_loss 0.158936008810997, R2 0.07076162099838257\n",
      "Train 264 | out_loss 0.3971887230873108: 100%|█| 125/125 [00:00<00:00, 263.79it/\n",
      "Train Epoch264 out_loss 0.15775886178016663, R2 0.07459777593612671\n",
      "Test Epoch264 layer0 out_loss 0.16041865944862366, R2 0.062093138694763184\n",
      "Test Epoch264 layer1 out_loss 0.15970097482204437, R2 0.06628912687301636\n",
      "Test Epoch264 layer2 out_loss 0.15899856388568878, R2 0.070395827293396\n",
      "Test Epoch264 layer3 out_loss 0.15870071947574615, R2 0.07213729619979858\n",
      "Test Epoch264 layer4 out_loss 0.15882647037506104, R2 0.07140201330184937\n",
      "Train 265 | out_loss 0.3973281681537628: 100%|█| 125/125 [00:00<00:00, 239.04it/\n",
      "Train Epoch265 out_loss 0.15786966681480408, R2 0.07394790649414062\n",
      "Test Epoch265 layer0 out_loss 0.1605280190706253, R2 0.06145375967025757\n",
      "Test Epoch265 layer1 out_loss 0.15962575376033783, R2 0.06672894954681396\n",
      "Test Epoch265 layer2 out_loss 0.1588876098394394, R2 0.07104462385177612\n",
      "Test Epoch265 layer3 out_loss 0.1585136204957962, R2 0.07323110103607178\n",
      "Test Epoch265 layer4 out_loss 0.15868201851844788, R2 0.07224655151367188\n",
      "Train 266 | out_loss 0.3973468244075775: 100%|█| 125/125 [00:00<00:00, 265.50it/\n",
      "Train Epoch266 out_loss 0.15788446366786957, R2 0.07386106252670288\n",
      "Test Epoch266 layer0 out_loss 0.1604284942150116, R2 0.062035560607910156\n",
      "Test Epoch266 layer1 out_loss 0.15976114571094513, R2 0.06593739986419678\n",
      "Test Epoch266 layer2 out_loss 0.1589423418045044, R2 0.07072460651397705\n",
      "Test Epoch266 layer3 out_loss 0.15873132646083832, R2 0.07195830345153809\n",
      "Test Epoch266 layer4 out_loss 0.15881319344043732, R2 0.0714796781539917\n",
      "Train 267 | out_loss 0.39731916785240173: 100%|█| 125/125 [00:00<00:00, 268.17it\n",
      "Train Epoch267 out_loss 0.15786251425743103, R2 0.07398974895477295\n",
      "Test Epoch267 layer0 out_loss 0.1604187786579132, R2 0.06209242343902588\n",
      "Test Epoch267 layer1 out_loss 0.15966127812862396, R2 0.06652116775512695\n",
      "Test Epoch267 layer2 out_loss 0.15890735387802124, R2 0.07092916965484619\n",
      "Test Epoch267 layer3 out_loss 0.1584736704826355, R2 0.07346475124359131\n",
      "Test Epoch267 layer4 out_loss 0.1586531102657318, R2 0.07241559028625488\n",
      "Train 268 | out_loss 0.397274911403656: 100%|█| 125/125 [00:00<00:00, 256.28it/s\n",
      "Train Epoch268 out_loss 0.15782734751701355, R2 0.07419610023498535\n",
      "Test Epoch268 layer0 out_loss 0.16044111549854279, R2 0.061961829662323\n",
      "Test Epoch268 layer1 out_loss 0.1596536934375763, R2 0.0665656328201294\n",
      "Test Epoch268 layer2 out_loss 0.158952996134758, R2 0.07066226005554199\n",
      "Test Epoch268 layer3 out_loss 0.1585073322057724, R2 0.0732678771018982\n",
      "Test Epoch268 layer4 out_loss 0.15867678821086884, R2 0.07227718830108643\n",
      "Train 269 | out_loss 0.39718902111053467: 100%|█| 125/125 [00:00<00:00, 263.05it\n",
      "Train Epoch269 out_loss 0.1577591449022293, R2 0.07459616661071777\n",
      "Test Epoch269 layer0 out_loss 0.16051128506660461, R2 0.06155151128768921\n",
      "Test Epoch269 layer1 out_loss 0.1603885442018509, R2 0.06226915121078491\n",
      "Test Epoch269 layer2 out_loss 0.15947949886322021, R2 0.06758403778076172\n",
      "Test Epoch269 layer3 out_loss 0.15987829864025116, R2 0.06525242328643799\n",
      "Test Epoch269 layer4 out_loss 0.15964257717132568, R2 0.0666305422782898\n",
      "Train 270 | out_loss 0.39747631549835205: 100%|█| 125/125 [00:00<00:00, 268.73it\n",
      "Train Epoch270 out_loss 0.15798743069171906, R2 0.07325708866119385\n",
      "Test Epoch270 layer0 out_loss 0.1603815257549286, R2 0.06231015920639038\n",
      "Test Epoch270 layer1 out_loss 0.15958605706691742, R2 0.06696099042892456\n",
      "Test Epoch270 layer2 out_loss 0.15883871912956238, R2 0.07133042812347412\n",
      "Test Epoch270 layer3 out_loss 0.15844394266605377, R2 0.07363855838775635\n",
      "Test Epoch270 layer4 out_loss 0.15859729051589966, R2 0.07274192571640015\n",
      "Train 271 | out_loss 0.3973129093647003: 100%|█| 125/125 [00:00<00:00, 267.56it/\n",
      "Train Epoch271 out_loss 0.1578575223684311, R2 0.07401901483535767\n",
      "Test Epoch271 layer0 out_loss 0.16038808226585388, R2 0.06227189302444458\n",
      "Test Epoch271 layer1 out_loss 0.1595853567123413, R2 0.06696510314941406\n",
      "Test Epoch271 layer2 out_loss 0.15881744027137756, R2 0.07145482301712036\n",
      "Test Epoch271 layer3 out_loss 0.15844286978244781, R2 0.07364475727081299\n",
      "Test Epoch271 layer4 out_loss 0.1585913747549057, R2 0.07277655601501465\n",
      "Train 272 | out_loss 0.3973657190799713: 100%|█| 125/125 [00:00<00:00, 258.12it/\n",
      "Train Epoch272 out_loss 0.15789952874183655, R2 0.0737726092338562\n",
      "Test Epoch272 layer0 out_loss 0.1604394167661667, R2 0.06197172403335571\n",
      "Test Epoch272 layer1 out_loss 0.16000103950500488, R2 0.06453478336334229\n",
      "Test Epoch272 layer2 out_loss 0.15902380645275116, R2 0.0702483057975769\n",
      "Test Epoch272 layer3 out_loss 0.15893115103244781, R2 0.07078999280929565\n",
      "Test Epoch272 layer4 out_loss 0.1589275598526001, R2 0.07081097364425659\n",
      "Train 273 | out_loss 0.3972267508506775: 100%|█| 125/125 [00:00<00:00, 265.32it/\n",
      "Train Epoch273 out_loss 0.15778908133506775, R2 0.07442057132720947\n",
      "Test Epoch273 layer0 out_loss 0.16037216782569885, R2 0.062364935874938965\n",
      "Test Epoch273 layer1 out_loss 0.15982097387313843, R2 0.06558758020401001\n",
      "Test Epoch273 layer2 out_loss 0.1589398980140686, R2 0.07073885202407837\n",
      "Test Epoch273 layer3 out_loss 0.15890003740787506, R2 0.07097190618515015\n",
      "Test Epoch273 layer4 out_loss 0.15886683762073517, R2 0.0711660385131836\n",
      "Train 274 | out_loss 0.3971521854400635: 100%|█| 125/125 [00:00<00:00, 266.10it/\n",
      "Train Epoch274 out_loss 0.157729834318161, R2 0.07476812601089478\n",
      "Test Epoch274 layer0 out_loss 0.16037574410438538, R2 0.0623440146446228\n",
      "Test Epoch274 layer1 out_loss 0.1596194952726364, R2 0.06676554679870605\n",
      "Test Epoch274 layer2 out_loss 0.15886838734149933, R2 0.07115691900253296\n",
      "Test Epoch274 layer3 out_loss 0.1584511399269104, R2 0.07359641790390015\n",
      "Test Epoch274 layer4 out_loss 0.15863516926765442, R2 0.07252055406570435\n",
      "Train 275 | out_loss 0.3970607817173004: 100%|█| 125/125 [00:00<00:00, 264.58it/\n",
      "Train Epoch275 out_loss 0.15765723586082458, R2 0.07519388198852539\n",
      "Test Epoch275 layer0 out_loss 0.1604042500257492, R2 0.06217736005783081\n",
      "Test Epoch275 layer1 out_loss 0.15969929099082947, R2 0.0662989616394043\n",
      "Test Epoch275 layer2 out_loss 0.1589115709066391, R2 0.0709044337272644\n",
      "Test Epoch275 layer3 out_loss 0.15858623385429382, R2 0.07280665636062622\n",
      "Test Epoch275 layer4 out_loss 0.1586894989013672, R2 0.07220286130905151\n",
      "Train 276 | out_loss 0.39717942476272583: 100%|█| 125/125 [00:00<00:00, 269.48it\n",
      "Train Epoch276 out_loss 0.15775154531002045, R2 0.07464075088500977\n",
      "Test Epoch276 layer0 out_loss 0.16053777933120728, R2 0.061396658420562744\n",
      "Test Epoch276 layer1 out_loss 0.1597592830657959, R2 0.0659482479095459\n",
      "Test Epoch276 layer2 out_loss 0.15915553271770477, R2 0.0694780945777893\n",
      "Test Epoch276 layer3 out_loss 0.1586703211069107, R2 0.07231497764587402\n",
      "Test Epoch276 layer4 out_loss 0.15892869234085083, R2 0.07080435752868652\n",
      "Train 277 | out_loss 0.3972984254360199: 100%|█| 125/125 [00:00<00:00, 267.45it/\n",
      "Train Epoch277 out_loss 0.15784601867198944, R2 0.07408654689788818\n",
      "Test Epoch277 layer0 out_loss 0.16035833954811096, R2 0.062445759773254395\n",
      "Test Epoch277 layer1 out_loss 0.15998750925064087, R2 0.064613938331604\n",
      "Test Epoch277 layer2 out_loss 0.15884636342525482, R2 0.07128572463989258\n",
      "Test Epoch277 layer3 out_loss 0.1588820070028305, R2 0.07107734680175781\n",
      "Test Epoch277 layer4 out_loss 0.1587495058774948, R2 0.07185196876525879\n",
      "Train 278 | out_loss 0.3972168266773224: 100%|█| 125/125 [00:00<00:00, 265.59it/\n",
      "Train Epoch278 out_loss 0.157781183719635, R2 0.07446682453155518\n",
      "Test Epoch278 layer0 out_loss 0.1603688895702362, R2 0.062384068965911865\n",
      "Test Epoch278 layer1 out_loss 0.15954425930976868, R2 0.06720536947250366\n",
      "Test Epoch278 layer2 out_loss 0.15874268114566803, R2 0.07189196348190308\n",
      "Test Epoch278 layer3 out_loss 0.15845537185668945, R2 0.07357174158096313\n",
      "Test Epoch278 layer4 out_loss 0.15857823193073273, R2 0.07285338640213013\n",
      "Train 279 | out_loss 0.3972487449645996: 100%|█| 125/125 [00:00<00:00, 249.52it/\n",
      "Train Epoch279 out_loss 0.15780654549598694, R2 0.0743180513381958\n",
      "Test Epoch279 layer0 out_loss 0.16036386787891388, R2 0.06241351366043091\n",
      "Test Epoch279 layer1 out_loss 0.15958403050899506, R2 0.06697285175323486\n",
      "Test Epoch279 layer2 out_loss 0.15877507627010345, R2 0.07170253992080688\n",
      "Test Epoch279 layer3 out_loss 0.15841738879680634, R2 0.07379376888275146\n",
      "Test Epoch279 layer4 out_loss 0.15856409072875977, R2 0.0729360580444336\n",
      "Train 280 | out_loss 0.3972022533416748: 100%|█| 125/125 [00:00<00:00, 257.30it/\n",
      "Train Epoch280 out_loss 0.1577696055173874, R2 0.07453477382659912\n",
      "Test Epoch280 layer0 out_loss 0.16051802039146423, R2 0.06151217222213745\n",
      "Test Epoch280 layer1 out_loss 0.15980863571166992, R2 0.06565964221954346\n",
      "Test Epoch280 layer2 out_loss 0.1590830534696579, R2 0.06990188360214233\n",
      "Test Epoch280 layer3 out_loss 0.1587643176317215, R2 0.07176542282104492\n",
      "Test Epoch280 layer4 out_loss 0.15889672935009003, R2 0.07099127769470215\n",
      "Train 281 | out_loss 0.3969952464103699: 100%|█| 125/125 [00:00<00:00, 266.09it/\n",
      "Train Epoch281 out_loss 0.15760520100593567, R2 0.07549923658370972\n",
      "Test Epoch281 layer0 out_loss 0.16034665703773499, R2 0.06251400709152222\n",
      "Test Epoch281 layer1 out_loss 0.15961147844791412, R2 0.06681239604949951\n",
      "Test Epoch281 layer2 out_loss 0.15880145132541656, R2 0.07154828310012817\n",
      "Test Epoch281 layer3 out_loss 0.1584552675485611, R2 0.07357233762741089\n",
      "Test Epoch281 layer4 out_loss 0.15861399471759796, R2 0.07264429330825806\n",
      "Train 282 | out_loss 0.3970184326171875: 100%|█| 125/125 [00:00<00:00, 265.72it/\n",
      "Train Epoch282 out_loss 0.15762364864349365, R2 0.07539099454879761\n",
      "Test Epoch282 layer0 out_loss 0.1603534072637558, R2 0.062474608421325684\n",
      "Test Epoch282 layer1 out_loss 0.1595335304737091, R2 0.06726813316345215\n",
      "Test Epoch282 layer2 out_loss 0.15874247252941132, R2 0.07189309597015381\n",
      "Test Epoch282 layer3 out_loss 0.1583908051252365, R2 0.07394915819168091\n",
      "Test Epoch282 layer4 out_loss 0.1585381031036377, R2 0.07308799028396606\n",
      "Train 283 | out_loss 0.397076815366745: 100%|█| 125/125 [00:00<00:00, 266.96it/s\n",
      "Train Epoch283 out_loss 0.15766990184783936, R2 0.07511961460113525\n",
      "Test Epoch283 layer0 out_loss 0.16038431227207184, R2 0.062293946743011475\n",
      "Test Epoch283 layer1 out_loss 0.15958325564861298, R2 0.06697738170623779\n",
      "Test Epoch283 layer2 out_loss 0.1587812304496765, R2 0.07166647911071777\n",
      "Test Epoch283 layer3 out_loss 0.15853054821491241, R2 0.07313215732574463\n",
      "Test Epoch283 layer4 out_loss 0.15865306556224823, R2 0.07241588830947876\n",
      "Train 284 | out_loss 0.39716240763664246: 100%|█| 125/125 [00:00<00:00, 253.29it\n",
      "Train Epoch284 out_loss 0.15773797035217285, R2 0.07472032308578491\n",
      "Test Epoch284 layer0 out_loss 0.1603374034166336, R2 0.06256818771362305\n",
      "Test Epoch284 layer1 out_loss 0.15958178043365479, R2 0.06698596477508545\n",
      "Test Epoch284 layer2 out_loss 0.15875251591205597, R2 0.07183438539505005\n",
      "Test Epoch284 layer3 out_loss 0.15853630006313324, R2 0.07309854030609131\n",
      "Test Epoch284 layer4 out_loss 0.15864066779613495, R2 0.07248830795288086\n",
      "Train 285 | out_loss 0.39710912108421326: 100%|█| 125/125 [00:00<00:00, 265.16it\n",
      "Train Epoch285 out_loss 0.15769566595554352, R2 0.07496851682662964\n",
      "Test Epoch285 layer0 out_loss 0.16034266352653503, R2 0.06253743171691895\n",
      "Test Epoch285 layer1 out_loss 0.15964393317699432, R2 0.06662261486053467\n",
      "Test Epoch285 layer2 out_loss 0.1589006930589676, R2 0.07096803188323975\n",
      "Test Epoch285 layer3 out_loss 0.15865205228328705, R2 0.07242178916931152\n",
      "Test Epoch285 layer4 out_loss 0.15871313214302063, R2 0.07206463813781738\n",
      "Train 286 | out_loss 0.3972020447254181: 100%|█| 125/125 [00:00<00:00, 266.95it/\n",
      "Train Epoch286 out_loss 0.15776945650577545, R2 0.07453566789627075\n",
      "Test Epoch286 layer0 out_loss 0.16043426096439362, R2 0.06200188398361206\n",
      "Test Epoch286 layer1 out_loss 0.15955792367458344, R2 0.06712549924850464\n",
      "Test Epoch286 layer2 out_loss 0.15874724090099335, R2 0.0718652606010437\n",
      "Test Epoch286 layer3 out_loss 0.1587914079427719, R2 0.07160699367523193\n",
      "Test Epoch286 layer4 out_loss 0.15873689949512482, R2 0.07192575931549072\n",
      "Train 287 | out_loss 0.39698347449302673: 100%|█| 125/125 [00:00<00:00, 267.67it\n",
      "Train Epoch287 out_loss 0.15759582817554474, R2 0.07555419206619263\n",
      "Test Epoch287 layer0 out_loss 0.1603696346282959, R2 0.06237977743148804\n",
      "Test Epoch287 layer1 out_loss 0.15958936512470245, R2 0.06694161891937256\n",
      "Test Epoch287 layer2 out_loss 0.15878210961818695, R2 0.07166141271591187\n",
      "Test Epoch287 layer3 out_loss 0.1584334373474121, R2 0.07369989156723022\n",
      "Test Epoch287 layer4 out_loss 0.15857571363449097, R2 0.07286810874938965\n",
      "Train 288 | out_loss 0.39716342091560364: 100%|█| 125/125 [00:00<00:00, 267.83it\n",
      "Train Epoch288 out_loss 0.1577388197183609, R2 0.07471537590026855\n",
      "Test Epoch288 layer0 out_loss 0.16034726798534393, R2 0.0625104308128357\n",
      "Test Epoch288 layer1 out_loss 0.1595044881105423, R2 0.06743794679641724\n",
      "Test Epoch288 layer2 out_loss 0.15867997705936432, R2 0.07225853204727173\n",
      "Test Epoch288 layer3 out_loss 0.1584153026342392, R2 0.07380598783493042\n",
      "Test Epoch288 layer4 out_loss 0.1585579514503479, R2 0.07297194004058838\n",
      "Train 289 | out_loss 0.39699336886405945: 100%|█| 125/125 [00:00<00:00, 265.75it\n",
      "Train Epoch289 out_loss 0.1576036959886551, R2 0.0755079984664917\n",
      "Test Epoch289 layer0 out_loss 0.1603332757949829, R2 0.0625922679901123\n",
      "Test Epoch289 layer1 out_loss 0.1595049947500229, R2 0.06743496656417847\n",
      "Test Epoch289 layer2 out_loss 0.15866893529891968, R2 0.07232308387756348\n",
      "Test Epoch289 layer3 out_loss 0.1583879142999649, R2 0.0739661455154419\n",
      "Test Epoch289 layer4 out_loss 0.1585274189710617, R2 0.07315045595169067\n",
      "Train 290 | out_loss 0.396881103515625: 100%|█| 125/125 [00:00<00:00, 257.28it/s\n",
      "Train Epoch290 out_loss 0.15751458704471588, R2 0.07603073120117188\n",
      "Test Epoch290 layer0 out_loss 0.16032549738883972, R2 0.06263774633407593\n",
      "Test Epoch290 layer1 out_loss 0.15948227047920227, R2 0.06756782531738281\n",
      "Test Epoch290 layer2 out_loss 0.15864679217338562, R2 0.07245254516601562\n",
      "Test Epoch290 layer3 out_loss 0.15836629271507263, R2 0.0740925669670105\n",
      "Test Epoch290 layer4 out_loss 0.15850979089736938, R2 0.07325351238250732\n",
      "Train 291 | out_loss 0.396965891122818: 100%|█| 125/125 [00:00<00:00, 263.66it/s\n",
      "Train Epoch291 out_loss 0.1575818955898285, R2 0.07563591003417969\n",
      "Test Epoch291 layer0 out_loss 0.16044192016124725, R2 0.06195706129074097\n",
      "Test Epoch291 layer1 out_loss 0.16007445752620697, R2 0.06410551071166992\n",
      "Test Epoch291 layer2 out_loss 0.15896061062812805, R2 0.07061773538589478\n",
      "Test Epoch291 layer3 out_loss 0.15937720239162445, R2 0.06818211078643799\n",
      "Test Epoch291 layer4 out_loss 0.15918989479541779, R2 0.06927722692489624\n",
      "Train 292 | out_loss 0.3968665301799774: 100%|█| 125/125 [00:00<00:00, 236.11it/\n",
      "Train Epoch292 out_loss 0.15750299394130707, R2 0.0760987401008606\n",
      "Test Epoch292 layer0 out_loss 0.16036772727966309, R2 0.062390923500061035\n",
      "Test Epoch292 layer1 out_loss 0.1597457379102707, R2 0.06602740287780762\n",
      "Test Epoch292 layer2 out_loss 0.15882878005504608, R2 0.07138854265213013\n",
      "Test Epoch292 layer3 out_loss 0.15867681801319122, R2 0.0722770094871521\n",
      "Test Epoch292 layer4 out_loss 0.1587974578142166, R2 0.07157164812088013\n",
      "Train 293 | out_loss 0.39686647057533264: 100%|█| 125/125 [00:00<00:00, 263.78it\n",
      "Train Epoch293 out_loss 0.15750300884246826, R2 0.07609862089157104\n",
      "Test Epoch293 layer0 out_loss 0.16044846177101135, R2 0.06191891431808472\n",
      "Test Epoch293 layer1 out_loss 0.15948887169361115, R2 0.06752920150756836\n",
      "Test Epoch293 layer2 out_loss 0.15865926444530487, R2 0.07237958908081055\n",
      "Test Epoch293 layer3 out_loss 0.1584230363368988, R2 0.0737607479095459\n",
      "Test Epoch293 layer4 out_loss 0.15853525698184967, R2 0.0731046199798584\n",
      "Train 294 | out_loss 0.3972282111644745: 100%|█| 125/125 [00:00<00:00, 265.05it/\n",
      "Train Epoch294 out_loss 0.15779021382331848, R2 0.0744139552116394\n",
      "Test Epoch294 layer0 out_loss 0.16031378507614136, R2 0.06270629167556763\n",
      "Test Epoch294 layer1 out_loss 0.15973755717277527, R2 0.06607520580291748\n",
      "Test Epoch294 layer2 out_loss 0.15892824530601501, R2 0.07080703973770142\n",
      "Test Epoch294 layer3 out_loss 0.1586311161518097, R2 0.0725441575050354\n",
      "Test Epoch294 layer4 out_loss 0.15869729220867157, R2 0.07215726375579834\n",
      "Train 295 | out_loss 0.39695096015930176: 100%|█| 125/125 [00:00<00:00, 264.54it\n",
      "Train Epoch295 out_loss 0.1575700342655182, R2 0.07570540904998779\n",
      "Test Epoch295 layer0 out_loss 0.16031450033187866, R2 0.06270205974578857\n",
      "Test Epoch295 layer1 out_loss 0.15951962769031525, R2 0.06734943389892578\n",
      "Test Epoch295 layer2 out_loss 0.15862035751342773, R2 0.07260704040527344\n",
      "Test Epoch295 layer3 out_loss 0.15839850902557373, R2 0.07390409708023071\n",
      "Test Epoch295 layer4 out_loss 0.15852516889572144, R2 0.07316368818283081\n",
      "Train 296 | out_loss 0.39691445231437683: 100%|█| 125/125 [00:00<00:00, 261.38it\n",
      "Train Epoch296 out_loss 0.15754108130931854, R2 0.07587534189224243\n",
      "Test Epoch296 layer0 out_loss 0.16031958162784576, R2 0.06267237663269043\n",
      "Test Epoch296 layer1 out_loss 0.15954892337322235, R2 0.06717813014984131\n",
      "Test Epoch296 layer2 out_loss 0.1586608737707138, R2 0.07237023115158081\n",
      "Test Epoch296 layer3 out_loss 0.15851861238479614, R2 0.07320195436477661\n",
      "Test Epoch296 layer4 out_loss 0.15859952569007874, R2 0.07272881269454956\n",
      "Train 297 | out_loss 0.3969390094280243: 100%|█| 125/125 [00:00<00:00, 265.27it/\n",
      "Train Epoch297 out_loss 0.1575605720281601, R2 0.07576096057891846\n",
      "Test Epoch297 layer0 out_loss 0.16035377979278564, R2 0.06247246265411377\n",
      "Test Epoch297 layer1 out_loss 0.15958184003829956, R2 0.06698572635650635\n",
      "Test Epoch297 layer2 out_loss 0.15874697268009186, R2 0.07186681032180786\n",
      "Test Epoch297 layer3 out_loss 0.1583833545446396, R2 0.07399272918701172\n",
      "Test Epoch297 layer4 out_loss 0.1585310995578766, R2 0.07312893867492676\n",
      "Train 298 | out_loss 0.3970610499382019: 100%|█| 125/125 [00:00<00:00, 259.21it/\n",
      "Train Epoch298 out_loss 0.1576574742794037, R2 0.07519257068634033\n",
      "Test Epoch298 layer0 out_loss 0.16030371189117432, R2 0.06276518106460571\n",
      "Test Epoch298 layer1 out_loss 0.15951122343540192, R2 0.06739848852157593\n",
      "Test Epoch298 layer2 out_loss 0.15869417786598206, R2 0.07217544317245483\n",
      "Test Epoch298 layer3 out_loss 0.15841808915138245, R2 0.07378965616226196\n",
      "Test Epoch298 layer4 out_loss 0.158534437417984, R2 0.0731094479560852\n",
      "Train 299 | out_loss 0.3971022963523865: 100%|█| 125/125 [00:00<00:00, 258.25it/\n",
      "Train Epoch299 out_loss 0.15769021213054657, R2 0.07500046491622925\n",
      "Test Epoch299 layer0 out_loss 0.16029764711856842, R2 0.0628005862236023\n",
      "Test Epoch299 layer1 out_loss 0.15948128700256348, R2 0.06757354736328125\n",
      "Test Epoch299 layer2 out_loss 0.15866762399673462, R2 0.0723307728767395\n",
      "Test Epoch299 layer3 out_loss 0.15842056274414062, R2 0.07377523183822632\n",
      "Test Epoch299 layer4 out_loss 0.1584925651550293, R2 0.07335424423217773\n",
      "Train 300 | out_loss 0.3969983160495758: 100%|█| 125/125 [00:00<00:00, 246.60it/\n",
      "Train Epoch300 out_loss 0.15760764479637146, R2 0.07548481225967407\n",
      "Test Epoch300 layer0 out_loss 0.16036489605903625, R2 0.06240743398666382\n",
      "Test Epoch300 layer1 out_loss 0.15947791934013367, R2 0.06759321689605713\n",
      "Test Epoch300 layer2 out_loss 0.15854933857917786, R2 0.07302236557006836\n",
      "Test Epoch300 layer3 out_loss 0.15853139758110046, R2 0.0731271505355835\n",
      "Test Epoch300 layer4 out_loss 0.15854769945144653, R2 0.07303190231323242\n",
      "Train 301 | out_loss 0.3967919945716858: 100%|█| 125/125 [00:00<00:00, 261.81it/\n",
      "Train Epoch301 out_loss 0.15744391083717346, R2 0.07644528150558472\n",
      "Test Epoch301 layer0 out_loss 0.1603795289993286, R2 0.06232184171676636\n",
      "Test Epoch301 layer1 out_loss 0.15969441831111908, R2 0.06632751226425171\n",
      "Test Epoch301 layer2 out_loss 0.15884077548980713, R2 0.07131838798522949\n",
      "Test Epoch301 layer3 out_loss 0.15902182459831238, R2 0.07025986909866333\n",
      "Test Epoch301 layer4 out_loss 0.1589745730161667, R2 0.07053607702255249\n",
      "Train 302 | out_loss 0.3970436453819275: 100%|█| 125/125 [00:00<00:00, 271.26it/\n",
      "Train Epoch302 out_loss 0.15764367580413818, R2 0.07527345418930054\n",
      "Test Epoch302 layer0 out_loss 0.16042742133140564, R2 0.06204193830490112\n",
      "Test Epoch302 layer1 out_loss 0.15999987721443176, R2 0.06454157829284668\n",
      "Test Epoch302 layer2 out_loss 0.15894949436187744, R2 0.07068276405334473\n",
      "Test Epoch302 layer3 out_loss 0.15933457016944885, R2 0.06843137741088867\n",
      "Test Epoch302 layer4 out_loss 0.1592460721731186, R2 0.06894874572753906\n",
      "Train 303 | out_loss 0.39697226881980896: 100%|█| 125/125 [00:00<00:00, 268.78it\n",
      "Train Epoch303 out_loss 0.15758700668811798, R2 0.07560592889785767\n",
      "Test Epoch303 layer0 out_loss 0.16028857231140137, R2 0.06285363435745239\n",
      "Test Epoch303 layer1 out_loss 0.15942896902561188, R2 0.06787943840026855\n",
      "Test Epoch303 layer2 out_loss 0.15859006345272064, R2 0.07278424501419067\n",
      "Test Epoch303 layer3 out_loss 0.15858687460422516, R2 0.07280290126800537\n",
      "Test Epoch303 layer4 out_loss 0.15858645737171173, R2 0.07280528545379639\n",
      "Train 304 | out_loss 0.39683040976524353: 100%|█| 125/125 [00:00<00:00, 266.52it\n",
      "Train Epoch304 out_loss 0.1574743688106537, R2 0.07626664638519287\n",
      "Test Epoch304 layer0 out_loss 0.1603444516658783, R2 0.06252694129943848\n",
      "Test Epoch304 layer1 out_loss 0.15942013263702393, R2 0.06793111562728882\n",
      "Test Epoch304 layer2 out_loss 0.15854217112064362, R2 0.07306420803070068\n",
      "Test Epoch304 layer3 out_loss 0.15836293995380402, R2 0.07411205768585205\n",
      "Test Epoch304 layer4 out_loss 0.15844997763633728, R2 0.07360321283340454\n",
      "Train 305 | out_loss 0.3967169523239136: 100%|█| 125/125 [00:00<00:00, 259.21it/\n",
      "Train Epoch305 out_loss 0.15738430619239807, R2 0.07679492235183716\n",
      "Test Epoch305 layer0 out_loss 0.16040804982185364, R2 0.06215512752532959\n",
      "Test Epoch305 layer1 out_loss 0.15945230424404144, R2 0.06774300336837769\n",
      "Test Epoch305 layer2 out_loss 0.15862353146076202, R2 0.07258850336074829\n",
      "Test Epoch305 layer3 out_loss 0.15835368633270264, R2 0.07416623830795288\n",
      "Test Epoch305 layer4 out_loss 0.15845276415348053, R2 0.07358700037002563\n",
      "Train 306 | out_loss 0.3967761695384979: 100%|█| 125/125 [00:00<00:00, 262.90it/\n",
      "Train Epoch306 out_loss 0.15743130445480347, R2 0.07651925086975098\n",
      "Test Epoch306 layer0 out_loss 0.16027705371379852, R2 0.06292098760604858\n",
      "Test Epoch306 layer1 out_loss 0.15959914028644562, R2 0.06688451766967773\n",
      "Test Epoch306 layer2 out_loss 0.15874598920345306, R2 0.07187259197235107\n",
      "Test Epoch306 layer3 out_loss 0.15836523473262787, R2 0.07409870624542236\n",
      "Test Epoch306 layer4 out_loss 0.1585450917482376, R2 0.07304716110229492\n",
      "Train 307 | out_loss 0.3970549404621124: 100%|█| 125/125 [00:00<00:00, 264.91it/\n",
      "Train Epoch307 out_loss 0.1576526165008545, R2 0.0752210021018982\n",
      "Test Epoch307 layer0 out_loss 0.16028918325901031, R2 0.06285011768341064\n",
      "Test Epoch307 layer1 out_loss 0.15939627587795258, R2 0.06807059049606323\n",
      "Test Epoch307 layer2 out_loss 0.15850520133972168, R2 0.07328033447265625\n",
      "Test Epoch307 layer3 out_loss 0.15837450325489044, R2 0.07404452562332153\n",
      "Test Epoch307 layer4 out_loss 0.15845660865306854, R2 0.07356446981430054\n",
      "Train 308 | out_loss 0.39692866802215576: 100%|█| 125/125 [00:00<00:00, 260.51it\n",
      "Train Epoch308 out_loss 0.15755242109298706, R2 0.0758088231086731\n",
      "Test Epoch308 layer0 out_loss 0.1602715700864792, R2 0.06295305490493774\n",
      "Test Epoch308 layer1 out_loss 0.15949395298957825, R2 0.06749945878982544\n",
      "Test Epoch308 layer2 out_loss 0.1585300862789154, R2 0.07313483953475952\n",
      "Test Epoch308 layer3 out_loss 0.1585700362920761, R2 0.07290130853652954\n",
      "Test Epoch308 layer4 out_loss 0.158599391579628, R2 0.07272964715957642\n",
      "Train 309 | out_loss 0.3967949450016022: 100%|█| 125/125 [00:00<00:00, 259.67it/\n",
      "Train Epoch309 out_loss 0.1574462652206421, R2 0.0764315128326416\n",
      "Test Epoch309 layer0 out_loss 0.16038167476654053, R2 0.0623093843460083\n",
      "Test Epoch309 layer1 out_loss 0.15946902334690094, R2 0.06764525175094604\n",
      "Test Epoch309 layer2 out_loss 0.1585909128189087, R2 0.07277923822402954\n",
      "Test Epoch309 layer3 out_loss 0.15841595828533173, R2 0.0738021731376648\n",
      "Test Epoch309 layer4 out_loss 0.15854080021381378, R2 0.07307219505310059\n",
      "Train 310 | out_loss 0.3965602219104767: 100%|█| 125/125 [00:00<00:00, 257.96it/\n",
      "Train Epoch310 out_loss 0.157260000705719, R2 0.07752412557601929\n",
      "Test Epoch310 layer0 out_loss 0.16030307114124298, R2 0.06276893615722656\n",
      "Test Epoch310 layer1 out_loss 0.15943995118141174, R2 0.06781518459320068\n",
      "Test Epoch310 layer2 out_loss 0.15857483446598053, R2 0.07287329435348511\n",
      "Test Epoch310 layer3 out_loss 0.1583959460258484, R2 0.07391911745071411\n",
      "Test Epoch310 layer4 out_loss 0.15849348902702332, R2 0.07334887981414795\n",
      "Train 311 | out_loss 0.3966551423072815: 100%|█| 125/125 [00:00<00:00, 261.69it/\n",
      "Train Epoch311 out_loss 0.1573353260755539, R2 0.07708221673965454\n",
      "Test Epoch311 layer0 out_loss 0.16035941243171692, R2 0.062439560890197754\n",
      "Test Epoch311 layer1 out_loss 0.15964116156101227, R2 0.06663882732391357\n",
      "Test Epoch311 layer2 out_loss 0.15884792804718018, R2 0.07127654552459717\n",
      "Test Epoch311 layer3 out_loss 0.15873666107654572, R2 0.07192707061767578\n",
      "Test Epoch311 layer4 out_loss 0.15872952342033386, R2 0.07196885347366333\n",
      "Train 312 | out_loss 0.39674460887908936: 100%|█| 125/125 [00:00<00:00, 256.92it\n",
      "Train Epoch312 out_loss 0.1574062556028366, R2 0.07666617631912231\n",
      "Test Epoch312 layer0 out_loss 0.16028323769569397, R2 0.0628848671913147\n",
      "Test Epoch312 layer1 out_loss 0.15936817228794098, R2 0.06823486089706421\n",
      "Test Epoch312 layer2 out_loss 0.1584787666797638, R2 0.07343494892120361\n",
      "Test Epoch312 layer3 out_loss 0.15840615332126617, R2 0.07385945320129395\n",
      "Test Epoch312 layer4 out_loss 0.1584622710943222, R2 0.0735313892364502\n",
      "Train 313 | out_loss 0.3966667652130127: 100%|█| 125/125 [00:00<00:00, 260.53it/\n",
      "Train Epoch313 out_loss 0.1573444902896881, R2 0.07702845335006714\n",
      "Test Epoch313 layer0 out_loss 0.16028253734111786, R2 0.0628889799118042\n",
      "Test Epoch313 layer1 out_loss 0.15937982499599457, R2 0.06816679239273071\n",
      "Test Epoch313 layer2 out_loss 0.1586093306541443, R2 0.07267159223556519\n",
      "Test Epoch313 layer3 out_loss 0.15845170617103577, R2 0.0735931396484375\n",
      "Test Epoch313 layer4 out_loss 0.15854580700397491, R2 0.07304292917251587\n",
      "Train 314 | out_loss 0.3966650664806366: 100%|█| 125/125 [00:00<00:00, 261.79it/\n",
      "Train Epoch314 out_loss 0.15734310448169708, R2 0.07703655958175659\n",
      "Test Epoch314 layer0 out_loss 0.16039475798606873, R2 0.0622328519821167\n",
      "Test Epoch314 layer1 out_loss 0.15945298969745636, R2 0.06773900985717773\n",
      "Test Epoch314 layer2 out_loss 0.1585269272327423, R2 0.07315331697463989\n",
      "Test Epoch314 layer3 out_loss 0.15856920182704926, R2 0.07290619611740112\n",
      "Test Epoch314 layer4 out_loss 0.1586121767759323, R2 0.07265490293502808\n",
      "Train 315 | out_loss 0.3968154489994049: 100%|█| 125/125 [00:00<00:00, 255.37it/\n",
      "Train Epoch315 out_loss 0.15746243298053741, R2 0.07633662223815918\n",
      "Test Epoch315 layer0 out_loss 0.160284623503685, R2 0.06287676095962524\n",
      "Test Epoch315 layer1 out_loss 0.15943282842636108, R2 0.06785684823989868\n",
      "Test Epoch315 layer2 out_loss 0.15848734974861145, R2 0.07338470220565796\n",
      "Test Epoch315 layer3 out_loss 0.15836896002292633, R2 0.07407695055007935\n",
      "Test Epoch315 layer4 out_loss 0.15846173465251923, R2 0.07353448867797852\n",
      "Train 316 | out_loss 0.3965972065925598: 100%|█| 125/125 [00:00<00:00, 259.17it/\n",
      "Train Epoch316 out_loss 0.1572893112897873, R2 0.07735216617584229\n",
      "Test Epoch316 layer0 out_loss 0.16027922928333282, R2 0.06290823221206665\n",
      "Test Epoch316 layer1 out_loss 0.1593867540359497, R2 0.06812626123428345\n",
      "Test Epoch316 layer2 out_loss 0.15844745934009552, R2 0.07361793518066406\n",
      "Test Epoch316 layer3 out_loss 0.15830472111701965, R2 0.07445251941680908\n",
      "Test Epoch316 layer4 out_loss 0.1584041714668274, R2 0.07387101650238037\n",
      "Train 317 | out_loss 0.3967572748661041: 100%|█| 125/125 [00:00<00:00, 269.02it/\n",
      "Train Epoch317 out_loss 0.15741631388664246, R2 0.0766071081161499\n",
      "Test Epoch317 layer0 out_loss 0.16029497981071472, R2 0.06281620264053345\n",
      "Test Epoch317 layer1 out_loss 0.1593581587076187, R2 0.06829345226287842\n",
      "Test Epoch317 layer2 out_loss 0.1586078405380249, R2 0.07268029451370239\n",
      "Test Epoch317 layer3 out_loss 0.15839117765426636, R2 0.073947012424469\n",
      "Test Epoch317 layer4 out_loss 0.15843705832958221, R2 0.07367879152297974\n",
      "Train 318 | out_loss 0.39682361483573914: 100%|█| 125/125 [00:00<00:00, 253.31it\n",
      "Train Epoch318 out_loss 0.15746892988681793, R2 0.0762985348701477\n",
      "Test Epoch318 layer0 out_loss 0.16043779253959656, R2 0.061981260776519775\n",
      "Test Epoch318 layer1 out_loss 0.1596265584230423, R2 0.06672424077987671\n",
      "Test Epoch318 layer2 out_loss 0.15869423747062683, R2 0.07217508554458618\n",
      "Test Epoch318 layer3 out_loss 0.1586625576019287, R2 0.07236039638519287\n",
      "Test Epoch318 layer4 out_loss 0.15876422822475433, R2 0.0717659592628479\n",
      "Train 319 | out_loss 0.39685821533203125: 100%|█| 125/125 [00:00<00:00, 263.83it\n",
      "Train Epoch319 out_loss 0.157496377825737, R2 0.0761374831199646\n",
      "Test Epoch319 layer0 out_loss 0.1602625995874405, R2 0.06300550699234009\n",
      "Test Epoch319 layer1 out_loss 0.15933538973331451, R2 0.06842654943466187\n",
      "Test Epoch319 layer2 out_loss 0.15844962000846863, R2 0.07360535860061646\n",
      "Test Epoch319 layer3 out_loss 0.15839534997940063, R2 0.07392257452011108\n",
      "Test Epoch319 layer4 out_loss 0.15843747556209564, R2 0.07367634773254395\n",
      "Train 320 | out_loss 0.3967270255088806: 100%|█| 125/125 [00:00<00:00, 266.32it/\n",
      "Train Epoch320 out_loss 0.15739235281944275, R2 0.07674777507781982\n",
      "Test Epoch320 layer0 out_loss 0.16031455993652344, R2 0.06270170211791992\n",
      "Test Epoch320 layer1 out_loss 0.15939977765083313, R2 0.0680500864982605\n",
      "Test Epoch320 layer2 out_loss 0.15860910713672638, R2 0.07267290353775024\n",
      "Test Epoch320 layer3 out_loss 0.15840068459510803, R2 0.07389146089553833\n",
      "Test Epoch320 layer4 out_loss 0.1585025042295456, R2 0.07329612970352173\n",
      "Train 321 | out_loss 0.3965860605239868: 100%|█| 125/125 [00:00<00:00, 254.76it/\n",
      "Train Epoch321 out_loss 0.15728044509887695, R2 0.0774042010307312\n",
      "Test Epoch321 layer0 out_loss 0.16028618812561035, R2 0.06286758184432983\n",
      "Test Epoch321 layer1 out_loss 0.15946869552135468, R2 0.06764721870422363\n",
      "Test Epoch321 layer2 out_loss 0.1585686355829239, R2 0.07290947437286377\n",
      "Test Epoch321 layer3 out_loss 0.1589348018169403, R2 0.07076865434646606\n",
      "Test Epoch321 layer4 out_loss 0.15881040692329407, R2 0.07149595022201538\n",
      "Train 322 | out_loss 0.3967195153236389: 100%|█| 125/125 [00:00<00:00, 267.05it/\n",
      "Train Epoch322 out_loss 0.15738634765148163, R2 0.0767829418182373\n",
      "Test Epoch322 layer0 out_loss 0.16023044288158417, R2 0.06319355964660645\n",
      "Test Epoch322 layer1 out_loss 0.15932540595531464, R2 0.06848490238189697\n",
      "Test Epoch322 layer2 out_loss 0.15839211642742157, R2 0.07394152879714966\n",
      "Test Epoch322 layer3 out_loss 0.15825751423835754, R2 0.07472854852676392\n",
      "Test Epoch322 layer4 out_loss 0.15835046768188477, R2 0.0741850733757019\n",
      "Train 323 | out_loss 0.3968717157840729: 100%|█| 125/125 [00:00<00:00, 268.26it/\n",
      "Train Epoch323 out_loss 0.15750710666179657, R2 0.07607454061508179\n",
      "Test Epoch323 layer0 out_loss 0.16038164496421814, R2 0.06230956315994263\n",
      "Test Epoch323 layer1 out_loss 0.1596108376979828, R2 0.06681609153747559\n",
      "Test Epoch323 layer2 out_loss 0.1584746390581131, R2 0.07345902919769287\n",
      "Test Epoch323 layer3 out_loss 0.15890467166900635, R2 0.07094478607177734\n",
      "Test Epoch323 layer4 out_loss 0.1589406579732895, R2 0.07073438167572021\n",
      "Train 324 | out_loss 0.39641791582107544: 100%|█| 125/125 [00:00<00:00, 263.21it\n",
      "Train Epoch324 out_loss 0.1571471244096756, R2 0.07818621397018433\n",
      "Test Epoch324 layer0 out_loss 0.1603538542985916, R2 0.062471985816955566\n",
      "Test Epoch324 layer1 out_loss 0.1593635082244873, R2 0.06826215982437134\n",
      "Test Epoch324 layer2 out_loss 0.1585082709789276, R2 0.07326239347457886\n",
      "Test Epoch324 layer3 out_loss 0.1582980751991272, R2 0.07449132204055786\n",
      "Test Epoch324 layer4 out_loss 0.1583753228187561, R2 0.07403969764709473\n",
      "Train 325 | out_loss 0.39683040976524353: 100%|█| 125/125 [00:00<00:00, 263.02it\n",
      "Train Epoch325 out_loss 0.1574743688106537, R2 0.07626664638519287\n",
      "Test Epoch325 layer0 out_loss 0.16022445261478424, R2 0.0632285475730896\n",
      "Test Epoch325 layer1 out_loss 0.15930788218975067, R2 0.06858742237091064\n",
      "Test Epoch325 layer2 out_loss 0.15840359032154083, R2 0.07387441396713257\n",
      "Test Epoch325 layer3 out_loss 0.1582714170217514, R2 0.07464718818664551\n",
      "Test Epoch325 layer4 out_loss 0.15835753083229065, R2 0.07414376735687256\n",
      "Train 326 | out_loss 0.39659884572029114: 100%|█| 125/125 [00:00<00:00, 263.19it\n",
      "Train Epoch326 out_loss 0.15729063749313354, R2 0.07734441757202148\n",
      "Test Epoch326 layer0 out_loss 0.16023923456668854, R2 0.06314218044281006\n",
      "Test Epoch326 layer1 out_loss 0.15950381755828857, R2 0.06744182109832764\n",
      "Test Epoch326 layer2 out_loss 0.15846267342567444, R2 0.07352900505065918\n",
      "Test Epoch326 layer3 out_loss 0.15848901867866516, R2 0.07337498664855957\n",
      "Test Epoch326 layer4 out_loss 0.15863613784313202, R2 0.07251483201980591\n",
      "Train 327 | out_loss 0.39673882722854614: 100%|█| 125/125 [00:00<00:00, 267.29it\n",
      "Train Epoch327 out_loss 0.1574016660451889, R2 0.07669305801391602\n",
      "Test Epoch327 layer0 out_loss 0.16025112569332123, R2 0.0630725622177124\n",
      "Test Epoch327 layer1 out_loss 0.1595028191804886, R2 0.06744766235351562\n",
      "Test Epoch327 layer2 out_loss 0.1584995537996292, R2 0.07331335544586182\n",
      "Test Epoch327 layer3 out_loss 0.1589282751083374, R2 0.07080686092376709\n",
      "Test Epoch327 layer4 out_loss 0.15877291560173035, R2 0.07171511650085449\n",
      "Train 328 | out_loss 0.39642980694770813: 100%|█| 125/125 [00:00<00:00, 266.97it\n",
      "Train Epoch328 out_loss 0.15715661644935608, R2 0.07813054323196411\n",
      "Test Epoch328 layer0 out_loss 0.16024577617645264, R2 0.06310391426086426\n",
      "Test Epoch328 layer1 out_loss 0.1594742387533188, R2 0.06761473417282104\n",
      "Test Epoch328 layer2 out_loss 0.15861740708351135, R2 0.07262438535690308\n",
      "Test Epoch328 layer3 out_loss 0.15869219601154327, R2 0.07218706607818604\n",
      "Test Epoch328 layer4 out_loss 0.15880246460437775, R2 0.07154238224029541\n",
      "Train 329 | out_loss 0.39673662185668945: 100%|█| 125/125 [00:00<00:00, 265.10it\n",
      "Train Epoch329 out_loss 0.1574000120162964, R2 0.07670283317565918\n",
      "Test Epoch329 layer0 out_loss 0.16024331748485565, R2 0.06311827898025513\n",
      "Test Epoch329 layer1 out_loss 0.1593146175146103, R2 0.06854802370071411\n",
      "Test Epoch329 layer2 out_loss 0.15838107466697693, R2 0.0740060806274414\n",
      "Test Epoch329 layer3 out_loss 0.15838854014873505, R2 0.07396245002746582\n",
      "Test Epoch329 layer4 out_loss 0.1584460586309433, R2 0.07362616062164307\n",
      "Train 330 | out_loss 0.39667490124702454: 100%|█| 125/125 [00:00<00:00, 268.23it\n",
      "Train Epoch330 out_loss 0.15735089778900146, R2 0.07699096202850342\n",
      "Test Epoch330 layer0 out_loss 0.16030339896678925, R2 0.06276696920394897\n",
      "Test Epoch330 layer1 out_loss 0.1593097597360611, R2 0.06857645511627197\n",
      "Test Epoch330 layer2 out_loss 0.15843160450458527, R2 0.0737106204032898\n",
      "Test Epoch330 layer3 out_loss 0.15836423635482788, R2 0.07410448789596558\n",
      "Test Epoch330 layer4 out_loss 0.15839390456676483, R2 0.07393103837966919\n",
      "Train 331 | out_loss 0.3966355323791504: 100%|█| 125/125 [00:00<00:00, 262.68it/\n",
      "Train Epoch331 out_loss 0.15731969475746155, R2 0.07717388868331909\n",
      "Test Epoch331 layer0 out_loss 0.16020146012306213, R2 0.0633629560470581\n",
      "Test Epoch331 layer1 out_loss 0.15925997495651245, R2 0.06886744499206543\n",
      "Test Epoch331 layer2 out_loss 0.15833911299705505, R2 0.07425141334533691\n",
      "Test Epoch331 layer3 out_loss 0.15825185179710388, R2 0.07476162910461426\n",
      "Test Epoch331 layer4 out_loss 0.1583261340856552, R2 0.07432723045349121\n",
      "Train 332 | out_loss 0.3965398669242859: 100%|█| 125/125 [00:00<00:00, 259.60it/\n",
      "Train Epoch332 out_loss 0.15724381804466248, R2 0.07761901617050171\n",
      "Test Epoch332 layer0 out_loss 0.16020864248275757, R2 0.06332093477249146\n",
      "Test Epoch332 layer1 out_loss 0.1592668890953064, R2 0.06882703304290771\n",
      "Test Epoch332 layer2 out_loss 0.15833917260169983, R2 0.07425105571746826\n",
      "Test Epoch332 layer3 out_loss 0.15826396644115448, R2 0.0746908187866211\n",
      "Test Epoch332 layer4 out_loss 0.15833966434001923, R2 0.07424819469451904\n",
      "Train 333 | out_loss 0.39642131328582764: 100%|█| 125/125 [00:00<00:00, 265.97it\n",
      "Train Epoch333 out_loss 0.15714983642101288, R2 0.0781702995300293\n",
      "Test Epoch333 layer0 out_loss 0.16022074222564697, R2 0.06325018405914307\n",
      "Test Epoch333 layer1 out_loss 0.15924644470214844, R2 0.06894659996032715\n",
      "Test Epoch333 layer2 out_loss 0.15834209322929382, R2 0.0742340087890625\n",
      "Test Epoch333 layer3 out_loss 0.15824081003665924, R2 0.07482612133026123\n",
      "Test Epoch333 layer4 out_loss 0.15832677483558655, R2 0.07432353496551514\n",
      "Train 334 | out_loss 0.3966608941555023: 100%|█| 125/125 [00:00<00:00, 261.86it/\n",
      "Train Epoch334 out_loss 0.15733982622623444, R2 0.07705581188201904\n",
      "Test Epoch334 layer0 out_loss 0.16019606590270996, R2 0.06339454650878906\n",
      "Test Epoch334 layer1 out_loss 0.1594378501176834, R2 0.06782752275466919\n",
      "Test Epoch334 layer2 out_loss 0.15867996215820312, R2 0.0722585916519165\n",
      "Test Epoch334 layer3 out_loss 0.1584814339876175, R2 0.07341933250427246\n",
      "Test Epoch334 layer4 out_loss 0.15853992104530334, R2 0.07307738065719604\n",
      "Train 335 | out_loss 0.3964173197746277: 100%|█| 125/125 [00:00<00:00, 265.42it/\n",
      "Train Epoch335 out_loss 0.1571466624736786, R2 0.07818889617919922\n",
      "Test Epoch335 layer0 out_loss 0.16024142503738403, R2 0.06312930583953857\n",
      "Test Epoch335 layer1 out_loss 0.1594342440366745, R2 0.0678485631942749\n",
      "Test Epoch335 layer2 out_loss 0.158405601978302, R2 0.07386261224746704\n",
      "Test Epoch335 layer3 out_loss 0.15853486955165863, R2 0.07310694456100464\n",
      "Test Epoch335 layer4 out_loss 0.15859350562095642, R2 0.07276409864425659\n",
      "Train 336 | out_loss 0.39664965867996216: 100%|█| 125/125 [00:00<00:00, 257.29it\n",
      "Train Epoch336 out_loss 0.15733090043067932, R2 0.07710820436477661\n",
      "Test Epoch336 layer0 out_loss 0.16031292080879211, R2 0.06271129846572876\n",
      "Test Epoch336 layer1 out_loss 0.15941664576530457, R2 0.067951500415802\n",
      "Test Epoch336 layer2 out_loss 0.15850727260112762, R2 0.07326823472976685\n",
      "Test Epoch336 layer3 out_loss 0.15855862200260162, R2 0.0729680061340332\n",
      "Test Epoch336 layer4 out_loss 0.15858542919158936, R2 0.07281124591827393\n",
      "Train 337 | out_loss 0.3965853154659271: 100%|█| 125/125 [00:00<00:00, 255.42it/\n",
      "Train Epoch337 out_loss 0.15727992355823517, R2 0.07740724086761475\n",
      "Test Epoch337 layer0 out_loss 0.16022461652755737, R2 0.06322765350341797\n",
      "Test Epoch337 layer1 out_loss 0.15932665765285492, R2 0.06847763061523438\n",
      "Test Epoch337 layer2 out_loss 0.1583932787179947, R2 0.07393473386764526\n",
      "Test Epoch337 layer3 out_loss 0.15836592018604279, R2 0.07409465312957764\n",
      "Test Epoch337 layer4 out_loss 0.15845610201358795, R2 0.0735674500465393\n",
      "Train 338 | out_loss 0.3969109356403351: 100%|█| 125/125 [00:00<00:00, 260.07it/\n",
      "Train Epoch338 out_loss 0.1575382798910141, R2 0.07589167356491089\n",
      "Test Epoch338 layer0 out_loss 0.16018161177635193, R2 0.06347906589508057\n",
      "Test Epoch338 layer1 out_loss 0.15971985459327698, R2 0.06617879867553711\n",
      "Test Epoch338 layer2 out_loss 0.1585647165775299, R2 0.0729324221611023\n",
      "Test Epoch338 layer3 out_loss 0.15907247364521027, R2 0.06996369361877441\n",
      "Test Epoch338 layer4 out_loss 0.15887685120105743, R2 0.07110750675201416\n",
      "Train 339 | out_loss 0.39666473865509033: 100%|█| 125/125 [00:00<00:00, 265.66it\n",
      "Train Epoch339 out_loss 0.15734295547008514, R2 0.07703745365142822\n",
      "Test Epoch339 layer0 out_loss 0.160259410738945, R2 0.06302416324615479\n",
      "Test Epoch339 layer1 out_loss 0.15923967957496643, R2 0.06898611783981323\n",
      "Test Epoch339 layer2 out_loss 0.15834084153175354, R2 0.07424134016036987\n",
      "Test Epoch339 layer3 out_loss 0.15836893022060394, R2 0.07407712936401367\n",
      "Test Epoch339 layer4 out_loss 0.15836983919143677, R2 0.07407176494598389\n",
      "Train 340 | out_loss 0.3965023159980774: 100%|█| 125/125 [00:00<00:00, 263.06it/\n",
      "Train Epoch340 out_loss 0.15721409022808075, R2 0.07779335975646973\n",
      "Test Epoch340 layer0 out_loss 0.16024014353752136, R2 0.06313681602478027\n",
      "Test Epoch340 layer1 out_loss 0.15934957563877106, R2 0.06834357976913452\n",
      "Test Epoch340 layer2 out_loss 0.15846297144889832, R2 0.0735272765159607\n",
      "Test Epoch340 layer3 out_loss 0.15827976167201996, R2 0.07459849119186401\n",
      "Test Epoch340 layer4 out_loss 0.158370241522789, R2 0.07406944036483765\n",
      "Train 341 | out_loss 0.39639195799827576: 100%|█| 125/125 [00:00<00:00, 255.68it\n",
      "Train Epoch341 out_loss 0.15712657570838928, R2 0.07830679416656494\n",
      "Test Epoch341 layer0 out_loss 0.16031472384929657, R2 0.06270074844360352\n",
      "Test Epoch341 layer1 out_loss 0.15940497815608978, R2 0.06801968812942505\n",
      "Test Epoch341 layer2 out_loss 0.1586085557937622, R2 0.07267612218856812\n",
      "Test Epoch341 layer3 out_loss 0.15878184139728546, R2 0.07166290283203125\n",
      "Test Epoch341 layer4 out_loss 0.1587592214345932, R2 0.07179522514343262\n",
      "Train 342 | out_loss 0.39642202854156494: 100%|█| 125/125 [00:00<00:00, 261.81it\n",
      "Train Epoch342 out_loss 0.15715038776397705, R2 0.07816708087921143\n",
      "Test Epoch342 layer0 out_loss 0.16019822657108307, R2 0.06338191032409668\n",
      "Test Epoch342 layer1 out_loss 0.15923817455768585, R2 0.06899493932723999\n",
      "Test Epoch342 layer2 out_loss 0.15831036865711212, R2 0.07441949844360352\n",
      "Test Epoch342 layer3 out_loss 0.15828128159046173, R2 0.0745895504951477\n",
      "Test Epoch342 layer4 out_loss 0.15834639966487885, R2 0.07420879602432251\n",
      "Train 343 | out_loss 0.3964375853538513: 100%|█| 125/125 [00:00<00:00, 265.85it/\n",
      "Train Epoch343 out_loss 0.15716277062892914, R2 0.078094482421875\n",
      "Test Epoch343 layer0 out_loss 0.160201296210289, R2 0.06336396932601929\n",
      "Test Epoch343 layer1 out_loss 0.1592988222837448, R2 0.06864035129547119\n",
      "Test Epoch343 layer2 out_loss 0.15844106674194336, R2 0.07365530729293823\n",
      "Test Epoch343 layer3 out_loss 0.15825271606445312, R2 0.07475656270980835\n",
      "Test Epoch343 layer4 out_loss 0.15835629403591156, R2 0.07415097951889038\n",
      "Train 344 | out_loss 0.3965426981449127: 100%|█| 125/125 [00:00<00:00, 267.76it/\n",
      "Train Epoch344 out_loss 0.15724612772464752, R2 0.0776054859161377\n",
      "Test Epoch344 layer0 out_loss 0.16037821769714355, R2 0.06232953071594238\n",
      "Test Epoch344 layer1 out_loss 0.15988227725028992, R2 0.06522911787033081\n",
      "Test Epoch344 layer2 out_loss 0.15882356464862823, R2 0.07141900062561035\n",
      "Test Epoch344 layer3 out_loss 0.1588919758796692, R2 0.0710189938545227\n",
      "Test Epoch344 layer4 out_loss 0.15891049802303314, R2 0.07091081142425537\n",
      "Train 345 | out_loss 0.39657649397850037: 100%|█| 125/125 [00:00<00:00, 255.46it\n",
      "Train Epoch345 out_loss 0.15727289021015167, R2 0.07744848728179932\n",
      "Test Epoch345 layer0 out_loss 0.16017717123031616, R2 0.06350499391555786\n",
      "Test Epoch345 layer1 out_loss 0.1593000739812851, R2 0.06863301992416382\n",
      "Test Epoch345 layer2 out_loss 0.15833674371242523, R2 0.0742652416229248\n",
      "Test Epoch345 layer3 out_loss 0.15836261212825775, R2 0.07411402463912964\n",
      "Test Epoch345 layer4 out_loss 0.1583556830883026, R2 0.0741545557975769\n",
      "Train 346 | out_loss 0.39641526341438293: 100%|█| 125/125 [00:00<00:00, 258.55it\n",
      "Train Epoch346 out_loss 0.15714500844478607, R2 0.07819867134094238\n",
      "Test Epoch346 layer0 out_loss 0.16016286611557007, R2 0.06358861923217773\n",
      "Test Epoch346 layer1 out_loss 0.15925942361354828, R2 0.0688706636428833\n",
      "Test Epoch346 layer2 out_loss 0.15828894078731537, R2 0.07454478740692139\n",
      "Test Epoch346 layer3 out_loss 0.1582133173942566, R2 0.07498687505722046\n",
      "Test Epoch346 layer4 out_loss 0.1582750380039215, R2 0.07462602853775024\n",
      "Train 347 | out_loss 0.3964230716228485: 100%|█| 125/125 [00:00<00:00, 255.23it/\n",
      "Train Epoch347 out_loss 0.1571512669324875, R2 0.07816195487976074\n",
      "Test Epoch347 layer0 out_loss 0.16015678644180298, R2 0.06362420320510864\n",
      "Test Epoch347 layer1 out_loss 0.15924078226089478, R2 0.06897968053817749\n",
      "Test Epoch347 layer2 out_loss 0.15834736824035645, R2 0.07420319318771362\n",
      "Test Epoch347 layer3 out_loss 0.158371701836586, R2 0.07406085729598999\n",
      "Test Epoch347 layer4 out_loss 0.15838594734668732, R2 0.07397764921188354\n",
      "Train 348 | out_loss 0.3964654505252838: 100%|█| 125/125 [00:00<00:00, 261.84it/\n",
      "Train Epoch348 out_loss 0.15718482434749603, R2 0.07796502113342285\n",
      "Test Epoch348 layer0 out_loss 0.1601872593164444, R2 0.063446044921875\n",
      "Test Epoch348 layer1 out_loss 0.1591780036687851, R2 0.06934672594070435\n",
      "Test Epoch348 layer2 out_loss 0.1582738310098648, R2 0.07463306188583374\n",
      "Test Epoch348 layer3 out_loss 0.15819993615150452, R2 0.07506513595581055\n",
      "Test Epoch348 layer4 out_loss 0.1582755595445633, R2 0.0746229887008667\n",
      "Train 349 | out_loss 0.39639943838119507: 100%|█| 125/125 [00:00<00:00, 250.26it\n",
      "Train Epoch349 out_loss 0.15713244676589966, R2 0.07827234268188477\n",
      "Test Epoch349 layer0 out_loss 0.1602882593870163, R2 0.0628555417060852\n",
      "Test Epoch349 layer1 out_loss 0.15938229858875275, R2 0.06815230846405029\n",
      "Test Epoch349 layer2 out_loss 0.15837465226650238, R2 0.0740436315536499\n",
      "Test Epoch349 layer3 out_loss 0.15837307274341583, R2 0.07405287027359009\n",
      "Test Epoch349 layer4 out_loss 0.15845757722854614, R2 0.07355880737304688\n",
      "Train 350 | out_loss 0.39635753631591797: 100%|█| 125/125 [00:00<00:00, 263.27it\n",
      "Train Epoch350 out_loss 0.15709924697875977, R2 0.07846707105636597\n",
      "Test Epoch350 layer0 out_loss 0.16032642126083374, R2 0.06263244152069092\n",
      "Test Epoch350 layer1 out_loss 0.159229576587677, R2 0.06904518604278564\n",
      "Test Epoch350 layer2 out_loss 0.1583903282880783, R2 0.07395195960998535\n",
      "Test Epoch350 layer3 out_loss 0.158223956823349, R2 0.07492470741271973\n",
      "Test Epoch350 layer4 out_loss 0.15830281376838684, R2 0.07446366548538208\n",
      "Train 351 | out_loss 0.3964204788208008: 100%|█| 125/125 [00:00<00:00, 262.73it/\n",
      "Train Epoch351 out_loss 0.15714916586875916, R2 0.07817423343658447\n",
      "Test Epoch351 layer0 out_loss 0.16016222536563873, R2 0.06359231472015381\n",
      "Test Epoch351 layer1 out_loss 0.15936090052127838, R2 0.06827741861343384\n",
      "Test Epoch351 layer2 out_loss 0.15834133327007294, R2 0.07423841953277588\n",
      "Test Epoch351 layer3 out_loss 0.15844647586345673, R2 0.07362371683120728\n",
      "Test Epoch351 layer4 out_loss 0.1584540158510208, R2 0.07357966899871826\n",
      "Train 352 | out_loss 0.39645570516586304: 100%|█| 125/125 [00:00<00:00, 265.21it\n",
      "Train Epoch352 out_loss 0.1571771204471588, R2 0.07801026105880737\n",
      "Test Epoch352 layer0 out_loss 0.16022631525993347, R2 0.0632176399230957\n",
      "Test Epoch352 layer1 out_loss 0.15947015583515167, R2 0.06763863563537598\n",
      "Test Epoch352 layer2 out_loss 0.15866893529891968, R2 0.07232308387756348\n",
      "Test Epoch352 layer3 out_loss 0.15855593979358673, R2 0.0729837417602539\n",
      "Test Epoch352 layer4 out_loss 0.1585603803396225, R2 0.07295775413513184\n",
      "Train 353 | out_loss 0.39639192819595337: 100%|█| 125/125 [00:00<00:00, 265.81it\n",
      "Train Epoch353 out_loss 0.15712657570838928, R2 0.07830679416656494\n",
      "Test Epoch353 layer0 out_loss 0.16014063358306885, R2 0.06371855735778809\n",
      "Test Epoch353 layer1 out_loss 0.1592140942811966, R2 0.06913572549819946\n",
      "Test Epoch353 layer2 out_loss 0.15824571251869202, R2 0.07479751110076904\n",
      "Test Epoch353 layer3 out_loss 0.15820224583148956, R2 0.07505166530609131\n",
      "Test Epoch353 layer4 out_loss 0.1582462191581726, R2 0.07479453086853027\n",
      "Train 354 | out_loss 0.396501749753952: 100%|█| 125/125 [00:00<00:00, 261.95it/s\n",
      "Train Epoch354 out_loss 0.15721362829208374, R2 0.07779616117477417\n",
      "Test Epoch354 layer0 out_loss 0.16013553738594055, R2 0.06374835968017578\n",
      "Test Epoch354 layer1 out_loss 0.159181609749794, R2 0.06932568550109863\n",
      "Test Epoch354 layer2 out_loss 0.1583116352558136, R2 0.07441210746765137\n",
      "Test Epoch354 layer3 out_loss 0.15852519869804382, R2 0.07316350936889648\n",
      "Test Epoch354 layer4 out_loss 0.15835605561733246, R2 0.07415241003036499\n",
      "Train 355 | out_loss 0.39647412300109863: 100%|█| 125/125 [00:00<00:00, 262.71it\n",
      "Train Epoch355 out_loss 0.1571916937828064, R2 0.07792478799819946\n",
      "Test Epoch355 layer0 out_loss 0.16018950939178467, R2 0.06343281269073486\n",
      "Test Epoch355 layer1 out_loss 0.1591583639383316, R2 0.06946152448654175\n",
      "Test Epoch355 layer2 out_loss 0.1582520455121994, R2 0.07476049661636353\n",
      "Test Epoch355 layer3 out_loss 0.158248633146286, R2 0.07478046417236328\n",
      "Test Epoch355 layer4 out_loss 0.1583288460969925, R2 0.07431149482727051\n",
      "Train 356 | out_loss 0.39656099677085876: 100%|█| 125/125 [00:00<00:00, 253.68it\n",
      "Train Epoch356 out_loss 0.15726062655448914, R2 0.07752048969268799\n",
      "Test Epoch356 layer0 out_loss 0.16012679040431976, R2 0.06379956007003784\n",
      "Test Epoch356 layer1 out_loss 0.15915341675281525, R2 0.06949055194854736\n",
      "Test Epoch356 layer2 out_loss 0.15825873613357544, R2 0.0747213363647461\n",
      "Test Epoch356 layer3 out_loss 0.15820209681987762, R2 0.07505244016647339\n",
      "Test Epoch356 layer4 out_loss 0.158238485455513, R2 0.0748397707939148\n",
      "Train 357 | out_loss 0.3963543474674225: 100%|█| 125/125 [00:00<00:00, 268.37it/\n",
      "Train Epoch357 out_loss 0.1570967584848404, R2 0.07848167419433594\n",
      "Test Epoch357 layer0 out_loss 0.16013482213020325, R2 0.06375259160995483\n",
      "Test Epoch357 layer1 out_loss 0.15920236706733704, R2 0.06920433044433594\n",
      "Test Epoch357 layer2 out_loss 0.15844640135765076, R2 0.07362419366836548\n",
      "Test Epoch357 layer3 out_loss 0.1584182232618332, R2 0.07378888130187988\n",
      "Test Epoch357 layer4 out_loss 0.15846559405326843, R2 0.07351195812225342\n",
      "Train 358 | out_loss 0.39646604657173157: 100%|█| 125/125 [00:00<00:00, 264.67it\n",
      "Train Epoch358 out_loss 0.157185360789299, R2 0.07796192169189453\n",
      "Test Epoch358 layer0 out_loss 0.16011928021907806, R2 0.0638434886932373\n",
      "Test Epoch358 layer1 out_loss 0.15916003286838531, R2 0.06945180892944336\n",
      "Test Epoch358 layer2 out_loss 0.15823443233966827, R2 0.07486343383789062\n",
      "Test Epoch358 layer3 out_loss 0.15820077061653137, R2 0.07506024837493896\n",
      "Test Epoch358 layer4 out_loss 0.15825848281383514, R2 0.07472282648086548\n",
      "Train 359 | out_loss 0.3964769244194031: 100%|█| 125/125 [00:00<00:00, 267.01it/\n",
      "Train Epoch359 out_loss 0.15719389915466309, R2 0.0779118537902832\n",
      "Test Epoch359 layer0 out_loss 0.16011980175971985, R2 0.06384044885635376\n",
      "Test Epoch359 layer1 out_loss 0.15924124419689178, R2 0.0689769983291626\n",
      "Test Epoch359 layer2 out_loss 0.15825645625591278, R2 0.07473468780517578\n",
      "Test Epoch359 layer3 out_loss 0.15826354920864105, R2 0.07469320297241211\n",
      "Test Epoch359 layer4 out_loss 0.1583200842142105, R2 0.07436269521713257\n",
      "Train 360 | out_loss 0.39619866013526917: 100%|█| 125/125 [00:00<00:00, 265.78it\n",
      "Train Epoch360 out_loss 0.15697337687015533, R2 0.0792054533958435\n",
      "Test Epoch360 layer0 out_loss 0.16013279557228088, R2 0.06376445293426514\n",
      "Test Epoch360 layer1 out_loss 0.15925456583499908, R2 0.06889915466308594\n",
      "Test Epoch360 layer2 out_loss 0.15834657847881317, R2 0.07420778274536133\n",
      "Test Epoch360 layer3 out_loss 0.15850070118904114, R2 0.07330667972564697\n",
      "Test Epoch360 layer4 out_loss 0.15849816799163818, R2 0.07332146167755127\n",
      "Train 361 | out_loss 0.3964425325393677: 100%|█| 125/125 [00:00<00:00, 265.05it/\n",
      "Train Epoch361 out_loss 0.15716665983200073, R2 0.07807165384292603\n",
      "Test Epoch361 layer0 out_loss 0.1601087898015976, R2 0.0639047622680664\n",
      "Test Epoch361 layer1 out_loss 0.15933990478515625, R2 0.06840014457702637\n",
      "Test Epoch361 layer2 out_loss 0.158321350812912, R2 0.07435524463653564\n",
      "Test Epoch361 layer3 out_loss 0.15828761458396912, R2 0.07455253601074219\n",
      "Test Epoch361 layer4 out_loss 0.15842410922050476, R2 0.07375448942184448\n",
      "Train 362 | out_loss 0.39636287093162537: 100%|█| 125/125 [00:00<00:00, 266.10it\n",
      "Train Epoch362 out_loss 0.1571034938097, R2 0.07844209671020508\n",
      "Test Epoch362 layer0 out_loss 0.16015610098838806, R2 0.0636281967163086\n",
      "Test Epoch362 layer1 out_loss 0.15917889773845673, R2 0.06934154033660889\n",
      "Test Epoch362 layer2 out_loss 0.1582193225622177, R2 0.07495176792144775\n",
      "Test Epoch362 layer3 out_loss 0.15837864577770233, R2 0.07402026653289795\n",
      "Test Epoch362 layer4 out_loss 0.15841436386108398, R2 0.07381141185760498\n",
      "Train 363 | out_loss 0.3965120315551758: 100%|█| 125/125 [00:00<00:00, 266.69it/\n",
      "Train Epoch363 out_loss 0.15722177922725677, R2 0.07774823904037476\n",
      "Test Epoch363 layer0 out_loss 0.16012878715991974, R2 0.06378787755966187\n",
      "Test Epoch363 layer1 out_loss 0.15925133228302002, R2 0.06891804933547974\n",
      "Test Epoch363 layer2 out_loss 0.15821029245853424, R2 0.07500457763671875\n",
      "Test Epoch363 layer3 out_loss 0.1583004891872406, R2 0.0744771957397461\n",
      "Test Epoch363 layer4 out_loss 0.15844012796878815, R2 0.07366085052490234\n",
      "Train 364 | out_loss 0.39645636081695557: 100%|█| 125/125 [00:00<00:00, 267.43it\n",
      "Train Epoch364 out_loss 0.15717759728431702, R2 0.07800745964050293\n",
      "Test Epoch364 layer0 out_loss 0.16029147803783417, R2 0.06283670663833618\n",
      "Test Epoch364 layer1 out_loss 0.15923355519771576, R2 0.06902199983596802\n",
      "Test Epoch364 layer2 out_loss 0.1584373563528061, R2 0.07367700338363647\n",
      "Test Epoch364 layer3 out_loss 0.1586986631155014, R2 0.07214921712875366\n",
      "Test Epoch364 layer4 out_loss 0.15857623517513275, R2 0.0728650689125061\n",
      "Train 365 | out_loss 0.39640453457832336: 100%|█| 125/125 [00:00<00:00, 250.10it\n",
      "Train Epoch365 out_loss 0.15713651478290558, R2 0.07824844121932983\n",
      "Test Epoch365 layer0 out_loss 0.16009987890720367, R2 0.0639568567276001\n",
      "Test Epoch365 layer1 out_loss 0.15912197530269623, R2 0.06967431306838989\n",
      "Test Epoch365 layer2 out_loss 0.15818020701408386, R2 0.07518047094345093\n",
      "Test Epoch365 layer3 out_loss 0.15819351375102997, R2 0.07510268688201904\n",
      "Test Epoch365 layer4 out_loss 0.15822608768939972, R2 0.07491225004196167\n",
      "Train 366 | out_loss 0.3964552879333496: 100%|█| 125/125 [00:00<00:00, 263.01it/\n",
      "Train Epoch366 out_loss 0.15717680752277374, R2 0.07801210880279541\n",
      "Test Epoch366 layer0 out_loss 0.16009511053562164, R2 0.06398481130599976\n",
      "Test Epoch366 layer1 out_loss 0.1591101735830307, R2 0.0697433352470398\n",
      "Test Epoch366 layer2 out_loss 0.15815794467926025, R2 0.07531064748764038\n",
      "Test Epoch366 layer3 out_loss 0.15821965038776398, R2 0.07494986057281494\n",
      "Test Epoch366 layer4 out_loss 0.15827244520187378, R2 0.0746411681175232\n",
      "Train 367 | out_loss 0.39689207077026367: 100%|█| 125/125 [00:00<00:00, 268.07it\n",
      "Train Epoch367 out_loss 0.1575232893228531, R2 0.07597970962524414\n",
      "Test Epoch367 layer0 out_loss 0.1601342260837555, R2 0.0637560486793518\n",
      "Test Epoch367 layer1 out_loss 0.15944360196590424, R2 0.06779390573501587\n",
      "Test Epoch367 layer2 out_loss 0.15827728807926178, R2 0.07461291551589966\n",
      "Test Epoch367 layer3 out_loss 0.15831592679023743, R2 0.07438695430755615\n",
      "Test Epoch367 layer4 out_loss 0.15848548710346222, R2 0.07339560985565186\n",
      "Train 368 | out_loss 0.3963146507740021: 100%|█| 125/125 [00:00<00:00, 257.67it/\n",
      "Train Epoch368 out_loss 0.1570652723312378, R2 0.0786663293838501\n",
      "Test Epoch368 layer0 out_loss 0.1601654589176178, R2 0.06357342004776001\n",
      "Test Epoch368 layer1 out_loss 0.15965335071086884, R2 0.06656754016876221\n",
      "Test Epoch368 layer2 out_loss 0.15865278244018555, R2 0.0724174976348877\n",
      "Test Epoch368 layer3 out_loss 0.15898431837558746, R2 0.07047915458679199\n",
      "Test Epoch368 layer4 out_loss 0.15898385643959045, R2 0.07048183679580688\n",
      "Train 369 | out_loss 0.39644527435302734: 100%|█| 125/125 [00:00<00:00, 259.06it\n",
      "Train Epoch369 out_loss 0.15716888010501862, R2 0.07805860042572021\n",
      "Test Epoch369 layer0 out_loss 0.16016323864459991, R2 0.06358647346496582\n",
      "Test Epoch369 layer1 out_loss 0.15921248495578766, R2 0.06914520263671875\n",
      "Test Epoch369 layer2 out_loss 0.15819405019283295, R2 0.07509958744049072\n",
      "Test Epoch369 layer3 out_loss 0.15826453268527985, R2 0.07468748092651367\n",
      "Test Epoch369 layer4 out_loss 0.15837907791137695, R2 0.07401776313781738\n",
      "Train 370 | out_loss 0.39621424674987793: 100%|█| 125/125 [00:00<00:00, 261.12it\n",
      "Train Epoch370 out_loss 0.15698571503162384, R2 0.0791330337524414\n",
      "Test Epoch370 layer0 out_loss 0.16009801626205444, R2 0.063967764377594\n",
      "Test Epoch370 layer1 out_loss 0.15911349654197693, R2 0.06972390413284302\n",
      "Test Epoch370 layer2 out_loss 0.15814299881458282, R2 0.07539796829223633\n",
      "Test Epoch370 layer3 out_loss 0.1581299751996994, R2 0.07547414302825928\n",
      "Test Epoch370 layer4 out_loss 0.15820126235485077, R2 0.07505738735198975\n",
      "Train 371 | out_loss 0.39636263251304626: 100%|█| 125/125 [00:00<00:00, 262.88it\n",
      "Train Epoch371 out_loss 0.15710334479808807, R2 0.07844299077987671\n",
      "Test Epoch371 layer0 out_loss 0.16020214557647705, R2 0.06335896253585815\n",
      "Test Epoch371 layer1 out_loss 0.15907731652259827, R2 0.06993544101715088\n",
      "Test Epoch371 layer2 out_loss 0.15818433463573456, R2 0.0751563310623169\n",
      "Test Epoch371 layer3 out_loss 0.1582767814397812, R2 0.07461583614349365\n",
      "Test Epoch371 layer4 out_loss 0.1582896113395691, R2 0.07454085350036621\n",
      "Train 372 | out_loss 0.39643943309783936: 100%|█| 125/125 [00:00<00:00, 258.46it\n",
      "Train Epoch372 out_loss 0.15716421604156494, R2 0.07808595895767212\n",
      "Test Epoch372 layer0 out_loss 0.1602105349302292, R2 0.06330990791320801\n",
      "Test Epoch372 layer1 out_loss 0.15957452356815338, R2 0.0670284628868103\n",
      "Test Epoch372 layer2 out_loss 0.15838168561458588, R2 0.07400250434875488\n",
      "Test Epoch372 layer3 out_loss 0.15852364897727966, R2 0.07317250967025757\n",
      "Test Epoch372 layer4 out_loss 0.15881715714931488, R2 0.07145649194717407\n",
      "Train 373 | out_loss 0.3963322043418884: 100%|█| 125/125 [00:00<00:00, 234.51it/\n",
      "Train Epoch373 out_loss 0.15707919001579285, R2 0.07858467102050781\n",
      "Test Epoch373 layer0 out_loss 0.16007299721240997, R2 0.0641140341758728\n",
      "Test Epoch373 layer1 out_loss 0.15905891358852386, R2 0.07004302740097046\n",
      "Test Epoch373 layer2 out_loss 0.1581220179796219, R2 0.07552063465118408\n",
      "Test Epoch373 layer3 out_loss 0.15811976790428162, R2 0.07553386688232422\n",
      "Test Epoch373 layer4 out_loss 0.15815018117427826, R2 0.07535606622695923\n",
      "Train 374 | out_loss 0.3965901732444763: 100%|█| 125/125 [00:00<00:00, 256.68it/\n",
      "Train Epoch374 out_loss 0.15728379786014557, R2 0.07738447189331055\n",
      "Test Epoch374 layer0 out_loss 0.16007745265960693, R2 0.06408798694610596\n",
      "Test Epoch374 layer1 out_loss 0.15967096388339996, R2 0.06646460294723511\n",
      "Test Epoch374 layer2 out_loss 0.1583254188299179, R2 0.07433146238327026\n",
      "Test Epoch374 layer3 out_loss 0.15869829058647156, R2 0.07215148210525513\n",
      "Test Epoch374 layer4 out_loss 0.158753901720047, R2 0.0718262791633606\n",
      "Train 375 | out_loss 0.3963159918785095: 100%|█| 125/125 [00:00<00:00, 261.91it/\n",
      "Train Epoch375 out_loss 0.15706634521484375, R2 0.07866007089614868\n",
      "Test Epoch375 layer0 out_loss 0.16007158160209656, R2 0.06412231922149658\n",
      "Test Epoch375 layer1 out_loss 0.15929564833641052, R2 0.06865888833999634\n",
      "Test Epoch375 layer2 out_loss 0.15822139382362366, R2 0.07493972778320312\n",
      "Test Epoch375 layer3 out_loss 0.15829671919345856, R2 0.07449924945831299\n",
      "Test Epoch375 layer4 out_loss 0.15826238691806793, R2 0.0746999979019165\n",
      "Train 376 | out_loss 0.39634865522384644: 100%|█| 125/125 [00:00<00:00, 262.94it\n",
      "Train Epoch376 out_loss 0.15709228813648224, R2 0.07850790023803711\n",
      "Test Epoch376 layer0 out_loss 0.1601065844297409, R2 0.06391769647598267\n",
      "Test Epoch376 layer1 out_loss 0.1590462625026703, R2 0.07011699676513672\n",
      "Test Epoch376 layer2 out_loss 0.15810568630695343, R2 0.07561618089675903\n",
      "Test Epoch376 layer3 out_loss 0.15824340283870697, R2 0.07481098175048828\n",
      "Test Epoch376 layer4 out_loss 0.1582706719636917, R2 0.07465153932571411\n",
      "Train 377 | out_loss 0.3962484300136566: 100%|█| 125/125 [00:00<00:00, 261.51it/\n",
      "Train Epoch377 out_loss 0.15701280534267426, R2 0.07897412776947021\n",
      "Test Epoch377 layer0 out_loss 0.1600879579782486, R2 0.0640265941619873\n",
      "Test Epoch377 layer1 out_loss 0.15913164615631104, R2 0.06961774826049805\n",
      "Test Epoch377 layer2 out_loss 0.15836885571479797, R2 0.0740775465965271\n",
      "Test Epoch377 layer3 out_loss 0.15838627517223358, R2 0.07397568225860596\n",
      "Test Epoch377 layer4 out_loss 0.1583523452281952, R2 0.07417410612106323\n",
      "Train 378 | out_loss 0.3961317241191864: 100%|█| 125/125 [00:00<00:00, 264.40it/\n",
      "Train Epoch378 out_loss 0.15692031383514404, R2 0.07951664924621582\n",
      "Test Epoch378 layer0 out_loss 0.16018101572990417, R2 0.06348252296447754\n",
      "Test Epoch378 layer1 out_loss 0.1597953587770462, R2 0.06573724746704102\n",
      "Test Epoch378 layer2 out_loss 0.15857097506523132, R2 0.0728958249092102\n",
      "Test Epoch378 layer3 out_loss 0.1585690975189209, R2 0.07290679216384888\n",
      "Test Epoch378 layer4 out_loss 0.1587861180305481, R2 0.07163792848587036\n",
      "Train 379 | out_loss 0.39615678787231445: 100%|█| 125/125 [00:00<00:00, 243.53it\n",
      "Train Epoch379 out_loss 0.15694019198417664, R2 0.07940012216567993\n",
      "Test Epoch379 layer0 out_loss 0.16005122661590576, R2 0.06424134969711304\n",
      "Test Epoch379 layer1 out_loss 0.1590760052204132, R2 0.06994307041168213\n",
      "Test Epoch379 layer2 out_loss 0.15810835361480713, R2 0.07560056447982788\n",
      "Test Epoch379 layer3 out_loss 0.15819083154201508, R2 0.07511842250823975\n",
      "Test Epoch379 layer4 out_loss 0.1583617925643921, R2 0.07411885261535645\n",
      "Train 380 | out_loss 0.3961125314235687: 100%|█| 125/125 [00:00<00:00, 263.60it/\n",
      "Train Epoch380 out_loss 0.1569051295518875, R2 0.07960569858551025\n",
      "Test Epoch380 layer0 out_loss 0.16004791855812073, R2 0.06426072120666504\n",
      "Test Epoch380 layer1 out_loss 0.15922635793685913, R2 0.06906402111053467\n",
      "Test Epoch380 layer2 out_loss 0.15838460624217987, R2 0.07398539781570435\n",
      "Test Epoch380 layer3 out_loss 0.15840697288513184, R2 0.07385468482971191\n",
      "Test Epoch380 layer4 out_loss 0.15838128328323364, R2 0.07400482892990112\n",
      "Train 381 | out_loss 0.3963819444179535: 100%|█| 125/125 [00:00<00:00, 264.66it/\n",
      "Train Epoch381 out_loss 0.15711860358715057, R2 0.07835352420806885\n",
      "Test Epoch381 layer0 out_loss 0.16005316376686096, R2 0.06423002481460571\n",
      "Test Epoch381 layer1 out_loss 0.1590432971715927, R2 0.07013428211212158\n",
      "Test Epoch381 layer2 out_loss 0.15810269117355347, R2 0.075633704662323\n",
      "Test Epoch381 layer3 out_loss 0.15813565254211426, R2 0.07544100284576416\n",
      "Test Epoch381 layer4 out_loss 0.15817612409591675, R2 0.07520437240600586\n",
      "Train 382 | out_loss 0.3959850072860718: 100%|█| 125/125 [00:00<00:00, 257.70it/\n",
      "Train Epoch382 out_loss 0.15680411458015442, R2 0.08019834756851196\n",
      "Test Epoch382 layer0 out_loss 0.16027893126010895, R2 0.06291002035140991\n",
      "Test Epoch382 layer1 out_loss 0.1590716540813446, R2 0.06996852159500122\n",
      "Test Epoch382 layer2 out_loss 0.1581467241048813, R2 0.07537621259689331\n",
      "Test Epoch382 layer3 out_loss 0.1581483781337738, R2 0.0753665566444397\n",
      "Test Epoch382 layer4 out_loss 0.15824823081493378, R2 0.07478278875350952\n",
      "Train 383 | out_loss 0.3960777521133423: 100%|█| 125/125 [00:00<00:00, 261.37it/\n",
      "Train Epoch383 out_loss 0.15687762200832367, R2 0.07976710796356201\n",
      "Test Epoch383 layer0 out_loss 0.16005317866802216, R2 0.06422996520996094\n",
      "Test Epoch383 layer1 out_loss 0.1590336412191391, R2 0.07019072771072388\n",
      "Test Epoch383 layer2 out_loss 0.1580839306116104, R2 0.07574343681335449\n",
      "Test Epoch383 layer3 out_loss 0.15819448232650757, R2 0.0750969648361206\n",
      "Test Epoch383 layer4 out_loss 0.15818992257118225, R2 0.07512366771697998\n",
      "Train 384 | out_loss 0.39632195234298706: 100%|█| 125/125 [00:00<00:00, 264.56it\n",
      "Train Epoch384 out_loss 0.1570710688829422, R2 0.07863229513168335\n",
      "Test Epoch384 layer0 out_loss 0.16005045175552368, R2 0.06424587965011597\n",
      "Test Epoch384 layer1 out_loss 0.15909089148044586, R2 0.06985604763031006\n",
      "Test Epoch384 layer2 out_loss 0.15809082984924316, R2 0.07570302486419678\n",
      "Test Epoch384 layer3 out_loss 0.1582624465227127, R2 0.07469964027404785\n",
      "Test Epoch384 layer4 out_loss 0.15829502046108246, R2 0.07450926303863525\n",
      "Train 385 | out_loss 0.3961969017982483: 100%|█| 125/125 [00:00<00:00, 259.12it/\n",
      "Train Epoch385 out_loss 0.1569719761610031, R2 0.07921367883682251\n",
      "Test Epoch385 layer0 out_loss 0.16019201278686523, R2 0.06341820955276489\n",
      "Test Epoch385 layer1 out_loss 0.1590447872877121, R2 0.07012563943862915\n",
      "Test Epoch385 layer2 out_loss 0.1581912785768509, R2 0.07511574029922485\n",
      "Test Epoch385 layer3 out_loss 0.15821252763271332, R2 0.07499152421951294\n",
      "Test Epoch385 layer4 out_loss 0.15826912224292755, R2 0.07466065883636475\n",
      "Train 386 | out_loss 0.39620837569236755: 100%|█| 125/125 [00:00<00:00, 268.18it\n",
      "Train Epoch386 out_loss 0.15698108077049255, R2 0.07916021347045898\n",
      "Test Epoch386 layer0 out_loss 0.16004274785518646, R2 0.06429088115692139\n",
      "Test Epoch386 layer1 out_loss 0.15908931195735931, R2 0.06986528635025024\n",
      "Test Epoch386 layer2 out_loss 0.15807440876960754, R2 0.07579910755157471\n",
      "Test Epoch386 layer3 out_loss 0.15811753273010254, R2 0.07554686069488525\n",
      "Test Epoch386 layer4 out_loss 0.15825481712818146, R2 0.07474422454833984\n",
      "Train 387 | out_loss 0.3961022198200226: 100%|█| 125/125 [00:00<00:00, 265.63it/\n",
      "Train Epoch387 out_loss 0.15689697861671448, R2 0.07965356111526489\n",
      "Test Epoch387 layer0 out_loss 0.1600361466407776, R2 0.06432950496673584\n",
      "Test Epoch387 layer1 out_loss 0.15904197096824646, R2 0.07014203071594238\n",
      "Test Epoch387 layer2 out_loss 0.15805687010288239, R2 0.0759015679359436\n",
      "Test Epoch387 layer3 out_loss 0.1581457257270813, R2 0.07538211345672607\n",
      "Test Epoch387 layer4 out_loss 0.15819746255874634, R2 0.07507956027984619\n",
      "Train 388 | out_loss 0.3961431682109833: 100%|█| 125/125 [00:00<00:00, 257.09it/\n",
      "Train Epoch388 out_loss 0.15692941844463348, R2 0.07946330308914185\n",
      "Test Epoch388 layer0 out_loss 0.16002525389194489, R2 0.06439322233200073\n",
      "Test Epoch388 layer1 out_loss 0.15901830792427063, R2 0.07028043270111084\n",
      "Test Epoch388 layer2 out_loss 0.15810711681842804, R2 0.07560783624649048\n",
      "Test Epoch388 layer3 out_loss 0.158173069357872, R2 0.07522225379943848\n",
      "Test Epoch388 layer4 out_loss 0.15817515552043915, R2 0.07521003484725952\n",
      "Train 389 | out_loss 0.39629629254341125: 100%|█| 125/125 [00:00<00:00, 258.06it\n",
      "Train Epoch389 out_loss 0.1570507436990738, R2 0.0787515640258789\n",
      "Test Epoch389 layer0 out_loss 0.1600264012813568, R2 0.06438648700714111\n",
      "Test Epoch389 layer1 out_loss 0.15899857878684998, R2 0.07039576768875122\n",
      "Test Epoch389 layer2 out_loss 0.15805548429489136, R2 0.07590973377227783\n",
      "Test Epoch389 layer3 out_loss 0.15805913507938385, R2 0.07588839530944824\n",
      "Test Epoch389 layer4 out_loss 0.15811364352703094, R2 0.07556962966918945\n",
      "Train 390 | out_loss 0.3960801362991333: 100%|█| 125/125 [00:00<00:00, 268.84it/\n",
      "Train Epoch390 out_loss 0.1568794548511505, R2 0.07975631952285767\n",
      "Test Epoch390 layer0 out_loss 0.16001740097999573, R2 0.06443911790847778\n",
      "Test Epoch390 layer1 out_loss 0.1590375006198883, R2 0.07016819715499878\n",
      "Test Epoch390 layer2 out_loss 0.1580650508403778, R2 0.07585376501083374\n",
      "Test Epoch390 layer3 out_loss 0.1580638885498047, R2 0.07586055994033813\n",
      "Test Epoch390 layer4 out_loss 0.15814441442489624, R2 0.0753898024559021\n",
      "Train 391 | out_loss 0.39606672525405884: 100%|█| 125/125 [00:00<00:00, 250.07it\n",
      "Train Epoch391 out_loss 0.1568688601255417, R2 0.0798184871673584\n",
      "Test Epoch391 layer0 out_loss 0.1600169688463211, R2 0.06444162130355835\n",
      "Test Epoch391 layer1 out_loss 0.15900985896587372, R2 0.07032984495162964\n",
      "Test Epoch391 layer2 out_loss 0.15808461606502533, R2 0.07573938369750977\n",
      "Test Epoch391 layer3 out_loss 0.1580991894006729, R2 0.07565420866012573\n",
      "Test Epoch391 layer4 out_loss 0.15817217528820038, R2 0.07522743940353394\n",
      "Train 392 | out_loss 0.39615684747695923: 100%|█| 125/125 [00:00<00:00, 246.56it\n",
      "Train Epoch392 out_loss 0.15694023668766022, R2 0.07939982414245605\n",
      "Test Epoch392 layer0 out_loss 0.1600135713815689, R2 0.06446152925491333\n",
      "Test Epoch392 layer1 out_loss 0.15936289727687836, R2 0.06826573610305786\n",
      "Test Epoch392 layer2 out_loss 0.15818530321121216, R2 0.07515066862106323\n",
      "Test Epoch392 layer3 out_loss 0.15828998386859894, R2 0.0745387077331543\n",
      "Test Epoch392 layer4 out_loss 0.1584017276763916, R2 0.07388532161712646\n",
      "Train 393 | out_loss 0.396261602640152: 100%|█| 125/125 [00:00<00:00, 261.70it/s\n",
      "Train Epoch393 out_loss 0.15702328085899353, R2 0.07891267538070679\n",
      "Test Epoch393 layer0 out_loss 0.16000044345855713, R2 0.06453824043273926\n",
      "Test Epoch393 layer1 out_loss 0.15897247195243835, R2 0.070548415184021\n",
      "Test Epoch393 layer2 out_loss 0.158036008477211, R2 0.07602351903915405\n",
      "Test Epoch393 layer3 out_loss 0.15803079307079315, R2 0.07605403661727905\n",
      "Test Epoch393 layer4 out_loss 0.15809136629104614, R2 0.07569986581802368\n",
      "Train 394 | out_loss 0.3958902955055237: 100%|█| 125/125 [00:00<00:00, 263.88it/\n",
      "Train Epoch394 out_loss 0.15672914683818817, R2 0.08063805103302002\n",
      "Test Epoch394 layer0 out_loss 0.16015928983688354, R2 0.0636095404624939\n",
      "Test Epoch394 layer1 out_loss 0.1596243530511856, R2 0.06673705577850342\n",
      "Test Epoch394 layer2 out_loss 0.15852226316928864, R2 0.07318061590194702\n",
      "Test Epoch394 layer3 out_loss 0.15845364332199097, R2 0.07358181476593018\n",
      "Test Epoch394 layer4 out_loss 0.15868937969207764, R2 0.07220357656478882\n",
      "Train 395 | out_loss 0.3961375653743744: 100%|█| 125/125 [00:00<00:00, 255.77it/\n",
      "Train Epoch395 out_loss 0.1569249927997589, R2 0.07948923110961914\n",
      "Test Epoch395 layer0 out_loss 0.1600884646177292, R2 0.06402361392974854\n",
      "Test Epoch395 layer1 out_loss 0.1591973453760147, R2 0.06923365592956543\n",
      "Test Epoch395 layer2 out_loss 0.158318892121315, R2 0.07436960935592651\n",
      "Test Epoch395 layer3 out_loss 0.15825310349464417, R2 0.07475429773330688\n",
      "Test Epoch395 layer4 out_loss 0.15834848582744598, R2 0.07419663667678833\n",
      "Train 396 | out_loss 0.39597055315971375: 100%|█| 125/125 [00:00<00:00, 253.77it\n",
      "Train Epoch396 out_loss 0.15679265558719635, R2 0.08026552200317383\n",
      "Test Epoch396 layer0 out_loss 0.16001641750335693, R2 0.06444483995437622\n",
      "Test Epoch396 layer1 out_loss 0.1590290367603302, R2 0.07021766901016235\n",
      "Test Epoch396 layer2 out_loss 0.15812915563583374, R2 0.07547897100448608\n",
      "Test Epoch396 layer3 out_loss 0.15813139081001282, R2 0.0754658579826355\n",
      "Test Epoch396 layer4 out_loss 0.15827293694019318, R2 0.07463836669921875\n",
      "Train 397 | out_loss 0.39598557353019714: 100%|█| 125/125 [00:00<00:00, 263.50it\n",
      "Train Epoch397 out_loss 0.15680457651615143, R2 0.08019554615020752\n",
      "Test Epoch397 layer0 out_loss 0.16004051268100739, R2 0.06430399417877197\n",
      "Test Epoch397 layer1 out_loss 0.15899352729320526, R2 0.07042527198791504\n",
      "Test Epoch397 layer2 out_loss 0.1581050157546997, R2 0.07562011480331421\n",
      "Test Epoch397 layer3 out_loss 0.15826310217380524, R2 0.07469576597213745\n",
      "Test Epoch397 layer4 out_loss 0.15828518569469452, R2 0.07456672191619873\n",
      "Train 398 | out_loss 0.39605626463890076: 100%|█| 125/125 [00:00<00:00, 264.03it\n",
      "Train Epoch398 out_loss 0.15686054527759552, R2 0.07986724376678467\n",
      "Test Epoch398 layer0 out_loss 0.15999378263950348, R2 0.06457716226577759\n",
      "Test Epoch398 layer1 out_loss 0.15901830792427063, R2 0.07028043270111084\n",
      "Test Epoch398 layer2 out_loss 0.1581118106842041, R2 0.07558035850524902\n",
      "Test Epoch398 layer3 out_loss 0.15855468809604645, R2 0.0729910135269165\n",
      "Test Epoch398 layer4 out_loss 0.15842635929584503, R2 0.07374131679534912\n",
      "Train 399 | out_loss 0.3960212469100952: 100%|█| 125/125 [00:00<00:00, 264.27it/\n",
      "Train Epoch399 out_loss 0.15683282911777496, R2 0.08002990484237671\n",
      "Test Epoch399 layer0 out_loss 0.16004496812820435, R2 0.06427794694900513\n",
      "Test Epoch399 layer1 out_loss 0.15897512435913086, R2 0.07053285837173462\n",
      "Test Epoch399 layer2 out_loss 0.1580628901720047, R2 0.07586634159088135\n",
      "Test Epoch399 layer3 out_loss 0.15800991654396057, R2 0.07617610692977905\n",
      "Test Epoch399 layer4 out_loss 0.1580989956855774, R2 0.07565528154373169\n",
      "Train 400 | out_loss 0.3964282274246216: 100%|█| 125/125 [00:00<00:00, 256.31it/\n",
      "Train Epoch400 out_loss 0.15715539455413818, R2 0.07813769578933716\n",
      "Test Epoch400 layer0 out_loss 0.1601429432630539, R2 0.06370508670806885\n",
      "Test Epoch400 layer1 out_loss 0.1589820832014084, R2 0.0704922080039978\n",
      "Test Epoch400 layer2 out_loss 0.15834017097949982, R2 0.07424527406692505\n",
      "Test Epoch400 layer3 out_loss 0.15805266797542572, R2 0.07592612504959106\n",
      "Test Epoch400 layer4 out_loss 0.15807606279850006, R2 0.07578933238983154\n",
      "Train 401 | out_loss 0.396271288394928: 100%|█| 125/125 [00:00<00:00, 268.16it/s\n",
      "Train Epoch401 out_loss 0.15703094005584717, R2 0.07886773347854614\n",
      "Test Epoch401 layer0 out_loss 0.160040944814682, R2 0.0643014907836914\n",
      "Test Epoch401 layer1 out_loss 0.15925811231136322, R2 0.06887835264205933\n",
      "Test Epoch401 layer2 out_loss 0.15815982222557068, R2 0.07529968023300171\n",
      "Test Epoch401 layer3 out_loss 0.15828892588615417, R2 0.07454484701156616\n",
      "Test Epoch401 layer4 out_loss 0.15845541656017303, R2 0.07357144355773926\n",
      "Train 402 | out_loss 0.39592984318733215: 100%|█| 125/125 [00:00<00:00, 266.77it\n",
      "Train Epoch402 out_loss 0.15676043927669525, R2 0.08045446872711182\n",
      "Test Epoch402 layer0 out_loss 0.16009832918643951, R2 0.06396597623825073\n",
      "Test Epoch402 layer1 out_loss 0.15944096446037292, R2 0.0678093433380127\n",
      "Test Epoch402 layer2 out_loss 0.15833435952663422, R2 0.07427918910980225\n",
      "Test Epoch402 layer3 out_loss 0.1583796590566635, R2 0.07401436567306519\n",
      "Test Epoch402 layer4 out_loss 0.15846657752990723, R2 0.0735061764717102\n",
      "Train 403 | out_loss 0.3961428999900818: 100%|█| 125/125 [00:00<00:00, 256.34it/\n",
      "Train Epoch403 out_loss 0.15692922472953796, R2 0.0794643759727478\n",
      "Test Epoch403 layer0 out_loss 0.15999428927898407, R2 0.06457418203353882\n",
      "Test Epoch403 layer1 out_loss 0.15920031070709229, R2 0.06921631097793579\n",
      "Test Epoch403 layer2 out_loss 0.1581948846578598, R2 0.07509464025497437\n",
      "Test Epoch403 layer3 out_loss 0.15826041996479034, R2 0.07471150159835815\n",
      "Test Epoch403 layer4 out_loss 0.15824688971042633, R2 0.07479065656661987\n",
      "Train 404 | out_loss 0.3960796892642975: 100%|█| 125/125 [00:00<00:00, 263.61it/\n",
      "Train Epoch404 out_loss 0.15687909722328186, R2 0.0797584056854248\n",
      "Test Epoch404 layer0 out_loss 0.15996716916561127, R2 0.06473284959793091\n",
      "Test Epoch404 layer1 out_loss 0.1592118740081787, R2 0.06914877891540527\n",
      "Test Epoch404 layer2 out_loss 0.15814413130283356, R2 0.07539141178131104\n",
      "Test Epoch404 layer3 out_loss 0.15816739201545715, R2 0.07525545358657837\n",
      "Test Epoch404 layer4 out_loss 0.15828216075897217, R2 0.07458436489105225\n",
      "Train 405 | out_loss 0.3960035741329193: 100%|█| 125/125 [00:00<00:00, 246.76it/\n",
      "Train Epoch405 out_loss 0.15681886672973633, R2 0.0801118016242981\n",
      "Test Epoch405 layer0 out_loss 0.15996883809566498, R2 0.06472301483154297\n",
      "Test Epoch405 layer1 out_loss 0.1590123325586319, R2 0.07031536102294922\n",
      "Test Epoch405 layer2 out_loss 0.15811866521835327, R2 0.07554030418395996\n",
      "Test Epoch405 layer3 out_loss 0.15820850431919098, R2 0.07501500844955444\n",
      "Test Epoch405 layer4 out_loss 0.15824636816978455, R2 0.07479363679885864\n",
      "Train 406 | out_loss 0.39598217606544495: 100%|█| 125/125 [00:00<00:00, 250.38it\n",
      "Train Epoch406 out_loss 0.15680181980133057, R2 0.08021175861358643\n",
      "Test Epoch406 layer0 out_loss 0.1599607616662979, R2 0.06477028131484985\n",
      "Test Epoch406 layer1 out_loss 0.15891218185424805, R2 0.07090085744857788\n",
      "Test Epoch406 layer2 out_loss 0.15796251595020294, R2 0.07645326852798462\n",
      "Test Epoch406 layer3 out_loss 0.15796665847301483, R2 0.07642900943756104\n",
      "Test Epoch406 layer4 out_loss 0.1580449938774109, R2 0.07597100734710693\n",
      "Train 407 | out_loss 0.39594578742980957: 100%|█| 125/125 [00:00<00:00, 265.07it\n",
      "Train Epoch407 out_loss 0.15677306056022644, R2 0.08038049936294556\n",
      "Test Epoch407 layer0 out_loss 0.16034571826457977, R2 0.06251955032348633\n",
      "Test Epoch407 layer1 out_loss 0.16053445637226105, R2 0.06141608953475952\n",
      "Test Epoch407 layer2 out_loss 0.1591159701347351, R2 0.0697094202041626\n",
      "Test Epoch407 layer3 out_loss 0.15960340201854706, R2 0.06685954332351685\n",
      "Test Epoch407 layer4 out_loss 0.15978676080703735, R2 0.06578761339187622\n",
      "Train 408 | out_loss 0.3965139091014862: 100%|█| 125/125 [00:00<00:00, 252.23it/\n",
      "Train Epoch408 out_loss 0.15722329914569855, R2 0.077739417552948\n",
      "Test Epoch408 layer0 out_loss 0.1599739044904709, R2 0.0646933913230896\n",
      "Test Epoch408 layer1 out_loss 0.15889637172222137, R2 0.07099330425262451\n",
      "Test Epoch408 layer2 out_loss 0.1579904705286026, R2 0.0762898325920105\n",
      "Test Epoch408 layer3 out_loss 0.1579347848892212, R2 0.07661539316177368\n",
      "Test Epoch408 layer4 out_loss 0.15798847377300262, R2 0.0763014554977417\n",
      "Train 409 | out_loss 0.39595842361450195: 100%|█| 125/125 [00:00<00:00, 247.14it\n",
      "Train Epoch409 out_loss 0.1567830741405487, R2 0.08032166957855225\n",
      "Test Epoch409 layer0 out_loss 0.1599562168121338, R2 0.0647968053817749\n",
      "Test Epoch409 layer1 out_loss 0.15916681289672852, R2 0.06941217184066772\n",
      "Test Epoch409 layer2 out_loss 0.1583191156387329, R2 0.07436829805374146\n",
      "Test Epoch409 layer3 out_loss 0.15826307237148285, R2 0.07469594478607178\n",
      "Test Epoch409 layer4 out_loss 0.15830858051776886, R2 0.07442992925643921\n",
      "Train 410 | out_loss 0.39599213004112244: 100%|█| 125/125 [00:00<00:00, 262.93it\n",
      "Train Epoch410 out_loss 0.1568097472190857, R2 0.08016520738601685\n",
      "Test Epoch410 layer0 out_loss 0.15995658934116364, R2 0.06479465961456299\n",
      "Test Epoch410 layer1 out_loss 0.15896284580230713, R2 0.07060468196868896\n",
      "Test Epoch410 layer2 out_loss 0.15795961022377014, R2 0.0764702558517456\n",
      "Test Epoch410 layer3 out_loss 0.15800181031227112, R2 0.07622349262237549\n",
      "Test Epoch410 layer4 out_loss 0.15811468660831451, R2 0.07556354999542236\n",
      "Train 411 | out_loss 0.39580968022346497: 100%|█| 125/125 [00:00<00:00, 259.58it\n",
      "Train Epoch411 out_loss 0.15666528046131134, R2 0.08101266622543335\n",
      "Test Epoch411 layer0 out_loss 0.16051682829856873, R2 0.06151914596557617\n",
      "Test Epoch411 layer1 out_loss 0.1610194593667984, R2 0.05858045816421509\n",
      "Test Epoch411 layer2 out_loss 0.15955249965190887, R2 0.06715714931488037\n",
      "Test Epoch411 layer3 out_loss 0.1598551869392395, R2 0.06538748741149902\n",
      "Test Epoch411 layer4 out_loss 0.16005617380142212, R2 0.06421244144439697\n",
      "Train 412 | out_loss 0.39637064933776855: 100%|█| 125/125 [00:00<00:00, 265.72it\n",
      "Train Epoch412 out_loss 0.15710964798927307, R2 0.07840603590011597\n",
      "Test Epoch412 layer0 out_loss 0.15994657576084137, R2 0.06485319137573242\n",
      "Test Epoch412 layer1 out_loss 0.15892164409160614, R2 0.07084554433822632\n",
      "Test Epoch412 layer2 out_loss 0.1580326110124588, R2 0.07604342699050903\n",
      "Test Epoch412 layer3 out_loss 0.15792003273963928, R2 0.07670164108276367\n",
      "Test Epoch412 layer4 out_loss 0.15799209475517273, R2 0.07628029584884644\n",
      "Train 413 | out_loss 0.3961150646209717: 100%|█| 125/125 [00:00<00:00, 256.96it/\n",
      "Train Epoch413 out_loss 0.15690715610980988, R2 0.07959389686584473\n",
      "Test Epoch413 layer0 out_loss 0.15993955731391907, R2 0.06489419937133789\n",
      "Test Epoch413 layer1 out_loss 0.15938422083854675, R2 0.06814110279083252\n",
      "Test Epoch413 layer2 out_loss 0.1581573486328125, R2 0.07531410455703735\n",
      "Test Epoch413 layer3 out_loss 0.15820224583148956, R2 0.07505166530609131\n",
      "Test Epoch413 layer4 out_loss 0.15823836624622345, R2 0.07484042644500732\n",
      "Train 414 | out_loss 0.3960573971271515: 100%|█| 125/125 [00:00<00:00, 255.40it/\n",
      "Train Epoch414 out_loss 0.15686146914958954, R2 0.07986187934875488\n",
      "Test Epoch414 layer0 out_loss 0.15993402898311615, R2 0.06492650508880615\n",
      "Test Epoch414 layer1 out_loss 0.15908123552799225, R2 0.06991249322891235\n",
      "Test Epoch414 layer2 out_loss 0.1581398993730545, R2 0.0754162073135376\n",
      "Test Epoch414 layer3 out_loss 0.15808363258838654, R2 0.0757451057434082\n",
      "Test Epoch414 layer4 out_loss 0.1582348346710205, R2 0.07486110925674438\n",
      "Train 415 | out_loss 0.3961418569087982: 100%|█| 125/125 [00:00<00:00, 259.25it/\n",
      "Train Epoch415 out_loss 0.15692836046218872, R2 0.07946950197219849\n",
      "Test Epoch415 layer0 out_loss 0.15993286669254303, R2 0.06493335962295532\n",
      "Test Epoch415 layer1 out_loss 0.15888036787509918, R2 0.07108688354492188\n",
      "Test Epoch415 layer2 out_loss 0.15797503292560577, R2 0.07638001441955566\n",
      "Test Epoch415 layer3 out_loss 0.15795709192752838, R2 0.07648491859436035\n",
      "Test Epoch415 layer4 out_loss 0.15801113843917847, R2 0.076168954372406\n",
      "Train 416 | out_loss 0.3960093855857849: 100%|█| 125/125 [00:00<00:00, 260.93it/\n",
      "Train Epoch416 out_loss 0.15682342648506165, R2 0.08008503913879395\n",
      "Test Epoch416 layer0 out_loss 0.1599869430065155, R2 0.06461721658706665\n",
      "Test Epoch416 layer1 out_loss 0.15897774696350098, R2 0.07051753997802734\n",
      "Test Epoch416 layer2 out_loss 0.15809185802936554, R2 0.07569700479507446\n",
      "Test Epoch416 layer3 out_loss 0.15797434747219086, R2 0.07638406753540039\n",
      "Test Epoch416 layer4 out_loss 0.15800487995147705, R2 0.0762055516242981\n",
      "Train 417 | out_loss 0.39605820178985596: 100%|█| 125/125 [00:00<00:00, 257.58it\n",
      "Train Epoch417 out_loss 0.1568620800971985, R2 0.07985830307006836\n",
      "Test Epoch417 layer0 out_loss 0.15994678437709808, R2 0.06485199928283691\n",
      "Test Epoch417 layer1 out_loss 0.15886175632476807, R2 0.07119572162628174\n",
      "Test Epoch417 layer2 out_loss 0.15796780586242676, R2 0.07642233371734619\n",
      "Test Epoch417 layer3 out_loss 0.15793129801750183, R2 0.07663571834564209\n",
      "Test Epoch417 layer4 out_loss 0.15798911452293396, R2 0.07629770040512085\n",
      "Train 418 | out_loss 0.3961552083492279: 100%|█| 125/125 [00:00<00:00, 260.48it/\n",
      "Train Epoch418 out_loss 0.15693897008895874, R2 0.07940727472305298\n",
      "Test Epoch418 layer0 out_loss 0.15992169082164764, R2 0.06499868631362915\n",
      "Test Epoch418 layer1 out_loss 0.15887899696826935, R2 0.07109493017196655\n",
      "Test Epoch418 layer2 out_loss 0.157912477850914, R2 0.07674580812454224\n",
      "Test Epoch418 layer3 out_loss 0.15787823498249054, R2 0.07694602012634277\n",
      "Test Epoch418 layer4 out_loss 0.1579529345035553, R2 0.07650923728942871\n",
      "Train 419 | out_loss 0.3959362506866455: 100%|█| 125/125 [00:00<00:00, 264.22it/\n",
      "Train Epoch419 out_loss 0.15676550567150116, R2 0.08042478561401367\n",
      "Test Epoch419 layer0 out_loss 0.16013094782829285, R2 0.06377524137496948\n",
      "Test Epoch419 layer1 out_loss 0.15909262001514435, R2 0.06984591484069824\n",
      "Test Epoch419 layer2 out_loss 0.15820102393627167, R2 0.07505881786346436\n",
      "Test Epoch419 layer3 out_loss 0.1582251638174057, R2 0.07491767406463623\n",
      "Test Epoch419 layer4 out_loss 0.15830951929092407, R2 0.0744243860244751\n",
      "Train 420 | out_loss 0.39587119221687317: 100%|█| 125/125 [00:00<00:00, 268.25it\n",
      "Train Epoch420 out_loss 0.1567140519618988, R2 0.08072662353515625\n",
      "Test Epoch420 layer0 out_loss 0.1599120795726776, R2 0.06505489349365234\n",
      "Test Epoch420 layer1 out_loss 0.1588747501373291, R2 0.07111972570419312\n",
      "Test Epoch420 layer2 out_loss 0.15791599452495575, R2 0.07672518491744995\n",
      "Test Epoch420 layer3 out_loss 0.1579504758119583, R2 0.07652366161346436\n",
      "Test Epoch420 layer4 out_loss 0.15799415111541748, R2 0.0762682557106018\n",
      "Train 421 | out_loss 0.39626020193099976: 100%|█| 125/125 [00:00<00:00, 263.39it\n",
      "Train Epoch421 out_loss 0.15702210366725922, R2 0.07891952991485596\n",
      "Test Epoch421 layer0 out_loss 0.16011165082454681, R2 0.0638880729675293\n",
      "Test Epoch421 layer1 out_loss 0.15891021490097046, R2 0.07091236114501953\n",
      "Test Epoch421 layer2 out_loss 0.15799380838871002, R2 0.07627028226852417\n",
      "Test Epoch421 layer3 out_loss 0.15802015364170074, R2 0.07611620426177979\n",
      "Test Epoch421 layer4 out_loss 0.15813866257667542, R2 0.07542341947555542\n",
      "Train 422 | out_loss 0.39589759707450867: 100%|█| 125/125 [00:00<00:00, 255.19it\n",
      "Train Epoch422 out_loss 0.15673492848873138, R2 0.08060413599014282\n",
      "Test Epoch422 layer0 out_loss 0.16002270579338074, R2 0.06440812349319458\n",
      "Test Epoch422 layer1 out_loss 0.15954920649528503, R2 0.0671764612197876\n",
      "Test Epoch422 layer2 out_loss 0.1583629995584488, R2 0.0741117000579834\n",
      "Test Epoch422 layer3 out_loss 0.1586097776889801, R2 0.07266896963119507\n",
      "Test Epoch422 layer4 out_loss 0.15867891907691956, R2 0.0722646713256836\n",
      "Train 423 | out_loss 0.39602604508399963: 100%|█| 125/125 [00:00<00:00, 256.69it\n",
      "Train Epoch423 out_loss 0.1568365842103958, R2 0.08000785112380981\n",
      "Test Epoch423 layer0 out_loss 0.1599305421113968, R2 0.06494694948196411\n",
      "Test Epoch423 layer1 out_loss 0.15911449491977692, R2 0.06971800327301025\n",
      "Test Epoch423 layer2 out_loss 0.15801437199115753, R2 0.07615005970001221\n",
      "Test Epoch423 layer3 out_loss 0.15832863748073578, R2 0.07431262731552124\n",
      "Test Epoch423 layer4 out_loss 0.15836092829704285, R2 0.07412385940551758\n",
      "Train 424 | out_loss 0.3959565758705139: 100%|█| 125/125 [00:00<00:00, 261.39it/\n",
      "Train Epoch424 out_loss 0.1567816138267517, R2 0.0803302526473999\n",
      "Test Epoch424 layer0 out_loss 0.1599045693874359, R2 0.06509876251220703\n",
      "Test Epoch424 layer1 out_loss 0.15884540975093842, R2 0.07129126787185669\n",
      "Test Epoch424 layer2 out_loss 0.15800346434116364, R2 0.07621383666992188\n",
      "Test Epoch424 layer3 out_loss 0.15810047090053558, R2 0.07564669847488403\n",
      "Test Epoch424 layer4 out_loss 0.15804116427898407, R2 0.0759933590888977\n",
      "Train 425 | out_loss 0.39600393176078796: 100%|█| 125/125 [00:00<00:00, 258.70it\n",
      "Train Epoch425 out_loss 0.15681913495063782, R2 0.08011019229888916\n",
      "Test Epoch425 layer0 out_loss 0.15998147428035736, R2 0.06464916467666626\n",
      "Test Epoch425 layer1 out_loss 0.15937356650829315, R2 0.0682033896446228\n",
      "Test Epoch425 layer2 out_loss 0.15815716981887817, R2 0.07531517744064331\n",
      "Test Epoch425 layer3 out_loss 0.1583205908536911, R2 0.0743597149848938\n",
      "Test Epoch425 layer4 out_loss 0.1584315001964569, R2 0.07371127605438232\n",
      "Train 426 | out_loss 0.39591318368911743: 100%|█| 125/125 [00:00<00:00, 251.74it\n",
      "Train Epoch426 out_loss 0.1567472219467163, R2 0.0805320143699646\n",
      "Test Epoch426 layer0 out_loss 0.1599002480506897, R2 0.0651240348815918\n",
      "Test Epoch426 layer1 out_loss 0.15887762606143951, R2 0.07110291719436646\n",
      "Test Epoch426 layer2 out_loss 0.15786094963550568, R2 0.07704704999923706\n",
      "Test Epoch426 layer3 out_loss 0.15787425637245178, R2 0.07696926593780518\n",
      "Test Epoch426 layer4 out_loss 0.15796741843223572, R2 0.07642453908920288\n",
      "Train 427 | out_loss 0.3958858847618103: 100%|█| 125/125 [00:00<00:00, 264.20it/\n",
      "Train Epoch427 out_loss 0.15672564506530762, R2 0.08065855503082275\n",
      "Test Epoch427 layer0 out_loss 0.15988799929618835, R2 0.06519567966461182\n",
      "Test Epoch427 layer1 out_loss 0.15888401865959167, R2 0.07106554508209229\n",
      "Test Epoch427 layer2 out_loss 0.15800601243972778, R2 0.07619893550872803\n",
      "Test Epoch427 layer3 out_loss 0.158018559217453, R2 0.0761256217956543\n",
      "Test Epoch427 layer4 out_loss 0.15799164772033691, R2 0.07628285884857178\n",
      "Train 428 | out_loss 0.39577922224998474: 100%|█| 125/125 [00:00<00:00, 253.75it\n",
      "Train Epoch428 out_loss 0.15664120018482208, R2 0.08115392923355103\n",
      "Test Epoch428 layer0 out_loss 0.1601123958826065, R2 0.0638837218284607\n",
      "Test Epoch428 layer1 out_loss 0.15950943529605865, R2 0.06740903854370117\n",
      "Test Epoch428 layer2 out_loss 0.15833710134029388, R2 0.07426321506500244\n",
      "Test Epoch428 layer3 out_loss 0.15851281583309174, R2 0.07323580980300903\n",
      "Test Epoch428 layer4 out_loss 0.15867890417575836, R2 0.07226473093032837\n",
      "Train 429 | out_loss 0.39599570631980896: 100%|█| 125/125 [00:00<00:00, 264.47it\n",
      "Train Epoch429 out_loss 0.1568126082420349, R2 0.08014845848083496\n",
      "Test Epoch429 layer0 out_loss 0.16003066301345825, R2 0.06436151266098022\n",
      "Test Epoch429 layer1 out_loss 0.1588752567768097, R2 0.07111674547195435\n",
      "Test Epoch429 layer2 out_loss 0.15794222056865692, R2 0.07657194137573242\n",
      "Test Epoch429 layer3 out_loss 0.15785862505435944, R2 0.07706063985824585\n",
      "Test Epoch429 layer4 out_loss 0.15794800221920013, R2 0.07653814554214478\n",
      "Train 430 | out_loss 0.3960460424423218: 100%|█| 125/125 [00:00<00:00, 264.69it/\n",
      "Train Epoch430 out_loss 0.15685248374938965, R2 0.07991456985473633\n",
      "Test Epoch430 layer0 out_loss 0.15988107025623322, R2 0.0652361512184143\n",
      "Test Epoch430 layer1 out_loss 0.15904825925827026, R2 0.07010531425476074\n",
      "Test Epoch430 layer2 out_loss 0.15810222923755646, R2 0.07563638687133789\n",
      "Test Epoch430 layer3 out_loss 0.15796853601932526, R2 0.07641804218292236\n",
      "Test Epoch430 layer4 out_loss 0.1580432504415512, R2 0.0759812593460083\n",
      "Train 431 | out_loss 0.3957960307598114: 100%|█| 125/125 [00:00<00:00, 264.43it/\n",
      "Train Epoch431 out_loss 0.15665452182292938, R2 0.08107578754425049\n",
      "Test Epoch431 layer0 out_loss 0.15988294780254364, R2 0.06522518396377563\n",
      "Test Epoch431 layer1 out_loss 0.1589672714471817, R2 0.07057881355285645\n",
      "Test Epoch431 layer2 out_loss 0.1580384522676468, R2 0.07600927352905273\n",
      "Test Epoch431 layer3 out_loss 0.15802983939647675, R2 0.07605957984924316\n",
      "Test Epoch431 layer4 out_loss 0.1581106334924698, R2 0.07558727264404297\n",
      "Train 432 | out_loss 0.39576542377471924: 100%|█| 125/125 [00:00<00:00, 260.23it\n",
      "Train Epoch432 out_loss 0.15663029253482819, R2 0.08121788501739502\n",
      "Test Epoch432 layer0 out_loss 0.1599113494157791, R2 0.06505918502807617\n",
      "Test Epoch432 layer1 out_loss 0.15893979370594025, R2 0.0707395076751709\n",
      "Test Epoch432 layer2 out_loss 0.15787054598331451, R2 0.07699096202850342\n",
      "Test Epoch432 layer3 out_loss 0.15794013440608978, R2 0.07658404111862183\n",
      "Test Epoch432 layer4 out_loss 0.15799270570278168, R2 0.07627671957015991\n",
      "Train 433 | out_loss 0.3959057629108429: 100%|█| 125/125 [00:00<00:00, 260.86it/\n",
      "Train Epoch433 out_loss 0.15674138069152832, R2 0.08056628704071045\n",
      "Test Epoch433 layer0 out_loss 0.15997031331062317, R2 0.06471443176269531\n",
      "Test Epoch433 layer1 out_loss 0.15902970731258392, R2 0.07021379470825195\n",
      "Test Epoch433 layer2 out_loss 0.15799883008003235, R2 0.07624095678329468\n",
      "Test Epoch433 layer3 out_loss 0.1580885648727417, R2 0.07571631669998169\n",
      "Test Epoch433 layer4 out_loss 0.15831682085990906, R2 0.0743817687034607\n",
      "Train 434 | out_loss 0.3959228992462158: 100%|█| 125/125 [00:00<00:00, 254.14it/\n",
      "Train Epoch434 out_loss 0.15675494074821472, R2 0.0804867148399353\n",
      "Test Epoch434 layer0 out_loss 0.15987056493759155, R2 0.06529760360717773\n",
      "Test Epoch434 layer1 out_loss 0.15886588394641876, R2 0.07117152214050293\n",
      "Test Epoch434 layer2 out_loss 0.15795248746871948, R2 0.0765119194984436\n",
      "Test Epoch434 layer3 out_loss 0.15803192555904388, R2 0.07604748010635376\n",
      "Test Epoch434 layer4 out_loss 0.15804681181907654, R2 0.07596039772033691\n",
      "Train 435 | out_loss 0.39593350887298584: 100%|█| 125/125 [00:00<00:00, 262.61it\n",
      "Train Epoch435 out_loss 0.15676334500312805, R2 0.08043742179870605\n",
      "Test Epoch435 layer0 out_loss 0.15985649824142456, R2 0.06537985801696777\n",
      "Test Epoch435 layer1 out_loss 0.15880922973155975, R2 0.07150280475616455\n",
      "Test Epoch435 layer2 out_loss 0.15783101320266724, R2 0.07722210884094238\n",
      "Test Epoch435 layer3 out_loss 0.15780189633369446, R2 0.0773923397064209\n",
      "Test Epoch435 layer4 out_loss 0.15788598358631134, R2 0.0769006609916687\n",
      "Train 436 | out_loss 0.39588019251823425: 100%|█| 125/125 [00:00<00:00, 261.57it\n",
      "Train Epoch436 out_loss 0.15672113001346588, R2 0.0806850790977478\n",
      "Test Epoch436 layer0 out_loss 0.1599123626947403, R2 0.06505316495895386\n",
      "Test Epoch436 layer1 out_loss 0.15918543934822083, R2 0.06930327415466309\n",
      "Test Epoch436 layer2 out_loss 0.15832944214344025, R2 0.07430797815322876\n",
      "Test Epoch436 layer3 out_loss 0.15808573365211487, R2 0.07573282718658447\n",
      "Test Epoch436 layer4 out_loss 0.15815585851669312, R2 0.07532286643981934\n",
      "Train 437 | out_loss 0.39584532380104065: 100%|█| 125/125 [00:00<00:00, 262.86it\n",
      "Train Epoch437 out_loss 0.15669351816177368, R2 0.08084708452224731\n",
      "Test Epoch437 layer0 out_loss 0.16000567376613617, R2 0.06450766324996948\n",
      "Test Epoch437 layer1 out_loss 0.1590123474597931, R2 0.07031530141830444\n",
      "Test Epoch437 layer2 out_loss 0.1583034098148346, R2 0.07446020841598511\n",
      "Test Epoch437 layer3 out_loss 0.15831102430820465, R2 0.07441568374633789\n",
      "Test Epoch437 layer4 out_loss 0.15838386118412018, R2 0.07398974895477295\n",
      "Train 438 | out_loss 0.3956262171268463: 100%|█| 125/125 [00:00<00:00, 265.81it/\n",
      "Train Epoch438 out_loss 0.15652012825012207, R2 0.0818641185760498\n",
      "Test Epoch438 layer0 out_loss 0.1601269394159317, R2 0.06379866600036621\n",
      "Test Epoch438 layer1 out_loss 0.15953779220581055, R2 0.06724315881729126\n",
      "Test Epoch438 layer2 out_loss 0.15854012966156006, R2 0.07307612895965576\n",
      "Test Epoch438 layer3 out_loss 0.15855009853839874, R2 0.0730178952217102\n",
      "Test Epoch438 layer4 out_loss 0.1586141437292099, R2 0.07264339923858643\n",
      "Train 439 | out_loss 0.39586615562438965: 100%|█| 125/125 [00:00<00:00, 248.48it\n",
      "Train Epoch439 out_loss 0.1567099541425705, R2 0.08075058460235596\n",
      "Test Epoch439 layer0 out_loss 0.15984545648097992, R2 0.06544440984725952\n",
      "Test Epoch439 layer1 out_loss 0.15891531109809875, R2 0.07088261842727661\n",
      "Test Epoch439 layer2 out_loss 0.1579081416130066, R2 0.07677114009857178\n",
      "Test Epoch439 layer3 out_loss 0.15788409113883972, R2 0.07691174745559692\n",
      "Test Epoch439 layer4 out_loss 0.15795649588108063, R2 0.0764884352684021\n",
      "Train 440 | out_loss 0.39600247144699097: 100%|█| 125/125 [00:00<00:00, 249.91it\n",
      "Train Epoch440 out_loss 0.15681788325309753, R2 0.08011752367019653\n",
      "Test Epoch440 layer0 out_loss 0.1599150449037552, R2 0.0650375485420227\n",
      "Test Epoch440 layer1 out_loss 0.15937015414237976, R2 0.06822335720062256\n",
      "Test Epoch440 layer2 out_loss 0.15830574929714203, R2 0.07444643974304199\n",
      "Test Epoch440 layer3 out_loss 0.15851537883281708, R2 0.07322084903717041\n",
      "Test Epoch440 layer4 out_loss 0.15838460624217987, R2 0.07398539781570435\n",
      "Train 441 | out_loss 0.3957845866680145: 100%|█| 125/125 [00:00<00:00, 252.81it/\n",
      "Train Epoch441 out_loss 0.15664544701576233, R2 0.08112901449203491\n",
      "Test Epoch441 layer0 out_loss 0.15987934172153473, R2 0.06524622440338135\n",
      "Test Epoch441 layer1 out_loss 0.15880447626113892, R2 0.07153064012527466\n",
      "Test Epoch441 layer2 out_loss 0.15794970095157623, R2 0.07652813196182251\n",
      "Test Epoch441 layer3 out_loss 0.15810461342334747, R2 0.07562243938446045\n",
      "Test Epoch441 layer4 out_loss 0.15805864334106445, R2 0.07589125633239746\n",
      "Train 442 | out_loss 0.3959624469280243: 100%|█| 125/125 [00:00<00:00, 262.78it/\n",
      "Train Epoch442 out_loss 0.1567862182855606, R2 0.08030325174331665\n",
      "Test Epoch442 layer0 out_loss 0.15987157821655273, R2 0.0652916431427002\n",
      "Test Epoch442 layer1 out_loss 0.1590098887681961, R2 0.07032966613769531\n",
      "Test Epoch442 layer2 out_loss 0.1579291671514511, R2 0.07664823532104492\n",
      "Test Epoch442 layer3 out_loss 0.15802830457687378, R2 0.07606858015060425\n",
      "Test Epoch442 layer4 out_loss 0.15811052918434143, R2 0.07558780908584595\n",
      "Train 443 | out_loss 0.3959558606147766: 100%|█| 125/125 [00:00<00:00, 242.40it/\n",
      "Train Epoch443 out_loss 0.15678104758262634, R2 0.08033359050750732\n",
      "Test Epoch443 layer0 out_loss 0.15983407199382782, R2 0.06551092863082886\n",
      "Test Epoch443 layer1 out_loss 0.15877416729927063, R2 0.07170778512954712\n",
      "Test Epoch443 layer2 out_loss 0.15780983865261078, R2 0.0773458480834961\n",
      "Test Epoch443 layer3 out_loss 0.1577681452035904, R2 0.07758969068527222\n",
      "Test Epoch443 layer4 out_loss 0.15786845982074738, R2 0.07700318098068237\n",
      "Train 444 | out_loss 0.39576244354248047: 100%|█| 125/125 [00:00<00:00, 261.19it\n",
      "Train Epoch444 out_loss 0.15662787854671478, R2 0.08123207092285156\n",
      "Test Epoch444 layer0 out_loss 0.15983566641807556, R2 0.06550168991088867\n",
      "Test Epoch444 layer1 out_loss 0.15882647037506104, R2 0.07140201330184937\n",
      "Test Epoch444 layer2 out_loss 0.15779957175254822, R2 0.07740586996078491\n",
      "Test Epoch444 layer3 out_loss 0.1578284054994583, R2 0.07723736763000488\n",
      "Test Epoch444 layer4 out_loss 0.1578768640756607, R2 0.07695400714874268\n",
      "Train 445 | out_loss 0.3959987163543701: 100%|█| 125/125 [00:00<00:00, 245.51it/\n",
      "Train Epoch445 out_loss 0.15681496262550354, R2 0.08013468980789185\n",
      "Test Epoch445 layer0 out_loss 0.1598249226808548, R2 0.06556445360183716\n",
      "Test Epoch445 layer1 out_loss 0.15896542370319366, R2 0.07058960199356079\n",
      "Test Epoch445 layer2 out_loss 0.15780527889728546, R2 0.07737255096435547\n",
      "Test Epoch445 layer3 out_loss 0.15775854885578156, R2 0.07764577865600586\n",
      "Test Epoch445 layer4 out_loss 0.15784473717212677, R2 0.07714182138442993\n",
      "Train 446 | out_loss 0.39587390422821045: 100%|█| 125/125 [00:00<00:00, 259.25it\n",
      "Train Epoch446 out_loss 0.15671613812446594, R2 0.08071434497833252\n",
      "Test Epoch446 layer0 out_loss 0.15984399616718292, R2 0.0654529333114624\n",
      "Test Epoch446 layer1 out_loss 0.15891477465629578, R2 0.07088571786880493\n",
      "Test Epoch446 layer2 out_loss 0.15779800713062286, R2 0.07741504907608032\n",
      "Test Epoch446 layer3 out_loss 0.1578024923801422, R2 0.07738882303237915\n",
      "Test Epoch446 layer4 out_loss 0.15789680182933807, R2 0.07683742046356201\n",
      "Train 447 | out_loss 0.39557453989982605: 100%|█| 125/125 [00:00<00:00, 257.43it\n",
      "Train Epoch447 out_loss 0.15647922456264496, R2 0.08210408687591553\n",
      "Test Epoch447 layer0 out_loss 0.15982000529766083, R2 0.0655931830406189\n",
      "Test Epoch447 layer1 out_loss 0.15885061025619507, R2 0.07126086950302124\n",
      "Test Epoch447 layer2 out_loss 0.15782175958156586, R2 0.07727617025375366\n",
      "Test Epoch447 layer3 out_loss 0.15820544958114624, R2 0.07503294944763184\n",
      "Test Epoch447 layer4 out_loss 0.15803325176239014, R2 0.07603967189788818\n",
      "Train 448 | out_loss 0.3957101106643677: 100%|█| 125/125 [00:00<00:00, 242.61it/\n",
      "Train Epoch448 out_loss 0.15658646821975708, R2 0.08147495985031128\n",
      "Test Epoch448 layer0 out_loss 0.15982036292552948, R2 0.06559115648269653\n",
      "Test Epoch448 layer1 out_loss 0.1590098738670349, R2 0.07032972574234009\n",
      "Test Epoch448 layer2 out_loss 0.15796440839767456, R2 0.07644224166870117\n",
      "Test Epoch448 layer3 out_loss 0.15809887647628784, R2 0.075655996799469\n",
      "Test Epoch448 layer4 out_loss 0.15818917751312256, R2 0.07512801885604858\n",
      "Train 449 | out_loss 0.3957693874835968: 100%|█| 125/125 [00:00<00:00, 238.20it/\n",
      "Train Epoch449 out_loss 0.1566334217786789, R2 0.0811995267868042\n",
      "Test Epoch449 layer0 out_loss 0.159857839345932, R2 0.0653720498085022\n",
      "Test Epoch449 layer1 out_loss 0.1589270681142807, R2 0.07081389427185059\n",
      "Test Epoch449 layer2 out_loss 0.15788087248802185, R2 0.07693058252334595\n",
      "Test Epoch449 layer3 out_loss 0.15798631310462952, R2 0.07631409168243408\n",
      "Test Epoch449 layer4 out_loss 0.15805204212665558, R2 0.07592976093292236\n",
      "Train 450 | out_loss 0.3956562280654907: 100%|█| 125/125 [00:00<00:00, 262.87it/\n",
      "Train Epoch450 out_loss 0.1565438210964203, R2 0.0817251205444336\n",
      "Test Epoch450 layer0 out_loss 0.15980923175811768, R2 0.06565618515014648\n",
      "Test Epoch450 layer1 out_loss 0.15874764323234558, R2 0.07186287641525269\n",
      "Test Epoch450 layer2 out_loss 0.15792375802993774, R2 0.07667988538742065\n",
      "Test Epoch450 layer3 out_loss 0.15800470113754272, R2 0.07620662450790405\n",
      "Test Epoch450 layer4 out_loss 0.15795375406742096, R2 0.07650446891784668\n",
      "Train 451 | out_loss 0.39565354585647583: 100%|█| 125/125 [00:00<00:00, 250.48it\n",
      "Train Epoch451 out_loss 0.15654167532920837, R2 0.08173775672912598\n",
      "Test Epoch451 layer0 out_loss 0.15987731516361237, R2 0.06525808572769165\n",
      "Test Epoch451 layer1 out_loss 0.1588686853647232, R2 0.07115525007247925\n",
      "Test Epoch451 layer2 out_loss 0.1580270528793335, R2 0.07607591152191162\n",
      "Test Epoch451 layer3 out_loss 0.15811890363693237, R2 0.07553893327713013\n",
      "Test Epoch451 layer4 out_loss 0.15821720659732819, R2 0.07496416568756104\n",
      "Train 452 | out_loss 0.3958524465560913: 100%|█| 125/125 [00:00<00:00, 251.45it/\n",
      "Train Epoch452 out_loss 0.15669912099838257, R2 0.08081412315368652\n",
      "Test Epoch452 layer0 out_loss 0.16008228063583374, R2 0.0640597939491272\n",
      "Test Epoch452 layer1 out_loss 0.1591322273015976, R2 0.06961435079574585\n",
      "Test Epoch452 layer2 out_loss 0.15860630571842194, R2 0.0726892352104187\n",
      "Test Epoch452 layer3 out_loss 0.15875652432441711, R2 0.07181096076965332\n",
      "Test Epoch452 layer4 out_loss 0.15882839262485504, R2 0.07139074802398682\n",
      "Train 453 | out_loss 0.3957805335521698: 100%|█| 125/125 [00:00<00:00, 252.25it/\n",
      "Train Epoch453 out_loss 0.15664218366146088, R2 0.08114820718765259\n",
      "Test Epoch453 layer0 out_loss 0.15987548232078552, R2 0.06526881456375122\n",
      "Test Epoch453 layer1 out_loss 0.15922684967517853, R2 0.06906116008758545\n",
      "Test Epoch453 layer2 out_loss 0.15831086039543152, R2 0.07441657781600952\n",
      "Test Epoch453 layer3 out_loss 0.15831886231899261, R2 0.07436978816986084\n",
      "Test Epoch453 layer4 out_loss 0.15825298428535461, R2 0.07475495338439941\n",
      "Train 454 | out_loss 0.3958718180656433: 100%|█| 125/125 [00:00<00:00, 254.76it/\n",
      "Train Epoch454 out_loss 0.15671448409557343, R2 0.08072412014007568\n",
      "Test Epoch454 layer0 out_loss 0.1598571538925171, R2 0.06537604331970215\n",
      "Test Epoch454 layer1 out_loss 0.1589883416891098, R2 0.07045561075210571\n",
      "Test Epoch454 layer2 out_loss 0.15814854204654694, R2 0.07536560297012329\n",
      "Test Epoch454 layer3 out_loss 0.15814624726772308, R2 0.07537907361984253\n",
      "Test Epoch454 layer4 out_loss 0.15811070799827576, R2 0.07558685541152954\n",
      "Train 455 | out_loss 0.3956768214702606: 100%|█| 125/125 [00:00<00:00, 259.03it/\n",
      "Train Epoch455 out_loss 0.15656013786792755, R2 0.08162945508956909\n",
      "Test Epoch455 layer0 out_loss 0.15979072451591492, R2 0.06576436758041382\n",
      "Test Epoch455 layer1 out_loss 0.15873585641384125, R2 0.07193183898925781\n",
      "Test Epoch455 layer2 out_loss 0.15773513913154602, R2 0.07778263092041016\n",
      "Test Epoch455 layer3 out_loss 0.1577197015285492, R2 0.07787293195724487\n",
      "Test Epoch455 layer4 out_loss 0.15782952308654785, R2 0.07723075151443481\n",
      "Train 456 | out_loss 0.39545774459838867: 100%|█| 125/125 [00:00<00:00, 263.85it\n",
      "Train Epoch456 out_loss 0.15638680756092072, R2 0.0826461911201477\n",
      "Test Epoch456 layer0 out_loss 0.1599382609128952, R2 0.06490176916122437\n",
      "Test Epoch456 layer1 out_loss 0.16082781553268433, R2 0.05970090627670288\n",
      "Test Epoch456 layer2 out_loss 0.1592181921005249, R2 0.06911182403564453\n",
      "Test Epoch456 layer3 out_loss 0.15984492003917694, R2 0.06544750928878784\n",
      "Test Epoch456 layer4 out_loss 0.160136416554451, R2 0.06374329328536987\n",
      "Train 457 | out_loss 0.39614546298980713: 100%|█| 125/125 [00:00<00:00, 259.33it\n",
      "Train Epoch457 out_loss 0.15693120658397675, R2 0.0794527530670166\n",
      "Test Epoch457 layer0 out_loss 0.1598002314567566, R2 0.06570881605148315\n",
      "Test Epoch457 layer1 out_loss 0.1587677299976349, R2 0.07174545526504517\n",
      "Test Epoch457 layer2 out_loss 0.15780124068260193, R2 0.07739615440368652\n",
      "Test Epoch457 layer3 out_loss 0.1577097624540329, R2 0.0779309868812561\n",
      "Test Epoch457 layer4 out_loss 0.15779832005500793, R2 0.07741320133209229\n",
      "Train 458 | out_loss 0.39573267102241516: 100%|█| 125/125 [00:00<00:00, 261.20it\n",
      "Train Epoch458 out_loss 0.1566043198108673, R2 0.08137023448944092\n",
      "Test Epoch458 layer0 out_loss 0.1599339246749878, R2 0.06492716073989868\n",
      "Test Epoch458 layer1 out_loss 0.15941186249256134, R2 0.06797945499420166\n",
      "Test Epoch458 layer2 out_loss 0.15815788507461548, R2 0.07531100511550903\n",
      "Test Epoch458 layer3 out_loss 0.15815222263336182, R2 0.07534408569335938\n",
      "Test Epoch458 layer4 out_loss 0.15834201872348785, R2 0.07423436641693115\n",
      "Train 459 | out_loss 0.39568284153938293: 100%|█| 125/125 [00:00<00:00, 252.44it\n",
      "Train Epoch459 out_loss 0.15656490623950958, R2 0.08160144090652466\n",
      "Test Epoch459 layer0 out_loss 0.15979160368442535, R2 0.06575924158096313\n",
      "Test Epoch459 layer1 out_loss 0.15883506834506989, R2 0.07135176658630371\n",
      "Test Epoch459 layer2 out_loss 0.157831609249115, R2 0.07721865177154541\n",
      "Test Epoch459 layer3 out_loss 0.15783298015594482, R2 0.07721060514450073\n",
      "Test Epoch459 layer4 out_loss 0.15782605111598969, R2 0.07725107669830322\n",
      "Train 460 | out_loss 0.39572322368621826: 100%|█| 125/125 [00:00<00:00, 236.96it\n",
      "Train Epoch460 out_loss 0.15659686923027039, R2 0.08141398429870605\n",
      "Test Epoch460 layer0 out_loss 0.15976646542549133, R2 0.06590622663497925\n",
      "Test Epoch460 layer1 out_loss 0.1587289422750473, R2 0.07197225093841553\n",
      "Test Epoch460 layer2 out_loss 0.15775130689144135, R2 0.07768803834915161\n",
      "Test Epoch460 layer3 out_loss 0.157733753323555, R2 0.07779073715209961\n",
      "Test Epoch460 layer4 out_loss 0.15785790979862213, R2 0.07706481218338013\n",
      "Train 461 | out_loss 0.39562514424324036: 100%|█| 125/125 [00:00<00:00, 260.44it\n",
      "Train Epoch461 out_loss 0.15651927888393402, R2 0.08186918497085571\n",
      "Test Epoch461 layer0 out_loss 0.1597764492034912, R2 0.06584787368774414\n",
      "Test Epoch461 layer1 out_loss 0.15874344110488892, R2 0.07188749313354492\n",
      "Test Epoch461 layer2 out_loss 0.15774613618850708, R2 0.07771831750869751\n",
      "Test Epoch461 layer3 out_loss 0.15773905813694, R2 0.07775968313217163\n",
      "Test Epoch461 layer4 out_loss 0.15787272155284882, R2 0.07697820663452148\n",
      "Train 462 | out_loss 0.3955726623535156: 100%|█| 125/125 [00:00<00:00, 255.89it/\n",
      "Train Epoch462 out_loss 0.15647770464420319, R2 0.08211296796798706\n",
      "Test Epoch462 layer0 out_loss 0.15978597104549408, R2 0.06579220294952393\n",
      "Test Epoch462 layer1 out_loss 0.15880410373210907, R2 0.07153278589248657\n",
      "Test Epoch462 layer2 out_loss 0.15786461532115936, R2 0.0770256519317627\n",
      "Test Epoch462 layer3 out_loss 0.1578506976366043, R2 0.0771070122718811\n",
      "Test Epoch462 layer4 out_loss 0.1579587310552597, R2 0.07647538185119629\n",
      "Train 463 | out_loss 0.3954886496067047: 100%|█| 125/125 [00:00<00:00, 263.73it/\n",
      "Train Epoch463 out_loss 0.15641123056411743, R2 0.08250296115875244\n",
      "Test Epoch463 layer0 out_loss 0.15976104140281677, R2 0.06593793630599976\n",
      "Test Epoch463 layer1 out_loss 0.15896110236644745, R2 0.07061487436294556\n",
      "Test Epoch463 layer2 out_loss 0.15786293148994446, R2 0.07703548669815063\n",
      "Test Epoch463 layer3 out_loss 0.1579093337059021, R2 0.07676416635513306\n",
      "Test Epoch463 layer4 out_loss 0.15809418261051178, R2 0.07568347454071045\n",
      "Train 464 | out_loss 0.3956547975540161: 100%|█| 125/125 [00:00<00:00, 247.95it/\n",
      "Train Epoch464 out_loss 0.15654268860816956, R2 0.08173173666000366\n",
      "Test Epoch464 layer0 out_loss 0.15983690321445465, R2 0.06549441814422607\n",
      "Test Epoch464 layer1 out_loss 0.15870289504528046, R2 0.07212454080581665\n",
      "Test Epoch464 layer2 out_loss 0.1577150672674179, R2 0.07789993286132812\n",
      "Test Epoch464 layer3 out_loss 0.15770386159420013, R2 0.07796549797058105\n",
      "Test Epoch464 layer4 out_loss 0.1577644795179367, R2 0.07761108875274658\n",
      "Train 465 | out_loss 0.39574769139289856: 100%|█| 125/125 [00:00<00:00, 254.25it\n",
      "Train Epoch465 out_loss 0.15661625564098358, R2 0.08130025863647461\n",
      "Test Epoch465 layer0 out_loss 0.15975579619407654, R2 0.0659685730934143\n",
      "Test Epoch465 layer1 out_loss 0.1587376743555069, R2 0.07192122936248779\n",
      "Test Epoch465 layer2 out_loss 0.15775786340236664, R2 0.07764977216720581\n",
      "Test Epoch465 layer3 out_loss 0.15765994787216187, R2 0.07822227478027344\n",
      "Test Epoch465 layer4 out_loss 0.1577417403459549, R2 0.0777440071105957\n",
      "Train 466 | out_loss 0.3956376612186432: 100%|█| 125/125 [00:00<00:00, 251.29it/\n",
      "Train Epoch466 out_loss 0.15652912855148315, R2 0.08181130886077881\n",
      "Test Epoch466 layer0 out_loss 0.15978235006332397, R2 0.06581336259841919\n",
      "Test Epoch466 layer1 out_loss 0.15871110558509827, R2 0.07207649946212769\n",
      "Test Epoch466 layer2 out_loss 0.15774483978748322, R2 0.07772588729858398\n",
      "Test Epoch466 layer3 out_loss 0.1576959192752838, R2 0.07801192998886108\n",
      "Test Epoch466 layer4 out_loss 0.15783250331878662, R2 0.0772133469581604\n",
      "Train 467 | out_loss 0.3954906761646271: 100%|█| 125/125 [00:00<00:00, 256.55it/\n",
      "Train Epoch467 out_loss 0.15641291439533234, R2 0.08249300718307495\n",
      "Test Epoch467 layer0 out_loss 0.15974684059619904, R2 0.06602096557617188\n",
      "Test Epoch467 layer1 out_loss 0.15871089696884155, R2 0.07207775115966797\n",
      "Test Epoch467 layer2 out_loss 0.15783342719078064, R2 0.07720804214477539\n",
      "Test Epoch467 layer3 out_loss 0.15798626840114594, R2 0.07631438970565796\n",
      "Test Epoch467 layer4 out_loss 0.15800733864307404, R2 0.07619118690490723\n",
      "Train 468 | out_loss 0.39560893177986145: 100%|█| 125/125 [00:00<00:00, 252.90it\n",
      "Train Epoch468 out_loss 0.15650643408298492, R2 0.08194440603256226\n",
      "Test Epoch468 layer0 out_loss 0.15974275767803192, R2 0.0660448670387268\n",
      "Test Epoch468 layer1 out_loss 0.15868906676769257, R2 0.07220536470413208\n",
      "Test Epoch468 layer2 out_loss 0.1577150970697403, R2 0.0778997540473938\n",
      "Test Epoch468 layer3 out_loss 0.1576930284500122, R2 0.0780288577079773\n",
      "Test Epoch468 layer4 out_loss 0.15775251388549805, R2 0.07768100500106812\n",
      "Train 469 | out_loss 0.3956495225429535: 100%|█| 125/125 [00:00<00:00, 252.81it/\n",
      "Train Epoch469 out_loss 0.15653851628303528, R2 0.08175629377365112\n",
      "Test Epoch469 layer0 out_loss 0.15994694828987122, R2 0.06485104560852051\n",
      "Test Epoch469 layer1 out_loss 0.159295916557312, R2 0.06865739822387695\n",
      "Test Epoch469 layer2 out_loss 0.1581307053565979, R2 0.07546991109848022\n",
      "Test Epoch469 layer3 out_loss 0.15830004215240479, R2 0.07447981834411621\n",
      "Test Epoch469 layer4 out_loss 0.15851962566375732, R2 0.07319605350494385\n",
      "Train 470 | out_loss 0.3958001434803009: 100%|█| 125/125 [00:00<00:00, 256.12it/\n",
      "Train Epoch470 out_loss 0.15665778517723083, R2 0.08105659484863281\n",
      "Test Epoch470 layer0 out_loss 0.15985073149204254, R2 0.06541353464126587\n",
      "Test Epoch470 layer1 out_loss 0.15949691832065582, R2 0.06748217344284058\n",
      "Test Epoch470 layer2 out_loss 0.15831486880779266, R2 0.07439321279525757\n",
      "Test Epoch470 layer3 out_loss 0.15791350603103638, R2 0.07673972845077515\n",
      "Test Epoch470 layer4 out_loss 0.15795689821243286, R2 0.07648611068725586\n",
      "Train 471 | out_loss 0.3955698013305664: 100%|█| 125/125 [00:00<00:00, 254.56it/\n",
      "Train Epoch471 out_loss 0.1564754992723465, R2 0.0821259617805481\n",
      "Test Epoch471 layer0 out_loss 0.15977519750595093, R2 0.06585520505905151\n",
      "Test Epoch471 layer1 out_loss 0.15867775678634644, R2 0.07227152585983276\n",
      "Test Epoch471 layer2 out_loss 0.15767771005630493, R2 0.07811844348907471\n",
      "Test Epoch471 layer3 out_loss 0.15766577422618866, R2 0.07818818092346191\n",
      "Test Epoch471 layer4 out_loss 0.15773096680641174, R2 0.07780700922012329\n",
      "Train 472 | out_loss 0.3955162465572357: 100%|█| 125/125 [00:00<00:00, 256.71it/\n",
      "Train Epoch472 out_loss 0.15643310546875, R2 0.08237457275390625\n",
      "Test Epoch472 layer0 out_loss 0.15976989269256592, R2 0.06588613986968994\n",
      "Test Epoch472 layer1 out_loss 0.15876014530658722, R2 0.07178980112075806\n",
      "Test Epoch472 layer2 out_loss 0.15792088210582733, R2 0.07669663429260254\n",
      "Test Epoch472 layer3 out_loss 0.15783283114433289, R2 0.07721149921417236\n",
      "Test Epoch472 layer4 out_loss 0.15793219208717346, R2 0.07663053274154663\n",
      "Train 473 | out_loss 0.3956010341644287: 100%|█| 125/125 [00:00<00:00, 260.24it/\n",
      "Train Epoch473 out_loss 0.1565002053976059, R2 0.08198106288909912\n",
      "Test Epoch473 layer0 out_loss 0.15972179174423218, R2 0.06616747379302979\n",
      "Test Epoch473 layer1 out_loss 0.15872634947299957, R2 0.07198739051818848\n",
      "Test Epoch473 layer2 out_loss 0.15768204629421234, R2 0.07809299230575562\n",
      "Test Epoch473 layer3 out_loss 0.1577165424823761, R2 0.07789140939712524\n",
      "Test Epoch473 layer4 out_loss 0.15784049034118652, R2 0.07716673612594604\n",
      "Train 474 | out_loss 0.39543020725250244: 100%|█| 125/125 [00:00<00:00, 259.18it\n",
      "Train Epoch474 out_loss 0.1563650369644165, R2 0.08277386426925659\n",
      "Test Epoch474 layer0 out_loss 0.1597927212715149, R2 0.06575268507003784\n",
      "Test Epoch474 layer1 out_loss 0.15920811891555786, R2 0.06917071342468262\n",
      "Test Epoch474 layer2 out_loss 0.15822356939315796, R2 0.07492697238922119\n",
      "Test Epoch474 layer3 out_loss 0.15809524059295654, R2 0.07567721605300903\n",
      "Test Epoch474 layer4 out_loss 0.15823867917060852, R2 0.07483863830566406\n",
      "Train 475 | out_loss 0.39574068784713745: 100%|█| 125/125 [00:00<00:00, 246.17it\n",
      "Train Epoch475 out_loss 0.15661069750785828, R2 0.08133280277252197\n",
      "Test Epoch475 layer0 out_loss 0.15978467464447021, R2 0.06579983234405518\n",
      "Test Epoch475 layer1 out_loss 0.1586916446685791, R2 0.0721902847290039\n",
      "Test Epoch475 layer2 out_loss 0.157743901014328, R2 0.0777314305305481\n",
      "Test Epoch475 layer3 out_loss 0.15786133706569672, R2 0.0770447850227356\n",
      "Test Epoch475 layer4 out_loss 0.15786369144916534, R2 0.07703101634979248\n",
      "Train 476 | out_loss 0.39543601870536804: 100%|█| 125/125 [00:00<00:00, 256.04it\n",
      "Train Epoch476 out_loss 0.15636959671974182, R2 0.08274710178375244\n",
      "Test Epoch476 layer0 out_loss 0.15972355008125305, R2 0.06615710258483887\n",
      "Test Epoch476 layer1 out_loss 0.1587618887424469, R2 0.07177960872650146\n",
      "Test Epoch476 layer2 out_loss 0.15777559578418732, R2 0.0775461196899414\n",
      "Test Epoch476 layer3 out_loss 0.1580229103565216, R2 0.0761001706123352\n",
      "Test Epoch476 layer4 out_loss 0.15796218812465668, R2 0.07645517587661743\n",
      "Train 477 | out_loss 0.39544612169265747: 100%|█| 125/125 [00:00<00:00, 261.69it\n",
      "Train Epoch477 out_loss 0.15637768805027008, R2 0.082699716091156\n",
      "Test Epoch477 layer0 out_loss 0.15970680117607117, R2 0.06625503301620483\n",
      "Test Epoch477 layer1 out_loss 0.15866713225841522, R2 0.07233363389968872\n",
      "Test Epoch477 layer2 out_loss 0.15766572952270508, R2 0.07818847894668579\n",
      "Test Epoch477 layer3 out_loss 0.1576947122812271, R2 0.07801896333694458\n",
      "Test Epoch477 layer4 out_loss 0.15770532190799713, R2 0.07795697450637817\n",
      "Train 478 | out_loss 0.3955020606517792: 100%|█| 125/125 [00:00<00:00, 259.48it/\n",
      "Train Epoch478 out_loss 0.15642182528972626, R2 0.08244073390960693\n",
      "Test Epoch478 layer0 out_loss 0.1597854495048523, R2 0.06579524278640747\n",
      "Test Epoch478 layer1 out_loss 0.16004842519760132, R2 0.0642576813697815\n",
      "Test Epoch478 layer2 out_loss 0.15847574174404144, R2 0.07345259189605713\n",
      "Test Epoch478 layer3 out_loss 0.15892507135868073, R2 0.07082557678222656\n",
      "Test Epoch478 layer4 out_loss 0.1590513288974762, R2 0.07008737325668335\n",
      "Train 479 | out_loss 0.39566510915756226: 100%|█| 125/125 [00:00<00:00, 261.76it\n",
      "Train Epoch479 out_loss 0.15655088424682617, R2 0.0816836953163147\n",
      "Test Epoch479 layer0 out_loss 0.15970541536808014, R2 0.06626319885253906\n",
      "Test Epoch479 layer1 out_loss 0.15867756307125092, R2 0.07227259874343872\n",
      "Test Epoch479 layer2 out_loss 0.15765045583248138, R2 0.07827776670455933\n",
      "Test Epoch479 layer3 out_loss 0.15765808522701263, R2 0.07823318243026733\n",
      "Test Epoch479 layer4 out_loss 0.15770459175109863, R2 0.07796120643615723\n",
      "Train 480 | out_loss 0.3956798315048218: 100%|█| 125/125 [00:00<00:00, 261.00it/\n",
      "Train Epoch480 out_loss 0.15656252205371857, R2 0.08161550760269165\n",
      "Test Epoch480 layer0 out_loss 0.15969416499137878, R2 0.06632894277572632\n",
      "Test Epoch480 layer1 out_loss 0.1589907854795456, R2 0.0704413652420044\n",
      "Test Epoch480 layer2 out_loss 0.15784408152103424, R2 0.07714563608169556\n",
      "Test Epoch480 layer3 out_loss 0.1576804220676422, R2 0.07810252904891968\n",
      "Test Epoch480 layer4 out_loss 0.15775927901268005, R2 0.07764148712158203\n",
      "Train 481 | out_loss 0.3953759968280792: 100%|█| 125/125 [00:00<00:00, 258.20it/\n",
      "Train Epoch481 out_loss 0.156322181224823, R2 0.08302533626556396\n",
      "Test Epoch481 layer0 out_loss 0.1597570925951004, R2 0.06596100330352783\n",
      "Test Epoch481 layer1 out_loss 0.1591409295797348, R2 0.06956350803375244\n",
      "Test Epoch481 layer2 out_loss 0.1579379290342331, R2 0.07659697532653809\n",
      "Test Epoch481 layer3 out_loss 0.15797947347164154, R2 0.07635408639907837\n",
      "Test Epoch481 layer4 out_loss 0.15803776681423187, R2 0.07601326704025269\n",
      "Train 482 | out_loss 0.3956570625305176: 100%|█| 125/125 [00:00<00:00, 260.73it/\n",
      "Train Epoch482 out_loss 0.1565445214509964, R2 0.08172100782394409\n",
      "Test Epoch482 layer0 out_loss 0.15973922610282898, R2 0.06606549024581909\n",
      "Test Epoch482 layer1 out_loss 0.15896862745285034, R2 0.07057088613510132\n",
      "Test Epoch482 layer2 out_loss 0.1580023318529129, R2 0.07622045278549194\n",
      "Test Epoch482 layer3 out_loss 0.1579160839319229, R2 0.07672476768493652\n",
      "Test Epoch482 layer4 out_loss 0.15802417695522308, R2 0.07609277963638306\n",
      "Train 483 | out_loss 0.39542046189308167: 100%|█| 125/125 [00:00<00:00, 248.96it\n",
      "Train Epoch483 out_loss 0.15635734796524048, R2 0.08281898498535156\n",
      "Test Epoch483 layer0 out_loss 0.15970905125141144, R2 0.06624191999435425\n",
      "Test Epoch483 layer1 out_loss 0.15892714262008667, R2 0.07081347703933716\n",
      "Test Epoch483 layer2 out_loss 0.15790149569511414, R2 0.07681006193161011\n",
      "Test Epoch483 layer3 out_loss 0.15785305202007294, R2 0.07709318399429321\n",
      "Test Epoch483 layer4 out_loss 0.1579279750585556, R2 0.07665514945983887\n",
      "Train 484 | out_loss 0.3955148458480835: 100%|█| 125/125 [00:00<00:00, 258.47it/\n",
      "Train Epoch484 out_loss 0.15643197298049927, R2 0.0823812484741211\n",
      "Test Epoch484 layer0 out_loss 0.1597009003162384, R2 0.06628960371017456\n",
      "Test Epoch484 layer1 out_loss 0.15920650959014893, R2 0.06918007135391235\n",
      "Test Epoch484 layer2 out_loss 0.15827059745788574, R2 0.07465201616287231\n",
      "Test Epoch484 layer3 out_loss 0.15863610804080963, R2 0.07251501083374023\n",
      "Test Epoch484 layer4 out_loss 0.15847066044807434, R2 0.07348233461380005\n",
      "Train 485 | out_loss 0.3956039547920227: 100%|█| 125/125 [00:00<00:00, 250.73it/\n",
      "Train Epoch485 out_loss 0.15650247037410736, R2 0.08196771144866943\n",
      "Test Epoch485 layer0 out_loss 0.15973074734210968, R2 0.06611508131027222\n",
      "Test Epoch485 layer1 out_loss 0.1587066948413849, R2 0.07210230827331543\n",
      "Test Epoch485 layer2 out_loss 0.1578238159418106, R2 0.07726413011550903\n",
      "Test Epoch485 layer3 out_loss 0.15792034566402435, R2 0.07669979333877563\n",
      "Test Epoch485 layer4 out_loss 0.15797676146030426, R2 0.07636994123458862\n",
      "Train 486 | out_loss 0.3953411281108856: 100%|█| 125/125 [00:00<00:00, 250.35it/\n",
      "Train Epoch486 out_loss 0.15629461407661438, R2 0.08318698406219482\n",
      "Test Epoch486 layer0 out_loss 0.1597009301185608, R2 0.06628942489624023\n",
      "Test Epoch486 layer1 out_loss 0.15890292823314667, R2 0.07095503807067871\n",
      "Test Epoch486 layer2 out_loss 0.15794414281845093, R2 0.07656067609786987\n",
      "Test Epoch486 layer3 out_loss 0.1580265611410141, R2 0.07607877254486084\n",
      "Test Epoch486 layer4 out_loss 0.1581214964389801, R2 0.07552379369735718\n",
      "Train 487 | out_loss 0.3952169120311737: 100%|█| 125/125 [00:00<00:00, 260.41it/\n",
      "Train Epoch487 out_loss 0.15619643032550812, R2 0.08376294374465942\n",
      "Test Epoch487 layer0 out_loss 0.1601083129644394, R2 0.06390762329101562\n",
      "Test Epoch487 layer1 out_loss 0.15940585732460022, R2 0.06801462173461914\n",
      "Test Epoch487 layer2 out_loss 0.15848737955093384, R2 0.07338452339172363\n",
      "Test Epoch487 layer3 out_loss 0.15836411714553833, R2 0.07410520315170288\n",
      "Test Epoch487 layer4 out_loss 0.15856024622917175, R2 0.07295858860015869\n",
      "Train 488 | out_loss 0.395441472530365: 100%|█| 125/125 [00:00<00:00, 259.26it/s\n",
      "Train Epoch488 out_loss 0.15637394785881042, R2 0.08272159099578857\n",
      "Test Epoch488 layer0 out_loss 0.15977218747138977, R2 0.06587278842926025\n",
      "Test Epoch488 layer1 out_loss 0.15873920917510986, R2 0.07191216945648193\n",
      "Test Epoch488 layer2 out_loss 0.157810240983963, R2 0.07734352350234985\n",
      "Test Epoch488 layer3 out_loss 0.1580788940191269, R2 0.07577288150787354\n",
      "Test Epoch488 layer4 out_loss 0.15786747634410858, R2 0.07700890302658081\n",
      "Train 489 | out_loss 0.3955705463886261: 100%|█| 125/125 [00:00<00:00, 265.03it/\n",
      "Train Epoch489 out_loss 0.15647606551647186, R2 0.08212262392044067\n",
      "Test Epoch489 layer0 out_loss 0.15981855988502502, R2 0.065601646900177\n",
      "Test Epoch489 layer1 out_loss 0.1592562049627304, R2 0.06888949871063232\n",
      "Test Epoch489 layer2 out_loss 0.15823373198509216, R2 0.07486754655838013\n",
      "Test Epoch489 layer3 out_loss 0.15837536752223969, R2 0.07403945922851562\n",
      "Test Epoch489 layer4 out_loss 0.15838304162025452, R2 0.07399457693099976\n",
      "Train 490 | out_loss 0.39545294642448425: 100%|█| 125/125 [00:00<00:00, 248.63it\n",
      "Train Epoch490 out_loss 0.15638300776481628, R2 0.08266842365264893\n",
      "Test Epoch490 layer0 out_loss 0.15969780087471008, R2 0.06630772352218628\n",
      "Test Epoch490 layer1 out_loss 0.1590389758348465, R2 0.07015961408615112\n",
      "Test Epoch490 layer2 out_loss 0.15785367786884308, R2 0.07708954811096191\n",
      "Test Epoch490 layer3 out_loss 0.15785448253154755, R2 0.07708489894866943\n",
      "Test Epoch490 layer4 out_loss 0.15801620483398438, R2 0.07613933086395264\n",
      "Train 491 | out_loss 0.39555296301841736: 100%|█| 125/125 [00:00<00:00, 261.06it\n",
      "Train Epoch491 out_loss 0.1564621478319168, R2 0.08220428228378296\n",
      "Test Epoch491 layer0 out_loss 0.15968404710292816, R2 0.0663880705833435\n",
      "Test Epoch491 layer1 out_loss 0.15861281752586365, R2 0.07265114784240723\n",
      "Test Epoch491 layer2 out_loss 0.15757741034030914, R2 0.078704833984375\n",
      "Test Epoch491 layer3 out_loss 0.1575734168291092, R2 0.07872819900512695\n",
      "Test Epoch491 layer4 out_loss 0.15770474076271057, R2 0.0779603123664856\n",
      "Train 492 | out_loss 0.3956142067909241: 100%|█| 125/125 [00:00<00:00, 253.90it/\n",
      "Train Epoch492 out_loss 0.15651057660579681, R2 0.08192014694213867\n",
      "Test Epoch492 layer0 out_loss 0.15966632962226868, R2 0.06649166345596313\n",
      "Test Epoch492 layer1 out_loss 0.1588653028011322, R2 0.07117503881454468\n",
      "Test Epoch492 layer2 out_loss 0.15791374444961548, R2 0.07673841714859009\n",
      "Test Epoch492 layer3 out_loss 0.1583758443593979, R2 0.07403671741485596\n",
      "Test Epoch492 layer4 out_loss 0.158307746052742, R2 0.07443487644195557\n",
      "Train 493 | out_loss 0.39548259973526: 100%|█| 125/125 [00:00<00:00, 253.00it/s]\n",
      "Train Epoch493 out_loss 0.1564064472913742, R2 0.08253097534179688\n",
      "Test Epoch493 layer0 out_loss 0.15965458750724792, R2 0.06656032800674438\n",
      "Test Epoch493 layer1 out_loss 0.1586265116930008, R2 0.07257109880447388\n",
      "Test Epoch493 layer2 out_loss 0.15760035812854767, R2 0.0785706639289856\n",
      "Test Epoch493 layer3 out_loss 0.15758346021175385, R2 0.07866942882537842\n",
      "Test Epoch493 layer4 out_loss 0.15766772627830505, R2 0.07817679643630981\n",
      "Train 494 | out_loss 0.3953654170036316: 100%|█| 125/125 [00:00<00:00, 253.58it/\n",
      "Train Epoch494 out_loss 0.15631386637687683, R2 0.08307403326034546\n",
      "Test Epoch494 layer0 out_loss 0.15970240533351898, R2 0.06628072261810303\n",
      "Test Epoch494 layer1 out_loss 0.15881097316741943, R2 0.07149261236190796\n",
      "Test Epoch494 layer2 out_loss 0.15776076912879944, R2 0.07763272523880005\n",
      "Test Epoch494 layer3 out_loss 0.15771853923797607, R2 0.07787966728210449\n",
      "Test Epoch494 layer4 out_loss 0.15783634781837463, R2 0.07719087600708008\n",
      "Train 495 | out_loss 0.39522039890289307: 100%|█| 125/125 [00:00<00:00, 256.00it\n",
      "Train Epoch495 out_loss 0.1561991423368454, R2 0.0837470293045044\n",
      "Test Epoch495 layer0 out_loss 0.15972064435482025, R2 0.06617414951324463\n",
      "Test Epoch495 layer1 out_loss 0.15867291390895844, R2 0.07229983806610107\n",
      "Test Epoch495 layer2 out_loss 0.15762180089950562, R2 0.07844531536102295\n",
      "Test Epoch495 layer3 out_loss 0.15758219361305237, R2 0.07867681980133057\n",
      "Test Epoch495 layer4 out_loss 0.15768542885780334, R2 0.07807326316833496\n",
      "Train 496 | out_loss 0.39545759558677673: 100%|█| 125/125 [00:00<00:00, 257.98it\n",
      "Train Epoch496 out_loss 0.15638671815395355, R2 0.08264672756195068\n",
      "Test Epoch496 layer0 out_loss 0.1596498042345047, R2 0.06658828258514404\n",
      "Test Epoch496 layer1 out_loss 0.15864282846450806, R2 0.07247573137283325\n",
      "Test Epoch496 layer2 out_loss 0.15758739411830902, R2 0.07864648103713989\n",
      "Test Epoch496 layer3 out_loss 0.1575758457183838, R2 0.07871389389038086\n",
      "Test Epoch496 layer4 out_loss 0.15766406059265137, R2 0.07819819450378418\n",
      "Train 497 | out_loss 0.3952736556529999: 100%|█| 125/125 [00:00<00:00, 256.06it/\n",
      "Train Epoch497 out_loss 0.15624119341373444, R2 0.08350032567977905\n",
      "Test Epoch497 layer0 out_loss 0.15965546667575836, R2 0.06655514240264893\n",
      "Test Epoch497 layer1 out_loss 0.15860770642757416, R2 0.0726810097694397\n",
      "Test Epoch497 layer2 out_loss 0.15763138234615326, R2 0.07838928699493408\n",
      "Test Epoch497 layer3 out_loss 0.15755586326122284, R2 0.0788307785987854\n",
      "Test Epoch497 layer4 out_loss 0.15761996805667877, R2 0.07845598459243774\n",
      "Train 498 | out_loss 0.39535459876060486: 100%|█| 125/125 [00:00<00:00, 256.83it\n",
      "Train Epoch498 out_loss 0.1563052535057068, R2 0.08312457799911499\n",
      "Test Epoch498 layer0 out_loss 0.1596607267856598, R2 0.06652450561523438\n",
      "Test Epoch498 layer1 out_loss 0.15865139663219452, R2 0.07242560386657715\n",
      "Test Epoch498 layer2 out_loss 0.157635897397995, R2 0.07836288213729858\n",
      "Test Epoch498 layer3 out_loss 0.1576063185930252, R2 0.07853573560714722\n",
      "Test Epoch498 layer4 out_loss 0.15769517421722412, R2 0.07801628112792969\n",
      "Train 499 | out_loss 0.3956441283226013: 100%|█| 125/125 [00:00<00:00, 261.84it/\n",
      "Train Epoch499 out_loss 0.15653425455093384, R2 0.08178126811981201\n",
      "Test Epoch499 layer0 out_loss 0.15966983139514923, R2 0.06647121906280518\n",
      "Test Epoch499 layer1 out_loss 0.15859727561473846, R2 0.0727420449256897\n",
      "Test Epoch499 layer2 out_loss 0.15761712193489075, R2 0.07847261428833008\n",
      "Test Epoch499 layer3 out_loss 0.1575470268726349, R2 0.07888245582580566\n",
      "Test Epoch499 layer4 out_loss 0.15758901834487915, R2 0.07863694429397583\n",
      "Train 500 | out_loss 0.3956552743911743: 100%|█| 125/125 [00:00<00:00, 238.07it/\n",
      "Train Epoch500 out_loss 0.15654301643371582, R2 0.08172988891601562\n",
      "Test Epoch500 layer0 out_loss 0.15963365137577057, R2 0.06668275594711304\n",
      "Test Epoch500 layer1 out_loss 0.15862947702407837, R2 0.07255381345748901\n",
      "Test Epoch500 layer2 out_loss 0.15760132670402527, R2 0.07856494188308716\n",
      "Test Epoch500 layer3 out_loss 0.15756919980049133, R2 0.07875281572341919\n",
      "Test Epoch500 layer4 out_loss 0.15766315162181854, R2 0.07820355892181396\n",
      "Train 501 | out_loss 0.39548787474632263: 100%|█| 125/125 [00:00<00:00, 262.22it\n",
      "Train Epoch501 out_loss 0.15641064941883087, R2 0.08250635862350464\n",
      "Test Epoch501 layer0 out_loss 0.15963919460773468, R2 0.06665033102035522\n",
      "Test Epoch501 layer1 out_loss 0.15869662165641785, R2 0.07216119766235352\n",
      "Test Epoch501 layer2 out_loss 0.15767395496368408, R2 0.07814037799835205\n",
      "Test Epoch501 layer3 out_loss 0.1577473133802414, R2 0.07771146297454834\n",
      "Test Epoch501 layer4 out_loss 0.15777529776096344, R2 0.07754784822463989\n",
      "Train 502 | out_loss 0.3953998386859894: 100%|█| 125/125 [00:00<00:00, 260.97it/\n",
      "Train Epoch502 out_loss 0.15634100139141083, R2 0.08291488885879517\n",
      "Test Epoch502 layer0 out_loss 0.15962614119052887, R2 0.06672662496566772\n",
      "Test Epoch502 layer1 out_loss 0.15857696533203125, R2 0.07286077737808228\n",
      "Test Epoch502 layer2 out_loss 0.15757407248020172, R2 0.07872438430786133\n",
      "Test Epoch502 layer3 out_loss 0.15750862658023834, R2 0.07910698652267456\n",
      "Test Epoch502 layer4 out_loss 0.1575736403465271, R2 0.07872682809829712\n",
      "Train 503 | out_loss 0.39543861150741577: 100%|█| 125/125 [00:00<00:00, 257.48it\n",
      "Train Epoch503 out_loss 0.15637169778347015, R2 0.08273476362228394\n",
      "Test Epoch503 layer0 out_loss 0.15964490175247192, R2 0.06661701202392578\n",
      "Test Epoch503 layer1 out_loss 0.1591162532567978, R2 0.06970775127410889\n",
      "Test Epoch503 layer2 out_loss 0.15798209607601166, R2 0.07633870840072632\n",
      "Test Epoch503 layer3 out_loss 0.15790483355522156, R2 0.07679051160812378\n",
      "Test Epoch503 layer4 out_loss 0.15795965492725372, R2 0.07646995782852173\n",
      "Train 504 | out_loss 0.39542320370674133: 100%|█| 125/125 [00:00<00:00, 255.41it\n",
      "Train Epoch504 out_loss 0.15635952353477478, R2 0.08280622959136963\n",
      "Test Epoch504 layer0 out_loss 0.15973879396915436, R2 0.06606799364089966\n",
      "Test Epoch504 layer1 out_loss 0.15901175141334534, R2 0.07031875848770142\n",
      "Test Epoch504 layer2 out_loss 0.15796564519405365, R2 0.0764349102973938\n",
      "Test Epoch504 layer3 out_loss 0.15789683163166046, R2 0.07683724164962769\n",
      "Test Epoch504 layer4 out_loss 0.158003568649292, R2 0.07621318101882935\n",
      "Train 505 | out_loss 0.395510196685791: 100%|█| 125/125 [00:00<00:00, 248.53it/s\n",
      "Train Epoch505 out_loss 0.15642830729484558, R2 0.08240270614624023\n",
      "Test Epoch505 layer0 out_loss 0.1596328616142273, R2 0.06668740510940552\n",
      "Test Epoch505 layer1 out_loss 0.15863364934921265, R2 0.0725293755531311\n",
      "Test Epoch505 layer2 out_loss 0.15759959816932678, R2 0.07857507467269897\n",
      "Test Epoch505 layer3 out_loss 0.15763288736343384, R2 0.07838046550750732\n",
      "Test Epoch505 layer4 out_loss 0.15772995352745056, R2 0.07781296968460083\n",
      "Train 506 | out_loss 0.3952253758907318: 100%|█| 125/125 [00:00<00:00, 264.42it/\n",
      "Train Epoch506 out_loss 0.15620309114456177, R2 0.08372384309768677\n",
      "Test Epoch506 layer0 out_loss 0.15960873663425446, R2 0.06682842969894409\n",
      "Test Epoch506 layer1 out_loss 0.15870104730129242, R2 0.072135329246521\n",
      "Test Epoch506 layer2 out_loss 0.15761902928352356, R2 0.07846146821975708\n",
      "Test Epoch506 layer3 out_loss 0.1575946807861328, R2 0.07860380411148071\n",
      "Test Epoch506 layer4 out_loss 0.15768933296203613, R2 0.07805043458938599\n",
      "Train 507 | out_loss 0.3953711688518524: 100%|█| 125/125 [00:00<00:00, 245.84it/\n",
      "Train Epoch507 out_loss 0.15631839632987976, R2 0.08304750919342041\n",
      "Test Epoch507 layer0 out_loss 0.1597750186920166, R2 0.0658562183380127\n",
      "Test Epoch507 layer1 out_loss 0.1588769257068634, R2 0.07110702991485596\n",
      "Test Epoch507 layer2 out_loss 0.15785610675811768, R2 0.07707536220550537\n",
      "Test Epoch507 layer3 out_loss 0.15783394873142242, R2 0.0772048830986023\n",
      "Test Epoch507 layer4 out_loss 0.15797899663448334, R2 0.07635682821273804\n",
      "Train 508 | out_loss 0.39537912607192993: 100%|█| 125/125 [00:00<00:00, 262.00it\n",
      "Train Epoch508 out_loss 0.1563245952129364, R2 0.08301109075546265\n",
      "Test Epoch508 layer0 out_loss 0.15959522128105164, R2 0.06690746545791626\n",
      "Test Epoch508 layer1 out_loss 0.15861697494983673, R2 0.07262688875198364\n",
      "Test Epoch508 layer2 out_loss 0.15755274891853333, R2 0.0788489580154419\n",
      "Test Epoch508 layer3 out_loss 0.15748576819896698, R2 0.07924062013626099\n",
      "Test Epoch508 layer4 out_loss 0.15759703516960144, R2 0.07859009504318237\n",
      "Train 509 | out_loss 0.3952644169330597: 100%|█| 125/125 [00:00<00:00, 259.69it/\n",
      "Train Epoch509 out_loss 0.15623393654823303, R2 0.08354294300079346\n",
      "Test Epoch509 layer0 out_loss 0.15960918366909027, R2 0.06682586669921875\n",
      "Test Epoch509 layer1 out_loss 0.15895754098892212, R2 0.07063573598861694\n",
      "Test Epoch509 layer2 out_loss 0.1577216237783432, R2 0.07786166667938232\n",
      "Test Epoch509 layer3 out_loss 0.15763631463050842, R2 0.07836037874221802\n",
      "Test Epoch509 layer4 out_loss 0.15774306654930115, R2 0.0777362585067749\n",
      "Train 510 | out_loss 0.39549314975738525: 100%|█| 125/125 [00:00<00:00, 256.53it\n",
      "Train Epoch510 out_loss 0.15641476213932037, R2 0.0824822187423706\n",
      "Test Epoch510 layer0 out_loss 0.15962524712085724, R2 0.06673187017440796\n",
      "Test Epoch510 layer1 out_loss 0.1586408019065857, R2 0.07248753309249878\n",
      "Test Epoch510 layer2 out_loss 0.15762567520141602, R2 0.0784226655960083\n",
      "Test Epoch510 layer3 out_loss 0.15754970908164978, R2 0.07886672019958496\n",
      "Test Epoch510 layer4 out_loss 0.1576474905014038, R2 0.07829505205154419\n",
      "Train 511 | out_loss 0.395495742559433: 100%|█| 125/125 [00:00<00:00, 256.61it/s\n",
      "Train Epoch511 out_loss 0.1564168781042099, R2 0.08246976137161255\n",
      "Test Epoch511 layer0 out_loss 0.15959037840366364, R2 0.06693577766418457\n",
      "Test Epoch511 layer1 out_loss 0.15854968130588531, R2 0.07302027940750122\n",
      "Test Epoch511 layer2 out_loss 0.15756887197494507, R2 0.07875466346740723\n",
      "Test Epoch511 layer3 out_loss 0.15744133293628693, R2 0.07950043678283691\n",
      "Test Epoch511 layer4 out_loss 0.15752454102039337, R2 0.07901394367218018\n",
      "Train 512 | out_loss 0.39520955085754395: 100%|█| 125/125 [00:00<00:00, 265.24it\n",
      "Train Epoch512 out_loss 0.15619060397148132, R2 0.0837971568107605\n",
      "Test Epoch512 layer0 out_loss 0.15958811342716217, R2 0.06694895029067993\n",
      "Test Epoch512 layer1 out_loss 0.15856727957725525, R2 0.07291746139526367\n",
      "Test Epoch512 layer2 out_loss 0.15766125917434692, R2 0.07821458578109741\n",
      "Test Epoch512 layer3 out_loss 0.1576383113861084, R2 0.07834875583648682\n",
      "Test Epoch512 layer4 out_loss 0.15767072141170502, R2 0.07815927267074585\n",
      "Train 513 | out_loss 0.3955787420272827: 100%|█| 125/125 [00:00<00:00, 252.31it/\n",
      "Train Epoch513 out_loss 0.15648253262043, R2 0.08208471536636353\n",
      "Test Epoch513 layer0 out_loss 0.15960924327373505, R2 0.0668255090713501\n",
      "Test Epoch513 layer1 out_loss 0.15879519283771515, R2 0.07158488035202026\n",
      "Test Epoch513 layer2 out_loss 0.15773512423038483, R2 0.07778269052505493\n",
      "Test Epoch513 layer3 out_loss 0.15768659114837646, R2 0.07806652784347534\n",
      "Test Epoch513 layer4 out_loss 0.15772514045238495, R2 0.07784104347229004\n",
      "Train 514 | out_loss 0.39532700181007385: 100%|█| 125/125 [00:00<00:00, 264.58it\n",
      "Train Epoch514 out_loss 0.1562834233045578, R2 0.0832526683807373\n",
      "Test Epoch514 layer0 out_loss 0.1596013754606247, R2 0.06687140464782715\n",
      "Test Epoch514 layer1 out_loss 0.15872998535633087, R2 0.07196617126464844\n",
      "Test Epoch514 layer2 out_loss 0.15780813992023468, R2 0.07735586166381836\n",
      "Test Epoch514 layer3 out_loss 0.15798687934875488, R2 0.07631081342697144\n",
      "Test Epoch514 layer4 out_loss 0.1581263244152069, R2 0.07549548149108887\n",
      "Train 515 | out_loss 0.39510905742645264: 100%|█| 125/125 [00:00<00:00, 259.00it\n",
      "Train Epoch515 out_loss 0.15611116588115692, R2 0.08426308631896973\n",
      "Test Epoch515 layer0 out_loss 0.15968933701515198, R2 0.06635719537734985\n",
      "Test Epoch515 layer1 out_loss 0.15881600975990295, R2 0.07146316766738892\n",
      "Test Epoch515 layer2 out_loss 0.15774263441562653, R2 0.07773882150650024\n",
      "Test Epoch515 layer3 out_loss 0.1576930284500122, R2 0.0780288577079773\n",
      "Test Epoch515 layer4 out_loss 0.1577528417110443, R2 0.07767915725708008\n",
      "Train 516 | out_loss 0.39537566900253296: 100%|█| 125/125 [00:00<00:00, 262.82it\n",
      "Train Epoch516 out_loss 0.15632197260856628, R2 0.0830264687538147\n",
      "Test Epoch516 layer0 out_loss 0.15966279804706573, R2 0.0665123462677002\n",
      "Test Epoch516 layer1 out_loss 0.15855471789836884, R2 0.07299083471298218\n",
      "Test Epoch516 layer2 out_loss 0.1575874239206314, R2 0.07864630222320557\n",
      "Test Epoch516 layer3 out_loss 0.1576465517282486, R2 0.0783005952835083\n",
      "Test Epoch516 layer4 out_loss 0.157610222697258, R2 0.07851290702819824\n",
      "Train 517 | out_loss 0.3954406976699829: 100%|█| 125/125 [00:00<00:00, 246.72it/\n",
      "Train Epoch517 out_loss 0.15637333691120148, R2 0.08272522687911987\n",
      "Test Epoch517 layer0 out_loss 0.1595674604177475, R2 0.06706976890563965\n",
      "Test Epoch517 layer1 out_loss 0.1588434875011444, R2 0.07130253314971924\n",
      "Test Epoch517 layer2 out_loss 0.1575760841369629, R2 0.0787125825881958\n",
      "Test Epoch517 layer3 out_loss 0.15747053921222687, R2 0.07932966947555542\n",
      "Test Epoch517 layer4 out_loss 0.157521590590477, R2 0.07903116941452026\n",
      "Train 518 | out_loss 0.3955900967121124: 100%|█| 125/125 [00:00<00:00, 257.75it/\n",
      "Train Epoch518 out_loss 0.15649151802062988, R2 0.0820319652557373\n",
      "Test Epoch518 layer0 out_loss 0.15965768694877625, R2 0.06654220819473267\n",
      "Test Epoch518 layer1 out_loss 0.1587640941143036, R2 0.0717666745185852\n",
      "Test Epoch518 layer2 out_loss 0.15786200761795044, R2 0.07704085111618042\n",
      "Test Epoch518 layer3 out_loss 0.15795090794563293, R2 0.07652109861373901\n",
      "Test Epoch518 layer4 out_loss 0.1579529047012329, R2 0.07650941610336304\n",
      "Train 519 | out_loss 0.3951764404773712: 100%|█| 125/125 [00:00<00:00, 255.67it/\n",
      "Train Epoch519 out_loss 0.15616439282894135, R2 0.08395081758499146\n",
      "Test Epoch519 layer0 out_loss 0.15956419706344604, R2 0.06708884239196777\n",
      "Test Epoch519 layer1 out_loss 0.1587478071451187, R2 0.07186198234558105\n",
      "Test Epoch519 layer2 out_loss 0.1576002985239029, R2 0.07857102155685425\n",
      "Test Epoch519 layer3 out_loss 0.1577281504869461, R2 0.07782351970672607\n",
      "Test Epoch519 layer4 out_loss 0.1577266901731491, R2 0.07783204317092896\n",
      "Train 520 | out_loss 0.3953610360622406: 100%|█| 125/125 [00:00<00:00, 229.54it/\n",
      "Train Epoch520 out_loss 0.1563103348016739, R2 0.0830947756767273\n",
      "Test Epoch520 layer0 out_loss 0.15954744815826416, R2 0.06718671321868896\n",
      "Test Epoch520 layer1 out_loss 0.1585504710674286, R2 0.07301563024520874\n",
      "Test Epoch520 layer2 out_loss 0.15752087533473969, R2 0.07903534173965454\n",
      "Test Epoch520 layer3 out_loss 0.15745064616203308, R2 0.07944589853286743\n",
      "Test Epoch520 layer4 out_loss 0.1575242429971695, R2 0.07901561260223389\n",
      "Train 521 | out_loss 0.3952494263648987: 100%|█| 125/125 [00:00<00:00, 252.83it/\n",
      "Train Epoch521 out_loss 0.15622210502624512, R2 0.08361232280731201\n",
      "Test Epoch521 layer0 out_loss 0.15968284010887146, R2 0.066395103931427\n",
      "Test Epoch521 layer1 out_loss 0.15853644907474518, R2 0.07309764623641968\n",
      "Test Epoch521 layer2 out_loss 0.15755686163902283, R2 0.07882499694824219\n",
      "Test Epoch521 layer3 out_loss 0.15750686824321747, R2 0.07911723852157593\n",
      "Test Epoch521 layer4 out_loss 0.15757392346858978, R2 0.07872515916824341\n",
      "Train 522 | out_loss 0.395409494638443: 100%|█| 125/125 [00:00<00:00, 248.38it/s\n",
      "Train Epoch522 out_loss 0.15634869039058685, R2 0.0828697681427002\n",
      "Test Epoch522 layer0 out_loss 0.15957532823085785, R2 0.06702369451522827\n",
      "Test Epoch522 layer1 out_loss 0.1589307188987732, R2 0.07079249620437622\n",
      "Test Epoch522 layer2 out_loss 0.15781061351299286, R2 0.07734137773513794\n",
      "Test Epoch522 layer3 out_loss 0.1579950749874115, R2 0.07626289129257202\n",
      "Test Epoch522 layer4 out_loss 0.15795652568340302, R2 0.07648825645446777\n",
      "Train 523 | out_loss 0.3952564001083374: 100%|█| 125/125 [00:00<00:00, 265.19it/\n",
      "Train Epoch523 out_loss 0.15622760355472565, R2 0.08358007669448853\n",
      "Test Epoch523 layer0 out_loss 0.159547358751297, R2 0.06718724966049194\n",
      "Test Epoch523 layer1 out_loss 0.15858404338359833, R2 0.07281941175460815\n",
      "Test Epoch523 layer2 out_loss 0.1575205773115158, R2 0.07903707027435303\n",
      "Test Epoch523 layer3 out_loss 0.15747827291488647, R2 0.0792844295501709\n",
      "Test Epoch523 layer4 out_loss 0.15749840438365936, R2 0.07916676998138428\n",
      "Train 524 | out_loss 0.3950824737548828: 100%|█| 125/125 [00:00<00:00, 257.83it/\n",
      "Train Epoch524 out_loss 0.1560901403427124, R2 0.08438646793365479\n",
      "Test Epoch524 layer0 out_loss 0.15958422422409058, R2 0.06697171926498413\n",
      "Test Epoch524 layer1 out_loss 0.15849550068378448, R2 0.07333707809448242\n",
      "Test Epoch524 layer2 out_loss 0.15767037868499756, R2 0.07816123962402344\n",
      "Test Epoch524 layer3 out_loss 0.15774013102054596, R2 0.07775348424911499\n",
      "Test Epoch524 layer4 out_loss 0.1576356589794159, R2 0.07836419343948364\n",
      "Train 525 | out_loss 0.3952319920063019: 100%|█| 125/125 [00:00<00:00, 245.31it/\n",
      "Train Epoch525 out_loss 0.1562083214521408, R2 0.08369314670562744\n",
      "Test Epoch525 layer0 out_loss 0.1595361977815628, R2 0.067252516746521\n",
      "Test Epoch525 layer1 out_loss 0.1585608571767807, R2 0.07295501232147217\n",
      "Test Epoch525 layer2 out_loss 0.15756092965602875, R2 0.07880115509033203\n",
      "Test Epoch525 layer3 out_loss 0.15747876465320587, R2 0.07928156852722168\n",
      "Test Epoch525 layer4 out_loss 0.15754151344299316, R2 0.07891464233398438\n",
      "Train 526 | out_loss 0.3952513635158539: 100%|█| 125/125 [00:00<00:00, 239.80it/\n",
      "Train Epoch526 out_loss 0.15622365474700928, R2 0.08360326290130615\n",
      "Test Epoch526 layer0 out_loss 0.15952543914318085, R2 0.06731545925140381\n",
      "Test Epoch526 layer1 out_loss 0.15851067006587982, R2 0.07324838638305664\n",
      "Test Epoch526 layer2 out_loss 0.15755991637706757, R2 0.0788070559501648\n",
      "Test Epoch526 layer3 out_loss 0.15747863054275513, R2 0.07928228378295898\n",
      "Test Epoch526 layer4 out_loss 0.1575000137090683, R2 0.07915729284286499\n",
      "Train 527 | out_loss 0.3951258361339569: 100%|█| 125/125 [00:00<00:00, 250.56it/\n",
      "Train Epoch527 out_loss 0.15612441301345825, R2 0.08418530225753784\n",
      "Test Epoch527 layer0 out_loss 0.15953774750232697, R2 0.06724345684051514\n",
      "Test Epoch527 layer1 out_loss 0.15860562026500702, R2 0.07269322872161865\n",
      "Test Epoch527 layer2 out_loss 0.1575976461172104, R2 0.07858651876449585\n",
      "Test Epoch527 layer3 out_loss 0.15759146213531494, R2 0.07862263917922974\n",
      "Test Epoch527 layer4 out_loss 0.1576128602027893, R2 0.07849758863449097\n",
      "Train 528 | out_loss 0.3953602612018585: 100%|█| 125/125 [00:00<00:00, 249.28it/\n",
      "Train Epoch528 out_loss 0.15630973875522614, R2 0.08309829235076904\n",
      "Test Epoch528 layer0 out_loss 0.15952004492282867, R2 0.06734693050384521\n",
      "Test Epoch528 layer1 out_loss 0.15862581133842468, R2 0.07257521152496338\n",
      "Test Epoch528 layer2 out_loss 0.15756535530090332, R2 0.07877528667449951\n",
      "Test Epoch528 layer3 out_loss 0.1574668437242508, R2 0.07935124635696411\n",
      "Test Epoch528 layer4 out_loss 0.15754254162311554, R2 0.07890868186950684\n",
      "Train 529 | out_loss 0.395065575838089: 100%|█| 125/125 [00:00<00:00, 254.45it/s\n",
      "Train Epoch529 out_loss 0.15607678890228271, R2 0.08446478843688965\n",
      "Test Epoch529 layer0 out_loss 0.15956243872642517, R2 0.06709909439086914\n",
      "Test Epoch529 layer1 out_loss 0.1585334837436676, R2 0.07311505079269409\n",
      "Test Epoch529 layer2 out_loss 0.15753455460071564, R2 0.07895535230636597\n",
      "Test Epoch529 layer3 out_loss 0.15750493109226227, R2 0.07912856340408325\n",
      "Test Epoch529 layer4 out_loss 0.15754809975624084, R2 0.07887619733810425\n",
      "Train 530 | out_loss 0.3951760530471802: 100%|█| 125/125 [00:00<00:00, 251.11it/\n",
      "Train Epoch530 out_loss 0.15616412460803986, R2 0.08395242691040039\n",
      "Test Epoch530 layer0 out_loss 0.1596030443906784, R2 0.06686168909072876\n",
      "Test Epoch530 layer1 out_loss 0.15907804667949677, R2 0.06993114948272705\n",
      "Test Epoch530 layer2 out_loss 0.15798157453536987, R2 0.07634186744689941\n",
      "Test Epoch530 layer3 out_loss 0.1578250676393509, R2 0.07725679874420166\n",
      "Test Epoch530 layer4 out_loss 0.15800204873085022, R2 0.07622212171554565\n",
      "Train 531 | out_loss 0.3950798511505127: 100%|█| 125/125 [00:00<00:00, 261.18it/\n",
      "Train Epoch531 out_loss 0.15608802437782288, R2 0.08439880609512329\n",
      "Test Epoch531 layer0 out_loss 0.15959201753139496, R2 0.06692618131637573\n",
      "Test Epoch531 layer1 out_loss 0.15863583981990814, R2 0.07251662015914917\n",
      "Test Epoch531 layer2 out_loss 0.15765155851840973, R2 0.07827126979827881\n",
      "Test Epoch531 layer3 out_loss 0.15755429863929749, R2 0.07883989810943604\n",
      "Test Epoch531 layer4 out_loss 0.15766461193561554, R2 0.07819497585296631\n",
      "Train 532 | out_loss 0.3951819837093353: 100%|█| 125/125 [00:00<00:00, 259.46it/\n",
      "Train Epoch532 out_loss 0.15616881847381592, R2 0.08392488956451416\n",
      "Test Epoch532 layer0 out_loss 0.15965032577514648, R2 0.0665852427482605\n",
      "Test Epoch532 layer1 out_loss 0.1586609184741974, R2 0.07236993312835693\n",
      "Test Epoch532 layer2 out_loss 0.15758605301380157, R2 0.0786542296409607\n",
      "Test Epoch532 layer3 out_loss 0.157545268535614, R2 0.07889270782470703\n",
      "Test Epoch532 layer4 out_loss 0.15772207081317902, R2 0.07785898447036743\n",
      "Train 533 | out_loss 0.39547327160835266: 100%|█| 125/125 [00:00<00:00, 256.72it\n",
      "Train Epoch533 out_loss 0.15639911592006683, R2 0.08257400989532471\n",
      "Test Epoch533 layer0 out_loss 0.15955126285552979, R2 0.06716442108154297\n",
      "Test Epoch533 layer1 out_loss 0.1584690511226654, R2 0.07349169254302979\n",
      "Test Epoch533 layer2 out_loss 0.15742702782154083, R2 0.07958400249481201\n",
      "Test Epoch533 layer3 out_loss 0.1574973613023758, R2 0.07917284965515137\n",
      "Test Epoch533 layer4 out_loss 0.15769249200820923, R2 0.07803195714950562\n",
      "Train 534 | out_loss 0.39520323276519775: 100%|█| 125/125 [00:00<00:00, 255.93it\n",
      "Train Epoch534 out_loss 0.15618562698364258, R2 0.08382630348205566\n",
      "Test Epoch534 layer0 out_loss 0.15949898958206177, R2 0.0674700140953064\n",
      "Test Epoch534 layer1 out_loss 0.15860110521316528, R2 0.07271963357925415\n",
      "Test Epoch534 layer2 out_loss 0.157621368765831, R2 0.07844781875610352\n",
      "Test Epoch534 layer3 out_loss 0.15753579139709473, R2 0.07894808053970337\n",
      "Test Epoch534 layer4 out_loss 0.15754136443138123, R2 0.078915536403656\n",
      "Train 535 | out_loss 0.3952721953392029: 100%|█| 125/125 [00:00<00:00, 250.04it/\n",
      "Train Epoch535 out_loss 0.1562400460243225, R2 0.08350706100463867\n",
      "Test Epoch535 layer0 out_loss 0.15949071943759918, R2 0.06751841306686401\n",
      "Test Epoch535 layer1 out_loss 0.1586643010377884, R2 0.0723501443862915\n",
      "Test Epoch535 layer2 out_loss 0.15761293470859528, R2 0.07849711179733276\n",
      "Test Epoch535 layer3 out_loss 0.15736426413059235, R2 0.07995104789733887\n",
      "Test Epoch535 layer4 out_loss 0.15741731226444244, R2 0.07964080572128296\n",
      "Train 536 | out_loss 0.3950795829296112: 100%|█| 125/125 [00:00<00:00, 256.09it/\n",
      "Train Epoch536 out_loss 0.15608786046504974, R2 0.0843997597694397\n",
      "Test Epoch536 layer0 out_loss 0.15953709185123444, R2 0.06724727153778076\n",
      "Test Epoch536 layer1 out_loss 0.15847273170948029, R2 0.07347017526626587\n",
      "Test Epoch536 layer2 out_loss 0.1574733853340149, R2 0.07931298017501831\n",
      "Test Epoch536 layer3 out_loss 0.15734775364398956, R2 0.08004754781723022\n",
      "Test Epoch536 layer4 out_loss 0.15741847455501556, R2 0.07963407039642334\n",
      "Train 537 | out_loss 0.3952217400074005: 100%|█| 125/125 [00:00<00:00, 260.80it/\n",
      "Train Epoch537 out_loss 0.15620021522045135, R2 0.08374077081680298\n",
      "Test Epoch537 layer0 out_loss 0.15948486328125, R2 0.06755262613296509\n",
      "Test Epoch537 layer1 out_loss 0.15849807858467102, R2 0.07332199811935425\n",
      "Test Epoch537 layer2 out_loss 0.15752355754375458, R2 0.07901966571807861\n",
      "Test Epoch537 layer3 out_loss 0.15753985941410065, R2 0.07892435789108276\n",
      "Test Epoch537 layer4 out_loss 0.15751463174819946, R2 0.07907181978225708\n",
      "Train 538 | out_loss 0.3952591121196747: 100%|█| 125/125 [00:00<00:00, 249.34it/\n",
      "Train Epoch538 out_loss 0.15622973442077637, R2 0.0835675597190857\n",
      "Test Epoch538 layer0 out_loss 0.1594758778810501, R2 0.06760519742965698\n",
      "Test Epoch538 layer1 out_loss 0.1586521565914154, R2 0.072421133518219\n",
      "Test Epoch538 layer2 out_loss 0.1575472503900528, R2 0.07888108491897583\n",
      "Test Epoch538 layer3 out_loss 0.1574261486530304, R2 0.07958918809890747\n",
      "Test Epoch538 layer4 out_loss 0.15746431052684784, R2 0.07936608791351318\n",
      "Train 539 | out_loss 0.3950521945953369: 100%|█| 125/125 [00:00<00:00, 261.52it/\n",
      "Train Epoch539 out_loss 0.15606628358364105, R2 0.08452636003494263\n",
      "Test Epoch539 layer0 out_loss 0.1594945341348648, R2 0.06749612092971802\n",
      "Test Epoch539 layer1 out_loss 0.15846392512321472, R2 0.0735216736793518\n",
      "Test Epoch539 layer2 out_loss 0.15749530494213104, R2 0.079184889793396\n",
      "Test Epoch539 layer3 out_loss 0.15746773779392242, R2 0.07934600114822388\n",
      "Test Epoch539 layer4 out_loss 0.15750685334205627, R2 0.07911735773086548\n",
      "Train 540 | out_loss 0.39512118697166443: 100%|█| 125/125 [00:00<00:00, 264.10it\n",
      "Train Epoch540 out_loss 0.15612074732780457, R2 0.08420687913894653\n",
      "Test Epoch540 layer0 out_loss 0.1595297008752823, R2 0.0672905445098877\n",
      "Test Epoch540 layer1 out_loss 0.1585025191307068, R2 0.07329607009887695\n",
      "Test Epoch540 layer2 out_loss 0.15759553015232086, R2 0.07859885692596436\n",
      "Test Epoch540 layer3 out_loss 0.15760982036590576, R2 0.07851535081863403\n",
      "Test Epoch540 layer4 out_loss 0.1577116698026657, R2 0.0779198408126831\n",
      "Train 541 | out_loss 0.3952525556087494: 100%|█| 125/125 [00:00<00:00, 260.34it/\n",
      "Train Epoch541 out_loss 0.1562245786190033, R2 0.08359777927398682\n",
      "Test Epoch541 layer0 out_loss 0.15946583449840546, R2 0.06766390800476074\n",
      "Test Epoch541 layer1 out_loss 0.15845783054828644, R2 0.07355737686157227\n",
      "Test Epoch541 layer2 out_loss 0.15749125182628632, R2 0.07920849323272705\n",
      "Test Epoch541 layer3 out_loss 0.15741154551506042, R2 0.07967454195022583\n",
      "Test Epoch541 layer4 out_loss 0.15743789076805115, R2 0.07952046394348145\n",
      "Train 542 | out_loss 0.39503660798072815: 100%|█| 125/125 [00:00<00:00, 258.93it\n",
      "Train Epoch542 out_loss 0.15605393052101135, R2 0.0845988392829895\n",
      "Test Epoch542 layer0 out_loss 0.15947069227695465, R2 0.06763553619384766\n",
      "Test Epoch542 layer1 out_loss 0.15896570682525635, R2 0.07058793306350708\n",
      "Test Epoch542 layer2 out_loss 0.15771017968654633, R2 0.07792860269546509\n",
      "Test Epoch542 layer3 out_loss 0.1575823277235031, R2 0.07867610454559326\n",
      "Test Epoch542 layer4 out_loss 0.15767885744571686, R2 0.07811164855957031\n",
      "Train 543 | out_loss 0.3953382670879364: 100%|█| 125/125 [00:00<00:00, 254.44it/\n",
      "Train Epoch543 out_loss 0.1562923938035965, R2 0.08320003747940063\n",
      "Test Epoch543 layer0 out_loss 0.1595148891210556, R2 0.06737709045410156\n",
      "Test Epoch543 layer1 out_loss 0.15876278281211853, R2 0.07177436351776123\n",
      "Test Epoch543 layer2 out_loss 0.15766410529613495, R2 0.0781978964805603\n",
      "Test Epoch543 layer3 out_loss 0.15765179693698883, R2 0.07826989889144897\n",
      "Test Epoch543 layer4 out_loss 0.15766291320323944, R2 0.07820487022399902\n",
      "Train 544 | out_loss 0.39504215121269226: 100%|█| 125/125 [00:00<00:00, 261.90it\n",
      "Train Epoch544 out_loss 0.15605828166007996, R2 0.08457326889038086\n",
      "Test Epoch544 layer0 out_loss 0.15967613458633423, R2 0.06643438339233398\n",
      "Test Epoch544 layer1 out_loss 0.15869170427322388, R2 0.07218992710113525\n",
      "Test Epoch544 layer2 out_loss 0.1577751189470291, R2 0.07754892110824585\n",
      "Test Epoch544 layer3 out_loss 0.15765537321567535, R2 0.07824891805648804\n",
      "Test Epoch544 layer4 out_loss 0.15784700214862823, R2 0.0771285891532898\n",
      "Train 545 | out_loss 0.394995778799057: 100%|█| 125/125 [00:00<00:00, 256.52it/s\n",
      "Train Epoch545 out_loss 0.15602163970470428, R2 0.0847882628440857\n",
      "Test Epoch545 layer0 out_loss 0.1594615876674652, R2 0.0676887035369873\n",
      "Test Epoch545 layer1 out_loss 0.15864577889442444, R2 0.07245844602584839\n",
      "Test Epoch545 layer2 out_loss 0.15761728584766388, R2 0.07847172021865845\n",
      "Test Epoch545 layer3 out_loss 0.1576816290616989, R2 0.07809549570083618\n",
      "Test Epoch545 layer4 out_loss 0.15766745805740356, R2 0.0781782865524292\n",
      "Train 546 | out_loss 0.395083487033844: 100%|█| 125/125 [00:00<00:00, 248.35it/s\n",
      "Train Epoch546 out_loss 0.15609097480773926, R2 0.08438152074813843\n",
      "Test Epoch546 layer0 out_loss 0.15946398675441742, R2 0.06767469644546509\n",
      "Test Epoch546 layer1 out_loss 0.15845565497875214, R2 0.07357001304626465\n",
      "Test Epoch546 layer2 out_loss 0.15749944746494293, R2 0.07916063070297241\n",
      "Test Epoch546 layer3 out_loss 0.15750037133693695, R2 0.07915526628494263\n",
      "Test Epoch546 layer4 out_loss 0.15750163793563843, R2 0.07914787530899048\n",
      "Train 547 | out_loss 0.3950536549091339: 100%|█| 125/125 [00:00<00:00, 253.86it/\n",
      "Train Epoch547 out_loss 0.156067356467247, R2 0.08452010154724121\n",
      "Test Epoch547 layer0 out_loss 0.15944834053516388, R2 0.06776624917984009\n",
      "Test Epoch547 layer1 out_loss 0.158540740609169, R2 0.07307255268096924\n",
      "Test Epoch547 layer2 out_loss 0.15753720700740814, R2 0.07893991470336914\n",
      "Test Epoch547 layer3 out_loss 0.15753275156021118, R2 0.07896596193313599\n",
      "Test Epoch547 layer4 out_loss 0.15760447084903717, R2 0.07854658365249634\n",
      "Train 548 | out_loss 0.3952106535434723: 100%|█| 125/125 [00:00<00:00, 245.29it/\n",
      "Train Epoch548 out_loss 0.15619146823883057, R2 0.08379203081130981\n",
      "Test Epoch548 layer0 out_loss 0.15947403013706207, R2 0.06761598587036133\n",
      "Test Epoch548 layer1 out_loss 0.15843811631202698, R2 0.0736725926399231\n",
      "Test Epoch548 layer2 out_loss 0.15752486884593964, R2 0.07901197671890259\n",
      "Test Epoch548 layer3 out_loss 0.15752242505550385, R2 0.0790262222290039\n",
      "Test Epoch548 layer4 out_loss 0.15755002200603485, R2 0.0788649320602417\n",
      "Train 549 | out_loss 0.3950159549713135: 100%|█| 125/125 [00:00<00:00, 248.40it/\n",
      "Train Epoch549 out_loss 0.1560375690460205, R2 0.08469480276107788\n",
      "Test Epoch549 layer0 out_loss 0.15945188701152802, R2 0.06774544715881348\n",
      "Test Epoch549 layer1 out_loss 0.15842661261558533, R2 0.07373988628387451\n",
      "Test Epoch549 layer2 out_loss 0.15742099285125732, R2 0.07961934804916382\n",
      "Test Epoch549 layer3 out_loss 0.15738427639007568, R2 0.079833984375\n",
      "Test Epoch549 layer4 out_loss 0.15739241242408752, R2 0.07978641986846924\n",
      "Train 550 | out_loss 0.39515015482902527: 100%|█| 125/125 [00:00<00:00, 252.06it\n",
      "Train Epoch550 out_loss 0.1561436504125595, R2 0.0840725302696228\n",
      "Test Epoch550 layer0 out_loss 0.15945816040039062, R2 0.06770879030227661\n",
      "Test Epoch550 layer1 out_loss 0.1584380865097046, R2 0.07367277145385742\n",
      "Test Epoch550 layer2 out_loss 0.15749521553516388, R2 0.07918542623519897\n",
      "Test Epoch550 layer3 out_loss 0.15735815465450287, R2 0.07998669147491455\n",
      "Test Epoch550 layer4 out_loss 0.1574011594057083, R2 0.07973533868789673\n",
      "Train 551 | out_loss 0.3949471712112427: 100%|█| 125/125 [00:00<00:00, 247.93it/\n",
      "Train Epoch551 out_loss 0.1559833139181137, R2 0.08501309156417847\n",
      "Test Epoch551 layer0 out_loss 0.15946218371391296, R2 0.06768524646759033\n",
      "Test Epoch551 layer1 out_loss 0.15843304991722107, R2 0.07370221614837646\n",
      "Test Epoch551 layer2 out_loss 0.1574917584657669, R2 0.07920557260513306\n",
      "Test Epoch551 layer3 out_loss 0.15748874843120575, R2 0.0792231559753418\n",
      "Test Epoch551 layer4 out_loss 0.1574685424566269, R2 0.0793413519859314\n",
      "Train 552 | out_loss 0.39511752128601074: 100%|█| 125/125 [00:00<00:00, 259.53it\n",
      "Train Epoch552 out_loss 0.15611785650253296, R2 0.08422380685806274\n",
      "Test Epoch552 layer0 out_loss 0.15944358706474304, R2 0.06779396533966064\n",
      "Test Epoch552 layer1 out_loss 0.15841278433799744, R2 0.07382071018218994\n",
      "Test Epoch552 layer2 out_loss 0.15742480754852295, R2 0.07959705591201782\n",
      "Test Epoch552 layer3 out_loss 0.157326340675354, R2 0.08017271757125854\n",
      "Test Epoch552 layer4 out_loss 0.1574069857597351, R2 0.0797012448310852\n",
      "Train 553 | out_loss 0.3950697183609009: 100%|█| 125/125 [00:00<00:00, 247.59it/\n",
      "Train Epoch553 out_loss 0.15608008205890656, R2 0.08444541692733765\n",
      "Test Epoch553 layer0 out_loss 0.15945516526699066, R2 0.0677262544631958\n",
      "Test Epoch553 layer1 out_loss 0.15884079039096832, R2 0.07131832838058472\n",
      "Test Epoch553 layer2 out_loss 0.15778768062591553, R2 0.07747548818588257\n",
      "Test Epoch553 layer3 out_loss 0.15758025646209717, R2 0.07868814468383789\n",
      "Test Epoch553 layer4 out_loss 0.15771661698818207, R2 0.07789093255996704\n",
      "Train 554 | out_loss 0.3952302634716034: 100%|█| 125/125 [00:00<00:00, 248.06it/\n",
      "Train Epoch554 out_loss 0.15620695054531097, R2 0.08370119333267212\n",
      "Test Epoch554 layer0 out_loss 0.1596226841211319, R2 0.06674689054489136\n",
      "Test Epoch554 layer1 out_loss 0.15903659164905548, R2 0.07017356157302856\n",
      "Test Epoch554 layer2 out_loss 0.15779727697372437, R2 0.07741934061050415\n",
      "Test Epoch554 layer3 out_loss 0.15750151872634888, R2 0.07914847135543823\n",
      "Test Epoch554 layer4 out_loss 0.15756312012672424, R2 0.07878834009170532\n",
      "Train 555 | out_loss 0.3950575292110443: 100%|█| 125/125 [00:00<00:00, 261.99it/\n",
      "Train Epoch555 out_loss 0.1560705006122589, R2 0.08450162410736084\n",
      "Test Epoch555 layer0 out_loss 0.1594785898923874, R2 0.06758934259414673\n",
      "Test Epoch555 layer1 out_loss 0.1585739403963089, R2 0.07287847995758057\n",
      "Test Epoch555 layer2 out_loss 0.1575300246477127, R2 0.07898181676864624\n",
      "Test Epoch555 layer3 out_loss 0.15748706459999084, R2 0.07923305034637451\n",
      "Test Epoch555 layer4 out_loss 0.15758544206619263, R2 0.07865780591964722\n",
      "Train 556 | out_loss 0.3948790431022644: 100%|█| 125/125 [00:00<00:00, 247.83it/\n",
      "Train Epoch556 out_loss 0.15592947602272034, R2 0.08532887697219849\n",
      "Test Epoch556 layer0 out_loss 0.15943650901317596, R2 0.06783539056777954\n",
      "Test Epoch556 layer1 out_loss 0.158420130610466, R2 0.07377773523330688\n",
      "Test Epoch556 layer2 out_loss 0.15749317407608032, R2 0.07919728755950928\n",
      "Test Epoch556 layer3 out_loss 0.1574336737394333, R2 0.07954519987106323\n",
      "Test Epoch556 layer4 out_loss 0.15754099190235138, R2 0.0789177417755127\n",
      "Train 557 | out_loss 0.3949592709541321: 100%|█| 125/125 [00:00<00:00, 259.60it/\n",
      "Train Epoch557 out_loss 0.155992791056633, R2 0.08495742082595825\n",
      "Test Epoch557 layer0 out_loss 0.1594831496477127, R2 0.06756263971328735\n",
      "Test Epoch557 layer1 out_loss 0.15859737992286682, R2 0.07274144887924194\n",
      "Test Epoch557 layer2 out_loss 0.15759263932704926, R2 0.07861578464508057\n",
      "Test Epoch557 layer3 out_loss 0.15740716457366943, R2 0.07970017194747925\n",
      "Test Epoch557 layer4 out_loss 0.15753315389156342, R2 0.0789635181427002\n",
      "Train 558 | out_loss 0.3950614035129547: 100%|█| 125/125 [00:00<00:00, 254.35it/\n",
      "Train Epoch558 out_loss 0.15607351064682007, R2 0.08448392152786255\n",
      "Test Epoch558 layer0 out_loss 0.15950621664524078, R2 0.0674278736114502\n",
      "Test Epoch558 layer1 out_loss 0.15857961773872375, R2 0.07284528017044067\n",
      "Test Epoch558 layer2 out_loss 0.15765199065208435, R2 0.07826876640319824\n",
      "Test Epoch558 layer3 out_loss 0.15752160549163818, R2 0.07903105020523071\n",
      "Test Epoch558 layer4 out_loss 0.15750262141227722, R2 0.07914203405380249\n",
      "Train 559 | out_loss 0.3950275778770447: 100%|█| 125/125 [00:00<00:00, 262.14it/\n",
      "Train Epoch559 out_loss 0.15604674816131592, R2 0.08464092016220093\n",
      "Test Epoch559 layer0 out_loss 0.15942977368831635, R2 0.0678747296333313\n",
      "Test Epoch559 layer1 out_loss 0.15890228748321533, R2 0.07095879316329956\n",
      "Test Epoch559 layer2 out_loss 0.15773312747478485, R2 0.07779443264007568\n",
      "Test Epoch559 layer3 out_loss 0.1576419323682785, R2 0.07832753658294678\n",
      "Test Epoch559 layer4 out_loss 0.1576659232378006, R2 0.07818728685379028\n",
      "Train 560 | out_loss 0.3949440121650696: 100%|█| 125/125 [00:00<00:00, 247.66it/\n",
      "Train Epoch560 out_loss 0.15598079562187195, R2 0.08502787351608276\n",
      "Test Epoch560 layer0 out_loss 0.15960685908794403, R2 0.06683939695358276\n",
      "Test Epoch560 layer1 out_loss 0.15864484012126923, R2 0.0724639892578125\n",
      "Test Epoch560 layer2 out_loss 0.1579563468694687, R2 0.07648932933807373\n",
      "Test Epoch560 layer3 out_loss 0.1581045389175415, R2 0.07562291622161865\n",
      "Test Epoch560 layer4 out_loss 0.15817883610725403, R2 0.0751885175704956\n",
      "Train 561 | out_loss 0.3950003385543823: 100%|█| 125/125 [00:00<00:00, 257.10it/\n",
      "Train Epoch561 out_loss 0.15602527558803558, R2 0.0847669243812561\n",
      "Test Epoch561 layer0 out_loss 0.1594056934118271, R2 0.068015456199646\n",
      "Test Epoch561 layer1 out_loss 0.1585383266210556, R2 0.073086678981781\n",
      "Test Epoch561 layer2 out_loss 0.15743869543075562, R2 0.07951587438583374\n",
      "Test Epoch561 layer3 out_loss 0.15729472041130066, R2 0.08035755157470703\n",
      "Test Epoch561 layer4 out_loss 0.1573774367570877, R2 0.07987391948699951\n",
      "Train 562 | out_loss 0.39510753750801086: 100%|█| 125/125 [00:00<00:00, 252.37it\n",
      "Train Epoch562 out_loss 0.15610992908477783, R2 0.08427035808563232\n",
      "Test Epoch562 layer0 out_loss 0.1594090610742569, R2 0.06799584627151489\n",
      "Test Epoch562 layer1 out_loss 0.15879778563976288, R2 0.07156968116760254\n",
      "Test Epoch562 layer2 out_loss 0.1577187031507492, R2 0.07787871360778809\n",
      "Test Epoch562 layer3 out_loss 0.1573931723833084, R2 0.07978194952011108\n",
      "Test Epoch562 layer4 out_loss 0.15741397440433502, R2 0.07966035604476929\n",
      "Train 563 | out_loss 0.39499130845069885: 100%|█| 125/125 [00:00<00:00, 249.36it\n",
      "Train Epoch563 out_loss 0.15601812303066254, R2 0.0848088264465332\n",
      "Test Epoch563 layer0 out_loss 0.1593974232673645, R2 0.06806385517120361\n",
      "Test Epoch563 layer1 out_loss 0.15845242142677307, R2 0.07358890771865845\n",
      "Test Epoch563 layer2 out_loss 0.15754006803035736, R2 0.07892310619354248\n",
      "Test Epoch563 layer3 out_loss 0.1573888659477234, R2 0.0798071026802063\n",
      "Test Epoch563 layer4 out_loss 0.15743884444236755, R2 0.07951498031616211\n",
      "Train 564 | out_loss 0.3950224816799164: 100%|█| 125/125 [00:00<00:00, 238.73it/\n",
      "Train Epoch564 out_loss 0.15604273974895477, R2 0.08466446399688721\n",
      "Test Epoch564 layer0 out_loss 0.15947747230529785, R2 0.06759583950042725\n",
      "Test Epoch564 layer1 out_loss 0.1585741490125656, R2 0.07287728786468506\n",
      "Test Epoch564 layer2 out_loss 0.1575576215982437, R2 0.07882052659988403\n",
      "Test Epoch564 layer3 out_loss 0.15733356773853302, R2 0.08013045787811279\n",
      "Test Epoch564 layer4 out_loss 0.15740619599819183, R2 0.07970589399337769\n",
      "Train 565 | out_loss 0.3950587511062622: 100%|█| 125/125 [00:00<00:00, 250.39it/\n",
      "Train Epoch565 out_loss 0.15607139468193054, R2 0.0844963788986206\n",
      "Test Epoch565 layer0 out_loss 0.15941104292869568, R2 0.06798428297042847\n",
      "Test Epoch565 layer1 out_loss 0.15838712453842163, R2 0.07397067546844482\n",
      "Test Epoch565 layer2 out_loss 0.15741896629333496, R2 0.07963120937347412\n",
      "Test Epoch565 layer3 out_loss 0.1573198437690735, R2 0.08021074533462524\n",
      "Test Epoch565 layer4 out_loss 0.15734057128429413, R2 0.0800895094871521\n",
      "Train 566 | out_loss 0.39496245980262756: 100%|█| 125/125 [00:00<00:00, 257.56it\n",
      "Train Epoch566 out_loss 0.15599532425403595, R2 0.08494257926940918\n",
      "Test Epoch566 layer0 out_loss 0.15943463146686554, R2 0.06784635782241821\n",
      "Test Epoch566 layer1 out_loss 0.158376082777977, R2 0.07403528690338135\n",
      "Test Epoch566 layer2 out_loss 0.15735870599746704, R2 0.07998347282409668\n",
      "Test Epoch566 layer3 out_loss 0.15725618600845337, R2 0.08058291673660278\n",
      "Test Epoch566 layer4 out_loss 0.15737849473953247, R2 0.07986778020858765\n",
      "Train 567 | out_loss 0.3950587809085846: 100%|█| 125/125 [00:00<00:00, 241.76it/\n",
      "Train Epoch567 out_loss 0.15607137978076935, R2 0.08449643850326538\n",
      "Test Epoch567 layer0 out_loss 0.15938566625118256, R2 0.06813263893127441\n",
      "Test Epoch567 layer1 out_loss 0.15837040543556213, R2 0.07406842708587646\n",
      "Test Epoch567 layer2 out_loss 0.15740442276000977, R2 0.07971620559692383\n",
      "Test Epoch567 layer3 out_loss 0.15733425319194794, R2 0.08012640476226807\n",
      "Test Epoch567 layer4 out_loss 0.15737031400203705, R2 0.07991558313369751\n",
      "Train 568 | out_loss 0.39499950408935547: 100%|█| 125/125 [00:00<00:00, 256.95it\n",
      "Train Epoch568 out_loss 0.15602460503578186, R2 0.0847707986831665\n",
      "Test Epoch568 layer0 out_loss 0.15938159823417664, R2 0.0681564211845398\n",
      "Test Epoch568 layer1 out_loss 0.15840879082679749, R2 0.07384401559829712\n",
      "Test Epoch568 layer2 out_loss 0.15759508311748505, R2 0.07860153913497925\n",
      "Test Epoch568 layer3 out_loss 0.15770751237869263, R2 0.07794409990310669\n",
      "Test Epoch568 layer4 out_loss 0.15744586288928986, R2 0.07947391271591187\n",
      "Train 569 | out_loss 0.39492470026016235: 100%|█| 125/125 [00:00<00:00, 259.79it\n",
      "Train Epoch569 out_loss 0.15596550703048706, R2 0.08511751890182495\n",
      "Test Epoch569 layer0 out_loss 0.15957516431808472, R2 0.06702470779418945\n",
      "Test Epoch569 layer1 out_loss 0.15894156694412231, R2 0.0707290768623352\n",
      "Test Epoch569 layer2 out_loss 0.15802328288555145, R2 0.07609796524047852\n",
      "Test Epoch569 layer3 out_loss 0.1583901047706604, R2 0.07395327091217041\n",
      "Test Epoch569 layer4 out_loss 0.15843035280704498, R2 0.07371795177459717\n",
      "Train 570 | out_loss 0.39499616622924805: 100%|█| 125/125 [00:00<00:00, 247.81it\n",
      "Train Epoch570 out_loss 0.15602193772792816, R2 0.08478647470474243\n",
      "Test Epoch570 layer0 out_loss 0.15936465561389923, R2 0.0682554841041565\n",
      "Test Epoch570 layer1 out_loss 0.15855076909065247, R2 0.07301396131515503\n",
      "Test Epoch570 layer2 out_loss 0.15750925242900848, R2 0.07910335063934326\n",
      "Test Epoch570 layer3 out_loss 0.1572984755039215, R2 0.08033561706542969\n",
      "Test Epoch570 layer4 out_loss 0.15732507407665253, R2 0.0801801085472107\n",
      "Train 571 | out_loss 0.3949586749076843: 100%|█| 125/125 [00:00<00:00, 261.11it/\n",
      "Train Epoch571 out_loss 0.15599234402179718, R2 0.08496010303497314\n",
      "Test Epoch571 layer0 out_loss 0.15935830771923065, R2 0.06829255819320679\n",
      "Test Epoch571 layer1 out_loss 0.15890030562877655, R2 0.07097035646438599\n",
      "Test Epoch571 layer2 out_loss 0.15793392062187195, R2 0.07662045955657959\n",
      "Test Epoch571 layer3 out_loss 0.15788616240024567, R2 0.07689958810806274\n",
      "Test Epoch571 layer4 out_loss 0.15772873163223267, R2 0.07782012224197388\n",
      "Train 572 | out_loss 0.39511698484420776: 100%|█| 125/125 [00:00<00:00, 248.71it\n",
      "Train Epoch572 out_loss 0.15611745417118073, R2 0.08422625064849854\n",
      "Test Epoch572 layer0 out_loss 0.15955685079097748, R2 0.06713175773620605\n",
      "Test Epoch572 layer1 out_loss 0.1584751307964325, R2 0.07345616817474365\n",
      "Test Epoch572 layer2 out_loss 0.1575918048620224, R2 0.07862061262130737\n",
      "Test Epoch572 layer3 out_loss 0.15762612223625183, R2 0.07841998338699341\n",
      "Test Epoch572 layer4 out_loss 0.15770764648914337, R2 0.07794338464736938\n",
      "Train 573 | out_loss 0.3950931131839752: 100%|█| 125/125 [00:00<00:00, 253.93it/\n",
      "Train Epoch573 out_loss 0.15609857439994812, R2 0.08433693647384644\n",
      "Test Epoch573 layer0 out_loss 0.1593877077102661, R2 0.06812065839767456\n",
      "Test Epoch573 layer1 out_loss 0.15845628082752228, R2 0.07356637716293335\n",
      "Test Epoch573 layer2 out_loss 0.15769873559474945, R2 0.07799547910690308\n",
      "Test Epoch573 layer3 out_loss 0.15784628689289093, R2 0.07713282108306885\n",
      "Test Epoch573 layer4 out_loss 0.157786563038826, R2 0.07748198509216309\n",
      "Train 574 | out_loss 0.3949495553970337: 100%|█| 125/125 [00:00<00:00, 247.96it/\n",
      "Train Epoch574 out_loss 0.15598516166210175, R2 0.08500218391418457\n",
      "Test Epoch574 layer0 out_loss 0.1594468057155609, R2 0.06777513027191162\n",
      "Test Epoch574 layer1 out_loss 0.1584480106830597, R2 0.07361471652984619\n",
      "Test Epoch574 layer2 out_loss 0.15742504596710205, R2 0.07959562540054321\n",
      "Test Epoch574 layer3 out_loss 0.15727227926254272, R2 0.08048880100250244\n",
      "Test Epoch574 layer4 out_loss 0.1573551595211029, R2 0.08000421524047852\n",
      "Train 575 | out_loss 0.3950140178203583: 100%|█| 125/125 [00:00<00:00, 257.87it/\n",
      "Train Epoch575 out_loss 0.15603607892990112, R2 0.08470356464385986\n",
      "Test Epoch575 layer0 out_loss 0.15935756266117096, R2 0.06829696893692017\n",
      "Test Epoch575 layer1 out_loss 0.15856516361236572, R2 0.0729297399520874\n",
      "Test Epoch575 layer2 out_loss 0.15738163888454437, R2 0.07984942197799683\n",
      "Test Epoch575 layer3 out_loss 0.1572878658771515, R2 0.0803976058959961\n",
      "Test Epoch575 layer4 out_loss 0.15730081498622894, R2 0.08032196760177612\n",
      "Train 576 | out_loss 0.39477232098579407: 100%|█| 125/125 [00:00<00:00, 255.76it\n",
      "Train Epoch576 out_loss 0.15584510564804077, R2 0.08582377433776855\n",
      "Test Epoch576 layer0 out_loss 0.15935149788856506, R2 0.06833237409591675\n",
      "Test Epoch576 layer1 out_loss 0.15839508175849915, R2 0.07392418384552002\n",
      "Test Epoch576 layer2 out_loss 0.1575525552034378, R2 0.0788501501083374\n",
      "Test Epoch576 layer3 out_loss 0.1575678586959839, R2 0.07876068353652954\n",
      "Test Epoch576 layer4 out_loss 0.15756572782993317, R2 0.07877308130264282\n",
      "Train 577 | out_loss 0.394965261220932: 100%|█| 125/125 [00:00<00:00, 257.01it/s\n",
      "Train Epoch577 out_loss 0.15599755942821503, R2 0.0849294662475586\n",
      "Test Epoch577 layer0 out_loss 0.15935172140598297, R2 0.06833112239837646\n",
      "Test Epoch577 layer1 out_loss 0.15841495990753174, R2 0.07380795478820801\n",
      "Test Epoch577 layer2 out_loss 0.15751199424266815, R2 0.0790872573852539\n",
      "Test Epoch577 layer3 out_loss 0.15733060240745544, R2 0.08014780282974243\n",
      "Test Epoch577 layer4 out_loss 0.15735264122486115, R2 0.08001893758773804\n",
      "Train 578 | out_loss 0.3948924243450165: 100%|█| 125/125 [00:00<00:00, 260.31it/\n",
      "Train Epoch578 out_loss 0.15594007074832916, R2 0.08526676893234253\n",
      "Test Epoch578 layer0 out_loss 0.159424290060997, R2 0.06790679693222046\n",
      "Test Epoch578 layer1 out_loss 0.15857592225074768, R2 0.07286685705184937\n",
      "Test Epoch578 layer2 out_loss 0.15766911208629608, R2 0.07816863059997559\n",
      "Test Epoch578 layer3 out_loss 0.15758241713047028, R2 0.07867556810379028\n",
      "Test Epoch578 layer4 out_loss 0.15762576460838318, R2 0.07842212915420532\n",
      "Train 579 | out_loss 0.39496296644210815: 100%|█| 125/125 [00:00<00:00, 256.10it\n",
      "Train Epoch579 out_loss 0.15599572658538818, R2 0.08494025468826294\n",
      "Test Epoch579 layer0 out_loss 0.15935513377189636, R2 0.06831115484237671\n",
      "Test Epoch579 layer1 out_loss 0.15836133062839508, R2 0.07412153482437134\n",
      "Test Epoch579 layer2 out_loss 0.15743295848369598, R2 0.07954937219619751\n",
      "Test Epoch579 layer3 out_loss 0.15745700895786285, R2 0.07940870523452759\n",
      "Test Epoch579 layer4 out_loss 0.15751056373119354, R2 0.07909566164016724\n",
      "Train 580 | out_loss 0.39500656723976135: 100%|█| 125/125 [00:00<00:00, 260.68it\n",
      "Train Epoch580 out_loss 0.15603016316890717, R2 0.08473819494247437\n",
      "Test Epoch580 layer0 out_loss 0.15936611592769623, R2 0.06824690103530884\n",
      "Test Epoch580 layer1 out_loss 0.15834182500839233, R2 0.07423555850982666\n",
      "Test Epoch580 layer2 out_loss 0.15749356150627136, R2 0.07919502258300781\n",
      "Test Epoch580 layer3 out_loss 0.15733776986598969, R2 0.08010590076446533\n",
      "Test Epoch580 layer4 out_loss 0.15735992789268494, R2 0.07997637987136841\n",
      "Train 581 | out_loss 0.39495736360549927: 100%|█| 125/125 [00:00<00:00, 254.42it\n",
      "Train Epoch581 out_loss 0.1559913158416748, R2 0.08496612310409546\n",
      "Test Epoch581 layer0 out_loss 0.15932996571063995, R2 0.06845825910568237\n",
      "Test Epoch581 layer1 out_loss 0.15833637118339539, R2 0.07426750659942627\n",
      "Test Epoch581 layer2 out_loss 0.15735799074172974, R2 0.07998770475387573\n",
      "Test Epoch581 layer3 out_loss 0.15721158683300018, R2 0.08084356784820557\n",
      "Test Epoch581 layer4 out_loss 0.15726590156555176, R2 0.08052611351013184\n",
      "Train 582 | out_loss 0.3948623239994049: 100%|█| 125/125 [00:00<00:00, 253.19it/\n",
      "Train Epoch582 out_loss 0.1559162139892578, R2 0.08540666103363037\n",
      "Test Epoch582 layer0 out_loss 0.15970098972320557, R2 0.06628906726837158\n",
      "Test Epoch582 layer1 out_loss 0.15883517265319824, R2 0.07135111093521118\n",
      "Test Epoch582 layer2 out_loss 0.15814942121505737, R2 0.07536053657531738\n",
      "Test Epoch582 layer3 out_loss 0.15817290544509888, R2 0.07522314786911011\n",
      "Test Epoch582 layer4 out_loss 0.15847069025039673, R2 0.07348215579986572\n",
      "Train 583 | out_loss 0.39501920342445374: 100%|█| 125/125 [00:00<00:00, 256.35it\n",
      "Train Epoch583 out_loss 0.15604014694690704, R2 0.08467966318130493\n",
      "Test Epoch583 layer0 out_loss 0.15932631492614746, R2 0.06847965717315674\n",
      "Test Epoch583 layer1 out_loss 0.1583380252122879, R2 0.07425779104232788\n",
      "Test Epoch583 layer2 out_loss 0.15734981000423431, R2 0.0800355076789856\n",
      "Test Epoch583 layer3 out_loss 0.15719959139823914, R2 0.08091378211975098\n",
      "Test Epoch583 layer4 out_loss 0.1572660207748413, R2 0.08052539825439453\n",
      "Train 584 | out_loss 0.3948819637298584: 100%|█| 125/125 [00:00<00:00, 261.09it/\n",
      "Train Epoch584 out_loss 0.1559317260980606, R2 0.08531564474105835\n",
      "Test Epoch584 layer0 out_loss 0.1593305617570877, R2 0.06845474243164062\n",
      "Test Epoch584 layer1 out_loss 0.1583375483751297, R2 0.07426053285598755\n",
      "Test Epoch584 layer2 out_loss 0.1574009358882904, R2 0.07973653078079224\n",
      "Test Epoch584 layer3 out_loss 0.15733663737773895, R2 0.0801125168800354\n",
      "Test Epoch584 layer4 out_loss 0.1573607325553894, R2 0.07997161149978638\n",
      "Train 585 | out_loss 0.3948915898799896: 100%|█| 125/125 [00:00<00:00, 258.49it/\n",
      "Train Epoch585 out_loss 0.15593942999839783, R2 0.08527052402496338\n",
      "Test Epoch585 layer0 out_loss 0.1593138426542282, R2 0.06855255365371704\n",
      "Test Epoch585 layer1 out_loss 0.158401221036911, R2 0.07388836145401001\n",
      "Test Epoch585 layer2 out_loss 0.15736933052539825, R2 0.0799214243888855\n",
      "Test Epoch585 layer3 out_loss 0.15724191069602966, R2 0.08066630363464355\n",
      "Test Epoch585 layer4 out_loss 0.15728619694709778, R2 0.08040744066238403\n",
      "Train 586 | out_loss 0.3949124217033386: 100%|█| 125/125 [00:00<00:00, 256.23it/\n",
      "Train Epoch586 out_loss 0.15595583617687225, R2 0.08517420291900635\n",
      "Test Epoch586 layer0 out_loss 0.15941275656223297, R2 0.0679742693901062\n",
      "Test Epoch586 layer1 out_loss 0.15875959396362305, R2 0.07179301977157593\n",
      "Test Epoch586 layer2 out_loss 0.15759138762950897, R2 0.07862311601638794\n",
      "Test Epoch586 layer3 out_loss 0.15750637650489807, R2 0.07912009954452515\n",
      "Test Epoch586 layer4 out_loss 0.1575358361005783, R2 0.07894784212112427\n",
      "Train 587 | out_loss 0.3948250114917755: 100%|█| 125/125 [00:00<00:00, 249.36it/\n",
      "Train Epoch587 out_loss 0.15588678419589996, R2 0.08557933568954468\n",
      "Test Epoch587 layer0 out_loss 0.15930798649787903, R2 0.06858682632446289\n",
      "Test Epoch587 layer1 out_loss 0.15841519832611084, R2 0.07380664348602295\n",
      "Test Epoch587 layer2 out_loss 0.15735742449760437, R2 0.07999098300933838\n",
      "Test Epoch587 layer3 out_loss 0.15719851851463318, R2 0.08092004060745239\n",
      "Test Epoch587 layer4 out_loss 0.15727050602436066, R2 0.08049917221069336\n",
      "Train 588 | out_loss 0.39483076333999634: 100%|█| 125/125 [00:00<00:00, 261.08it\n",
      "Train Epoch588 out_loss 0.1558912992477417, R2 0.08555281162261963\n",
      "Test Epoch588 layer0 out_loss 0.15943609178066254, R2 0.06783777475357056\n",
      "Test Epoch588 layer1 out_loss 0.15870241820812225, R2 0.07212728261947632\n",
      "Test Epoch588 layer2 out_loss 0.15764044225215912, R2 0.07833629846572876\n",
      "Test Epoch588 layer3 out_loss 0.1575252264738083, R2 0.07900995016098022\n",
      "Test Epoch588 layer4 out_loss 0.15759466588497162, R2 0.07860392332077026\n",
      "Train 589 | out_loss 0.3949676752090454: 100%|█| 125/125 [00:00<00:00, 254.60it/\n",
      "Train Epoch589 out_loss 0.15599946677684784, R2 0.0849183201789856\n",
      "Test Epoch589 layer0 out_loss 0.15934106707572937, R2 0.06839340925216675\n",
      "Test Epoch589 layer1 out_loss 0.1583419144153595, R2 0.07423502206802368\n",
      "Test Epoch589 layer2 out_loss 0.15738928318023682, R2 0.07980471849441528\n",
      "Test Epoch589 layer3 out_loss 0.1572580635547638, R2 0.08057194948196411\n",
      "Test Epoch589 layer4 out_loss 0.1573457419872284, R2 0.0800592303276062\n",
      "Train 590 | out_loss 0.3948921263217926: 100%|█| 125/125 [00:00<00:00, 257.23it/\n",
      "Train Epoch590 out_loss 0.15593980252742767, R2 0.08526825904846191\n",
      "Test Epoch590 layer0 out_loss 0.1593012660741806, R2 0.0686260461807251\n",
      "Test Epoch590 layer1 out_loss 0.1586286574602127, R2 0.07255852222442627\n",
      "Test Epoch590 layer2 out_loss 0.1575097143650055, R2 0.07910054922103882\n",
      "Test Epoch590 layer3 out_loss 0.15732602775096893, R2 0.0801745057106018\n",
      "Test Epoch590 layer4 out_loss 0.15736845135688782, R2 0.0799264907836914\n",
      "Train 591 | out_loss 0.3945074677467346: 100%|█| 125/125 [00:00<00:00, 254.63it/\n",
      "Train Epoch591 out_loss 0.15563611686229706, R2 0.08704966306686401\n",
      "Test Epoch591 layer0 out_loss 0.15975071489810944, R2 0.06599831581115723\n",
      "Test Epoch591 layer1 out_loss 0.1586567759513855, R2 0.07239419221878052\n",
      "Test Epoch591 layer2 out_loss 0.15804991126060486, R2 0.0759422779083252\n",
      "Test Epoch591 layer3 out_loss 0.15821149945259094, R2 0.07499748468399048\n",
      "Test Epoch591 layer4 out_loss 0.15836796164512634, R2 0.07408273220062256\n",
      "Train 592 | out_loss 0.3949776589870453: 100%|█| 125/125 [00:00<00:00, 250.75it/\n",
      "Train Epoch592 out_loss 0.156007319688797, R2 0.08487218618392944\n",
      "Test Epoch592 layer0 out_loss 0.1593288779258728, R2 0.06846469640731812\n",
      "Test Epoch592 layer1 out_loss 0.1584215760231018, R2 0.07376933097839355\n",
      "Test Epoch592 layer2 out_loss 0.15742278099060059, R2 0.07960891723632812\n",
      "Test Epoch592 layer3 out_loss 0.15730667114257812, R2 0.08028769493103027\n",
      "Test Epoch592 layer4 out_loss 0.1573677957057953, R2 0.07993030548095703\n",
      "Train 593 | out_loss 0.3948599398136139: 100%|█| 125/125 [00:00<00:00, 254.39it/\n",
      "Train Epoch593 out_loss 0.15591436624526978, R2 0.08541750907897949\n",
      "Test Epoch593 layer0 out_loss 0.15936896204948425, R2 0.06823033094406128\n",
      "Test Epoch593 layer1 out_loss 0.15842831134796143, R2 0.07372987270355225\n",
      "Test Epoch593 layer2 out_loss 0.15742020308971405, R2 0.0796239972114563\n",
      "Test Epoch593 layer3 out_loss 0.1572982519865036, R2 0.08033698797225952\n",
      "Test Epoch593 layer4 out_loss 0.15733568370342255, R2 0.08011811971664429\n",
      "Train 594 | out_loss 0.3948892056941986: 100%|█| 125/125 [00:00<00:00, 260.29it/\n",
      "Train Epoch594 out_loss 0.1559375524520874, R2 0.08528149127960205\n",
      "Test Epoch594 layer0 out_loss 0.15933668613433838, R2 0.06841897964477539\n",
      "Test Epoch594 layer1 out_loss 0.15846829116344452, R2 0.07349616289138794\n",
      "Test Epoch594 layer2 out_loss 0.15748390555381775, R2 0.07925152778625488\n",
      "Test Epoch594 layer3 out_loss 0.1573568433523178, R2 0.07999438047409058\n",
      "Test Epoch594 layer4 out_loss 0.1573774367570877, R2 0.07987391948699951\n",
      "Train 595 | out_loss 0.39477604627609253: 100%|█| 125/125 [00:00<00:00, 253.41it\n",
      "Train Epoch595 out_loss 0.15584814548492432, R2 0.08580595254898071\n",
      "Test Epoch595 layer0 out_loss 0.15934863686561584, R2 0.06834912300109863\n",
      "Test Epoch595 layer1 out_loss 0.1584242731332779, R2 0.07375353574752808\n",
      "Test Epoch595 layer2 out_loss 0.15739503502845764, R2 0.07977110147476196\n",
      "Test Epoch595 layer3 out_loss 0.15724796056747437, R2 0.08063101768493652\n",
      "Test Epoch595 layer4 out_loss 0.15732496976852417, R2 0.08018076419830322\n",
      "Train 596 | out_loss 0.3948235809803009: 100%|█| 125/125 [00:00<00:00, 256.08it/\n",
      "Train Epoch596 out_loss 0.15588566660881042, R2 0.0855858325958252\n",
      "Test Epoch596 layer0 out_loss 0.15942907333374023, R2 0.06787878274917603\n",
      "Test Epoch596 layer1 out_loss 0.1582927703857422, R2 0.07452237606048584\n",
      "Test Epoch596 layer2 out_loss 0.15735431015491486, R2 0.08000922203063965\n",
      "Test Epoch596 layer3 out_loss 0.15734368562698364, R2 0.08007127046585083\n",
      "Test Epoch596 layer4 out_loss 0.1574617475271225, R2 0.0793810486793518\n",
      "Train 597 | out_loss 0.3948839008808136: 100%|█| 125/125 [00:00<00:00, 262.35it/\n",
      "Train Epoch597 out_loss 0.15593333542346954, R2 0.08530622720718384\n",
      "Test Epoch597 layer0 out_loss 0.15928323566913605, R2 0.06873148679733276\n",
      "Test Epoch597 layer1 out_loss 0.1586577445268631, R2 0.07238847017288208\n",
      "Test Epoch597 layer2 out_loss 0.15750102698802948, R2 0.07915139198303223\n",
      "Test Epoch597 layer3 out_loss 0.15730497241020203, R2 0.08029758930206299\n",
      "Test Epoch597 layer4 out_loss 0.15729273855686188, R2 0.08036917448043823\n",
      "Train 598 | out_loss 0.3949011266231537: 100%|█| 125/125 [00:00<00:00, 261.30it/\n",
      "Train Epoch598 out_loss 0.15594692528247833, R2 0.08522647619247437\n",
      "Test Epoch598 layer0 out_loss 0.15930795669555664, R2 0.06858700513839722\n",
      "Test Epoch598 layer1 out_loss 0.1582777053117752, R2 0.07461041212081909\n",
      "Test Epoch598 layer2 out_loss 0.15732109546661377, R2 0.08020341396331787\n",
      "Test Epoch598 layer3 out_loss 0.15716399252414703, R2 0.08112192153930664\n",
      "Test Epoch598 layer4 out_loss 0.15721286833286285, R2 0.08083611726760864\n",
      "Train 599 | out_loss 0.39477014541625977: 100%|█| 125/125 [00:00<00:00, 254.03it\n",
      "Train Epoch599 out_loss 0.15584349632263184, R2 0.08583325147628784\n",
      "Test Epoch599 layer0 out_loss 0.15928137302398682, R2 0.06874239444732666\n",
      "Test Epoch599 layer1 out_loss 0.15846240520477295, R2 0.07353061437606812\n",
      "Test Epoch599 layer2 out_loss 0.15742544829845428, R2 0.07959330081939697\n",
      "Test Epoch599 layer3 out_loss 0.15720993280410767, R2 0.08085334300994873\n",
      "Test Epoch599 layer4 out_loss 0.15724125504493713, R2 0.08067017793655396\n",
      "Train 600 | out_loss 0.39479729533195496: 100%|█| 125/125 [00:00<00:00, 265.17it\n",
      "Train Epoch600 out_loss 0.155864879488945, R2 0.08570772409439087\n",
      "Test Epoch600 layer0 out_loss 0.1592922806739807, R2 0.06867861747741699\n",
      "Test Epoch600 layer1 out_loss 0.15840129554271698, R2 0.0738878846168518\n",
      "Test Epoch600 layer2 out_loss 0.1574081927537918, R2 0.07969421148300171\n",
      "Test Epoch600 layer3 out_loss 0.15736013650894165, R2 0.07997512817382812\n",
      "Test Epoch600 layer4 out_loss 0.1573493331670761, R2 0.08003824949264526\n",
      "Train 601 | out_loss 0.39482322335243225: 100%|█| 125/125 [00:00<00:00, 257.52it\n",
      "Train Epoch601 out_loss 0.15588536858558655, R2 0.08558756113052368\n",
      "Test Epoch601 layer0 out_loss 0.15927466750144958, R2 0.06878155469894409\n",
      "Test Epoch601 layer1 out_loss 0.1582828313112259, R2 0.07458043098449707\n",
      "Test Epoch601 layer2 out_loss 0.15741732716560364, R2 0.07964074611663818\n",
      "Test Epoch601 layer3 out_loss 0.1573086678981781, R2 0.0802760124206543\n",
      "Test Epoch601 layer4 out_loss 0.15731465816497803, R2 0.08024108409881592\n",
      "Train 602 | out_loss 0.3949423134326935: 100%|█| 125/125 [00:00<00:00, 253.05it/\n",
      "Train Epoch602 out_loss 0.15597942471504211, R2 0.08503580093383789\n",
      "Test Epoch602 layer0 out_loss 0.1593131422996521, R2 0.06855666637420654\n",
      "Test Epoch602 layer1 out_loss 0.15827730298042297, R2 0.07461285591125488\n",
      "Test Epoch602 layer2 out_loss 0.15743853151798248, R2 0.07951676845550537\n",
      "Test Epoch602 layer3 out_loss 0.15727488696575165, R2 0.08047354221343994\n",
      "Test Epoch602 layer4 out_loss 0.15733015537261963, R2 0.08015042543411255\n",
      "Train 603 | out_loss 0.39486855268478394: 100%|█| 125/125 [00:00<00:00, 257.53it\n",
      "Train Epoch603 out_loss 0.15592114627361298, R2 0.0853777527809143\n",
      "Test Epoch603 layer0 out_loss 0.1593467742204666, R2 0.06836003065109253\n",
      "Test Epoch603 layer1 out_loss 0.158734992146492, R2 0.07193690538406372\n",
      "Test Epoch603 layer2 out_loss 0.1575559675693512, R2 0.07883018255233765\n",
      "Test Epoch603 layer3 out_loss 0.1573205590248108, R2 0.08020651340484619\n",
      "Test Epoch603 layer4 out_loss 0.15740060806274414, R2 0.07973849773406982\n",
      "Train 604 | out_loss 0.39488694071769714: 100%|█| 125/125 [00:00<00:00, 225.14it\n",
      "Train Epoch604 out_loss 0.15593567490577698, R2 0.0852925181388855\n",
      "Test Epoch604 layer0 out_loss 0.1593395620584488, R2 0.06840211153030396\n",
      "Test Epoch604 layer1 out_loss 0.15842357277870178, R2 0.07375764846801758\n",
      "Test Epoch604 layer2 out_loss 0.1573510766029358, R2 0.08002811670303345\n",
      "Test Epoch604 layer3 out_loss 0.15717266499996185, R2 0.08107119798660278\n",
      "Test Epoch604 layer4 out_loss 0.15724577009677887, R2 0.08064377307891846\n",
      "Train 605 | out_loss 0.3949030041694641: 100%|█| 125/125 [00:00<00:00, 251.61it/\n",
      "Train Epoch605 out_loss 0.15594837069511414, R2 0.08521807193756104\n",
      "Test Epoch605 layer0 out_loss 0.1592845320701599, R2 0.06872391700744629\n",
      "Test Epoch605 layer1 out_loss 0.1583348959684372, R2 0.07427603006362915\n",
      "Test Epoch605 layer2 out_loss 0.1573839783668518, R2 0.07983577251434326\n",
      "Test Epoch605 layer3 out_loss 0.15718531608581543, R2 0.08099722862243652\n",
      "Test Epoch605 layer4 out_loss 0.15725409984588623, R2 0.08059507608413696\n",
      "Train 606 | out_loss 0.3947887420654297: 100%|█| 125/125 [00:00<00:00, 254.95it/\n",
      "Train Epoch606 out_loss 0.15585815906524658, R2 0.08574724197387695\n",
      "Test Epoch606 layer0 out_loss 0.15925689041614532, R2 0.06888550519943237\n",
      "Test Epoch606 layer1 out_loss 0.15831170976161957, R2 0.07441163063049316\n",
      "Test Epoch606 layer2 out_loss 0.157351553440094, R2 0.080025315284729\n",
      "Test Epoch606 layer3 out_loss 0.15719759464263916, R2 0.08092540502548218\n",
      "Test Epoch606 layer4 out_loss 0.157248392701149, R2 0.0806283950805664\n",
      "Train 607 | out_loss 0.39481791853904724: 100%|█| 125/125 [00:00<00:00, 250.83it\n",
      "Train Epoch607 out_loss 0.15588121116161346, R2 0.08561199903488159\n",
      "Test Epoch607 layer0 out_loss 0.15925385057926178, R2 0.06890332698822021\n",
      "Test Epoch607 layer1 out_loss 0.1582866609096527, R2 0.0745580792427063\n",
      "Test Epoch607 layer2 out_loss 0.15737701952457428, R2 0.07987642288208008\n",
      "Test Epoch607 layer3 out_loss 0.15725235641002655, R2 0.08060532808303833\n",
      "Test Epoch607 layer4 out_loss 0.15726155042648315, R2 0.08055150508880615\n",
      "Train 608 | out_loss 0.3946785628795624: 100%|█| 125/125 [00:00<00:00, 258.10it/\n",
      "Train Epoch608 out_loss 0.15577125549316406, R2 0.0862569808959961\n",
      "Test Epoch608 layer0 out_loss 0.1593167781829834, R2 0.06853538751602173\n",
      "Test Epoch608 layer1 out_loss 0.15900366008281708, R2 0.0703660249710083\n",
      "Test Epoch608 layer2 out_loss 0.15797196328639984, R2 0.07639795541763306\n",
      "Test Epoch608 layer3 out_loss 0.1577530950307846, R2 0.07767760753631592\n",
      "Test Epoch608 layer4 out_loss 0.15784905850887299, R2 0.07711654901504517\n",
      "Train 609 | out_loss 0.39475029706954956: 100%|█| 125/125 [00:00<00:00, 255.37it\n",
      "Train Epoch609 out_loss 0.15582777559757233, R2 0.08592545986175537\n",
      "Test Epoch609 layer0 out_loss 0.15955771505832672, R2 0.06712675094604492\n",
      "Test Epoch609 layer1 out_loss 0.15888625383377075, R2 0.0710524320602417\n",
      "Test Epoch609 layer2 out_loss 0.15789921581745148, R2 0.07682335376739502\n",
      "Test Epoch609 layer3 out_loss 0.15790648758411407, R2 0.07678079605102539\n",
      "Test Epoch609 layer4 out_loss 0.1578381061553955, R2 0.07718062400817871\n",
      "Train 610 | out_loss 0.3948167860507965: 100%|█| 125/125 [00:00<00:00, 241.81it/\n",
      "Train Epoch610 out_loss 0.15588027238845825, R2 0.08561742305755615\n",
      "Test Epoch610 layer0 out_loss 0.15938140451908112, R2 0.06815749406814575\n",
      "Test Epoch610 layer1 out_loss 0.15848778188228607, R2 0.07338225841522217\n",
      "Test Epoch610 layer2 out_loss 0.1576760858297348, R2 0.07812786102294922\n",
      "Test Epoch610 layer3 out_loss 0.15777906775474548, R2 0.077525794506073\n",
      "Test Epoch610 layer4 out_loss 0.15774889290332794, R2 0.07770222425460815\n",
      "Train 611 | out_loss 0.39471235871315: 100%|█| 125/125 [00:00<00:00, 261.53it/s]\n",
      "Train Epoch611 out_loss 0.1557978242635727, R2 0.08610111474990845\n",
      "Test Epoch611 layer0 out_loss 0.15928103029727936, R2 0.06874436140060425\n",
      "Test Epoch611 layer1 out_loss 0.1582702249288559, R2 0.074654221534729\n",
      "Test Epoch611 layer2 out_loss 0.1573987454175949, R2 0.07974940538406372\n",
      "Test Epoch611 layer3 out_loss 0.15725606679916382, R2 0.08058351278305054\n",
      "Test Epoch611 layer4 out_loss 0.15731669962406158, R2 0.08022910356521606\n",
      "Train 612 | out_loss 0.394796222448349: 100%|█| 125/125 [00:00<00:00, 252.22it/s\n",
      "Train Epoch612 out_loss 0.15586403012275696, R2 0.08571279048919678\n",
      "Test Epoch612 layer0 out_loss 0.15927629172801971, R2 0.0687720775604248\n",
      "Test Epoch612 layer1 out_loss 0.15857745707035065, R2 0.07285791635513306\n",
      "Test Epoch612 layer2 out_loss 0.15751752257347107, R2 0.07905495166778564\n",
      "Test Epoch612 layer3 out_loss 0.15726426243782043, R2 0.0805356502532959\n",
      "Test Epoch612 layer4 out_loss 0.15733382105827332, R2 0.08012902736663818\n",
      "Train 613 | out_loss 0.3947504758834839: 100%|█| 125/125 [00:00<00:00, 239.26it/\n",
      "Train Epoch613 out_loss 0.15582792460918427, R2 0.08592456579208374\n",
      "Test Epoch613 layer0 out_loss 0.15923000872135162, R2 0.06904268264770508\n",
      "Test Epoch613 layer1 out_loss 0.1582832932472229, R2 0.07457774877548218\n",
      "Test Epoch613 layer2 out_loss 0.1573270708322525, R2 0.08016842603683472\n",
      "Test Epoch613 layer3 out_loss 0.15722225606441498, R2 0.08078122138977051\n",
      "Test Epoch613 layer4 out_loss 0.1572953164577484, R2 0.08035409450531006\n",
      "Train 614 | out_loss 0.39482924342155457: 100%|█| 125/125 [00:00<00:00, 253.53it\n",
      "Train Epoch614 out_loss 0.15589012205600739, R2 0.08555972576141357\n",
      "Test Epoch614 layer0 out_loss 0.1592375636100769, R2 0.06899851560592651\n",
      "Test Epoch614 layer1 out_loss 0.15825363993644714, R2 0.07475107908248901\n",
      "Test Epoch614 layer2 out_loss 0.15736767649650574, R2 0.07993102073669434\n",
      "Test Epoch614 layer3 out_loss 0.15726178884506226, R2 0.08055007457733154\n",
      "Test Epoch614 layer4 out_loss 0.15734083950519562, R2 0.08008795976638794\n",
      "Train 615 | out_loss 0.3946966826915741: 100%|█| 125/125 [00:00<00:00, 258.62it/\n",
      "Train Epoch615 out_loss 0.1557854413986206, R2 0.08617371320724487\n",
      "Test Epoch615 layer0 out_loss 0.15922674536705017, R2 0.0690617561340332\n",
      "Test Epoch615 layer1 out_loss 0.15824446082115173, R2 0.07480478286743164\n",
      "Test Epoch615 layer2 out_loss 0.15735794603824615, R2 0.07998794317245483\n",
      "Test Epoch615 layer3 out_loss 0.15723098814487457, R2 0.08073019981384277\n",
      "Test Epoch615 layer4 out_loss 0.15721914172172546, R2 0.08079946041107178\n",
      "Train 616 | out_loss 0.3946608603000641: 100%|█| 125/125 [00:00<00:00, 260.79it/\n",
      "Train Epoch616 out_loss 0.15575715899467468, R2 0.08633965253829956\n",
      "Test Epoch616 layer0 out_loss 0.15925991535186768, R2 0.06886780261993408\n",
      "Test Epoch616 layer1 out_loss 0.15835803747177124, R2 0.07414078712463379\n",
      "Test Epoch616 layer2 out_loss 0.1575298309326172, R2 0.07898300886154175\n",
      "Test Epoch616 layer3 out_loss 0.15759623050689697, R2 0.07859474420547485\n",
      "Test Epoch616 layer4 out_loss 0.15743374824523926, R2 0.0795447826385498\n",
      "Train 617 | out_loss 0.3946898579597473: 100%|█| 125/125 [00:00<00:00, 262.41it/\n",
      "Train Epoch617 out_loss 0.1557801067829132, R2 0.08620506525039673\n",
      "Test Epoch617 layer0 out_loss 0.159224271774292, R2 0.06907624006271362\n",
      "Test Epoch617 layer1 out_loss 0.15829886496067047, R2 0.07448679208755493\n",
      "Test Epoch617 layer2 out_loss 0.15764115750789642, R2 0.07833206653594971\n",
      "Test Epoch617 layer3 out_loss 0.15797503292560577, R2 0.07638001441955566\n",
      "Test Epoch617 layer4 out_loss 0.15765361487865448, R2 0.07825928926467896\n",
      "Train 618 | out_loss 0.39491403102874756: 100%|█| 125/125 [00:00<00:00, 255.15it\n",
      "Train Epoch618 out_loss 0.15595707297325134, R2 0.08516693115234375\n",
      "Test Epoch618 layer0 out_loss 0.1592056006193161, R2 0.06918543577194214\n",
      "Test Epoch618 layer1 out_loss 0.15844769775867462, R2 0.07361656427383423\n",
      "Test Epoch618 layer2 out_loss 0.15736305713653564, R2 0.07995808124542236\n",
      "Test Epoch618 layer3 out_loss 0.15719570219516754, R2 0.0809364914894104\n",
      "Test Epoch618 layer4 out_loss 0.15720856189727783, R2 0.08086133003234863\n",
      "Train 619 | out_loss 0.39478158950805664: 100%|█| 125/125 [00:00<00:00, 262.72it\n",
      "Train Epoch619 out_loss 0.1558525413274765, R2 0.08578014373779297\n",
      "Test Epoch619 layer0 out_loss 0.15926401317119598, R2 0.06884390115737915\n",
      "Test Epoch619 layer1 out_loss 0.15871377289295197, R2 0.07206088304519653\n",
      "Test Epoch619 layer2 out_loss 0.15755315124988556, R2 0.07884669303894043\n",
      "Test Epoch619 layer3 out_loss 0.15747658908367157, R2 0.07929426431655884\n",
      "Test Epoch619 layer4 out_loss 0.1574774980545044, R2 0.07928895950317383\n",
      "Train 620 | out_loss 0.3949255645275116: 100%|█| 125/125 [00:00<00:00, 247.98it/\n",
      "Train Epoch620 out_loss 0.15596622228622437, R2 0.08511334657669067\n",
      "Test Epoch620 layer0 out_loss 0.1592247039079666, R2 0.06907373666763306\n",
      "Test Epoch620 layer1 out_loss 0.1585729569196701, R2 0.07288426160812378\n",
      "Test Epoch620 layer2 out_loss 0.1574527770280838, R2 0.07943350076675415\n",
      "Test Epoch620 layer3 out_loss 0.1572679579257965, R2 0.08051407337188721\n",
      "Test Epoch620 layer4 out_loss 0.15735282003879547, R2 0.08001792430877686\n",
      "Train 621 | out_loss 0.3946942389011383: 100%|█| 125/125 [00:00<00:00, 256.78it/\n",
      "Train Epoch621 out_loss 0.1557835340499878, R2 0.08618491888046265\n",
      "Test Epoch621 layer0 out_loss 0.15924286842346191, R2 0.06896746158599854\n",
      "Test Epoch621 layer1 out_loss 0.15828701853752136, R2 0.07455599308013916\n",
      "Test Epoch621 layer2 out_loss 0.15742579102516174, R2 0.07959121465682983\n",
      "Test Epoch621 layer3 out_loss 0.1573946177959442, R2 0.07977348566055298\n",
      "Test Epoch621 layer4 out_loss 0.15746954083442688, R2 0.07933545112609863\n",
      "Train 622 | out_loss 0.39467668533325195: 100%|█| 125/125 [00:00<00:00, 256.55it\n",
      "Train Epoch622 out_loss 0.1557697206735611, R2 0.08626598119735718\n",
      "Test Epoch622 layer0 out_loss 0.15926267206668854, R2 0.0688517689704895\n",
      "Test Epoch622 layer1 out_loss 0.15935304760932922, R2 0.06832337379455566\n",
      "Test Epoch622 layer2 out_loss 0.1579519361257553, R2 0.0765150785446167\n",
      "Test Epoch622 layer3 out_loss 0.1577451229095459, R2 0.07772427797317505\n",
      "Test Epoch622 layer4 out_loss 0.15776857733726501, R2 0.07758718729019165\n",
      "Train 623 | out_loss 0.39478549361228943: 100%|█| 125/125 [00:00<00:00, 259.01it\n",
      "Train Epoch623 out_loss 0.15585559606552124, R2 0.08576226234436035\n",
      "Test Epoch623 layer0 out_loss 0.15921379625797272, R2 0.06913751363754272\n",
      "Test Epoch623 layer1 out_loss 0.15851151943206787, R2 0.07324343919754028\n",
      "Test Epoch623 layer2 out_loss 0.15746493637561798, R2 0.07936239242553711\n",
      "Test Epoch623 layer3 out_loss 0.15732556581497192, R2 0.08017724752426147\n",
      "Test Epoch623 layer4 out_loss 0.157409206032753, R2 0.0796881914138794\n",
      "Train 624 | out_loss 0.39475515484809875: 100%|█| 125/125 [00:00<00:00, 257.59it\n",
      "Train Epoch624 out_loss 0.15583163499832153, R2 0.08590281009674072\n",
      "Test Epoch624 layer0 out_loss 0.15919747948646545, R2 0.06923288106918335\n",
      "Test Epoch624 layer1 out_loss 0.15822836756706238, R2 0.07489895820617676\n",
      "Test Epoch624 layer2 out_loss 0.15726009011268616, R2 0.08056008815765381\n",
      "Test Epoch624 layer3 out_loss 0.1571098417043686, R2 0.08143848180770874\n",
      "Test Epoch624 layer4 out_loss 0.1571783572435379, R2 0.08103787899017334\n",
      "Train 625 | out_loss 0.39468786120414734: 100%|█| 125/125 [00:00<00:00, 249.62it\n",
      "Train Epoch625 out_loss 0.15577846765518188, R2 0.08621460199356079\n",
      "Test Epoch625 layer0 out_loss 0.15919001400470734, R2 0.06927651166915894\n",
      "Test Epoch625 layer1 out_loss 0.15829192101955414, R2 0.07452738285064697\n",
      "Test Epoch625 layer2 out_loss 0.1575545072555542, R2 0.07883870601654053\n",
      "Test Epoch625 layer3 out_loss 0.15745672583580017, R2 0.0794103741645813\n",
      "Test Epoch625 layer4 out_loss 0.1573208123445511, R2 0.0802050232887268\n",
      "Train 626 | out_loss 0.39474618434906006: 100%|█| 125/125 [00:00<00:00, 264.67it\n",
      "Train Epoch626 out_loss 0.15582455694675446, R2 0.0859442949295044\n",
      "Test Epoch626 layer0 out_loss 0.1592032015323639, R2 0.06919944286346436\n",
      "Test Epoch626 layer1 out_loss 0.1582772433757782, R2 0.07461321353912354\n",
      "Test Epoch626 layer2 out_loss 0.15741465985774994, R2 0.07965636253356934\n",
      "Test Epoch626 layer3 out_loss 0.15729649364948273, R2 0.08034723997116089\n",
      "Test Epoch626 layer4 out_loss 0.15725067257881165, R2 0.0806151032447815\n",
      "Train 627 | out_loss 0.39476460218429565: 100%|█| 125/125 [00:00<00:00, 262.46it\n",
      "Train Epoch627 out_loss 0.15583908557891846, R2 0.08585911989212036\n",
      "Test Epoch627 layer0 out_loss 0.15932001173496246, R2 0.06851649284362793\n",
      "Test Epoch627 layer1 out_loss 0.1586516946554184, R2 0.07242393493652344\n",
      "Test Epoch627 layer2 out_loss 0.15751074254512787, R2 0.07909458875656128\n",
      "Test Epoch627 layer3 out_loss 0.15740810334682465, R2 0.07969474792480469\n",
      "Test Epoch627 layer4 out_loss 0.15752215683460236, R2 0.07902783155441284\n",
      "Train 628 | out_loss 0.3946124315261841: 100%|█| 125/125 [00:00<00:00, 259.90it/\n",
      "Train Epoch628 out_loss 0.15571893751621246, R2 0.08656388521194458\n",
      "Test Epoch628 layer0 out_loss 0.1591930240392685, R2 0.0692589282989502\n",
      "Test Epoch628 layer1 out_loss 0.1582544445991516, R2 0.07474648952484131\n",
      "Test Epoch628 layer2 out_loss 0.15733380615711212, R2 0.08012908697128296\n",
      "Test Epoch628 layer3 out_loss 0.1572229415178299, R2 0.08077722787857056\n",
      "Test Epoch628 layer4 out_loss 0.15728144347667694, R2 0.08043515682220459\n",
      "Train 629 | out_loss 0.3946353793144226: 100%|█| 125/125 [00:00<00:00, 262.99it/\n",
      "Train Epoch629 out_loss 0.15573705732822418, R2 0.08645755052566528\n",
      "Test Epoch629 layer0 out_loss 0.15930406749248505, R2 0.06860971450805664\n",
      "Test Epoch629 layer1 out_loss 0.15874533355236053, R2 0.0718764066696167\n",
      "Test Epoch629 layer2 out_loss 0.15760180354118347, R2 0.07856214046478271\n",
      "Test Epoch629 layer3 out_loss 0.15740665793418884, R2 0.07970309257507324\n",
      "Test Epoch629 layer4 out_loss 0.15747863054275513, R2 0.07928228378295898\n",
      "Train 630 | out_loss 0.39482995867729187: 100%|█| 125/125 [00:00<00:00, 250.44it\n",
      "Train Epoch630 out_loss 0.15589070320129395, R2 0.08555632829666138\n",
      "Test Epoch630 layer0 out_loss 0.15918497741222382, R2 0.06930595636367798\n",
      "Test Epoch630 layer1 out_loss 0.15824401378631592, R2 0.07480740547180176\n",
      "Test Epoch630 layer2 out_loss 0.1572977751493454, R2 0.08033972978591919\n",
      "Test Epoch630 layer3 out_loss 0.1571173071861267, R2 0.08139485120773315\n",
      "Test Epoch630 layer4 out_loss 0.1571567952632904, R2 0.08116394281387329\n",
      "Train 631 | out_loss 0.39473670721054077: 100%|█| 125/125 [00:00<00:00, 253.12it\n",
      "Train Epoch631 out_loss 0.15581704676151276, R2 0.08598834276199341\n",
      "Test Epoch631 layer0 out_loss 0.15919950604438782, R2 0.06922101974487305\n",
      "Test Epoch631 layer1 out_loss 0.15824419260025024, R2 0.07480639219284058\n",
      "Test Epoch631 layer2 out_loss 0.15725396573543549, R2 0.08059585094451904\n",
      "Test Epoch631 layer3 out_loss 0.15714560449123383, R2 0.08122944831848145\n",
      "Test Epoch631 layer4 out_loss 0.15720601379871368, R2 0.08087623119354248\n",
      "Train 632 | out_loss 0.39471235871315: 100%|█| 125/125 [00:00<00:00, 257.24it/s]\n",
      "Train Epoch632 out_loss 0.15579786896705627, R2 0.08610087633132935\n",
      "Test Epoch632 layer0 out_loss 0.15918155014514923, R2 0.06932604312896729\n",
      "Test Epoch632 layer1 out_loss 0.1583080291748047, R2 0.07443314790725708\n",
      "Test Epoch632 layer2 out_loss 0.15737906098365784, R2 0.079864501953125\n",
      "Test Epoch632 layer3 out_loss 0.15725751221179962, R2 0.08057516813278198\n",
      "Test Epoch632 layer4 out_loss 0.1573023945093155, R2 0.08031266927719116\n",
      "Train 633 | out_loss 0.3946346640586853: 100%|█| 125/125 [00:00<00:00, 246.23it/\n",
      "Train Epoch633 out_loss 0.15573656558990479, R2 0.0864604115486145\n",
      "Test Epoch633 layer0 out_loss 0.159221351146698, R2 0.06909334659576416\n",
      "Test Epoch633 layer1 out_loss 0.1588960886001587, R2 0.07099497318267822\n",
      "Test Epoch633 layer2 out_loss 0.15768250823020935, R2 0.07809031009674072\n",
      "Test Epoch633 layer3 out_loss 0.15738415718078613, R2 0.0798346996307373\n",
      "Test Epoch633 layer4 out_loss 0.1574297398328781, R2 0.07956820726394653\n",
      "Train 634 | out_loss 0.3946959972381592: 100%|█| 125/125 [00:00<00:00, 248.00it/\n",
      "Train Epoch634 out_loss 0.15578483045101166, R2 0.0861772894859314\n",
      "Test Epoch634 layer0 out_loss 0.15922744572162628, R2 0.0690576434135437\n",
      "Test Epoch634 layer1 out_loss 0.15856370329856873, R2 0.07293832302093506\n",
      "Test Epoch634 layer2 out_loss 0.15764294564723969, R2 0.07832163572311401\n",
      "Test Epoch634 layer3 out_loss 0.15740954875946045, R2 0.0796862244606018\n",
      "Test Epoch634 layer4 out_loss 0.1574379801750183, R2 0.07952004671096802\n",
      "Train 635 | out_loss 0.3947608172893524: 100%|█| 125/125 [00:00<00:00, 236.45it/\n",
      "Train Epoch635 out_loss 0.15583612024784088, R2 0.08587646484375\n",
      "Test Epoch635 layer0 out_loss 0.15918105840682983, R2 0.06932884454727173\n",
      "Test Epoch635 layer1 out_loss 0.15823157131671906, R2 0.07488018274307251\n",
      "Test Epoch635 layer2 out_loss 0.15742678940296173, R2 0.07958543300628662\n",
      "Test Epoch635 layer3 out_loss 0.1573215276002884, R2 0.08020079135894775\n",
      "Test Epoch635 layer4 out_loss 0.15736912190914154, R2 0.07992255687713623\n",
      "Train 636 | out_loss 0.3946610987186432: 100%|█| 125/125 [00:00<00:00, 255.54it/\n",
      "Train Epoch636 out_loss 0.1557573527097702, R2 0.08633852005004883\n",
      "Test Epoch636 layer0 out_loss 0.15942925214767456, R2 0.06787782907485962\n",
      "Test Epoch636 layer1 out_loss 0.15850095450878143, R2 0.07330513000488281\n",
      "Test Epoch636 layer2 out_loss 0.1573101133108139, R2 0.08026754856109619\n",
      "Test Epoch636 layer3 out_loss 0.15715435147285461, R2 0.08117824792861938\n",
      "Test Epoch636 layer4 out_loss 0.1572382003068924, R2 0.0806879997253418\n",
      "Train 637 | out_loss 0.39470532536506653: 100%|█| 125/125 [00:00<00:00, 248.83it\n",
      "Train Epoch637 out_loss 0.1557922512292862, R2 0.08613377809524536\n",
      "Test Epoch637 layer0 out_loss 0.15934635698795319, R2 0.06836241483688354\n",
      "Test Epoch637 layer1 out_loss 0.15873205661773682, R2 0.07195401191711426\n",
      "Test Epoch637 layer2 out_loss 0.1574096381664276, R2 0.07968568801879883\n",
      "Test Epoch637 layer3 out_loss 0.1573212593793869, R2 0.08020240068435669\n",
      "Test Epoch637 layer4 out_loss 0.15741485357284546, R2 0.07965517044067383\n",
      "Train 638 | out_loss 0.3947222828865051: 100%|█| 125/125 [00:00<00:00, 245.69it/\n",
      "Train Epoch638 out_loss 0.15580566227436066, R2 0.08605509996414185\n",
      "Test Epoch638 layer0 out_loss 0.15921896696090698, R2 0.0691072940826416\n",
      "Test Epoch638 layer1 out_loss 0.15823839604854584, R2 0.074840247631073\n",
      "Test Epoch638 layer2 out_loss 0.15739111602306366, R2 0.07979398965835571\n",
      "Test Epoch638 layer3 out_loss 0.15742039680480957, R2 0.07962280511856079\n",
      "Test Epoch638 layer4 out_loss 0.15742087364196777, R2 0.07962006330490112\n",
      "Train 639 | out_loss 0.3947710394859314: 100%|█| 125/125 [00:00<00:00, 248.88it/\n",
      "Train Epoch639 out_loss 0.15584413707256317, R2 0.08582949638366699\n",
      "Test Epoch639 layer0 out_loss 0.15919965505599976, R2 0.06922012567520142\n",
      "Test Epoch639 layer1 out_loss 0.15821796655654907, R2 0.07495969533920288\n",
      "Test Epoch639 layer2 out_loss 0.15726277232170105, R2 0.0805443525314331\n",
      "Test Epoch639 layer3 out_loss 0.15709847211837769, R2 0.08150500059127808\n",
      "Test Epoch639 layer4 out_loss 0.1571573168039322, R2 0.08116090297698975\n",
      "Train 640 | out_loss 0.3946552574634552: 100%|█| 125/125 [00:00<00:00, 245.20it/\n",
      "Train Epoch640 out_loss 0.1557527333498001, R2 0.08636564016342163\n",
      "Test Epoch640 layer0 out_loss 0.1591421216726303, R2 0.06955653429031372\n",
      "Test Epoch640 layer1 out_loss 0.1585274487733841, R2 0.07315027713775635\n",
      "Test Epoch640 layer2 out_loss 0.15741746127605438, R2 0.0796399712562561\n",
      "Test Epoch640 layer3 out_loss 0.15723899006843567, R2 0.08068346977233887\n",
      "Test Epoch640 layer4 out_loss 0.15731604397296906, R2 0.08023291826248169\n",
      "Train 641 | out_loss 0.39480170607566833: 100%|█| 125/125 [00:00<00:00, 248.37it\n",
      "Train Epoch641 out_loss 0.15586838126182556, R2 0.08568727970123291\n",
      "Test Epoch641 layer0 out_loss 0.1591838300228119, R2 0.06931275129318237\n",
      "Test Epoch641 layer1 out_loss 0.15820670127868652, R2 0.07502561807632446\n",
      "Test Epoch641 layer2 out_loss 0.15722255408763885, R2 0.0807795524597168\n",
      "Test Epoch641 layer3 out_loss 0.157088041305542, R2 0.0815659761428833\n",
      "Test Epoch641 layer4 out_loss 0.15715384483337402, R2 0.08118128776550293\n",
      "Train 642 | out_loss 0.39461812376976013: 100%|█| 125/125 [00:00<00:00, 253.45it\n",
      "Train Epoch642 out_loss 0.15572349727153778, R2 0.08653712272644043\n",
      "Test Epoch642 layer0 out_loss 0.15931148827075958, R2 0.06856626272201538\n",
      "Test Epoch642 layer1 out_loss 0.15832532942295074, R2 0.07433199882507324\n",
      "Test Epoch642 layer2 out_loss 0.15769897401332855, R2 0.07799404859542847\n",
      "Test Epoch642 layer3 out_loss 0.15751370787620544, R2 0.07907730340957642\n",
      "Test Epoch642 layer4 out_loss 0.15760651230812073, R2 0.07853466272354126\n",
      "Train 643 | out_loss 0.39472827315330505: 100%|█| 125/125 [00:00<00:00, 256.45it\n",
      "Train Epoch643 out_loss 0.1558104008436203, R2 0.08602738380432129\n",
      "Test Epoch643 layer0 out_loss 0.1592855155467987, R2 0.06871813535690308\n",
      "Test Epoch643 layer1 out_loss 0.1591072976589203, R2 0.06976014375686646\n",
      "Test Epoch643 layer2 out_loss 0.15775874257087708, R2 0.07764464616775513\n",
      "Test Epoch643 layer3 out_loss 0.15760833024978638, R2 0.07852399349212646\n",
      "Test Epoch643 layer4 out_loss 0.15773513913154602, R2 0.07778263092041016\n",
      "Train 644 | out_loss 0.3946220874786377: 100%|█| 125/125 [00:00<00:00, 244.26it/\n",
      "Train Epoch644 out_loss 0.1557265669107437, R2 0.08651912212371826\n",
      "Test Epoch644 layer0 out_loss 0.1591629683971405, R2 0.06943470239639282\n",
      "Test Epoch644 layer1 out_loss 0.1582004725933075, R2 0.07506203651428223\n",
      "Test Epoch644 layer2 out_loss 0.15728506445884705, R2 0.0804140567779541\n",
      "Test Epoch644 layer3 out_loss 0.1571357548236847, R2 0.0812869668006897\n",
      "Test Epoch644 layer4 out_loss 0.1571524292230606, R2 0.08118945360183716\n",
      "Train 645 | out_loss 0.3947156071662903: 100%|█| 125/125 [00:00<00:00, 248.33it/\n",
      "Train Epoch645 out_loss 0.15580043196678162, R2 0.08608585596084595\n",
      "Test Epoch645 layer0 out_loss 0.15918517112731934, R2 0.06930488348007202\n",
      "Test Epoch645 layer1 out_loss 0.15820221602916718, R2 0.07505184412002563\n",
      "Test Epoch645 layer2 out_loss 0.15744297206401825, R2 0.0794907808303833\n",
      "Test Epoch645 layer3 out_loss 0.15726608037948608, R2 0.08052504062652588\n",
      "Test Epoch645 layer4 out_loss 0.15723836421966553, R2 0.08068710565567017\n",
      "Train 646 | out_loss 0.39465779066085815: 100%|█| 125/125 [00:00<00:00, 243.61it\n",
      "Train Epoch646 out_loss 0.1557547003030777, R2 0.0863540768623352\n",
      "Test Epoch646 layer0 out_loss 0.15916654467582703, R2 0.06941372156143188\n",
      "Test Epoch646 layer1 out_loss 0.15863430500030518, R2 0.0725255012512207\n",
      "Test Epoch646 layer2 out_loss 0.1574617624282837, R2 0.07938092947006226\n",
      "Test Epoch646 layer3 out_loss 0.15752984583377838, R2 0.0789828896522522\n",
      "Test Epoch646 layer4 out_loss 0.15742681920528412, R2 0.0795852541923523\n",
      "Train 647 | out_loss 0.394761323928833: 100%|█| 125/125 [00:00<00:00, 259.86it/s\n",
      "Train Epoch647 out_loss 0.15583646297454834, R2 0.08587449789047241\n",
      "Test Epoch647 layer0 out_loss 0.15921851992607117, R2 0.06910985708236694\n",
      "Test Epoch647 layer1 out_loss 0.1586846262216568, R2 0.07223129272460938\n",
      "Test Epoch647 layer2 out_loss 0.15730750560760498, R2 0.08028280735015869\n",
      "Test Epoch647 layer3 out_loss 0.15729117393493652, R2 0.08037835359573364\n",
      "Test Epoch647 layer4 out_loss 0.157284677028656, R2 0.08041626214981079\n",
      "Train 648 | out_loss 0.3946242332458496: 100%|█| 125/125 [00:00<00:00, 246.64it/\n",
      "Train Epoch648 out_loss 0.1557282954454422, R2 0.08650892972946167\n",
      "Test Epoch648 layer0 out_loss 0.1592501848936081, R2 0.06892472505569458\n",
      "Test Epoch648 layer1 out_loss 0.15816371142864227, R2 0.07527691125869751\n",
      "Test Epoch648 layer2 out_loss 0.1572214961051941, R2 0.08078569173812866\n",
      "Test Epoch648 layer3 out_loss 0.1571137011051178, R2 0.08141589164733887\n",
      "Test Epoch648 layer4 out_loss 0.15718208253383636, R2 0.08101612329483032\n",
      "Train 649 | out_loss 0.3947957456111908: 100%|█| 125/125 [00:00<00:00, 259.78it/\n",
      "Train Epoch649 out_loss 0.1558636873960495, R2 0.08571475744247437\n",
      "Test Epoch649 layer0 out_loss 0.1592760980129242, R2 0.06877326965332031\n",
      "Test Epoch649 layer1 out_loss 0.1583762913942337, R2 0.07403403520584106\n",
      "Test Epoch649 layer2 out_loss 0.1573566049337387, R2 0.07999581098556519\n",
      "Test Epoch649 layer3 out_loss 0.15718524158000946, R2 0.08099764585494995\n",
      "Test Epoch649 layer4 out_loss 0.1573610156774521, R2 0.07996994256973267\n",
      "Train 650 | out_loss 0.3946414291858673: 100%|█| 125/125 [00:00<00:00, 250.57it/\n",
      "Train Epoch650 out_loss 0.1557418555021286, R2 0.0864294171333313\n",
      "Test Epoch650 layer0 out_loss 0.15913017094135284, R2 0.06962639093399048\n",
      "Test Epoch650 layer1 out_loss 0.15815992653369904, R2 0.07529902458190918\n",
      "Test Epoch650 layer2 out_loss 0.15727318823337555, R2 0.08048343658447266\n",
      "Test Epoch650 layer3 out_loss 0.15716974437236786, R2 0.08108830451965332\n",
      "Test Epoch650 layer4 out_loss 0.157210573554039, R2 0.08084958791732788\n",
      "Train 651 | out_loss 0.39466390013694763: 100%|█| 125/125 [00:00<00:00, 249.75it\n",
      "Train Epoch651 out_loss 0.15575961768627167, R2 0.08632528781890869\n",
      "Test Epoch651 layer0 out_loss 0.15918177366256714, R2 0.06932467222213745\n",
      "Test Epoch651 layer1 out_loss 0.158309668302536, R2 0.07442361116409302\n",
      "Test Epoch651 layer2 out_loss 0.15737682580947876, R2 0.07987749576568604\n",
      "Test Epoch651 layer3 out_loss 0.1573495715856552, R2 0.08003681898117065\n",
      "Test Epoch651 layer4 out_loss 0.15738315880298615, R2 0.07984048128128052\n",
      "Train 652 | out_loss 0.39456191658973694: 100%|█| 125/125 [00:00<00:00, 245.83it\n",
      "Train Epoch652 out_loss 0.15567916631698608, R2 0.08679711818695068\n",
      "Test Epoch652 layer0 out_loss 0.15918172895908356, R2 0.06932497024536133\n",
      "Test Epoch652 layer1 out_loss 0.15854758024215698, R2 0.07303261756896973\n",
      "Test Epoch652 layer2 out_loss 0.15766531229019165, R2 0.0781908631324768\n",
      "Test Epoch652 layer3 out_loss 0.1578628420829773, R2 0.07703602313995361\n",
      "Test Epoch652 layer4 out_loss 0.15776696801185608, R2 0.07759654521942139\n",
      "Train 653 | out_loss 0.3946179449558258: 100%|█| 125/125 [00:00<00:00, 252.90it/\n",
      "Train Epoch653 out_loss 0.15572327375411987, R2 0.08653849363327026\n",
      "Test Epoch653 layer0 out_loss 0.1591373234987259, R2 0.06958454847335815\n",
      "Test Epoch653 layer1 out_loss 0.15827761590480804, R2 0.07461094856262207\n",
      "Test Epoch653 layer2 out_loss 0.157258540391922, R2 0.08056908845901489\n",
      "Test Epoch653 layer3 out_loss 0.1570844203233719, R2 0.08158707618713379\n",
      "Test Epoch653 layer4 out_loss 0.15716539323329926, R2 0.08111375570297241\n",
      "Train 654 | out_loss 0.3943479359149933: 100%|█| 125/125 [00:00<00:00, 258.15it/\n",
      "Train Epoch654 out_loss 0.15551024675369263, R2 0.08778798580169678\n",
      "Test Epoch654 layer0 out_loss 0.15942120552062988, R2 0.0679248571395874\n",
      "Test Epoch654 layer1 out_loss 0.15832211077213287, R2 0.07435083389282227\n",
      "Test Epoch654 layer2 out_loss 0.15784043073654175, R2 0.0771670937538147\n",
      "Test Epoch654 layer3 out_loss 0.15785923600196838, R2 0.07705706357955933\n",
      "Test Epoch654 layer4 out_loss 0.15798242390155792, R2 0.07633686065673828\n",
      "Train 655 | out_loss 0.39465683698654175: 100%|█| 125/125 [00:00<00:00, 252.62it\n",
      "Train Epoch655 out_loss 0.1557539701461792, R2 0.08635836839675903\n",
      "Test Epoch655 layer0 out_loss 0.15916134417057037, R2 0.06944411993026733\n",
      "Test Epoch655 layer1 out_loss 0.15816934406757355, R2 0.0752440094947815\n",
      "Test Epoch655 layer2 out_loss 0.1572781503200531, R2 0.08045446872711182\n",
      "Test Epoch655 layer3 out_loss 0.15708568692207336, R2 0.08157968521118164\n",
      "Test Epoch655 layer4 out_loss 0.15715356171131134, R2 0.08118289709091187\n",
      "Train 656 | out_loss 0.39452508091926575: 100%|█| 125/125 [00:00<00:00, 259.49it\n",
      "Train Epoch656 out_loss 0.15564998984336853, R2 0.0869683027267456\n",
      "Test Epoch656 layer0 out_loss 0.1592342108488083, R2 0.06901812553405762\n",
      "Test Epoch656 layer1 out_loss 0.15861214697360992, R2 0.0726550817489624\n",
      "Test Epoch656 layer2 out_loss 0.15774081647396088, R2 0.07774943113327026\n",
      "Test Epoch656 layer3 out_loss 0.157615527510643, R2 0.07848197221755981\n",
      "Test Epoch656 layer4 out_loss 0.15763400495052338, R2 0.07837390899658203\n",
      "Train 657 | out_loss 0.39461836218833923: 100%|█| 125/125 [00:00<00:00, 253.74it\n",
      "Train Epoch657 out_loss 0.15572363138198853, R2 0.08653628826141357\n",
      "Test Epoch657 layer0 out_loss 0.1591033786535263, R2 0.0697830319404602\n",
      "Test Epoch657 layer1 out_loss 0.15822048485279083, R2 0.07494497299194336\n",
      "Test Epoch657 layer2 out_loss 0.1573265939950943, R2 0.08017122745513916\n",
      "Test Epoch657 layer3 out_loss 0.1571599692106247, R2 0.08114540576934814\n",
      "Test Epoch657 layer4 out_loss 0.15721186995506287, R2 0.0808420181274414\n",
      "Train 658 | out_loss 0.39459145069122314: 100%|█| 125/125 [00:00<00:00, 238.29it\n",
      "Train Epoch658 out_loss 0.15570242702960968, R2 0.08666074275970459\n",
      "Test Epoch658 layer0 out_loss 0.15911078453063965, R2 0.06973975896835327\n",
      "Test Epoch658 layer1 out_loss 0.158219575881958, R2 0.07495033740997314\n",
      "Test Epoch658 layer2 out_loss 0.15726161003112793, R2 0.0805511474609375\n",
      "Test Epoch658 layer3 out_loss 0.15725521743297577, R2 0.08058851957321167\n",
      "Test Epoch658 layer4 out_loss 0.15721441805362701, R2 0.08082711696624756\n",
      "Train 659 | out_loss 0.39448899030685425: 100%|█| 125/125 [00:00<00:00, 249.37it\n",
      "Train Epoch659 out_loss 0.15562158823013306, R2 0.0871349573135376\n",
      "Test Epoch659 layer0 out_loss 0.15918207168579102, R2 0.06932300329208374\n",
      "Test Epoch659 layer1 out_loss 0.15903149545192719, R2 0.07020330429077148\n",
      "Test Epoch659 layer2 out_loss 0.15778440237045288, R2 0.0774945616722107\n",
      "Test Epoch659 layer3 out_loss 0.157769113779068, R2 0.07758396863937378\n",
      "Test Epoch659 layer4 out_loss 0.15777553617954254, R2 0.07754641771316528\n",
      "Train 660 | out_loss 0.3946981728076935: 100%|█| 125/125 [00:00<00:00, 256.62it/\n",
      "Train Epoch660 out_loss 0.15578658878803253, R2 0.08616703748703003\n",
      "Test Epoch660 layer0 out_loss 0.15909621119499207, R2 0.06982499361038208\n",
      "Test Epoch660 layer1 out_loss 0.1581970453262329, R2 0.07508200407028198\n",
      "Test Epoch660 layer2 out_loss 0.1572333127260208, R2 0.08071666955947876\n",
      "Test Epoch660 layer3 out_loss 0.15706361830234528, R2 0.08170878887176514\n",
      "Test Epoch660 layer4 out_loss 0.15714293718338013, R2 0.08124494552612305\n",
      "Train 661 | out_loss 0.39450040459632874: 100%|█| 125/125 [00:00<00:00, 255.50it\n",
      "Train Epoch661 out_loss 0.15563058853149414, R2 0.0870821475982666\n",
      "Test Epoch661 layer0 out_loss 0.15912748873233795, R2 0.0696420669555664\n",
      "Test Epoch661 layer1 out_loss 0.15829680860042572, R2 0.07449871301651001\n",
      "Test Epoch661 layer2 out_loss 0.1573105901479721, R2 0.08026480674743652\n",
      "Test Epoch661 layer3 out_loss 0.15721237659454346, R2 0.08083903789520264\n",
      "Test Epoch661 layer4 out_loss 0.15730167925357819, R2 0.08031690120697021\n",
      "Train 662 | out_loss 0.39480406045913696: 100%|█| 125/125 [00:00<00:00, 251.34it\n",
      "Train Epoch662 out_loss 0.15587027370929718, R2 0.08567613363265991\n",
      "Test Epoch662 layer0 out_loss 0.1591218113899231, R2 0.0696752667427063\n",
      "Test Epoch662 layer1 out_loss 0.15818767249584198, R2 0.07513684034347534\n",
      "Test Epoch662 layer2 out_loss 0.15724724531173706, R2 0.0806351900100708\n",
      "Test Epoch662 layer3 out_loss 0.157080739736557, R2 0.08160865306854248\n",
      "Test Epoch662 layer4 out_loss 0.15717585384845734, R2 0.08105254173278809\n",
      "Train 663 | out_loss 0.394573837518692: 100%|█| 125/125 [00:00<00:00, 246.33it/s\n",
      "Train Epoch663 out_loss 0.15568850934505463, R2 0.0867423415184021\n",
      "Test Epoch663 layer0 out_loss 0.15910787880420685, R2 0.06975674629211426\n",
      "Test Epoch663 layer1 out_loss 0.1582450121641159, R2 0.07480162382125854\n",
      "Test Epoch663 layer2 out_loss 0.1572665572166443, R2 0.08052223920822144\n",
      "Test Epoch663 layer3 out_loss 0.15707449615001678, R2 0.0816451907157898\n",
      "Test Epoch663 layer4 out_loss 0.15716446936130524, R2 0.08111906051635742\n",
      "Train 664 | out_loss 0.3944661617279053: 100%|█| 125/125 [00:00<00:00, 253.22it/\n",
      "Train Epoch664 out_loss 0.1556035578250885, R2 0.08724063634872437\n",
      "Test Epoch664 layer0 out_loss 0.1590973138809204, R2 0.06981849670410156\n",
      "Test Epoch664 layer1 out_loss 0.15819142758846283, R2 0.075114905834198\n",
      "Test Epoch664 layer2 out_loss 0.15726229548454285, R2 0.08054715394973755\n",
      "Test Epoch664 layer3 out_loss 0.15709975361824036, R2 0.08149755001068115\n",
      "Test Epoch664 layer4 out_loss 0.15717245638370514, R2 0.08107244968414307\n",
      "Train 665 | out_loss 0.3945156931877136: 100%|█| 125/125 [00:00<00:00, 248.05it/\n",
      "Train Epoch665 out_loss 0.15564262866973877, R2 0.08701151609420776\n",
      "Test Epoch665 layer0 out_loss 0.15908461809158325, R2 0.06989270448684692\n",
      "Test Epoch665 layer1 out_loss 0.1581224501132965, R2 0.07551813125610352\n",
      "Test Epoch665 layer2 out_loss 0.15726345777511597, R2 0.08054035902023315\n",
      "Test Epoch665 layer3 out_loss 0.15707510709762573, R2 0.08164161443710327\n",
      "Test Epoch665 layer4 out_loss 0.15718168020248413, R2 0.08101844787597656\n",
      "Train 666 | out_loss 0.3945840001106262: 100%|█| 125/125 [00:00<00:00, 253.56it/\n",
      "Train Epoch666 out_loss 0.1556965410709381, R2 0.08669519424438477\n",
      "Test Epoch666 layer0 out_loss 0.15929199755191803, R2 0.0686802864074707\n",
      "Test Epoch666 layer1 out_loss 0.15923810005187988, R2 0.0689954161643982\n",
      "Test Epoch666 layer2 out_loss 0.15769849717617035, R2 0.07799685001373291\n",
      "Test Epoch666 layer3 out_loss 0.15760692954063416, R2 0.07853221893310547\n",
      "Test Epoch666 layer4 out_loss 0.1577436625957489, R2 0.07773274183273315\n",
      "Train 667 | out_loss 0.3946225345134735: 100%|█| 125/125 [00:00<00:00, 245.77it/\n",
      "Train Epoch667 out_loss 0.15572698414325714, R2 0.0865166187286377\n",
      "Test Epoch667 layer0 out_loss 0.15914538502693176, R2 0.0695374608039856\n",
      "Test Epoch667 layer1 out_loss 0.15821516513824463, R2 0.07497608661651611\n",
      "Test Epoch667 layer2 out_loss 0.15720349550247192, R2 0.080890953540802\n",
      "Test Epoch667 layer3 out_loss 0.15708450973033905, R2 0.08158665895462036\n",
      "Test Epoch667 layer4 out_loss 0.1571510136127472, R2 0.08119773864746094\n",
      "Train 668 | out_loss 0.39446601271629333: 100%|█| 125/125 [00:00<00:00, 256.43it\n",
      "Train Epoch668 out_loss 0.15560342371463776, R2 0.08724147081375122\n",
      "Test Epoch668 layer0 out_loss 0.15915945172309875, R2 0.06945520639419556\n",
      "Test Epoch668 layer1 out_loss 0.15828870236873627, R2 0.07454615831375122\n",
      "Test Epoch668 layer2 out_loss 0.15758217871189117, R2 0.07867687940597534\n",
      "Test Epoch668 layer3 out_loss 0.1576714664697647, R2 0.07815492153167725\n",
      "Test Epoch668 layer4 out_loss 0.1575905680656433, R2 0.07862794399261475\n",
      "Train 669 | out_loss 0.3945784270763397: 100%|█| 125/125 [00:00<00:00, 242.70it/\n",
      "Train Epoch669 out_loss 0.15569214522838593, R2 0.08672106266021729\n",
      "Test Epoch669 layer0 out_loss 0.15912918746471405, R2 0.06963217258453369\n",
      "Test Epoch669 layer1 out_loss 0.15823468565940857, R2 0.07486200332641602\n",
      "Test Epoch669 layer2 out_loss 0.1572849154472351, R2 0.08041495084762573\n",
      "Test Epoch669 layer3 out_loss 0.15712860226631165, R2 0.08132880926132202\n",
      "Test Epoch669 layer4 out_loss 0.15722784399986267, R2 0.0807485580444336\n",
      "Train 670 | out_loss 0.3946027159690857: 100%|█| 125/125 [00:00<00:00, 256.45it/\n",
      "Train Epoch670 out_loss 0.1557113230228424, R2 0.08660852909088135\n",
      "Test Epoch670 layer0 out_loss 0.15910740196704865, R2 0.06975948810577393\n",
      "Test Epoch670 layer1 out_loss 0.15812687575817108, R2 0.075492262840271\n",
      "Test Epoch670 layer2 out_loss 0.15725111961364746, R2 0.08061254024505615\n",
      "Test Epoch670 layer3 out_loss 0.1570751965045929, R2 0.08164107799530029\n",
      "Test Epoch670 layer4 out_loss 0.15714868903160095, R2 0.0812113881111145\n",
      "Train 671 | out_loss 0.3945818245410919: 100%|█| 125/125 [00:00<00:00, 254.94it/\n",
      "Train Epoch671 out_loss 0.15569481253623962, R2 0.08670538663864136\n",
      "Test Epoch671 layer0 out_loss 0.15909433364868164, R2 0.06983596086502075\n",
      "Test Epoch671 layer1 out_loss 0.1581081748008728, R2 0.07560163736343384\n",
      "Test Epoch671 layer2 out_loss 0.15722714364528656, R2 0.0807526707649231\n",
      "Test Epoch671 layer3 out_loss 0.15705907344818115, R2 0.08173537254333496\n",
      "Test Epoch671 layer4 out_loss 0.15712592005729675, R2 0.08134454488754272\n",
      "Train 672 | out_loss 0.39463603496551514: 100%|█| 125/125 [00:00<00:00, 245.22it\n",
      "Train Epoch672 out_loss 0.15573757886886597, R2 0.08645451068878174\n",
      "Test Epoch672 layer0 out_loss 0.1590658575296402, R2 0.07000243663787842\n",
      "Test Epoch672 layer1 out_loss 0.15812264382839203, R2 0.07551699876785278\n",
      "Test Epoch672 layer2 out_loss 0.15720760822296143, R2 0.08086687326431274\n",
      "Test Epoch672 layer3 out_loss 0.15706664323806763, R2 0.08169102668762207\n",
      "Test Epoch672 layer4 out_loss 0.15709945559501648, R2 0.08149921894073486\n",
      "Train 673 | out_loss 0.3945590555667877: 100%|█| 125/125 [00:00<00:00, 251.85it/\n",
      "Train Epoch673 out_loss 0.15567685663700104, R2 0.08681076765060425\n",
      "Test Epoch673 layer0 out_loss 0.15908315777778625, R2 0.0699012279510498\n",
      "Test Epoch673 layer1 out_loss 0.1581086963415146, R2 0.07559853792190552\n",
      "Test Epoch673 layer2 out_loss 0.1572180688381195, R2 0.08080577850341797\n",
      "Test Epoch673 layer3 out_loss 0.1570780873298645, R2 0.0816240906715393\n",
      "Test Epoch673 layer4 out_loss 0.15715068578720093, R2 0.08119970560073853\n",
      "Train 674 | out_loss 0.39460399746894836: 100%|█| 125/125 [00:00<00:00, 250.21it\n",
      "Train Epoch674 out_loss 0.1557123064994812, R2 0.08660274744033813\n",
      "Test Epoch674 layer0 out_loss 0.15906590223312378, R2 0.07000213861465454\n",
      "Test Epoch674 layer1 out_loss 0.1583678126335144, R2 0.07408362627029419\n",
      "Test Epoch674 layer2 out_loss 0.15727874636650085, R2 0.08045095205307007\n",
      "Test Epoch674 layer3 out_loss 0.15718771517276764, R2 0.0809832215309143\n",
      "Test Epoch674 layer4 out_loss 0.15721476078033447, R2 0.08082503080368042\n",
      "Train 675 | out_loss 0.3944011628627777: 100%|█| 125/125 [00:00<00:00, 245.62it/\n",
      "Train Epoch675 out_loss 0.1555522382259369, R2 0.08754175901412964\n",
      "Test Epoch675 layer0 out_loss 0.15913863480091095, R2 0.06957685947418213\n",
      "Test Epoch675 layer1 out_loss 0.15831418335437775, R2 0.07439720630645752\n",
      "Test Epoch675 layer2 out_loss 0.15770916640758514, R2 0.07793444395065308\n",
      "Test Epoch675 layer3 out_loss 0.15748338401317596, R2 0.07925456762313843\n",
      "Test Epoch675 layer4 out_loss 0.15758302807807922, R2 0.07867199182510376\n",
      "Train 676 | out_loss 0.3945732116699219: 100%|█| 125/125 [00:00<00:00, 254.61it/\n",
      "Train Epoch676 out_loss 0.15568798780441284, R2 0.08674538135528564\n",
      "Test Epoch676 layer0 out_loss 0.15915420651435852, R2 0.06948590278625488\n",
      "Test Epoch676 layer1 out_loss 0.15821772813796997, R2 0.07496112585067749\n",
      "Test Epoch676 layer2 out_loss 0.15727205574512482, R2 0.0804901123046875\n",
      "Test Epoch676 layer3 out_loss 0.15710169076919556, R2 0.08148616552352905\n",
      "Test Epoch676 layer4 out_loss 0.15718287229537964, R2 0.08101147413253784\n",
      "Train 677 | out_loss 0.39462387561798096: 100%|█| 125/125 [00:00<00:00, 255.46it\n",
      "Train Epoch677 out_loss 0.1557280272245407, R2 0.0865105390548706\n",
      "Test Epoch677 layer0 out_loss 0.15913736820220947, R2 0.06958431005477905\n",
      "Test Epoch677 layer1 out_loss 0.15816201269626617, R2 0.07528680562973022\n",
      "Test Epoch677 layer2 out_loss 0.15724964439868927, R2 0.08062112331390381\n",
      "Test Epoch677 layer3 out_loss 0.1570928394794464, R2 0.08153796195983887\n",
      "Test Epoch677 layer4 out_loss 0.15711136162281036, R2 0.08142966032028198\n",
      "Train 678 | out_loss 0.3945375084877014: 100%|█| 125/125 [00:00<00:00, 256.41it/\n",
      "Train Epoch678 out_loss 0.15565982460975647, R2 0.0869106650352478\n",
      "Test Epoch678 layer0 out_loss 0.15920855104923248, R2 0.0691680908203125\n",
      "Test Epoch678 layer1 out_loss 0.15844200551509857, R2 0.07364988327026367\n",
      "Test Epoch678 layer2 out_loss 0.15737123787403107, R2 0.0799102783203125\n",
      "Test Epoch678 layer3 out_loss 0.15723587572574615, R2 0.08070164918899536\n",
      "Test Epoch678 layer4 out_loss 0.15729309618473053, R2 0.0803670883178711\n",
      "Train 679 | out_loss 0.39459073543548584: 100%|█| 125/125 [00:00<00:00, 252.98it\n",
      "Train Epoch679 out_loss 0.1557018905878067, R2 0.08666384220123291\n",
      "Test Epoch679 layer0 out_loss 0.15906119346618652, R2 0.07002973556518555\n",
      "Test Epoch679 layer1 out_loss 0.15806478261947632, R2 0.0758553147315979\n",
      "Test Epoch679 layer2 out_loss 0.15715710818767548, R2 0.08116215467453003\n",
      "Test Epoch679 layer3 out_loss 0.1569911539554596, R2 0.08213245868682861\n",
      "Test Epoch679 layer4 out_loss 0.15705826878547668, R2 0.08174002170562744\n",
      "Train 680 | out_loss 0.39453575015068054: 100%|█| 125/125 [00:00<00:00, 253.38it\n",
      "Train Epoch680 out_loss 0.15565845370292664, R2 0.08691871166229248\n",
      "Test Epoch680 layer0 out_loss 0.1590755581855774, R2 0.06994569301605225\n",
      "Test Epoch680 layer1 out_loss 0.15810789167881012, R2 0.07560324668884277\n",
      "Test Epoch680 layer2 out_loss 0.15714360773563385, R2 0.08124113082885742\n",
      "Test Epoch680 layer3 out_loss 0.1569773405790329, R2 0.08221316337585449\n",
      "Test Epoch680 layer4 out_loss 0.15705551207065582, R2 0.08175617456436157\n",
      "Train 681 | out_loss 0.39455538988113403: 100%|█| 125/125 [00:00<00:00, 247.12it\n",
      "Train Epoch681 out_loss 0.15567392110824585, R2 0.08682793378829956\n",
      "Test Epoch681 layer0 out_loss 0.15905547142028809, R2 0.07006317377090454\n",
      "Test Epoch681 layer1 out_loss 0.15811015665531158, R2 0.07559007406234741\n",
      "Test Epoch681 layer2 out_loss 0.15717440843582153, R2 0.08106100559234619\n",
      "Test Epoch681 layer3 out_loss 0.15708808600902557, R2 0.08156567811965942\n",
      "Test Epoch681 layer4 out_loss 0.15708765387535095, R2 0.08156824111938477\n",
      "Train 682 | out_loss 0.3945377767086029: 100%|█| 125/125 [00:00<00:00, 252.54it/\n",
      "Train Epoch682 out_loss 0.1556600034236908, R2 0.08690953254699707\n",
      "Test Epoch682 layer0 out_loss 0.15905357897281647, R2 0.07007426023483276\n",
      "Test Epoch682 layer1 out_loss 0.1582271158695221, R2 0.07490622997283936\n",
      "Test Epoch682 layer2 out_loss 0.15716341137886047, R2 0.08112531900405884\n",
      "Test Epoch682 layer3 out_loss 0.15697909891605377, R2 0.08220291137695312\n",
      "Test Epoch682 layer4 out_loss 0.15702688694000244, R2 0.0819234848022461\n",
      "Train 683 | out_loss 0.3944767713546753: 100%|█| 125/125 [00:00<00:00, 242.04it/\n",
      "Train Epoch683 out_loss 0.15561187267303467, R2 0.0871918797492981\n",
      "Test Epoch683 layer0 out_loss 0.15906375646591187, R2 0.07001471519470215\n",
      "Test Epoch683 layer1 out_loss 0.15833869576454163, R2 0.0742538571357727\n",
      "Test Epoch683 layer2 out_loss 0.1578790843486786, R2 0.07694101333618164\n",
      "Test Epoch683 layer3 out_loss 0.15762534737586975, R2 0.07842451333999634\n",
      "Test Epoch683 layer4 out_loss 0.1577892154455185, R2 0.07746648788452148\n",
      "Train 684 | out_loss 0.39465591311454773: 100%|█| 125/125 [00:00<00:00, 256.24it\n",
      "Train Epoch684 out_loss 0.15575329959392548, R2 0.08636230230331421\n",
      "Test Epoch684 layer0 out_loss 0.15903514623641968, R2 0.07018202543258667\n",
      "Test Epoch684 layer1 out_loss 0.1583402156829834, R2 0.07424497604370117\n",
      "Test Epoch684 layer2 out_loss 0.15728698670864105, R2 0.08040279150009155\n",
      "Test Epoch684 layer3 out_loss 0.15721207857131958, R2 0.08084076642990112\n",
      "Test Epoch684 layer4 out_loss 0.1571614146232605, R2 0.08113700151443481\n",
      "Train 685 | out_loss 0.394569456577301: 100%|█| 125/125 [00:00<00:00, 246.43it/s\n",
      "Train Epoch685 out_loss 0.15568502247333527, R2 0.08676278591156006\n",
      "Test Epoch685 layer0 out_loss 0.15903373062610626, R2 0.0701901912689209\n",
      "Test Epoch685 layer1 out_loss 0.15806204080581665, R2 0.07587134838104248\n",
      "Test Epoch685 layer2 out_loss 0.15725424885749817, R2 0.08059424161911011\n",
      "Test Epoch685 layer3 out_loss 0.15716736018657684, R2 0.08110219240188599\n",
      "Test Epoch685 layer4 out_loss 0.1571056842803955, R2 0.0814628005027771\n",
      "Train 686 | out_loss 0.39458543062210083: 100%|█| 125/125 [00:00<00:00, 256.97it\n",
      "Train Epoch686 out_loss 0.15569768846035004, R2 0.08668845891952515\n",
      "Test Epoch686 layer0 out_loss 0.15907645225524902, R2 0.06994050741195679\n",
      "Test Epoch686 layer1 out_loss 0.15806515514850616, R2 0.07585316896438599\n",
      "Test Epoch686 layer2 out_loss 0.15719300508499146, R2 0.0809522271156311\n",
      "Test Epoch686 layer3 out_loss 0.15701231360435486, R2 0.0820087194442749\n",
      "Test Epoch686 layer4 out_loss 0.15708649158477783, R2 0.08157503604888916\n",
      "Train 687 | out_loss 0.3943811357021332: 100%|█| 125/125 [00:00<00:00, 250.90it/\n",
      "Train Epoch687 out_loss 0.1555364727973938, R2 0.08763420581817627\n",
      "Test Epoch687 layer0 out_loss 0.15928064286708832, R2 0.06874668598175049\n",
      "Test Epoch687 layer1 out_loss 0.1584087312221527, R2 0.07384437322616577\n",
      "Test Epoch687 layer2 out_loss 0.1575472056865692, R2 0.07888138294219971\n",
      "Test Epoch687 layer3 out_loss 0.15740086138248444, R2 0.07973700761795044\n",
      "Test Epoch687 layer4 out_loss 0.15757714211940765, R2 0.07870632410049438\n",
      "Train 688 | out_loss 0.3944784700870514: 100%|█| 125/125 [00:00<00:00, 233.09it/\n",
      "Train Epoch688 out_loss 0.1556132733821869, R2 0.08718371391296387\n",
      "Test Epoch688 layer0 out_loss 0.15907205641269684, R2 0.06996619701385498\n",
      "Test Epoch688 layer1 out_loss 0.15813636779785156, R2 0.07543677091598511\n",
      "Test Epoch688 layer2 out_loss 0.15740269422531128, R2 0.07972627878189087\n",
      "Test Epoch688 layer3 out_loss 0.15714211761951447, R2 0.08124977350234985\n",
      "Test Epoch688 layer4 out_loss 0.15720851719379425, R2 0.08086162805557251\n",
      "Train 689 | out_loss 0.39457860589027405: 100%|█| 125/125 [00:00<00:00, 241.77it\n",
      "Train Epoch689 out_loss 0.15569227933883667, R2 0.08672022819519043\n",
      "Test Epoch689 layer0 out_loss 0.1590694934129715, R2 0.06998121738433838\n",
      "Test Epoch689 layer1 out_loss 0.15821325778961182, R2 0.07498723268508911\n",
      "Test Epoch689 layer2 out_loss 0.15740056335926056, R2 0.0797387957572937\n",
      "Test Epoch689 layer3 out_loss 0.15721683204174042, R2 0.08081299066543579\n",
      "Test Epoch689 layer4 out_loss 0.1573178768157959, R2 0.0802222490310669\n",
      "Train 690 | out_loss 0.39460089802742004: 100%|█| 125/125 [00:00<00:00, 251.70it\n",
      "Train Epoch690 out_loss 0.15570977330207825, R2 0.08661758899688721\n",
      "Test Epoch690 layer0 out_loss 0.15920673310756683, R2 0.06917881965637207\n",
      "Test Epoch690 layer1 out_loss 0.15836215019226074, R2 0.07411670684814453\n",
      "Test Epoch690 layer2 out_loss 0.1576007455587387, R2 0.07856839895248413\n",
      "Test Epoch690 layer3 out_loss 0.1574162244796753, R2 0.07964718341827393\n",
      "Test Epoch690 layer4 out_loss 0.15745724737644196, R2 0.07940739393234253\n",
      "Train 691 | out_loss 0.3946433663368225: 100%|█| 125/125 [00:00<00:00, 254.75it/\n",
      "Train Epoch691 out_loss 0.15574334561824799, R2 0.08642065525054932\n",
      "Test Epoch691 layer0 out_loss 0.15903335809707642, R2 0.07019245624542236\n",
      "Test Epoch691 layer1 out_loss 0.1580735594034195, R2 0.07580399513244629\n",
      "Test Epoch691 layer2 out_loss 0.15718020498752594, R2 0.081027090549469\n",
      "Test Epoch691 layer3 out_loss 0.15697817504405975, R2 0.08220827579498291\n",
      "Test Epoch691 layer4 out_loss 0.15702147781848907, R2 0.08195507526397705\n",
      "Train 692 | out_loss 0.394418865442276: 100%|█| 125/125 [00:00<00:00, 238.83it/s\n",
      "Train Epoch692 out_loss 0.15556620061397552, R2 0.0874597430229187\n",
      "Test Epoch692 layer0 out_loss 0.15902848541736603, R2 0.07022088766098022\n",
      "Test Epoch692 layer1 out_loss 0.15808504819869995, R2 0.07573682069778442\n",
      "Test Epoch692 layer2 out_loss 0.15726624429225922, R2 0.0805240273475647\n",
      "Test Epoch692 layer3 out_loss 0.15718920528888702, R2 0.08097445964813232\n",
      "Test Epoch692 layer4 out_loss 0.15721124410629272, R2 0.0808456540107727\n",
      "Train 693 | out_loss 0.3946603536605835: 100%|█| 125/125 [00:00<00:00, 249.10it/\n",
      "Train Epoch693 out_loss 0.15575675666332245, R2 0.0863419771194458\n",
      "Test Epoch693 layer0 out_loss 0.15902723371982574, R2 0.0702282190322876\n",
      "Test Epoch693 layer1 out_loss 0.1581142097711563, R2 0.0755663514137268\n",
      "Test Epoch693 layer2 out_loss 0.15723910927772522, R2 0.08068275451660156\n",
      "Test Epoch693 layer3 out_loss 0.15714026987552643, R2 0.0812605619430542\n",
      "Test Epoch693 layer4 out_loss 0.15711893141269684, R2 0.08138537406921387\n",
      "Train 694 | out_loss 0.3945123851299286: 100%|█| 125/125 [00:00<00:00, 235.04it/\n",
      "Train Epoch694 out_loss 0.15564002096652985, R2 0.08702683448791504\n",
      "Test Epoch694 layer0 out_loss 0.15904133021831512, R2 0.07014578580856323\n",
      "Test Epoch694 layer1 out_loss 0.15810078382492065, R2 0.075644850730896\n",
      "Test Epoch694 layer2 out_loss 0.15729954838752747, R2 0.08032935857772827\n",
      "Test Epoch694 layer3 out_loss 0.1570814847946167, R2 0.08160430192947388\n",
      "Test Epoch694 layer4 out_loss 0.15711554884910583, R2 0.08140510320663452\n",
      "Train 695 | out_loss 0.39442864060401917: 100%|█| 125/125 [00:00<00:00, 244.46it\n",
      "Train Epoch695 out_loss 0.15557394921779633, R2 0.08741438388824463\n",
      "Test Epoch695 layer0 out_loss 0.1590605527162552, R2 0.0700334906578064\n",
      "Test Epoch695 layer1 out_loss 0.15805433690547943, R2 0.07591640949249268\n",
      "Test Epoch695 layer2 out_loss 0.15717892348766327, R2 0.0810346007347107\n",
      "Test Epoch695 layer3 out_loss 0.15696118772029877, R2 0.08230763673782349\n",
      "Test Epoch695 layer4 out_loss 0.15700598061084747, R2 0.08204573392868042\n",
      "Train 696 | out_loss 0.39449697732925415: 100%|█| 125/125 [00:00<00:00, 251.55it\n",
      "Train Epoch696 out_loss 0.15562781691551208, R2 0.08709836006164551\n",
      "Test Epoch696 layer0 out_loss 0.1590103805065155, R2 0.0703268051147461\n",
      "Test Epoch696 layer1 out_loss 0.15811650454998016, R2 0.07555294036865234\n",
      "Test Epoch696 layer2 out_loss 0.15725503861904144, R2 0.08058959245681763\n",
      "Test Epoch696 layer3 out_loss 0.15702182054519653, R2 0.08195310831069946\n",
      "Test Epoch696 layer4 out_loss 0.15706196427345276, R2 0.08171838521957397\n",
      "Train 697 | out_loss 0.39444687962532043: 100%|█| 125/125 [00:00<00:00, 248.82it\n",
      "Train Epoch697 out_loss 0.1555883288383484, R2 0.08732998371124268\n",
      "Test Epoch697 layer0 out_loss 0.15904831886291504, R2 0.07010495662689209\n",
      "Test Epoch697 layer1 out_loss 0.15812234580516815, R2 0.07551878690719604\n",
      "Test Epoch697 layer2 out_loss 0.15717872977256775, R2 0.08103573322296143\n",
      "Test Epoch697 layer3 out_loss 0.15700729191303253, R2 0.0820380449295044\n",
      "Test Epoch697 layer4 out_loss 0.15710008144378662, R2 0.08149558305740356\n",
      "Train 698 | out_loss 0.39448076486587524: 100%|█| 125/125 [00:00<00:00, 257.09it\n",
      "Train Epoch698 out_loss 0.15561506152153015, R2 0.08717316389083862\n",
      "Test Epoch698 layer0 out_loss 0.15904846787452698, R2 0.07010412216186523\n",
      "Test Epoch698 layer1 out_loss 0.1580268293619156, R2 0.07607722282409668\n",
      "Test Epoch698 layer2 out_loss 0.1571735441684723, R2 0.0810660719871521\n",
      "Test Epoch698 layer3 out_loss 0.15712052583694458, R2 0.08137601613998413\n",
      "Test Epoch698 layer4 out_loss 0.1571044921875, R2 0.08146977424621582\n",
      "Train 699 | out_loss 0.3946233093738556: 100%|█| 125/125 [00:00<00:00, 238.31it/\n",
      "Train Epoch699 out_loss 0.1557275354862213, R2 0.08651340007781982\n",
      "Test Epoch699 layer0 out_loss 0.15911687910556793, R2 0.06970411539077759\n",
      "Test Epoch699 layer1 out_loss 0.15916113555431366, R2 0.06944537162780762\n",
      "Test Epoch699 layer2 out_loss 0.15801484882831573, R2 0.07614725828170776\n",
      "Test Epoch699 layer3 out_loss 0.15780650079250336, R2 0.07736539840698242\n",
      "Test Epoch699 layer4 out_loss 0.15789051353931427, R2 0.07687419652938843\n",
      "Train 700 | out_loss 0.39459192752838135: 100%|█| 125/125 [00:00<00:00, 252.55it\n",
      "Train Epoch700 out_loss 0.15570278465747833, R2 0.08665859699249268\n",
      "Test Epoch700 layer0 out_loss 0.1590055525302887, R2 0.07035499811172485\n",
      "Test Epoch700 layer1 out_loss 0.15827840566635132, R2 0.07460629940032959\n",
      "Test Epoch700 layer2 out_loss 0.15720109641551971, R2 0.08090496063232422\n",
      "Test Epoch700 layer3 out_loss 0.1569967418909073, R2 0.0820997953414917\n",
      "Test Epoch700 layer4 out_loss 0.15699920058250427, R2 0.08208537101745605\n",
      "Train 701 | out_loss 0.39446279406547546: 100%|█| 125/125 [00:00<00:00, 261.58it\n",
      "Train Epoch701 out_loss 0.155600905418396, R2 0.08725619316101074\n",
      "Test Epoch701 layer0 out_loss 0.1590246856212616, R2 0.07024312019348145\n",
      "Test Epoch701 layer1 out_loss 0.15817351639270782, R2 0.07521957159042358\n",
      "Test Epoch701 layer2 out_loss 0.15735512971878052, R2 0.08000439405441284\n",
      "Test Epoch701 layer3 out_loss 0.15725377202033997, R2 0.08059698343276978\n",
      "Test Epoch701 layer4 out_loss 0.15734723210334778, R2 0.08005058765411377\n",
      "Train 702 | out_loss 0.39444291591644287: 100%|█| 125/125 [00:00<00:00, 252.54it\n",
      "Train Epoch702 out_loss 0.15558519959449768, R2 0.0873483419418335\n",
      "Test Epoch702 layer0 out_loss 0.15901285409927368, R2 0.07031232118606567\n",
      "Test Epoch702 layer1 out_loss 0.15803158283233643, R2 0.07604938745498657\n",
      "Test Epoch702 layer2 out_loss 0.15716008841991425, R2 0.08114469051361084\n",
      "Test Epoch702 layer3 out_loss 0.15698406100273132, R2 0.08217394351959229\n",
      "Test Epoch702 layer4 out_loss 0.15703034400939941, R2 0.08190327882766724\n",
      "Train 703 | out_loss 0.3945275843143463: 100%|█| 125/125 [00:00<00:00, 253.04it/\n",
      "Train Epoch703 out_loss 0.1556520164012909, R2 0.08695638179779053\n",
      "Test Epoch703 layer0 out_loss 0.15904773771762848, R2 0.07010829448699951\n",
      "Test Epoch703 layer1 out_loss 0.15806350111961365, R2 0.07586276531219482\n",
      "Test Epoch703 layer2 out_loss 0.15713448822498322, R2 0.08129435777664185\n",
      "Test Epoch703 layer3 out_loss 0.1569775938987732, R2 0.08221167325973511\n",
      "Test Epoch703 layer4 out_loss 0.1570596992969513, R2 0.08173167705535889\n",
      "Train 704 | out_loss 0.3944512903690338: 100%|█| 125/125 [00:00<00:00, 259.20it/\n",
      "Train Epoch704 out_loss 0.15559180080890656, R2 0.08730965852737427\n",
      "Test Epoch704 layer0 out_loss 0.15903258323669434, R2 0.07019698619842529\n",
      "Test Epoch704 layer1 out_loss 0.15811416506767273, R2 0.07556658983230591\n",
      "Test Epoch704 layer2 out_loss 0.15727674961090088, R2 0.08046263456344604\n",
      "Test Epoch704 layer3 out_loss 0.1570414900779724, R2 0.08183813095092773\n",
      "Test Epoch704 layer4 out_loss 0.15708480775356293, R2 0.0815848708152771\n",
      "Train 705 | out_loss 0.39451175928115845: 100%|█| 125/125 [00:00<00:00, 250.00it\n",
      "Train Epoch705 out_loss 0.15563955903053284, R2 0.08702951669692993\n",
      "Test Epoch705 layer0 out_loss 0.15905028581619263, R2 0.07009339332580566\n",
      "Test Epoch705 layer1 out_loss 0.1580842286348343, R2 0.07574164867401123\n",
      "Test Epoch705 layer2 out_loss 0.15728618204593658, R2 0.08040750026702881\n",
      "Test Epoch705 layer3 out_loss 0.1570655256509781, R2 0.08169764280319214\n",
      "Test Epoch705 layer4 out_loss 0.1571820229291916, R2 0.08101648092269897\n",
      "Train 706 | out_loss 0.39444196224212646: 100%|█| 125/125 [00:00<00:00, 256.24it\n",
      "Train Epoch706 out_loss 0.155584454536438, R2 0.0873526930809021\n",
      "Test Epoch706 layer0 out_loss 0.15899839997291565, R2 0.07039684057235718\n",
      "Test Epoch706 layer1 out_loss 0.15807737410068512, R2 0.07578170299530029\n",
      "Test Epoch706 layer2 out_loss 0.15721993148326874, R2 0.08079487085342407\n",
      "Test Epoch706 layer3 out_loss 0.15703946352005005, R2 0.08184999227523804\n",
      "Test Epoch706 layer4 out_loss 0.15700754523277283, R2 0.08203661441802979\n",
      "Train 707 | out_loss 0.3944542109966278: 100%|█| 125/125 [00:00<00:00, 250.78it/\n",
      "Train Epoch707 out_loss 0.1555940955877304, R2 0.08729612827301025\n",
      "Test Epoch707 layer0 out_loss 0.15899410843849182, R2 0.07042187452316284\n",
      "Test Epoch707 layer1 out_loss 0.15862683951854706, R2 0.07256913185119629\n",
      "Test Epoch707 layer2 out_loss 0.15735973417758942, R2 0.07997745275497437\n",
      "Test Epoch707 layer3 out_loss 0.15732131898403168, R2 0.08020204305648804\n",
      "Test Epoch707 layer4 out_loss 0.15720303356647491, R2 0.0808936357498169\n",
      "Train 708 | out_loss 0.39441466331481934: 100%|█| 125/125 [00:00<00:00, 249.54it\n",
      "Train Epoch708 out_loss 0.1555628627538681, R2 0.0874793529510498\n",
      "Test Epoch708 layer0 out_loss 0.15904180705547333, R2 0.07014304399490356\n",
      "Test Epoch708 layer1 out_loss 0.1580400913953781, R2 0.07599973678588867\n",
      "Test Epoch708 layer2 out_loss 0.1571779102087021, R2 0.08104050159454346\n",
      "Test Epoch708 layer3 out_loss 0.15700463950634003, R2 0.08205360174179077\n",
      "Test Epoch708 layer4 out_loss 0.15702715516090393, R2 0.08192187547683716\n",
      "Train 709 | out_loss 0.3943724036216736: 100%|█| 125/125 [00:00<00:00, 254.50it/\n",
      "Train Epoch709 out_loss 0.15552961826324463, R2 0.08767437934875488\n",
      "Test Epoch709 layer0 out_loss 0.15900097787380219, R2 0.070381760597229\n",
      "Test Epoch709 layer1 out_loss 0.15805573761463165, R2 0.07590818405151367\n",
      "Test Epoch709 layer2 out_loss 0.15721485018730164, R2 0.08082461357116699\n",
      "Test Epoch709 layer3 out_loss 0.15707701444625854, R2 0.08163046836853027\n",
      "Test Epoch709 layer4 out_loss 0.15709847211837769, R2 0.08150500059127808\n",
      "Train 710 | out_loss 0.39424583315849304: 100%|█| 125/125 [00:00<00:00, 236.66it\n",
      "Train Epoch710 out_loss 0.15542979538440704, R2 0.08825993537902832\n",
      "Test Epoch710 layer0 out_loss 0.15917004644870758, R2 0.06939327716827393\n",
      "Test Epoch710 layer1 out_loss 0.15815307199954987, R2 0.07533907890319824\n",
      "Test Epoch710 layer2 out_loss 0.15722326934337616, R2 0.08077532052993774\n",
      "Test Epoch710 layer3 out_loss 0.15705767273902893, R2 0.08174347877502441\n",
      "Test Epoch710 layer4 out_loss 0.15714354813098907, R2 0.08124136924743652\n",
      "Train 711 | out_loss 0.39437583088874817: 100%|█| 125/125 [00:00<00:00, 242.70it\n",
      "Train Epoch711 out_loss 0.15553224086761475, R2 0.08765900135040283\n",
      "Test Epoch711 layer0 out_loss 0.15899385511875153, R2 0.070423424243927\n",
      "Test Epoch711 layer1 out_loss 0.15807417035102844, R2 0.07580041885375977\n",
      "Test Epoch711 layer2 out_loss 0.15716023743152618, R2 0.08114385604858398\n",
      "Test Epoch711 layer3 out_loss 0.15696848928928375, R2 0.08226490020751953\n",
      "Test Epoch711 layer4 out_loss 0.1569974571466446, R2 0.08209562301635742\n",
      "Train 712 | out_loss 0.3945440948009491: 100%|█| 125/125 [00:00<00:00, 255.93it/\n",
      "Train Epoch712 out_loss 0.15566504001617432, R2 0.08688002824783325\n",
      "Test Epoch712 layer0 out_loss 0.1589953601360321, R2 0.07041460275650024\n",
      "Test Epoch712 layer1 out_loss 0.15805670619010925, R2 0.07590258121490479\n",
      "Test Epoch712 layer2 out_loss 0.15716269612312317, R2 0.08112949132919312\n",
      "Test Epoch712 layer3 out_loss 0.1570112407207489, R2 0.08201497793197632\n",
      "Test Epoch712 layer4 out_loss 0.1570086032152176, R2 0.08203041553497314\n",
      "Train 713 | out_loss 0.39441418647766113: 100%|█| 125/125 [00:00<00:00, 258.67it\n",
      "Train Epoch713 out_loss 0.15556249022483826, R2 0.08748161792755127\n",
      "Test Epoch713 layer0 out_loss 0.1589958220720291, R2 0.07041192054748535\n",
      "Test Epoch713 layer1 out_loss 0.15801307559013367, R2 0.07615762948989868\n",
      "Test Epoch713 layer2 out_loss 0.15714812278747559, R2 0.08121472597122192\n",
      "Test Epoch713 layer3 out_loss 0.15693898499011993, R2 0.08243745565414429\n",
      "Test Epoch713 layer4 out_loss 0.15699833631515503, R2 0.08209043741226196\n",
      "Train 714 | out_loss 0.39438411593437195: 100%|█| 125/125 [00:00<00:00, 254.71it\n",
      "Train Epoch714 out_loss 0.15553884208202362, R2 0.08762025833129883\n",
      "Test Epoch714 layer0 out_loss 0.15899038314819336, R2 0.07044368982315063\n",
      "Test Epoch714 layer1 out_loss 0.15811803936958313, R2 0.07554394006729126\n",
      "Test Epoch714 layer2 out_loss 0.15711550414562225, R2 0.08140534162521362\n",
      "Test Epoch714 layer3 out_loss 0.1569403111934662, R2 0.08242970705032349\n",
      "Test Epoch714 layer4 out_loss 0.15696929395198822, R2 0.08226019144058228\n",
      "Train 715 | out_loss 0.3943547308444977: 100%|█| 125/125 [00:00<00:00, 243.33it/\n",
      "Train Epoch715 out_loss 0.1555156707763672, R2 0.0877562165260315\n",
      "Test Epoch715 layer0 out_loss 0.1591409295797348, R2 0.06956350803375244\n",
      "Test Epoch715 layer1 out_loss 0.15855099260807037, R2 0.07301265001296997\n",
      "Test Epoch715 layer2 out_loss 0.1579182744026184, R2 0.07671189308166504\n",
      "Test Epoch715 layer3 out_loss 0.15770407021045685, R2 0.07796424627304077\n",
      "Test Epoch715 layer4 out_loss 0.1578497588634491, R2 0.07711249589920044\n",
      "Train 716 | out_loss 0.39456605911254883: 100%|█| 125/125 [00:00<00:00, 254.43it\n",
      "Train Epoch716 out_loss 0.15568237006664276, R2 0.08677834272384644\n",
      "Test Epoch716 layer0 out_loss 0.15901309251785278, R2 0.07031089067459106\n",
      "Test Epoch716 layer1 out_loss 0.15812508761882782, R2 0.07550269365310669\n",
      "Test Epoch716 layer2 out_loss 0.15724851191043854, R2 0.08062779903411865\n",
      "Test Epoch716 layer3 out_loss 0.15702873468399048, R2 0.08191269636154175\n",
      "Test Epoch716 layer4 out_loss 0.1570301055908203, R2 0.08190470933914185\n",
      "Train 717 | out_loss 0.3944476246833801: 100%|█| 125/125 [00:00<00:00, 257.44it/\n",
      "Train Epoch717 out_loss 0.15558886528015137, R2 0.08732682466506958\n",
      "Test Epoch717 layer0 out_loss 0.15899024903774261, R2 0.07044446468353271\n",
      "Test Epoch717 layer1 out_loss 0.15806138515472412, R2 0.0758751630783081\n",
      "Test Epoch717 layer2 out_loss 0.15737181901931763, R2 0.07990676164627075\n",
      "Test Epoch717 layer3 out_loss 0.1572226583957672, R2 0.08077889680862427\n",
      "Test Epoch717 layer4 out_loss 0.1571933925151825, R2 0.08095002174377441\n",
      "Train 718 | out_loss 0.394405722618103: 100%|█| 125/125 [00:00<00:00, 254.64it/s\n",
      "Train Epoch718 out_loss 0.155555859208107, R2 0.08752042055130005\n",
      "Test Epoch718 layer0 out_loss 0.15924468636512756, R2 0.06895685195922852\n",
      "Test Epoch718 layer1 out_loss 0.15815779566764832, R2 0.07531154155731201\n",
      "Test Epoch718 layer2 out_loss 0.15735970437526703, R2 0.07997763156890869\n",
      "Test Epoch718 layer3 out_loss 0.15709203481674194, R2 0.08154255151748657\n",
      "Test Epoch718 layer4 out_loss 0.15710121393203735, R2 0.0814889669418335\n",
      "Train 719 | out_loss 0.39443492889404297: 100%|█| 125/125 [00:00<00:00, 241.50it\n",
      "Train Epoch719 out_loss 0.1555788815021515, R2 0.08738535642623901\n",
      "Test Epoch719 layer0 out_loss 0.15897682309150696, R2 0.0705229640007019\n",
      "Test Epoch719 layer1 out_loss 0.15799875557422638, R2 0.0762413740158081\n",
      "Test Epoch719 layer2 out_loss 0.1571415364742279, R2 0.08125317096710205\n",
      "Test Epoch719 layer3 out_loss 0.15695703029632568, R2 0.08233189582824707\n",
      "Test Epoch719 layer4 out_loss 0.15694431960582733, R2 0.08240622282028198\n",
      "Train 720 | out_loss 0.39438533782958984: 100%|█| 125/125 [00:00<00:00, 250.31it\n",
      "Train Epoch720 out_loss 0.15553979575634003, R2 0.08761471509933472\n",
      "Test Epoch720 layer0 out_loss 0.15897369384765625, R2 0.07054126262664795\n",
      "Test Epoch720 layer1 out_loss 0.1580672413110733, R2 0.07584095001220703\n",
      "Test Epoch720 layer2 out_loss 0.15719866752624512, R2 0.08091914653778076\n",
      "Test Epoch720 layer3 out_loss 0.15697413682937622, R2 0.08223193883895874\n",
      "Test Epoch720 layer4 out_loss 0.15698766708374023, R2 0.08215278387069702\n",
      "Train 721 | out_loss 0.3944544494152069: 100%|█| 125/125 [00:00<00:00, 254.03it/\n",
      "Train Epoch721 out_loss 0.15559430420398712, R2 0.08729499578475952\n",
      "Test Epoch721 layer0 out_loss 0.15914110839366913, R2 0.06956243515014648\n",
      "Test Epoch721 layer1 out_loss 0.15825186669826508, R2 0.07476156949996948\n",
      "Test Epoch721 layer2 out_loss 0.15715590119361877, R2 0.0811692476272583\n",
      "Test Epoch721 layer3 out_loss 0.1569730043411255, R2 0.08223849534988403\n",
      "Test Epoch721 layer4 out_loss 0.15700823068618774, R2 0.08203262090682983\n",
      "Train 722 | out_loss 0.39440736174583435: 100%|█| 125/125 [00:00<00:00, 249.26it\n",
      "Train Epoch722 out_loss 0.15555718541145325, R2 0.08751267194747925\n",
      "Test Epoch722 layer0 out_loss 0.15900757908821106, R2 0.07034313678741455\n",
      "Test Epoch722 layer1 out_loss 0.15827146172523499, R2 0.07464689016342163\n",
      "Test Epoch722 layer2 out_loss 0.15718890726566315, R2 0.08097624778747559\n",
      "Test Epoch722 layer3 out_loss 0.15708503127098083, R2 0.08158349990844727\n",
      "Test Epoch722 layer4 out_loss 0.1570606529712677, R2 0.08172607421875\n",
      "Train 723 | out_loss 0.39439165592193604: 100%|█| 125/125 [00:00<00:00, 260.61it\n",
      "Train Epoch723 out_loss 0.15554474294185638, R2 0.0875856876373291\n",
      "Test Epoch723 layer0 out_loss 0.15895986557006836, R2 0.07062208652496338\n",
      "Test Epoch723 layer1 out_loss 0.15809570252895355, R2 0.07567453384399414\n",
      "Test Epoch723 layer2 out_loss 0.15733790397644043, R2 0.08010506629943848\n",
      "Test Epoch723 layer3 out_loss 0.1570698767900467, R2 0.08167219161987305\n",
      "Test Epoch723 layer4 out_loss 0.1571456491947174, R2 0.08122915029525757\n",
      "Train 724 | out_loss 0.3944489657878876: 100%|█| 125/125 [00:00<00:00, 238.31it/\n",
      "Train Epoch724 out_loss 0.1555900275707245, R2 0.08732002973556519\n",
      "Test Epoch724 layer0 out_loss 0.15899989008903503, R2 0.0703880786895752\n",
      "Test Epoch724 layer1 out_loss 0.15821607410907745, R2 0.07497072219848633\n",
      "Test Epoch724 layer2 out_loss 0.15733648836612701, R2 0.08011341094970703\n",
      "Test Epoch724 layer3 out_loss 0.15714794397354126, R2 0.08121567964553833\n",
      "Test Epoch724 layer4 out_loss 0.1572059839963913, R2 0.0808764100074768\n",
      "Train 725 | out_loss 0.3943379819393158: 100%|█| 125/125 [00:00<00:00, 247.63it/\n",
      "Train Epoch725 out_loss 0.15550242364406586, R2 0.08783388137817383\n",
      "Test Epoch725 layer0 out_loss 0.15895777940750122, R2 0.07063430547714233\n",
      "Test Epoch725 layer1 out_loss 0.15797735750675201, R2 0.07636648416519165\n",
      "Test Epoch725 layer2 out_loss 0.1571669727563858, R2 0.08110445737838745\n",
      "Test Epoch725 layer3 out_loss 0.15694856643676758, R2 0.08238142728805542\n",
      "Test Epoch725 layer4 out_loss 0.1569194495677948, R2 0.08255165815353394\n",
      "Train 726 | out_loss 0.39441198110580444: 100%|█| 125/125 [00:00<00:00, 252.83it\n",
      "Train Epoch726 out_loss 0.15556076169013977, R2 0.08749169111251831\n",
      "Test Epoch726 layer0 out_loss 0.15896445512771606, R2 0.07059526443481445\n",
      "Test Epoch726 layer1 out_loss 0.15800030529499054, R2 0.07623231410980225\n",
      "Test Epoch726 layer2 out_loss 0.15712502598762512, R2 0.08134967088699341\n",
      "Test Epoch726 layer3 out_loss 0.15692946314811707, R2 0.0824931263923645\n",
      "Test Epoch726 layer4 out_loss 0.1569460928440094, R2 0.08239591121673584\n",
      "Train 727 | out_loss 0.3943100571632385: 100%|█| 125/125 [00:00<00:00, 258.56it/\n",
      "Train Epoch727 out_loss 0.15548042953014374, R2 0.08796298503875732\n",
      "Test Epoch727 layer0 out_loss 0.1590241938829422, R2 0.07024598121643066\n",
      "Test Epoch727 layer1 out_loss 0.15798154473304749, R2 0.07634192705154419\n",
      "Test Epoch727 layer2 out_loss 0.1571437567472458, R2 0.08124023675918579\n",
      "Test Epoch727 layer3 out_loss 0.1570267230272293, R2 0.08192449808120728\n",
      "Test Epoch727 layer4 out_loss 0.1570628136396408, R2 0.08171343803405762\n",
      "Train 728 | out_loss 0.3944531977176666: 100%|█| 125/125 [00:00<00:00, 246.78it/\n",
      "Train Epoch728 out_loss 0.15559326112270355, R2 0.08730107545852661\n",
      "Test Epoch728 layer0 out_loss 0.15898330509662628, R2 0.07048505544662476\n",
      "Test Epoch728 layer1 out_loss 0.15811879932880402, R2 0.0755394697189331\n",
      "Test Epoch728 layer2 out_loss 0.15715573728084564, R2 0.08117020130157471\n",
      "Test Epoch728 layer3 out_loss 0.15704472362995148, R2 0.08181923627853394\n",
      "Test Epoch728 layer4 out_loss 0.1570453941822052, R2 0.08181530237197876\n",
      "Train 729 | out_loss 0.39434900879859924: 100%|█| 125/125 [00:00<00:00, 248.75it\n",
      "Train Epoch729 out_loss 0.15551114082336426, R2 0.08778280019760132\n",
      "Test Epoch729 layer0 out_loss 0.1589616984128952, R2 0.07061135768890381\n",
      "Test Epoch729 layer1 out_loss 0.15816272795200348, R2 0.07528263330459595\n",
      "Test Epoch729 layer2 out_loss 0.15725135803222656, R2 0.08061110973358154\n",
      "Test Epoch729 layer3 out_loss 0.1571231335401535, R2 0.08136075735092163\n",
      "Test Epoch729 layer4 out_loss 0.15707245469093323, R2 0.08165711164474487\n",
      "Train 730 | out_loss 0.3942285478115082: 100%|█| 125/125 [00:00<00:00, 244.81it/\n",
      "Train Epoch730 out_loss 0.1554160863161087, R2 0.08834034204483032\n",
      "Test Epoch730 layer0 out_loss 0.15912124514579773, R2 0.06967860460281372\n",
      "Test Epoch730 layer1 out_loss 0.15798638761043549, R2 0.07631367444992065\n",
      "Test Epoch730 layer2 out_loss 0.15714095532894135, R2 0.08125656843185425\n",
      "Test Epoch730 layer3 out_loss 0.15695594251155853, R2 0.08233827352523804\n",
      "Test Epoch730 layer4 out_loss 0.15700699388980865, R2 0.08203983306884766\n",
      "Train 731 | out_loss 0.3943714201450348: 100%|█| 125/125 [00:00<00:00, 249.08it/\n",
      "Train Epoch731 out_loss 0.15552878379821777, R2 0.08767932653427124\n",
      "Test Epoch731 layer0 out_loss 0.15896277129650116, R2 0.07060515880584717\n",
      "Test Epoch731 layer1 out_loss 0.1579798310995102, R2 0.07635200023651123\n",
      "Test Epoch731 layer2 out_loss 0.15710219740867615, R2 0.08148318529129028\n",
      "Test Epoch731 layer3 out_loss 0.15695039927959442, R2 0.08237075805664062\n",
      "Test Epoch731 layer4 out_loss 0.15697547793388367, R2 0.08222407102584839\n",
      "Train 732 | out_loss 0.3943149149417877: 100%|█| 125/125 [00:00<00:00, 250.02it/\n",
      "Train Epoch732 out_loss 0.15548425912857056, R2 0.087940514087677\n",
      "Test Epoch732 layer0 out_loss 0.15901526808738708, R2 0.07029813528060913\n",
      "Test Epoch732 layer1 out_loss 0.15802854299545288, R2 0.07606714963912964\n",
      "Test Epoch732 layer2 out_loss 0.15710346400737762, R2 0.08147573471069336\n",
      "Test Epoch732 layer3 out_loss 0.15686675906181335, R2 0.08285969495773315\n",
      "Test Epoch732 layer4 out_loss 0.1568736433982849, R2 0.08281946182250977\n",
      "Train 733 | out_loss 0.39430058002471924: 100%|█| 125/125 [00:00<00:00, 253.97it\n",
      "Train Epoch733 out_loss 0.15547293424606323, R2 0.08800691366195679\n",
      "Test Epoch733 layer0 out_loss 0.15898124873638153, R2 0.07049709558486938\n",
      "Test Epoch733 layer1 out_loss 0.15797004103660583, R2 0.0764092206954956\n",
      "Test Epoch733 layer2 out_loss 0.15714283287525177, R2 0.08124560117721558\n",
      "Test Epoch733 layer3 out_loss 0.15704740583896637, R2 0.08180350065231323\n",
      "Test Epoch733 layer4 out_loss 0.1570271998643875, R2 0.08192163705825806\n",
      "Train 734 | out_loss 0.39433395862579346: 100%|█| 125/125 [00:00<00:00, 239.86it\n",
      "Train Epoch734 out_loss 0.15549927949905396, R2 0.0878523588180542\n",
      "Test Epoch734 layer0 out_loss 0.1589503437280655, R2 0.0706777572631836\n",
      "Test Epoch734 layer1 out_loss 0.1579834222793579, R2 0.07633095979690552\n",
      "Test Epoch734 layer2 out_loss 0.15706278383731842, R2 0.08171361684799194\n",
      "Test Epoch734 layer3 out_loss 0.15687046945095062, R2 0.08283799886703491\n",
      "Test Epoch734 layer4 out_loss 0.15692871809005737, R2 0.0824974775314331\n",
      "Train 735 | out_loss 0.39420995116233826: 100%|█| 125/125 [00:00<00:00, 238.08it\n",
      "Train Epoch735 out_loss 0.15540145337581635, R2 0.08842617273330688\n",
      "Test Epoch735 layer0 out_loss 0.15897566080093384, R2 0.0705297589302063\n",
      "Test Epoch735 layer1 out_loss 0.1579759120941162, R2 0.07637494802474976\n",
      "Test Epoch735 layer2 out_loss 0.15712250769138336, R2 0.08136439323425293\n",
      "Test Epoch735 layer3 out_loss 0.15684624016284943, R2 0.08297967910766602\n",
      "Test Epoch735 layer4 out_loss 0.1568947732448578, R2 0.08269590139389038\n",
      "Train 736 | out_loss 0.39434194564819336: 100%|█| 125/125 [00:00<00:00, 242.45it\n",
      "Train Epoch736 out_loss 0.15550553798675537, R2 0.08781564235687256\n",
      "Test Epoch736 layer0 out_loss 0.15896812081336975, R2 0.07057380676269531\n",
      "Test Epoch736 layer1 out_loss 0.1580716222524643, R2 0.07581532001495361\n",
      "Test Epoch736 layer2 out_loss 0.15708059072494507, R2 0.08160948753356934\n",
      "Test Epoch736 layer3 out_loss 0.15687920153141022, R2 0.08278697729110718\n",
      "Test Epoch736 layer4 out_loss 0.15691031515598297, R2 0.08260506391525269\n",
      "Train 737 | out_loss 0.3942278325557709: 100%|█| 125/125 [00:00<00:00, 261.31it/\n",
      "Train Epoch737 out_loss 0.1554155796766281, R2 0.08834338188171387\n",
      "Test Epoch737 layer0 out_loss 0.1590392291545868, R2 0.07015812397003174\n",
      "Test Epoch737 layer1 out_loss 0.1582583785057068, R2 0.07472342252731323\n",
      "Test Epoch737 layer2 out_loss 0.15748485922813416, R2 0.07924586534500122\n",
      "Test Epoch737 layer3 out_loss 0.15735702216625214, R2 0.07999330759048462\n",
      "Test Epoch737 layer4 out_loss 0.15715055167675018, R2 0.08120042085647583\n",
      "Train 738 | out_loss 0.3944268822669983: 100%|█| 125/125 [00:00<00:00, 255.42it/\n",
      "Train Epoch738 out_loss 0.15557251870632172, R2 0.08742272853851318\n",
      "Test Epoch738 layer0 out_loss 0.158945694565773, R2 0.07070499658584595\n",
      "Test Epoch738 layer1 out_loss 0.15799444913864136, R2 0.07626652717590332\n",
      "Test Epoch738 layer2 out_loss 0.1572185456752777, R2 0.08080291748046875\n",
      "Test Epoch738 layer3 out_loss 0.1569666713476181, R2 0.08227556943893433\n",
      "Test Epoch738 layer4 out_loss 0.1569434404373169, R2 0.08241140842437744\n",
      "Train 739 | out_loss 0.3942526876926422: 100%|█| 125/125 [00:00<00:00, 239.08it/\n",
      "Train Epoch739 out_loss 0.15543517470359802, R2 0.08822834491729736\n",
      "Test Epoch739 layer0 out_loss 0.1589757800102234, R2 0.070529043674469\n",
      "Test Epoch739 layer1 out_loss 0.15810184180736542, R2 0.07563865184783936\n",
      "Test Epoch739 layer2 out_loss 0.15710601210594177, R2 0.08146083354949951\n",
      "Test Epoch739 layer3 out_loss 0.15693645179271698, R2 0.08245229721069336\n",
      "Test Epoch739 layer4 out_loss 0.15698158740997314, R2 0.08218836784362793\n",
      "Train 740 | out_loss 0.3943338096141815: 100%|█| 125/125 [00:00<00:00, 253.44it/\n",
      "Train Epoch740 out_loss 0.15549913048744202, R2 0.08785325288772583\n",
      "Test Epoch740 layer0 out_loss 0.15895278751850128, R2 0.07066351175308228\n",
      "Test Epoch740 layer1 out_loss 0.15796786546707153, R2 0.07642197608947754\n",
      "Test Epoch740 layer2 out_loss 0.15723301470279694, R2 0.08071833848953247\n",
      "Test Epoch740 layer3 out_loss 0.15700246393680573, R2 0.0820663571357727\n",
      "Test Epoch740 layer4 out_loss 0.1570262461900711, R2 0.08192723989486694\n",
      "Train 741 | out_loss 0.394347220659256: 100%|█| 125/125 [00:00<00:00, 234.47it/s\n",
      "Train Epoch741 out_loss 0.15550968050956726, R2 0.08779138326644897\n",
      "Test Epoch741 layer0 out_loss 0.15896564722061157, R2 0.07058829069137573\n",
      "Test Epoch741 layer1 out_loss 0.15795277059078217, R2 0.07651019096374512\n",
      "Test Epoch741 layer2 out_loss 0.15722954273223877, R2 0.08073866367340088\n",
      "Test Epoch741 layer3 out_loss 0.15704497694969177, R2 0.08181768655776978\n",
      "Test Epoch741 layer4 out_loss 0.15701539814472198, R2 0.08199065923690796\n",
      "Train 742 | out_loss 0.39445239305496216: 100%|█| 125/125 [00:00<00:00, 243.66it\n",
      "Train Epoch742 out_loss 0.155592679977417, R2 0.08730447292327881\n",
      "Test Epoch742 layer0 out_loss 0.15893813967704773, R2 0.07074910402297974\n",
      "Test Epoch742 layer1 out_loss 0.15804867446422577, R2 0.07594949007034302\n",
      "Test Epoch742 layer2 out_loss 0.15706998109817505, R2 0.08167159557342529\n",
      "Test Epoch742 layer3 out_loss 0.15686172246932983, R2 0.0828891396522522\n",
      "Test Epoch742 layer4 out_loss 0.15688163042068481, R2 0.08277273178100586\n",
      "Train 743 | out_loss 0.39435216784477234: 100%|█| 125/125 [00:00<00:00, 241.98it\n",
      "Train Epoch743 out_loss 0.15551362931728363, R2 0.08776819705963135\n",
      "Test Epoch743 layer0 out_loss 0.15894801914691925, R2 0.07069140672683716\n",
      "Test Epoch743 layer1 out_loss 0.15794186294078827, R2 0.07657396793365479\n",
      "Test Epoch743 layer2 out_loss 0.1571534425020218, R2 0.08118361234664917\n",
      "Test Epoch743 layer3 out_loss 0.15699023008346558, R2 0.0821378231048584\n",
      "Test Epoch743 layer4 out_loss 0.15700334310531616, R2 0.08206117153167725\n",
      "Train 744 | out_loss 0.39420852065086365: 100%|█| 125/125 [00:00<00:00, 258.38it\n",
      "Train Epoch744 out_loss 0.1554003804922104, R2 0.08843255043029785\n",
      "Test Epoch744 layer0 out_loss 0.15903064608573914, R2 0.07020831108093262\n",
      "Test Epoch744 layer1 out_loss 0.15824949741363525, R2 0.07477539777755737\n",
      "Test Epoch744 layer2 out_loss 0.15736278891563416, R2 0.07995957136154175\n",
      "Test Epoch744 layer3 out_loss 0.15727347135543823, R2 0.08048182725906372\n",
      "Test Epoch744 layer4 out_loss 0.15726211667060852, R2 0.0805482268333435\n",
      "Train 745 | out_loss 0.3944312632083893: 100%|█| 125/125 [00:00<00:00, 256.06it/\n",
      "Train Epoch745 out_loss 0.15557602047920227, R2 0.08740222454071045\n",
      "Test Epoch745 layer0 out_loss 0.15893405675888062, R2 0.07077306509017944\n",
      "Test Epoch745 layer1 out_loss 0.15794606506824493, R2 0.0765494704246521\n",
      "Test Epoch745 layer2 out_loss 0.15712766349315643, R2 0.08133429288864136\n",
      "Test Epoch745 layer3 out_loss 0.15695004165172577, R2 0.08237278461456299\n",
      "Test Epoch745 layer4 out_loss 0.1569458395242691, R2 0.08239734172821045\n",
      "Train 746 | out_loss 0.3943323493003845: 100%|█| 125/125 [00:00<00:00, 249.93it/\n",
      "Train Epoch746 out_loss 0.1554979681968689, R2 0.08786004781723022\n",
      "Test Epoch746 layer0 out_loss 0.15892428159713745, R2 0.07083010673522949\n",
      "Test Epoch746 layer1 out_loss 0.15791885554790497, R2 0.07670849561691284\n",
      "Test Epoch746 layer2 out_loss 0.15704381465911865, R2 0.08182460069656372\n",
      "Test Epoch746 layer3 out_loss 0.15684649348258972, R2 0.08297818899154663\n",
      "Test Epoch746 layer4 out_loss 0.1568625420331955, R2 0.08288431167602539\n",
      "Train 747 | out_loss 0.39435625076293945: 100%|█| 125/125 [00:00<00:00, 253.35it\n",
      "Train Epoch747 out_loss 0.15551689267158508, R2 0.08774906396865845\n",
      "Test Epoch747 layer0 out_loss 0.1589425504207611, R2 0.07072335481643677\n",
      "Test Epoch747 layer1 out_loss 0.15798477828502655, R2 0.07632309198379517\n",
      "Test Epoch747 layer2 out_loss 0.15711332857608795, R2 0.08141809701919556\n",
      "Test Epoch747 layer3 out_loss 0.1569194346666336, R2 0.08255171775817871\n",
      "Test Epoch747 layer4 out_loss 0.156916081905365, R2 0.08257138729095459\n",
      "Train 748 | out_loss 0.39430949091911316: 100%|█| 125/125 [00:00<00:00, 255.14it\n",
      "Train Epoch748 out_loss 0.15547999739646912, R2 0.08796548843383789\n",
      "Test Epoch748 layer0 out_loss 0.15898093581199646, R2 0.07049888372421265\n",
      "Test Epoch748 layer1 out_loss 0.15815569460391998, R2 0.07532376050949097\n",
      "Test Epoch748 layer2 out_loss 0.1578369289636612, R2 0.07718747854232788\n",
      "Test Epoch748 layer3 out_loss 0.1577027291059494, R2 0.07797211408615112\n",
      "Test Epoch748 layer4 out_loss 0.15761639177799225, R2 0.0784769058227539\n",
      "Train 749 | out_loss 0.3943176567554474: 100%|█| 125/125 [00:00<00:00, 248.55it/\n",
      "Train Epoch749 out_loss 0.15548640489578247, R2 0.08792787790298462\n",
      "Test Epoch749 layer0 out_loss 0.1589936763048172, R2 0.07042437791824341\n",
      "Test Epoch749 layer1 out_loss 0.15793776512145996, R2 0.07659798860549927\n",
      "Test Epoch749 layer2 out_loss 0.15722911059856415, R2 0.08074116706848145\n",
      "Test Epoch749 layer3 out_loss 0.15712279081344604, R2 0.081362783908844\n",
      "Test Epoch749 layer4 out_loss 0.1570342779159546, R2 0.08188033103942871\n",
      "Train 750 | out_loss 0.3942444920539856: 100%|█| 125/125 [00:00<00:00, 240.89it/\n",
      "Train Epoch750 out_loss 0.15542873740196228, R2 0.08826613426208496\n",
      "Test Epoch750 layer0 out_loss 0.15897506475448608, R2 0.07053321599960327\n",
      "Test Epoch750 layer1 out_loss 0.1581985205411911, R2 0.07507342100143433\n",
      "Test Epoch750 layer2 out_loss 0.1571812480688095, R2 0.0810210108757019\n",
      "Test Epoch750 layer3 out_loss 0.157016322016716, R2 0.0819852352142334\n",
      "Test Epoch750 layer4 out_loss 0.15703944861888885, R2 0.08185011148452759\n",
      "Train 751 | out_loss 0.39428043365478516: 100%|█| 125/125 [00:00<00:00, 241.19it\n",
      "Train Epoch751 out_loss 0.1554570496082306, R2 0.08810007572174072\n",
      "Test Epoch751 layer0 out_loss 0.15895533561706543, R2 0.07064861059188843\n",
      "Test Epoch751 layer1 out_loss 0.15809744596481323, R2 0.07566440105438232\n",
      "Test Epoch751 layer2 out_loss 0.15736453235149384, R2 0.07994943857192993\n",
      "Test Epoch751 layer3 out_loss 0.15713158249855042, R2 0.08131140470504761\n",
      "Test Epoch751 layer4 out_loss 0.15718112885951996, R2 0.08102172613143921\n",
      "Train 752 | out_loss 0.3943782150745392: 100%|█| 125/125 [00:00<00:00, 248.69it/\n",
      "Train Epoch752 out_loss 0.15553413331508636, R2 0.08764791488647461\n",
      "Test Epoch752 layer0 out_loss 0.15895593166351318, R2 0.07064509391784668\n",
      "Test Epoch752 layer1 out_loss 0.15796740353107452, R2 0.07642465829849243\n",
      "Test Epoch752 layer2 out_loss 0.15715399384498596, R2 0.0811803936958313\n",
      "Test Epoch752 layer3 out_loss 0.15692204236984253, R2 0.08253645896911621\n",
      "Test Epoch752 layer4 out_loss 0.1569482535123825, R2 0.08238321542739868\n",
      "Train 753 | out_loss 0.3942822813987732: 100%|█| 125/125 [00:00<00:00, 245.32it/\n",
      "Train Epoch753 out_loss 0.1554585099220276, R2 0.08809149265289307\n",
      "Test Epoch753 layer0 out_loss 0.15893685817718506, R2 0.07075661420822144\n",
      "Test Epoch753 layer1 out_loss 0.15794886648654938, R2 0.07653301954269409\n",
      "Test Epoch753 layer2 out_loss 0.1571565866470337, R2 0.08116519451141357\n",
      "Test Epoch753 layer3 out_loss 0.15697655081748962, R2 0.08221781253814697\n",
      "Test Epoch753 layer4 out_loss 0.15697917342185974, R2 0.0822024941444397\n",
      "Train 754 | out_loss 0.3942943513393402: 100%|█| 125/125 [00:00<00:00, 239.97it/\n",
      "Train Epoch754 out_loss 0.15546797215938568, R2 0.08803600072860718\n",
      "Test Epoch754 layer0 out_loss 0.15896117687225342, R2 0.07061439752578735\n",
      "Test Epoch754 layer1 out_loss 0.15793044865131378, R2 0.07664072513580322\n",
      "Test Epoch754 layer2 out_loss 0.1571824699640274, R2 0.08101391792297363\n",
      "Test Epoch754 layer3 out_loss 0.1569000780582428, R2 0.0826648473739624\n",
      "Test Epoch754 layer4 out_loss 0.15694017708301544, R2 0.08243048191070557\n",
      "Train 755 | out_loss 0.3943568468093872: 100%|█| 125/125 [00:00<00:00, 252.72it/\n",
      "Train Epoch755 out_loss 0.15551729500293732, R2 0.08774673938751221\n",
      "Test Epoch755 layer0 out_loss 0.15894906222820282, R2 0.07068526744842529\n",
      "Test Epoch755 layer1 out_loss 0.15795081853866577, R2 0.07652163505554199\n",
      "Test Epoch755 layer2 out_loss 0.15710130333900452, R2 0.08148843050003052\n",
      "Test Epoch755 layer3 out_loss 0.1568927764892578, R2 0.08270758390426636\n",
      "Test Epoch755 layer4 out_loss 0.15689536929130554, R2 0.08269244432449341\n",
      "Train 756 | out_loss 0.3941614031791687: 100%|█| 125/125 [00:00<00:00, 249.81it/\n",
      "Train Epoch756 out_loss 0.15536320209503174, R2 0.08865058422088623\n",
      "Test Epoch756 layer0 out_loss 0.159044086933136, R2 0.07012975215911865\n",
      "Test Epoch756 layer1 out_loss 0.15839970111846924, R2 0.07389718294143677\n",
      "Test Epoch756 layer2 out_loss 0.15754780173301697, R2 0.07887786626815796\n",
      "Test Epoch756 layer3 out_loss 0.15764494240283966, R2 0.07830995321273804\n",
      "Test Epoch756 layer4 out_loss 0.15770457684993744, R2 0.07796132564544678\n",
      "Train 757 | out_loss 0.3942530155181885: 100%|█| 125/125 [00:00<00:00, 240.48it/\n",
      "Train Epoch757 out_loss 0.15543542802333832, R2 0.08822691440582275\n",
      "Test Epoch757 layer0 out_loss 0.1589234173297882, R2 0.07083523273468018\n",
      "Test Epoch757 layer1 out_loss 0.15795397758483887, R2 0.07650315761566162\n",
      "Test Epoch757 layer2 out_loss 0.15716272592544556, R2 0.08112937211990356\n",
      "Test Epoch757 layer3 out_loss 0.15698231756687164, R2 0.0821840763092041\n",
      "Test Epoch757 layer4 out_loss 0.15701141953468323, R2 0.08201396465301514\n",
      "Train 758 | out_loss 0.3943229913711548: 100%|█| 125/125 [00:00<00:00, 258.07it/\n",
      "Train Epoch758 out_loss 0.15549060702323914, R2 0.08790326118469238\n",
      "Test Epoch758 layer0 out_loss 0.15897582471370697, R2 0.07052880525588989\n",
      "Test Epoch758 layer1 out_loss 0.15840063989162445, R2 0.07389169931411743\n",
      "Test Epoch758 layer2 out_loss 0.1572231501340866, R2 0.08077603578567505\n",
      "Test Epoch758 layer3 out_loss 0.15706069767475128, R2 0.0817258358001709\n",
      "Test Epoch758 layer4 out_loss 0.1570845991373062, R2 0.08158612251281738\n",
      "Train 759 | out_loss 0.39420920610427856: 100%|█| 125/125 [00:00<00:00, 256.95it\n",
      "Train Epoch759 out_loss 0.1554008573293686, R2 0.08842962980270386\n",
      "Test Epoch759 layer0 out_loss 0.1590476930141449, R2 0.07010859251022339\n",
      "Test Epoch759 layer1 out_loss 0.15844005346298218, R2 0.07366126775741577\n",
      "Test Epoch759 layer2 out_loss 0.15742555260658264, R2 0.07959264516830444\n",
      "Test Epoch759 layer3 out_loss 0.1572740375995636, R2 0.08047854900360107\n",
      "Test Epoch759 layer4 out_loss 0.1573624610900879, R2 0.07996153831481934\n",
      "Train 760 | out_loss 0.3941415250301361: 100%|█| 125/125 [00:00<00:00, 252.92it/\n",
      "Train Epoch760 out_loss 0.1553475260734558, R2 0.08874255418777466\n",
      "Test Epoch760 layer0 out_loss 0.15900279581546783, R2 0.07037115097045898\n",
      "Test Epoch760 layer1 out_loss 0.15791507065296173, R2 0.07673066854476929\n",
      "Test Epoch760 layer2 out_loss 0.15709060430526733, R2 0.0815509557723999\n",
      "Test Epoch760 layer3 out_loss 0.1568630337715149, R2 0.08288145065307617\n",
      "Test Epoch760 layer4 out_loss 0.15688076615333557, R2 0.08277779817581177\n",
      "Train 761 | out_loss 0.3941868245601654: 100%|█| 125/125 [00:00<00:00, 253.87it/\n",
      "Train Epoch761 out_loss 0.15538325905799866, R2 0.08853292465209961\n",
      "Test Epoch761 layer0 out_loss 0.15900447964668274, R2 0.07036125659942627\n",
      "Test Epoch761 layer1 out_loss 0.1585383415222168, R2 0.07308655977249146\n",
      "Test Epoch761 layer2 out_loss 0.1574159860610962, R2 0.07964861392974854\n",
      "Test Epoch761 layer3 out_loss 0.15734341740608215, R2 0.08007287979125977\n",
      "Test Epoch761 layer4 out_loss 0.15745744109153748, R2 0.07940620183944702\n",
      "Train 762 | out_loss 0.39428460597991943: 100%|█| 125/125 [00:00<00:00, 236.47it\n",
      "Train Epoch762 out_loss 0.15546035766601562, R2 0.08808064460754395\n",
      "Test Epoch762 layer0 out_loss 0.15891040861606598, R2 0.0709112286567688\n",
      "Test Epoch762 layer1 out_loss 0.15800534188747406, R2 0.0762028694152832\n",
      "Test Epoch762 layer2 out_loss 0.15718655288219452, R2 0.0809900164604187\n",
      "Test Epoch762 layer3 out_loss 0.15709403157234192, R2 0.08153098821640015\n",
      "Test Epoch762 layer4 out_loss 0.15712237358093262, R2 0.08136522769927979\n",
      "Train 763 | out_loss 0.39424747228622437: 100%|█| 125/125 [00:00<00:00, 261.39it\n",
      "Train Epoch763 out_loss 0.15543101727962494, R2 0.08825278282165527\n",
      "Test Epoch763 layer0 out_loss 0.15891233086585999, R2 0.07089996337890625\n",
      "Test Epoch763 layer1 out_loss 0.15809647738933563, R2 0.07567000389099121\n",
      "Test Epoch763 layer2 out_loss 0.15715742111206055, R2 0.08116030693054199\n",
      "Test Epoch763 layer3 out_loss 0.15700766444206238, R2 0.08203589916229248\n",
      "Test Epoch763 layer4 out_loss 0.15696436166763306, R2 0.08228909969329834\n",
      "Train 764 | out_loss 0.39417141675949097: 100%|█| 125/125 [00:00<00:00, 224.32it\n",
      "Train Epoch764 out_loss 0.15537108480930328, R2 0.0886043906211853\n",
      "Test Epoch764 layer0 out_loss 0.15891174972057343, R2 0.070903480052948\n",
      "Test Epoch764 layer1 out_loss 0.15792104601860046, R2 0.07669568061828613\n",
      "Test Epoch764 layer2 out_loss 0.1570451855659485, R2 0.08181655406951904\n",
      "Test Epoch764 layer3 out_loss 0.15687938034534454, R2 0.08278590440750122\n",
      "Test Epoch764 layer4 out_loss 0.15691523253917694, R2 0.08257627487182617\n",
      "Train 765 | out_loss 0.394275963306427: 100%|█| 125/125 [00:00<00:00, 251.66it/s\n",
      "Train Epoch765 out_loss 0.15545351803302765, R2 0.08812075853347778\n",
      "Test Epoch765 layer0 out_loss 0.15892189741134644, R2 0.07084411382675171\n",
      "Test Epoch765 layer1 out_loss 0.15797220170497894, R2 0.076396644115448\n",
      "Test Epoch765 layer2 out_loss 0.1572771817445755, R2 0.08046013116836548\n",
      "Test Epoch765 layer3 out_loss 0.15715442597866058, R2 0.08117789030075073\n",
      "Test Epoch765 layer4 out_loss 0.1572006195783615, R2 0.08090776205062866\n",
      "Train 766 | out_loss 0.3942476212978363: 100%|█| 125/125 [00:00<00:00, 257.73it/\n",
      "Train Epoch766 out_loss 0.15543118119239807, R2 0.08825182914733887\n",
      "Test Epoch766 layer0 out_loss 0.1588922142982483, R2 0.07101768255233765\n",
      "Test Epoch766 layer1 out_loss 0.15787793695926666, R2 0.07694780826568604\n",
      "Test Epoch766 layer2 out_loss 0.15705952048301697, R2 0.08173269033432007\n",
      "Test Epoch766 layer3 out_loss 0.15689578652381897, R2 0.08269000053405762\n",
      "Test Epoch766 layer4 out_loss 0.15690244734287262, R2 0.08265101909637451\n",
      "Train 767 | out_loss 0.3942057192325592: 100%|█| 125/125 [00:00<00:00, 251.88it/\n",
      "Train Epoch767 out_loss 0.15539811551570892, R2 0.08844578266143799\n",
      "Test Epoch767 layer0 out_loss 0.15895378589630127, R2 0.07065767049789429\n",
      "Test Epoch767 layer1 out_loss 0.1580013781785965, R2 0.07622605562210083\n",
      "Test Epoch767 layer2 out_loss 0.15707193315029144, R2 0.08166015148162842\n",
      "Test Epoch767 layer3 out_loss 0.15689584612846375, R2 0.08268964290618896\n",
      "Test Epoch767 layer4 out_loss 0.1569526642560959, R2 0.08235746622085571\n",
      "Train 768 | out_loss 0.3941883146762848: 100%|█| 125/125 [00:00<00:00, 249.85it/\n",
      "Train Epoch768 out_loss 0.15538446605205536, R2 0.08852589130401611\n",
      "Test Epoch768 layer0 out_loss 0.15889857709407806, R2 0.07098042964935303\n",
      "Test Epoch768 layer1 out_loss 0.15793785452842712, R2 0.07659745216369629\n",
      "Test Epoch768 layer2 out_loss 0.15705053508281708, R2 0.08178520202636719\n",
      "Test Epoch768 layer3 out_loss 0.15686991810798645, R2 0.08284121751785278\n",
      "Test Epoch768 layer4 out_loss 0.15692275762557983, R2 0.08253228664398193\n",
      "Train 769 | out_loss 0.3941969573497772: 100%|█| 125/125 [00:00<00:00, 221.05it/\n",
      "Train Epoch769 out_loss 0.15539123117923737, R2 0.08848613500595093\n",
      "Test Epoch769 layer0 out_loss 0.15891894698143005, R2 0.0708613395690918\n",
      "Test Epoch769 layer1 out_loss 0.15844570100307465, R2 0.07362830638885498\n",
      "Test Epoch769 layer2 out_loss 0.1573844701051712, R2 0.07983279228210449\n",
      "Test Epoch769 layer3 out_loss 0.15716813504695892, R2 0.08109766244888306\n",
      "Test Epoch769 layer4 out_loss 0.1572079062461853, R2 0.08086520433425903\n",
      "Train 770 | out_loss 0.3943324089050293: 100%|█| 125/125 [00:00<00:00, 244.44it/\n",
      "Train Epoch770 out_loss 0.15549807250499725, R2 0.0878593921661377\n",
      "Test Epoch770 layer0 out_loss 0.1588936746120453, R2 0.07100909948348999\n",
      "Test Epoch770 layer1 out_loss 0.15792134404182434, R2 0.07669395208358765\n",
      "Test Epoch770 layer2 out_loss 0.15718621015548706, R2 0.08099204301834106\n",
      "Test Epoch770 layer3 out_loss 0.1570274829864502, R2 0.08192002773284912\n",
      "Test Epoch770 layer4 out_loss 0.1570335030555725, R2 0.08188480138778687\n",
      "Train 771 | out_loss 0.39426010847091675: 100%|█| 125/125 [00:00<00:00, 246.96it\n",
      "Train Epoch771 out_loss 0.15544100105762482, R2 0.08819425106048584\n",
      "Test Epoch771 layer0 out_loss 0.15899430215358734, R2 0.07042074203491211\n",
      "Test Epoch771 layer1 out_loss 0.15799367427825928, R2 0.07627111673355103\n",
      "Test Epoch771 layer2 out_loss 0.1570383459329605, R2 0.08185648918151855\n",
      "Test Epoch771 layer3 out_loss 0.15688318014144897, R2 0.08276373147964478\n",
      "Test Epoch771 layer4 out_loss 0.15687085688114166, R2 0.08283579349517822\n",
      "Train 772 | out_loss 0.39412862062454224: 100%|█| 125/125 [00:00<00:00, 244.72it\n",
      "Train Epoch772 out_loss 0.155337393283844, R2 0.08880198001861572\n",
      "Test Epoch772 layer0 out_loss 0.15891025960445404, R2 0.07091212272644043\n",
      "Test Epoch772 layer1 out_loss 0.15792308747768402, R2 0.07668381929397583\n",
      "Test Epoch772 layer2 out_loss 0.15717867016792297, R2 0.08103609085083008\n",
      "Test Epoch772 layer3 out_loss 0.1569918990135193, R2 0.08212804794311523\n",
      "Test Epoch772 layer4 out_loss 0.15703953802585602, R2 0.08184957504272461\n",
      "Train 773 | out_loss 0.3943067193031311: 100%|█| 125/125 [00:00<00:00, 237.07it/\n",
      "Train Epoch773 out_loss 0.15547779202461243, R2 0.08797836303710938\n",
      "Test Epoch773 layer0 out_loss 0.1588921695947647, R2 0.07101792097091675\n",
      "Test Epoch773 layer1 out_loss 0.157939150929451, R2 0.07658988237380981\n",
      "Test Epoch773 layer2 out_loss 0.15705035626888275, R2 0.08178627490997314\n",
      "Test Epoch773 layer3 out_loss 0.15683521330356598, R2 0.08304411172866821\n",
      "Test Epoch773 layer4 out_loss 0.15681712329387665, R2 0.08314990997314453\n",
      "Train 774 | out_loss 0.3941921293735504: 100%|█| 125/125 [00:00<00:00, 232.04it/\n",
      "Train Epoch774 out_loss 0.15538741648197174, R2 0.0885084867477417\n",
      "Test Epoch774 layer0 out_loss 0.15888872742652893, R2 0.07103800773620605\n",
      "Test Epoch774 layer1 out_loss 0.15790337324142456, R2 0.07679909467697144\n",
      "Test Epoch774 layer2 out_loss 0.15710388123989105, R2 0.08147335052490234\n",
      "Test Epoch774 layer3 out_loss 0.15697035193443298, R2 0.08225405216217041\n",
      "Test Epoch774 layer4 out_loss 0.15701016783714294, R2 0.08202129602432251\n",
      "Train 775 | out_loss 0.39429304003715515: 100%|█| 125/125 [00:00<00:00, 257.82it\n",
      "Train Epoch775 out_loss 0.15546701848506927, R2 0.08804154396057129\n",
      "Test Epoch775 layer0 out_loss 0.15888409316539764, R2 0.07106512784957886\n",
      "Test Epoch775 layer1 out_loss 0.15786218643188477, R2 0.07703983783721924\n",
      "Test Epoch775 layer2 out_loss 0.15702395141124725, R2 0.0819406509399414\n",
      "Test Epoch775 layer3 out_loss 0.1567993462085724, R2 0.08325386047363281\n",
      "Test Epoch775 layer4 out_loss 0.1568194329738617, R2 0.08313643932342529\n",
      "Train 776 | out_loss 0.3944389224052429: 100%|█| 125/125 [00:00<00:00, 243.17it/\n",
      "Train Epoch776 out_loss 0.15558207035064697, R2 0.08736670017242432\n",
      "Test Epoch776 layer0 out_loss 0.15912839770317078, R2 0.0696367621421814\n",
      "Test Epoch776 layer1 out_loss 0.15815529227256775, R2 0.07532614469528198\n",
      "Test Epoch776 layer2 out_loss 0.1571730524301529, R2 0.08106893301010132\n",
      "Test Epoch776 layer3 out_loss 0.15694113075733185, R2 0.08242487907409668\n",
      "Test Epoch776 layer4 out_loss 0.15695306658744812, R2 0.08235514163970947\n",
      "Train 777 | out_loss 0.39415180683135986: 100%|█| 125/125 [00:00<00:00, 260.76it\n",
      "Train Epoch777 out_loss 0.15535566210746765, R2 0.0886947512626648\n",
      "Test Epoch777 layer0 out_loss 0.1589467078447342, R2 0.07069909572601318\n",
      "Test Epoch777 layer1 out_loss 0.1583065688610077, R2 0.07444173097610474\n",
      "Test Epoch777 layer2 out_loss 0.15734274685382843, R2 0.08007681369781494\n",
      "Test Epoch777 layer3 out_loss 0.15711644291877747, R2 0.08139991760253906\n",
      "Test Epoch777 layer4 out_loss 0.15705016255378723, R2 0.08178746700286865\n",
      "Train 778 | out_loss 0.394283264875412: 100%|█| 125/125 [00:00<00:00, 259.98it/s\n",
      "Train Epoch778 out_loss 0.15545925498008728, R2 0.08808714151382446\n",
      "Test Epoch778 layer0 out_loss 0.15890376269817352, R2 0.07095015048980713\n",
      "Test Epoch778 layer1 out_loss 0.15787124633789062, R2 0.07698684930801392\n",
      "Test Epoch778 layer2 out_loss 0.1570848673582077, R2 0.08158451318740845\n",
      "Test Epoch778 layer3 out_loss 0.1569213718175888, R2 0.08254039287567139\n",
      "Test Epoch778 layer4 out_loss 0.15691202878952026, R2 0.08259499073028564\n",
      "Train 779 | out_loss 0.39423105120658875: 100%|█| 125/125 [00:00<00:00, 241.16it\n",
      "Train Epoch779 out_loss 0.15541812777519226, R2 0.08832842111587524\n",
      "Test Epoch779 layer0 out_loss 0.1589145064353943, R2 0.07088732719421387\n",
      "Test Epoch779 layer1 out_loss 0.15801019966602325, R2 0.07617449760437012\n",
      "Test Epoch779 layer2 out_loss 0.15715718269348145, R2 0.0811617374420166\n",
      "Test Epoch779 layer3 out_loss 0.1569194793701172, R2 0.08255147933959961\n",
      "Test Epoch779 layer4 out_loss 0.15690292418003082, R2 0.08264827728271484\n",
      "Train 780 | out_loss 0.3940970301628113: 100%|█| 125/125 [00:00<00:00, 233.42it/\n",
      "Train Epoch780 out_loss 0.1553124487400055, R2 0.08894824981689453\n",
      "Test Epoch780 layer0 out_loss 0.15890803933143616, R2 0.07092511653900146\n",
      "Test Epoch780 layer1 out_loss 0.15786221623420715, R2 0.07703965902328491\n",
      "Test Epoch780 layer2 out_loss 0.15722405910491943, R2 0.08077073097229004\n",
      "Test Epoch780 layer3 out_loss 0.15694691240787506, R2 0.08239108324050903\n",
      "Test Epoch780 layer4 out_loss 0.15699486434459686, R2 0.08211076259613037\n",
      "Train 781 | out_loss 0.394269734621048: 100%|█| 125/125 [00:00<00:00, 248.17it/s\n",
      "Train Epoch781 out_loss 0.15544863045215607, R2 0.08814948797225952\n",
      "Test Epoch781 layer0 out_loss 0.15887856483459473, R2 0.07109743356704712\n",
      "Test Epoch781 layer1 out_loss 0.15785706043243408, R2 0.07706981897354126\n",
      "Test Epoch781 layer2 out_loss 0.157037153840065, R2 0.08186346292495728\n",
      "Test Epoch781 layer3 out_loss 0.1568077653646469, R2 0.08320462703704834\n",
      "Test Epoch781 layer4 out_loss 0.1568300426006317, R2 0.08307433128356934\n",
      "Train 782 | out_loss 0.3940678536891937: 100%|█| 125/125 [00:00<00:00, 261.32it/\n",
      "Train Epoch782 out_loss 0.15528947114944458, R2 0.08908307552337646\n",
      "Test Epoch782 layer0 out_loss 0.1588878184556961, R2 0.07104337215423584\n",
      "Test Epoch782 layer1 out_loss 0.15808972716331482, R2 0.0757095217704773\n",
      "Test Epoch782 layer2 out_loss 0.15765069425106049, R2 0.07827633619308472\n",
      "Test Epoch782 layer3 out_loss 0.15728077292442322, R2 0.08043909072875977\n",
      "Test Epoch782 layer4 out_loss 0.1571807861328125, R2 0.0810236930847168\n",
      "Train 783 | out_loss 0.39415210485458374: 100%|█| 125/125 [00:00<00:00, 249.46it\n",
      "Train Epoch783 out_loss 0.15535588562488556, R2 0.08869349956512451\n",
      "Test Epoch783 layer0 out_loss 0.15888091921806335, R2 0.071083664894104\n",
      "Test Epoch783 layer1 out_loss 0.15786013007164001, R2 0.07705187797546387\n",
      "Test Epoch783 layer2 out_loss 0.15702804923057556, R2 0.08191674947738647\n",
      "Test Epoch783 layer3 out_loss 0.15682414174079895, R2 0.08310890197753906\n",
      "Test Epoch783 layer4 out_loss 0.15684610605239868, R2 0.08298051357269287\n",
      "Train 784 | out_loss 0.3940468728542328: 100%|█| 125/125 [00:00<00:00, 250.88it/\n",
      "Train Epoch784 out_loss 0.1552729308605194, R2 0.0891801118850708\n",
      "Test Epoch784 layer0 out_loss 0.15914049744606018, R2 0.06956601142883301\n",
      "Test Epoch784 layer1 out_loss 0.15845651924610138, R2 0.07356500625610352\n",
      "Test Epoch784 layer2 out_loss 0.15751498937606812, R2 0.07906979322433472\n",
      "Test Epoch784 layer3 out_loss 0.15727639198303223, R2 0.08046472072601318\n",
      "Test Epoch784 layer4 out_loss 0.1573597937822342, R2 0.07997715473175049\n",
      "Train 785 | out_loss 0.39415648579597473: 100%|█| 125/125 [00:00<00:00, 257.94it\n",
      "Train Epoch785 out_loss 0.15535935759544373, R2 0.08867311477661133\n",
      "Test Epoch785 layer0 out_loss 0.15914607048034668, R2 0.06953340768814087\n",
      "Test Epoch785 layer1 out_loss 0.1585189700126648, R2 0.07319986820220947\n",
      "Test Epoch785 layer2 out_loss 0.15762068331241608, R2 0.07845181226730347\n",
      "Test Epoch785 layer3 out_loss 0.15778377652168274, R2 0.07749819755554199\n",
      "Test Epoch785 layer4 out_loss 0.15767459571361542, R2 0.0781366229057312\n",
      "Train 786 | out_loss 0.3941923975944519: 100%|█| 125/125 [00:00<00:00, 261.38it/\n",
      "Train Epoch786 out_loss 0.15538766980171204, R2 0.08850705623626709\n",
      "Test Epoch786 layer0 out_loss 0.15888097882270813, R2 0.07108330726623535\n",
      "Test Epoch786 layer1 out_loss 0.15786924958229065, R2 0.07699853181838989\n",
      "Test Epoch786 layer2 out_loss 0.15706373751163483, R2 0.08170807361602783\n",
      "Test Epoch786 layer3 out_loss 0.15684176981449127, R2 0.08300584554672241\n",
      "Test Epoch786 layer4 out_loss 0.15684683620929718, R2 0.08297622203826904\n",
      "Train 787 | out_loss 0.39432281255722046: 100%|█| 125/125 [00:00<00:00, 243.05it\n",
      "Train Epoch787 out_loss 0.15549048781394958, R2 0.08790391683578491\n",
      "Test Epoch787 layer0 out_loss 0.15891508758068085, R2 0.07088392972946167\n",
      "Test Epoch787 layer1 out_loss 0.1580268293619156, R2 0.07607722282409668\n",
      "Test Epoch787 layer2 out_loss 0.15708406269550323, R2 0.0815892219543457\n",
      "Test Epoch787 layer3 out_loss 0.15687741339206696, R2 0.08279740810394287\n",
      "Test Epoch787 layer4 out_loss 0.1569184809923172, R2 0.08255726099014282\n",
      "Train 788 | out_loss 0.3941524624824524: 100%|█| 125/125 [00:00<00:00, 259.15it/\n",
      "Train Epoch788 out_loss 0.15535615384578705, R2 0.08869189023971558\n",
      "Test Epoch788 layer0 out_loss 0.1588786095380783, R2 0.07109713554382324\n",
      "Test Epoch788 layer1 out_loss 0.15787677466869354, R2 0.07695454359054565\n",
      "Test Epoch788 layer2 out_loss 0.15719634294509888, R2 0.08093273639678955\n",
      "Test Epoch788 layer3 out_loss 0.15697970986366272, R2 0.0821993350982666\n",
      "Test Epoch788 layer4 out_loss 0.15696008503437042, R2 0.08231407403945923\n",
      "Train 789 | out_loss 0.3942486047744751: 100%|█| 125/125 [00:00<00:00, 255.21it/\n",
      "Train Epoch789 out_loss 0.15543191134929657, R2 0.08824753761291504\n",
      "Test Epoch789 layer0 out_loss 0.1589014083147049, R2 0.07096385955810547\n",
      "Test Epoch789 layer1 out_loss 0.15788045525550842, R2 0.07693296670913696\n",
      "Test Epoch789 layer2 out_loss 0.1570335030555725, R2 0.08188480138778687\n",
      "Test Epoch789 layer3 out_loss 0.1568402349948883, R2 0.08301472663879395\n",
      "Test Epoch789 layer4 out_loss 0.1568486988544464, R2 0.08296531438827515\n",
      "Train 790 | out_loss 0.3941107392311096: 100%|█| 125/125 [00:00<00:00, 251.88it/\n",
      "Train Epoch790 out_loss 0.15532322227954865, R2 0.08888506889343262\n",
      "Test Epoch790 layer0 out_loss 0.15893913805484772, R2 0.07074332237243652\n",
      "Test Epoch790 layer1 out_loss 0.1578829139471054, R2 0.0769186019897461\n",
      "Test Epoch790 layer2 out_loss 0.1570940762758255, R2 0.08153069019317627\n",
      "Test Epoch790 layer3 out_loss 0.15692126750946045, R2 0.08254104852676392\n",
      "Test Epoch790 layer4 out_loss 0.15682275593280792, R2 0.08311700820922852\n",
      "Train 791 | out_loss 0.3941885828971863: 100%|█| 125/125 [00:00<00:00, 245.85it/\n",
      "Train Epoch791 out_loss 0.1553846150636673, R2 0.08852499723434448\n",
      "Test Epoch791 layer0 out_loss 0.1589248776435852, R2 0.07082664966583252\n",
      "Test Epoch791 layer1 out_loss 0.15788407623767853, R2 0.0769118070602417\n",
      "Test Epoch791 layer2 out_loss 0.15707464516162872, R2 0.08164429664611816\n",
      "Test Epoch791 layer3 out_loss 0.15688976645469666, R2 0.0827251672744751\n",
      "Test Epoch791 layer4 out_loss 0.15692085027694702, R2 0.08254343271255493\n",
      "Train 792 | out_loss 0.39419394731521606: 100%|█| 125/125 [00:00<00:00, 263.73it\n",
      "Train Epoch792 out_loss 0.15538884699344635, R2 0.08850020170211792\n",
      "Test Epoch792 layer0 out_loss 0.15887810289859772, R2 0.07110017538070679\n",
      "Test Epoch792 layer1 out_loss 0.1579015851020813, R2 0.07680952548980713\n",
      "Test Epoch792 layer2 out_loss 0.1571580469608307, R2 0.0811566710472107\n",
      "Test Epoch792 layer3 out_loss 0.15698125958442688, R2 0.08219027519226074\n",
      "Test Epoch792 layer4 out_loss 0.15700776875019073, R2 0.08203530311584473\n",
      "Train 793 | out_loss 0.3941485583782196: 100%|█| 125/125 [00:00<00:00, 247.32it/\n",
      "Train Epoch793 out_loss 0.15535302460193634, R2 0.0887102484703064\n",
      "Test Epoch793 layer0 out_loss 0.15887048840522766, R2 0.071144700050354\n",
      "Test Epoch793 layer1 out_loss 0.15783248841762543, R2 0.07721346616744995\n",
      "Test Epoch793 layer2 out_loss 0.15701377391815186, R2 0.08200013637542725\n",
      "Test Epoch793 layer3 out_loss 0.15680895745754242, R2 0.08319765329360962\n",
      "Test Epoch793 layer4 out_loss 0.15682531893253326, R2 0.08310198783874512\n",
      "Train 794 | out_loss 0.3942378759384155: 100%|█| 125/125 [00:00<00:00, 246.70it/\n",
      "Train Epoch794 out_loss 0.15542350709438324, R2 0.08829683065414429\n",
      "Test Epoch794 layer0 out_loss 0.15894463658332825, R2 0.07071113586425781\n",
      "Test Epoch794 layer1 out_loss 0.15818937122821808, R2 0.07512688636779785\n",
      "Test Epoch794 layer2 out_loss 0.15730346739292145, R2 0.08030647039413452\n",
      "Test Epoch794 layer3 out_loss 0.1571713238954544, R2 0.08107900619506836\n",
      "Test Epoch794 layer4 out_loss 0.15725629031658173, R2 0.08058232069015503\n",
      "Train 795 | out_loss 0.3941357731819153: 100%|█| 125/125 [00:00<00:00, 246.94it/\n",
      "Train Epoch795 out_loss 0.1553429365158081, R2 0.08876943588256836\n",
      "Test Epoch795 layer0 out_loss 0.15887591242790222, R2 0.07111293077468872\n",
      "Test Epoch795 layer1 out_loss 0.1578875184059143, R2 0.07689177989959717\n",
      "Test Epoch795 layer2 out_loss 0.15709863603115082, R2 0.08150404691696167\n",
      "Test Epoch795 layer3 out_loss 0.15690095722675323, R2 0.0826597809791565\n",
      "Test Epoch795 layer4 out_loss 0.15684819221496582, R2 0.08296829462051392\n",
      "Train 796 | out_loss 0.394091933965683: 100%|█| 125/125 [00:00<00:00, 248.51it/s\n",
      "Train Epoch796 out_loss 0.15530847012996674, R2 0.08897161483764648\n",
      "Test Epoch796 layer0 out_loss 0.15891315042972565, R2 0.070895254611969\n",
      "Test Epoch796 layer1 out_loss 0.15794776380062103, R2 0.07653945684432983\n",
      "Test Epoch796 layer2 out_loss 0.15748070180416107, R2 0.07927024364471436\n",
      "Test Epoch796 layer3 out_loss 0.15709716081619263, R2 0.08151257038116455\n",
      "Test Epoch796 layer4 out_loss 0.1570442169904709, R2 0.08182215690612793\n",
      "Train 797 | out_loss 0.39408954977989197: 100%|█| 125/125 [00:00<00:00, 249.93it\n",
      "Train Epoch797 out_loss 0.15530650317668915, R2 0.08898317813873291\n",
      "Test Epoch797 layer0 out_loss 0.1588815152645111, R2 0.07108020782470703\n",
      "Test Epoch797 layer1 out_loss 0.15826085209846497, R2 0.07470899820327759\n",
      "Test Epoch797 layer2 out_loss 0.15725530683994293, R2 0.08058798313140869\n",
      "Test Epoch797 layer3 out_loss 0.15730227530002594, R2 0.08031338453292847\n",
      "Test Epoch797 layer4 out_loss 0.15729129314422607, R2 0.08037763833999634\n",
      "Train 798 | out_loss 0.39432311058044434: 100%|█| 125/125 [00:00<00:00, 247.36it\n",
      "Train Epoch798 out_loss 0.1554907262325287, R2 0.08790254592895508\n",
      "Test Epoch798 layer0 out_loss 0.1588936746120453, R2 0.07100909948348999\n",
      "Test Epoch798 layer1 out_loss 0.15802854299545288, R2 0.07606714963912964\n",
      "Test Epoch798 layer2 out_loss 0.15726538002490997, R2 0.0805290937423706\n",
      "Test Epoch798 layer3 out_loss 0.15699295699596405, R2 0.08212190866470337\n",
      "Test Epoch798 layer4 out_loss 0.15695986151695251, R2 0.08231544494628906\n",
      "Train 799 | out_loss 0.3940446078777313: 100%|█| 125/125 [00:00<00:00, 255.59it/\n",
      "Train Epoch799 out_loss 0.15527115762233734, R2 0.08919048309326172\n",
      "Test Epoch799 layer0 out_loss 0.15891093015670776, R2 0.07090818881988525\n",
      "Test Epoch799 layer1 out_loss 0.15817339718341827, R2 0.07522028684616089\n",
      "Test Epoch799 layer2 out_loss 0.1575089693069458, R2 0.0791049599647522\n",
      "Test Epoch799 layer3 out_loss 0.15730807185173035, R2 0.08027952909469604\n",
      "Test Epoch799 layer4 out_loss 0.15739665925502777, R2 0.07976162433624268\n",
      "Train 800 | out_loss 0.3941800892353058: 100%|█| 125/125 [00:00<00:00, 250.77it/\n",
      "Train Epoch800 out_loss 0.15537792444229126, R2 0.08856421709060669\n",
      "Test Epoch800 layer0 out_loss 0.1588629186153412, R2 0.07118892669677734\n",
      "Test Epoch800 layer1 out_loss 0.15781517326831818, R2 0.07731473445892334\n",
      "Test Epoch800 layer2 out_loss 0.15699335932731628, R2 0.08211952447891235\n",
      "Test Epoch800 layer3 out_loss 0.15678343176841736, R2 0.0833469033241272\n",
      "Test Epoch800 layer4 out_loss 0.15680252015590668, R2 0.08323526382446289\n",
      "Train 801 | out_loss 0.3940679132938385: 100%|█| 125/125 [00:00<00:00, 259.27it/\n",
      "Train Epoch801 out_loss 0.15528951585292816, R2 0.08908277750015259\n",
      "Test Epoch801 layer0 out_loss 0.15886540710926056, R2 0.07117438316345215\n",
      "Test Epoch801 layer1 out_loss 0.15791448950767517, R2 0.07673400640487671\n",
      "Test Epoch801 layer2 out_loss 0.15705879032611847, R2 0.0817369818687439\n",
      "Test Epoch801 layer3 out_loss 0.1568130999803543, R2 0.08317345380783081\n",
      "Test Epoch801 layer4 out_loss 0.15677936375141144, R2 0.08337068557739258\n",
      "Train 802 | out_loss 0.3941284120082855: 100%|█| 125/125 [00:00<00:00, 254.30it/\n",
      "Train Epoch802 out_loss 0.15533724427223206, R2 0.08880281448364258\n",
      "Test Epoch802 layer0 out_loss 0.15891535580158234, R2 0.07088232040405273\n",
      "Test Epoch802 layer1 out_loss 0.15812906622886658, R2 0.07547950744628906\n",
      "Test Epoch802 layer2 out_loss 0.15728530287742615, R2 0.08041262626647949\n",
      "Test Epoch802 layer3 out_loss 0.15703386068344116, R2 0.08188271522521973\n",
      "Test Epoch802 layer4 out_loss 0.15706130862236023, R2 0.08172225952148438\n",
      "Train 803 | out_loss 0.39414024353027344: 100%|█| 125/125 [00:00<00:00, 246.83it\n",
      "Train Epoch803 out_loss 0.1553465574979782, R2 0.08874821662902832\n",
      "Test Epoch803 layer0 out_loss 0.15887506306171417, R2 0.07111793756484985\n",
      "Test Epoch803 layer1 out_loss 0.1578342318534851, R2 0.07720327377319336\n",
      "Test Epoch803 layer2 out_loss 0.15702316164970398, R2 0.08194530010223389\n",
      "Test Epoch803 layer3 out_loss 0.15680573880672455, R2 0.08321648836135864\n",
      "Test Epoch803 layer4 out_loss 0.15681208670139313, R2 0.08317935466766357\n",
      "Train 804 | out_loss 0.3940449059009552: 100%|█| 125/125 [00:00<00:00, 261.34it/\n",
      "Train Epoch804 out_loss 0.15527138113975525, R2 0.08918923139572144\n",
      "Test Epoch804 layer0 out_loss 0.1589193493127823, R2 0.07085901498794556\n",
      "Test Epoch804 layer1 out_loss 0.1580413579940796, R2 0.07599234580993652\n",
      "Test Epoch804 layer2 out_loss 0.1572466939687729, R2 0.08063840866088867\n",
      "Test Epoch804 layer3 out_loss 0.15690502524375916, R2 0.08263593912124634\n",
      "Test Epoch804 layer4 out_loss 0.1568991243839264, R2 0.08267050981521606\n",
      "Train 805 | out_loss 0.3942418396472931: 100%|█| 125/125 [00:00<00:00, 249.76it/\n",
      "Train Epoch805 out_loss 0.15542662143707275, R2 0.08827859163284302\n",
      "Test Epoch805 layer0 out_loss 0.1588943749666214, R2 0.07100498676300049\n",
      "Test Epoch805 layer1 out_loss 0.15789517760276794, R2 0.07684695720672607\n",
      "Test Epoch805 layer2 out_loss 0.15727443993091583, R2 0.08047610521316528\n",
      "Test Epoch805 layer3 out_loss 0.1570027619600296, R2 0.08206456899642944\n",
      "Test Epoch805 layer4 out_loss 0.15699459612369537, R2 0.08211231231689453\n",
      "Train 806 | out_loss 0.39406144618988037: 100%|█| 125/125 [00:00<00:00, 250.31it\n",
      "Train Epoch806 out_loss 0.15528441965579987, R2 0.08911269903182983\n",
      "Test Epoch806 layer0 out_loss 0.15889202058315277, R2 0.07101881504058838\n",
      "Test Epoch806 layer1 out_loss 0.1583234816789627, R2 0.07434278726577759\n",
      "Test Epoch806 layer2 out_loss 0.15742827951908112, R2 0.07957667112350464\n",
      "Test Epoch806 layer3 out_loss 0.1574430912733078, R2 0.07949018478393555\n",
      "Test Epoch806 layer4 out_loss 0.1574465036392212, R2 0.07947015762329102\n",
      "Train 807 | out_loss 0.3941619098186493: 100%|█| 125/125 [00:00<00:00, 251.19it/\n",
      "Train Epoch807 out_loss 0.1553635597229004, R2 0.08864849805831909\n",
      "Test Epoch807 layer0 out_loss 0.1589585840702057, R2 0.07062959671020508\n",
      "Test Epoch807 layer1 out_loss 0.15796303749084473, R2 0.0764501690864563\n",
      "Test Epoch807 layer2 out_loss 0.15720060467720032, R2 0.08090782165527344\n",
      "Test Epoch807 layer3 out_loss 0.15686944127082825, R2 0.08284401893615723\n",
      "Test Epoch807 layer4 out_loss 0.15691767632961273, R2 0.08256202936172485\n",
      "Train 808 | out_loss 0.3941780924797058: 100%|█| 125/125 [00:00<00:00, 256.86it/\n",
      "Train Epoch808 out_loss 0.1553763598203659, R2 0.0885733962059021\n",
      "Test Epoch808 layer0 out_loss 0.15890967845916748, R2 0.07091552019119263\n",
      "Test Epoch808 layer1 out_loss 0.15792499482631683, R2 0.07667267322540283\n",
      "Test Epoch808 layer2 out_loss 0.15724167227745056, R2 0.08066773414611816\n",
      "Test Epoch808 layer3 out_loss 0.15710055828094482, R2 0.08149278163909912\n",
      "Test Epoch808 layer4 out_loss 0.15699969232082367, R2 0.08208250999450684\n",
      "Train 809 | out_loss 0.3941148817539215: 100%|█| 125/125 [00:00<00:00, 248.53it/\n",
      "Train Epoch809 out_loss 0.1553265005350113, R2 0.08886581659317017\n",
      "Test Epoch809 layer0 out_loss 0.1588515341281891, R2 0.07125550508499146\n",
      "Test Epoch809 layer1 out_loss 0.15780200064182281, R2 0.07739168405532837\n",
      "Test Epoch809 layer2 out_loss 0.15698225796222687, R2 0.08218443393707275\n",
      "Test Epoch809 layer3 out_loss 0.15677377581596375, R2 0.08340334892272949\n",
      "Test Epoch809 layer4 out_loss 0.15677104890346527, R2 0.0834193229675293\n",
      "Train 810 | out_loss 0.39416107535362244: 100%|█| 125/125 [00:00<00:00, 257.70it\n",
      "Train Epoch810 out_loss 0.15536294877529144, R2 0.08865207433700562\n",
      "Test Epoch810 layer0 out_loss 0.15898531675338745, R2 0.070473313331604\n",
      "Test Epoch810 layer1 out_loss 0.15842679142951965, R2 0.07373881340026855\n",
      "Test Epoch810 layer2 out_loss 0.1575646847486496, R2 0.07877922058105469\n",
      "Test Epoch810 layer3 out_loss 0.15734168887138367, R2 0.0800829529762268\n",
      "Test Epoch810 layer4 out_loss 0.15741214156150818, R2 0.07967108488082886\n",
      "Train 811 | out_loss 0.39421379566192627: 100%|█| 125/125 [00:00<00:00, 253.89it\n",
      "Train Epoch811 out_loss 0.1554044783115387, R2 0.0884084701538086\n",
      "Test Epoch811 layer0 out_loss 0.15888796746730804, R2 0.07104247808456421\n",
      "Test Epoch811 layer1 out_loss 0.15794925391674042, R2 0.0765308141708374\n",
      "Test Epoch811 layer2 out_loss 0.15708425641059875, R2 0.08158808946609497\n",
      "Test Epoch811 layer3 out_loss 0.1568039059638977, R2 0.08322715759277344\n",
      "Test Epoch811 layer4 out_loss 0.15678450465202332, R2 0.08334064483642578\n",
      "Train 812 | out_loss 0.3941100835800171: 100%|█| 125/125 [00:00<00:00, 264.51it/\n",
      "Train Epoch812 out_loss 0.15532268583774567, R2 0.08888822793960571\n",
      "Test Epoch812 layer0 out_loss 0.15887926518917084, R2 0.07109332084655762\n",
      "Test Epoch812 layer1 out_loss 0.15778443217277527, R2 0.07749438285827637\n",
      "Test Epoch812 layer2 out_loss 0.15707378089427948, R2 0.08164936304092407\n",
      "Test Epoch812 layer3 out_loss 0.15682393312454224, R2 0.08311015367507935\n",
      "Test Epoch812 layer4 out_loss 0.15685983002185822, R2 0.08290022611618042\n",
      "Train 813 | out_loss 0.3940848410129547: 100%|█| 125/125 [00:00<00:00, 250.02it/\n",
      "Train Epoch813 out_loss 0.15530282258987427, R2 0.0890047550201416\n",
      "Test Epoch813 layer0 out_loss 0.1589781492948532, R2 0.0705152153968811\n",
      "Test Epoch813 layer1 out_loss 0.15811239182949066, R2 0.07557696104049683\n",
      "Test Epoch813 layer2 out_loss 0.15724268555641174, R2 0.08066177368164062\n",
      "Test Epoch813 layer3 out_loss 0.15705713629722595, R2 0.08174669742584229\n",
      "Test Epoch813 layer4 out_loss 0.15707305073738098, R2 0.0816536545753479\n",
      "Train 814 | out_loss 0.3941287100315094: 100%|█| 125/125 [00:00<00:00, 251.63it/\n",
      "Train Epoch814 out_loss 0.1553374081850052, R2 0.08880192041397095\n",
      "Test Epoch814 layer0 out_loss 0.15889635682106018, R2 0.07099336385726929\n",
      "Test Epoch814 layer1 out_loss 0.15795961022377014, R2 0.0764702558517456\n",
      "Test Epoch814 layer2 out_loss 0.15703120827674866, R2 0.0818982720375061\n",
      "Test Epoch814 layer3 out_loss 0.15691056847572327, R2 0.0826035737991333\n",
      "Test Epoch814 layer4 out_loss 0.15690787136554718, R2 0.08261936902999878\n",
      "Train 815 | out_loss 0.39401474595069885: 100%|█| 125/125 [00:00<00:00, 242.96it\n",
      "Train Epoch815 out_loss 0.15524758398532867, R2 0.08932876586914062\n",
      "Test Epoch815 layer0 out_loss 0.1588764786720276, R2 0.07110965251922607\n",
      "Test Epoch815 layer1 out_loss 0.15779723227024078, R2 0.07741963863372803\n",
      "Test Epoch815 layer2 out_loss 0.15702693164348602, R2 0.08192324638366699\n",
      "Test Epoch815 layer3 out_loss 0.15687014162540436, R2 0.08283990621566772\n",
      "Test Epoch815 layer4 out_loss 0.15686918795108795, R2 0.08284550905227661\n",
      "Train 816 | out_loss 0.3941527009010315: 100%|█| 125/125 [00:00<00:00, 258.36it/\n",
      "Train Epoch816 out_loss 0.15535631775856018, R2 0.08869093656539917\n",
      "Test Epoch816 layer0 out_loss 0.15886110067367554, R2 0.07119953632354736\n",
      "Test Epoch816 layer1 out_loss 0.158172145485878, R2 0.07522761821746826\n",
      "Test Epoch816 layer2 out_loss 0.15712173283100128, R2 0.08136898279190063\n",
      "Test Epoch816 layer3 out_loss 0.15696482360363007, R2 0.08228635787963867\n",
      "Test Epoch816 layer4 out_loss 0.1569231003522873, R2 0.08253031969070435\n",
      "Train 817 | out_loss 0.39413538575172424: 100%|█| 125/125 [00:00<00:00, 239.79it\n",
      "Train Epoch817 out_loss 0.1553427129983902, R2 0.08877074718475342\n",
      "Test Epoch817 layer0 out_loss 0.1590062975883484, R2 0.07035064697265625\n",
      "Test Epoch817 layer1 out_loss 0.1579374074935913, R2 0.07660001516342163\n",
      "Test Epoch817 layer2 out_loss 0.15709197521209717, R2 0.08154290914535522\n",
      "Test Epoch817 layer3 out_loss 0.1569809466600418, R2 0.08219212293624878\n",
      "Test Epoch817 layer4 out_loss 0.15701265633106232, R2 0.08200675249099731\n",
      "Train 818 | out_loss 0.3940686285495758: 100%|█| 125/125 [00:00<00:00, 243.39it/\n",
      "Train Epoch818 out_loss 0.15529009699821472, R2 0.08907938003540039\n",
      "Test Epoch818 layer0 out_loss 0.15884515643119812, R2 0.07129281759262085\n",
      "Test Epoch818 layer1 out_loss 0.15786853432655334, R2 0.07700276374816895\n",
      "Test Epoch818 layer2 out_loss 0.15703831613063812, R2 0.08185666799545288\n",
      "Test Epoch818 layer3 out_loss 0.15681397914886475, R2 0.08316826820373535\n",
      "Test Epoch818 layer4 out_loss 0.15681466460227966, R2 0.0831642746925354\n",
      "Train 819 | out_loss 0.39411941170692444: 100%|█| 125/125 [00:00<00:00, 248.15it\n",
      "Train Epoch819 out_loss 0.1553301066160202, R2 0.08884471654891968\n",
      "Test Epoch819 layer0 out_loss 0.15885579586029053, R2 0.07123059034347534\n",
      "Test Epoch819 layer1 out_loss 0.1578468680381775, R2 0.07712942361831665\n",
      "Test Epoch819 layer2 out_loss 0.15711858868598938, R2 0.08138734102249146\n",
      "Test Epoch819 layer3 out_loss 0.1568039208650589, R2 0.08322709798812866\n",
      "Test Epoch819 layer4 out_loss 0.15679620206356049, R2 0.08327221870422363\n",
      "Train 820 | out_loss 0.39407259225845337: 100%|█| 125/125 [00:00<00:00, 256.71it\n",
      "Train Epoch820 out_loss 0.15529321134090424, R2 0.08906114101409912\n",
      "Test Epoch820 layer0 out_loss 0.15893389284610748, R2 0.07077395915985107\n",
      "Test Epoch820 layer1 out_loss 0.15814727544784546, R2 0.07537299394607544\n",
      "Test Epoch820 layer2 out_loss 0.1575239896774292, R2 0.07901716232299805\n",
      "Test Epoch820 layer3 out_loss 0.15720543265342712, R2 0.08087962865829468\n",
      "Test Epoch820 layer4 out_loss 0.1570594161748886, R2 0.08173328638076782\n",
      "Train 821 | out_loss 0.39421942830085754: 100%|█| 125/125 [00:00<00:00, 251.89it\n",
      "Train Epoch821 out_loss 0.15540896356105804, R2 0.08838212490081787\n",
      "Test Epoch821 layer0 out_loss 0.15885233879089355, R2 0.0712507963180542\n",
      "Test Epoch821 layer1 out_loss 0.15788574516773224, R2 0.07690209150314331\n",
      "Test Epoch821 layer2 out_loss 0.15705163776874542, R2 0.08177882432937622\n",
      "Test Epoch821 layer3 out_loss 0.15685072541236877, R2 0.08295345306396484\n",
      "Test Epoch821 layer4 out_loss 0.15686477720737457, R2 0.08287131786346436\n",
      "Train 822 | out_loss 0.39413291215896606: 100%|█| 125/125 [00:00<00:00, 245.24it\n",
      "Train Epoch822 out_loss 0.1553407609462738, R2 0.08878225088119507\n",
      "Test Epoch822 layer0 out_loss 0.15886282920837402, R2 0.07118946313858032\n",
      "Test Epoch822 layer1 out_loss 0.15781429409980774, R2 0.07731980085372925\n",
      "Test Epoch822 layer2 out_loss 0.15704384446144104, R2 0.0818244218826294\n",
      "Test Epoch822 layer3 out_loss 0.15681377053260803, R2 0.08316951990127563\n",
      "Test Epoch822 layer4 out_loss 0.15678012371063232, R2 0.0833662748336792\n",
      "Train 823 | out_loss 0.39406949281692505: 100%|█| 125/125 [00:00<00:00, 251.71it\n",
      "Train Epoch823 out_loss 0.15529073774814606, R2 0.08907562494277954\n",
      "Test Epoch823 layer0 out_loss 0.15901443362236023, R2 0.07030308246612549\n",
      "Test Epoch823 layer1 out_loss 0.15829357504844666, R2 0.07451766729354858\n",
      "Test Epoch823 layer2 out_loss 0.1571868509054184, R2 0.08098828792572021\n",
      "Test Epoch823 layer3 out_loss 0.15705882012844086, R2 0.08173680305480957\n",
      "Test Epoch823 layer4 out_loss 0.15708890557289124, R2 0.08156085014343262\n",
      "Train 824 | out_loss 0.39421510696411133: 100%|█| 125/125 [00:00<00:00, 249.51it\n",
      "Train Epoch824 out_loss 0.15540555119514465, R2 0.0884021520614624\n",
      "Test Epoch824 layer0 out_loss 0.15884584188461304, R2 0.07128876447677612\n",
      "Test Epoch824 layer1 out_loss 0.15780609846115112, R2 0.07736778259277344\n",
      "Test Epoch824 layer2 out_loss 0.1571544110774994, R2 0.08117794990539551\n",
      "Test Epoch824 layer3 out_loss 0.1568664014339447, R2 0.08286184072494507\n",
      "Test Epoch824 layer4 out_loss 0.15688584744930267, R2 0.08274811506271362\n",
      "Train 825 | out_loss 0.39407601952552795: 100%|█| 125/125 [00:00<00:00, 240.18it\n",
      "Train Epoch825 out_loss 0.15529592335224152, R2 0.08904522657394409\n",
      "Test Epoch825 layer0 out_loss 0.15896016359329224, R2 0.07062041759490967\n",
      "Test Epoch825 layer1 out_loss 0.15798935294151306, R2 0.07629632949829102\n",
      "Test Epoch825 layer2 out_loss 0.15729355812072754, R2 0.08036434650421143\n",
      "Test Epoch825 layer3 out_loss 0.15688851475715637, R2 0.08273249864578247\n",
      "Test Epoch825 layer4 out_loss 0.15684449672698975, R2 0.08298987150192261\n",
      "Train 826 | out_loss 0.39429983496665955: 100%|█| 125/125 [00:00<00:00, 250.62it\n",
      "Train Epoch826 out_loss 0.15547239780426025, R2 0.08801007270812988\n",
      "Test Epoch826 layer0 out_loss 0.158867746591568, R2 0.07116067409515381\n",
      "Test Epoch826 layer1 out_loss 0.15795022249221802, R2 0.07652509212493896\n",
      "Test Epoch826 layer2 out_loss 0.15719494223594666, R2 0.08094090223312378\n",
      "Test Epoch826 layer3 out_loss 0.15701864659786224, R2 0.08197170495986938\n",
      "Test Epoch826 layer4 out_loss 0.15711663663387299, R2 0.08139878511428833\n",
      "Train 827 | out_loss 0.3942040801048279: 100%|█| 125/125 [00:00<00:00, 238.14it/\n",
      "Train Epoch827 out_loss 0.15539683401584625, R2 0.08845329284667969\n",
      "Test Epoch827 layer0 out_loss 0.158848375082016, R2 0.07127398252487183\n",
      "Test Epoch827 layer1 out_loss 0.1579807549715042, R2 0.07634657621383667\n",
      "Test Epoch827 layer2 out_loss 0.15700185298919678, R2 0.08206993341445923\n",
      "Test Epoch827 layer3 out_loss 0.15696324408054352, R2 0.08229559659957886\n",
      "Test Epoch827 layer4 out_loss 0.156906396150589, R2 0.08262795209884644\n",
      "Train 828 | out_loss 0.3940637707710266: 100%|█| 125/125 [00:00<00:00, 233.97it/\n",
      "Train Epoch828 out_loss 0.1552862524986267, R2 0.08910197019577026\n",
      "Test Epoch828 layer0 out_loss 0.15889285504817963, R2 0.0710139274597168\n",
      "Test Epoch828 layer1 out_loss 0.157930389046669, R2 0.07664108276367188\n",
      "Test Epoch828 layer2 out_loss 0.15726958215236664, R2 0.08050459623336792\n",
      "Test Epoch828 layer3 out_loss 0.15700018405914307, R2 0.08207964897155762\n",
      "Test Epoch828 layer4 out_loss 0.15703967213630676, R2 0.08184874057769775\n",
      "Train 829 | out_loss 0.3941817879676819: 100%|█| 125/125 [00:00<00:00, 256.11it/\n",
      "Train Epoch829 out_loss 0.15537922084331512, R2 0.08855658769607544\n",
      "Test Epoch829 layer0 out_loss 0.15885192155838013, R2 0.07125318050384521\n",
      "Test Epoch829 layer1 out_loss 0.15793181955814362, R2 0.07663267850875854\n",
      "Test Epoch829 layer2 out_loss 0.15760649740695953, R2 0.07853478193283081\n",
      "Test Epoch829 layer3 out_loss 0.15710242092609406, R2 0.08148193359375\n",
      "Test Epoch829 layer4 out_loss 0.15721522271633148, R2 0.08082234859466553\n",
      "Train 830 | out_loss 0.3941710591316223: 100%|█| 125/125 [00:00<00:00, 251.42it/\n",
      "Train Epoch830 out_loss 0.1553708165884018, R2 0.08860588073730469\n",
      "Test Epoch830 layer0 out_loss 0.15888051688671112, R2 0.07108598947525024\n",
      "Test Epoch830 layer1 out_loss 0.15782660245895386, R2 0.07724791765213013\n",
      "Test Epoch830 layer2 out_loss 0.15694503486156464, R2 0.0824020504951477\n",
      "Test Epoch830 layer3 out_loss 0.1567264348268509, R2 0.08368009328842163\n",
      "Test Epoch830 layer4 out_loss 0.15674090385437012, R2 0.08359551429748535\n",
      "Train 831 | out_loss 0.3939467966556549: 100%|█| 125/125 [00:00<00:00, 239.57it/\n",
      "Train Epoch831 out_loss 0.15519410371780396, R2 0.0896424651145935\n",
      "Test Epoch831 layer0 out_loss 0.1588461548089981, R2 0.07128691673278809\n",
      "Test Epoch831 layer1 out_loss 0.15781670808792114, R2 0.07730567455291748\n",
      "Test Epoch831 layer2 out_loss 0.1570977419614792, R2 0.08150923252105713\n",
      "Test Epoch831 layer3 out_loss 0.15684552490711212, R2 0.08298391103744507\n",
      "Test Epoch831 layer4 out_loss 0.1568073332309723, R2 0.08320719003677368\n",
      "Train 832 | out_loss 0.3940332233905792: 100%|█| 125/125 [00:00<00:00, 258.23it/\n",
      "Train Epoch832 out_loss 0.15526214241981506, R2 0.08924335241317749\n",
      "Test Epoch832 layer0 out_loss 0.1588497757911682, R2 0.07126575708389282\n",
      "Test Epoch832 layer1 out_loss 0.1578650027513504, R2 0.07702332735061646\n",
      "Test Epoch832 layer2 out_loss 0.15702949464321136, R2 0.0819082260131836\n",
      "Test Epoch832 layer3 out_loss 0.1568838208913803, R2 0.08275997638702393\n",
      "Test Epoch832 layer4 out_loss 0.15689782798290253, R2 0.08267807960510254\n",
      "Train 833 | out_loss 0.3942146301269531: 100%|█| 125/125 [00:00<00:00, 251.43it/\n",
      "Train Epoch833 out_loss 0.15540511906147003, R2 0.08840471506118774\n",
      "Test Epoch833 layer0 out_loss 0.15885114669799805, R2 0.07125771045684814\n",
      "Test Epoch833 layer1 out_loss 0.1578323394060135, R2 0.07721436023712158\n",
      "Test Epoch833 layer2 out_loss 0.1570543348789215, R2 0.08176302909851074\n",
      "Test Epoch833 layer3 out_loss 0.15685030817985535, R2 0.08295589685440063\n",
      "Test Epoch833 layer4 out_loss 0.15683354437351227, R2 0.08305394649505615\n",
      "Train 834 | out_loss 0.39408937096595764: 100%|█| 125/125 [00:00<00:00, 253.14it\n",
      "Train Epoch834 out_loss 0.15530642867088318, R2 0.08898359537124634\n",
      "Test Epoch834 layer0 out_loss 0.15883760154247284, R2 0.07133692502975464\n",
      "Test Epoch834 layer1 out_loss 0.15777748823165894, R2 0.07753509283065796\n",
      "Test Epoch834 layer2 out_loss 0.15717047452926636, R2 0.08108401298522949\n",
      "Test Epoch834 layer3 out_loss 0.15681391954421997, R2 0.083168625831604\n",
      "Test Epoch834 layer4 out_loss 0.1568070948123932, R2 0.08320850133895874\n",
      "Train 835 | out_loss 0.39415812492370605: 100%|█| 125/125 [00:00<00:00, 256.86it\n",
      "Train Epoch835 out_loss 0.15536056458950043, R2 0.08866602182388306\n",
      "Test Epoch835 layer0 out_loss 0.1588415801525116, R2 0.07131367921829224\n",
      "Test Epoch835 layer1 out_loss 0.1578192561864853, R2 0.07729083299636841\n",
      "Test Epoch835 layer2 out_loss 0.15710574388504028, R2 0.08146244287490845\n",
      "Test Epoch835 layer3 out_loss 0.1568690538406372, R2 0.08284634351730347\n",
      "Test Epoch835 layer4 out_loss 0.15686410665512085, R2 0.08287525177001953\n",
      "Train 836 | out_loss 0.3940228819847107: 100%|█| 125/125 [00:00<00:00, 249.00it/\n",
      "Train Epoch836 out_loss 0.155254065990448, R2 0.0892907977104187\n",
      "Test Epoch836 layer0 out_loss 0.15884068608283997, R2 0.07131892442703247\n",
      "Test Epoch836 layer1 out_loss 0.15780560672283173, R2 0.07737064361572266\n",
      "Test Epoch836 layer2 out_loss 0.15702050924301147, R2 0.08196079730987549\n",
      "Test Epoch836 layer3 out_loss 0.15678757429122925, R2 0.08332270383834839\n",
      "Test Epoch836 layer4 out_loss 0.15677423775196075, R2 0.0834006667137146\n",
      "Train 837 | out_loss 0.39394012093544006: 100%|█| 125/125 [00:00<00:00, 255.55it\n",
      "Train Epoch837 out_loss 0.15518885850906372, R2 0.08967328071594238\n",
      "Test Epoch837 layer0 out_loss 0.15885034203529358, R2 0.07126247882843018\n",
      "Test Epoch837 layer1 out_loss 0.1577947437763214, R2 0.07743418216705322\n",
      "Test Epoch837 layer2 out_loss 0.15709027647972107, R2 0.08155292272567749\n",
      "Test Epoch837 layer3 out_loss 0.1570022851228714, R2 0.08206731081008911\n",
      "Test Epoch837 layer4 out_loss 0.15703243017196655, R2 0.08189111948013306\n",
      "Train 838 | out_loss 0.393912136554718: 100%|█| 125/125 [00:00<00:00, 237.31it/s\n",
      "Train Epoch838 out_loss 0.15516674518585205, R2 0.08980298042297363\n",
      "Test Epoch838 layer0 out_loss 0.15887995064258575, R2 0.07108938694000244\n",
      "Test Epoch838 layer1 out_loss 0.15812812745571136, R2 0.0754849910736084\n",
      "Test Epoch838 layer2 out_loss 0.15761549770832062, R2 0.07848215103149414\n",
      "Test Epoch838 layer3 out_loss 0.15747159719467163, R2 0.079323410987854\n",
      "Test Epoch838 layer4 out_loss 0.15728984773159027, R2 0.08038610219955444\n",
      "Train 839 | out_loss 0.3939940333366394: 100%|█| 125/125 [00:00<00:00, 258.76it/\n",
      "Train Epoch839 out_loss 0.155231311917305, R2 0.0894242525100708\n",
      "Test Epoch839 layer0 out_loss 0.15897071361541748, R2 0.07055866718292236\n",
      "Test Epoch839 layer1 out_loss 0.15832440555095673, R2 0.0743374228477478\n",
      "Test Epoch839 layer2 out_loss 0.1574830263853073, R2 0.07925659418106079\n",
      "Test Epoch839 layer3 out_loss 0.15748293697834015, R2 0.07925713062286377\n",
      "Test Epoch839 layer4 out_loss 0.15757863223552704, R2 0.07869768142700195\n",
      "Train 840 | out_loss 0.39394742250442505: 100%|█| 125/125 [00:00<00:00, 260.84it\n",
      "Train Epoch840 out_loss 0.15519453585147858, R2 0.08963996171951294\n",
      "Test Epoch840 layer0 out_loss 0.15891778469085693, R2 0.07086813449859619\n",
      "Test Epoch840 layer1 out_loss 0.1579182744026184, R2 0.07671189308166504\n",
      "Test Epoch840 layer2 out_loss 0.1574929654598236, R2 0.07919853925704956\n",
      "Test Epoch840 layer3 out_loss 0.15758460760116577, R2 0.0786626935005188\n",
      "Test Epoch840 layer4 out_loss 0.1574559509754181, R2 0.079414963722229\n",
      "Train 841 | out_loss 0.394147127866745: 100%|█| 125/125 [00:00<00:00, 241.75it/s\n",
      "Train Epoch841 out_loss 0.15535196661949158, R2 0.08871650695800781\n",
      "Test Epoch841 layer0 out_loss 0.1588488519191742, R2 0.07127118110656738\n",
      "Test Epoch841 layer1 out_loss 0.15777842700481415, R2 0.07752954959869385\n",
      "Test Epoch841 layer2 out_loss 0.15695127844810486, R2 0.08236557245254517\n",
      "Test Epoch841 layer3 out_loss 0.15675008296966553, R2 0.08354192972183228\n",
      "Test Epoch841 layer4 out_loss 0.15675656497478485, R2 0.08350402116775513\n",
      "Train 842 | out_loss 0.3941687047481537: 100%|█| 125/125 [00:00<00:00, 249.08it/\n",
      "Train Epoch842 out_loss 0.15536899864673615, R2 0.08861654996871948\n",
      "Test Epoch842 layer0 out_loss 0.15883751213550568, R2 0.07133746147155762\n",
      "Test Epoch842 layer1 out_loss 0.15778480470180511, R2 0.07749223709106445\n",
      "Test Epoch842 layer2 out_loss 0.15706942975521088, R2 0.08167481422424316\n",
      "Test Epoch842 layer3 out_loss 0.15705229341983795, R2 0.08177495002746582\n",
      "Test Epoch842 layer4 out_loss 0.15693359076976776, R2 0.0824689269065857\n",
      "Train 843 | out_loss 0.3940282166004181: 100%|█| 125/125 [00:00<00:00, 247.75it/\n",
      "Train Epoch843 out_loss 0.15525825321674347, R2 0.08926618099212646\n",
      "Test Epoch843 layer0 out_loss 0.15885567665100098, R2 0.07123124599456787\n",
      "Test Epoch843 layer1 out_loss 0.15781891345977783, R2 0.07729285955429077\n",
      "Test Epoch843 layer2 out_loss 0.15711034834384918, R2 0.08143550157546997\n",
      "Test Epoch843 layer3 out_loss 0.15702974796295166, R2 0.08190673589706421\n",
      "Test Epoch843 layer4 out_loss 0.1569981873035431, R2 0.0820913314819336\n",
      "Train 844 | out_loss 0.3940771818161011: 100%|█| 125/125 [00:00<00:00, 236.80it/\n",
      "Train Epoch844 out_loss 0.15529686212539673, R2 0.08903974294662476\n",
      "Test Epoch844 layer0 out_loss 0.1588815301656723, R2 0.07108008861541748\n",
      "Test Epoch844 layer1 out_loss 0.1578601747751236, R2 0.07705163955688477\n",
      "Test Epoch844 layer2 out_loss 0.157271608710289, R2 0.08049273490905762\n",
      "Test Epoch844 layer3 out_loss 0.15720003843307495, R2 0.08091109991073608\n",
      "Test Epoch844 layer4 out_loss 0.15712888538837433, R2 0.08132714033126831\n",
      "Train 845 | out_loss 0.3939453661441803: 100%|█| 125/125 [00:00<00:00, 244.80it/\n",
      "Train Epoch845 out_loss 0.15519288182258606, R2 0.08964961767196655\n",
      "Test Epoch845 layer0 out_loss 0.1588468849658966, R2 0.07128268480300903\n",
      "Test Epoch845 layer1 out_loss 0.15783190727233887, R2 0.07721686363220215\n",
      "Test Epoch845 layer2 out_loss 0.15718770027160645, R2 0.08098328113555908\n",
      "Test Epoch845 layer3 out_loss 0.15689486265182495, R2 0.0826953649520874\n",
      "Test Epoch845 layer4 out_loss 0.15684804320335388, R2 0.08296918869018555\n",
      "Train 846 | out_loss 0.39417752623558044: 100%|█| 125/125 [00:00<00:00, 255.27it\n",
      "Train Epoch846 out_loss 0.1553758978843689, R2 0.08857607841491699\n",
      "Test Epoch846 layer0 out_loss 0.1588437706232071, R2 0.0713009238243103\n",
      "Test Epoch846 layer1 out_loss 0.15774913132190704, R2 0.07770079374313354\n",
      "Test Epoch846 layer2 out_loss 0.15704281628131866, R2 0.08183038234710693\n",
      "Test Epoch846 layer3 out_loss 0.15673886239528656, R2 0.0836074948310852\n",
      "Test Epoch846 layer4 out_loss 0.15672916173934937, R2 0.08366423845291138\n",
      "Train 847 | out_loss 0.39405038952827454: 100%|█| 125/125 [00:00<00:00, 257.34it\n",
      "Train Epoch847 out_loss 0.1552756428718567, R2 0.089164137840271\n",
      "Test Epoch847 layer0 out_loss 0.1589556336402893, R2 0.07064688205718994\n",
      "Test Epoch847 layer1 out_loss 0.15802136063575745, R2 0.07610917091369629\n",
      "Test Epoch847 layer2 out_loss 0.1572677493095398, R2 0.08051526546478271\n",
      "Test Epoch847 layer3 out_loss 0.15698948502540588, R2 0.082142174243927\n",
      "Test Epoch847 layer4 out_loss 0.15700656175613403, R2 0.08204233646392822\n",
      "Train 848 | out_loss 0.3938879370689392: 100%|█| 125/125 [00:00<00:00, 251.16it/\n",
      "Train Epoch848 out_loss 0.1551477015018463, R2 0.08991467952728271\n",
      "Test Epoch848 layer0 out_loss 0.1588798612356186, R2 0.07108980417251587\n",
      "Test Epoch848 layer1 out_loss 0.1583951860666275, R2 0.07392358779907227\n",
      "Test Epoch848 layer2 out_loss 0.1574234962463379, R2 0.07960468530654907\n",
      "Test Epoch848 layer3 out_loss 0.15730616450309753, R2 0.08029067516326904\n",
      "Test Epoch848 layer4 out_loss 0.15733669698238373, R2 0.08011215925216675\n",
      "Train 849 | out_loss 0.39392298460006714: 100%|█| 125/125 [00:00<00:00, 245.69it\n",
      "Train Epoch849 out_loss 0.1551753133535385, R2 0.08975273370742798\n",
      "Test Epoch849 layer0 out_loss 0.1588512510061264, R2 0.07125711441040039\n",
      "Test Epoch849 layer1 out_loss 0.15778227150440216, R2 0.07750707864761353\n",
      "Test Epoch849 layer2 out_loss 0.15700912475585938, R2 0.0820273756980896\n",
      "Test Epoch849 layer3 out_loss 0.15669915080070496, R2 0.08383959531784058\n",
      "Test Epoch849 layer4 out_loss 0.15669482946395874, R2 0.08386492729187012\n",
      "Train 850 | out_loss 0.3940631151199341: 100%|█| 125/125 [00:00<00:00, 248.46it/\n",
      "Train Epoch850 out_loss 0.15528573095798492, R2 0.08910501003265381\n",
      "Test Epoch850 layer0 out_loss 0.15883085131645203, R2 0.07137638330459595\n",
      "Test Epoch850 layer1 out_loss 0.15777210891246796, R2 0.07756644487380981\n",
      "Test Epoch850 layer2 out_loss 0.15704503655433655, R2 0.08181732892990112\n",
      "Test Epoch850 layer3 out_loss 0.156923308968544, R2 0.08252906799316406\n",
      "Test Epoch850 layer4 out_loss 0.15690405666828156, R2 0.08264166116714478\n",
      "Train 851 | out_loss 0.3938855230808258: 100%|█| 125/125 [00:00<00:00, 241.61it/\n",
      "Train Epoch851 out_loss 0.1551458090543747, R2 0.08992576599121094\n",
      "Test Epoch851 layer0 out_loss 0.1588413566350937, R2 0.0713149905204773\n",
      "Test Epoch851 layer1 out_loss 0.15799908339977264, R2 0.07623940706253052\n",
      "Test Epoch851 layer2 out_loss 0.1572488695383072, R2 0.08062565326690674\n",
      "Test Epoch851 layer3 out_loss 0.15678925812244415, R2 0.08331280946731567\n",
      "Test Epoch851 layer4 out_loss 0.15683327615261078, R2 0.08305543661117554\n",
      "Train 852 | out_loss 0.3939424157142639: 100%|█| 125/125 [00:00<00:00, 246.97it/\n",
      "Train Epoch852 out_loss 0.15519064664840698, R2 0.08966279029846191\n",
      "Test Epoch852 layer0 out_loss 0.15900413691997528, R2 0.07036328315734863\n",
      "Test Epoch852 layer1 out_loss 0.15806052088737488, R2 0.07588028907775879\n",
      "Test Epoch852 layer2 out_loss 0.15703779458999634, R2 0.08185970783233643\n",
      "Test Epoch852 layer3 out_loss 0.15683089196681976, R2 0.08306944370269775\n",
      "Test Epoch852 layer4 out_loss 0.15677008032798767, R2 0.08342492580413818\n",
      "Train 853 | out_loss 0.3939543664455414: 100%|█| 125/125 [00:00<00:00, 249.82it/\n",
      "Train Epoch853 out_loss 0.1552000790834427, R2 0.08960747718811035\n",
      "Test Epoch853 layer0 out_loss 0.15883469581604004, R2 0.07135391235351562\n",
      "Test Epoch853 layer1 out_loss 0.15777043998241425, R2 0.07757622003555298\n",
      "Test Epoch853 layer2 out_loss 0.156980499625206, R2 0.0821947455406189\n",
      "Test Epoch853 layer3 out_loss 0.15670102834701538, R2 0.0838286280632019\n",
      "Test Epoch853 layer4 out_loss 0.15669716894626617, R2 0.08385121822357178\n",
      "Train 854 | out_loss 0.39402109384536743: 100%|█| 125/125 [00:00<00:00, 244.33it\n",
      "Train Epoch854 out_loss 0.1552526205778122, R2 0.08929920196533203\n",
      "Test Epoch854 layer0 out_loss 0.158840611577034, R2 0.07131940126419067\n",
      "Test Epoch854 layer1 out_loss 0.15776784718036652, R2 0.07759135961532593\n",
      "Test Epoch854 layer2 out_loss 0.15703019499778748, R2 0.08190417289733887\n",
      "Test Epoch854 layer3 out_loss 0.15690062940120697, R2 0.08266162872314453\n",
      "Test Epoch854 layer4 out_loss 0.1569039672613144, R2 0.08264219760894775\n",
      "Train 855 | out_loss 0.3940819501876831: 100%|█| 125/125 [00:00<00:00, 242.69it/\n",
      "Train Epoch855 out_loss 0.15530060231685638, R2 0.08901780843734741\n",
      "Test Epoch855 layer0 out_loss 0.15881739556789398, R2 0.07145506143569946\n",
      "Test Epoch855 layer1 out_loss 0.15787848830223083, R2 0.07694447040557861\n",
      "Test Epoch855 layer2 out_loss 0.15709201991558075, R2 0.08154267072677612\n",
      "Test Epoch855 layer3 out_loss 0.15682733058929443, R2 0.08309024572372437\n",
      "Test Epoch855 layer4 out_loss 0.15681077539920807, R2 0.0831870436668396\n",
      "Train 856 | out_loss 0.39390450716018677: 100%|█| 125/125 [00:00<00:00, 239.90it\n",
      "Train Epoch856 out_loss 0.15516075491905212, R2 0.08983814716339111\n",
      "Test Epoch856 layer0 out_loss 0.15884286165237427, R2 0.07130616903305054\n",
      "Test Epoch856 layer1 out_loss 0.15780100226402283, R2 0.07739758491516113\n",
      "Test Epoch856 layer2 out_loss 0.1570536494255066, R2 0.0817670226097107\n",
      "Test Epoch856 layer3 out_loss 0.15689727663993835, R2 0.08268129825592041\n",
      "Test Epoch856 layer4 out_loss 0.15687759220600128, R2 0.08279633522033691\n",
      "Train 857 | out_loss 0.39388778805732727: 100%|█| 125/125 [00:00<00:00, 264.46it\n",
      "Train Epoch857 out_loss 0.15514759719371796, R2 0.08991527557373047\n",
      "Test Epoch857 layer0 out_loss 0.15886390209197998, R2 0.07118314504623413\n",
      "Test Epoch857 layer1 out_loss 0.15832556784152985, R2 0.07433062791824341\n",
      "Test Epoch857 layer2 out_loss 0.15732818841934204, R2 0.0801619291305542\n",
      "Test Epoch857 layer3 out_loss 0.15721222758293152, R2 0.08083987236022949\n",
      "Test Epoch857 layer4 out_loss 0.1572890281677246, R2 0.08039087057113647\n",
      "Train 858 | out_loss 0.39402446150779724: 100%|█| 125/125 [00:00<00:00, 257.33it\n",
      "Train Epoch858 out_loss 0.1552552580833435, R2 0.08928370475769043\n",
      "Test Epoch858 layer0 out_loss 0.15888512134552002, R2 0.07105916738510132\n",
      "Test Epoch858 layer1 out_loss 0.15791422128677368, R2 0.07673561573028564\n",
      "Test Epoch858 layer2 out_loss 0.15735498070716858, R2 0.08000528812408447\n",
      "Test Epoch858 layer3 out_loss 0.15703518688678741, R2 0.08187496662139893\n",
      "Test Epoch858 layer4 out_loss 0.15689772367477417, R2 0.08267867565155029\n",
      "Train 859 | out_loss 0.39417070150375366: 100%|█| 125/125 [00:00<00:00, 250.82it\n",
      "Train Epoch859 out_loss 0.1553705334663391, R2 0.08860760927200317\n",
      "Test Epoch859 layer0 out_loss 0.15882322192192078, R2 0.07142096757888794\n",
      "Test Epoch859 layer1 out_loss 0.15794287621974945, R2 0.07656806707382202\n",
      "Test Epoch859 layer2 out_loss 0.15732184052467346, R2 0.08019906282424927\n",
      "Test Epoch859 layer3 out_loss 0.1574234664440155, R2 0.0796048641204834\n",
      "Test Epoch859 layer4 out_loss 0.15739138424396515, R2 0.07979238033294678\n",
      "Train 860 | out_loss 0.3938744366168976: 100%|█| 125/125 [00:00<00:00, 260.17it/\n",
      "Train Epoch860 out_loss 0.1551371067762375, R2 0.08997684717178345\n",
      "Test Epoch860 layer0 out_loss 0.1588321179151535, R2 0.0713689923286438\n",
      "Test Epoch860 layer1 out_loss 0.1579148769378662, R2 0.07673180103302002\n",
      "Test Epoch860 layer2 out_loss 0.15737277269363403, R2 0.07990121841430664\n",
      "Test Epoch860 layer3 out_loss 0.1571073681116104, R2 0.08145302534103394\n",
      "Test Epoch860 layer4 out_loss 0.1569783091545105, R2 0.0822075605392456\n",
      "Train 861 | out_loss 0.393971711397171: 100%|█| 125/125 [00:00<00:00, 259.41it/s\n",
      "Train Epoch861 out_loss 0.15521369874477386, R2 0.08952754735946655\n",
      "Test Epoch861 layer0 out_loss 0.15882916748523712, R2 0.07138627767562866\n",
      "Test Epoch861 layer1 out_loss 0.1577814221382141, R2 0.07751202583312988\n",
      "Test Epoch861 layer2 out_loss 0.15697820484638214, R2 0.08220809698104858\n",
      "Test Epoch861 layer3 out_loss 0.1567407250404358, R2 0.08359658718109131\n",
      "Test Epoch861 layer4 out_loss 0.1567537635564804, R2 0.08352029323577881\n",
      "Train 862 | out_loss 0.3940497636795044: 100%|█| 125/125 [00:00<00:00, 249.13it/\n",
      "Train Epoch862 out_loss 0.15527521073818207, R2 0.08916676044464111\n",
      "Test Epoch862 layer0 out_loss 0.15884412825107574, R2 0.07129877805709839\n",
      "Test Epoch862 layer1 out_loss 0.1577722132205963, R2 0.07756584882736206\n",
      "Test Epoch862 layer2 out_loss 0.15703365206718445, R2 0.08188396692276001\n",
      "Test Epoch862 layer3 out_loss 0.1566922515630722, R2 0.08388000726699829\n",
      "Test Epoch862 layer4 out_loss 0.15672464668750763, R2 0.08369064331054688\n",
      "Train 863 | out_loss 0.39402076601982117: 100%|█| 125/125 [00:00<00:00, 251.34it\n",
      "Train Epoch863 out_loss 0.1552523970603943, R2 0.08930057287216187\n",
      "Test Epoch863 layer0 out_loss 0.15883876383304596, R2 0.07133018970489502\n",
      "Test Epoch863 layer1 out_loss 0.15827535092830658, R2 0.07462424039840698\n",
      "Test Epoch863 layer2 out_loss 0.1582060009241104, R2 0.07502973079681396\n",
      "Test Epoch863 layer3 out_loss 0.1582828313112259, R2 0.07458043098449707\n",
      "Test Epoch863 layer4 out_loss 0.1583528071641922, R2 0.07417130470275879\n",
      "Train 864 | out_loss 0.393955260515213: 100%|█| 125/125 [00:00<00:00, 246.08it/s\n",
      "Train Epoch864 out_loss 0.15520073473453522, R2 0.08960360288619995\n",
      "Test Epoch864 layer0 out_loss 0.1588727980852127, R2 0.07113111019134521\n",
      "Test Epoch864 layer1 out_loss 0.15789386630058289, R2 0.0768546462059021\n",
      "Test Epoch864 layer2 out_loss 0.15732192993164062, R2 0.08019852638244629\n",
      "Test Epoch864 layer3 out_loss 0.1571475863456726, R2 0.08121782541275024\n",
      "Test Epoch864 layer4 out_loss 0.15685193240642548, R2 0.08294641971588135\n",
      "Train 865 | out_loss 0.3939688205718994: 100%|█| 125/125 [00:00<00:00, 259.00it/\n",
      "Train Epoch865 out_loss 0.15521138906478882, R2 0.08954107761383057\n",
      "Test Epoch865 layer0 out_loss 0.1588650494813919, R2 0.07117646932601929\n",
      "Test Epoch865 layer1 out_loss 0.15786556899547577, R2 0.07702004909515381\n",
      "Test Epoch865 layer2 out_loss 0.15711365640163422, R2 0.08141618967056274\n",
      "Test Epoch865 layer3 out_loss 0.15678730607032776, R2 0.08332425355911255\n",
      "Test Epoch865 layer4 out_loss 0.15678058564662933, R2 0.0833635926246643\n",
      "Train 866 | out_loss 0.3939206600189209: 100%|█| 125/125 [00:00<00:00, 253.31it/\n",
      "Train Epoch866 out_loss 0.15517351031303406, R2 0.08976328372955322\n",
      "Test Epoch866 layer0 out_loss 0.15883010625839233, R2 0.07138073444366455\n",
      "Test Epoch866 layer1 out_loss 0.1579159051179886, R2 0.07672572135925293\n",
      "Test Epoch866 layer2 out_loss 0.1572708785533905, R2 0.08049702644348145\n",
      "Test Epoch866 layer3 out_loss 0.1569758653640747, R2 0.0822218656539917\n",
      "Test Epoch866 layer4 out_loss 0.15695321559906006, R2 0.08235424757003784\n",
      "Train 867 | out_loss 0.3939881920814514: 100%|█| 125/125 [00:00<00:00, 253.69it/\n",
      "Train Epoch867 out_loss 0.1552266925573349, R2 0.08945131301879883\n",
      "Test Epoch867 layer0 out_loss 0.15888571739196777, R2 0.07105565071105957\n",
      "Test Epoch867 layer1 out_loss 0.15789295732975006, R2 0.07685989141464233\n",
      "Test Epoch867 layer2 out_loss 0.15690447390079498, R2 0.08263915777206421\n",
      "Test Epoch867 layer3 out_loss 0.15666800737380981, R2 0.0840216875076294\n",
      "Test Epoch867 layer4 out_loss 0.1566588431596756, R2 0.08407533168792725\n",
      "Train 868 | out_loss 0.39424848556518555: 100%|█| 125/125 [00:00<00:00, 246.99it\n",
      "Train Epoch868 out_loss 0.1554318368434906, R2 0.08824795484542847\n",
      "Test Epoch868 layer0 out_loss 0.15893873572349548, R2 0.07074564695358276\n",
      "Test Epoch868 layer1 out_loss 0.15789474546909332, R2 0.07684946060180664\n",
      "Test Epoch868 layer2 out_loss 0.1568949669599533, R2 0.08269476890563965\n",
      "Test Epoch868 layer3 out_loss 0.15664564073085785, R2 0.0841524600982666\n",
      "Test Epoch868 layer4 out_loss 0.15665532648563385, R2 0.08409589529037476\n",
      "Train 869 | out_loss 0.39396989345550537: 100%|█| 125/125 [00:00<00:00, 257.75it\n",
      "Train Epoch869 out_loss 0.15521225333213806, R2 0.08953607082366943\n",
      "Test Epoch869 layer0 out_loss 0.15884141623973846, R2 0.07131463289260864\n",
      "Test Epoch869 layer1 out_loss 0.15793001651763916, R2 0.07664322853088379\n",
      "Test Epoch869 layer2 out_loss 0.15746444463729858, R2 0.07936525344848633\n",
      "Test Epoch869 layer3 out_loss 0.15720652043819427, R2 0.08087319135665894\n",
      "Test Epoch869 layer4 out_loss 0.15710154175758362, R2 0.08148699998855591\n",
      "Train 870 | out_loss 0.3941000699996948: 100%|█| 125/125 [00:00<00:00, 255.73it/\n",
      "Train Epoch870 out_loss 0.15531490743160248, R2 0.08893382549285889\n",
      "Test Epoch870 layer0 out_loss 0.15883684158325195, R2 0.07134139537811279\n",
      "Test Epoch870 layer1 out_loss 0.15789276361465454, R2 0.07686108350753784\n",
      "Test Epoch870 layer2 out_loss 0.1570872664451599, R2 0.08157050609588623\n",
      "Test Epoch870 layer3 out_loss 0.15692056715488434, R2 0.08254516124725342\n",
      "Test Epoch870 layer4 out_loss 0.15692846477031708, R2 0.08249890804290771\n",
      "Train 871 | out_loss 0.39378654956817627: 100%|█| 125/125 [00:00<00:00, 262.01it\n",
      "Train Epoch871 out_loss 0.15506784617900848, R2 0.09038317203521729\n",
      "Test Epoch871 layer0 out_loss 0.15893127024173737, R2 0.07078927755355835\n",
      "Test Epoch871 layer1 out_loss 0.15784165263175964, R2 0.0771598219871521\n",
      "Test Epoch871 layer2 out_loss 0.15716436505317688, R2 0.08111971616744995\n",
      "Test Epoch871 layer3 out_loss 0.15682244300842285, R2 0.08311879634857178\n",
      "Test Epoch871 layer4 out_loss 0.1567600667476654, R2 0.08348345756530762\n",
      "Train 872 | out_loss 0.39392343163490295: 100%|█| 125/125 [00:00<00:00, 256.29it\n",
      "Train Epoch872 out_loss 0.15517565608024597, R2 0.08975070714950562\n",
      "Test Epoch872 layer0 out_loss 0.1588638722896576, R2 0.07118332386016846\n",
      "Test Epoch872 layer1 out_loss 0.15786869823932648, R2 0.07700175046920776\n",
      "Test Epoch872 layer2 out_loss 0.15718868374824524, R2 0.08097749948501587\n",
      "Test Epoch872 layer3 out_loss 0.15723229944705963, R2 0.08072251081466675\n",
      "Test Epoch872 layer4 out_loss 0.15708981454372406, R2 0.08155560493469238\n",
      "Train 873 | out_loss 0.39400824904441833: 100%|█| 125/125 [00:00<00:00, 252.29it\n",
      "Train Epoch873 out_loss 0.15524250268936157, R2 0.08935856819152832\n",
      "Test Epoch873 layer0 out_loss 0.15889954566955566, R2 0.07097476720809937\n",
      "Test Epoch873 layer1 out_loss 0.15807560086250305, R2 0.07579201459884644\n",
      "Test Epoch873 layer2 out_loss 0.15716387331485748, R2 0.08112257719039917\n",
      "Test Epoch873 layer3 out_loss 0.1569116711616516, R2 0.08259713649749756\n",
      "Test Epoch873 layer4 out_loss 0.15697228908538818, R2 0.08224272727966309\n",
      "Train 874 | out_loss 0.39376336336135864: 100%|█| 125/125 [00:00<00:00, 255.92it\n",
      "Train Epoch874 out_loss 0.1550496518611908, R2 0.09048986434936523\n",
      "Test Epoch874 layer0 out_loss 0.15898261964321136, R2 0.07048904895782471\n",
      "Test Epoch874 layer1 out_loss 0.15773145854473114, R2 0.07780414819717407\n",
      "Test Epoch874 layer2 out_loss 0.1569175124168396, R2 0.08256298303604126\n",
      "Test Epoch874 layer3 out_loss 0.15666915476322174, R2 0.08401501178741455\n",
      "Test Epoch874 layer4 out_loss 0.15665920078754425, R2 0.08407324552536011\n",
      "Train 875 | out_loss 0.39397677779197693: 100%|█| 125/125 [00:00<00:00, 240.72it\n",
      "Train Epoch875 out_loss 0.1552177518606186, R2 0.08950376510620117\n",
      "Test Epoch875 layer0 out_loss 0.1588168442249298, R2 0.07145828008651733\n",
      "Test Epoch875 layer1 out_loss 0.15783056616783142, R2 0.07722467184066772\n",
      "Test Epoch875 layer2 out_loss 0.15710270404815674, R2 0.08148020505905151\n",
      "Test Epoch875 layer3 out_loss 0.15714602172374725, R2 0.08122694492340088\n",
      "Test Epoch875 layer4 out_loss 0.1571027636528015, R2 0.08147984743118286\n",
      "Train 876 | out_loss 0.3941407799720764: 100%|█| 125/125 [00:00<00:00, 250.55it/\n",
      "Train Epoch876 out_loss 0.15534695982933044, R2 0.08874589204788208\n",
      "Test Epoch876 layer0 out_loss 0.15888048708438873, R2 0.07108616828918457\n",
      "Test Epoch876 layer1 out_loss 0.15783627331256866, R2 0.07719135284423828\n",
      "Test Epoch876 layer2 out_loss 0.1570005863904953, R2 0.08207732439041138\n",
      "Test Epoch876 layer3 out_loss 0.15669696033000946, R2 0.08385246992111206\n",
      "Test Epoch876 layer4 out_loss 0.15665987133979797, R2 0.08406931161880493\n",
      "Train 877 | out_loss 0.3940718173980713: 100%|█| 125/125 [00:00<00:00, 253.04it/\n",
      "Train Epoch877 out_loss 0.1552925705909729, R2 0.08906489610671997\n",
      "Test Epoch877 layer0 out_loss 0.15880849957466125, R2 0.07150709629058838\n",
      "Test Epoch877 layer1 out_loss 0.15782666206359863, R2 0.07724756002426147\n",
      "Test Epoch877 layer2 out_loss 0.15714450180530548, R2 0.08123588562011719\n",
      "Test Epoch877 layer3 out_loss 0.1569024920463562, R2 0.08265078067779541\n",
      "Test Epoch877 layer4 out_loss 0.15692807734012604, R2 0.08250123262405396\n",
      "Train 878 | out_loss 0.3939024806022644: 100%|█| 125/125 [00:00<00:00, 253.84it/\n",
      "Train Epoch878 out_loss 0.15515920519828796, R2 0.08984720706939697\n",
      "Test Epoch878 layer0 out_loss 0.15880796313285828, R2 0.0715101957321167\n",
      "Test Epoch878 layer1 out_loss 0.15777026116847992, R2 0.07757729291915894\n",
      "Test Epoch878 layer2 out_loss 0.15723171830177307, R2 0.08072590827941895\n",
      "Test Epoch878 layer3 out_loss 0.15723225474357605, R2 0.08072280883789062\n",
      "Test Epoch878 layer4 out_loss 0.15705913305282593, R2 0.08173501491546631\n",
      "Train 879 | out_loss 0.3937655985355377: 100%|█| 125/125 [00:00<00:00, 256.77it/\n",
      "Train Epoch879 out_loss 0.1550513654947281, R2 0.0904797911643982\n",
      "Test Epoch879 layer0 out_loss 0.158907949924469, R2 0.07092565298080444\n",
      "Test Epoch879 layer1 out_loss 0.15863890945911407, R2 0.07249855995178223\n",
      "Test Epoch879 layer2 out_loss 0.15814508497714996, R2 0.07538586854934692\n",
      "Test Epoch879 layer3 out_loss 0.15783239901065826, R2 0.07721400260925293\n",
      "Test Epoch879 layer4 out_loss 0.15816357731819153, R2 0.07527774572372437\n",
      "Train 880 | out_loss 0.3941379189491272: 100%|█| 125/125 [00:00<00:00, 249.84it/\n",
      "Train Epoch880 out_loss 0.15534467995166779, R2 0.08875924348831177\n",
      "Test Epoch880 layer0 out_loss 0.1588003933429718, R2 0.07155442237854004\n",
      "Test Epoch880 layer1 out_loss 0.1578548699617386, R2 0.0770825743675232\n",
      "Test Epoch880 layer2 out_loss 0.15739040076732635, R2 0.07979822158813477\n",
      "Test Epoch880 layer3 out_loss 0.15734311938285828, R2 0.08007460832595825\n",
      "Test Epoch880 layer4 out_loss 0.1574920415878296, R2 0.07920396327972412\n",
      "Train 881 | out_loss 0.3940005302429199: 100%|█| 125/125 [00:00<00:00, 255.56it/\n",
      "Train Epoch881 out_loss 0.1552364081144333, R2 0.08939439058303833\n",
      "Test Epoch881 layer0 out_loss 0.15883615612983704, R2 0.07134544849395752\n",
      "Test Epoch881 layer1 out_loss 0.15792550146579742, R2 0.07666963338851929\n",
      "Test Epoch881 layer2 out_loss 0.15732762217521667, R2 0.08016520738601685\n",
      "Test Epoch881 layer3 out_loss 0.15730908513069153, R2 0.08027362823486328\n",
      "Test Epoch881 layer4 out_loss 0.15730123221874237, R2 0.08031946420669556\n",
      "Train 882 | out_loss 0.39393460750579834: 100%|█| 125/125 [00:00<00:00, 254.28it\n",
      "Train Epoch882 out_loss 0.15518444776535034, R2 0.0896991491317749\n",
      "Test Epoch882 layer0 out_loss 0.1588064283132553, R2 0.07151919603347778\n",
      "Test Epoch882 layer1 out_loss 0.15776512026786804, R2 0.07760733366012573\n",
      "Test Epoch882 layer2 out_loss 0.15707404911518097, R2 0.08164775371551514\n",
      "Test Epoch882 layer3 out_loss 0.15685847401618958, R2 0.08290815353393555\n",
      "Test Epoch882 layer4 out_loss 0.1568938046693802, R2 0.08270162343978882\n",
      "Train 883 | out_loss 0.39392831921577454: 100%|█| 125/125 [00:00<00:00, 241.85it\n",
      "Train Epoch883 out_loss 0.15517956018447876, R2 0.08972775936126709\n",
      "Test Epoch883 layer0 out_loss 0.1588234305381775, R2 0.07141983509063721\n",
      "Test Epoch883 layer1 out_loss 0.1577424556016922, R2 0.07773983478546143\n",
      "Test Epoch883 layer2 out_loss 0.157021164894104, R2 0.08195698261260986\n",
      "Test Epoch883 layer3 out_loss 0.1567230224609375, R2 0.08370006084442139\n",
      "Test Epoch883 layer4 out_loss 0.1567109376192093, R2 0.083770751953125\n",
      "Train 884 | out_loss 0.3939698338508606: 100%|█| 125/125 [00:00<00:00, 251.08it/\n",
      "Train Epoch884 out_loss 0.1552121937274933, R2 0.08953642845153809\n",
      "Test Epoch884 layer0 out_loss 0.1588643193244934, R2 0.07118076086044312\n",
      "Test Epoch884 layer1 out_loss 0.1582643687725067, R2 0.0746883749961853\n",
      "Test Epoch884 layer2 out_loss 0.15777012705802917, R2 0.07757806777954102\n",
      "Test Epoch884 layer3 out_loss 0.15743550658226013, R2 0.07953447103500366\n",
      "Test Epoch884 layer4 out_loss 0.157406747341156, R2 0.07970255613327026\n",
      "Train 885 | out_loss 0.3940536379814148: 100%|█| 125/125 [00:00<00:00, 255.20it/\n",
      "Train Epoch885 out_loss 0.1552782654762268, R2 0.08914881944656372\n",
      "Test Epoch885 layer0 out_loss 0.15882043540477753, R2 0.0714372992515564\n",
      "Test Epoch885 layer1 out_loss 0.15774548053741455, R2 0.07772213220596313\n",
      "Test Epoch885 layer2 out_loss 0.1570231169462204, R2 0.08194553852081299\n",
      "Test Epoch885 layer3 out_loss 0.1568470150232315, R2 0.08297514915466309\n",
      "Test Epoch885 layer4 out_loss 0.15687280893325806, R2 0.08282434940338135\n",
      "Train 886 | out_loss 0.3940351605415344: 100%|█| 125/125 [00:00<00:00, 244.05it/\n",
      "Train Epoch886 out_loss 0.15526370704174042, R2 0.08923423290252686\n",
      "Test Epoch886 layer0 out_loss 0.15882596373558044, R2 0.07140499353408813\n",
      "Test Epoch886 layer1 out_loss 0.15770316123962402, R2 0.07796961069107056\n",
      "Test Epoch886 layer2 out_loss 0.15696825087070465, R2 0.08226633071899414\n",
      "Test Epoch886 layer3 out_loss 0.15677793323993683, R2 0.08337903022766113\n",
      "Test Epoch886 layer4 out_loss 0.1567898988723755, R2 0.08330905437469482\n",
      "Train 887 | out_loss 0.39400312304496765: 100%|█| 125/125 [00:00<00:00, 256.60it\n",
      "Train Epoch887 out_loss 0.15523850917816162, R2 0.08938199281692505\n",
      "Test Epoch887 layer0 out_loss 0.15881752967834473, R2 0.07145428657531738\n",
      "Test Epoch887 layer1 out_loss 0.15771909058094025, R2 0.0778765082359314\n",
      "Test Epoch887 layer2 out_loss 0.1570262312889099, R2 0.0819273591041565\n",
      "Test Epoch887 layer3 out_loss 0.1568482220172882, R2 0.08296811580657959\n",
      "Test Epoch887 layer4 out_loss 0.15682245790958405, R2 0.08311867713928223\n",
      "Train 888 | out_loss 0.39402762055397034: 100%|█| 125/125 [00:00<00:00, 254.81it\n",
      "Train Epoch888 out_loss 0.1552577167749405, R2 0.08926934003829956\n",
      "Test Epoch888 layer0 out_loss 0.15880678594112396, R2 0.07151705026626587\n",
      "Test Epoch888 layer1 out_loss 0.15771450102329254, R2 0.07790327072143555\n",
      "Test Epoch888 layer2 out_loss 0.15693216025829315, R2 0.08247733116149902\n",
      "Test Epoch888 layer3 out_loss 0.15681740641593933, R2 0.08314824104309082\n",
      "Test Epoch888 layer4 out_loss 0.1568572223186493, R2 0.08291548490524292\n",
      "Train 889 | out_loss 0.3939475417137146: 100%|█| 125/125 [00:00<00:00, 251.97it/\n",
      "Train Epoch889 out_loss 0.15519466996192932, R2 0.08963918685913086\n",
      "Test Epoch889 layer0 out_loss 0.15881657600402832, R2 0.07145988941192627\n",
      "Test Epoch889 layer1 out_loss 0.15774844586849213, R2 0.07770484685897827\n",
      "Test Epoch889 layer2 out_loss 0.15691648423671722, R2 0.0825689435005188\n",
      "Test Epoch889 layer3 out_loss 0.15666240453720093, R2 0.08405452966690063\n",
      "Test Epoch889 layer4 out_loss 0.1566624790430069, R2 0.08405405282974243\n",
      "Train 890 | out_loss 0.3938818871974945: 100%|█| 125/125 [00:00<00:00, 256.01it/\n",
      "Train Epoch890 out_loss 0.1551429182291031, R2 0.08994275331497192\n",
      "Test Epoch890 layer0 out_loss 0.15884587168693542, R2 0.0712885856628418\n",
      "Test Epoch890 layer1 out_loss 0.15780234336853027, R2 0.07738971710205078\n",
      "Test Epoch890 layer2 out_loss 0.15712177753448486, R2 0.08136868476867676\n",
      "Test Epoch890 layer3 out_loss 0.15699970722198486, R2 0.08208239078521729\n",
      "Test Epoch890 layer4 out_loss 0.15703558921813965, R2 0.08187264204025269\n",
      "Train 891 | out_loss 0.3939215838909149: 100%|█| 125/125 [00:00<00:00, 252.30it/\n",
      "Train Epoch891 out_loss 0.15517428517341614, R2 0.08975875377655029\n",
      "Test Epoch891 layer0 out_loss 0.1588963121175766, R2 0.07099366188049316\n",
      "Test Epoch891 layer1 out_loss 0.15768571197986603, R2 0.07807159423828125\n",
      "Test Epoch891 layer2 out_loss 0.1569107323884964, R2 0.0826026201248169\n",
      "Test Epoch891 layer3 out_loss 0.15677380561828613, R2 0.08340317010879517\n",
      "Test Epoch891 layer4 out_loss 0.15679261088371277, R2 0.08329319953918457\n",
      "Train 892 | out_loss 0.3939712345600128: 100%|█| 125/125 [00:00<00:00, 256.51it/\n",
      "Train Epoch892 out_loss 0.15521331131458282, R2 0.08952981233596802\n",
      "Test Epoch892 layer0 out_loss 0.158847376704216, R2 0.07127976417541504\n",
      "Test Epoch892 layer1 out_loss 0.15796518325805664, R2 0.07643765211105347\n",
      "Test Epoch892 layer2 out_loss 0.15699051320552826, R2 0.08213621377944946\n",
      "Test Epoch892 layer3 out_loss 0.1567727029323578, R2 0.08340960741043091\n",
      "Test Epoch892 layer4 out_loss 0.15668116509914398, R2 0.08394479751586914\n",
      "Train 893 | out_loss 0.39381518959999084: 100%|█| 125/125 [00:00<00:00, 246.70it\n",
      "Train Epoch893 out_loss 0.15509040653705597, R2 0.09025079011917114\n",
      "Test Epoch893 layer0 out_loss 0.1588236540555954, R2 0.07141846418380737\n",
      "Test Epoch893 layer1 out_loss 0.1577976644039154, R2 0.07741701602935791\n",
      "Test Epoch893 layer2 out_loss 0.1571393460035324, R2 0.08126604557037354\n",
      "Test Epoch893 layer3 out_loss 0.1568068414926529, R2 0.0832100510597229\n",
      "Test Epoch893 layer4 out_loss 0.15672941505908966, R2 0.08366268873214722\n",
      "Train 894 | out_loss 0.39410707354545593: 100%|█| 125/125 [00:00<00:00, 263.11it\n",
      "Train Epoch894 out_loss 0.15532039105892181, R2 0.08890169858932495\n",
      "Test Epoch894 layer0 out_loss 0.15890608727931976, R2 0.07093656063079834\n",
      "Test Epoch894 layer1 out_loss 0.15771900117397308, R2 0.07787704467773438\n",
      "Test Epoch894 layer2 out_loss 0.15693870186805725, R2 0.08243906497955322\n",
      "Test Epoch894 layer3 out_loss 0.15673598647117615, R2 0.08362424373626709\n",
      "Test Epoch894 layer4 out_loss 0.15669353306293488, R2 0.08387255668640137\n",
      "Train 895 | out_loss 0.39399102330207825: 100%|█| 125/125 [00:00<00:00, 239.87it\n",
      "Train Epoch895 out_loss 0.15522894263267517, R2 0.08943814039230347\n",
      "Test Epoch895 layer0 out_loss 0.15880310535430908, R2 0.07153868675231934\n",
      "Test Epoch895 layer1 out_loss 0.15768380463123322, R2 0.07808274030685425\n",
      "Test Epoch895 layer2 out_loss 0.15687668323516846, R2 0.0828016996383667\n",
      "Test Epoch895 layer3 out_loss 0.15668702125549316, R2 0.08391052484512329\n",
      "Test Epoch895 layer4 out_loss 0.15668083727359772, R2 0.08394676446914673\n",
      "Train 896 | out_loss 0.39398637413978577: 100%|█| 125/125 [00:00<00:00, 255.83it\n",
      "Train Epoch896 out_loss 0.1552252173423767, R2 0.08945995569229126\n",
      "Test Epoch896 layer0 out_loss 0.1588197499513626, R2 0.07144135236740112\n",
      "Test Epoch896 layer1 out_loss 0.1579887419939041, R2 0.07629990577697754\n",
      "Test Epoch896 layer2 out_loss 0.15754009783267975, R2 0.07892292737960815\n",
      "Test Epoch896 layer3 out_loss 0.15700049698352814, R2 0.0820777416229248\n",
      "Test Epoch896 layer4 out_loss 0.1569157838821411, R2 0.0825730562210083\n",
      "Train 897 | out_loss 0.39381876587867737: 100%|█| 125/125 [00:00<00:00, 244.43it\n",
      "Train Epoch897 out_loss 0.1550932377576828, R2 0.09023422002792358\n",
      "Test Epoch897 layer0 out_loss 0.15881481766700745, R2 0.07147014141082764\n",
      "Test Epoch897 layer1 out_loss 0.1578233689069748, R2 0.07726681232452393\n",
      "Test Epoch897 layer2 out_loss 0.15722782909870148, R2 0.08074867725372314\n",
      "Test Epoch897 layer3 out_loss 0.15696905553340912, R2 0.08226162195205688\n",
      "Test Epoch897 layer4 out_loss 0.15677261352539062, R2 0.08341014385223389\n",
      "Train 898 | out_loss 0.39369162917137146: 100%|█| 125/125 [00:00<00:00, 239.15it\n",
      "Train Epoch898 out_loss 0.15499308705329895, R2 0.09082162380218506\n",
      "Test Epoch898 layer0 out_loss 0.1588578224182129, R2 0.07121872901916504\n",
      "Test Epoch898 layer1 out_loss 0.15780174732208252, R2 0.07739323377609253\n",
      "Test Epoch898 layer2 out_loss 0.15710985660552979, R2 0.08143836259841919\n",
      "Test Epoch898 layer3 out_loss 0.15703897178173065, R2 0.08185285329818726\n",
      "Test Epoch898 layer4 out_loss 0.1568976193666458, R2 0.08267933130264282\n",
      "Train 899 | out_loss 0.393655002117157: 100%|█| 125/125 [00:00<00:00, 250.81it/s\n",
      "Train Epoch899 out_loss 0.15496426820755005, R2 0.09099072217941284\n",
      "Test Epoch899 layer0 out_loss 0.1592075526714325, R2 0.06917399168014526\n",
      "Test Epoch899 layer1 out_loss 0.1582455188035965, R2 0.07479864358901978\n",
      "Test Epoch899 layer2 out_loss 0.15735365450382233, R2 0.08001303672790527\n",
      "Test Epoch899 layer3 out_loss 0.15742120146751404, R2 0.07961809635162354\n",
      "Test Epoch899 layer4 out_loss 0.15759184956550598, R2 0.07862043380737305\n",
      "Train 900 | out_loss 0.39390140771865845: 100%|█| 125/125 [00:00<00:00, 253.23it\n",
      "Train Epoch900 out_loss 0.15515834093093872, R2 0.08985227346420288\n",
      "Test Epoch900 layer0 out_loss 0.15881338715553284, R2 0.07147854566574097\n",
      "Test Epoch900 layer1 out_loss 0.15779536962509155, R2 0.07743048667907715\n",
      "Test Epoch900 layer2 out_loss 0.15747439861297607, R2 0.07930707931518555\n",
      "Test Epoch900 layer3 out_loss 0.15730294585227966, R2 0.08030951023101807\n",
      "Test Epoch900 layer4 out_loss 0.15718115866184235, R2 0.08102154731750488\n",
      "Train 901 | out_loss 0.3939080238342285: 100%|█| 125/125 [00:00<00:00, 248.05it/\n",
      "Train Epoch901 out_loss 0.15516354143619537, R2 0.08982175588607788\n",
      "Test Epoch901 layer0 out_loss 0.1588062345981598, R2 0.07152038812637329\n",
      "Test Epoch901 layer1 out_loss 0.15774117410182953, R2 0.07774728536605835\n",
      "Test Epoch901 layer2 out_loss 0.15688678622245789, R2 0.08274263143539429\n",
      "Test Epoch901 layer3 out_loss 0.15661126375198364, R2 0.08435344696044922\n",
      "Test Epoch901 layer4 out_loss 0.15662357211112976, R2 0.0842815637588501\n",
      "Train 902 | out_loss 0.3939736485481262: 100%|█| 125/125 [00:00<00:00, 239.38it/\n",
      "Train Epoch902 out_loss 0.15521524846553802, R2 0.08951842784881592\n",
      "Test Epoch902 layer0 out_loss 0.1587984561920166, R2 0.07156580686569214\n",
      "Test Epoch902 layer1 out_loss 0.15766975283622742, R2 0.07816493511199951\n",
      "Test Epoch902 layer2 out_loss 0.1569286286830902, R2 0.08249795436859131\n",
      "Test Epoch902 layer3 out_loss 0.1569417119026184, R2 0.08242148160934448\n",
      "Test Epoch902 layer4 out_loss 0.15687313675880432, R2 0.08282244205474854\n",
      "Train 903 | out_loss 0.3938261568546295: 100%|█| 125/125 [00:00<00:00, 255.40it/\n",
      "Train Epoch903 out_loss 0.155099019408226, R2 0.09020030498504639\n",
      "Test Epoch903 layer0 out_loss 0.15888294577598572, R2 0.0710718035697937\n",
      "Test Epoch903 layer1 out_loss 0.15772373974323273, R2 0.07784926891326904\n",
      "Test Epoch903 layer2 out_loss 0.15690185129642487, R2 0.08265453577041626\n",
      "Test Epoch903 layer3 out_loss 0.15674009919166565, R2 0.08360028266906738\n",
      "Test Epoch903 layer4 out_loss 0.15676777064800262, R2 0.08343839645385742\n",
      "Train 904 | out_loss 0.3939110338687897: 100%|█| 125/125 [00:00<00:00, 250.32it/\n",
      "Train Epoch904 out_loss 0.15516595542430878, R2 0.08980762958526611\n",
      "Test Epoch904 layer0 out_loss 0.15882229804992676, R2 0.07142645120620728\n",
      "Test Epoch904 layer1 out_loss 0.15776394307613373, R2 0.0776141881942749\n",
      "Test Epoch904 layer2 out_loss 0.1571127027273178, R2 0.08142179250717163\n",
      "Test Epoch904 layer3 out_loss 0.15678100287914276, R2 0.08336108922958374\n",
      "Test Epoch904 layer4 out_loss 0.15671804547309875, R2 0.08372914791107178\n",
      "Train 905 | out_loss 0.393838107585907: 100%|█| 125/125 [00:00<00:00, 253.88it/s\n",
      "Train Epoch905 out_loss 0.1551084816455841, R2 0.0901448130607605\n",
      "Test Epoch905 layer0 out_loss 0.15890835225582123, R2 0.07092326879501343\n",
      "Test Epoch905 layer1 out_loss 0.15816731750965118, R2 0.0752558708190918\n",
      "Test Epoch905 layer2 out_loss 0.15724506974220276, R2 0.08064788579940796\n",
      "Test Epoch905 layer3 out_loss 0.15706509351730347, R2 0.0817001461982727\n",
      "Test Epoch905 layer4 out_loss 0.15720559656620026, R2 0.08087867498397827\n",
      "Train 906 | out_loss 0.3940286636352539: 100%|█| 125/125 [00:00<00:00, 255.63it/\n",
      "Train Epoch906 out_loss 0.15525858104228973, R2 0.08926433324813843\n",
      "Test Epoch906 layer0 out_loss 0.15880152583122253, R2 0.07154786586761475\n",
      "Test Epoch906 layer1 out_loss 0.1576738953590393, R2 0.0781407356262207\n",
      "Test Epoch906 layer2 out_loss 0.15691374242305756, R2 0.08258503675460815\n",
      "Test Epoch906 layer3 out_loss 0.15679268538951874, R2 0.08329284191131592\n",
      "Test Epoch906 layer4 out_loss 0.15675529837608337, R2 0.08351141214370728\n",
      "Train 907 | out_loss 0.3938887119293213: 100%|█| 125/125 [00:00<00:00, 255.75it/\n",
      "Train Epoch907 out_loss 0.15514826774597168, R2 0.08991134166717529\n",
      "Test Epoch907 layer0 out_loss 0.15881337225437164, R2 0.07147860527038574\n",
      "Test Epoch907 layer1 out_loss 0.1577233523130417, R2 0.07785147428512573\n",
      "Test Epoch907 layer2 out_loss 0.1570732146501541, R2 0.08165264129638672\n",
      "Test Epoch907 layer3 out_loss 0.1567670851945877, R2 0.08344244956970215\n",
      "Test Epoch907 layer4 out_loss 0.1567825973033905, R2 0.08335179090499878\n",
      "Train 908 | out_loss 0.3939540386199951: 100%|█| 125/125 [00:00<00:00, 261.97it/\n",
      "Train Epoch908 out_loss 0.15519976615905762, R2 0.08960926532745361\n",
      "Test Epoch908 layer0 out_loss 0.1588907092809677, R2 0.07102638483047485\n",
      "Test Epoch908 layer1 out_loss 0.15795879065990448, R2 0.07647502422332764\n",
      "Test Epoch908 layer2 out_loss 0.15692049264907837, R2 0.08254557847976685\n",
      "Test Epoch908 layer3 out_loss 0.15665945410728455, R2 0.08407175540924072\n",
      "Test Epoch908 layer4 out_loss 0.15668557584285736, R2 0.0839189887046814\n",
      "Train 909 | out_loss 0.39384332299232483: 100%|█| 125/125 [00:00<00:00, 256.54it\n",
      "Train Epoch909 out_loss 0.15511256456375122, R2 0.09012079238891602\n",
      "Test Epoch909 layer0 out_loss 0.15881088376045227, R2 0.07149314880371094\n",
      "Test Epoch909 layer1 out_loss 0.1578902006149292, R2 0.07687604427337646\n",
      "Test Epoch909 layer2 out_loss 0.1570269614458084, R2 0.08192306756973267\n",
      "Test Epoch909 layer3 out_loss 0.1569736897945404, R2 0.08223450183868408\n",
      "Test Epoch909 layer4 out_loss 0.1570318192243576, R2 0.08189469575881958\n",
      "Train 910 | out_loss 0.39385929703712463: 100%|█| 125/125 [00:00<00:00, 261.87it\n",
      "Train Epoch910 out_loss 0.15512512624263763, R2 0.09004712104797363\n",
      "Test Epoch910 layer0 out_loss 0.1588021218776703, R2 0.071544349193573\n",
      "Test Epoch910 layer1 out_loss 0.15768824517726898, R2 0.07805675268173218\n",
      "Test Epoch910 layer2 out_loss 0.15696333348751068, R2 0.08229506015777588\n",
      "Test Epoch910 layer3 out_loss 0.15667787194252014, R2 0.08396404981613159\n",
      "Test Epoch910 layer4 out_loss 0.1566942036151886, R2 0.08386862277984619\n",
      "Train 911 | out_loss 0.3938566744327545: 100%|█| 125/125 [00:00<00:00, 259.34it/\n",
      "Train Epoch911 out_loss 0.1551230549812317, R2 0.09005922079086304\n",
      "Test Epoch911 layer0 out_loss 0.1588042974472046, R2 0.07153171300888062\n",
      "Test Epoch911 layer1 out_loss 0.15766173601150513, R2 0.0782117247581482\n",
      "Test Epoch911 layer2 out_loss 0.15686510503292084, R2 0.08286935091018677\n",
      "Test Epoch911 layer3 out_loss 0.15662425756454468, R2 0.08427757024765015\n",
      "Test Epoch911 layer4 out_loss 0.15666598081588745, R2 0.0840335488319397\n",
      "Train 912 | out_loss 0.3937322497367859: 100%|█| 125/125 [00:00<00:00, 253.07it/\n",
      "Train Epoch912 out_loss 0.15502505004405975, R2 0.09063410758972168\n",
      "Test Epoch912 layer0 out_loss 0.15887993574142456, R2 0.07108944654464722\n",
      "Test Epoch912 layer1 out_loss 0.15777577459812164, R2 0.07754504680633545\n",
      "Test Epoch912 layer2 out_loss 0.1571853905916214, R2 0.08099675178527832\n",
      "Test Epoch912 layer3 out_loss 0.1568811982870102, R2 0.0827752947807312\n",
      "Test Epoch912 layer4 out_loss 0.15684492886066437, R2 0.08298736810684204\n",
      "Train 913 | out_loss 0.39383625984191895: 100%|█| 125/125 [00:00<00:00, 252.60it\n",
      "Train Epoch913 out_loss 0.15510699152946472, R2 0.09015345573425293\n",
      "Test Epoch913 layer0 out_loss 0.1588069200515747, R2 0.07151633501052856\n",
      "Test Epoch913 layer1 out_loss 0.1576727330684662, R2 0.0781475305557251\n",
      "Test Epoch913 layer2 out_loss 0.1568433791399002, R2 0.08299636840820312\n",
      "Test Epoch913 layer3 out_loss 0.15665219724178314, R2 0.0841141939163208\n",
      "Test Epoch913 layer4 out_loss 0.15667840838432312, R2 0.08396095037460327\n",
      "Train 914 | out_loss 0.39378300309181213: 100%|█| 125/125 [00:00<00:00, 248.78it\n",
      "Train Epoch914 out_loss 0.15506502985954285, R2 0.09039962291717529\n",
      "Test Epoch914 layer0 out_loss 0.1588720977306366, R2 0.07113522291183472\n",
      "Test Epoch914 layer1 out_loss 0.15787680447101593, R2 0.07695436477661133\n",
      "Test Epoch914 layer2 out_loss 0.15694700181484222, R2 0.08239054679870605\n",
      "Test Epoch914 layer3 out_loss 0.1568012833595276, R2 0.08324253559112549\n",
      "Test Epoch914 layer4 out_loss 0.15684475004673004, R2 0.08298832178115845\n",
      "Train 915 | out_loss 0.39389291405677795: 100%|█| 125/125 [00:00<00:00, 249.05it\n",
      "Train Epoch915 out_loss 0.1551515907049179, R2 0.08989185094833374\n",
      "Test Epoch915 layer0 out_loss 0.1587945520877838, R2 0.07158863544464111\n",
      "Test Epoch915 layer1 out_loss 0.15764819085597992, R2 0.07829093933105469\n",
      "Test Epoch915 layer2 out_loss 0.15684561431407928, R2 0.08298337459564209\n",
      "Test Epoch915 layer3 out_loss 0.15657955408096313, R2 0.08453887701034546\n",
      "Test Epoch915 layer4 out_loss 0.15660706162452698, R2 0.08437812328338623\n",
      "Train 916 | out_loss 0.39394456148147583: 100%|█| 125/125 [00:00<00:00, 242.88it\n",
      "Train Epoch916 out_loss 0.1551923006772995, R2 0.08965301513671875\n",
      "Test Epoch916 layer0 out_loss 0.1588200479745865, R2 0.07143956422805786\n",
      "Test Epoch916 layer1 out_loss 0.1578826755285263, R2 0.0769200325012207\n",
      "Test Epoch916 layer2 out_loss 0.15696604549884796, R2 0.08227920532226562\n",
      "Test Epoch916 layer3 out_loss 0.1567322015762329, R2 0.08364635705947876\n",
      "Test Epoch916 layer4 out_loss 0.15679125487804413, R2 0.0833011269569397\n",
      "Train 917 | out_loss 0.39376577734947205: 100%|█| 125/125 [00:00<00:00, 248.44it\n",
      "Train Epoch917 out_loss 0.15505149960517883, R2 0.09047901630401611\n",
      "Test Epoch917 layer0 out_loss 0.15892575681209564, R2 0.07082158327102661\n",
      "Test Epoch917 layer1 out_loss 0.15790042281150818, R2 0.07681626081466675\n",
      "Test Epoch917 layer2 out_loss 0.156999409198761, R2 0.08208417892456055\n",
      "Test Epoch917 layer3 out_loss 0.1566990166902542, R2 0.08384042978286743\n",
      "Test Epoch917 layer4 out_loss 0.15669192373752594, R2 0.0838819146156311\n",
      "Train 918 | out_loss 0.39387252926826477: 100%|█| 125/125 [00:00<00:00, 249.16it\n",
      "Train Epoch918 out_loss 0.15513552725315094, R2 0.08998608589172363\n",
      "Test Epoch918 layer0 out_loss 0.1588013917207718, R2 0.07154864072799683\n",
      "Test Epoch918 layer1 out_loss 0.15766148269176483, R2 0.07821327447891235\n",
      "Test Epoch918 layer2 out_loss 0.1568908393383026, R2 0.08271890878677368\n",
      "Test Epoch918 layer3 out_loss 0.15665875375270844, R2 0.08407586812973022\n",
      "Test Epoch918 layer4 out_loss 0.15669231116771698, R2 0.08387964963912964\n",
      "Train 919 | out_loss 0.39370736479759216: 100%|█| 125/125 [00:00<00:00, 254.74it\n",
      "Train Epoch919 out_loss 0.15500551462173462, R2 0.09074878692626953\n",
      "Test Epoch919 layer0 out_loss 0.1587848961353302, R2 0.07164508104324341\n",
      "Test Epoch919 layer1 out_loss 0.15773074328899384, R2 0.07780832052230835\n",
      "Test Epoch919 layer2 out_loss 0.15691503882408142, R2 0.08257746696472168\n",
      "Test Epoch919 layer3 out_loss 0.15674015879631042, R2 0.08359992504119873\n",
      "Test Epoch919 layer4 out_loss 0.15680420398712158, R2 0.08322548866271973\n",
      "Train 920 | out_loss 0.3938648998737335: 100%|█| 125/125 [00:00<00:00, 244.73it/\n",
      "Train Epoch920 out_loss 0.1551295816898346, R2 0.09002101421356201\n",
      "Test Epoch920 layer0 out_loss 0.15879738330841064, R2 0.07157212495803833\n",
      "Test Epoch920 layer1 out_loss 0.15768088400363922, R2 0.07809984683990479\n",
      "Test Epoch920 layer2 out_loss 0.15697748959064484, R2 0.08221226930618286\n",
      "Test Epoch920 layer3 out_loss 0.1567603051662445, R2 0.08348214626312256\n",
      "Test Epoch920 layer4 out_loss 0.15681259334087372, R2 0.08317643404006958\n",
      "Train 921 | out_loss 0.39379948377609253: 100%|█| 125/125 [00:00<00:00, 245.00it\n",
      "Train Epoch921 out_loss 0.15507802367210388, R2 0.09032344818115234\n",
      "Test Epoch921 layer0 out_loss 0.1587853580713272, R2 0.07164239883422852\n",
      "Test Epoch921 layer1 out_loss 0.15771886706352234, R2 0.07787775993347168\n",
      "Test Epoch921 layer2 out_loss 0.1573069989681244, R2 0.08028584718704224\n",
      "Test Epoch921 layer3 out_loss 0.15735405683517456, R2 0.08001071214675903\n",
      "Test Epoch921 layer4 out_loss 0.1575758010149002, R2 0.07871419191360474\n",
      "Train 922 | out_loss 0.39397120475769043: 100%|█| 125/125 [00:00<00:00, 226.12it\n",
      "Train Epoch922 out_loss 0.15521329641342163, R2 0.08952987194061279\n",
      "Test Epoch922 layer0 out_loss 0.15880611538887024, R2 0.07152098417282104\n",
      "Test Epoch922 layer1 out_loss 0.1576804220676422, R2 0.07810252904891968\n",
      "Test Epoch922 layer2 out_loss 0.1569252759218216, R2 0.08251756429672241\n",
      "Test Epoch922 layer3 out_loss 0.15669284760951996, R2 0.08387655019760132\n",
      "Test Epoch922 layer4 out_loss 0.15672487020492554, R2 0.08368927240371704\n",
      "Train 923 | out_loss 0.3937419056892395: 100%|█| 125/125 [00:00<00:00, 245.83it/\n",
      "Train Epoch923 out_loss 0.155032679438591, R2 0.09058934450149536\n",
      "Test Epoch923 layer0 out_loss 0.1588374823331833, R2 0.07133764028549194\n",
      "Test Epoch923 layer1 out_loss 0.1578771024942398, R2 0.07695257663726807\n",
      "Test Epoch923 layer2 out_loss 0.15761905908584595, R2 0.07846128940582275\n",
      "Test Epoch923 layer3 out_loss 0.15720921754837036, R2 0.08085751533508301\n",
      "Test Epoch923 layer4 out_loss 0.15713420510292053, R2 0.08129608631134033\n",
      "Train 924 | out_loss 0.39370104670524597: 100%|█| 125/125 [00:00<00:00, 246.29it\n",
      "Train Epoch924 out_loss 0.15500056743621826, R2 0.09077775478363037\n",
      "Test Epoch924 layer0 out_loss 0.1588597148656845, R2 0.07120770215988159\n",
      "Test Epoch924 layer1 out_loss 0.1577051430940628, R2 0.07795804738998413\n",
      "Test Epoch924 layer2 out_loss 0.15689700841903687, R2 0.08268290758132935\n",
      "Test Epoch924 layer3 out_loss 0.15662185847759247, R2 0.08429157733917236\n",
      "Test Epoch924 layer4 out_loss 0.15664707124233246, R2 0.08414417505264282\n",
      "Train 925 | out_loss 0.3938852846622467: 100%|█| 125/125 [00:00<00:00, 258.16it/\n",
      "Train Epoch925 out_loss 0.15514561533927917, R2 0.08992695808410645\n",
      "Test Epoch925 layer0 out_loss 0.1587953269481659, R2 0.07158404588699341\n",
      "Test Epoch925 layer1 out_loss 0.1576719880104065, R2 0.0781518816947937\n",
      "Test Epoch925 layer2 out_loss 0.15686406195163727, R2 0.08287549018859863\n",
      "Test Epoch925 layer3 out_loss 0.15661779046058655, R2 0.08431529998779297\n",
      "Test Epoch925 layer4 out_loss 0.15663430094718933, R2 0.08421874046325684\n",
      "Train 926 | out_loss 0.39357978105545044: 100%|█| 125/125 [00:00<00:00, 257.35it\n",
      "Train Epoch926 out_loss 0.15490509569644928, R2 0.09133780002593994\n",
      "Test Epoch926 layer0 out_loss 0.15880721807479858, R2 0.0715145468711853\n",
      "Test Epoch926 layer1 out_loss 0.15810230374336243, R2 0.07563591003417969\n",
      "Test Epoch926 layer2 out_loss 0.15727795660495758, R2 0.08045560121536255\n",
      "Test Epoch926 layer3 out_loss 0.1574631929397583, R2 0.0793725848197937\n",
      "Test Epoch926 layer4 out_loss 0.15758252143859863, R2 0.07867491245269775\n",
      "Train 927 | out_loss 0.3939723074436188: 100%|█| 125/125 [00:00<00:00, 243.67it/\n",
      "Train Epoch927 out_loss 0.15521419048309326, R2 0.08952468633651733\n",
      "Test Epoch927 layer0 out_loss 0.15887539088726044, R2 0.07111597061157227\n",
      "Test Epoch927 layer1 out_loss 0.15811581909656525, R2 0.0755569338798523\n",
      "Test Epoch927 layer2 out_loss 0.15779617428779602, R2 0.07742577791213989\n",
      "Test Epoch927 layer3 out_loss 0.15777488052845, R2 0.07755023241043091\n",
      "Test Epoch927 layer4 out_loss 0.15766704082489014, R2 0.07818078994750977\n",
      "Train 928 | out_loss 0.39389166235923767: 100%|█| 125/125 [00:00<00:00, 257.87it\n",
      "Train Epoch928 out_loss 0.1551506519317627, R2 0.08989739418029785\n",
      "Test Epoch928 layer0 out_loss 0.15879061818122864, R2 0.07161164283752441\n",
      "Test Epoch928 layer1 out_loss 0.15762676298618317, R2 0.07841622829437256\n",
      "Test Epoch928 layer2 out_loss 0.1569126695394516, R2 0.0825912356376648\n",
      "Test Epoch928 layer3 out_loss 0.15663394331932068, R2 0.08422088623046875\n",
      "Test Epoch928 layer4 out_loss 0.15667064487934113, R2 0.08400636911392212\n",
      "Train 929 | out_loss 0.39380812644958496: 100%|█| 125/125 [00:00<00:00, 257.75it\n",
      "Train Epoch929 out_loss 0.15508484840393066, R2 0.0902833342552185\n",
      "Test Epoch929 layer0 out_loss 0.15883421897888184, R2 0.07135677337646484\n",
      "Test Epoch929 layer1 out_loss 0.15786556899547577, R2 0.07702004909515381\n",
      "Test Epoch929 layer2 out_loss 0.1569761037826538, R2 0.08222043514251709\n",
      "Test Epoch929 layer3 out_loss 0.1567375808954239, R2 0.0836150050163269\n",
      "Test Epoch929 layer4 out_loss 0.1567518711090088, R2 0.08353137969970703\n",
      "Train 930 | out_loss 0.39383941888809204: 100%|█| 125/125 [00:00<00:00, 262.81it\n",
      "Train Epoch930 out_loss 0.1551094353199005, R2 0.09013915061950684\n",
      "Test Epoch930 layer0 out_loss 0.15879638493061066, R2 0.07157790660858154\n",
      "Test Epoch930 layer1 out_loss 0.1577899008989334, R2 0.07746243476867676\n",
      "Test Epoch930 layer2 out_loss 0.1573239266872406, R2 0.08018684387207031\n",
      "Test Epoch930 layer3 out_loss 0.1570511758327484, R2 0.08178150653839111\n",
      "Test Epoch930 layer4 out_loss 0.15707004070281982, R2 0.08167123794555664\n",
      "Train 931 | out_loss 0.39393505454063416: 100%|█| 125/125 [00:00<00:00, 259.02it\n",
      "Train Epoch931 out_loss 0.15518485009670258, R2 0.08969682455062866\n",
      "Test Epoch931 layer0 out_loss 0.15881066024303436, R2 0.07149451971054077\n",
      "Test Epoch931 layer1 out_loss 0.15763241052627563, R2 0.07838320732116699\n",
      "Test Epoch931 layer2 out_loss 0.15688210725784302, R2 0.08276993036270142\n",
      "Test Epoch931 layer3 out_loss 0.1566966474056244, R2 0.0838543176651001\n",
      "Test Epoch931 layer4 out_loss 0.15671338140964508, R2 0.0837564468383789\n",
      "Train 932 | out_loss 0.3938016891479492: 100%|█| 125/125 [00:00<00:00, 261.08it/\n",
      "Train Epoch932 out_loss 0.15507973730564117, R2 0.09031331539154053\n",
      "Test Epoch932 layer0 out_loss 0.15879298746585846, R2 0.07159781455993652\n",
      "Test Epoch932 layer1 out_loss 0.15782983601093292, R2 0.07722896337509155\n",
      "Test Epoch932 layer2 out_loss 0.1568448841571808, R2 0.08298760652542114\n",
      "Test Epoch932 layer3 out_loss 0.15655691921710968, R2 0.0846712589263916\n",
      "Test Epoch932 layer4 out_loss 0.15655021369457245, R2 0.08471041917800903\n",
      "Train 933 | out_loss 0.39376282691955566: 100%|█| 125/125 [00:00<00:00, 251.76it\n",
      "Train Epoch933 out_loss 0.1550491750240326, R2 0.0904926061630249\n",
      "Test Epoch933 layer0 out_loss 0.15888483822345734, R2 0.07106077671051025\n",
      "Test Epoch933 layer1 out_loss 0.15783721208572388, R2 0.07718580961227417\n",
      "Test Epoch933 layer2 out_loss 0.15705086290836334, R2 0.08178335428237915\n",
      "Test Epoch933 layer3 out_loss 0.15675006806850433, R2 0.08354198932647705\n",
      "Test Epoch933 layer4 out_loss 0.1568247228860855, R2 0.08310550451278687\n",
      "Train 934 | out_loss 0.3936421871185303: 100%|█| 125/125 [00:00<00:00, 257.70it/\n",
      "Train Epoch934 out_loss 0.15495416522026062, R2 0.09104996919631958\n",
      "Test Epoch934 layer0 out_loss 0.1587926596403122, R2 0.07159966230392456\n",
      "Test Epoch934 layer1 out_loss 0.15767936408519745, R2 0.07810872793197632\n",
      "Test Epoch934 layer2 out_loss 0.1570216715335846, R2 0.0819540023803711\n",
      "Test Epoch934 layer3 out_loss 0.1567615419626236, R2 0.08347493410110474\n",
      "Test Epoch934 layer4 out_loss 0.15690283477306366, R2 0.08264881372451782\n",
      "Train 935 | out_loss 0.39392414689064026: 100%|█| 125/125 [00:00<00:00, 253.35it\n",
      "Train Epoch935 out_loss 0.15517619252204895, R2 0.08974754810333252\n",
      "Test Epoch935 layer0 out_loss 0.1587846279144287, R2 0.07164669036865234\n",
      "Test Epoch935 layer1 out_loss 0.1577288955450058, R2 0.0778191089630127\n",
      "Test Epoch935 layer2 out_loss 0.15691693127155304, R2 0.08256638050079346\n",
      "Test Epoch935 layer3 out_loss 0.15669967234134674, R2 0.08383655548095703\n",
      "Test Epoch935 layer4 out_loss 0.15674665570259094, R2 0.08356189727783203\n",
      "Train 936 | out_loss 0.39354947209358215: 100%|█| 125/125 [00:00<00:00, 254.47it\n",
      "Train Epoch936 out_loss 0.15488113462924957, R2 0.09147834777832031\n",
      "Test Epoch936 layer0 out_loss 0.15893718600273132, R2 0.0707547664642334\n",
      "Test Epoch936 layer1 out_loss 0.15772703289985657, R2 0.07783001661300659\n",
      "Test Epoch936 layer2 out_loss 0.15703009068965912, R2 0.08190476894378662\n",
      "Test Epoch936 layer3 out_loss 0.15676796436309814, R2 0.08343738317489624\n",
      "Test Epoch936 layer4 out_loss 0.1568538397550583, R2 0.08293527364730835\n",
      "Train 937 | out_loss 0.39378294348716736: 100%|█| 125/125 [00:00<00:00, 254.54it\n",
      "Train Epoch937 out_loss 0.15506504476070404, R2 0.09039950370788574\n",
      "Test Epoch937 layer0 out_loss 0.15882450342178345, R2 0.07141351699829102\n",
      "Test Epoch937 layer1 out_loss 0.15768347680568695, R2 0.07808470726013184\n",
      "Test Epoch937 layer2 out_loss 0.15688180923461914, R2 0.08277171850204468\n",
      "Test Epoch937 layer3 out_loss 0.15676817297935486, R2 0.08343613147735596\n",
      "Test Epoch937 layer4 out_loss 0.15679152309894562, R2 0.08329963684082031\n",
      "Train 938 | out_loss 0.3937743306159973: 100%|█| 125/125 [00:00<00:00, 249.17it/\n",
      "Train Epoch938 out_loss 0.15505823493003845, R2 0.0904395580291748\n",
      "Test Epoch938 layer0 out_loss 0.15879707038402557, R2 0.07157391309738159\n",
      "Test Epoch938 layer1 out_loss 0.1576198786497116, R2 0.07845652103424072\n",
      "Test Epoch938 layer2 out_loss 0.15687517821788788, R2 0.08281046152114868\n",
      "Test Epoch938 layer3 out_loss 0.15678495168685913, R2 0.08333796262741089\n",
      "Test Epoch938 layer4 out_loss 0.15676669776439667, R2 0.08344477415084839\n",
      "Train 939 | out_loss 0.39383506774902344: 100%|█| 125/125 [00:00<00:00, 243.59it\n",
      "Train Epoch939 out_loss 0.1551060527563095, R2 0.09015899896621704\n",
      "Test Epoch939 layer0 out_loss 0.15886354446411133, R2 0.07118529081344604\n",
      "Test Epoch939 layer1 out_loss 0.15777425467967987, R2 0.07755386829376221\n",
      "Test Epoch939 layer2 out_loss 0.15681657195091248, R2 0.0831531286239624\n",
      "Test Epoch939 layer3 out_loss 0.15663380920886993, R2 0.0842217206954956\n",
      "Test Epoch939 layer4 out_loss 0.15670880675315857, R2 0.08378320932388306\n",
      "Train 940 | out_loss 0.3938179612159729: 100%|█| 125/125 [00:00<00:00, 240.23it/\n",
      "Train Epoch940 out_loss 0.15509265661239624, R2 0.09023761749267578\n",
      "Test Epoch940 layer0 out_loss 0.15878871083259583, R2 0.07162278890609741\n",
      "Test Epoch940 layer1 out_loss 0.15760287642478943, R2 0.07855594158172607\n",
      "Test Epoch940 layer2 out_loss 0.15687799453735352, R2 0.08279407024383545\n",
      "Test Epoch940 layer3 out_loss 0.15665346384048462, R2 0.08410680294036865\n",
      "Test Epoch940 layer4 out_loss 0.15668819844722748, R2 0.08390367031097412\n",
      "Train 941 | out_loss 0.3938812017440796: 100%|█| 125/125 [00:00<00:00, 254.18it/\n",
      "Train Epoch941 out_loss 0.1551423966884613, R2 0.08994579315185547\n",
      "Test Epoch941 layer0 out_loss 0.15880319476127625, R2 0.07153815031051636\n",
      "Test Epoch941 layer1 out_loss 0.15763044357299805, R2 0.07839471101760864\n",
      "Test Epoch941 layer2 out_loss 0.15690939128398895, R2 0.08261042833328247\n",
      "Test Epoch941 layer3 out_loss 0.15671414136886597, R2 0.08375203609466553\n",
      "Test Epoch941 layer4 out_loss 0.15675054490566254, R2 0.08353912830352783\n",
      "Train 942 | out_loss 0.39359575510025024: 100%|█| 125/125 [00:00<00:00, 240.18it\n",
      "Train Epoch942 out_loss 0.15491756796836853, R2 0.09126460552215576\n",
      "Test Epoch942 layer0 out_loss 0.15901923179626465, R2 0.07027506828308105\n",
      "Test Epoch942 layer1 out_loss 0.15895508229732513, R2 0.07065010070800781\n",
      "Test Epoch942 layer2 out_loss 0.15816234052181244, R2 0.07528495788574219\n",
      "Test Epoch942 layer3 out_loss 0.1582859754562378, R2 0.07456207275390625\n",
      "Test Epoch942 layer4 out_loss 0.1586359292268753, R2 0.07251608371734619\n",
      "Train 943 | out_loss 0.39375418424606323: 100%|█| 125/125 [00:00<00:00, 246.19it\n",
      "Train Epoch943 out_loss 0.155042365193367, R2 0.09053260087966919\n",
      "Test Epoch943 layer0 out_loss 0.15896354615688324, R2 0.07060056924819946\n",
      "Test Epoch943 layer1 out_loss 0.15798553824424744, R2 0.07631868124008179\n",
      "Test Epoch943 layer2 out_loss 0.15696604549884796, R2 0.08227920532226562\n",
      "Test Epoch943 layer3 out_loss 0.1569221019744873, R2 0.08253616094589233\n",
      "Test Epoch943 layer4 out_loss 0.15714584290981293, R2 0.08122801780700684\n",
      "Train 944 | out_loss 0.39396533370018005: 100%|█| 125/125 [00:00<00:00, 253.61it\n",
      "Train Epoch944 out_loss 0.15520863234996796, R2 0.08955729007720947\n",
      "Test Epoch944 layer0 out_loss 0.1589118093252182, R2 0.07090312242507935\n",
      "Test Epoch944 layer1 out_loss 0.15788634121418, R2 0.07689863443374634\n",
      "Test Epoch944 layer2 out_loss 0.15693169832229614, R2 0.08248001337051392\n",
      "Test Epoch944 layer3 out_loss 0.156679168343544, R2 0.08395648002624512\n",
      "Test Epoch944 layer4 out_loss 0.15676623582839966, R2 0.08344745635986328\n",
      "Train 945 | out_loss 0.3937016725540161: 100%|█| 125/125 [00:00<00:00, 245.47it/\n",
      "Train Epoch945 out_loss 0.1550009846687317, R2 0.0907752513885498\n",
      "Test Epoch945 layer0 out_loss 0.15886077284812927, R2 0.07120144367218018\n",
      "Test Epoch945 layer1 out_loss 0.15767860412597656, R2 0.0781131386756897\n",
      "Test Epoch945 layer2 out_loss 0.15688909590244293, R2 0.08272910118103027\n",
      "Test Epoch945 layer3 out_loss 0.1568795144557953, R2 0.08278512954711914\n",
      "Test Epoch945 layer4 out_loss 0.15689948201179504, R2 0.08266842365264893\n",
      "Train 946 | out_loss 0.39372286200523376: 100%|█| 125/125 [00:00<00:00, 234.34it\n",
      "Train Epoch946 out_loss 0.1550176739692688, R2 0.09067744016647339\n",
      "Test Epoch946 layer0 out_loss 0.15887030959129333, R2 0.07114565372467041\n",
      "Test Epoch946 layer1 out_loss 0.15768256783485413, R2 0.07808995246887207\n",
      "Test Epoch946 layer2 out_loss 0.1569480001926422, R2 0.08238476514816284\n",
      "Test Epoch946 layer3 out_loss 0.15665924549102783, R2 0.084073007106781\n",
      "Test Epoch946 layer4 out_loss 0.15668067336082458, R2 0.08394765853881836\n",
      "Train 947 | out_loss 0.3936173915863037: 100%|█| 125/125 [00:00<00:00, 247.99it/\n",
      "Train Epoch947 out_loss 0.15493462979793549, R2 0.09116452932357788\n",
      "Test Epoch947 layer0 out_loss 0.15878687798976898, R2 0.07163351774215698\n",
      "Test Epoch947 layer1 out_loss 0.1576032191514969, R2 0.07855385541915894\n",
      "Test Epoch947 layer2 out_loss 0.1568605601787567, R2 0.08289593458175659\n",
      "Test Epoch947 layer3 out_loss 0.15663088858127594, R2 0.08423876762390137\n",
      "Test Epoch947 layer4 out_loss 0.15668566524982452, R2 0.08391845226287842\n",
      "Train 948 | out_loss 0.3936789631843567: 100%|█| 125/125 [00:00<00:00, 251.64it/\n",
      "Train Epoch948 out_loss 0.15498317778110504, R2 0.09087973833084106\n",
      "Test Epoch948 layer0 out_loss 0.15877538919448853, R2 0.07170069217681885\n",
      "Test Epoch948 layer1 out_loss 0.1576095074415207, R2 0.0785171389579773\n",
      "Test Epoch948 layer2 out_loss 0.1569904088973999, R2 0.08213680982589722\n",
      "Test Epoch948 layer3 out_loss 0.1568623185157776, R2 0.08288568258285522\n",
      "Test Epoch948 layer4 out_loss 0.1569555401802063, R2 0.08234065771102905\n",
      "Train 949 | out_loss 0.3937336504459381: 100%|█| 125/125 [00:00<00:00, 262.29it/\n",
      "Train Epoch949 out_loss 0.1550261527299881, R2 0.09062767028808594\n",
      "Test Epoch949 layer0 out_loss 0.15879245102405548, R2 0.07160091400146484\n",
      "Test Epoch949 layer1 out_loss 0.1576187014579773, R2 0.07846343517303467\n",
      "Test Epoch949 layer2 out_loss 0.15682028234004974, R2 0.08313143253326416\n",
      "Test Epoch949 layer3 out_loss 0.15664233267307281, R2 0.0841718316078186\n",
      "Test Epoch949 layer4 out_loss 0.1566668450832367, R2 0.08402854204177856\n",
      "Train 950 | out_loss 0.393832266330719: 100%|█| 125/125 [00:00<00:00, 253.32it/s\n",
      "Train Epoch950 out_loss 0.15510383248329163, R2 0.09017199277877808\n",
      "Test Epoch950 layer0 out_loss 0.15878735482692719, R2 0.07163071632385254\n",
      "Test Epoch950 layer1 out_loss 0.15759482979774475, R2 0.07860291004180908\n",
      "Test Epoch950 layer2 out_loss 0.15687787532806396, R2 0.08279478549957275\n",
      "Test Epoch950 layer3 out_loss 0.15667158365249634, R2 0.08400082588195801\n",
      "Test Epoch950 layer4 out_loss 0.156769260764122, R2 0.08342975378036499\n",
      "Train 951 | out_loss 0.39364156126976013: 100%|█| 125/125 [00:00<00:00, 238.62it\n",
      "Train Epoch951 out_loss 0.15495365858078003, R2 0.09105294942855835\n",
      "Test Epoch951 layer0 out_loss 0.15878038108348846, R2 0.0716714859008789\n",
      "Test Epoch951 layer1 out_loss 0.1576276570558548, R2 0.0784110426902771\n",
      "Test Epoch951 layer2 out_loss 0.15691757202148438, R2 0.08256262540817261\n",
      "Test Epoch951 layer3 out_loss 0.15676510334014893, R2 0.08345401287078857\n",
      "Test Epoch951 layer4 out_loss 0.15686005353927612, R2 0.08289897441864014\n",
      "Train 952 | out_loss 0.3938259184360504: 100%|█| 125/125 [00:00<00:00, 250.47it/\n",
      "Train Epoch952 out_loss 0.15509885549545288, R2 0.09020119905471802\n",
      "Test Epoch952 layer0 out_loss 0.15884633362293243, R2 0.0712859034538269\n",
      "Test Epoch952 layer1 out_loss 0.15774455666542053, R2 0.07772761583328247\n",
      "Test Epoch952 layer2 out_loss 0.15735875070095062, R2 0.07998323440551758\n",
      "Test Epoch952 layer3 out_loss 0.15724146366119385, R2 0.08066892623901367\n",
      "Test Epoch952 layer4 out_loss 0.1572915017604828, R2 0.08037638664245605\n",
      "Train 953 | out_loss 0.39360833168029785: 100%|█| 125/125 [00:00<00:00, 242.94it\n",
      "Train Epoch953 out_loss 0.15492752194404602, R2 0.09120625257492065\n",
      "Test Epoch953 layer0 out_loss 0.15878908336162567, R2 0.0716206431388855\n",
      "Test Epoch953 layer1 out_loss 0.157901331782341, R2 0.07681095600128174\n",
      "Test Epoch953 layer2 out_loss 0.1571836918592453, R2 0.08100676536560059\n",
      "Test Epoch953 layer3 out_loss 0.15688562393188477, R2 0.08274942636489868\n",
      "Test Epoch953 layer4 out_loss 0.15707634389400482, R2 0.08163440227508545\n",
      "Train 954 | out_loss 0.39371275901794434: 100%|█| 125/125 [00:00<00:00, 254.38it\n",
      "Train Epoch954 out_loss 0.15500973165035248, R2 0.09072399139404297\n",
      "Test Epoch954 layer0 out_loss 0.15882155299186707, R2 0.07143080234527588\n",
      "Test Epoch954 layer1 out_loss 0.1577730029821396, R2 0.07756119966506958\n",
      "Test Epoch954 layer2 out_loss 0.1569441258907318, R2 0.08240741491317749\n",
      "Test Epoch954 layer3 out_loss 0.1569025218486786, R2 0.08265060186386108\n",
      "Test Epoch954 layer4 out_loss 0.1569877564907074, R2 0.08215224742889404\n",
      "Train 955 | out_loss 0.39356598258018494: 100%|█| 125/125 [00:00<00:00, 253.71it\n",
      "Train Epoch955 out_loss 0.15489411354064941, R2 0.09140217304229736\n",
      "Test Epoch955 layer0 out_loss 0.1588621735572815, R2 0.07119333744049072\n",
      "Test Epoch955 layer1 out_loss 0.15773195028305054, R2 0.07780128717422485\n",
      "Test Epoch955 layer2 out_loss 0.15692225098609924, R2 0.0825352668762207\n",
      "Test Epoch955 layer3 out_loss 0.1567234843969345, R2 0.08369743824005127\n",
      "Test Epoch955 layer4 out_loss 0.15670065581798553, R2 0.08383089303970337\n",
      "Train 956 | out_loss 0.3937210738658905: 100%|█| 125/125 [00:00<00:00, 239.35it/\n",
      "Train Epoch956 out_loss 0.15501630306243896, R2 0.09068548679351807\n",
      "Test Epoch956 layer0 out_loss 0.15882132947444916, R2 0.07143205404281616\n",
      "Test Epoch956 layer1 out_loss 0.15804362297058105, R2 0.07597905397415161\n",
      "Test Epoch956 layer2 out_loss 0.15691743791103363, R2 0.08256345987319946\n",
      "Test Epoch956 layer3 out_loss 0.15655621886253357, R2 0.0846753716468811\n",
      "Test Epoch956 layer4 out_loss 0.15662206709384918, R2 0.08429032564163208\n",
      "Train 957 | out_loss 0.39371275901794434: 100%|█| 125/125 [00:00<00:00, 250.15it\n",
      "Train Epoch957 out_loss 0.1550096869468689, R2 0.09072422981262207\n",
      "Test Epoch957 layer0 out_loss 0.15878863632678986, R2 0.07162320613861084\n",
      "Test Epoch957 layer1 out_loss 0.1576511412858963, R2 0.07827377319335938\n",
      "Test Epoch957 layer2 out_loss 0.15682414174079895, R2 0.08310890197753906\n",
      "Test Epoch957 layer3 out_loss 0.15654052793979645, R2 0.08476704359054565\n",
      "Test Epoch957 layer4 out_loss 0.15663312375545502, R2 0.08422571420669556\n",
      "Train 958 | out_loss 0.39363908767700195: 100%|█| 125/125 [00:00<00:00, 247.46it\n",
      "Train Epoch958 out_loss 0.15495169162750244, R2 0.091064453125\n",
      "Test Epoch958 layer0 out_loss 0.1588166505098343, R2 0.07145941257476807\n",
      "Test Epoch958 layer1 out_loss 0.15763071179389954, R2 0.07839322090148926\n",
      "Test Epoch958 layer2 out_loss 0.1567913144826889, R2 0.08330076932907104\n",
      "Test Epoch958 layer3 out_loss 0.1565677672624588, R2 0.08460783958435059\n",
      "Test Epoch958 layer4 out_loss 0.15664194524288177, R2 0.08417409658432007\n",
      "Train 959 | out_loss 0.3938274383544922: 100%|█| 125/125 [00:00<00:00, 237.88it/\n",
      "Train Epoch959 out_loss 0.1551000475883484, R2 0.0901942253112793\n",
      "Test Epoch959 layer0 out_loss 0.15881486237049103, R2 0.07146990299224854\n",
      "Test Epoch959 layer1 out_loss 0.15770263969898224, R2 0.0779726505279541\n",
      "Test Epoch959 layer2 out_loss 0.1569044291973114, R2 0.08263945579528809\n",
      "Test Epoch959 layer3 out_loss 0.15680213272571564, R2 0.08323752880096436\n",
      "Test Epoch959 layer4 out_loss 0.15683306753635406, R2 0.08305668830871582\n",
      "Train 960 | out_loss 0.3935854732990265: 100%|█| 125/125 [00:00<00:00, 248.32it/\n",
      "Train Epoch960 out_loss 0.15490953624248505, R2 0.09131181240081787\n",
      "Test Epoch960 layer0 out_loss 0.1588413566350937, R2 0.0713149905204773\n",
      "Test Epoch960 layer1 out_loss 0.15778854489326477, R2 0.07747036218643188\n",
      "Test Epoch960 layer2 out_loss 0.15693213045597076, R2 0.08247750997543335\n",
      "Test Epoch960 layer3 out_loss 0.15672831237316132, R2 0.08366912603378296\n",
      "Test Epoch960 layer4 out_loss 0.15682661533355713, R2 0.08309441804885864\n",
      "Train 961 | out_loss 0.3940236568450928: 100%|█| 125/125 [00:00<00:00, 240.71it/\n",
      "Train Epoch961 out_loss 0.1552545577287674, R2 0.08928787708282471\n",
      "Test Epoch961 layer0 out_loss 0.15897494554519653, R2 0.07053393125534058\n",
      "Test Epoch961 layer1 out_loss 0.1584860384464264, R2 0.07339239120483398\n",
      "Test Epoch961 layer2 out_loss 0.1573798805475235, R2 0.0798596739768982\n",
      "Test Epoch961 layer3 out_loss 0.157553568482399, R2 0.07884418964385986\n",
      "Test Epoch961 layer4 out_loss 0.15766210854053497, R2 0.07820957899093628\n",
      "Train 962 | out_loss 0.3934108018875122: 100%|█| 125/125 [00:00<00:00, 244.96it/\n",
      "Train Epoch962 out_loss 0.15477204322814941, R2 0.0921182632446289\n",
      "Test Epoch962 layer0 out_loss 0.15951736271381378, R2 0.06736260652542114\n",
      "Test Epoch962 layer1 out_loss 0.15816129744052887, R2 0.07529103755950928\n",
      "Test Epoch962 layer2 out_loss 0.15715232491493225, R2 0.08119010925292969\n",
      "Test Epoch962 layer3 out_loss 0.1568317860364914, R2 0.08306419849395752\n",
      "Test Epoch962 layer4 out_loss 0.15702815353870392, R2 0.08191609382629395\n",
      "Train 963 | out_loss 0.3937559425830841: 100%|█| 125/125 [00:00<00:00, 237.95it/\n",
      "Train Epoch963 out_loss 0.15504373610019684, R2 0.09052455425262451\n",
      "Test Epoch963 layer0 out_loss 0.15894527733325958, R2 0.07070738077163696\n",
      "Test Epoch963 layer1 out_loss 0.1577383130788803, R2 0.07776409387588501\n",
      "Test Epoch963 layer2 out_loss 0.15706191956996918, R2 0.08171868324279785\n",
      "Test Epoch963 layer3 out_loss 0.15704140067100525, R2 0.08183866739273071\n",
      "Test Epoch963 layer4 out_loss 0.15720199048519135, R2 0.08089971542358398\n",
      "Train 964 | out_loss 0.3937411308288574: 100%|█| 125/125 [00:00<00:00, 253.68it/\n",
      "Train Epoch964 out_loss 0.15503203868865967, R2 0.09059315919876099\n",
      "Test Epoch964 layer0 out_loss 0.15877807140350342, R2 0.07168495655059814\n",
      "Test Epoch964 layer1 out_loss 0.15767917037010193, R2 0.07810986042022705\n",
      "Test Epoch964 layer2 out_loss 0.15702691674232483, R2 0.08192330598831177\n",
      "Test Epoch964 layer3 out_loss 0.15693475306034088, R2 0.08246219158172607\n",
      "Test Epoch964 layer4 out_loss 0.15704984962940216, R2 0.08178925514221191\n",
      "Train 965 | out_loss 0.3936214745044708: 100%|█| 125/125 [00:00<00:00, 251.66it/\n",
      "Train Epoch965 out_loss 0.15493783354759216, R2 0.09114575386047363\n",
      "Test Epoch965 layer0 out_loss 0.15890413522720337, R2 0.07094800472259521\n",
      "Test Epoch965 layer1 out_loss 0.1578453779220581, R2 0.07713806629180908\n",
      "Test Epoch965 layer2 out_loss 0.1568772792816162, R2 0.08279824256896973\n",
      "Test Epoch965 layer3 out_loss 0.15657338500022888, R2 0.08457499742507935\n",
      "Test Epoch965 layer4 out_loss 0.15660721063613892, R2 0.0843772292137146\n",
      "Train 966 | out_loss 0.3936266303062439: 100%|█| 125/125 [00:00<00:00, 252.44it/\n",
      "Train Epoch966 out_loss 0.15494193136692047, R2 0.09112173318862915\n",
      "Test Epoch966 layer0 out_loss 0.15885615348815918, R2 0.07122844457626343\n",
      "Test Epoch966 layer1 out_loss 0.15761928260326385, R2 0.07846003770828247\n",
      "Test Epoch966 layer2 out_loss 0.15680702030658722, R2 0.08320897817611694\n",
      "Test Epoch966 layer3 out_loss 0.1564866304397583, R2 0.08508217334747314\n",
      "Test Epoch966 layer4 out_loss 0.15654940903186798, R2 0.08471512794494629\n",
      "Train 967 | out_loss 0.3935767114162445: 100%|█| 125/125 [00:00<00:00, 257.82it/\n",
      "Train Epoch967 out_loss 0.15490268170833588, R2 0.09135198593139648\n",
      "Test Epoch967 layer0 out_loss 0.15908440947532654, R2 0.06989395618438721\n",
      "Test Epoch967 layer1 out_loss 0.15802913904190063, R2 0.07606369256973267\n",
      "Test Epoch967 layer2 out_loss 0.15715566277503967, R2 0.08117055892944336\n",
      "Test Epoch967 layer3 out_loss 0.15716518461704254, R2 0.08111488819122314\n",
      "Test Epoch967 layer4 out_loss 0.15738746523857117, R2 0.0798153281211853\n",
      "Train 968 | out_loss 0.3939349055290222: 100%|█| 125/125 [00:00<00:00, 249.12it/\n",
      "Train Epoch968 out_loss 0.1551847606897354, R2 0.08969736099243164\n",
      "Test Epoch968 layer0 out_loss 0.15879617631435394, R2 0.07157915830612183\n",
      "Test Epoch968 layer1 out_loss 0.15763838589191437, R2 0.07834833860397339\n",
      "Test Epoch968 layer2 out_loss 0.15682101249694824, R2 0.08312714099884033\n",
      "Test Epoch968 layer3 out_loss 0.15666323900222778, R2 0.08404964208602905\n",
      "Test Epoch968 layer4 out_loss 0.1567210853099823, R2 0.08371138572692871\n",
      "Train 969 | out_loss 0.3935394585132599: 100%|█| 125/125 [00:00<00:00, 253.31it/\n",
      "Train Epoch969 out_loss 0.1548732966184616, R2 0.09152436256408691\n",
      "Test Epoch969 layer0 out_loss 0.15884320437908173, R2 0.07130420207977295\n",
      "Test Epoch969 layer1 out_loss 0.15757885575294495, R2 0.0786963701248169\n",
      "Test Epoch969 layer2 out_loss 0.15688499808311462, R2 0.08275312185287476\n",
      "Test Epoch969 layer3 out_loss 0.15667150914669037, R2 0.08400124311447144\n",
      "Test Epoch969 layer4 out_loss 0.15669602155685425, R2 0.08385801315307617\n",
      "Train 970 | out_loss 0.3935199975967407: 100%|█| 125/125 [00:00<00:00, 261.19it/\n",
      "Train Epoch970 out_loss 0.1548580378293991, R2 0.09161388874053955\n",
      "Test Epoch970 layer0 out_loss 0.15894968807697296, R2 0.07068157196044922\n",
      "Test Epoch970 layer1 out_loss 0.15764681994915009, R2 0.07829898595809937\n",
      "Test Epoch970 layer2 out_loss 0.15696275234222412, R2 0.08229845762252808\n",
      "Test Epoch970 layer3 out_loss 0.15670451521873474, R2 0.0838083028793335\n",
      "Test Epoch970 layer4 out_loss 0.15675146877765656, R2 0.08353376388549805\n",
      "Train 971 | out_loss 0.3936193287372589: 100%|█| 125/125 [00:00<00:00, 261.48it/\n",
      "Train Epoch971 out_loss 0.15493613481521606, R2 0.09115570783615112\n",
      "Test Epoch971 layer0 out_loss 0.15879075229167938, R2 0.07161080837249756\n",
      "Test Epoch971 layer1 out_loss 0.15758763253688812, R2 0.07864505052566528\n",
      "Test Epoch971 layer2 out_loss 0.15680010616779327, R2 0.08324939012527466\n",
      "Test Epoch971 layer3 out_loss 0.15658332407474518, R2 0.08451682329177856\n",
      "Test Epoch971 layer4 out_loss 0.15665099024772644, R2 0.0841212272644043\n",
      "Train 972 | out_loss 0.393517404794693: 100%|█| 125/125 [00:00<00:00, 248.40it/s\n",
      "Train Epoch972 out_loss 0.15485592186450958, R2 0.09162622690200806\n",
      "Test Epoch972 layer0 out_loss 0.1590067595243454, R2 0.07034796476364136\n",
      "Test Epoch972 layer1 out_loss 0.15797899663448334, R2 0.07635682821273804\n",
      "Test Epoch972 layer2 out_loss 0.1569589525461197, R2 0.0823206901550293\n",
      "Test Epoch972 layer3 out_loss 0.15691791474819183, R2 0.08256065845489502\n",
      "Test Epoch972 layer4 out_loss 0.15710221230983734, R2 0.08148306608200073\n",
      "Train 973 | out_loss 0.39364686608314514: 100%|█| 125/125 [00:00<00:00, 259.71it\n",
      "Train Epoch973 out_loss 0.1549578458070755, R2 0.09102839231491089\n",
      "Test Epoch973 layer0 out_loss 0.15887410938739777, R2 0.07112348079681396\n",
      "Test Epoch973 layer1 out_loss 0.1576060652732849, R2 0.07853728532791138\n",
      "Test Epoch973 layer2 out_loss 0.1568509191274643, R2 0.08295232057571411\n",
      "Test Epoch973 layer3 out_loss 0.15659074485301971, R2 0.08447349071502686\n",
      "Test Epoch973 layer4 out_loss 0.1566218137741089, R2 0.08429181575775146\n",
      "Train 974 | out_loss 0.3936997950077057: 100%|█| 125/125 [00:00<00:00, 239.02it/\n",
      "Train Epoch974 out_loss 0.1549995094537735, R2 0.09078395366668701\n",
      "Test Epoch974 layer0 out_loss 0.15880820155143738, R2 0.07150888442993164\n",
      "Test Epoch974 layer1 out_loss 0.15758977830410004, R2 0.07863247394561768\n",
      "Test Epoch974 layer2 out_loss 0.15674251317977905, R2 0.08358615636825562\n",
      "Test Epoch974 layer3 out_loss 0.15659797191619873, R2 0.08443117141723633\n",
      "Test Epoch974 layer4 out_loss 0.15665361285209656, R2 0.08410590887069702\n",
      "Train 975 | out_loss 0.3937891125679016: 100%|█| 125/125 [00:00<00:00, 251.78it/\n",
      "Train Epoch975 out_loss 0.15506984293460846, R2 0.09037142992019653\n",
      "Test Epoch975 layer0 out_loss 0.1588563770055771, R2 0.07122713327407837\n",
      "Test Epoch975 layer1 out_loss 0.15806053578853607, R2 0.07588016986846924\n",
      "Test Epoch975 layer2 out_loss 0.15810464322566986, R2 0.07562226057052612\n",
      "Test Epoch975 layer3 out_loss 0.15820057690143585, R2 0.0750613808631897\n",
      "Test Epoch975 layer4 out_loss 0.15841156244277954, R2 0.07382786273956299\n",
      "Train 976 | out_loss 0.39360150694847107: 100%|█| 125/125 [00:00<00:00, 261.79it\n",
      "Train Epoch976 out_loss 0.15492211282253265, R2 0.09123796224594116\n",
      "Test Epoch976 layer0 out_loss 0.1587941199541092, R2 0.07159119844436646\n",
      "Test Epoch976 layer1 out_loss 0.1575920134782791, R2 0.07861942052841187\n",
      "Test Epoch976 layer2 out_loss 0.1568649709224701, R2 0.08287012577056885\n",
      "Test Epoch976 layer3 out_loss 0.1567266285419464, R2 0.08367902040481567\n",
      "Test Epoch976 layer4 out_loss 0.15674173831939697, R2 0.08359062671661377\n",
      "Train 977 | out_loss 0.3935708701610565: 100%|█| 125/125 [00:00<00:00, 244.98it/\n",
      "Train Epoch977 out_loss 0.15489798784255981, R2 0.09137946367263794\n",
      "Test Epoch977 layer0 out_loss 0.15893177688121796, R2 0.07078635692596436\n",
      "Test Epoch977 layer1 out_loss 0.15804779529571533, R2 0.07595467567443848\n",
      "Test Epoch977 layer2 out_loss 0.15716639161109924, R2 0.08110785484313965\n",
      "Test Epoch977 layer3 out_loss 0.15683278441429138, R2 0.08305835723876953\n",
      "Test Epoch977 layer4 out_loss 0.15683245658874512, R2 0.08306026458740234\n",
      "Train 978 | out_loss 0.3937236964702606: 100%|█| 125/125 [00:00<00:00, 264.00it/\n",
      "Train Epoch978 out_loss 0.1550183892250061, R2 0.09067326784133911\n",
      "Test Epoch978 layer0 out_loss 0.1588422805070877, R2 0.07130956649780273\n",
      "Test Epoch978 layer1 out_loss 0.1575809121131897, R2 0.07868427038192749\n",
      "Test Epoch978 layer2 out_loss 0.15692071616649628, R2 0.08254426717758179\n",
      "Test Epoch978 layer3 out_loss 0.15669116377830505, R2 0.08388638496398926\n",
      "Test Epoch978 layer4 out_loss 0.15676891803741455, R2 0.08343172073364258\n",
      "Train 979 | out_loss 0.3937322497367859: 100%|█| 125/125 [00:00<00:00, 257.35it/\n",
      "Train Epoch979 out_loss 0.15502506494522095, R2 0.0906340479850769\n",
      "Test Epoch979 layer0 out_loss 0.1588376760482788, R2 0.07133650779724121\n",
      "Test Epoch979 layer1 out_loss 0.1580667644739151, R2 0.07584375143051147\n",
      "Test Epoch979 layer2 out_loss 0.15757879614830017, R2 0.07869672775268555\n",
      "Test Epoch979 layer3 out_loss 0.1575566679239273, R2 0.07882606983184814\n",
      "Test Epoch979 layer4 out_loss 0.1576617807149887, R2 0.07821154594421387\n",
      "Train 980 | out_loss 0.39363208413124084: 100%|█| 125/125 [00:00<00:00, 253.70it\n",
      "Train Epoch980 out_loss 0.1549461930990219, R2 0.09109669923782349\n",
      "Test Epoch980 layer0 out_loss 0.15887030959129333, R2 0.07114565372467041\n",
      "Test Epoch980 layer1 out_loss 0.15827222168445587, R2 0.07464253902435303\n",
      "Test Epoch980 layer2 out_loss 0.1576540321111679, R2 0.07825678586959839\n",
      "Test Epoch980 layer3 out_loss 0.1575791984796524, R2 0.0786944031715393\n",
      "Test Epoch980 layer4 out_loss 0.1576606184244156, R2 0.07821834087371826\n",
      "Train 981 | out_loss 0.39383983612060547: 100%|█| 125/125 [00:00<00:00, 251.55it\n",
      "Train Epoch981 out_loss 0.15510982275009155, R2 0.09013688564300537\n",
      "Test Epoch981 layer0 out_loss 0.15930883586406708, R2 0.06858181953430176\n",
      "Test Epoch981 layer1 out_loss 0.15852679312229156, R2 0.07315409183502197\n",
      "Test Epoch981 layer2 out_loss 0.15764184296131134, R2 0.07832807302474976\n",
      "Test Epoch981 layer3 out_loss 0.15752992033958435, R2 0.07898247241973877\n",
      "Test Epoch981 layer4 out_loss 0.1578918695449829, R2 0.0768662691116333\n",
      "Train 982 | out_loss 0.3937367796897888: 100%|█| 125/125 [00:00<00:00, 256.75it/\n",
      "Train Epoch982 out_loss 0.15502867102622986, R2 0.09061288833618164\n",
      "Test Epoch982 layer0 out_loss 0.15884217619895935, R2 0.07131016254425049\n",
      "Test Epoch982 layer1 out_loss 0.1578526645898819, R2 0.07709550857543945\n",
      "Test Epoch982 layer2 out_loss 0.1574360728263855, R2 0.07953119277954102\n",
      "Test Epoch982 layer3 out_loss 0.15714618563652039, R2 0.08122605085372925\n",
      "Test Epoch982 layer4 out_loss 0.157428577542305, R2 0.07957500219345093\n",
      "Train 983 | out_loss 0.3935321569442749: 100%|█| 125/125 [00:00<00:00, 251.24it/\n",
      "Train Epoch983 out_loss 0.1548675298690796, R2 0.09155821800231934\n",
      "Test Epoch983 layer0 out_loss 0.15879109501838684, R2 0.07160884141921997\n",
      "Test Epoch983 layer1 out_loss 0.15790419280529022, R2 0.07679426670074463\n",
      "Test Epoch983 layer2 out_loss 0.1569528430700302, R2 0.08235639333724976\n",
      "Test Epoch983 layer3 out_loss 0.15681825578212738, R2 0.08314329385757446\n",
      "Test Epoch983 layer4 out_loss 0.1569877713918686, R2 0.08215218782424927\n",
      "Train 984 | out_loss 0.3935558795928955: 100%|█| 125/125 [00:00<00:00, 242.57it/\n",
      "Train Epoch984 out_loss 0.15488620102405548, R2 0.09144866466522217\n",
      "Test Epoch984 layer0 out_loss 0.15882368385791779, R2 0.07141828536987305\n",
      "Test Epoch984 layer1 out_loss 0.15778455138206482, R2 0.07749378681182861\n",
      "Test Epoch984 layer2 out_loss 0.1569330096244812, R2 0.08247232437133789\n",
      "Test Epoch984 layer3 out_loss 0.15680044889450073, R2 0.08324742317199707\n",
      "Test Epoch984 layer4 out_loss 0.15684175491333008, R2 0.08300590515136719\n",
      "Train 985 | out_loss 0.39360591769218445: 100%|█| 125/125 [00:00<00:00, 244.97it\n",
      "Train Epoch985 out_loss 0.15492558479309082, R2 0.09121763706207275\n",
      "Test Epoch985 layer0 out_loss 0.1587795466184616, R2 0.07167643308639526\n",
      "Test Epoch985 layer1 out_loss 0.157568097114563, R2 0.07875925302505493\n",
      "Test Epoch985 layer2 out_loss 0.15682768821716309, R2 0.08308815956115723\n",
      "Test Epoch985 layer3 out_loss 0.15674948692321777, R2 0.08354538679122925\n",
      "Test Epoch985 layer4 out_loss 0.15676601231098175, R2 0.08344876766204834\n",
      "Train 986 | out_loss 0.3937096893787384: 100%|█| 125/125 [00:00<00:00, 240.33it/\n",
      "Train Epoch986 out_loss 0.1550072282552719, R2 0.09073865413665771\n",
      "Test Epoch986 layer0 out_loss 0.15877459943294525, R2 0.07170528173446655\n",
      "Test Epoch986 layer1 out_loss 0.1575438678264618, R2 0.07890093326568604\n",
      "Test Epoch986 layer2 out_loss 0.15673699975013733, R2 0.08361834287643433\n",
      "Test Epoch986 layer3 out_loss 0.15648801624774933, R2 0.08507406711578369\n",
      "Test Epoch986 layer4 out_loss 0.15658673644065857, R2 0.08449691534042358\n",
      "Train 987 | out_loss 0.3935631215572357: 100%|█| 125/125 [00:00<00:00, 263.23it/\n",
      "Train Epoch987 out_loss 0.1548919677734375, R2 0.09141480922698975\n",
      "Test Epoch987 layer0 out_loss 0.15876907110214233, R2 0.07173758745193481\n",
      "Test Epoch987 layer1 out_loss 0.15761612355709076, R2 0.07847851514816284\n",
      "Test Epoch987 layer2 out_loss 0.15682080388069153, R2 0.08312839269638062\n",
      "Test Epoch987 layer3 out_loss 0.15671703219413757, R2 0.08373516798019409\n",
      "Test Epoch987 layer4 out_loss 0.15679602324962616, R2 0.08327329158782959\n",
      "Train 988 | out_loss 0.3936755359172821: 100%|█| 125/125 [00:00<00:00, 238.35it/\n",
      "Train Epoch988 out_loss 0.15498040616512299, R2 0.09089601039886475\n",
      "Test Epoch988 layer0 out_loss 0.15880663692951202, R2 0.0715179443359375\n",
      "Test Epoch988 layer1 out_loss 0.1575768142938614, R2 0.07870829105377197\n",
      "Test Epoch988 layer2 out_loss 0.15693403780460358, R2 0.08246636390686035\n",
      "Test Epoch988 layer3 out_loss 0.1566949486732483, R2 0.08386421203613281\n",
      "Test Epoch988 layer4 out_loss 0.1567184180021286, R2 0.08372700214385986\n",
      "Train 989 | out_loss 0.3937098979949951: 100%|█| 125/125 [00:00<00:00, 256.96it/\n",
      "Train Epoch989 out_loss 0.15500745177268982, R2 0.09073740243911743\n",
      "Test Epoch989 layer0 out_loss 0.15877088904380798, R2 0.0717269778251648\n",
      "Test Epoch989 layer1 out_loss 0.1575687676668167, R2 0.07875531911849976\n",
      "Test Epoch989 layer2 out_loss 0.15685199201107025, R2 0.0829460620880127\n",
      "Test Epoch989 layer3 out_loss 0.15674884617328644, R2 0.0835491418838501\n",
      "Test Epoch989 layer4 out_loss 0.1567872166633606, R2 0.08332479000091553\n",
      "Train 990 | out_loss 0.3936464786529541: 100%|█| 125/125 [00:00<00:00, 265.10it/\n",
      "Train Epoch990 out_loss 0.15495756268501282, R2 0.09103000164031982\n",
      "Test Epoch990 layer0 out_loss 0.15879876911640167, R2 0.07156401872634888\n",
      "Test Epoch990 layer1 out_loss 0.15755560994148254, R2 0.07883232831954956\n",
      "Test Epoch990 layer2 out_loss 0.15680615603923798, R2 0.08321404457092285\n",
      "Test Epoch990 layer3 out_loss 0.15659888088703156, R2 0.0844259262084961\n",
      "Test Epoch990 layer4 out_loss 0.15666867792606354, R2 0.08401787281036377\n",
      "Train 991 | out_loss 0.3936580419540405: 100%|█| 125/125 [00:00<00:00, 264.81it/\n",
      "Train Epoch991 out_loss 0.15496669709682465, R2 0.09097647666931152\n",
      "Test Epoch991 layer0 out_loss 0.15876653790473938, R2 0.07175242900848389\n",
      "Test Epoch991 layer1 out_loss 0.15755657851696014, R2 0.07882660627365112\n",
      "Test Epoch991 layer2 out_loss 0.15675652027130127, R2 0.08350425958633423\n",
      "Test Epoch991 layer3 out_loss 0.15658803284168243, R2 0.08448934555053711\n",
      "Test Epoch991 layer4 out_loss 0.15666943788528442, R2 0.08401340246200562\n",
      "Train 992 | out_loss 0.39380431175231934: 100%|█| 125/125 [00:00<00:00, 243.88it\n",
      "Train Epoch992 out_loss 0.1550818532705307, R2 0.09030097723007202\n",
      "Test Epoch992 layer0 out_loss 0.15877757966518402, R2 0.07168793678283691\n",
      "Test Epoch992 layer1 out_loss 0.1575336754322052, R2 0.07896047830581665\n",
      "Test Epoch992 layer2 out_loss 0.15675777196884155, R2 0.08349692821502686\n",
      "Test Epoch992 layer3 out_loss 0.15657617151737213, R2 0.08455866575241089\n",
      "Test Epoch992 layer4 out_loss 0.15665997564792633, R2 0.08406871557235718\n",
      "Train 993 | out_loss 0.3935088515281677: 100%|█| 125/125 [00:00<00:00, 253.76it/\n",
      "Train Epoch993 out_loss 0.15484924614429474, R2 0.09166544675827026\n",
      "Test Epoch993 layer0 out_loss 0.15876732766628265, R2 0.0717477798461914\n",
      "Test Epoch993 layer1 out_loss 0.15755197405815125, R2 0.0788535475730896\n",
      "Test Epoch993 layer2 out_loss 0.15681076049804688, R2 0.08318710327148438\n",
      "Test Epoch993 layer3 out_loss 0.15657946467399597, R2 0.08453941345214844\n",
      "Test Epoch993 layer4 out_loss 0.15663406252861023, R2 0.08422017097473145\n",
      "Train 994 | out_loss 0.3933352530002594: 100%|█| 125/125 [00:00<00:00, 249.00it/\n",
      "Train Epoch994 out_loss 0.15471258759498596, R2 0.09246706962585449\n",
      "Test Epoch994 layer0 out_loss 0.15894712507724762, R2 0.07069659233093262\n",
      "Test Epoch994 layer1 out_loss 0.15791986882686615, R2 0.0767025351524353\n",
      "Test Epoch994 layer2 out_loss 0.1573866456747055, R2 0.07982015609741211\n",
      "Test Epoch994 layer3 out_loss 0.15746106207370758, R2 0.07938504219055176\n",
      "Test Epoch994 layer4 out_loss 0.15743203461170197, R2 0.0795547366142273\n",
      "Train 995 | out_loss 0.3936203122138977: 100%|█| 125/125 [00:00<00:00, 258.17it/\n",
      "Train Epoch995 out_loss 0.15493696928024292, R2 0.09115082025527954\n",
      "Test Epoch995 layer0 out_loss 0.1588153839111328, R2 0.07146686315536499\n",
      "Test Epoch995 layer1 out_loss 0.15754877030849457, R2 0.07887226343154907\n",
      "Test Epoch995 layer2 out_loss 0.15675365924835205, R2 0.08352094888687134\n",
      "Test Epoch995 layer3 out_loss 0.15662845969200134, R2 0.08425295352935791\n",
      "Test Epoch995 layer4 out_loss 0.1567109078168869, R2 0.08377093076705933\n",
      "Train 996 | out_loss 0.3936966061592102: 100%|█| 125/125 [00:00<00:00, 245.56it/\n",
      "Train Epoch996 out_loss 0.15499700605869293, R2 0.09079861640930176\n",
      "Test Epoch996 layer0 out_loss 0.15878495573997498, R2 0.07164472341537476\n",
      "Test Epoch996 layer1 out_loss 0.1575895994901657, R2 0.07863354682922363\n",
      "Test Epoch996 layer2 out_loss 0.15686987340450287, R2 0.08284151554107666\n",
      "Test Epoch996 layer3 out_loss 0.15667293965816498, R2 0.08399289846420288\n",
      "Test Epoch996 layer4 out_loss 0.15676796436309814, R2 0.08343738317489624\n",
      "Train 997 | out_loss 0.3935571312904358: 100%|█| 125/125 [00:00<00:00, 254.72it/\n",
      "Train Epoch997 out_loss 0.15488725900650024, R2 0.09144246578216553\n",
      "Test Epoch997 layer0 out_loss 0.15877777338027954, R2 0.0716867446899414\n",
      "Test Epoch997 layer1 out_loss 0.1578560471534729, R2 0.07707571983337402\n",
      "Test Epoch997 layer2 out_loss 0.15726728737354279, R2 0.08051794767379761\n",
      "Test Epoch997 layer3 out_loss 0.15697680413722992, R2 0.08221632242202759\n",
      "Test Epoch997 layer4 out_loss 0.15704794228076935, R2 0.08180040121078491\n",
      "Train 998 | out_loss 0.3936304748058319: 100%|█| 125/125 [00:00<00:00, 253.94it/\n",
      "Train Epoch998 out_loss 0.15494495630264282, R2 0.09110397100448608\n",
      "Test Epoch998 layer0 out_loss 0.15876691043376923, R2 0.07175028324127197\n",
      "Test Epoch998 layer1 out_loss 0.15755043923854828, R2 0.07886242866516113\n",
      "Test Epoch998 layer2 out_loss 0.15687595307826996, R2 0.08280599117279053\n",
      "Test Epoch998 layer3 out_loss 0.15665993094444275, R2 0.08406895399093628\n",
      "Test Epoch998 layer4 out_loss 0.1566678285598755, R2 0.08402276039123535\n",
      "Train 999 | out_loss 0.39360782504081726: 100%|█| 125/125 [00:00<00:00, 257.71it\n",
      "Train Epoch999 out_loss 0.1549271047115326, R2 0.09120869636535645\n",
      "Test Epoch999 layer0 out_loss 0.15877749025821686, R2 0.07168835401535034\n",
      "Test Epoch999 layer1 out_loss 0.1575496941804886, R2 0.07886683940887451\n",
      "Test Epoch999 layer2 out_loss 0.15680858492851257, R2 0.08319985866546631\n",
      "Test Epoch999 layer3 out_loss 0.15658743679523468, R2 0.08449280261993408\n",
      "Test Epoch999 layer4 out_loss 0.15665146708488464, R2 0.08411842584609985\n",
      "Best loss 0.1564866304397583 at L3\n",
      "Figure(640x480)\n",
      "Figure(640x480)\n",
      "Figure(640x480)\n",
      "Figure(640x480)\n"
     ]
    }
   ],
   "source": [
    "# LinearAL ailerons\n",
    "\n",
    "data = \"criteo\"\n",
    "\n",
    "model =  \"linearal\"\n",
    "#for layer in range(1,11):\n",
    "for layer in [5]:\n",
    "    log = f\"result/{data}_{model}_l{layer}.log\"\n",
    "    !python3 dis_train_al.py --dataset {data} --model {model} --epoch 1000 --num-layer {layer} --lr 0.0001 --task regression # > {log}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchmetrics\n",
      "  Downloading torchmetrics-0.9.3-py3-none-any.whl (419 kB)\n",
      "\u001b[K     |████████████████████████████████| 419 kB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.3.1 in /opt/conda/lib/python3.9/site-packages (from torchmetrics) (1.12.0+cu113)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /opt/conda/lib/python3.9/site-packages (from torchmetrics) (1.21.5)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from torchmetrics) (21.3)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch>=1.3.1->torchmetrics) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging->torchmetrics) (3.0.4)\n",
      "Installing collected packages: torchmetrics\n",
      "Successfully installed torchmetrics-0.9.3\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ea8a7f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0008725115898554677\n",
      "0.0004098966061174112\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "print(statistics.mean(y))\n",
    "print(statistics.stdev(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46d15db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
