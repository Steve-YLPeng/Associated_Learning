{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "06dd4036",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "df = pd.read_csv(\"criteo_small.csv\")\n",
    "\n",
    "col_target = [\"label\"]\n",
    "col_dense = [f\"I{i}\" for i in range(1,14)]\n",
    "col_sparse = [f\"C{i}\" for i in range(1,27)]\n",
    "\n",
    "df[col_sparse] = df[col_sparse].fillna('-1', )\n",
    "df[col_dense] = df[col_dense].fillna(0,)\n",
    "\n",
    "for feat in col_sparse:\n",
    "    lbe = LabelEncoder()\n",
    "    df[feat] = lbe.fit_transform(df[feat])\n",
    "    \n",
    "mms = MinMaxScaler(feature_range=(0,1))\n",
    "df[col_dense] = mms.fit_transform(df[col_dense])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0f28f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "house_dataset = fetch_california_housing()\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    house_dataset.data,\n",
    "    columns=house_dataset.feature_names\n",
    ")\n",
    "df.loc[:,\"Price\"] = house_dataset.target\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "df.loc[:,:] = scaler.fit_transform(df)\n",
    "\n",
    "col_feature = house_dataset.feature_names\n",
    "col_target = [\"Price\"]\n",
    "\n",
    "y = df[col_target]\n",
    "x = df[col_feature]\n",
    "x = x.fillna(0)\n",
    "target_num = 1\n",
    "args.feature_dim = 8\n",
    "feature_train, feature_test, train_target, test_target = train_test_split(x, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f78ad47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.34476576,  0.98214266,  0.62855945, ...,  1.05254828,\n",
       "        -1.32783522,  2.12963148],\n",
       "       [ 2.33223796, -0.60701891,  0.32704136, ...,  1.04318455,\n",
       "        -1.32284391,  1.31415614],\n",
       "       [ 1.7826994 ,  1.85618152,  1.15562047, ...,  1.03850269,\n",
       "        -1.33282653,  1.25869341],\n",
       "       ...,\n",
       "       [-1.14259331, -0.92485123, -0.09031802, ...,  1.77823747,\n",
       "        -0.8237132 , -0.99274649],\n",
       "       [-1.05458292, -0.84539315, -0.04021111, ...,  1.77823747,\n",
       "        -0.87362627, -1.05860847],\n",
       "       [-0.78012947, -1.00430931, -0.07044252, ...,  1.75014627,\n",
       "        -0.83369581, -1.01787803]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.344766</td>\n",
       "      <td>0.982143</td>\n",
       "      <td>0.628559</td>\n",
       "      <td>-0.153758</td>\n",
       "      <td>-0.974429</td>\n",
       "      <td>-0.049597</td>\n",
       "      <td>1.052548</td>\n",
       "      <td>-1.327835</td>\n",
       "      <td>2.129631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.332238</td>\n",
       "      <td>-0.607019</td>\n",
       "      <td>0.327041</td>\n",
       "      <td>-0.263336</td>\n",
       "      <td>0.861439</td>\n",
       "      <td>-0.092512</td>\n",
       "      <td>1.043185</td>\n",
       "      <td>-1.322844</td>\n",
       "      <td>1.314156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.782699</td>\n",
       "      <td>1.856182</td>\n",
       "      <td>1.155620</td>\n",
       "      <td>-0.049016</td>\n",
       "      <td>-0.820777</td>\n",
       "      <td>-0.025843</td>\n",
       "      <td>1.038503</td>\n",
       "      <td>-1.332827</td>\n",
       "      <td>1.258693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.932968</td>\n",
       "      <td>1.856182</td>\n",
       "      <td>0.156966</td>\n",
       "      <td>-0.049833</td>\n",
       "      <td>-0.766028</td>\n",
       "      <td>-0.050329</td>\n",
       "      <td>1.038503</td>\n",
       "      <td>-1.337818</td>\n",
       "      <td>1.165100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.012881</td>\n",
       "      <td>1.856182</td>\n",
       "      <td>0.344711</td>\n",
       "      <td>-0.032906</td>\n",
       "      <td>-0.759847</td>\n",
       "      <td>-0.085616</td>\n",
       "      <td>1.038503</td>\n",
       "      <td>-1.337818</td>\n",
       "      <td>1.172900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20635</th>\n",
       "      <td>-1.216128</td>\n",
       "      <td>-0.289187</td>\n",
       "      <td>-0.155023</td>\n",
       "      <td>0.077354</td>\n",
       "      <td>-0.512592</td>\n",
       "      <td>-0.049110</td>\n",
       "      <td>1.801647</td>\n",
       "      <td>-0.758826</td>\n",
       "      <td>-1.115804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20636</th>\n",
       "      <td>-0.691593</td>\n",
       "      <td>-0.845393</td>\n",
       "      <td>0.276881</td>\n",
       "      <td>0.462365</td>\n",
       "      <td>-0.944405</td>\n",
       "      <td>0.005021</td>\n",
       "      <td>1.806329</td>\n",
       "      <td>-0.818722</td>\n",
       "      <td>-1.124470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20637</th>\n",
       "      <td>-1.142593</td>\n",
       "      <td>-0.924851</td>\n",
       "      <td>-0.090318</td>\n",
       "      <td>0.049414</td>\n",
       "      <td>-0.369537</td>\n",
       "      <td>-0.071735</td>\n",
       "      <td>1.778237</td>\n",
       "      <td>-0.823713</td>\n",
       "      <td>-0.992746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20638</th>\n",
       "      <td>-1.054583</td>\n",
       "      <td>-0.845393</td>\n",
       "      <td>-0.040211</td>\n",
       "      <td>0.158778</td>\n",
       "      <td>-0.604429</td>\n",
       "      <td>-0.091225</td>\n",
       "      <td>1.778237</td>\n",
       "      <td>-0.873626</td>\n",
       "      <td>-1.058608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20639</th>\n",
       "      <td>-0.780129</td>\n",
       "      <td>-1.004309</td>\n",
       "      <td>-0.070443</td>\n",
       "      <td>0.138403</td>\n",
       "      <td>-0.033977</td>\n",
       "      <td>-0.043682</td>\n",
       "      <td>1.750146</td>\n",
       "      <td>-0.833696</td>\n",
       "      <td>-1.017878</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20640 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  \\\n",
       "0      2.344766  0.982143  0.628559  -0.153758   -0.974429 -0.049597   \n",
       "1      2.332238 -0.607019  0.327041  -0.263336    0.861439 -0.092512   \n",
       "2      1.782699  1.856182  1.155620  -0.049016   -0.820777 -0.025843   \n",
       "3      0.932968  1.856182  0.156966  -0.049833   -0.766028 -0.050329   \n",
       "4     -0.012881  1.856182  0.344711  -0.032906   -0.759847 -0.085616   \n",
       "...         ...       ...       ...        ...         ...       ...   \n",
       "20635 -1.216128 -0.289187 -0.155023   0.077354   -0.512592 -0.049110   \n",
       "20636 -0.691593 -0.845393  0.276881   0.462365   -0.944405  0.005021   \n",
       "20637 -1.142593 -0.924851 -0.090318   0.049414   -0.369537 -0.071735   \n",
       "20638 -1.054583 -0.845393 -0.040211   0.158778   -0.604429 -0.091225   \n",
       "20639 -0.780129 -1.004309 -0.070443   0.138403   -0.033977 -0.043682   \n",
       "\n",
       "       Latitude  Longitude     Price  \n",
       "0      1.052548  -1.327835  2.129631  \n",
       "1      1.043185  -1.322844  1.314156  \n",
       "2      1.038503  -1.332827  1.258693  \n",
       "3      1.038503  -1.337818  1.165100  \n",
       "4      1.038503  -1.337818  1.172900  \n",
       "...         ...        ...       ...  \n",
       "20635  1.801647  -0.758826 -1.115804  \n",
       "20636  1.806329  -0.818722 -1.124470  \n",
       "20637  1.778237  -0.823713 -0.992746  \n",
       "20638  1.778237  -0.873626 -1.058608  \n",
       "20639  1.750146  -0.833696 -1.017878  \n",
       "\n",
       "[20640 rows x 9 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "da3ef581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "      <td>4.526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "      <td>3.585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "      <td>3.521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20635</th>\n",
       "      <td>1.5603</td>\n",
       "      <td>25.0</td>\n",
       "      <td>5.045455</td>\n",
       "      <td>1.133333</td>\n",
       "      <td>845.0</td>\n",
       "      <td>2.560606</td>\n",
       "      <td>39.48</td>\n",
       "      <td>-121.09</td>\n",
       "      <td>0.781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20636</th>\n",
       "      <td>2.5568</td>\n",
       "      <td>18.0</td>\n",
       "      <td>6.114035</td>\n",
       "      <td>1.315789</td>\n",
       "      <td>356.0</td>\n",
       "      <td>3.122807</td>\n",
       "      <td>39.49</td>\n",
       "      <td>-121.21</td>\n",
       "      <td>0.771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20637</th>\n",
       "      <td>1.7000</td>\n",
       "      <td>17.0</td>\n",
       "      <td>5.205543</td>\n",
       "      <td>1.120092</td>\n",
       "      <td>1007.0</td>\n",
       "      <td>2.325635</td>\n",
       "      <td>39.43</td>\n",
       "      <td>-121.22</td>\n",
       "      <td>0.923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20638</th>\n",
       "      <td>1.8672</td>\n",
       "      <td>18.0</td>\n",
       "      <td>5.329513</td>\n",
       "      <td>1.171920</td>\n",
       "      <td>741.0</td>\n",
       "      <td>2.123209</td>\n",
       "      <td>39.43</td>\n",
       "      <td>-121.32</td>\n",
       "      <td>0.847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20639</th>\n",
       "      <td>2.3886</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5.254717</td>\n",
       "      <td>1.162264</td>\n",
       "      <td>1387.0</td>\n",
       "      <td>2.616981</td>\n",
       "      <td>39.37</td>\n",
       "      <td>-121.24</td>\n",
       "      <td>0.894</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20640 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0      8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
       "1      8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       "2      7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       "3      5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       "4      3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
       "...       ...       ...       ...        ...         ...       ...       ...   \n",
       "20635  1.5603      25.0  5.045455   1.133333       845.0  2.560606     39.48   \n",
       "20636  2.5568      18.0  6.114035   1.315789       356.0  3.122807     39.49   \n",
       "20637  1.7000      17.0  5.205543   1.120092      1007.0  2.325635     39.43   \n",
       "20638  1.8672      18.0  5.329513   1.171920       741.0  2.123209     39.43   \n",
       "20639  2.3886      16.0  5.254717   1.162264      1387.0  2.616981     39.37   \n",
       "\n",
       "       Longitude  Price  \n",
       "0        -122.23  4.526  \n",
       "1        -122.22  3.585  \n",
       "2        -122.24  3.521  \n",
       "3        -122.25  3.413  \n",
       "4        -122.25  3.422  \n",
       "...          ...    ...  \n",
       "20635    -121.09  0.781  \n",
       "20636    -121.21  0.771  \n",
       "20637    -121.22  0.923  \n",
       "20638    -121.32  0.847  \n",
       "20639    -121.24  0.894  \n",
       "\n",
       "[20640 rows x 9 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad319671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(635, 125)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_target = data.columns[:6]\n",
    "col_feature1 = data.columns[6:33].to_list() # 27 cols\n",
    "col_feature2 = data.columns[33:43].to_list() # 10 cols\n",
    "col_feature3 = data.columns[43:103].to_list() # 60 cols\n",
    "col_feature4 = data.columns[103:].to_list() # 28 cols\n",
    "y = data[col_target]\n",
    "x = data[col_feature1 + col_feature2 + col_feature3 + col_feature4]\n",
    "x = x.fillna(0)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2f394a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 92, 100,  78,  78,  84,  68])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_train, clean_test, train_label, test_label = train_test_split(x, y, test_size=0.2)\n",
    "train_label.to_numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e22348e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensor_point5_i_value     0\n",
      "sensor_point6_i_value     0\n",
      "sensor_point7_i_value     0\n",
      "sensor_point8_i_value     0\n",
      "sensor_point9_i_value     0\n",
      "sensor_point10_i_value    0\n",
      "dtype: int64\n",
      "sensor_point5_i_value     0\n",
      "sensor_point6_i_value     0\n",
      "sensor_point7_i_value     0\n",
      "sensor_point8_i_value     0\n",
      "sensor_point9_i_value     0\n",
      "sensor_point10_i_value    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print((y == 0).sum())\n",
    "print((y.isna()).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57f55723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      22\n",
      "238     1\n",
      "286     3\n",
      "621     1\n",
      "dtype: int64 \n",
      "\n",
      "0    10\n",
      "dtype: int64 \n",
      "\n",
      "0      20\n",
      "16      2\n",
      "18      3\n",
      "21      5\n",
      "131     5\n",
      "162    10\n",
      "164     3\n",
      "167     1\n",
      "171     1\n",
      "573    10\n",
      "dtype: int64 \n",
      "\n",
      "0      15\n",
      "27      5\n",
      "65      5\n",
      "111     1\n",
      "117     1\n",
      "579     1\n",
      "dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for col in [col_feature1,col_feature2,col_feature3,col_feature4]:\n",
    "    print((x[col]==0).sum(axis=0).value_counts().sort_index(),\"\\n\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d93b4abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    321\n",
      "2     28\n",
      "3     14\n",
      "4     62\n",
      "5    210\n",
      "dtype: int64 \n",
      "\n",
      "0    635\n",
      "dtype: int64 \n",
      "\n",
      "0      62\n",
      "10    345\n",
      "11      4\n",
      "12      3\n",
      "15     33\n",
      "20     69\n",
      "25      3\n",
      "30    109\n",
      "38      2\n",
      "40      5\n",
      "dtype: int64 \n",
      "\n",
      "0     52\n",
      "1    380\n",
      "2     86\n",
      "3     25\n",
      "6      4\n",
      "7     88\n",
      "dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for col in [col_feature1,col_feature2,col_feature3,col_feature4]:\n",
    "    print((x[col]==0).sum(axis=1).value_counts().sort_index(),\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbac891a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training\n",
      "  0%|                                                   | 0/258 [00:00<?, ?it/s]/home/AL_main_new/utils.py:99: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  x = torch.tensor([x for x,y in batch], dtype=torch.float32)\n",
      "Train 0 | out_loss 0.6323021650314331: 100%|█| 258/258 [00:02<00:00, 122.68it/s]\n",
      "Train Epoch0 out_loss 0.39980587363243103, R2 0.5999491214752197\n",
      "Test Epoch0 layer0 out_loss 0.31538090109825134, R2 0.6853834390640259\n",
      "Test Epoch0 layer1 out_loss 0.2953587472438812, R2 0.7053570747375488\n",
      "Test Epoch0 layer2 out_loss 0.29398369789123535, R2 0.7067288160324097\n",
      "Test Epoch0 layer3 out_loss 0.2986447811126709, R2 0.7020789980888367\n",
      "Test Epoch0 layer4 out_loss 0.3024398982524872, R2 0.6982930898666382\n",
      "Train 1 | out_loss 0.5403031706809998: 100%|█| 258/258 [00:01<00:00, 154.00it/s]\n",
      "Train Epoch1 out_loss 0.2919274866580963, R2 0.7078936100006104\n",
      "Test Epoch1 layer0 out_loss 0.2996157109737396, R2 0.7011104822158813\n",
      "Test Epoch1 layer1 out_loss 0.2886955440044403, R2 0.7120041251182556\n",
      "Test Epoch1 layer2 out_loss 0.28521156311035156, R2 0.7154796123504639\n",
      "Test Epoch1 layer3 out_loss 0.2886626124382019, R2 0.7120369672775269\n",
      "Test Epoch1 layer4 out_loss 0.293377548456192, R2 0.7073334455490112\n",
      "Train 2 | out_loss 0.5260453224182129: 100%|█| 258/258 [00:01<00:00, 151.23it/s]\n",
      "Train Epoch2 out_loss 0.27672383189201355, R2 0.7231066226959229\n",
      "Test Epoch2 layer0 out_loss 0.2991916835308075, R2 0.7015334367752075\n",
      "Test Epoch2 layer1 out_loss 0.2996504306793213, R2 0.7010757923126221\n",
      "Test Epoch2 layer2 out_loss 0.2929011285305023, R2 0.7078087329864502\n",
      "Test Epoch2 layer3 out_loss 0.2926892936229706, R2 0.7080200910568237\n",
      "Test Epoch2 layer4 out_loss 0.2936505973339081, R2 0.7070610523223877\n",
      "Train 3 | out_loss 0.5187503099441528: 100%|█| 258/258 [00:01<00:00, 158.23it/s]\n",
      "Train Epoch3 out_loss 0.2691018581390381, R2 0.7307332754135132\n",
      "Test Epoch3 layer0 out_loss 0.2899460792541504, R2 0.7107565999031067\n",
      "Test Epoch3 layer1 out_loss 0.2740139067173004, R2 0.7266501784324646\n",
      "Test Epoch3 layer2 out_loss 0.2707899808883667, R2 0.7298662662506104\n",
      "Test Epoch3 layer3 out_loss 0.26960957050323486, R2 0.731043815612793\n",
      "Test Epoch3 layer4 out_loss 0.27131417393684387, R2 0.7293434143066406\n",
      "Train 4 | out_loss 0.512669563293457: 100%|██| 258/258 [00:01<00:00, 140.68it/s]\n",
      "Train Epoch4 out_loss 0.26283004879951477, R2 0.7370088696479797\n",
      "Test Epoch4 layer0 out_loss 0.29101040959358215, R2 0.7096948623657227\n",
      "Test Epoch4 layer1 out_loss 0.2820530831813812, R2 0.7186304926872253\n",
      "Test Epoch4 layer2 out_loss 0.2752547264099121, R2 0.7254123687744141\n",
      "Test Epoch4 layer3 out_loss 0.27107614278793335, R2 0.729580819606781\n",
      "Test Epoch4 layer4 out_loss 0.2717556357383728, R2 0.7289029955863953\n",
      "Train 5 | out_loss 0.5144549012184143: 100%|█| 258/258 [00:01<00:00, 175.40it/s]\n",
      "Train Epoch5 out_loss 0.26466381549835205, R2 0.7351740598678589\n",
      "Test Epoch5 layer0 out_loss 0.2841125726699829, R2 0.7165759801864624\n",
      "Test Epoch5 layer1 out_loss 0.2699568569660187, R2 0.7306973934173584\n",
      "Test Epoch5 layer2 out_loss 0.2667420208454132, R2 0.7339044809341431\n",
      "Test Epoch5 layer3 out_loss 0.26813796162605286, R2 0.7325118780136108\n",
      "Test Epoch5 layer4 out_loss 0.27116623520851135, R2 0.7294909358024597\n",
      "Train 6 | out_loss 0.5106232762336731: 100%|█| 258/258 [00:01<00:00, 152.99it/s]\n",
      "Train Epoch6 out_loss 0.26073622703552246, R2 0.7391040325164795\n",
      "Test Epoch6 layer0 out_loss 0.28306716680526733, R2 0.7176188826560974\n",
      "Test Epoch6 layer1 out_loss 0.26580679416656494, R2 0.7348374128341675\n",
      "Test Epoch6 layer2 out_loss 0.2635367810726166, R2 0.7371019124984741\n",
      "Test Epoch6 layer3 out_loss 0.26641562581062317, R2 0.7342300415039062\n",
      "Test Epoch6 layer4 out_loss 0.2699366509914398, R2 0.7307175397872925\n",
      "Train 7 | out_loss 0.5066637992858887: 100%|█| 258/258 [00:01<00:00, 175.41it/s]\n",
      "Train Epoch7 out_loss 0.25670817494392395, R2 0.7431344985961914\n",
      "Test Epoch7 layer0 out_loss 0.28413206338882446, R2 0.7165565490722656\n",
      "Test Epoch7 layer1 out_loss 0.2680339217185974, R2 0.7326157093048096\n",
      "Test Epoch7 layer2 out_loss 0.2719613015651703, R2 0.7286977767944336\n",
      "Test Epoch7 layer3 out_loss 0.2746715843677521, R2 0.7259941101074219\n",
      "Test Epoch7 layer4 out_loss 0.2718660831451416, R2 0.7287927865982056\n",
      "Train 8 | out_loss 0.5083458423614502: 100%|█| 258/258 [00:01<00:00, 166.35it/s]\n",
      "Train Epoch8 out_loss 0.2584156095981598, R2 0.7414260506629944\n",
      "Test Epoch8 layer0 out_loss 0.2866936922073364, R2 0.7140011191368103\n",
      "Test Epoch8 layer1 out_loss 0.2666005492210388, R2 0.7340455651283264\n",
      "Test Epoch8 layer2 out_loss 0.2656494677066803, R2 0.7349943518638611\n",
      "Test Epoch8 layer3 out_loss 0.27076393365859985, R2 0.7298922538757324\n",
      "Test Epoch8 layer4 out_loss 0.27134087681770325, R2 0.7293167114257812\n",
      "Train 9 | out_loss 0.5001214742660522: 100%|█| 258/258 [00:01<00:00, 147.05it/s]\n",
      "Train Epoch9 out_loss 0.25012147426605225, R2 0.7497252225875854\n",
      "Test Epoch9 layer0 out_loss 0.289949506521225, R2 0.7107532024383545\n",
      "Test Epoch9 layer1 out_loss 0.26219138503074646, R2 0.7384440898895264\n",
      "Test Epoch9 layer2 out_loss 0.25649359822273254, R2 0.7441280484199524\n",
      "Test Epoch9 layer3 out_loss 0.25758013129234314, R2 0.7430441379547119\n",
      "Test Epoch9 layer4 out_loss 0.2627941370010376, R2 0.7378427982330322\n",
      "Train 10 | out_loss 0.5003877878189087: 100%|█| 258/258 [00:01<00:00, 161.82it/s\n",
      "Train Epoch10 out_loss 0.2503880262374878, R2 0.7494585514068604\n",
      "Test Epoch10 layer0 out_loss 0.2888861894607544, R2 0.7118139266967773\n",
      "Test Epoch10 layer1 out_loss 0.2610270082950592, R2 0.7396056056022644\n",
      "Test Epoch10 layer2 out_loss 0.25831276178359985, R2 0.7423132658004761\n",
      "Test Epoch10 layer3 out_loss 0.2571943700313568, R2 0.7434289455413818\n",
      "Test Epoch10 layer4 out_loss 0.2559095025062561, R2 0.7447106838226318\n",
      "Train 11 | out_loss 0.4977800250053406: 100%|█| 258/258 [00:01<00:00, 146.59it/s\n",
      "Train Epoch11 out_loss 0.24778491258621216, R2 0.7520632147789001\n",
      "Test Epoch11 layer0 out_loss 0.2801414430141449, R2 0.720537543296814\n",
      "Test Epoch11 layer1 out_loss 0.2591699957847595, R2 0.7414581179618835\n",
      "Test Epoch11 layer2 out_loss 0.25838109850883484, R2 0.742245078086853\n",
      "Test Epoch11 layer3 out_loss 0.25732487440109253, R2 0.7432987689971924\n",
      "Test Epoch11 layer4 out_loss 0.25879743695259094, R2 0.7418297529220581\n",
      "Train 12 | out_loss 0.496290922164917: 100%|█| 258/258 [00:01<00:00, 176.04it/s]\n",
      "Train Epoch12 out_loss 0.24630475044250488, R2 0.753544270992279\n",
      "Test Epoch12 layer0 out_loss 0.281938761472702, R2 0.7187445163726807\n",
      "Test Epoch12 layer1 out_loss 0.2608969807624817, R2 0.7397353053092957\n",
      "Test Epoch12 layer2 out_loss 0.2566634714603424, R2 0.743958592414856\n",
      "Test Epoch12 layer3 out_loss 0.2562694251537323, R2 0.7443516850471497\n",
      "Test Epoch12 layer4 out_loss 0.2567128837108612, R2 0.7439092397689819\n",
      "Train 13 | out_loss 0.4962654411792755: 100%|█| 258/258 [00:01<00:00, 154.56it/s\n",
      "Train Epoch13 out_loss 0.24627931416034698, R2 0.7535697221755981\n",
      "Test Epoch13 layer0 out_loss 0.28071510791778564, R2 0.7199652194976807\n",
      "Test Epoch13 layer1 out_loss 0.25598376989364624, R2 0.7446366548538208\n",
      "Test Epoch13 layer2 out_loss 0.2508990466594696, R2 0.7497090101242065\n",
      "Test Epoch13 layer3 out_loss 0.25197672843933105, R2 0.7486339807510376\n",
      "Test Epoch13 layer4 out_loss 0.27150073647499084, R2 0.7291572690010071\n",
      "Train 14 | out_loss 0.49373486638069153: 100%|█| 258/258 [00:01<00:00, 165.64it/\n",
      "Train Epoch14 out_loss 0.2437741607427597, R2 0.756076455116272\n",
      "Test Epoch14 layer0 out_loss 0.2738335132598877, R2 0.7268301248550415\n",
      "Test Epoch14 layer1 out_loss 0.2534683644771576, R2 0.74714595079422\n",
      "Test Epoch14 layer2 out_loss 0.25154173374176025, R2 0.7490679025650024\n",
      "Test Epoch14 layer3 out_loss 0.24820375442504883, R2 0.7523977756500244\n",
      "Test Epoch14 layer4 out_loss 0.24687518179416656, R2 0.75372314453125\n",
      "Train 15 | out_loss 0.48927903175354004: 100%|█| 258/258 [00:01<00:00, 176.88it/\n",
      "Train Epoch15 out_loss 0.23939402401447296, R2 0.760459303855896\n",
      "Test Epoch15 layer0 out_loss 0.2764398753643036, R2 0.7242301106452942\n",
      "Test Epoch15 layer1 out_loss 0.2508278787136078, R2 0.7497800588607788\n",
      "Test Epoch15 layer2 out_loss 0.24840259552001953, R2 0.7521994113922119\n",
      "Test Epoch15 layer3 out_loss 0.24546971917152405, R2 0.7551251649856567\n",
      "Test Epoch15 layer4 out_loss 0.24358844757080078, R2 0.7570018768310547\n",
      "Train 16 | out_loss 0.48713061213493347: 100%|█| 258/258 [00:01<00:00, 166.92it/\n",
      "Train Epoch16 out_loss 0.2372962236404419, R2 0.7625583410263062\n",
      "Test Epoch16 layer0 out_loss 0.2804107367992401, R2 0.7202688455581665\n",
      "Test Epoch16 layer1 out_loss 0.25408750772476196, R2 0.7465282678604126\n",
      "Test Epoch16 layer2 out_loss 0.2508315443992615, R2 0.7497763633728027\n",
      "Test Epoch16 layer3 out_loss 0.24772091209888458, R2 0.7528794407844543\n",
      "Test Epoch16 layer4 out_loss 0.2470943182706833, R2 0.7535045146942139\n",
      "Train 17 | out_loss 0.4851473569869995: 100%|█| 258/258 [00:01<00:00, 178.49it/s\n",
      "Train Epoch17 out_loss 0.2353680580854416, R2 0.7644876837730408\n",
      "Test Epoch17 layer0 out_loss 0.2770459055900574, R2 0.7236255407333374\n",
      "Test Epoch17 layer1 out_loss 0.2504020929336548, R2 0.7502048015594482\n",
      "Test Epoch17 layer2 out_loss 0.2445363998413086, R2 0.7560562491416931\n",
      "Test Epoch17 layer3 out_loss 0.2492554485797882, R2 0.751348614692688\n",
      "Test Epoch17 layer4 out_loss 0.2534373104572296, R2 0.7471768856048584\n",
      "Train 18 | out_loss 0.481009840965271: 100%|█| 258/258 [00:01<00:00, 155.31it/s]\n",
      "Train Epoch18 out_loss 0.2313704937696457, R2 0.7684876918792725\n",
      "Test Epoch18 layer0 out_loss 0.28301364183425903, R2 0.7176722288131714\n",
      "Test Epoch18 layer1 out_loss 0.24859532713890076, R2 0.7520071268081665\n",
      "Test Epoch18 layer2 out_loss 0.24032200872898102, R2 0.7602604627609253\n",
      "Test Epoch18 layer3 out_loss 0.23596444725990295, R2 0.7646074295043945\n",
      "Test Epoch18 layer4 out_loss 0.23687759041786194, R2 0.7636964917182922\n",
      "Train 19 | out_loss 0.48041293025016785: 100%|█| 258/258 [00:01<00:00, 148.97it/\n",
      "Train Epoch19 out_loss 0.23079665005207062, R2 0.7690619230270386\n",
      "Test Epoch19 layer0 out_loss 0.27702978253364563, R2 0.7236416339874268\n",
      "Test Epoch19 layer1 out_loss 0.24301615357398987, R2 0.7575728297233582\n",
      "Test Epoch19 layer2 out_loss 0.24136421084403992, R2 0.7592207789421082\n",
      "Test Epoch19 layer3 out_loss 0.23776639997959137, R2 0.7628098726272583\n",
      "Test Epoch19 layer4 out_loss 0.2407263219356537, R2 0.7598570585250854\n",
      "Train 20 | out_loss 0.47789788246154785: 100%|█| 258/258 [00:01<00:00, 162.81it/\n",
      "Train Epoch20 out_loss 0.22838625311851501, R2 0.77147376537323\n",
      "Test Epoch20 layer0 out_loss 0.27481433749198914, R2 0.7258516550064087\n",
      "Test Epoch20 layer1 out_loss 0.24123571813106537, R2 0.7593489289283752\n",
      "Test Epoch20 layer2 out_loss 0.24006755650043488, R2 0.7605142593383789\n",
      "Test Epoch20 layer3 out_loss 0.23810356855392456, R2 0.762473464012146\n",
      "Test Epoch20 layer4 out_loss 0.23625649511814117, R2 0.7643160820007324\n",
      "Train 21 | out_loss 0.47706368565559387:  24%|▏| 62/258 [00:00<00:01, 153.82it/s^C\n",
      "Train 21 | out_loss 0.47572827339172363:  26%|▎| 67/258 [00:00<00:01, 148.52it/s\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/AL_main_new/dis_train_al.py\", line 480, in <module>\n",
      "    main()\n",
      "  File \"/home/AL_main_new/dis_train_al.py\", line 453, in main\n",
      "    train(model, train_loader, epoch, task=\"regression\")\n",
      "  File \"/home/AL_main_new/dis_train_al.py\", line 332, in train\n",
      "    data_loader.set_description(f'Train {epoch} | out_loss {torch.sqrt(out_loss/num)}')\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/tqdm/std.py\", line 1408, in set_description\n",
      "    self.refresh()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/tqdm/std.py\", line 1361, in refresh\n",
      "    self.display()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/tqdm/std.py\", line 1509, in display\n",
      "    self.sp(self.__str__() if msg is None else msg)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/tqdm/std.py\", line 1165, in __str__\n",
      "    return self.format_meter(**self.format_dict)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/tqdm/std.py\", line 485, in format_meter\n",
      "    format_dict = dict(\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# LinearAL \n",
    "\n",
    "data = \"ca_housing\"\n",
    "#data = \"paint\"\n",
    "model =  \"linearal\"\n",
    "#for layer in range(1,11):\n",
    "for layer in [5]:\n",
    "    log = f\"result/{data}_{model}_l{layer}.log\"\n",
    "    !python3 dis_train_al.py --dataset {data} --model {model} --epoch 200 --num-layer {layer} --lr 0.001 --task regression # > {log}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ae70865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training\n",
      "  0%|                                                   | 0/125 [00:00<?, ?it/s]/home/AL_main_new/utils.py:99: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  x = torch.tensor([x for x,y in batch], dtype=torch.float32)\n",
      "Train 0 | Acc 0.637 (5096/8000): 100%|████████| 125/125 [00:01<00:00, 71.50it/s]\n",
      "[[ 57.02572745 117.37733167]\n",
      " [ 11.3576537  100.7301451 ]\n",
      " [  4.08579389 101.68638885]\n",
      " [  1.71285066  91.91430813]\n",
      " [  0.9954262   69.36403099]\n",
      " [  0.63190512  30.1725354 ]\n",
      " [  0.56684489  10.27649009]\n",
      " [  0.5206641    2.858054  ]\n",
      " [  0.5829535    0.94288965]\n",
      " [  0.449379     0.36212885]]\n",
      "Train Epoch0 Acc 0.637 (5096/8000)\n",
      "Test Epoch0 layer0 Acc 0.7525\n",
      "Test Epoch0 layer1 Acc 0.778\n",
      "Test Epoch0 layer2 Acc 0.778\n",
      "Test Epoch0 layer3 Acc 0.778\n",
      "Test Epoch0 layer4 Acc 0.778\n",
      "Test Epoch0 layer5 Acc 0.778\n",
      "Test Epoch0 layer6 Acc 0.778\n",
      "Test Epoch0 layer7 Acc 0.778\n",
      "Test Epoch0 layer8 Acc 0.778\n",
      "Test Epoch0 layer9 Acc 0.778\n",
      "Train 1 | Acc 0.78275 (6262/8000): 100%|██████| 125/125 [00:01<00:00, 89.29it/s]\n",
      "[[38.18602593 84.77744555]\n",
      " [ 0.3706391  75.93106765]\n",
      " [ 0.28641708 81.27746147]\n",
      " [ 0.24247164 70.80641663]\n",
      " [ 0.31970693 39.45152688]\n",
      " [ 0.33647256  7.29741142]\n",
      " [ 0.33546512  1.93493958]\n",
      " [ 0.44716654  0.79313688]\n",
      " [ 0.28667888  0.31597156]\n",
      " [ 0.12021199  0.15097512]]\n",
      "Train Epoch1 Acc 0.78275 (6262/8000)\n",
      "Test Epoch1 layer0 Acc 0.778\n",
      "Test Epoch1 layer1 Acc 0.778\n",
      "Test Epoch1 layer2 Acc 0.778\n",
      "Test Epoch1 layer3 Acc 0.778\n",
      "Test Epoch1 layer4 Acc 0.778\n",
      "Test Epoch1 layer5 Acc 0.778\n",
      "Test Epoch1 layer6 Acc 0.778\n",
      "Test Epoch1 layer7 Acc 0.778\n",
      "Test Epoch1 layer8 Acc 0.778\n",
      "Test Epoch1 layer9 Acc 0.778\n",
      "Train 2 | Acc 0.7754934210526315 (943/1216):  10%| | 12/125 [00:00<00:00, 115.58^C\n",
      "Train 2 | Acc 0.7754934210526315 (943/1216):  15%|▏| 19/125 [00:00<00:00, 110.71\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/AL_main_new/dis_train_al.py\", line 481, in <module>\n",
      "    main()\n",
      "  File \"/home/AL_main_new/dis_train_al.py\", line 435, in main\n",
      "    train(model, train_loader, epoch, task=args.task)\n",
      "  File \"/home/AL_main_new/dis_train_al.py\", line 294, in train\n",
      "    losses = model(x, y)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/AL_main_new/distributed_model.py\", line 696, in forward\n",
      "    x_out, y_out, ae_out, as_out = layer(x_out, y_out)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/AL_main_new/distributed_model.py\", line 272, in forward\n",
      "    tgt = enc_y.clone().detach()\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# LinearAL \n",
    "\n",
    "data = \"criteo\"\n",
    "\n",
    "model =  \"linearal\"\n",
    "#for layer in range(1,11):\n",
    "for layer in [10]:\n",
    "    log = f\"result/{data}_{model}_l{layer}.log\"\n",
    "    !python3 dis_train_al.py --dataset {data} --model {model} --epoch 100 --num-layer {layer} --task classification --lr 0.001  # > {log}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "833b414e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8800,)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target.to_numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7675b11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]/home/AL_main_new/utils.py:99: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  x = torch.tensor([x for x,y in batch], dtype=torch.float32)\n",
      "Train 0 | out_loss 0.9701134562492371: 100%|██████| 8/8 [00:00<00:00, 19.52it/s]\n",
      "Train Epoch0 out_loss 0.9411200881004333, R2 -0.0022379159927368164\n",
      "Test Epoch0 layer0 out_loss 0.9687603712081909, R2 0.21278250217437744\n",
      "Test Epoch0 layer1 out_loss 1.110060214996338, R2 0.0979619026184082\n",
      "Test Epoch0 layer2 out_loss 1.1896744966506958, R2 0.03326714038848877\n",
      "Test Epoch0 layer3 out_loss 1.235206127166748, R2 -0.0037320852279663086\n",
      "Test Epoch0 layer4 out_loss 1.241876244544983, R2 -0.00915217399597168\n",
      "Train 1 | out_loss 0.972903847694397: 100%|██████| 8/8 [00:00<00:00, 254.18it/s]\n",
      "Train Epoch1 out_loss 0.9465418457984924, R2 -0.008011698722839355\n",
      "Test Epoch1 layer0 out_loss 0.8677021861076355, R2 0.2949026823043823\n",
      "Test Epoch1 layer1 out_loss 0.9176527857780457, R2 0.2543126940727234\n",
      "Test Epoch1 layer2 out_loss 1.0184664726257324, R2 0.17239117622375488\n",
      "Test Epoch1 layer3 out_loss 1.1492228507995605, R2 0.06613814830780029\n",
      "Test Epoch1 layer4 out_loss 1.3024436235427856, R2 -0.05836939811706543\n",
      "Train 2 | out_loss 0.8895187973976135: 100%|█████| 8/8 [00:00<00:00, 249.11it/s]\n",
      "Train Epoch2 out_loss 0.7912437319755554, R2 0.15737169981002808\n",
      "Test Epoch2 layer0 out_loss 0.7841998338699341, R2 0.36275696754455566\n",
      "Test Epoch2 layer1 out_loss 0.861016035079956, R2 0.3003358840942383\n",
      "Test Epoch2 layer2 out_loss 0.9267027974128723, R2 0.2469586730003357\n",
      "Test Epoch2 layer3 out_loss 0.8698157668113708, R2 0.29318517446517944\n",
      "Test Epoch2 layer4 out_loss 0.9736538529396057, R2 0.2088060975074768\n",
      "Train 3 | out_loss 0.7842167615890503: 100%|█████| 8/8 [00:00<00:00, 247.07it/s]\n",
      "Train Epoch3 out_loss 0.6149959564208984, R2 0.34506529569625854\n",
      "Test Epoch3 layer0 out_loss 0.73554527759552, R2 0.40229374170303345\n",
      "Test Epoch3 layer1 out_loss 0.733462929725647, R2 0.4039859175682068\n",
      "Test Epoch3 layer2 out_loss 0.7776775360107422, R2 0.3680570125579834\n",
      "Test Epoch3 layer3 out_loss 0.7538282871246338, R2 0.38743698596954346\n",
      "Test Epoch3 layer4 out_loss 0.9272353649139404, R2 0.24652588367462158\n",
      "Train 4 | out_loss 0.7878190875053406: 100%|█████| 8/8 [00:00<00:00, 246.80it/s]\n",
      "Train Epoch4 out_loss 0.6206589341163635, R2 0.3390344977378845\n",
      "Test Epoch4 layer0 out_loss 0.7422552704811096, R2 0.39684122800827026\n",
      "Test Epoch4 layer1 out_loss 0.8176336884498596, R2 0.3355885148048401\n",
      "Test Epoch4 layer2 out_loss 0.8237403035163879, R2 0.3306262493133545\n",
      "Test Epoch4 layer3 out_loss 0.8121611475944519, R2 0.3400355577468872\n",
      "Test Epoch4 layer4 out_loss 0.7997884154319763, R2 0.3500896096229553\n",
      "Train 5 | out_loss 0.797677218914032: 100%|██████| 8/8 [00:00<00:00, 251.93it/s]\n",
      "Train Epoch5 out_loss 0.6362889409065247, R2 0.32238948345184326\n",
      "Test Epoch5 layer0 out_loss 0.6986374855041504, R2 0.4322851896286011\n",
      "Test Epoch5 layer1 out_loss 0.7015277743339539, R2 0.4299365282058716\n",
      "Test Epoch5 layer2 out_loss 0.7242419719696045, R2 0.4114788770675659\n",
      "Test Epoch5 layer3 out_loss 0.7675999402999878, R2 0.3762460947036743\n",
      "Test Epoch5 layer4 out_loss 0.8687397837638855, R2 0.29405951499938965\n",
      "Train 6 | out_loss 0.7534722685813904: 100%|█████| 8/8 [00:00<00:00, 250.43it/s]\n",
      "Train Epoch6 out_loss 0.5677205324172974, R2 0.39541077613830566\n",
      "Test Epoch6 layer0 out_loss 0.6811466813087463, R2 0.44649821519851685\n",
      "Test Epoch6 layer1 out_loss 0.6987101435661316, R2 0.43222612142562866\n",
      "Test Epoch6 layer2 out_loss 0.7452362775802612, R2 0.3944188952445984\n",
      "Test Epoch6 layer3 out_loss 0.7525142431259155, R2 0.3885048031806946\n",
      "Test Epoch6 layer4 out_loss 0.798965573310852, R2 0.35075825452804565\n",
      "Train 7 | out_loss 0.6660410761833191: 100%|█████| 8/8 [00:00<00:00, 249.48it/s]\n",
      "Train Epoch7 out_loss 0.4436107277870178, R2 0.5275804400444031\n",
      "Test Epoch7 layer0 out_loss 0.6649014353752136, R2 0.4596991539001465\n",
      "Test Epoch7 layer1 out_loss 0.7013137340545654, R2 0.43011045455932617\n",
      "Test Epoch7 layer2 out_loss 0.708541989326477, R2 0.4242367148399353\n",
      "Test Epoch7 layer3 out_loss 0.7288714647293091, R2 0.4077169895172119\n",
      "Test Epoch7 layer4 out_loss 0.7354909181594849, R2 0.4023379683494568\n",
      "Train 8 | out_loss 0.6679121255874634: 100%|█████| 8/8 [00:00<00:00, 249.47it/s]\n",
      "Train Epoch8 out_loss 0.4461066424846649, R2 0.5249224305152893\n",
      "Test Epoch8 layer0 out_loss 0.6567820906639099, R2 0.4662969708442688\n",
      "Test Epoch8 layer1 out_loss 0.6683642268180847, R2 0.45688533782958984\n",
      "Test Epoch8 layer2 out_loss 0.6705496907234192, R2 0.4551093578338623\n",
      "Test Epoch8 layer3 out_loss 0.6699007749557495, R2 0.4556366801261902\n",
      "Test Epoch8 layer4 out_loss 0.6824741959571838, R2 0.4454194903373718\n",
      "Train 9 | out_loss 0.6808974146842957: 100%|█████| 8/8 [00:00<00:00, 234.52it/s]\n",
      "Train Epoch9 out_loss 0.4636213779449463, R2 0.5062702894210815\n",
      "Test Epoch9 layer0 out_loss 0.6417856812477112, R2 0.47848308086395264\n",
      "Test Epoch9 layer1 out_loss 0.6370231509208679, R2 0.482353150844574\n",
      "Test Epoch9 layer2 out_loss 0.6776067018508911, R2 0.44937485456466675\n",
      "Test Epoch9 layer3 out_loss 0.7397058010101318, R2 0.3989129662513733\n",
      "Test Epoch9 layer4 out_loss 0.7674000263214111, R2 0.37640851736068726\n",
      "Train 10 | out_loss 0.6601459383964539: 100%|████| 8/8 [00:00<00:00, 244.00it/s]\n",
      "Train Epoch10 out_loss 0.4357927143573761, R2 0.535906195640564\n",
      "Test Epoch10 layer0 out_loss 0.6509984135627747, R2 0.4709968566894531\n",
      "Test Epoch10 layer1 out_loss 0.6618564128875732, R2 0.46217358112335205\n",
      "Test Epoch10 layer2 out_loss 0.6684719920158386, R2 0.45679771900177\n",
      "Test Epoch10 layer3 out_loss 0.6592771410942078, R2 0.4642695188522339\n",
      "Test Epoch10 layer4 out_loss 0.6658968329429626, R2 0.45889031887054443\n",
      "Train 11 | out_loss 0.6308413743972778: 100%|████| 8/8 [00:00<00:00, 255.40it/s]\n",
      "Train Epoch11 out_loss 0.3979608714580536, R2 0.5761949419975281\n",
      "Test Epoch11 layer0 out_loss 0.6128794550895691, R2 0.5019723773002625\n",
      "Test Epoch11 layer1 out_loss 0.6141911149024963, R2 0.5009065270423889\n",
      "Test Epoch11 layer2 out_loss 0.6308643221855164, R2 0.48735785484313965\n",
      "Test Epoch11 layer3 out_loss 0.6411293745040894, R2 0.47901642322540283\n",
      "Test Epoch11 layer4 out_loss 0.6471132040023804, R2 0.47415393590927124\n",
      "Train 12 | out_loss 0.6220070123672485: 100%|████| 8/8 [00:00<00:00, 247.98it/s]\n",
      "Train Epoch12 out_loss 0.3868926465511322, R2 0.5879819393157959\n",
      "Test Epoch12 layer0 out_loss 0.6236104369163513, R2 0.49325239658355713\n",
      "Test Epoch12 layer1 out_loss 0.6333191990852356, R2 0.4853629469871521\n",
      "Test Epoch12 layer2 out_loss 0.6473237872123718, R2 0.4739828109741211\n",
      "Test Epoch12 layer3 out_loss 0.6376486420631409, R2 0.4818449020385742\n",
      "Test Epoch12 layer4 out_loss 0.6462647318840027, R2 0.4748433828353882\n",
      "Train 13 | out_loss 0.6101108193397522: 100%|████| 8/8 [00:00<00:00, 251.22it/s]\n",
      "Train Epoch13 out_loss 0.37223514914512634, R2 0.6035913228988647\n",
      "Test Epoch13 layer0 out_loss 0.6125591397285461, R2 0.5022326707839966\n",
      "Test Epoch13 layer1 out_loss 0.6125264167785645, R2 0.5022592544555664\n",
      "Test Epoch13 layer2 out_loss 0.6244057416915894, R2 0.49260610342025757\n",
      "Test Epoch13 layer3 out_loss 0.6176249384880066, R2 0.4981161952018738\n",
      "Test Epoch13 layer4 out_loss 0.620378851890564, R2 0.49587833881378174\n",
      "Train 14 | out_loss 0.6259228587150574: 100%|████| 8/8 [00:00<00:00, 243.71it/s]\n",
      "Train Epoch14 out_loss 0.39177942276000977, R2 0.5827778577804565\n",
      "Test Epoch14 layer0 out_loss 0.6063855886459351, R2 0.5072493553161621\n",
      "Test Epoch14 layer1 out_loss 0.5951651334762573, R2 0.5163670778274536\n",
      "Test Epoch14 layer2 out_loss 0.6046092510223389, R2 0.508692741394043\n",
      "Test Epoch14 layer3 out_loss 0.6029245853424072, R2 0.5100617408752441\n",
      "Test Epoch14 layer4 out_loss 0.6116519570350647, R2 0.5029698610305786\n",
      "Train 15 | out_loss 0.6064721941947937: 100%|████| 8/8 [00:00<00:00, 249.79it/s]\n",
      "Train Epoch15 out_loss 0.3678085207939148, R2 0.6083054542541504\n",
      "Test Epoch15 layer0 out_loss 0.5853222608566284, R2 0.5243654251098633\n",
      "Test Epoch15 layer1 out_loss 0.5785696506500244, R2 0.5298526287078857\n",
      "Test Epoch15 layer2 out_loss 0.6142783164978027, R2 0.500835657119751\n",
      "Test Epoch15 layer3 out_loss 0.6527049541473389, R2 0.4696100354194641\n",
      "Test Epoch15 layer4 out_loss 0.6819384694099426, R2 0.4458548426628113\n",
      "Train 16 | out_loss 0.6251310110092163: 100%|████| 8/8 [00:00<00:00, 251.53it/s]\n",
      "Train Epoch16 out_loss 0.3907887935638428, R2 0.5838327407836914\n",
      "Test Epoch16 layer0 out_loss 0.6218141317367554, R2 0.49471205472946167\n",
      "Test Epoch16 layer1 out_loss 0.6494286060333252, R2 0.47227245569229126\n",
      "Test Epoch16 layer2 out_loss 0.6526685357093811, R2 0.46963971853256226\n",
      "Test Epoch16 layer3 out_loss 0.6419246792793274, R2 0.47837013006210327\n",
      "Test Epoch16 layer4 out_loss 0.6496299505233765, R2 0.4721088409423828\n",
      "Train 17 | out_loss 0.6366280913352966: 100%|████| 8/8 [00:00<00:00, 247.47it/s]\n",
      "Train Epoch17 out_loss 0.4052952826023102, R2 0.5683842301368713\n",
      "Test Epoch17 layer0 out_loss 0.5866822600364685, R2 0.5232603549957275\n",
      "Test Epoch17 layer1 out_loss 0.5808373689651489, R2 0.5280098915100098\n",
      "Test Epoch17 layer2 out_loss 0.5901032090187073, R2 0.5204803943634033\n",
      "Test Epoch17 layer3 out_loss 0.6126888394355774, R2 0.5021272897720337\n",
      "Test Epoch17 layer4 out_loss 0.6302434206008911, R2 0.48786234855651855\n",
      "Train 18 | out_loss 0.6323659420013428: 100%|████| 8/8 [00:00<00:00, 250.25it/s]\n",
      "Train Epoch18 out_loss 0.3998866379261017, R2 0.5741441249847412\n",
      "Test Epoch18 layer0 out_loss 0.5702294707298279, R2 0.5366299152374268\n",
      "Test Epoch18 layer1 out_loss 0.5632416009902954, R2 0.5423082113265991\n",
      "Test Epoch18 layer2 out_loss 0.580821692943573, R2 0.5280225872993469\n",
      "Test Epoch18 layer3 out_loss 0.584979772567749, R2 0.5246437191963196\n",
      "Test Epoch18 layer4 out_loss 0.5934485793113708, R2 0.5177619457244873\n",
      "Train 19 | out_loss 0.6031514406204224: 100%|████| 8/8 [00:00<00:00, 248.53it/s]\n",
      "Train Epoch19 out_loss 0.36379164457321167, R2 0.6125831604003906\n",
      "Test Epoch19 layer0 out_loss 0.5766785144805908, R2 0.5313893556594849\n",
      "Test Epoch19 layer1 out_loss 0.5771650671958923, R2 0.5309939980506897\n",
      "Test Epoch19 layer2 out_loss 0.576928436756134, R2 0.5311862826347351\n",
      "Test Epoch19 layer3 out_loss 0.574415385723114, R2 0.5332283973693848\n",
      "Test Epoch19 layer4 out_loss 0.5825766921043396, R2 0.5265964865684509\n",
      "Train 20 | out_loss 0.5929718613624573: 100%|████| 8/8 [00:00<00:00, 256.09it/s]\n",
      "Train Epoch20 out_loss 0.35161563754081726, R2 0.6255499124526978\n",
      "Test Epoch20 layer0 out_loss 0.5798511505126953, R2 0.5288112759590149\n",
      "Test Epoch20 layer1 out_loss 0.5838462114334106, R2 0.5255649089813232\n",
      "Test Epoch20 layer2 out_loss 0.6147010326385498, R2 0.5004921555519104\n",
      "Test Epoch20 layer3 out_loss 0.6357779502868652, R2 0.483364999294281\n",
      "Test Epoch20 layer4 out_loss 0.6607387065887451, R2 0.46308183670043945\n",
      "Train 21 | out_loss 0.5844488143920898: 100%|████| 8/8 [00:00<00:00, 249.88it/s]\n",
      "Train Epoch21 out_loss 0.34158042073249817, R2 0.6362367868423462\n",
      "Test Epoch21 layer0 out_loss 0.563025712966919, R2 0.5424836874008179\n",
      "Test Epoch21 layer1 out_loss 0.5634332299232483, R2 0.5421525239944458\n",
      "Test Epoch21 layer2 out_loss 0.5770412683486938, R2 0.5310946106910706\n",
      "Test Epoch21 layer3 out_loss 0.5861608982086182, R2 0.5236839652061462\n",
      "Test Epoch21 layer4 out_loss 0.5924817323684692, R2 0.5185476541519165\n",
      "Train 22 | out_loss 0.5845990777015686: 100%|████| 8/8 [00:00<00:00, 252.88it/s]\n",
      "Train Epoch22 out_loss 0.34175610542297363, R2 0.6360496282577515\n",
      "Test Epoch22 layer0 out_loss 0.5699133276939392, R2 0.5368868112564087\n",
      "Test Epoch22 layer1 out_loss 0.5786049962043762, R2 0.529823899269104\n",
      "Test Epoch22 layer2 out_loss 0.5903821587562561, R2 0.5202537775039673\n",
      "Test Epoch22 layer3 out_loss 0.5994928479194641, R2 0.5128504037857056\n",
      "Test Epoch22 layer4 out_loss 0.6260287761688232, R2 0.4912872314453125\n",
      "Train 23 | out_loss 0.6087679862976074: 100%|████| 8/8 [00:00<00:00, 255.07it/s]\n",
      "Train Epoch23 out_loss 0.3705984652042389, R2 0.6053342819213867\n",
      "Test Epoch23 layer0 out_loss 0.5954375267028809, R2 0.5161457061767578\n",
      "Test Epoch23 layer1 out_loss 0.5992424488067627, R2 0.5130538940429688\n",
      "Test Epoch23 layer2 out_loss 0.6069863438606262, R2 0.5067611932754517\n",
      "Test Epoch23 layer3 out_loss 0.6084955334663391, R2 0.5055347681045532\n",
      "Test Epoch23 layer4 out_loss 0.6221553087234497, R2 0.49443477392196655\n",
      "Train 24 | out_loss 0.5879083275794983: 100%|████| 8/8 [00:00<00:00, 257.06it/s]\n",
      "Train Epoch24 out_loss 0.3456362187862396, R2 0.6319175958633423\n",
      "Test Epoch24 layer0 out_loss 0.5458434820175171, R2 0.5564460158348083\n",
      "Test Epoch24 layer1 out_loss 0.5465563535690308, R2 0.5558667182922363\n",
      "Test Epoch24 layer2 out_loss 0.5578482747077942, R2 0.5466908812522888\n",
      "Test Epoch24 layer3 out_loss 0.5705868005752563, R2 0.536339521408081\n",
      "Test Epoch24 layer4 out_loss 0.5802462697029114, R2 0.5284901857376099\n",
      "Train 25 | out_loss 0.5746726393699646: 100%|████| 8/8 [00:00<00:00, 251.40it/s]\n",
      "Train Epoch25 out_loss 0.3302486538887024, R2 0.6483044624328613\n",
      "Test Epoch25 layer0 out_loss 0.5705400109291077, R2 0.5363775491714478\n",
      "Test Epoch25 layer1 out_loss 0.5777856111526489, R2 0.5304898023605347\n",
      "Test Epoch25 layer2 out_loss 0.5846620798110962, R2 0.5249019265174866\n",
      "Test Epoch25 layer3 out_loss 0.5754093527793884, R2 0.5324206948280334\n",
      "Test Epoch25 layer4 out_loss 0.5757155418395996, R2 0.5321718454360962\n",
      "Train 26 | out_loss 0.5765363574028015: 100%|████| 8/8 [00:00<00:00, 257.39it/s]\n",
      "Train Epoch26 out_loss 0.3323941230773926, R2 0.646019697189331\n",
      "Test Epoch26 layer0 out_loss 0.598010241985321, R2 0.5140551328659058\n",
      "Test Epoch26 layer1 out_loss 0.6055669188499451, R2 0.5079145431518555\n",
      "Test Epoch26 layer2 out_loss 0.6362646222114563, R2 0.4829695224761963\n",
      "Test Epoch26 layer3 out_loss 0.6825782656669617, R2 0.44533491134643555\n",
      "Test Epoch26 layer4 out_loss 0.7070919871330261, R2 0.4254150390625\n",
      "Train 27 | out_loss 0.6008912324905396: 100%|████| 8/8 [00:00<00:00, 260.40it/s]\n",
      "Train Epoch27 out_loss 0.36107027530670166, R2 0.6154812574386597\n",
      "Test Epoch27 layer0 out_loss 0.5523651242256165, R2 0.5511465072631836\n",
      "Test Epoch27 layer1 out_loss 0.5563855171203613, R2 0.5478795766830444\n",
      "Test Epoch27 layer2 out_loss 0.5694318413734436, R2 0.5372780561447144\n",
      "Test Epoch27 layer3 out_loss 0.5875198841094971, R2 0.5225796699523926\n",
      "Test Epoch27 layer4 out_loss 0.6105621457099915, R2 0.5038554668426514\n",
      "Train 28 | out_loss 0.6069433689117432: 100%|████| 8/8 [00:00<00:00, 252.01it/s]\n",
      "Train Epoch28 out_loss 0.3683803081512451, R2 0.6076964735984802\n",
      "Test Epoch28 layer0 out_loss 0.5900580883026123, R2 0.520517110824585\n",
      "Test Epoch28 layer1 out_loss 0.6067124009132385, R2 0.506983757019043\n",
      "Test Epoch28 layer2 out_loss 0.6205655336380005, R2 0.49572664499282837\n",
      "Test Epoch28 layer3 out_loss 0.6223663687705994, R2 0.49426329135894775\n",
      "Test Epoch28 layer4 out_loss 0.6305329203605652, R2 0.48762714862823486\n",
      "Train 29 | out_loss 0.5905364155769348: 100%|████| 8/8 [00:00<00:00, 258.11it/s]\n",
      "Train Epoch29 out_loss 0.34873324632644653, R2 0.6286194324493408\n",
      "Test Epoch29 layer0 out_loss 0.5591012835502625, R2 0.5456727147102356\n",
      "Test Epoch29 layer1 out_loss 0.5563212633132935, R2 0.5479317307472229\n",
      "Test Epoch29 layer2 out_loss 0.5645574331283569, R2 0.5412390232086182\n",
      "Test Epoch29 layer3 out_loss 0.5812878012657166, R2 0.5276439189910889\n",
      "Test Epoch29 layer4 out_loss 0.6050385236740112, R2 0.5083439350128174\n",
      "Train 30 | out_loss 0.5772660374641418: 100%|████| 8/8 [00:00<00:00, 255.25it/s]\n",
      "Train Epoch30 out_loss 0.33323609828948975, R2 0.6451230645179749\n",
      "Test Epoch30 layer0 out_loss 0.5547989010810852, R2 0.5491688251495361\n",
      "Test Epoch30 layer1 out_loss 0.5676788091659546, R2 0.5387026071548462\n",
      "Test Epoch30 layer2 out_loss 0.5756546258926392, R2 0.5322213768959045\n",
      "Test Epoch30 layer3 out_loss 0.5683580040931702, R2 0.5381506681442261\n",
      "Test Epoch30 layer4 out_loss 0.5694477558135986, R2 0.5372651219367981\n",
      "Train 31 | out_loss 0.5606815814971924: 100%|████| 8/8 [00:00<00:00, 257.77it/s]\n",
      "Train Epoch31 out_loss 0.3143637776374817, R2 0.6652209758758545\n",
      "Test Epoch31 layer0 out_loss 0.5596997141838074, R2 0.5451864004135132\n",
      "Test Epoch31 layer1 out_loss 0.5663103461265564, R2 0.5398145914077759\n",
      "Test Epoch31 layer2 out_loss 0.5607214570045471, R2 0.544356107711792\n",
      "Test Epoch31 layer3 out_loss 0.55595862865448, R2 0.5482264757156372\n",
      "Test Epoch31 layer4 out_loss 0.5602210760116577, R2 0.5447627305984497\n",
      "Train 32 | out_loss 0.5763894319534302: 100%|████| 8/8 [00:00<00:00, 247.26it/s]\n",
      "Train Epoch32 out_loss 0.3322247862815857, R2 0.6462000608444214\n",
      "Test Epoch32 layer0 out_loss 0.5716522932052612, R2 0.5354737043380737\n",
      "Test Epoch32 layer1 out_loss 0.5790653228759766, R2 0.5294498205184937\n",
      "Test Epoch32 layer2 out_loss 0.6138675212860107, R2 0.5011695027351379\n",
      "Test Epoch32 layer3 out_loss 0.6521100401878357, R2 0.4700934886932373\n",
      "Test Epoch32 layer4 out_loss 0.6879695653915405, R2 0.44095396995544434\n",
      "Train 33 | out_loss 0.5771883130073547: 100%|████| 8/8 [00:00<00:00, 243.25it/s]\n",
      "Train Epoch33 out_loss 0.333146333694458, R2 0.6452186107635498\n",
      "Test Epoch33 layer0 out_loss 0.5506181120872498, R2 0.5525661706924438\n",
      "Test Epoch33 layer1 out_loss 0.5665313005447388, R2 0.5396350026130676\n",
      "Test Epoch33 layer2 out_loss 0.5581836700439453, R2 0.546418309211731\n",
      "Test Epoch33 layer3 out_loss 0.5492438077926636, R2 0.5536828637123108\n",
      "Test Epoch33 layer4 out_loss 0.5508599877357483, R2 0.5523695945739746\n",
      "Train 34 | out_loss 0.5686757564544678: 100%|████| 8/8 [00:00<00:00, 250.82it/s]\n",
      "Train Epoch34 out_loss 0.3233921229839325, R2 0.6556062698364258\n",
      "Test Epoch34 layer0 out_loss 0.5954925417900085, R2 0.5161010026931763\n",
      "Test Epoch34 layer1 out_loss 0.6075387597084045, R2 0.5063122510910034\n",
      "Test Epoch34 layer2 out_loss 0.6184870600700378, R2 0.49741560220718384\n",
      "Test Epoch34 layer3 out_loss 0.6491151452064514, R2 0.4725271463394165\n",
      "Test Epoch34 layer4 out_loss 0.6890992522239685, R2 0.44003599882125854\n",
      "Train 35 | out_loss 0.598233699798584: 100%|█████| 8/8 [00:00<00:00, 253.10it/s]\n",
      "Train Epoch35 out_loss 0.3578835129737854, R2 0.6188749074935913\n",
      "Test Epoch35 layer0 out_loss 0.5609386563301086, R2 0.5441796183586121\n",
      "Test Epoch35 layer1 out_loss 0.5680195689201355, R2 0.5384256839752197\n",
      "Test Epoch35 layer2 out_loss 0.5807534456253052, R2 0.5280780792236328\n",
      "Test Epoch35 layer3 out_loss 0.5907019972801208, R2 0.5199938416481018\n",
      "Test Epoch35 layer4 out_loss 0.591719925403595, R2 0.5191667079925537\n",
      "Train 36 | out_loss 0.5772784948348999: 100%|████| 8/8 [00:00<00:00, 255.84it/s]\n",
      "Train Epoch36 out_loss 0.3332504630088806, R2 0.6451077461242676\n",
      "Test Epoch36 layer0 out_loss 0.5612526535987854, R2 0.5439245104789734\n",
      "Test Epoch36 layer1 out_loss 0.5701416730880737, R2 0.5367012619972229\n",
      "Test Epoch36 layer2 out_loss 0.5801540017127991, R2 0.5285651683807373\n",
      "Test Epoch36 layer3 out_loss 0.5944420099258423, R2 0.5169547200202942\n",
      "Test Epoch36 layer4 out_loss 0.6172367930412292, R2 0.49843156337738037\n",
      "Train 37 | out_loss 0.5712430477142334: 100%|████| 8/8 [00:00<00:00, 251.04it/s]\n",
      "Train Epoch37 out_loss 0.32631856203079224, R2 0.6524897813796997\n",
      "Test Epoch37 layer0 out_loss 0.6291162967681885, R2 0.488778293132782\n",
      "Test Epoch37 layer1 out_loss 0.6390463709831238, R2 0.4807090759277344\n",
      "Test Epoch37 layer2 out_loss 0.6442766785621643, R2 0.47645890712738037\n",
      "Test Epoch37 layer3 out_loss 0.652093231678009, R2 0.47010713815689087\n",
      "Test Epoch37 layer4 out_loss 0.6730004549026489, R2 0.45311790704727173\n",
      "Train 38 | out_loss 0.577564001083374: 100%|█████| 8/8 [00:00<00:00, 255.58it/s]\n",
      "Train Epoch38 out_loss 0.3335801661014557, R2 0.6447566151618958\n",
      "Test Epoch38 layer0 out_loss 0.5599655508995056, R2 0.5449703931808472\n",
      "Test Epoch38 layer1 out_loss 0.5683917999267578, R2 0.5381231307983398\n",
      "Test Epoch38 layer2 out_loss 0.5693243741989136, R2 0.5373653173446655\n",
      "Test Epoch38 layer3 out_loss 0.5752324461936951, R2 0.5325644612312317\n",
      "Test Epoch38 layer4 out_loss 0.6014276146888733, R2 0.5112781524658203\n",
      "Train 39 | out_loss 0.5672348737716675: 100%|████| 8/8 [00:00<00:00, 255.08it/s]\n",
      "Train Epoch39 out_loss 0.32175540924072266, R2 0.6573492884635925\n",
      "Test Epoch39 layer0 out_loss 0.5630162358283997, R2 0.5424914360046387\n",
      "Test Epoch39 layer1 out_loss 0.5583893060684204, R2 0.5462512373924255\n",
      "Test Epoch39 layer2 out_loss 0.5671136379241943, R2 0.5391618013381958\n",
      "Test Epoch39 layer3 out_loss 0.5892047882080078, R2 0.5212104916572571\n",
      "Test Epoch39 layer4 out_loss 0.616811215877533, R2 0.49877744913101196\n",
      "Train 40 | out_loss 0.5590523481369019: 100%|████| 8/8 [00:00<00:00, 258.16it/s]\n",
      "Train Epoch40 out_loss 0.3125394880771637, R2 0.6671637296676636\n",
      "Test Epoch40 layer0 out_loss 0.5392674803733826, R2 0.5617896914482117\n",
      "Test Epoch40 layer1 out_loss 0.5457773208618164, R2 0.5564997792243958\n",
      "Test Epoch40 layer2 out_loss 0.54367995262146, R2 0.5582040548324585\n",
      "Test Epoch40 layer3 out_loss 0.533525288105011, R2 0.5664558410644531\n",
      "Test Epoch40 layer4 out_loss 0.5382301807403564, R2 0.5626326203346252\n",
      "Train 41 | out_loss 0.5717400312423706: 100%|████| 8/8 [00:00<00:00, 244.89it/s]\n",
      "Train Epoch41 out_loss 0.32688668370246887, R2 0.6518847942352295\n",
      "Test Epoch41 layer0 out_loss 0.5640510320663452, R2 0.5416505336761475\n",
      "Test Epoch41 layer1 out_loss 0.5559253692626953, R2 0.5482534170150757\n",
      "Test Epoch41 layer2 out_loss 0.574443519115448, R2 0.533205509185791\n",
      "Test Epoch41 layer3 out_loss 0.5980206727981567, R2 0.5140466690063477\n",
      "Test Epoch41 layer4 out_loss 0.6410290598869324, R2 0.47909796237945557\n",
      "Train 42 | out_loss 0.5884043574333191: 100%|████| 8/8 [00:00<00:00, 248.64it/s]\n",
      "Train Epoch42 out_loss 0.3462197184562683, R2 0.6312962174415588\n",
      "Test Epoch42 layer0 out_loss 0.56300288438797, R2 0.542502224445343\n",
      "Test Epoch42 layer1 out_loss 0.5735733509063721, R2 0.5339126586914062\n",
      "Test Epoch42 layer2 out_loss 0.5895258188247681, R2 0.5209496021270752\n",
      "Test Epoch42 layer3 out_loss 0.611478865146637, R2 0.5031105279922485\n",
      "Test Epoch42 layer4 out_loss 0.6227401494979858, R2 0.49395954608917236\n",
      "Train 43 | out_loss 0.5903042554855347: 100%|████| 8/8 [00:00<00:00, 253.43it/s]\n",
      "Train Epoch43 out_loss 0.3484591245651245, R2 0.6289113759994507\n",
      "Test Epoch43 layer0 out_loss 0.5370389223098755, R2 0.5636006593704224\n",
      "Test Epoch43 layer1 out_loss 0.540402889251709, R2 0.5608670711517334\n",
      "Test Epoch43 layer2 out_loss 0.5486938953399658, R2 0.5541297197341919\n",
      "Test Epoch43 layer3 out_loss 0.5769848823547363, R2 0.5311404466629028\n",
      "Test Epoch43 layer4 out_loss 0.6314097046852112, R2 0.48691463470458984\n",
      "Train 44 | out_loss 0.593891441822052: 100%|█████| 8/8 [00:00<00:00, 254.51it/s]\n",
      "Train Epoch44 out_loss 0.35270699858665466, R2 0.6243876218795776\n",
      "Test Epoch44 layer0 out_loss 0.5888240933418274, R2 0.5215198397636414\n",
      "Test Epoch44 layer1 out_loss 0.6022866368293762, R2 0.5105801820755005\n",
      "Test Epoch44 layer2 out_loss 0.641569972038269, R2 0.47865837812423706\n",
      "Test Epoch44 layer3 out_loss 0.6886600255966187, R2 0.4403928518295288\n",
      "Test Epoch44 layer4 out_loss 0.7526507377624512, R2 0.3883938193321228\n",
      "Train 45 | out_loss 0.5839759707450867: 100%|████| 8/8 [00:00<00:00, 256.74it/s]\n",
      "Train Epoch45 out_loss 0.3410279154777527, R2 0.6368252038955688\n",
      "Test Epoch45 layer0 out_loss 0.5370292067527771, R2 0.5636085271835327\n",
      "Test Epoch45 layer1 out_loss 0.5386589169502258, R2 0.5622842311859131\n",
      "Test Epoch45 layer2 out_loss 0.5364512205123901, R2 0.5640782117843628\n",
      "Test Epoch45 layer3 out_loss 0.5517075657844543, R2 0.5516808032989502\n",
      "Test Epoch45 layer4 out_loss 0.5786051750183105, R2 0.5298237800598145\n",
      "Train 46 | out_loss 0.5694538950920105: 100%|████| 8/8 [00:00<00:00, 250.47it/s]\n",
      "Train Epoch46 out_loss 0.32427775859832764, R2 0.6546630859375\n",
      "Test Epoch46 layer0 out_loss 0.5609024167060852, R2 0.5442091226577759\n",
      "Test Epoch46 layer1 out_loss 0.5663201808929443, R2 0.539806604385376\n",
      "Test Epoch46 layer2 out_loss 0.5848931074142456, R2 0.5247141718864441\n",
      "Test Epoch46 layer3 out_loss 0.6237980127334595, R2 0.4930999279022217\n",
      "Test Epoch46 layer4 out_loss 0.6309212446212769, R2 0.48731160163879395\n",
      "Train 47 | out_loss 0.5634351968765259: 100%|████| 8/8 [00:00<00:00, 256.67it/s]\n",
      "Train Epoch47 out_loss 0.3174591660499573, R2 0.6619245409965515\n",
      "Test Epoch47 layer0 out_loss 0.6206071376800537, R2 0.4956928491592407\n",
      "Test Epoch47 layer1 out_loss 0.6452106833457947, R2 0.4756999611854553\n",
      "Test Epoch47 layer2 out_loss 0.6354337334632874, R2 0.4836447238922119\n",
      "Test Epoch47 layer3 out_loss 0.6295328736305237, R2 0.48843973875045776\n",
      "Test Epoch47 layer4 out_loss 0.6155235171318054, R2 0.49982380867004395\n",
      "Train 48 | out_loss 0.5747119784355164: 100%|████| 8/8 [00:00<00:00, 254.70it/s]\n",
      "Train Epoch48 out_loss 0.33029377460479736, R2 0.6482564210891724\n",
      "Test Epoch48 layer0 out_loss 0.5509522557258606, R2 0.5522946119308472\n",
      "Test Epoch48 layer1 out_loss 0.5500617027282715, R2 0.55301833152771\n",
      "Test Epoch48 layer2 out_loss 0.5550137162208557, R2 0.5489943027496338\n",
      "Test Epoch48 layer3 out_loss 0.5550888180732727, R2 0.5489332675933838\n",
      "Test Epoch48 layer4 out_loss 0.5670332908630371, R2 0.5392271280288696\n",
      "Train 49 | out_loss 0.5434113144874573: 100%|████| 8/8 [00:00<00:00, 238.53it/s]\n",
      "Train Epoch49 out_loss 0.2952958643436432, R2 0.6855272054672241\n",
      "Test Epoch49 layer0 out_loss 0.5663042664527893, R2 0.5398195385932922\n",
      "Test Epoch49 layer1 out_loss 0.5675434470176697, R2 0.538812518119812\n",
      "Test Epoch49 layer2 out_loss 0.5687794089317322, R2 0.5378081798553467\n",
      "Test Epoch49 layer3 out_loss 0.5632349252700806, R2 0.5423136949539185\n",
      "Test Epoch49 layer4 out_loss 0.5645795464515686, R2 0.541221022605896\n",
      "Train 50 | out_loss 0.5450716614723206: 100%|████| 8/8 [00:00<00:00, 237.73it/s]\n",
      "Train Epoch50 out_loss 0.2971031367778778, R2 0.6836025714874268\n",
      "Test Epoch50 layer0 out_loss 0.5833489894866943, R2 0.5259689092636108\n",
      "Test Epoch50 layer1 out_loss 0.5829957723617554, R2 0.5262559652328491\n",
      "Test Epoch50 layer2 out_loss 0.5894160270690918, R2 0.521038830280304\n",
      "Test Epoch50 layer3 out_loss 0.5883452296257019, R2 0.5219089984893799\n",
      "Test Epoch50 layer4 out_loss 0.5996823906898499, R2 0.512696385383606\n",
      "Train 51 | out_loss 0.5452975630760193: 100%|████| 8/8 [00:00<00:00, 205.25it/s]\n",
      "Train Epoch51 out_loss 0.2973494529724121, R2 0.6833401918411255\n",
      "Test Epoch51 layer0 out_loss 0.5489603877067566, R2 0.5539132356643677\n",
      "Test Epoch51 layer1 out_loss 0.5529788136482239, R2 0.5506478548049927\n",
      "Test Epoch51 layer2 out_loss 0.559691846370697, R2 0.5451928377151489\n",
      "Test Epoch51 layer3 out_loss 0.564332127571106, R2 0.5414220690727234\n",
      "Test Epoch51 layer4 out_loss 0.5793522000312805, R2 0.5292167663574219\n",
      "Train 52 | out_loss 0.5598129630088806: 100%|████| 8/8 [00:00<00:00, 229.59it/s]\n",
      "Train Epoch52 out_loss 0.3133905827999115, R2 0.6662573218345642\n",
      "Test Epoch52 layer0 out_loss 0.5950127243995667, R2 0.5164909362792969\n",
      "Test Epoch52 layer1 out_loss 0.6027424335479736, R2 0.5102097988128662\n",
      "Test Epoch52 layer2 out_loss 0.6021652817726135, R2 0.510678768157959\n",
      "Test Epoch52 layer3 out_loss 0.6010753512382507, R2 0.5115644335746765\n",
      "Test Epoch52 layer4 out_loss 0.6050792932510376, R2 0.5083107948303223\n",
      "Train 53 | out_loss 0.5812188386917114: 100%|████| 8/8 [00:00<00:00, 254.58it/s]\n",
      "Train Epoch53 out_loss 0.3378153443336487, R2 0.6402463912963867\n",
      "Test Epoch53 layer0 out_loss 0.5813046097755432, R2 0.5276302099227905\n",
      "Test Epoch53 layer1 out_loss 0.5881869196891785, R2 0.5220376253128052\n",
      "Test Epoch53 layer2 out_loss 0.5947561264038086, R2 0.5166994333267212\n",
      "Test Epoch53 layer3 out_loss 0.6027842164039612, R2 0.5101758241653442\n",
      "Test Epoch53 layer4 out_loss 0.6141911745071411, R2 0.5009064674377441\n",
      "Train 54 | out_loss 0.6060860753059387: 100%|████| 8/8 [00:00<00:00, 255.51it/s]\n",
      "Train Epoch54 out_loss 0.3673403263092041, R2 0.6088039875030518\n",
      "Test Epoch54 layer0 out_loss 0.5280258059501648, R2 0.570924699306488\n",
      "Test Epoch54 layer1 out_loss 0.5249015688896179, R2 0.5734634399414062\n",
      "Test Epoch54 layer2 out_loss 0.5250020027160645, R2 0.5733819007873535\n",
      "Test Epoch54 layer3 out_loss 0.5272728204727173, R2 0.5715366005897522\n",
      "Test Epoch54 layer4 out_loss 0.5345194339752197, R2 0.5656479597091675\n",
      "Train 55 | out_loss 0.5785330533981323: 100%|████| 8/8 [00:00<00:00, 242.58it/s]\n",
      "Train Epoch55 out_loss 0.33470049500465393, R2 0.6435635089874268\n",
      "Test Epoch55 layer0 out_loss 0.5861056447029114, R2 0.5237288475036621\n",
      "Test Epoch55 layer1 out_loss 0.5826992988586426, R2 0.5264968872070312\n",
      "Test Epoch55 layer2 out_loss 0.585635244846344, R2 0.5241110920906067\n",
      "Test Epoch55 layer3 out_loss 0.582678496837616, R2 0.5265138149261475\n",
      "Test Epoch55 layer4 out_loss 0.5830308794975281, R2 0.5262274146080017\n",
      "Train 56 | out_loss 0.5527920722961426: 100%|████| 8/8 [00:00<00:00, 250.37it/s]\n",
      "Train Epoch56 out_loss 0.30557912588119507, R2 0.6745760440826416\n",
      "Test Epoch56 layer0 out_loss 0.544002115726471, R2 0.557942271232605\n",
      "Test Epoch56 layer1 out_loss 0.5512037873268127, R2 0.5520901679992676\n",
      "Test Epoch56 layer2 out_loss 0.5627780556678772, R2 0.5426849126815796\n",
      "Test Epoch56 layer3 out_loss 0.5755783915519714, R2 0.5322833061218262\n",
      "Test Epoch56 layer4 out_loss 0.5887179374694824, R2 0.5216060876846313\n",
      "Train 57 | out_loss 0.5634925961494446: 100%|████| 8/8 [00:00<00:00, 238.27it/s]\n",
      "Train Epoch57 out_loss 0.31752392649650574, R2 0.6618555784225464\n",
      "Test Epoch57 layer0 out_loss 0.6348862051963806, R2 0.4840896725654602\n",
      "Test Epoch57 layer1 out_loss 0.6335199475288391, R2 0.48519986867904663\n",
      "Test Epoch57 layer2 out_loss 0.6349261999130249, R2 0.48405712842941284\n",
      "Test Epoch57 layer3 out_loss 0.6254997849464417, R2 0.4917171001434326\n",
      "Test Epoch57 layer4 out_loss 0.6369492411613464, R2 0.4824131727218628\n",
      "Train 58 | out_loss 0.5659780502319336: 100%|████| 8/8 [00:00<00:00, 256.01it/s]\n",
      "Train Epoch58 out_loss 0.3203311860561371, R2 0.6588659882545471\n",
      "Test Epoch58 layer0 out_loss 0.5396967530250549, R2 0.5614408254623413\n",
      "Test Epoch58 layer1 out_loss 0.5486572980880737, R2 0.5541595220565796\n",
      "Test Epoch58 layer2 out_loss 0.5567761659622192, R2 0.5475621223449707\n",
      "Test Epoch58 layer3 out_loss 0.5690032243728638, R2 0.537626326084137\n",
      "Test Epoch58 layer4 out_loss 0.5881263613700867, R2 0.5220868587493896\n",
      "Train 59 | out_loss 0.5642572641372681: 100%|████| 8/8 [00:00<00:00, 246.51it/s]\n",
      "Train Epoch59 out_loss 0.3183862268924713, R2 0.6609372496604919\n",
      "Test Epoch59 layer0 out_loss 0.5783644914627075, R2 0.5300193428993225\n",
      "Test Epoch59 layer1 out_loss 0.5770502090454102, R2 0.531087338924408\n",
      "Test Epoch59 layer2 out_loss 0.5833740234375, R2 0.5259485840797424\n",
      "Test Epoch59 layer3 out_loss 0.5812929272651672, R2 0.5276396870613098\n",
      "Test Epoch59 layer4 out_loss 0.5895668268203735, R2 0.5209163427352905\n",
      "Train 60 | out_loss 0.5510665774345398: 100%|████| 8/8 [00:00<00:00, 240.64it/s]\n",
      "Train Epoch60 out_loss 0.3036743402481079, R2 0.6766045093536377\n",
      "Test Epoch60 layer0 out_loss 0.55448979139328, R2 0.5494199991226196\n",
      "Test Epoch60 layer1 out_loss 0.5583061575889587, R2 0.546318769454956\n",
      "Test Epoch60 layer2 out_loss 0.5592344403266907, R2 0.5455644726753235\n",
      "Test Epoch60 layer3 out_loss 0.5595358610153198, R2 0.5453195571899414\n",
      "Test Epoch60 layer4 out_loss 0.5639302134513855, R2 0.5417486429214478\n",
      "Train 61 | out_loss 0.5456452369689941: 100%|████| 8/8 [00:00<00:00, 255.40it/s]\n",
      "Train Epoch61 out_loss 0.29772865772247314, R2 0.682936429977417\n",
      "Test Epoch61 layer0 out_loss 0.5773273706436157, R2 0.5308620929718018\n",
      "Test Epoch61 layer1 out_loss 0.575823187828064, R2 0.5320844054222107\n",
      "Test Epoch61 layer2 out_loss 0.5884953737258911, R2 0.5217869281768799\n",
      "Test Epoch61 layer3 out_loss 0.5976009964942932, R2 0.5143877267837524\n",
      "Test Epoch61 layer4 out_loss 0.6112974882125854, R2 0.5032578706741333\n",
      "Train 62 | out_loss 0.5555850267410278: 100%|████| 8/8 [00:00<00:00, 259.78it/s]\n",
      "Train Epoch62 out_loss 0.308674693107605, R2 0.6712794303894043\n",
      "Test Epoch62 layer0 out_loss 0.5589891672134399, R2 0.5457637906074524\n",
      "Test Epoch62 layer1 out_loss 0.5702505111694336, R2 0.5366128087043762\n",
      "Test Epoch62 layer2 out_loss 0.5675402879714966, R2 0.5388151407241821\n",
      "Test Epoch62 layer3 out_loss 0.5719313025474548, R2 0.5352469682693481\n",
      "Test Epoch62 layer4 out_loss 0.577682375907898, R2 0.5305736064910889\n",
      "Train 63 | out_loss 0.5319955348968506: 100%|████| 8/8 [00:00<00:00, 255.62it/s]\n",
      "Train Epoch63 out_loss 0.2830193042755127, R2 0.6986010074615479\n",
      "Test Epoch63 layer0 out_loss 0.5873710513114929, R2 0.5227006077766418\n",
      "Test Epoch63 layer1 out_loss 0.5797756910324097, R2 0.5288726091384888\n",
      "Test Epoch63 layer2 out_loss 0.585125744342804, R2 0.5245251655578613\n",
      "Test Epoch63 layer3 out_loss 0.5890270471572876, R2 0.5213549137115479\n",
      "Test Epoch63 layer4 out_loss 0.5948822498321533, R2 0.5165969133377075\n",
      "Train 64 | out_loss 0.5411431193351746: 100%|████| 8/8 [00:00<00:00, 251.26it/s]\n",
      "Train Epoch64 out_loss 0.29283586144447327, R2 0.6881468892097473\n",
      "Test Epoch64 layer0 out_loss 0.5562697649002075, R2 0.5479735732078552\n",
      "Test Epoch64 layer1 out_loss 0.559377133846283, R2 0.5454485416412354\n",
      "Test Epoch64 layer2 out_loss 0.564935564994812, R2 0.5409317016601562\n",
      "Test Epoch64 layer3 out_loss 0.5649173855781555, R2 0.5409464836120605\n",
      "Test Epoch64 layer4 out_loss 0.5672891139984131, R2 0.5390192270278931\n",
      "Train 65 | out_loss 0.5388709306716919: 100%|████| 8/8 [00:00<00:00, 259.89it/s]\n",
      "Train Epoch65 out_loss 0.29038187861442566, R2 0.6907602548599243\n",
      "Test Epoch65 layer0 out_loss 0.578859269618988, R2 0.5296173095703125\n",
      "Test Epoch65 layer1 out_loss 0.5899592638015747, R2 0.5205973982810974\n",
      "Test Epoch65 layer2 out_loss 0.5960462093353271, R2 0.5156511068344116\n",
      "Test Epoch65 layer3 out_loss 0.5943422317504883, R2 0.5170358419418335\n",
      "Test Epoch65 layer4 out_loss 0.602497935295105, R2 0.5104084610939026\n",
      "Train 66 | out_loss 0.5354552268981934: 100%|████| 8/8 [00:00<00:00, 253.74it/s]\n",
      "Train Epoch66 out_loss 0.28671228885650635, R2 0.694668173789978\n",
      "Test Epoch66 layer0 out_loss 0.5463427901268005, R2 0.5560402870178223\n",
      "Test Epoch66 layer1 out_loss 0.5513001680374146, R2 0.5520119071006775\n",
      "Test Epoch66 layer2 out_loss 0.5581528544425964, R2 0.5464433431625366\n",
      "Test Epoch66 layer3 out_loss 0.560836672782898, R2 0.5442625284194946\n",
      "Test Epoch66 layer4 out_loss 0.561481237411499, R2 0.5437387228012085\n",
      "Train 67 | out_loss 0.550941526889801: 100%|█████| 8/8 [00:00<00:00, 242.29it/s]\n",
      "Train Epoch67 out_loss 0.303536593914032, R2 0.6767512559890747\n",
      "Test Epoch67 layer0 out_loss 0.5577867031097412, R2 0.5467408895492554\n",
      "Test Epoch67 layer1 out_loss 0.5527592301368713, R2 0.5508262515068054\n",
      "Test Epoch67 layer2 out_loss 0.5521682500839233, R2 0.5513064861297607\n",
      "Test Epoch67 layer3 out_loss 0.5501939058303833, R2 0.5529108047485352\n",
      "Test Epoch67 layer4 out_loss 0.5580688118934631, R2 0.5465116500854492\n",
      "Train 68 | out_loss 0.5552066564559937: 100%|████| 8/8 [00:00<00:00, 233.59it/s]\n",
      "Train Epoch68 out_loss 0.3082544505596161, R2 0.6717270612716675\n",
      "Test Epoch68 layer0 out_loss 0.609208345413208, R2 0.504955530166626\n",
      "Test Epoch68 layer1 out_loss 0.6065787076950073, R2 0.5070923566818237\n",
      "Test Epoch68 layer2 out_loss 0.6265963912010193, R2 0.49082595109939575\n",
      "Test Epoch68 layer3 out_loss 0.6392168998718262, R2 0.4805704951286316\n",
      "Test Epoch68 layer4 out_loss 0.6526938080787659, R2 0.46961915493011475\n",
      "Train 69 | out_loss 0.5558798909187317: 100%|████| 8/8 [00:00<00:00, 216.36it/s]\n",
      "Train Epoch69 out_loss 0.3090023994445801, R2 0.6709305047988892\n",
      "Test Epoch69 layer0 out_loss 0.57920902967453, R2 0.5293331146240234\n",
      "Test Epoch69 layer1 out_loss 0.5761054754257202, R2 0.5318549871444702\n",
      "Test Epoch69 layer2 out_loss 0.5780632495880127, R2 0.530264139175415\n",
      "Test Epoch69 layer3 out_loss 0.5776921510696411, R2 0.5305656790733337\n",
      "Test Epoch69 layer4 out_loss 0.5885701775550842, R2 0.521726131439209\n",
      "Train 70 | out_loss 0.5403090715408325: 100%|████| 8/8 [00:00<00:00, 243.87it/s]\n",
      "Train Epoch70 out_loss 0.2919338643550873, R2 0.6891075372695923\n",
      "Test Epoch70 layer0 out_loss 0.5293422341346741, R2 0.5698549747467041\n",
      "Test Epoch70 layer1 out_loss 0.5399661660194397, R2 0.5612219572067261\n",
      "Test Epoch70 layer2 out_loss 0.5476553440093994, R2 0.5549736618995667\n",
      "Test Epoch70 layer3 out_loss 0.5575853586196899, R2 0.5469045639038086\n",
      "Test Epoch70 layer4 out_loss 0.5594571232795715, R2 0.5453835129737854\n",
      "Train 71 | out_loss 0.5341582894325256: 100%|████| 8/8 [00:00<00:00, 242.95it/s]\n",
      "Train Epoch71 out_loss 0.2853250205516815, R2 0.6961455345153809\n",
      "Test Epoch71 layer0 out_loss 0.5678027868270874, R2 0.538601815700531\n",
      "Test Epoch71 layer1 out_loss 0.5701738595962524, R2 0.5366750955581665\n",
      "Test Epoch71 layer2 out_loss 0.5692101716995239, R2 0.5374581813812256\n",
      "Test Epoch71 layer3 out_loss 0.5730260610580444, R2 0.5343574285507202\n",
      "Test Epoch71 layer4 out_loss 0.5846510529518127, R2 0.5249108076095581\n",
      "Train 72 | out_loss 0.5361953973770142: 100%|████| 8/8 [00:00<00:00, 223.88it/s]\n",
      "Train Epoch72 out_loss 0.287505567073822, R2 0.6938233375549316\n",
      "Test Epoch72 layer0 out_loss 0.5577638745307922, R2 0.5467594861984253\n",
      "Test Epoch72 layer1 out_loss 0.5594756603240967, R2 0.545368492603302\n",
      "Test Epoch72 layer2 out_loss 0.5697322487831116, R2 0.5370339155197144\n",
      "Test Epoch72 layer3 out_loss 0.5690421462059021, R2 0.5375946760177612\n",
      "Test Epoch72 layer4 out_loss 0.5694762468338013, R2 0.5372419357299805\n",
      "Train 73 | out_loss 0.5304415225982666: 100%|████| 8/8 [00:00<00:00, 246.08it/s]\n",
      "Train Epoch73 out_loss 0.281368225812912, R2 0.7003592848777771\n",
      "Test Epoch73 layer0 out_loss 0.5566768646240234, R2 0.5476428270339966\n",
      "Test Epoch73 layer1 out_loss 0.5595774054527283, R2 0.5452858209609985\n",
      "Test Epoch73 layer2 out_loss 0.5631622672080994, R2 0.5423727035522461\n",
      "Test Epoch73 layer3 out_loss 0.5679585337638855, R2 0.5384752750396729\n",
      "Test Epoch73 layer4 out_loss 0.5712863206863403, R2 0.5357711315155029\n",
      "Train 74 | out_loss 0.5300589203834534: 100%|████| 8/8 [00:00<00:00, 251.24it/s]\n",
      "Train Epoch74 out_loss 0.28096240758895874, R2 0.7007914781570435\n",
      "Test Epoch74 layer0 out_loss 0.5500609278678894, R2 0.5530189275741577\n",
      "Test Epoch74 layer1 out_loss 0.5560673475265503, R2 0.5481380820274353\n",
      "Test Epoch74 layer2 out_loss 0.5609706044197083, R2 0.5441536903381348\n",
      "Test Epoch74 layer3 out_loss 0.5639350414276123, R2 0.5417447686195374\n",
      "Test Epoch74 layer4 out_loss 0.5724854469299316, R2 0.5347967147827148\n",
      "Train 75 | out_loss 0.5321208834648132: 100%|████| 8/8 [00:00<00:00, 251.91it/s]\n",
      "Train Epoch75 out_loss 0.28315261006355286, R2 0.6984590291976929\n",
      "Test Epoch75 layer0 out_loss 0.6639323830604553, R2 0.46048665046691895\n",
      "Test Epoch75 layer1 out_loss 0.6577673554420471, R2 0.46549636125564575\n",
      "Test Epoch75 layer2 out_loss 0.6532774567604065, R2 0.4691448211669922\n",
      "Test Epoch75 layer3 out_loss 0.655303418636322, R2 0.4674985408782959\n",
      "Test Epoch75 layer4 out_loss 0.6480277180671692, R2 0.47341078519821167\n",
      "Train 76 | out_loss 0.5445738434791565: 100%|████| 8/8 [00:00<00:00, 252.33it/s]\n",
      "Train Epoch76 out_loss 0.2965606451034546, R2 0.6841802597045898\n",
      "Test Epoch76 layer0 out_loss 0.5554584860801697, R2 0.5486328601837158\n",
      "Test Epoch76 layer1 out_loss 0.5594114661216736, R2 0.5454206466674805\n",
      "Test Epoch76 layer2 out_loss 0.5772589445114136, R2 0.5309177041053772\n",
      "Test Epoch76 layer3 out_loss 0.5857905745506287, R2 0.5239849090576172\n",
      "Test Epoch76 layer4 out_loss 0.6090649962425232, R2 0.5050719976425171\n",
      "Train 77 | out_loss 0.5373921990394592: 100%|████| 8/8 [00:00<00:00, 246.71it/s]\n",
      "Train Epoch77 out_loss 0.28879037499427795, R2 0.6924551129341125\n",
      "Test Epoch77 layer0 out_loss 0.546492874622345, R2 0.5559183359146118\n",
      "Test Epoch77 layer1 out_loss 0.5441151857376099, R2 0.5578504204750061\n",
      "Test Epoch77 layer2 out_loss 0.548945426940918, R2 0.5539253354072571\n",
      "Test Epoch77 layer3 out_loss 0.5478752851486206, R2 0.55479496717453\n",
      "Test Epoch77 layer4 out_loss 0.5515927672386169, R2 0.5517741441726685\n",
      "Train 78 | out_loss 0.5392986536026001: 100%|████| 8/8 [00:00<00:00, 247.65it/s]\n",
      "Train Epoch78 out_loss 0.29084303975105286, R2 0.6902691721916199\n",
      "Test Epoch78 layer0 out_loss 0.5786064863204956, R2 0.5298227071762085\n",
      "Test Epoch78 layer1 out_loss 0.5849978923797607, R2 0.5246289968490601\n",
      "Test Epoch78 layer2 out_loss 0.5904930233955383, R2 0.5201636552810669\n",
      "Test Epoch78 layer3 out_loss 0.5966820120811462, R2 0.5151344537734985\n",
      "Test Epoch78 layer4 out_loss 0.6117624044418335, R2 0.5028800964355469\n",
      "Train 79 | out_loss 0.5323570966720581: 100%|████| 8/8 [00:00<00:00, 246.92it/s]\n",
      "Train Epoch79 out_loss 0.28340408205986023, R2 0.698191225528717\n",
      "Test Epoch79 layer0 out_loss 0.5663838386535645, R2 0.5397548675537109\n",
      "Test Epoch79 layer1 out_loss 0.5718587636947632, R2 0.5353059768676758\n",
      "Test Epoch79 layer2 out_loss 0.5813819766044617, R2 0.5275673270225525\n",
      "Test Epoch79 layer3 out_loss 0.5877454280853271, R2 0.5223963856697083\n",
      "Test Epoch79 layer4 out_loss 0.5866949558258057, R2 0.5232499837875366\n",
      "Train 80 | out_loss 0.5284255743026733: 100%|████| 8/8 [00:00<00:00, 252.93it/s]\n",
      "Train Epoch80 out_loss 0.2792336046695709, R2 0.702632486820221\n",
      "Test Epoch80 layer0 out_loss 0.6064777970314026, R2 0.5071743726730347\n",
      "Test Epoch80 layer1 out_loss 0.6016652584075928, R2 0.511085033416748\n",
      "Test Epoch80 layer2 out_loss 0.6003050208091736, R2 0.5121904015541077\n",
      "Test Epoch80 layer3 out_loss 0.5936707854270935, R2 0.5175814628601074\n",
      "Test Epoch80 layer4 out_loss 0.5924241542816162, R2 0.5185943841934204\n",
      "Train 81 | out_loss 0.5361202955245972: 100%|████| 8/8 [00:00<00:00, 247.50it/s]\n",
      "Train Epoch81 out_loss 0.2874249517917633, R2 0.693909227848053\n",
      "Test Epoch81 layer0 out_loss 0.5509855151176453, R2 0.5522675514221191\n",
      "Test Epoch81 layer1 out_loss 0.55750572681427, R2 0.5469692349433899\n",
      "Test Epoch81 layer2 out_loss 0.5683888792991638, R2 0.5381255745887756\n",
      "Test Epoch81 layer3 out_loss 0.5808793902397156, R2 0.5279757380485535\n",
      "Test Epoch81 layer4 out_loss 0.5890017747879028, R2 0.5213754773139954\n",
      "Train 82 | out_loss 0.5313642621040344: 100%|████| 8/8 [00:00<00:00, 238.06it/s]\n",
      "Train Epoch82 out_loss 0.28234800696372986, R2 0.6993159055709839\n",
      "Test Epoch82 layer0 out_loss 0.5799437165260315, R2 0.5287360548973083\n",
      "Test Epoch82 layer1 out_loss 0.5818695425987244, R2 0.5271711349487305\n",
      "Test Epoch82 layer2 out_loss 0.5901600122451782, R2 0.5204342603683472\n",
      "Test Epoch82 layer3 out_loss 0.5831014513969421, R2 0.5261700749397278\n",
      "Test Epoch82 layer4 out_loss 0.5828278660774231, R2 0.5263923406600952\n",
      "Train 83 | out_loss 0.5353624224662781: 100%|████| 8/8 [00:00<00:00, 240.50it/s]\n",
      "Train Epoch83 out_loss 0.2866128981113434, R2 0.6947739720344543\n",
      "Test Epoch83 layer0 out_loss 0.5473974347114563, R2 0.5551832318305969\n",
      "Test Epoch83 layer1 out_loss 0.5489721894264221, R2 0.5539035797119141\n",
      "Test Epoch83 layer2 out_loss 0.5572028160095215, R2 0.5472154021263123\n",
      "Test Epoch83 layer3 out_loss 0.5661625266075134, R2 0.5399346947669983\n",
      "Test Epoch83 layer4 out_loss 0.5771068930625916, R2 0.5310412645339966\n",
      "Train 84 | out_loss 0.519038736820221: 100%|█████| 8/8 [00:00<00:00, 244.12it/s]\n",
      "Train Epoch84 out_loss 0.2694011926651001, R2 0.7131034731864929\n",
      "Test Epoch84 layer0 out_loss 0.5644471049308777, R2 0.5413286685943604\n",
      "Test Epoch84 layer1 out_loss 0.5726931691169739, R2 0.5346279144287109\n",
      "Test Epoch84 layer2 out_loss 0.5840573310852051, R2 0.5253933668136597\n",
      "Test Epoch84 layer3 out_loss 0.604763925075531, R2 0.5085670948028564\n",
      "Test Epoch84 layer4 out_loss 0.6155181527137756, R2 0.49982815980911255\n",
      "Train 85 | out_loss 0.54521644115448: 100%|██████| 8/8 [00:00<00:00, 225.23it/s]\n",
      "Train Epoch85 out_loss 0.29726094007492065, R2 0.6834344863891602\n",
      "Test Epoch85 layer0 out_loss 0.5819845199584961, R2 0.5270776748657227\n",
      "Test Epoch85 layer1 out_loss 0.5951494574546814, R2 0.5163798332214355\n",
      "Test Epoch85 layer2 out_loss 0.5993515253067017, R2 0.512965202331543\n",
      "Test Epoch85 layer3 out_loss 0.6135308146476746, R2 0.5014431476593018\n",
      "Test Epoch85 layer4 out_loss 0.6258366703987122, R2 0.49144333600997925\n",
      "Train 86 | out_loss 0.528306782245636: 100%|█████| 8/8 [00:00<00:00, 207.50it/s]\n",
      "Train Epoch86 out_loss 0.2791080176830292, R2 0.7027662396430969\n",
      "Test Epoch86 layer0 out_loss 0.5671402812004089, R2 0.5391401648521423\n",
      "Test Epoch86 layer1 out_loss 0.5657674074172974, R2 0.5402557849884033\n",
      "Test Epoch86 layer2 out_loss 0.5729706287384033, R2 0.5344023704528809\n",
      "Test Epoch86 layer3 out_loss 0.5749188661575317, R2 0.5328192710876465\n",
      "Test Epoch86 layer4 out_loss 0.5818352103233337, R2 0.5271990299224854\n",
      "Train 87 | out_loss 0.5243516564369202: 100%|████| 8/8 [00:00<00:00, 239.95it/s]\n",
      "Train Epoch87 out_loss 0.27494463324546814, R2 0.7072000503540039\n",
      "Test Epoch87 layer0 out_loss 0.5771295428276062, R2 0.5310229063034058\n",
      "Test Epoch87 layer1 out_loss 0.5950350165367126, R2 0.5164728164672852\n",
      "Test Epoch87 layer2 out_loss 0.610609769821167, R2 0.5038167238235474\n",
      "Test Epoch87 layer3 out_loss 0.629766047000885, R2 0.4882503151893616\n",
      "Test Epoch87 layer4 out_loss 0.6311582922935486, R2 0.4871189594268799\n",
      "Train 88 | out_loss 0.531467854976654: 100%|█████| 8/8 [00:00<00:00, 211.88it/s]\n",
      "Train Epoch88 out_loss 0.2824581265449524, R2 0.6991986036300659\n",
      "Test Epoch88 layer0 out_loss 0.5560060143470764, R2 0.5481879115104675\n",
      "Test Epoch88 layer1 out_loss 0.5640274286270142, R2 0.5416696667671204\n",
      "Test Epoch88 layer2 out_loss 0.5710119605064392, R2 0.5359940528869629\n",
      "Test Epoch88 layer3 out_loss 0.571392834186554, R2 0.5356845855712891\n",
      "Test Epoch88 layer4 out_loss 0.5755341649055481, R2 0.5323192477226257\n",
      "Train 89 | out_loss 0.5327203273773193: 100%|████| 8/8 [00:00<00:00, 238.37it/s]\n",
      "Train Epoch89 out_loss 0.2837909162044525, R2 0.6977792382240295\n",
      "Test Epoch89 layer0 out_loss 0.5635340213775635, R2 0.5420706272125244\n",
      "Test Epoch89 layer1 out_loss 0.578671395778656, R2 0.529770016670227\n",
      "Test Epoch89 layer2 out_loss 0.5897097587585449, R2 0.5208001136779785\n",
      "Test Epoch89 layer3 out_loss 0.6089978814125061, R2 0.5051265954971313\n",
      "Test Epoch89 layer4 out_loss 0.6091095805168152, R2 0.5050357580184937\n",
      "Train 90 | out_loss 0.5274533033370972: 100%|████| 8/8 [00:00<00:00, 196.55it/s]\n",
      "Train Epoch90 out_loss 0.278207004070282, R2 0.7037258148193359\n",
      "Test Epoch90 layer0 out_loss 0.567785918712616, R2 0.5386155247688293\n",
      "Test Epoch90 layer1 out_loss 0.5707617402076721, R2 0.5361973643302917\n",
      "Test Epoch90 layer2 out_loss 0.5879172682762146, R2 0.5222567319869995\n",
      "Test Epoch90 layer3 out_loss 0.5899428725242615, R2 0.5206106901168823\n",
      "Test Epoch90 layer4 out_loss 0.5882437229156494, R2 0.5219914317131042\n",
      "Train 91 | out_loss 0.5379018187522888: 100%|████| 8/8 [00:00<00:00, 207.92it/s]\n",
      "Train Epoch91 out_loss 0.2893383800983429, R2 0.6918715238571167\n",
      "Test Epoch91 layer0 out_loss 0.5622225403785706, R2 0.5431363582611084\n",
      "Test Epoch91 layer1 out_loss 0.5849242210388184, R2 0.5246888399124146\n",
      "Test Epoch91 layer2 out_loss 0.6024774312973022, R2 0.5104250907897949\n",
      "Test Epoch91 layer3 out_loss 0.6371146440505981, R2 0.48227882385253906\n",
      "Test Epoch91 layer4 out_loss 0.6655831336975098, R2 0.459145188331604\n",
      "Train 92 | out_loss 0.5412077307701111: 100%|████| 8/8 [00:00<00:00, 248.63it/s]\n",
      "Train Epoch92 out_loss 0.2929057776927948, R2 0.6880724430084229\n",
      "Test Epoch92 layer0 out_loss 0.5461731553077698, R2 0.556178092956543\n",
      "Test Epoch92 layer1 out_loss 0.5488792657852173, R2 0.5539791584014893\n",
      "Test Epoch92 layer2 out_loss 0.5533156991004944, R2 0.5503740310668945\n",
      "Test Epoch92 layer3 out_loss 0.5565956234931946, R2 0.5477087497711182\n",
      "Test Epoch92 layer4 out_loss 0.5582762956619263, R2 0.5463430881500244\n",
      "Train 93 | out_loss 0.5171269178390503: 100%|████| 8/8 [00:00<00:00, 249.38it/s]\n",
      "Train Epoch93 out_loss 0.2674202024936676, R2 0.7152131199836731\n",
      "Test Epoch93 layer0 out_loss 0.5911757946014404, R2 0.5196088552474976\n",
      "Test Epoch93 layer1 out_loss 0.5916933417320251, R2 0.5191882848739624\n",
      "Test Epoch93 layer2 out_loss 0.6055290699005127, R2 0.5079452991485596\n",
      "Test Epoch93 layer3 out_loss 0.6102723479270935, R2 0.5040909647941589\n",
      "Test Epoch93 layer4 out_loss 0.6236492991447449, R2 0.49322080612182617\n",
      "Train 94 | out_loss 0.5228996872901917: 100%|████| 8/8 [00:00<00:00, 252.21it/s]\n",
      "Train Epoch94 out_loss 0.27342408895492554, R2 0.7088192701339722\n",
      "Test Epoch94 layer0 out_loss 0.5545618534088135, R2 0.5493614673614502\n",
      "Test Epoch94 layer1 out_loss 0.5654016733169556, R2 0.5405529737472534\n",
      "Test Epoch94 layer2 out_loss 0.5780844688415527, R2 0.530246913433075\n",
      "Test Epoch94 layer3 out_loss 0.58199143409729, R2 0.5270720720291138\n",
      "Test Epoch94 layer4 out_loss 0.586802065372467, R2 0.5231629610061646\n",
      "Train 95 | out_loss 0.5238103866577148: 100%|████| 8/8 [00:00<00:00, 249.47it/s]\n",
      "Train Epoch95 out_loss 0.27437734603881836, R2 0.7078041434288025\n",
      "Test Epoch95 layer0 out_loss 0.5655823945999146, R2 0.5404061079025269\n",
      "Test Epoch95 layer1 out_loss 0.5778464674949646, R2 0.5304402709007263\n",
      "Test Epoch95 layer2 out_loss 0.5878394842147827, R2 0.5223199129104614\n",
      "Test Epoch95 layer3 out_loss 0.5925106406211853, R2 0.518524169921875\n",
      "Test Epoch95 layer4 out_loss 0.5965479612350464, R2 0.515243411064148\n",
      "Train 96 | out_loss 0.5356515049934387: 100%|████| 8/8 [00:00<00:00, 252.84it/s]\n",
      "Train Epoch96 out_loss 0.28692251443862915, R2 0.6944442987442017\n",
      "Test Epoch96 layer0 out_loss 0.5521497130393982, R2 0.5513215065002441\n",
      "Test Epoch96 layer1 out_loss 0.5563116073608398, R2 0.5479395985603333\n",
      "Test Epoch96 layer2 out_loss 0.5611628890037537, R2 0.5439974069595337\n",
      "Test Epoch96 layer3 out_loss 0.5589199662208557, R2 0.5458199977874756\n",
      "Test Epoch96 layer4 out_loss 0.5618923902511597, R2 0.5434045791625977\n",
      "Train 97 | out_loss 0.5329580307006836: 100%|████| 8/8 [00:00<00:00, 252.45it/s]\n",
      "Train Epoch97 out_loss 0.2840442955493927, R2 0.6975094676017761\n",
      "Test Epoch97 layer0 out_loss 0.5917271375656128, R2 0.5191608667373657\n",
      "Test Epoch97 layer1 out_loss 0.5901530385017395, R2 0.5204399824142456\n",
      "Test Epoch97 layer2 out_loss 0.594475507736206, R2 0.5169274806976318\n",
      "Test Epoch97 layer3 out_loss 0.5977486968040466, R2 0.5142676830291748\n",
      "Test Epoch97 layer4 out_loss 0.5968101024627686, R2 0.5150303840637207\n",
      "Train 98 | out_loss 0.5319114923477173: 100%|████| 8/8 [00:00<00:00, 203.05it/s]\n",
      "Train Epoch98 out_loss 0.28292983770370483, R2 0.6986962556838989\n",
      "Test Epoch98 layer0 out_loss 0.5476277470588684, R2 0.554996132850647\n",
      "Test Epoch98 layer1 out_loss 0.5573472380638123, R2 0.5470980405807495\n",
      "Test Epoch98 layer2 out_loss 0.5691747069358826, R2 0.5374870300292969\n",
      "Test Epoch98 layer3 out_loss 0.5714284181594849, R2 0.5356556177139282\n",
      "Test Epoch98 layer4 out_loss 0.5750502943992615, R2 0.532712459564209\n",
      "Train 99 | out_loss 0.5237182974815369: 100%|████| 8/8 [00:00<00:00, 244.76it/s]\n",
      "Train Epoch99 out_loss 0.274280846118927, R2 0.70790696144104\n",
      "Test Epoch99 layer0 out_loss 0.6267023086547852, R2 0.4907398819923401\n",
      "Test Epoch99 layer1 out_loss 0.6242595314979553, R2 0.4927248954772949\n",
      "Test Epoch99 layer2 out_loss 0.6230459213256836, R2 0.49371111392974854\n",
      "Test Epoch99 layer3 out_loss 0.6226088404655457, R2 0.4940662980079651\n",
      "Test Epoch99 layer4 out_loss 0.6273756623268127, R2 0.490192711353302\n",
      "Best r2 0.5734634399414062 at L1\n",
      "Figure(640x480)\n",
      "Figure(640x480)\n",
      "Figure(640x480)\n",
      "Figure(640x480)\n"
     ]
    }
   ],
   "source": [
    "# LinearAL ailerons\n",
    "\n",
    "data = \"paint\"\n",
    "\n",
    "model =  \"linearal\"\n",
    "#for layer in range(1,11):\n",
    "for layer in [5]:\n",
    "    log = f\"result/{data}_{model}_l{layer}.log\"\n",
    "    !python3 dis_train_al.py --dataset {data} --model {model} --epoch 100 --num-layer {layer} --lr 0.001 --task regression # > {log}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchmetrics\n",
      "  Downloading torchmetrics-0.9.3-py3-none-any.whl (419 kB)\n",
      "\u001b[K     |████████████████████████████████| 419 kB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.3.1 in /opt/conda/lib/python3.9/site-packages (from torchmetrics) (1.12.0+cu113)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /opt/conda/lib/python3.9/site-packages (from torchmetrics) (1.21.5)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from torchmetrics) (21.3)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch>=1.3.1->torchmetrics) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging->torchmetrics) (3.0.4)\n",
      "Installing collected packages: torchmetrics\n",
      "Successfully installed torchmetrics-0.9.3\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ea8a7f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0008725115898554677\n",
      "0.0004098966061174112\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "print(statistics.mean(y))\n",
    "print(statistics.stdev(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46d15db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
