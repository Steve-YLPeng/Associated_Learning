total count words 222751
vocab size 30000
train size 40000, valid size 5000, test size 5000
found 27937 words in glove
model: TransformerModelML(
  (layers): ModuleList(
    (0): EMBLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Embedding(30000, 300)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=2, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=2, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.weight 9000000
layers.0.ae.g.0.weight 256
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 256
layers.0.ae.h.0.bias 2
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
Total Trainable Params: 13671778
init_time 22.66787815093994
Start Training
gc 0
Train Epoch0 Acc 0.5052 (20208/40000), AUC 0.5078514814376831
ep0_train_time 98.31390762329102
Test Epoch0 layer4 Acc 0.7522, AUC 0.9130881428718567, avg_entr 0.6691398024559021
ep0_l4_test_time 3.232592821121216
Save ckpt to ckpt/imdb_transformeral_l5base_pad300//imdb_transformeral_l5.pt  ,ep 0
gc 0
Train Epoch1 Acc 0.86485 (34594/40000), AUC 0.9333049058914185
ep1_train_time 97.1640100479126
Test Epoch1 layer4 Acc 0.8628, AUC 0.9549674391746521, avg_entr 0.14092965424060822
ep1_l4_test_time 3.171191930770874
Save ckpt to ckpt/imdb_transformeral_l5base_pad300//imdb_transformeral_l5.pt  ,ep 1
gc 0
Train Epoch2 Acc 0.91285 (36514/40000), AUC 0.9678112864494324
ep2_train_time 97.17528986930847
Test Epoch2 layer4 Acc 0.8928, AUC 0.9560864567756653, avg_entr 0.053832922130823135
ep2_l4_test_time 3.20241117477417
Save ckpt to ckpt/imdb_transformeral_l5base_pad300//imdb_transformeral_l5.pt  ,ep 2
gc 0
Train Epoch3 Acc 0.942625 (37705/40000), AUC 0.9818934202194214
ep3_train_time 97.1063916683197
Test Epoch3 layer4 Acc 0.8818, AUC 0.9509471654891968, avg_entr 0.045696403831243515
ep3_l4_test_time 3.2000508308410645
gc 0
Train Epoch4 Acc 0.955275 (38211/40000), AUC 0.986708402633667
ep4_train_time 97.2292149066925
Test Epoch4 layer4 Acc 0.8768, AUC 0.9475482702255249, avg_entr 0.03473534807562828
ep4_l4_test_time 3.2215142250061035
gc 0
Train Epoch5 Acc 0.96115 (38446/40000), AUC 0.9886903762817383
ep5_train_time 97.14742183685303
Test Epoch5 layer4 Acc 0.8788, AUC 0.9455338716506958, avg_entr 0.034167222678661346
ep5_l4_test_time 3.223233699798584
gc 0
Train Epoch6 Acc 0.967275 (38691/40000), AUC 0.9909718632698059
ep6_train_time 78.16772937774658
Test Epoch6 layer4 Acc 0.8718, AUC 0.9433697462081909, avg_entr 0.027331119403243065
ep6_l4_test_time 3.2241499423980713
gc 0
Train Epoch7 Acc 0.971275 (38851/40000), AUC 0.9930769205093384
ep7_train_time 97.17178297042847
Test Epoch7 layer4 Acc 0.8724, AUC 0.9410766363143921, avg_entr 0.0236648116260767
ep7_l4_test_time 3.1964545249938965
gc 0
Train Epoch8 Acc 0.974625 (38985/40000), AUC 0.9935371279716492
ep8_train_time 97.1461250782013
Test Epoch8 layer4 Acc 0.8556, AUC 0.9391705989837646, avg_entr 0.024588411673903465
ep8_l4_test_time 3.199140787124634
gc 0
Train Epoch9 Acc 0.97795 (39118/40000), AUC 0.9944453239440918
ep9_train_time 97.1731550693512
Test Epoch9 layer4 Acc 0.8672, AUC 0.9356534481048584, avg_entr 0.01889069192111492
ep9_l4_test_time 3.1431338787078857
gc 0
Train Epoch10 Acc 0.979825 (39193/40000), AUC 0.9952683448791504
ep10_train_time 97.23343229293823
Test Epoch10 layer4 Acc 0.8642, AUC 0.934288501739502, avg_entr 0.01734822988510132
ep10_l4_test_time 3.2128024101257324
gc 0
Train Epoch11 Acc 0.981925 (39277/40000), AUC 0.996334433555603
ep11_train_time 97.13096213340759
Test Epoch11 layer4 Acc 0.8612, AUC 0.9323246479034424, avg_entr 0.015962284058332443
ep11_l4_test_time 3.21804141998291
gc 0
Train Epoch12 Acc 0.983575 (39343/40000), AUC 0.9964134693145752
ep12_train_time 97.11262655258179
Test Epoch12 layer4 Acc 0.8528, AUC 0.9313845634460449, avg_entr 0.01637687161564827
ep12_l4_test_time 3.2172954082489014
gc 0
Train Epoch13 Acc 0.985075 (39403/40000), AUC 0.9969656467437744
ep13_train_time 97.05248284339905
Test Epoch13 layer4 Acc 0.8564, AUC 0.9179618954658508, avg_entr 0.013153503648936749
ep13_l4_test_time 3.216156482696533
gc 0
Train Epoch14 Acc 0.985975 (39439/40000), AUC 0.9971916675567627
ep14_train_time 97.19036984443665
Test Epoch14 layer4 Acc 0.8552, AUC 0.9184054136276245, avg_entr 0.013332403264939785
ep14_l4_test_time 3.186171770095825
gc 0
Train Epoch15 Acc 0.987875 (39515/40000), AUC 0.9975504279136658
ep15_train_time 97.2433729171753
Test Epoch15 layer4 Acc 0.8538, AUC 0.9160171747207642, avg_entr 0.012722226791083813
ep15_l4_test_time 3.143099784851074
gc 0
Train Epoch16 Acc 0.98865 (39546/40000), AUC 0.9976640939712524
ep16_train_time 384.6853013038635
Test Epoch16 layer4 Acc 0.8508, AUC 0.908469557762146, avg_entr 0.010022038593888283
ep16_l4_test_time 3.2253100872039795
gc 0
Train Epoch17 Acc 0.989475 (39579/40000), AUC 0.9980974197387695
ep17_train_time 97.0420753955841
Test Epoch17 layer4 Acc 0.84, AUC 0.8918031454086304, avg_entr 0.010345052927732468
ep17_l4_test_time 3.208235740661621
gc 0
Train Epoch18 Acc 0.99035 (39614/40000), AUC 0.998110294342041
ep18_train_time 97.19689607620239
Test Epoch18 layer4 Acc 0.8516, AUC 0.9048254489898682, avg_entr 0.011119321919977665
ep18_l4_test_time 3.2053589820861816
gc 0
Train Epoch19 Acc 0.9904 (39616/40000), AUC 0.9983572959899902
ep19_train_time 97.21528577804565
Test Epoch19 layer4 Acc 0.8444, AUC 0.9004141092300415, avg_entr 0.010085553862154484
ep19_l4_test_time 3.2176942825317383
gc 0
Train Epoch20 Acc 0.9914 (39656/40000), AUC 0.9987176060676575
ep20_train_time 97.24548292160034
Test Epoch20 layer4 Acc 0.843, AUC 0.8950420022010803, avg_entr 0.00898102018982172
ep20_l4_test_time 3.170076370239258
gc 0
Train Epoch21 Acc 0.99195 (39678/40000), AUC 0.9985290765762329
ep21_train_time 97.22059917449951
Test Epoch21 layer4 Acc 0.845, AUC 0.8922981023788452, avg_entr 0.008808527141809464
ep21_l4_test_time 3.196892261505127
gc 0
Train Epoch22 Acc 0.99235 (39694/40000), AUC 0.9986988306045532
ep22_train_time 97.23703145980835
Test Epoch22 layer4 Acc 0.8428, AUC 0.8913965225219727, avg_entr 0.008300967514514923
ep22_l4_test_time 3.1858394145965576
gc 0
Train Epoch23 Acc 0.99305 (39722/40000), AUC 0.999116063117981
ep23_train_time 78.07810997962952
Test Epoch23 layer4 Acc 0.8424, AUC 0.8813064098358154, avg_entr 0.009422797709703445
ep23_l4_test_time 3.2158374786376953
gc 0
Train Epoch24 Acc 0.993325 (39733/40000), AUC 0.9989733695983887
ep24_train_time 97.21564221382141
Test Epoch24 layer4 Acc 0.8356, AUC 0.8774089813232422, avg_entr 0.009153895080089569
ep24_l4_test_time 3.208712339401245
gc 0
Train Epoch25 Acc 0.99355 (39742/40000), AUC 0.9990310668945312
ep25_train_time 97.2565279006958
Test Epoch25 layer4 Acc 0.8338, AUC 0.8678073883056641, avg_entr 0.006271288730204105
ep25_l4_test_time 3.189807415008545
gc 0
Train Epoch26 Acc 0.99415 (39766/40000), AUC 0.9991656541824341
ep26_train_time 97.21170806884766
Test Epoch26 layer4 Acc 0.8368, AUC 0.8720642924308777, avg_entr 0.007909604348242283
ep26_l4_test_time 3.1915574073791504
gc 0
Train Epoch27 Acc 0.994425 (39777/40000), AUC 0.9993127584457397
ep27_train_time 97.25377535820007
Test Epoch27 layer4 Acc 0.8324, AUC 0.8685556650161743, avg_entr 0.00692412257194519
ep27_l4_test_time 3.195924758911133
gc 0
Train Epoch28 Acc 0.99515 (39806/40000), AUC 0.9992319345474243
ep28_train_time 97.33827757835388
Test Epoch28 layer4 Acc 0.8362, AUC 0.8507983088493347, avg_entr 0.004555385094136
ep28_l4_test_time 3.2082552909851074
gc 0
Train Epoch29 Acc 0.99525 (39810/40000), AUC 0.9992679953575134
ep29_train_time 97.2358341217041
Test Epoch29 layer4 Acc 0.8326, AUC 0.8515355587005615, avg_entr 0.004230048973113298
ep29_l4_test_time 3.22090220451355
gc 0
Train Epoch30 Acc 0.995675 (39827/40000), AUC 0.9995613098144531
ep30_train_time 97.22127223014832
Test Epoch30 layer4 Acc 0.828, AUC 0.8558409810066223, avg_entr 0.0033847892191261053
ep30_l4_test_time 3.2099592685699463
gc 0
Train Epoch31 Acc 0.9956 (39824/40000), AUC 0.9994586706161499
ep31_train_time 97.21747279167175
Test Epoch31 layer4 Acc 0.8296, AUC 0.8462895154953003, avg_entr 0.004752364009618759
ep31_l4_test_time 3.2046966552734375
gc 0
Train Epoch32 Acc 0.996225 (39849/40000), AUC 0.9993952512741089
ep32_train_time 97.28536343574524
Test Epoch32 layer4 Acc 0.8294, AUC 0.849585771560669, avg_entr 0.0033185905776917934
ep32_l4_test_time 3.2236945629119873
gc 0
Train Epoch33 Acc 0.996475 (39859/40000), AUC 0.9994579553604126
ep33_train_time 97.1204264163971
Test Epoch33 layer4 Acc 0.8272, AUC 0.8382492661476135, avg_entr 0.0038240845315158367
ep33_l4_test_time 3.1969311237335205
gc 0
Train Epoch34 Acc 0.9967 (39868/40000), AUC 0.9995326995849609
ep34_train_time 97.20893549919128
Test Epoch34 layer4 Acc 0.8188, AUC 0.8319320678710938, avg_entr 0.004339111037552357
ep34_l4_test_time 3.2181780338287354
gc 0
Train Epoch35 Acc 0.996625 (39865/40000), AUC 0.9995537996292114
ep35_train_time 97.12917590141296
Test Epoch35 layer4 Acc 0.8234, AUC 0.8360772132873535, avg_entr 0.003994042985141277
ep35_l4_test_time 3.2312989234924316
gc 0
Train Epoch36 Acc 0.99705 (39882/40000), AUC 0.9995687007904053
ep36_train_time 97.21593260765076
Test Epoch36 layer4 Acc 0.8262, AUC 0.8410988450050354, avg_entr 0.004662406165152788
ep36_l4_test_time 3.1699414253234863
gc 0
Train Epoch37 Acc 0.9974 (39896/40000), AUC 0.9995495676994324
ep37_train_time 97.26185750961304
Test Epoch37 layer4 Acc 0.826, AUC 0.8461635112762451, avg_entr 0.003847618820145726
ep37_l4_test_time 3.2235107421875
gc 0
Train Epoch38 Acc 0.997275 (39891/40000), AUC 0.9996063113212585
ep38_train_time 97.22248840332031
Test Epoch38 layer4 Acc 0.8224, AUC 0.8324654698371887, avg_entr 0.004448562860488892
ep38_l4_test_time 3.225743293762207
gc 0
Train Epoch39 Acc 0.9978 (39912/40000), AUC 0.9997661709785461
ep39_train_time 97.34975075721741
Test Epoch39 layer4 Acc 0.8216, AUC 0.8296939134597778, avg_entr 0.005455056205391884
ep39_l4_test_time 3.175128221511841
gc 0
Train Epoch40 Acc 0.997775 (39911/40000), AUC 0.9998123645782471
ep40_train_time 81.45741415023804
Test Epoch40 layer4 Acc 0.8186, AUC 0.8245083093643188, avg_entr 0.0025764862075448036
ep40_l4_test_time 1.6824772357940674
gc 0
Train Epoch41 Acc 0.99815 (39926/40000), AUC 0.9997051358222961
ep41_train_time 82.11859345436096
Test Epoch41 layer4 Acc 0.8194, AUC 0.8238775730133057, avg_entr 0.0019126739352941513
ep41_l4_test_time 2.987795114517212
gc 0
Train Epoch42 Acc 0.99815 (39926/40000), AUC 0.9997422695159912
ep42_train_time 89.72954177856445
Test Epoch42 layer4 Acc 0.8116, AUC 0.8010814189910889, avg_entr 0.0018586200894787908
ep42_l4_test_time 2.9853122234344482
gc 0
Train Epoch43 Acc 0.99815 (39926/40000), AUC 0.9998090863227844
ep43_train_time 90.34374666213989
Test Epoch43 layer4 Acc 0.8222, AUC 0.816969633102417, avg_entr 0.0025517623871564865
ep43_l4_test_time 2.98675274848938
gc 0
Train Epoch44 Acc 0.998225 (39929/40000), AUC 0.9997220039367676
ep44_train_time 89.6580228805542
Test Epoch44 layer4 Acc 0.8186, AUC 0.8065512180328369, avg_entr 0.002606012625619769
ep44_l4_test_time 2.9761013984680176
gc 0
Train Epoch45 Acc 0.99855 (39942/40000), AUC 0.9998600482940674
ep45_train_time 89.73713445663452
Test Epoch45 layer4 Acc 0.8172, AUC 0.8088992834091187, avg_entr 0.002387459622696042
ep45_l4_test_time 2.978688955307007
gc 0
Train Epoch46 Acc 0.9986 (39944/40000), AUC 0.9997323751449585
ep46_train_time 90.43192100524902
Test Epoch46 layer4 Acc 0.8164, AUC 0.817330002784729, avg_entr 0.0015493001556023955
ep46_l4_test_time 2.975615978240967
gc 0
Train Epoch47 Acc 0.998725 (39949/40000), AUC 0.9997717142105103
ep47_train_time 89.67112517356873
Test Epoch47 layer4 Acc 0.8204, AUC 0.8151676654815674, avg_entr 0.0036261684726923704
ep47_l4_test_time 2.9808387756347656
gc 0
Train Epoch48 Acc 0.99865 (39946/40000), AUC 0.9998090267181396
ep48_train_time 89.65444827079773
Test Epoch48 layer4 Acc 0.8146, AUC 0.804580569267273, avg_entr 0.0023830048739910126
ep48_l4_test_time 2.9821341037750244
gc 0
Train Epoch49 Acc 0.998975 (39959/40000), AUC 0.9998655319213867
ep49_train_time 90.42521286010742
Test Epoch49 layer4 Acc 0.8126, AUC 0.800034761428833, avg_entr 0.0026976740919053555
ep49_l4_test_time 2.9805493354797363
Best AUC 0.9560864567756653
train_loss (2, 5, 50)
valid_acc (1, 50)
valid_AUC (1, 50)
train_acc (50,)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
total_train+valid_time 5180.9017152786255
Start Testing
Load ckpt at ckpt/imdb_transformeral_l5base_pad300//imdb_transformeral_l5.pt
Test layer4 Acc 0.8908, AUC 0.9549686908721924, avg_entr 0.05792711302638054
l4_test_time 2.973292827606201
