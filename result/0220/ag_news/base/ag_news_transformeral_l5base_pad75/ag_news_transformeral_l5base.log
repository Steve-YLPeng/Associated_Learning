total count words 102019
vocab size 30000
train size 120000, valid size 3800, test size 3800
found 26754 words in glove
model: TransformerModelML(
  (layers): ModuleList(
    (0): EMBLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Embedding(30000, 300)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=4, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=4, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.weight 9000000
layers.0.ae.g.0.weight 512
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 512
layers.0.ae.h.0.bias 4
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
Total Trainable Params: 13672292
init_time 19.56470012664795
Start Training
gc 0
Train Epoch0 Acc 0.6116666666666667 (73400/120000), AUC 0.8477181792259216
ep0_train_time 69.2627944946289
Test Epoch0 layer4 Acc 0.9063157894736842, AUC 0.9804700016975403, avg_entr 0.14457391202449799
ep0_l4_test_time 0.6012611389160156
Save ckpt to ckpt/ag_news_transformeral_l5base_pad75//ag_news_transformeral_l5.pt  ,ep 0
gc 0
Train Epoch1 Acc 0.9248916666666667 (110987/120000), AUC 0.9827657341957092
ep1_train_time 68.21046948432922
Test Epoch1 layer4 Acc 0.9239473684210526, AUC 0.983733594417572, avg_entr 0.05587068200111389
ep1_l4_test_time 0.6073629856109619
Save ckpt to ckpt/ag_news_transformeral_l5base_pad75//ag_news_transformeral_l5.pt  ,ep 1
gc 0
Train Epoch2 Acc 0.9378416666666667 (112541/120000), AUC 0.9875708222389221
ep2_train_time 68.1286678314209
Test Epoch2 layer4 Acc 0.9236842105263158, AUC 0.9842290878295898, avg_entr 0.03199348971247673
ep2_l4_test_time 0.5578789710998535
Save ckpt to ckpt/ag_news_transformeral_l5base_pad75//ag_news_transformeral_l5.pt  ,ep 2
gc 0
Train Epoch3 Acc 0.9457333333333333 (113488/120000), AUC 0.9897435903549194
ep3_train_time 67.82433700561523
Test Epoch3 layer4 Acc 0.9239473684210526, AUC 0.983726978302002, avg_entr 0.023616768419742584
ep3_l4_test_time 0.5855331420898438
gc 0
Train Epoch4 Acc 0.9508916666666667 (114107/120000), AUC 0.9911402463912964
ep4_train_time 68.16897439956665
Test Epoch4 layer4 Acc 0.9210526315789473, AUC 0.9836704730987549, avg_entr 0.01969527266919613
ep4_l4_test_time 0.5917887687683105
gc 0
Train Epoch5 Acc 0.9545666666666667 (114548/120000), AUC 0.992093563079834
ep5_train_time 68.28160524368286
Test Epoch5 layer4 Acc 0.9205263157894736, AUC 0.9793480634689331, avg_entr 0.016212211921811104
ep5_l4_test_time 0.5922596454620361
gc 0
Train Epoch6 Acc 0.9578333333333333 (114940/120000), AUC 0.9931067228317261
ep6_train_time 67.99109959602356
Test Epoch6 layer4 Acc 0.9173684210526316, AUC 0.9813161492347717, avg_entr 0.014466028660535812
ep6_l4_test_time 0.5928361415863037
gc 0
Train Epoch7 Acc 0.96085 (115302/120000), AUC 0.9936410784721375
ep7_train_time 68.32437252998352
Test Epoch7 layer4 Acc 0.9168421052631579, AUC 0.9810101985931396, avg_entr 0.013220909982919693
ep7_l4_test_time 0.6070327758789062
gc 0
Train Epoch8 Acc 0.9628416666666667 (115541/120000), AUC 0.9942003488540649
ep8_train_time 68.2864716053009
Test Epoch8 layer4 Acc 0.9160526315789473, AUC 0.9809451699256897, avg_entr 0.011599035002291203
ep8_l4_test_time 0.5892236232757568
gc 0
Train Epoch9 Acc 0.9647666666666667 (115772/120000), AUC 0.9945242404937744
ep9_train_time 67.84199857711792
Test Epoch9 layer4 Acc 0.9157894736842105, AUC 0.9810227155685425, avg_entr 0.010200396180152893
ep9_l4_test_time 0.563018798828125
gc 0
Train Epoch10 Acc 0.9665333333333334 (115984/120000), AUC 0.9949253797531128
ep10_train_time 68.42033195495605
Test Epoch10 layer4 Acc 0.9144736842105263, AUC 0.9794780611991882, avg_entr 0.01039796695113182
ep10_l4_test_time 0.5924162864685059
gc 0
Train Epoch11 Acc 0.968375 (116205/120000), AUC 0.9952647089958191
ep11_train_time 68.11460542678833
Test Epoch11 layer4 Acc 0.9144736842105263, AUC 0.9796321392059326, avg_entr 0.009109055623412132
ep11_l4_test_time 0.5944974422454834
gc 0
Train Epoch12 Acc 0.970375 (116445/120000), AUC 0.9953895211219788
ep12_train_time 68.24428296089172
Test Epoch12 layer4 Acc 0.9160526315789473, AUC 0.9761805534362793, avg_entr 0.009053273126482964
ep12_l4_test_time 0.5990447998046875
gc 0
Train Epoch13 Acc 0.971725 (116607/120000), AUC 0.995713472366333
ep13_train_time 67.92189693450928
Test Epoch13 layer4 Acc 0.9144736842105263, AUC 0.9731202125549316, avg_entr 0.008026550523936749
ep13_l4_test_time 0.611506462097168
gc 0
Train Epoch14 Acc 0.973875 (116865/120000), AUC 0.995836615562439
ep14_train_time 68.20311284065247
Test Epoch14 layer4 Acc 0.9126315789473685, AUC 0.9759091138839722, avg_entr 0.007937589660286903
ep14_l4_test_time 0.5856387615203857
gc 0
Train Epoch15 Acc 0.975225 (117027/120000), AUC 0.9960149526596069
ep15_train_time 68.25107502937317
Test Epoch15 layer4 Acc 0.9086842105263158, AUC 0.9740754961967468, avg_entr 0.007688732352107763
ep15_l4_test_time 0.5900442600250244
gc 0
Train Epoch16 Acc 0.9768583333333334 (117223/120000), AUC 0.9961576461791992
ep16_train_time 67.97916507720947
Test Epoch16 layer4 Acc 0.9123684210526316, AUC 0.9718643426895142, avg_entr 0.006141805090010166
ep16_l4_test_time 0.585059642791748
gc 0
Train Epoch17 Acc 0.9779333333333333 (117352/120000), AUC 0.9964683055877686
ep17_train_time 68.32178616523743
Test Epoch17 layer4 Acc 0.9094736842105263, AUC 0.9711105823516846, avg_entr 0.006265287287533283
ep17_l4_test_time 0.6149108409881592
gc 0
Train Epoch18 Acc 0.9791166666666666 (117494/120000), AUC 0.9966577291488647
ep18_train_time 68.04529762268066
Test Epoch18 layer4 Acc 0.9078947368421053, AUC 0.9695027470588684, avg_entr 0.004849396180361509
ep18_l4_test_time 0.597869873046875
gc 0
Train Epoch19 Acc 0.980425 (117651/120000), AUC 0.9967613220214844
ep19_train_time 68.27081990242004
Test Epoch19 layer4 Acc 0.9036842105263158, AUC 0.9684292078018188, avg_entr 0.005300779826939106
ep19_l4_test_time 0.5903384685516357
gc 0
Train Epoch20 Acc 0.9816583333333333 (117799/120000), AUC 0.9968174695968628
ep20_train_time 68.01913499832153
Test Epoch20 layer4 Acc 0.9084210526315789, AUC 0.9703209400177002, avg_entr 0.0042502386495471
ep20_l4_test_time 0.5976238250732422
gc 0
Train Epoch21 Acc 0.9826166666666667 (117914/120000), AUC 0.9971282482147217
ep21_train_time 68.38610887527466
Test Epoch21 layer4 Acc 0.905, AUC 0.9659852981567383, avg_entr 0.006886009592562914
ep21_l4_test_time 0.5982518196105957
gc 0
Train Epoch22 Acc 0.9835583333333333 (118027/120000), AUC 0.9972950220108032
ep22_train_time 68.12526345252991
Test Epoch22 layer4 Acc 0.9063157894736842, AUC 0.9689826965332031, avg_entr 0.005273556802421808
ep22_l4_test_time 0.6109745502471924
gc 0
Train Epoch23 Acc 0.9848166666666667 (118178/120000), AUC 0.9973657727241516
ep23_train_time 67.94067239761353
Test Epoch23 layer4 Acc 0.9036842105263158, AUC 0.9612977504730225, avg_entr 0.0063944594003260136
ep23_l4_test_time 0.6004459857940674
gc 0
Train Epoch24 Acc 0.9851666666666666 (118220/120000), AUC 0.9975295066833496
ep24_train_time 68.0455276966095
Test Epoch24 layer4 Acc 0.9023684210526316, AUC 0.965706467628479, avg_entr 0.006931704469025135
ep24_l4_test_time 0.5864248275756836
gc 0
Train Epoch25 Acc 0.985875 (118305/120000), AUC 0.9975384473800659
ep25_train_time 68.37862730026245
Test Epoch25 layer4 Acc 0.9023684210526316, AUC 0.9672972559928894, avg_entr 0.005053672473877668
ep25_l4_test_time 0.6205601692199707
gc 0
Train Epoch26 Acc 0.9864416666666667 (118373/120000), AUC 0.9977186918258667
ep26_train_time 68.04151892662048
Test Epoch26 layer4 Acc 0.9047368421052632, AUC 0.9608224630355835, avg_entr 0.004818700719624758
ep26_l4_test_time 0.5942347049713135
gc 0
Train Epoch27 Acc 0.9872083333333334 (118465/120000), AUC 0.9979251623153687
ep27_train_time 67.95226716995239
Test Epoch27 layer4 Acc 0.901578947368421, AUC 0.9649353623390198, avg_entr 0.005440519191324711
ep27_l4_test_time 0.5983564853668213
gc 0
Train Epoch28 Acc 0.9876416666666666 (118517/120000), AUC 0.9980058670043945
ep28_train_time 68.20283484458923
Test Epoch28 layer4 Acc 0.8986842105263158, AUC 0.9632712602615356, avg_entr 0.004593982361257076
ep28_l4_test_time 0.601811408996582
gc 0
Train Epoch29 Acc 0.9884166666666667 (118610/120000), AUC 0.9979752898216248
ep29_train_time 68.21497368812561
Test Epoch29 layer4 Acc 0.9005263157894737, AUC 0.9580911993980408, avg_entr 0.004856356419622898
ep29_l4_test_time 0.5933077335357666
gc 0
Train Epoch30 Acc 0.988975 (118677/120000), AUC 0.9980624914169312
ep30_train_time 68.16998028755188
Test Epoch30 layer4 Acc 0.9013157894736842, AUC 0.9626460075378418, avg_entr 0.004790798295289278
ep30_l4_test_time 0.5946731567382812
gc 0
Train Epoch31 Acc 0.9894166666666667 (118730/120000), AUC 0.9982366561889648
ep31_train_time 68.28022646903992
Test Epoch31 layer4 Acc 0.9060526315789473, AUC 0.9626393914222717, avg_entr 0.0034651749301701784
ep31_l4_test_time 0.594449520111084
gc 0
Train Epoch32 Acc 0.989925 (118791/120000), AUC 0.9982325434684753
ep32_train_time 68.39842510223389
Test Epoch32 layer4 Acc 0.9073684210526316, AUC 0.964078426361084, avg_entr 0.0046776398085057735
ep32_l4_test_time 0.5933260917663574
gc 0
Train Epoch33 Acc 0.990175 (118821/120000), AUC 0.9983581304550171
ep33_train_time 68.33623576164246
Test Epoch33 layer4 Acc 0.9021052631578947, AUC 0.9608089923858643, avg_entr 0.004654783755540848
ep33_l4_test_time 0.5974874496459961
gc 0
Train Epoch34 Acc 0.9905083333333333 (118861/120000), AUC 0.9983251094818115
ep34_train_time 68.03558707237244
Test Epoch34 layer4 Acc 0.9, AUC 0.9564417004585266, avg_entr 0.004740212112665176
ep34_l4_test_time 0.592648983001709
gc 0
Train Epoch35 Acc 0.9910333333333333 (118924/120000), AUC 0.9982742071151733
ep35_train_time 68.29906415939331
Test Epoch35 layer4 Acc 0.9026315789473685, AUC 0.9564482569694519, avg_entr 0.004063402768224478
ep35_l4_test_time 0.5816302299499512
gc 0
Train Epoch36 Acc 0.9914833333333334 (118978/120000), AUC 0.9984742403030396
ep36_train_time 68.38003015518188
Test Epoch36 layer4 Acc 0.9060526315789473, AUC 0.9587864875793457, avg_entr 0.004080855753272772
ep36_l4_test_time 0.606104850769043
gc 0
Train Epoch37 Acc 0.991825 (119019/120000), AUC 0.9984691143035889
ep37_train_time 68.01304125785828
Test Epoch37 layer4 Acc 0.8978947368421053, AUC 0.9578027129173279, avg_entr 0.003939544316381216
ep37_l4_test_time 0.592214822769165
gc 0
Train Epoch38 Acc 0.9919083333333333 (119029/120000), AUC 0.9985184669494629
ep38_train_time 68.43465662002563
Test Epoch38 layer4 Acc 0.9002631578947369, AUC 0.9602931141853333, avg_entr 0.0038584056310355663
ep38_l4_test_time 0.5844554901123047
gc 0
Train Epoch39 Acc 0.992475 (119097/120000), AUC 0.9985791444778442
ep39_train_time 68.40374684333801
Test Epoch39 layer4 Acc 0.9039473684210526, AUC 0.9582127332687378, avg_entr 0.0032609477639198303
ep39_l4_test_time 0.5872409343719482
gc 0
Train Epoch40 Acc 0.9926 (119112/120000), AUC 0.9986501932144165
ep40_train_time 68.35311627388
Test Epoch40 layer4 Acc 0.9002631578947369, AUC 0.9549648761749268, avg_entr 0.003623046213760972
ep40_l4_test_time 0.5593762397766113
gc 0
Train Epoch41 Acc 0.9928583333333333 (119143/120000), AUC 0.9987098574638367
ep41_train_time 68.09618592262268
Test Epoch41 layer4 Acc 0.9023684210526316, AUC 0.9545071125030518, avg_entr 0.002713855355978012
ep41_l4_test_time 0.5994923114776611
gc 0
Train Epoch42 Acc 0.9930083333333334 (119161/120000), AUC 0.9986382722854614
ep42_train_time 68.2774555683136
Test Epoch42 layer4 Acc 0.9018421052631579, AUC 0.9594743847846985, avg_entr 0.003312935819849372
ep42_l4_test_time 0.5940806865692139
gc 0
Train Epoch43 Acc 0.9934166666666666 (119210/120000), AUC 0.998758852481842
ep43_train_time 68.38932943344116
Test Epoch43 layer4 Acc 0.9010526315789473, AUC 0.9540678262710571, avg_entr 0.00312146102078259
ep43_l4_test_time 0.6034111976623535
gc 0
Train Epoch44 Acc 0.9934333333333333 (119212/120000), AUC 0.9987552165985107
ep44_train_time 68.01281571388245
Test Epoch44 layer4 Acc 0.9042105263157895, AUC 0.9494156837463379, avg_entr 0.0031355745159089565
ep44_l4_test_time 0.5998868942260742
gc 0
Train Epoch45 Acc 0.9936083333333333 (119233/120000), AUC 0.9988014698028564
ep45_train_time 68.26716947555542
Test Epoch45 layer4 Acc 0.9042105263157895, AUC 0.9546866416931152, avg_entr 0.002801052061840892
ep45_l4_test_time 0.6025083065032959
gc 0
Train Epoch46 Acc 0.9938416666666666 (119261/120000), AUC 0.9988812804222107
ep46_train_time 68.19691562652588
Test Epoch46 layer4 Acc 0.9018421052631579, AUC 0.9526527523994446, avg_entr 0.0032524261623620987
ep46_l4_test_time 0.5924942493438721
gc 0
Train Epoch47 Acc 0.993975 (119277/120000), AUC 0.998844563961029
ep47_train_time 68.07021355628967
Test Epoch47 layer4 Acc 0.9007894736842105, AUC 0.9559009671211243, avg_entr 0.00354296644218266
ep47_l4_test_time 0.5925242900848389
gc 0
Train Epoch48 Acc 0.994325 (119319/120000), AUC 0.9989360570907593
ep48_train_time 68.20374417304993
Test Epoch48 layer4 Acc 0.8989473684210526, AUC 0.9551080465316772, avg_entr 0.003368550445884466
ep48_l4_test_time 0.5911452770233154
gc 0
Train Epoch49 Acc 0.9943333333333333 (119320/120000), AUC 0.9989097118377686
ep49_train_time 68.24974393844604
Test Epoch49 layer4 Acc 0.9034210526315789, AUC 0.9539949297904968, avg_entr 0.002594362013041973
ep49_l4_test_time 0.5955817699432373
Best AUC 0.9842290878295898
train_loss (2, 5, 50)
valid_acc (1, 50)
valid_AUC (1, 50)
train_acc (50,)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
total_train+valid_time 3443.011662006378
Start Testing
Load ckpt at ckpt/ag_news_transformeral_l5base_pad75//ag_news_transformeral_l5.pt
Test layer4 Acc 0.9123684210526316, AUC 0.9820185899734497, avg_entr 0.034799519926309586
l4_test_time 0.5853941440582275
