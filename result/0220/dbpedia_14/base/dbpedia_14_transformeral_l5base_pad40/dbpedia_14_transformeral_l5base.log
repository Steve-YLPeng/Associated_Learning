total count words 887881
vocab size 30000
train size 560000, valid size 35000, test size 35000
found 28354 words in glove
model: TransformerModelML(
  (layers): ModuleList(
    (0): EMBLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Embedding(30000, 300)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=14, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=14, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.weight 9000000
layers.0.ae.g.0.weight 1792
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 1792
layers.0.ae.h.0.bias 14
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
Total Trainable Params: 13674862
init_time 57.60438561439514
Start Training
gc 0
Train Epoch0 Acc 0.8426642857142858 (471892/560000), AUC 0.981717050075531
ep0_train_time 212.9515643119812
Test Epoch0 layer4 Acc 0.9736, AUC 0.9978832602500916, avg_entr 0.02311915159225464
ep0_l4_test_time 3.9153554439544678
Save ckpt to ckpt/dbpedia_14_transformeral_l5base_pad40//dbpedia_14_transformeral_l5.pt  ,ep 0
gc 0
Train Epoch1 Acc 0.9798964285714286 (548742/560000), AUC 0.9975438714027405
ep1_train_time 232.9336380958557
Test Epoch1 layer4 Acc 0.9785142857142857, AUC 0.9974198341369629, avg_entr 0.0055793351493775845
ep1_l4_test_time 3.869577407836914
gc 0
Train Epoch2 Acc 0.9841071428571428 (551100/560000), AUC 0.9978389739990234
ep2_train_time 232.7627124786377
Test Epoch2 layer4 Acc 0.9789714285714286, AUC 0.9970957040786743, avg_entr 0.0032050362788140774
ep2_l4_test_time 3.9205026626586914
gc 0
Train Epoch3 Acc 0.9857910714285715 (552043/560000), AUC 0.9980149865150452
ep3_train_time 232.60272097587585
Test Epoch3 layer4 Acc 0.9797142857142858, AUC 0.996727466583252, avg_entr 0.0024084309116005898
ep3_l4_test_time 3.843759536743164
gc 0
Train Epoch4 Acc 0.9869875 (552713/560000), AUC 0.9981898665428162
ep4_train_time 232.88912415504456
Test Epoch4 layer4 Acc 0.9795142857142857, AUC 0.9967182278633118, avg_entr 0.0018888992490246892
ep4_l4_test_time 3.819337844848633
gc 0
Train Epoch5 Acc 0.9879910714285715 (553275/560000), AUC 0.9983552098274231
ep5_train_time 232.9336040019989
Test Epoch5 layer4 Acc 0.9792571428571428, AUC 0.9966612458229065, avg_entr 0.0019781512673944235
ep5_l4_test_time 3.861708641052246
gc 0
Train Epoch6 Acc 0.9887839285714286 (553719/560000), AUC 0.9985716938972473
ep6_train_time 214.70684695243835
Test Epoch6 layer4 Acc 0.9792571428571428, AUC 0.9964379072189331, avg_entr 0.0018580196192488074
ep6_l4_test_time 3.868572235107422
gc 0
Train Epoch7 Acc 0.9894089285714286 (554069/560000), AUC 0.9986304640769958
ep7_train_time 232.4163863658905
Test Epoch7 layer4 Acc 0.9790285714285715, AUC 0.9965512156486511, avg_entr 0.0021530762314796448
ep7_l4_test_time 3.889970541000366
gc 0
Train Epoch8 Acc 0.9899571428571429 (554376/560000), AUC 0.998687744140625
ep8_train_time 232.27100920677185
Test Epoch8 layer4 Acc 0.9793714285714286, AUC 0.9959000945091248, avg_entr 0.0020150786731392145
ep8_l4_test_time 3.866238594055176
gc 0
Train Epoch9 Acc 0.9904696428571429 (554663/560000), AUC 0.9986879229545593
ep9_train_time 232.3002154827118
Test Epoch9 layer4 Acc 0.9797428571428571, AUC 0.9956779479980469, avg_entr 0.0015517715364694595
ep9_l4_test_time 3.79266619682312
gc 0
Train Epoch10 Acc 0.9910071428571429 (554964/560000), AUC 0.9987287521362305
ep10_train_time 232.3360240459442
Test Epoch10 layer4 Acc 0.9798, AUC 0.995531439781189, avg_entr 0.0018257631454616785
ep10_l4_test_time 3.8754398822784424
gc 0
Train Epoch11 Acc 0.9913678571428571 (555166/560000), AUC 0.9987807273864746
ep11_train_time 232.47976970672607
Test Epoch11 layer4 Acc 0.9792571428571428, AUC 0.9952747225761414, avg_entr 0.001811626716516912
ep11_l4_test_time 3.869288206100464
gc 0
Train Epoch12 Acc 0.9918339285714286 (555427/560000), AUC 0.9988080859184265
ep12_train_time 232.50401258468628
Test Epoch12 layer4 Acc 0.9786571428571429, AUC 0.9952747225761414, avg_entr 0.0016678035026416183
ep12_l4_test_time 3.812150478363037
gc 0
Train Epoch13 Acc 0.9921964285714285 (555630/560000), AUC 0.9988086819648743
ep13_train_time 214.1919674873352
Test Epoch13 layer4 Acc 0.9788285714285714, AUC 0.9948630928993225, avg_entr 0.001767415669746697
ep13_l4_test_time 3.872587203979492
gc 0
Train Epoch14 Acc 0.9925017857142857 (555801/560000), AUC 0.9988144040107727
ep14_train_time 232.7299587726593
Test Epoch14 layer4 Acc 0.9789142857142857, AUC 0.9952424168586731, avg_entr 0.0015920045552775264
ep14_l4_test_time 3.8748035430908203
gc 0
Train Epoch15 Acc 0.9928017857142857 (555969/560000), AUC 0.9988675117492676
ep15_train_time 232.86050295829773
Test Epoch15 layer4 Acc 0.9780571428571428, AUC 0.9950194954872131, avg_entr 0.0015970536042004824
ep15_l4_test_time 3.8131322860717773
gc 0
Train Epoch16 Acc 0.9931553571428572 (556167/560000), AUC 0.9988645315170288
ep16_train_time 232.98260521888733
Test Epoch16 layer4 Acc 0.9786571428571429, AUC 0.9949051141738892, avg_entr 0.001680440385825932
ep16_l4_test_time 3.892237663269043
gc 0
Train Epoch17 Acc 0.9934803571428571 (556349/560000), AUC 0.9989185333251953
ep17_train_time 232.58247137069702
Test Epoch17 layer4 Acc 0.9785428571428572, AUC 0.9946483969688416, avg_entr 0.0015858534025028348
ep17_l4_test_time 3.871006727218628
gc 0
Train Epoch18 Acc 0.9937214285714285 (556484/560000), AUC 0.9989207983016968
ep18_train_time 233.120831489563
Test Epoch18 layer4 Acc 0.9781142857142857, AUC 0.9944230318069458, avg_entr 0.001638126326724887
ep18_l4_test_time 3.8449177742004395
gc 0
Train Epoch19 Acc 0.9938875 (556577/560000), AUC 0.998953640460968
ep19_train_time 215.27431058883667
Test Epoch19 layer4 Acc 0.9780571428571428, AUC 0.9940689206123352, avg_entr 0.001593084423802793
ep19_l4_test_time 3.8038132190704346
gc 0
Train Epoch20 Acc 0.9942589285714286 (556785/560000), AUC 0.9989520311355591
ep20_train_time 232.92136669158936
Test Epoch20 layer4 Acc 0.9779428571428571, AUC 0.9939454197883606, avg_entr 0.0017287327209487557
ep20_l4_test_time 3.8929922580718994
gc 0
Train Epoch21 Acc 0.9944553571428572 (556895/560000), AUC 0.9989585280418396
ep21_train_time 232.95234847068787
Test Epoch21 layer4 Acc 0.9779142857142857, AUC 0.9942294955253601, avg_entr 0.0014303603675216436
ep21_l4_test_time 3.9081456661224365
gc 0
Train Epoch22 Acc 0.9945464285714286 (556946/560000), AUC 0.9990411996841431
ep22_train_time 232.973046541214
Test Epoch22 layer4 Acc 0.9771428571428571, AUC 0.99357670545578, avg_entr 0.001455938327126205
ep22_l4_test_time 3.8755857944488525
gc 0
Train Epoch23 Acc 0.9949089285714285 (557149/560000), AUC 0.9990260004997253
ep23_train_time 233.23268151283264
Test Epoch23 layer4 Acc 0.9769714285714286, AUC 0.9936223030090332, avg_entr 0.0016072128200903535
ep23_l4_test_time 3.8731112480163574
gc 0
Train Epoch24 Acc 0.9950767857142857 (557243/560000), AUC 0.9990530610084534
ep24_train_time 233.02411198616028
Test Epoch24 layer4 Acc 0.9771142857142857, AUC 0.9937576055526733, avg_entr 0.0012739574303850532
ep24_l4_test_time 3.849261999130249
gc 0
Train Epoch25 Acc 0.9952285714285715 (557328/560000), AUC 0.9990715384483337
ep25_train_time 233.08694744110107
Test Epoch25 layer4 Acc 0.9768857142857142, AUC 0.9932912588119507, avg_entr 0.0012705577537417412
ep25_l4_test_time 3.8572185039520264
gc 0
Train Epoch26 Acc 0.9953464285714285 (557394/560000), AUC 0.9990869760513306
ep26_train_time 212.62861132621765
Test Epoch26 layer4 Acc 0.9769428571428571, AUC 0.9934231638908386, avg_entr 0.0015053128590807319
ep26_l4_test_time 3.8732943534851074
gc 0
Train Epoch27 Acc 0.9954642857142857 (557460/560000), AUC 0.9991138577461243
ep27_train_time 233.19666242599487
Test Epoch27 layer4 Acc 0.9771142857142857, AUC 0.9934527277946472, avg_entr 0.0014454019255936146
ep27_l4_test_time 3.855619430541992
gc 0
Train Epoch28 Acc 0.9955535714285715 (557510/560000), AUC 0.9991345405578613
ep28_train_time 232.958354473114
Test Epoch28 layer4 Acc 0.9765714285714285, AUC 0.9931685328483582, avg_entr 0.001192591618746519
ep28_l4_test_time 3.879634141921997
gc 0
Train Epoch29 Acc 0.9958107142857143 (557654/560000), AUC 0.9991410970687866
ep29_train_time 232.92178654670715
Test Epoch29 layer4 Acc 0.9762285714285714, AUC 0.9930801391601562, avg_entr 0.0013265640009194613
ep29_l4_test_time 3.850092887878418
gc 0
Train Epoch30 Acc 0.9958607142857143 (557682/560000), AUC 0.9991958737373352
ep30_train_time 232.92491030693054
Test Epoch30 layer4 Acc 0.9772, AUC 0.993061363697052, avg_entr 0.0012391268974170089
ep30_l4_test_time 3.8587796688079834
gc 0
Train Epoch31 Acc 0.9961464285714285 (557842/560000), AUC 0.9991697669029236
ep31_train_time 232.94980335235596
Test Epoch31 layer4 Acc 0.9769142857142857, AUC 0.9932031035423279, avg_entr 0.0014106474118307233
ep31_l4_test_time 3.8722617626190186
gc 0
Train Epoch32 Acc 0.9960875 (557809/560000), AUC 0.9992193579673767
ep32_train_time 210.83663702011108
Test Epoch32 layer4 Acc 0.9776571428571429, AUC 0.9934029579162598, avg_entr 0.0013915699673816562
ep32_l4_test_time 3.8180668354034424
gc 0
Train Epoch33 Acc 0.9963160714285715 (557937/560000), AUC 0.9992368817329407
ep33_train_time 234.71871876716614
Test Epoch33 layer4 Acc 0.9767714285714286, AUC 0.9926637411117554, avg_entr 0.0011360255302861333
ep33_l4_test_time 3.8501405715942383
gc 0
Train Epoch34 Acc 0.9963785714285714 (557972/560000), AUC 0.9992218613624573
ep34_train_time 235.01596403121948
Test Epoch34 layer4 Acc 0.9757714285714286, AUC 0.9925436973571777, avg_entr 0.0011814498575404286
ep34_l4_test_time 3.813870668411255
gc 0
Train Epoch35 Acc 0.9964392857142857 (558006/560000), AUC 0.9992805123329163
ep35_train_time 234.60780572891235
Test Epoch35 layer4 Acc 0.9763142857142857, AUC 0.9925447702407837, avg_entr 0.0013175426283851266
ep35_l4_test_time 3.8688206672668457
gc 0
Train Epoch36 Acc 0.9965839285714285 (558087/560000), AUC 0.9992794990539551
ep36_train_time 234.68554782867432
Test Epoch36 layer4 Acc 0.9769142857142857, AUC 0.9925500154495239, avg_entr 0.0011889876332134008
ep36_l4_test_time 3.757568836212158
gc 0
Train Epoch37 Acc 0.9967125 (558159/560000), AUC 0.9992873072624207
ep37_train_time 234.86018133163452
Test Epoch37 layer4 Acc 0.9761714285714286, AUC 0.9930302500724792, avg_entr 0.001174623379483819
ep37_l4_test_time 3.840543746948242
gc 0
Train Epoch38 Acc 0.9967803571428572 (558197/560000), AUC 0.9992888569831848
ep38_train_time 224.91206049919128
Test Epoch38 layer4 Acc 0.9758285714285714, AUC 0.9930570721626282, avg_entr 0.0011953541543334723
ep38_l4_test_time 1.7818381786346436
gc 0
Train Epoch39 Acc 0.9967964285714286 (558206/560000), AUC 0.9993202090263367
ep39_train_time 228.47309041023254
Test Epoch39 layer4 Acc 0.9763714285714286, AUC 0.9924556612968445, avg_entr 0.0014281657058745623
ep39_l4_test_time 3.806121826171875
gc 0
Train Epoch40 Acc 0.9969571428571429 (558296/560000), AUC 0.9993481636047363
ep40_train_time 234.9293897151947
Test Epoch40 layer4 Acc 0.9756285714285714, AUC 0.9920275807380676, avg_entr 0.0011033390183001757
ep40_l4_test_time 3.8617660999298096
gc 0
Train Epoch41 Acc 0.9970375 (558341/560000), AUC 0.9993377923965454
ep41_train_time 234.6420557498932
Test Epoch41 layer4 Acc 0.9766571428571429, AUC 0.992728054523468, avg_entr 0.0012989436509087682
ep41_l4_test_time 3.8190970420837402
gc 0
Train Epoch42 Acc 0.9970732142857143 (558361/560000), AUC 0.9993411302566528
ep42_train_time 234.6849970817566
Test Epoch42 layer4 Acc 0.9763142857142857, AUC 0.9927456974983215, avg_entr 0.0012716441415250301
ep42_l4_test_time 3.8215034008026123
gc 0
Train Epoch43 Acc 0.9970946428571429 (558373/560000), AUC 0.9993492960929871
ep43_train_time 234.92045140266418
Test Epoch43 layer4 Acc 0.9758857142857142, AUC 0.9919910430908203, avg_entr 0.0010678316466510296
ep43_l4_test_time 3.785374879837036
gc 0
Train Epoch44 Acc 0.9972410714285714 (558455/560000), AUC 0.9993738532066345
ep44_train_time 234.93505144119263
Test Epoch44 layer4 Acc 0.9762571428571428, AUC 0.9924761652946472, avg_entr 0.001294430112466216
ep44_l4_test_time 3.829030752182007
gc 0
Train Epoch45 Acc 0.9973071428571428 (558492/560000), AUC 0.9993602633476257
ep45_train_time 215.879296541214
Test Epoch45 layer4 Acc 0.9765428571428572, AUC 0.9920473694801331, avg_entr 0.0011411099694669247
ep45_l4_test_time 3.7891499996185303
gc 0
Train Epoch46 Acc 0.9973642857142857 (558524/560000), AUC 0.9993769526481628
ep46_train_time 234.89630961418152
Test Epoch46 layer4 Acc 0.9765142857142857, AUC 0.9913965463638306, avg_entr 0.0010545850964263082
ep46_l4_test_time 3.832862377166748
gc 0
Train Epoch47 Acc 0.9973089285714286 (558493/560000), AUC 0.9993621706962585
ep47_train_time 234.58602666854858
Test Epoch47 layer4 Acc 0.9760285714285715, AUC 0.9923032522201538, avg_entr 0.0012600300833582878
ep47_l4_test_time 3.8073365688323975
gc 0
Train Epoch48 Acc 0.9974803571428571 (558589/560000), AUC 0.9993771910667419
ep48_train_time 234.9105155467987
Test Epoch48 layer4 Acc 0.9765714285714285, AUC 0.991901695728302, avg_entr 0.0012640329077839851
ep48_l4_test_time 3.781064748764038
gc 0
Train Epoch49 Acc 0.9974821428571429 (558590/560000), AUC 0.9994308352470398
ep49_train_time 234.73563814163208
Test Epoch49 layer4 Acc 0.9761428571428571, AUC 0.991490364074707, avg_entr 0.0013851536205038428
ep49_l4_test_time 3.8418633937835693
Best AUC 0.9978832602500916
train_loss (2, 5, 50)
valid_acc (1, 50)
valid_AUC (1, 50)
train_acc (50,)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
total_train+valid_time 11716.21803188324
Start Testing
Load ckpt at ckpt/dbpedia_14_transformeral_l5base_pad40//dbpedia_14_transformeral_l5.pt
Test layer4 Acc 0.9743142857142857, AUC 0.998116135597229, avg_entr 0.02319151535630226
l4_test_time 3.8331449031829834
