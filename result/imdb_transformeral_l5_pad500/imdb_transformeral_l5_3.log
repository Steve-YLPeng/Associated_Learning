total count words 222751
vocab size 30000
train size 40000, valid size 5000, test size 5000
found 27937 words in glove
model: TransformerModelML(
  (layers): ModuleList(
    (0): EMBLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Embedding(30000, 300)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=2, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=2, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.weight 9000000
layers.0.ae.g.0.weight 256
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 256
layers.0.ae.h.0.bias 2
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
Total Trainable Params: 13671778
init_time 21.523748439000002
Start Training
gc 0
Train Epoch0 Acc 0.5622 (22488/40000), AUC 0.5972161293029785
ep0_train_time 92.07829279699999
Test Epoch0 layer0 Acc 0.8154, AUC 0.8867473602294922, avg_entr 0.5608533024787903, f1 0.8154000043869019
ep0_l0_test_time 0.5598997649999973
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 0
Test Epoch0 layer1 Acc 0.823, AUC 0.9059451818466187, avg_entr 0.35224607586860657, f1 0.8230000138282776
ep0_l1_test_time 1.1882848139999993
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 0
Test Epoch0 layer2 Acc 0.8208, AUC 0.9065589904785156, avg_entr 0.4792786240577698, f1 0.8208000063896179
ep0_l2_test_time 1.8165221739999993
Test Epoch0 layer3 Acc 0.8048, AUC 0.9045767784118652, avg_entr 0.6032083034515381, f1 0.8047999739646912
ep0_l3_test_time 2.441061499
Test Epoch0 layer4 Acc 0.8082, AUC 0.9034067988395691, avg_entr 0.6162753105163574, f1 0.808199942111969
ep0_l4_test_time 3.0751632529999995
gc 0
Train Epoch1 Acc 0.858175 (34327/40000), AUC 0.9327566623687744
ep1_train_time 92.01708126599999
Test Epoch1 layer0 Acc 0.8686, AUC 0.944796085357666, avg_entr 0.28271880745887756, f1 0.8686000108718872
ep1_l0_test_time 0.5647032040000113
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 1
Test Epoch1 layer1 Acc 0.8876, AUC 0.9528083801269531, avg_entr 0.19524791836738586, f1 0.8876000046730042
ep1_l1_test_time 1.1929094350000184
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 1
Test Epoch1 layer2 Acc 0.8888, AUC 0.9540743827819824, avg_entr 0.14900311827659607, f1 0.8888000249862671
ep1_l2_test_time 1.820784322999998
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 1
Test Epoch1 layer3 Acc 0.8876, AUC 0.953336238861084, avg_entr 0.11878456920385361, f1 0.8876000046730042
ep1_l3_test_time 2.445058520999993
Test Epoch1 layer4 Acc 0.8892, AUC 0.9534748792648315, avg_entr 0.12032396346330643, f1 0.88919997215271
ep1_l4_test_time 3.0743164300000103
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 1
gc 0
Train Epoch2 Acc 0.91595 (36638/40000), AUC 0.9684966802597046
ep2_train_time 91.76328921099997
Test Epoch2 layer0 Acc 0.8906, AUC 0.9547743797302246, avg_entr 0.22345559298992157, f1 0.8906000256538391
ep2_l0_test_time 0.5638120159999858
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 2
Test Epoch2 layer1 Acc 0.8968, AUC 0.9585240483283997, avg_entr 0.14014308154582977, f1 0.8967999815940857
ep2_l1_test_time 1.190249958000038
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 2
Test Epoch2 layer2 Acc 0.8974, AUC 0.956760048866272, avg_entr 0.0705060213804245, f1 0.8974000215530396
ep2_l2_test_time 1.8247119989999874
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 2
Test Epoch2 layer3 Acc 0.8968, AUC 0.9577952027320862, avg_entr 0.05651545897126198, f1 0.8967999815940857
ep2_l3_test_time 2.4468330729999934
Test Epoch2 layer4 Acc 0.8972, AUC 0.9590917825698853, avg_entr 0.05408072471618652, f1 0.8971999883651733
ep2_l4_test_time 3.0753475089999824
gc 0
Train Epoch3 Acc 0.93965 (37586/40000), AUC 0.9803686141967773
ep3_train_time 92.027575018
Test Epoch3 layer0 Acc 0.8914, AUC 0.9579475522041321, avg_entr 0.19286635518074036, f1 0.8913999795913696
ep3_l0_test_time 0.5691241089999721
Test Epoch3 layer1 Acc 0.8928, AUC 0.954808235168457, avg_entr 0.0768745094537735, f1 0.892799973487854
ep3_l1_test_time 1.1871987129999866
Test Epoch3 layer2 Acc 0.894, AUC 0.9540614485740662, avg_entr 0.04379410669207573, f1 0.8939999938011169
ep3_l2_test_time 1.8125531520000209
Test Epoch3 layer3 Acc 0.8936, AUC 0.9554516673088074, avg_entr 0.039409954100847244, f1 0.8935999870300293
ep3_l3_test_time 2.444689522000033
Test Epoch3 layer4 Acc 0.8942, AUC 0.9555214643478394, avg_entr 0.03794077783823013, f1 0.8942000269889832
ep3_l4_test_time 3.071353894999959
gc 0
Train Epoch4 Acc 0.9522 (38088/40000), AUC 0.9850670695304871
ep4_train_time 91.84899590200001
Test Epoch4 layer0 Acc 0.8986, AUC 0.95878005027771, avg_entr 0.17469504475593567, f1 0.8985999822616577
ep4_l0_test_time 0.5598324180000418
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 4
Test Epoch4 layer1 Acc 0.8888, AUC 0.9516083002090454, avg_entr 0.052353620529174805, f1 0.8888000249862671
ep4_l1_test_time 1.19448619100001
Test Epoch4 layer2 Acc 0.8898, AUC 0.953344464302063, avg_entr 0.03763427957892418, f1 0.8898000121116638
ep4_l2_test_time 1.8146099660000345
Test Epoch4 layer3 Acc 0.8896, AUC 0.954316258430481, avg_entr 0.0369572788476944, f1 0.8895999193191528
ep4_l3_test_time 2.446755981000024
Test Epoch4 layer4 Acc 0.8884, AUC 0.9543764591217041, avg_entr 0.03588719666004181, f1 0.8884000182151794
ep4_l4_test_time 3.0725445620000755
gc 0
Train Epoch5 Acc 0.957975 (38319/40000), AUC 0.9881147146224976
ep5_train_time 91.94697287000008
Test Epoch5 layer0 Acc 0.8986, AUC 0.9583573341369629, avg_entr 0.15869192779064178, f1 0.8985999822616577
ep5_l0_test_time 0.5577939579999338
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 5
Test Epoch5 layer1 Acc 0.892, AUC 0.950636088848114, avg_entr 0.05030437186360359, f1 0.8920000195503235
ep5_l1_test_time 1.1911836350000158
Test Epoch5 layer2 Acc 0.8898, AUC 0.9530069828033447, avg_entr 0.04150614142417908, f1 0.8898000121116638
ep5_l2_test_time 1.8095899019999706
Test Epoch5 layer3 Acc 0.891, AUC 0.9532044529914856, avg_entr 0.041449956595897675, f1 0.890999972820282
ep5_l3_test_time 2.4387935529999822
Test Epoch5 layer4 Acc 0.8914, AUC 0.9530408978462219, avg_entr 0.040070436894893646, f1 0.8913999795913696
ep5_l4_test_time 3.073484338999947
gc 0
Train Epoch6 Acc 0.96285 (38514/40000), AUC 0.9888533353805542
ep6_train_time 91.84152008199999
Test Epoch6 layer0 Acc 0.8932, AUC 0.9572007656097412, avg_entr 0.14879503846168518, f1 0.8931999802589417
ep6_l0_test_time 0.5588467739999032
Test Epoch6 layer1 Acc 0.8874, AUC 0.9435306191444397, avg_entr 0.037080321460962296, f1 0.8873999714851379
ep6_l1_test_time 1.187303572000019
Test Epoch6 layer2 Acc 0.8874, AUC 0.9495435953140259, avg_entr 0.028331900015473366, f1 0.8873999714851379
ep6_l2_test_time 1.809294088999991
Test Epoch6 layer3 Acc 0.8878, AUC 0.9499756693840027, avg_entr 0.027610545977950096, f1 0.8877999782562256
ep6_l3_test_time 2.4372272429999384
Test Epoch6 layer4 Acc 0.887, AUC 0.94978928565979, avg_entr 0.02612057514488697, f1 0.8870000243186951
ep6_l4_test_time 3.0674743780000426
gc 0
Train Epoch7 Acc 0.967 (38680/40000), AUC 0.9916914701461792
ep7_train_time 91.78685790999998
Test Epoch7 layer0 Acc 0.8854, AUC 0.9554620981216431, avg_entr 0.14050810039043427, f1 0.8853999972343445
ep7_l0_test_time 0.5575481660000605
Test Epoch7 layer1 Acc 0.8784, AUC 0.9408812522888184, avg_entr 0.03380216658115387, f1 0.8784000277519226
ep7_l1_test_time 1.1898801559999583
Test Epoch7 layer2 Acc 0.8804, AUC 0.9477487206459045, avg_entr 0.02555534616112709, f1 0.8804000020027161
ep7_l2_test_time 1.8206935660000454
Test Epoch7 layer3 Acc 0.8804, AUC 0.948753833770752, avg_entr 0.02490236796438694, f1 0.8804000020027161
ep7_l3_test_time 2.4379161210000575
Test Epoch7 layer4 Acc 0.88, AUC 0.9483829736709595, avg_entr 0.023750394582748413, f1 0.8799999952316284
ep7_l4_test_time 3.0677215030000298
gc 0
Train Epoch8 Acc 0.96955 (38782/40000), AUC 0.9924305081367493
ep8_train_time 91.86001347399997
Test Epoch8 layer0 Acc 0.894, AUC 0.9543466567993164, avg_entr 0.13554254174232483, f1 0.8939999938011169
ep8_l0_test_time 0.5593523469999582
Test Epoch8 layer1 Acc 0.877, AUC 0.9362558126449585, avg_entr 0.03027484379708767, f1 0.8769999742507935
ep8_l1_test_time 1.1908407010000701
Test Epoch8 layer2 Acc 0.8776, AUC 0.9451837539672852, avg_entr 0.02316480129957199, f1 0.8776000142097473
ep8_l2_test_time 1.8124635350000062
Test Epoch8 layer3 Acc 0.8774, AUC 0.9462289214134216, avg_entr 0.02242608554661274, f1 0.8773999810218811
ep8_l3_test_time 2.439284255000075
Test Epoch8 layer4 Acc 0.878, AUC 0.9460198879241943, avg_entr 0.021362479776144028, f1 0.878000020980835
ep8_l4_test_time 3.073495786999956
gc 0
Train Epoch9 Acc 0.973425 (38937/40000), AUC 0.9932800531387329
ep9_train_time 91.86013236199994
Test Epoch9 layer0 Acc 0.8912, AUC 0.9533689618110657, avg_entr 0.13515117764472961, f1 0.8912000060081482
ep9_l0_test_time 0.5576353849999123
Test Epoch9 layer1 Acc 0.8792, AUC 0.9347318410873413, avg_entr 0.02942390739917755, f1 0.8791999816894531
ep9_l1_test_time 1.1905221869999423
Test Epoch9 layer2 Acc 0.8802, AUC 0.942290186882019, avg_entr 0.021598706021904945, f1 0.8802000284194946
ep9_l2_test_time 1.8120644860000539
Test Epoch9 layer3 Acc 0.8802, AUC 0.9450268745422363, avg_entr 0.020607514306902885, f1 0.8802000284194946
ep9_l3_test_time 2.443910525999854
Test Epoch9 layer4 Acc 0.8798, AUC 0.9456179738044739, avg_entr 0.019408302381634712, f1 0.879800021648407
ep9_l4_test_time 3.074579232999895
gc 0
Train Epoch10 Acc 0.97375 (38950/40000), AUC 0.9930830001831055
ep10_train_time 91.87404450000008
Test Epoch10 layer0 Acc 0.8904, AUC 0.9522250294685364, avg_entr 0.13133955001831055, f1 0.8903999924659729
ep10_l0_test_time 0.5551254780000363
Test Epoch10 layer1 Acc 0.8766, AUC 0.9335591197013855, avg_entr 0.030991140753030777, f1 0.8766000270843506
ep10_l1_test_time 1.1922161770000912
Test Epoch10 layer2 Acc 0.876, AUC 0.9419889450073242, avg_entr 0.02336711995303631, f1 0.8760000467300415
ep10_l2_test_time 1.8096990240001105
Test Epoch10 layer3 Acc 0.8764, AUC 0.9435460567474365, avg_entr 0.02254651114344597, f1 0.8763999938964844
ep10_l3_test_time 2.4365034599998125
Test Epoch10 layer4 Acc 0.876, AUC 0.9439846277236938, avg_entr 0.021457644179463387, f1 0.8760000467300415
ep10_l4_test_time 3.0652859250001256
gc 0
Train Epoch11 Acc 0.975925 (39037/40000), AUC 0.9946290254592896
ep11_train_time 91.76057738099985
Test Epoch11 layer0 Acc 0.8872, AUC 0.9511843919754028, avg_entr 0.13058245182037354, f1 0.8871999979019165
ep11_l0_test_time 0.5570669170001565
Test Epoch11 layer1 Acc 0.8772, AUC 0.9298251867294312, avg_entr 0.02655390277504921, f1 0.8772000074386597
ep11_l1_test_time 1.1880392759999268
Test Epoch11 layer2 Acc 0.8768, AUC 0.9404720067977905, avg_entr 0.020770471543073654, f1 0.876800000667572
ep11_l2_test_time 1.8102764980001211
Test Epoch11 layer3 Acc 0.877, AUC 0.9419572353363037, avg_entr 0.020515069365501404, f1 0.8769999742507935
ep11_l3_test_time 2.4379883290000635
Test Epoch11 layer4 Acc 0.8768, AUC 0.9427633285522461, avg_entr 0.01974007487297058, f1 0.876800000667572
ep11_l4_test_time 3.0671891249999135
gc 0
Train Epoch12 Acc 0.9765 (39060/40000), AUC 0.9950422048568726
ep12_train_time 91.82433687899993
Test Epoch12 layer0 Acc 0.8854, AUC 0.9499568939208984, avg_entr 0.12818337976932526, f1 0.8853999972343445
ep12_l0_test_time 0.556696521999811
Test Epoch12 layer1 Acc 0.8746, AUC 0.9288619756698608, avg_entr 0.02676081471145153, f1 0.8745999932289124
ep12_l1_test_time 1.186238398000114
Test Epoch12 layer2 Acc 0.8748, AUC 0.9366281032562256, avg_entr 0.02050098031759262, f1 0.8748000264167786
ep12_l2_test_time 1.813337095000179
Test Epoch12 layer3 Acc 0.8744, AUC 0.9400402307510376, avg_entr 0.020066792145371437, f1 0.8744000792503357
ep12_l3_test_time 2.43743448500004
Test Epoch12 layer4 Acc 0.8742, AUC 0.9423670172691345, avg_entr 0.019240424036979675, f1 0.8741999864578247
ep12_l4_test_time 3.0745340309999847
gc 0
Train Epoch13 Acc 0.978675 (39147/40000), AUC 0.9953698515892029
ep13_train_time 91.81846453999992
Test Epoch13 layer0 Acc 0.8864, AUC 0.9494961500167847, avg_entr 0.1241845116019249, f1 0.8863999843597412
ep13_l0_test_time 0.5589320199999293
Test Epoch13 layer1 Acc 0.8744, AUC 0.9273936748504639, avg_entr 0.025091342628002167, f1 0.8744000792503357
ep13_l1_test_time 1.188723090999929
Test Epoch13 layer2 Acc 0.8746, AUC 0.9322693943977356, avg_entr 0.01731886900961399, f1 0.8745999932289124
ep13_l2_test_time 1.809514333999914
Test Epoch13 layer3 Acc 0.8746, AUC 0.9379453063011169, avg_entr 0.016466187313199043, f1 0.8745999932289124
ep13_l3_test_time 2.439835314999982
Test Epoch13 layer4 Acc 0.8744, AUC 0.9409300088882446, avg_entr 0.015797441825270653, f1 0.8744000792503357
ep13_l4_test_time 3.068305344999999
gc 0
Train Epoch14 Acc 0.97895 (39158/40000), AUC 0.995420515537262
ep14_train_time 91.79100449199996
Test Epoch14 layer0 Acc 0.8856, AUC 0.9489848613739014, avg_entr 0.122810497879982, f1 0.8855999708175659
ep14_l0_test_time 0.5613795409999511
Test Epoch14 layer1 Acc 0.8746, AUC 0.9263255596160889, avg_entr 0.024669306352734566, f1 0.8745999932289124
ep14_l1_test_time 1.1880432410000594
Test Epoch14 layer2 Acc 0.8742, AUC 0.9327586889266968, avg_entr 0.01782829873263836, f1 0.8741999864578247
ep14_l2_test_time 1.8128703639999912
Test Epoch14 layer3 Acc 0.8744, AUC 0.9374881982803345, avg_entr 0.017107611522078514, f1 0.8744000792503357
ep14_l3_test_time 2.4423917229999006
Test Epoch14 layer4 Acc 0.8746, AUC 0.9403237104415894, avg_entr 0.016529133543372154, f1 0.8745999932289124
ep14_l4_test_time 3.072717715999943
gc 0
Train Epoch15 Acc 0.979475 (39179/40000), AUC 0.9955796003341675
ep15_train_time 92.02883355199992
Test Epoch15 layer0 Acc 0.8848, AUC 0.9482505917549133, avg_entr 0.12046712636947632, f1 0.8848000168800354
ep15_l0_test_time 0.5565867360000993
Test Epoch15 layer1 Acc 0.8728, AUC 0.9234768152236938, avg_entr 0.023139722645282745, f1 0.8727999925613403
ep15_l1_test_time 1.1885468539999238
Test Epoch15 layer2 Acc 0.8712, AUC 0.9304168820381165, avg_entr 0.017196690663695335, f1 0.8712000846862793
ep15_l2_test_time 1.8136754909999127
Test Epoch15 layer3 Acc 0.8712, AUC 0.9360200762748718, avg_entr 0.016525888815522194, f1 0.8712000846862793
ep15_l3_test_time 2.441828222999902
Test Epoch15 layer4 Acc 0.8712, AUC 0.938560962677002, avg_entr 0.015970950946211815, f1 0.8712000846862793
ep15_l4_test_time 3.0686890409999705
gc 0
Train Epoch16 Acc 0.980075 (39203/40000), AUC 0.9957137703895569
ep16_train_time 91.81335878000004
Test Epoch16 layer0 Acc 0.8836, AUC 0.9476089477539062, avg_entr 0.11886315792798996, f1 0.8835999965667725
ep16_l0_test_time 0.5615207319999627
Test Epoch16 layer1 Acc 0.8724, AUC 0.9232841730117798, avg_entr 0.02424711361527443, f1 0.8723999857902527
ep16_l1_test_time 1.1923880570000165
Test Epoch16 layer2 Acc 0.8728, AUC 0.9294029474258423, avg_entr 0.018265511840581894, f1 0.8727999925613403
ep16_l2_test_time 1.8127352540000174
Test Epoch16 layer3 Acc 0.8726, AUC 0.9350919127464294, avg_entr 0.017564402893185616, f1 0.8726000189781189
ep16_l3_test_time 2.4467813599999317
Test Epoch16 layer4 Acc 0.8724, AUC 0.9379319548606873, avg_entr 0.01701025664806366, f1 0.8723999857902527
ep16_l4_test_time 3.075866466999969
gc 0
Train Epoch17 Acc 0.98095 (39238/40000), AUC 0.9956539869308472
ep17_train_time 91.80516893799995
Test Epoch17 layer0 Acc 0.8828, AUC 0.9475216865539551, avg_entr 0.11875462532043457, f1 0.8827999830245972
ep17_l0_test_time 0.556227601000046
Test Epoch17 layer1 Acc 0.8736, AUC 0.9247390031814575, avg_entr 0.023157330229878426, f1 0.8736000061035156
ep17_l1_test_time 1.192508104999888
Test Epoch17 layer2 Acc 0.8728, AUC 0.9278493523597717, avg_entr 0.01658470183610916, f1 0.8727999925613403
ep17_l2_test_time 1.8119824099999278
Test Epoch17 layer3 Acc 0.8728, AUC 0.9343010187149048, avg_entr 0.01573631353676319, f1 0.8727999925613403
ep17_l3_test_time 2.436313985999959
Test Epoch17 layer4 Acc 0.8732, AUC 0.9378060102462769, avg_entr 0.01524973101913929, f1 0.873199999332428
ep17_l4_test_time 3.069468162000021
gc 0
Train Epoch18 Acc 0.981025 (39241/40000), AUC 0.9957308173179626
ep18_train_time 91.84273449800003
Test Epoch18 layer0 Acc 0.884, AUC 0.9471644759178162, avg_entr 0.11847607791423798, f1 0.8840000033378601
ep18_l0_test_time 0.5725923979998697
Test Epoch18 layer1 Acc 0.8746, AUC 0.9233475923538208, avg_entr 0.02366948314011097, f1 0.8745999932289124
ep18_l1_test_time 1.1884025349997955
Test Epoch18 layer2 Acc 0.8736, AUC 0.9269784092903137, avg_entr 0.017665991559624672, f1 0.8736000061035156
ep18_l2_test_time 1.8097800840000673
Test Epoch18 layer3 Acc 0.8738, AUC 0.9326843023300171, avg_entr 0.016983818262815475, f1 0.8737999796867371
ep18_l3_test_time 2.4476548840000305
Test Epoch18 layer4 Acc 0.8734, AUC 0.9370947480201721, avg_entr 0.016453182324767113, f1 0.8733999729156494
ep18_l4_test_time 3.066774887999827
gc 0
Train Epoch19 Acc 0.98165 (39266/40000), AUC 0.9961597919464111
ep19_train_time 91.81442520300016
Test Epoch19 layer0 Acc 0.8848, AUC 0.9468339681625366, avg_entr 0.1173374280333519, f1 0.8848000168800354
ep19_l0_test_time 0.5634009310001602
Test Epoch19 layer1 Acc 0.8734, AUC 0.9221805334091187, avg_entr 0.022680053487420082, f1 0.8733999729156494
ep19_l1_test_time 1.1909052829998927
Test Epoch19 layer2 Acc 0.8734, AUC 0.9250849485397339, avg_entr 0.016927620396018028, f1 0.8733999729156494
ep19_l2_test_time 1.809327532999987
Test Epoch19 layer3 Acc 0.8736, AUC 0.9317445755004883, avg_entr 0.016160083934664726, f1 0.8736000061035156
ep19_l3_test_time 2.4490958080000382
Test Epoch19 layer4 Acc 0.8732, AUC 0.9359638690948486, avg_entr 0.015601187013089657, f1 0.873199999332428
ep19_l4_test_time 3.0708105899998372
gc 0
Train Epoch20 Acc 0.981825 (39273/40000), AUC 0.9960821866989136
ep20_train_time 91.79110267800002
Test Epoch20 layer0 Acc 0.8834, AUC 0.9465634822845459, avg_entr 0.11625537276268005, f1 0.8834000825881958
ep20_l0_test_time 0.571805580000273
Test Epoch20 layer1 Acc 0.8724, AUC 0.9216053485870361, avg_entr 0.022639794275164604, f1 0.8723999857902527
ep20_l1_test_time 1.1913670239996463
Test Epoch20 layer2 Acc 0.8722, AUC 0.9236698150634766, avg_entr 0.01703982800245285, f1 0.872200071811676
ep20_l2_test_time 1.8190329230001225
Test Epoch20 layer3 Acc 0.872, AUC 0.9306421875953674, avg_entr 0.01619730517268181, f1 0.871999979019165
ep20_l3_test_time 2.454174979000072
Test Epoch20 layer4 Acc 0.8722, AUC 0.9353424310684204, avg_entr 0.015629418194293976, f1 0.872200071811676
ep20_l4_test_time 3.092077117000372
gc 0
Train Epoch21 Acc 0.98215 (39286/40000), AUC 0.9960097074508667
ep21_train_time 91.80755956300027
Test Epoch21 layer0 Acc 0.8834, AUC 0.9464107751846313, avg_entr 0.11578971147537231, f1 0.8834000825881958
ep21_l0_test_time 0.5596726280000439
Test Epoch21 layer1 Acc 0.8716, AUC 0.9210140109062195, avg_entr 0.02246485836803913, f1 0.8715999722480774
ep21_l1_test_time 1.1870064989998355
Test Epoch21 layer2 Acc 0.8714, AUC 0.9229432940483093, avg_entr 0.016712799668312073, f1 0.871399998664856
ep21_l2_test_time 1.8111578500001997
Test Epoch21 layer3 Acc 0.8718, AUC 0.9298892021179199, avg_entr 0.015872688964009285, f1 0.8718000650405884
ep21_l3_test_time 2.4409448850001354
Test Epoch21 layer4 Acc 0.8716, AUC 0.9347971677780151, avg_entr 0.015328695997595787, f1 0.8715999722480774
ep21_l4_test_time 3.072177808000106
gc 0
Train Epoch22 Acc 0.981875 (39275/40000), AUC 0.9964803457260132
ep22_train_time 91.98941103400011
Test Epoch22 layer0 Acc 0.8842, AUC 0.9462603330612183, avg_entr 0.11518217623233795, f1 0.8842000365257263
ep22_l0_test_time 0.5745551569998497
Test Epoch22 layer1 Acc 0.8724, AUC 0.9215704202651978, avg_entr 0.022144740447402, f1 0.8723999857902527
ep22_l1_test_time 1.1878714089998539
Test Epoch22 layer2 Acc 0.8718, AUC 0.9223678708076477, avg_entr 0.016573721542954445, f1 0.8718000650405884
ep22_l2_test_time 1.8135762189999696
Test Epoch22 layer3 Acc 0.8726, AUC 0.9295884966850281, avg_entr 0.01582743041217327, f1 0.8726000189781189
ep22_l3_test_time 2.4387224419997438
Test Epoch22 layer4 Acc 0.8724, AUC 0.9348084926605225, avg_entr 0.01534279901534319, f1 0.8723999857902527
ep22_l4_test_time 3.071566391000033
gc 0
Train Epoch23 Acc 0.982525 (39301/40000), AUC 0.9962880611419678
ep23_train_time 91.88295987899983
Test Epoch23 layer0 Acc 0.8838, AUC 0.9461140632629395, avg_entr 0.11481671035289764, f1 0.8838000297546387
ep23_l0_test_time 0.5570366729998568
Test Epoch23 layer1 Acc 0.8718, AUC 0.9213664531707764, avg_entr 0.022242795675992966, f1 0.8718000650405884
ep23_l1_test_time 1.1900249670002268
Test Epoch23 layer2 Acc 0.8708, AUC 0.9213041663169861, avg_entr 0.016498243436217308, f1 0.8708000183105469
ep23_l2_test_time 1.8110504280002715
Test Epoch23 layer3 Acc 0.8724, AUC 0.928803563117981, avg_entr 0.0158308707177639, f1 0.8723999857902527
ep23_l3_test_time 2.4408236689996556
Test Epoch23 layer4 Acc 0.8722, AUC 0.9345041513442993, avg_entr 0.015373388305306435, f1 0.872200071811676
ep23_l4_test_time 3.067356192000261
gc 0
Train Epoch24 Acc 0.98215 (39286/40000), AUC 0.9962869882583618
ep24_train_time 91.75798395699985
Test Epoch24 layer0 Acc 0.883, AUC 0.9459724426269531, avg_entr 0.11419019848108292, f1 0.8830000162124634
ep24_l0_test_time 0.5559585880000668
Test Epoch24 layer1 Acc 0.8726, AUC 0.9210408926010132, avg_entr 0.02193596214056015, f1 0.8726000189781189
ep24_l1_test_time 1.190436415000022
Test Epoch24 layer2 Acc 0.8718, AUC 0.9210800528526306, avg_entr 0.016386913135647774, f1 0.8718000650405884
ep24_l2_test_time 1.8100055730001259
Test Epoch24 layer3 Acc 0.8722, AUC 0.928364634513855, avg_entr 0.01565311662852764, f1 0.872200071811676
ep24_l3_test_time 2.43853448200025
Test Epoch24 layer4 Acc 0.872, AUC 0.9340671300888062, avg_entr 0.01512217242270708, f1 0.871999979019165
ep24_l4_test_time 3.067081894000239
gc 0
Train Epoch25 Acc 0.9825 (39300/40000), AUC 0.9964148998260498
ep25_train_time 91.79377122799997
Test Epoch25 layer0 Acc 0.8812, AUC 0.9458591938018799, avg_entr 0.11274214833974838, f1 0.8812000155448914
ep25_l0_test_time 0.5621362420001788
Test Epoch25 layer1 Acc 0.8726, AUC 0.9207881093025208, avg_entr 0.021746909245848656, f1 0.8726000189781189
ep25_l1_test_time 1.196811040999819
Test Epoch25 layer2 Acc 0.872, AUC 0.9194616079330444, avg_entr 0.01639411970973015, f1 0.871999979019165
ep25_l2_test_time 1.8124073760000101
Test Epoch25 layer3 Acc 0.8718, AUC 0.9272106885910034, avg_entr 0.015577292069792747, f1 0.8718000650405884
ep25_l3_test_time 2.441834066999945
Test Epoch25 layer4 Acc 0.8716, AUC 0.9333224296569824, avg_entr 0.01502779871225357, f1 0.8715999722480774
ep25_l4_test_time 3.066257234999739
gc 0
Train Epoch26 Acc 0.982525 (39301/40000), AUC 0.9966316223144531
ep26_train_time 91.90466774700008
Test Epoch26 layer0 Acc 0.8834, AUC 0.945837676525116, avg_entr 0.11306911706924438, f1 0.8834000825881958
ep26_l0_test_time 0.5593834630003585
Test Epoch26 layer1 Acc 0.8722, AUC 0.9202000498771667, avg_entr 0.02162189595401287, f1 0.872200071811676
ep26_l1_test_time 1.1895105369999328
Test Epoch26 layer2 Acc 0.8716, AUC 0.9194156527519226, avg_entr 0.016290374100208282, f1 0.8715999722480774
ep26_l2_test_time 1.8097540009998738
Test Epoch26 layer3 Acc 0.8712, AUC 0.9268142580986023, avg_entr 0.015414486639201641, f1 0.8712000846862793
ep26_l3_test_time 2.4407607369998914
Test Epoch26 layer4 Acc 0.8712, AUC 0.9329055547714233, avg_entr 0.014861551113426685, f1 0.8712000846862793
ep26_l4_test_time 3.0741954349996377
gc 0
Train Epoch27 Acc 0.98285 (39314/40000), AUC 0.9968017935752869
ep27_train_time 91.88163126300014
Test Epoch27 layer0 Acc 0.882, AUC 0.9457420110702515, avg_entr 0.11219306290149689, f1 0.8820000290870667
ep27_l0_test_time 0.5584023340002204
Test Epoch27 layer1 Acc 0.8718, AUC 0.9201509952545166, avg_entr 0.021492332220077515, f1 0.8718000650405884
ep27_l1_test_time 1.1891548009998587
Test Epoch27 layer2 Acc 0.8708, AUC 0.91935133934021, avg_entr 0.016101740300655365, f1 0.8708000183105469
ep27_l2_test_time 1.8125213570001506
Test Epoch27 layer3 Acc 0.8704, AUC 0.9267535209655762, avg_entr 0.01521360781043768, f1 0.8704000115394592
ep27_l3_test_time 2.444357645999844
Test Epoch27 layer4 Acc 0.8706, AUC 0.9327945709228516, avg_entr 0.014653680846095085, f1 0.8705999851226807
ep27_l4_test_time 3.0724570669999594
gc 0
Train Epoch28 Acc 0.98295 (39318/40000), AUC 0.9965828657150269
ep28_train_time 92.00592138799993
Test Epoch28 layer0 Acc 0.8812, AUC 0.9457134008407593, avg_entr 0.11162171512842178, f1 0.8812000155448914
ep28_l0_test_time 0.5573199739997108
Test Epoch28 layer1 Acc 0.8718, AUC 0.9199648499488831, avg_entr 0.02143733762204647, f1 0.8718000650405884
ep28_l1_test_time 1.189469680000002
Test Epoch28 layer2 Acc 0.871, AUC 0.9189658761024475, avg_entr 0.016198696568608284, f1 0.8709999918937683
ep28_l2_test_time 1.8102847379996092
Test Epoch28 layer3 Acc 0.871, AUC 0.9263907670974731, avg_entr 0.015269389376044273, f1 0.8709999918937683
ep28_l3_test_time 2.4394601259996307
Test Epoch28 layer4 Acc 0.8706, AUC 0.9326083660125732, avg_entr 0.014708764851093292, f1 0.8705999851226807
ep28_l4_test_time 3.0665030049999586
gc 0
Train Epoch29 Acc 0.982875 (39315/40000), AUC 0.9964994192123413
ep29_train_time 91.88111409300018
Test Epoch29 layer0 Acc 0.8814, AUC 0.9456793665885925, avg_entr 0.11136378347873688, f1 0.8813999891281128
ep29_l0_test_time 0.5576671020003232
Test Epoch29 layer1 Acc 0.8714, AUC 0.9195663332939148, avg_entr 0.021362919360399246, f1 0.871399998664856
ep29_l1_test_time 1.1954067830001804
Test Epoch29 layer2 Acc 0.8716, AUC 0.9191356897354126, avg_entr 0.015927322208881378, f1 0.8715999722480774
ep29_l2_test_time 1.817966668000281
Test Epoch29 layer3 Acc 0.8706, AUC 0.92647385597229, avg_entr 0.015066789463162422, f1 0.8705999851226807
ep29_l3_test_time 2.4424048220002987
Test Epoch29 layer4 Acc 0.871, AUC 0.9325329065322876, avg_entr 0.014531738124787807, f1 0.8709999918937683
ep29_l4_test_time 3.0692572419998214
Best AUC tensor(0.8986) 5 0
train_loss (2, 5, 30)
valid_acc (5, 30)
valid_AUC (5, 30)
train_acc (30,)
total_train+valid_time 3030.394083154
Start Testing
Load ckpt at ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt
Test layer0 Acc 0.8902, AUC 0.9549222588539124, avg_entr 0.16083671152591705, f1 0.8902000188827515
l0_test_time 0.5569838169999457
Test layer1 Acc 0.8822, AUC 0.9456202983856201, avg_entr 0.0502157025039196, f1 0.8822000026702881
l1_test_time 1.1944158629999038
Test layer2 Acc 0.883, AUC 0.9497114419937134, avg_entr 0.04132763296365738, f1 0.8830000162124634
l2_test_time 1.8232218690000082
Test layer3 Acc 0.8832, AUC 0.9503988027572632, avg_entr 0.04167603328824043, f1 0.8831999897956848
l3_test_time 2.4399218499997914
Test layer4 Acc 0.8832, AUC 0.9503498077392578, avg_entr 0.04018513485789299, f1 0.8831999897956848
l4_test_time 3.06802523999977
