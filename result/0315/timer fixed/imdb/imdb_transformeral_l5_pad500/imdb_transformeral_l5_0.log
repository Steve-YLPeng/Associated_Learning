total count words 222751
vocab size 30000
train size 40000, valid size 5000, test size 5000
found 27937 words in glove
model: TransformerModelML(
  (layers): ModuleList(
    (0): EMBLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Embedding(30000, 300)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=2, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=2, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.weight 9000000
layers.0.ae.g.0.weight 256
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 256
layers.0.ae.h.0.bias 2
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
Total Trainable Params: 13671778
init_time 21.297948509999998
Start Training
gc 0
Train Epoch0 Acc 0.523425 (20937/40000), AUC 0.5323115587234497
ep0_train_time 92.09032322899999
Test Epoch0 layer0 Acc 0.8118, AUC 0.8898662328720093, avg_entr 0.5561925172805786, f1 0.8118000030517578
ep0_l0_test_time 0.5487531520000033
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 0
Test Epoch0 layer1 Acc 0.8248, AUC 0.9034914970397949, avg_entr 0.3660300374031067, f1 0.8248000144958496
ep0_l1_test_time 1.1772842809999986
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 0
Test Epoch0 layer2 Acc 0.8192, AUC 0.8989003300666809, avg_entr 0.557711124420166, f1 0.8191999197006226
ep0_l2_test_time 1.8208300479999906
Test Epoch0 layer3 Acc 0.8138, AUC 0.8938906788825989, avg_entr 0.6246351003646851, f1 0.8137999773025513
ep0_l3_test_time 2.4326102229999975
Test Epoch0 layer4 Acc 0.7208, AUC 0.8984450101852417, avg_entr 0.6740193963050842, f1 0.7207999229431152
ep0_l4_test_time 3.0653046270000033
gc 0
Train Epoch1 Acc 0.8582 (34328/40000), AUC 0.9243680834770203
ep1_train_time 91.887583578
Test Epoch1 layer0 Acc 0.8826, AUC 0.9464670419692993, avg_entr 0.2828001379966736, f1 0.8826000094413757
ep1_l0_test_time 0.5545716950000212
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 1
Test Epoch1 layer1 Acc 0.889, AUC 0.9529356956481934, avg_entr 0.2013503462076187, f1 0.8889999985694885
ep1_l1_test_time 1.1867573410000034
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 1
Test Epoch1 layer2 Acc 0.8878, AUC 0.9540838599205017, avg_entr 0.16843712329864502, f1 0.8877999782562256
ep1_l2_test_time 1.814583526000007
Test Epoch1 layer3 Acc 0.8864, AUC 0.9537044167518616, avg_entr 0.15162652730941772, f1 0.8863999843597412
ep1_l3_test_time 2.4381943610000008
Test Epoch1 layer4 Acc 0.8864, AUC 0.9537668228149414, avg_entr 0.14524394273757935, f1 0.8863999843597412
ep1_l4_test_time 3.0650296569999966
gc 0
Train Epoch2 Acc 0.91175 (36470/40000), AUC 0.9674570560455322
ep2_train_time 91.90123240099999
Test Epoch2 layer0 Acc 0.8926, AUC 0.9557645320892334, avg_entr 0.22399401664733887, f1 0.8925999999046326
ep2_l0_test_time 0.5635918959999913
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 2
Test Epoch2 layer1 Acc 0.8952, AUC 0.9576209783554077, avg_entr 0.15629418194293976, f1 0.8952000141143799
ep2_l1_test_time 1.1827193810000267
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 2
Test Epoch2 layer2 Acc 0.8938, AUC 0.9573723077774048, avg_entr 0.08797924965620041, f1 0.8938000202178955
ep2_l2_test_time 1.8189880250000101
Test Epoch2 layer3 Acc 0.894, AUC 0.9577846527099609, avg_entr 0.06582093238830566, f1 0.8939999938011169
ep2_l3_test_time 2.436750982000035
Test Epoch2 layer4 Acc 0.8944, AUC 0.9581636190414429, avg_entr 0.06388895958662033, f1 0.8944000005722046
ep2_l4_test_time 3.064529881999988
gc 0
Train Epoch3 Acc 0.937375 (37495/40000), AUC 0.9789038300514221
ep3_train_time 91.87617652100005
Test Epoch3 layer0 Acc 0.897, AUC 0.958332359790802, avg_entr 0.19304992258548737, f1 0.8970000147819519
ep3_l0_test_time 0.550576102999969
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 3
Test Epoch3 layer1 Acc 0.893, AUC 0.9539064168930054, avg_entr 0.1032923087477684, f1 0.8930000066757202
ep3_l1_test_time 1.1847317460000113
Test Epoch3 layer2 Acc 0.8908, AUC 0.9509392976760864, avg_entr 0.04769430682063103, f1 0.8907999992370605
ep3_l2_test_time 1.80815168700002
Test Epoch3 layer3 Acc 0.8908, AUC 0.9540809988975525, avg_entr 0.04213416948914528, f1 0.8907999992370605
ep3_l3_test_time 2.4403565509999794
Test Epoch3 layer4 Acc 0.8912, AUC 0.954664945602417, avg_entr 0.04127510264515877, f1 0.8912000060081482
ep3_l4_test_time 3.0647715009999956
gc 0
Train Epoch4 Acc 0.952325 (38093/40000), AUC 0.9855419397354126
ep4_train_time 91.81539234300004
Test Epoch4 layer0 Acc 0.8974, AUC 0.9590647220611572, avg_entr 0.1775089055299759, f1 0.8974000215530396
ep4_l0_test_time 0.5566484970000829
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 4
Test Epoch4 layer1 Acc 0.8902, AUC 0.9527153968811035, avg_entr 0.05651732161641121, f1 0.8902000188827515
ep4_l1_test_time 1.1958688619999975
Test Epoch4 layer2 Acc 0.8896, AUC 0.9529369473457336, avg_entr 0.03647921234369278, f1 0.8895999193191528
ep4_l2_test_time 1.8130669920000173
Test Epoch4 layer3 Acc 0.889, AUC 0.9535777568817139, avg_entr 0.03760978579521179, f1 0.8889999985694885
ep4_l3_test_time 2.438310499999943
Test Epoch4 layer4 Acc 0.889, AUC 0.9534863233566284, avg_entr 0.03721589222550392, f1 0.8889999985694885
ep4_l4_test_time 3.071832190000009
gc 0
Train Epoch5 Acc 0.9581 (38324/40000), AUC 0.9879951477050781
ep5_train_time 91.87057291400004
Test Epoch5 layer0 Acc 0.8754, AUC 0.9578665494918823, avg_entr 0.15781864523887634, f1 0.8754000067710876
ep5_l0_test_time 0.5678770189999796
Test Epoch5 layer1 Acc 0.8786, AUC 0.9471731781959534, avg_entr 0.04165966808795929, f1 0.878600001335144
ep5_l1_test_time 1.181675266999946
Test Epoch5 layer2 Acc 0.8786, AUC 0.9517326354980469, avg_entr 0.029336975887417793, f1 0.878600001335144
ep5_l2_test_time 1.8012812590000067
Test Epoch5 layer3 Acc 0.8784, AUC 0.9518575072288513, avg_entr 0.02958766184747219, f1 0.8784000277519226
ep5_l3_test_time 2.432055840999965
Test Epoch5 layer4 Acc 0.8784, AUC 0.9522713422775269, avg_entr 0.029527883976697922, f1 0.8784000277519226
ep5_l4_test_time 3.06159736099994
gc 0
Train Epoch6 Acc 0.96255 (38502/40000), AUC 0.9893237352371216
ep6_train_time 91.8155506889999
Test Epoch6 layer0 Acc 0.8974, AUC 0.9569432735443115, avg_entr 0.1489039808511734, f1 0.8974000215530396
ep6_l0_test_time 0.5461443609999606
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 6
Test Epoch6 layer1 Acc 0.885, AUC 0.943315863609314, avg_entr 0.03998387232422829, f1 0.8849999904632568
ep6_l1_test_time 1.1835415570000123
Test Epoch6 layer2 Acc 0.8878, AUC 0.9494082927703857, avg_entr 0.02948317863047123, f1 0.8877999782562256
ep6_l2_test_time 1.8005565700000261
Test Epoch6 layer3 Acc 0.8864, AUC 0.9499718546867371, avg_entr 0.0308243278414011, f1 0.8863999843597412
ep6_l3_test_time 2.4362425089999533
Test Epoch6 layer4 Acc 0.8862, AUC 0.9501975774765015, avg_entr 0.030647555366158485, f1 0.8862000107765198
ep6_l4_test_time 3.062264513000059
gc 0
Train Epoch7 Acc 0.967725 (38709/40000), AUC 0.9915685653686523
ep7_train_time 91.86181139899998
Test Epoch7 layer0 Acc 0.8958, AUC 0.9550867080688477, avg_entr 0.14246799051761627, f1 0.895799994468689
ep7_l0_test_time 0.555624124000019
Test Epoch7 layer1 Acc 0.881, AUC 0.9376746416091919, avg_entr 0.03451928496360779, f1 0.8809999823570251
ep7_l1_test_time 1.18886837499997
Test Epoch7 layer2 Acc 0.8816, AUC 0.9461690783500671, avg_entr 0.02507014386355877, f1 0.881600022315979
ep7_l2_test_time 1.8223412360000566
Test Epoch7 layer3 Acc 0.881, AUC 0.9466193914413452, avg_entr 0.024965116754174232, f1 0.8809999823570251
ep7_l3_test_time 2.438825953999981
Test Epoch7 layer4 Acc 0.8814, AUC 0.9471845626831055, avg_entr 0.02528681606054306, f1 0.8813999891281128
ep7_l4_test_time 3.064125519000072
gc 0
Train Epoch8 Acc 0.97055 (38822/40000), AUC 0.992467999458313
ep8_train_time 91.83895058799999
Test Epoch8 layer0 Acc 0.8938, AUC 0.9536600112915039, avg_entr 0.13687968254089355, f1 0.8938000202178955
ep8_l0_test_time 0.5469551339999725
Test Epoch8 layer1 Acc 0.8774, AUC 0.9355249404907227, avg_entr 0.0318058580160141, f1 0.8773999810218811
ep8_l1_test_time 1.181074198000033
Test Epoch8 layer2 Acc 0.8788, AUC 0.9446973204612732, avg_entr 0.022484343498945236, f1 0.8787999749183655
ep8_l2_test_time 1.8013427710000087
Test Epoch8 layer3 Acc 0.879, AUC 0.9451195001602173, avg_entr 0.021686172112822533, f1 0.8790000081062317
ep8_l3_test_time 2.4307658210000227
Test Epoch8 layer4 Acc 0.8788, AUC 0.9456580877304077, avg_entr 0.02160334214568138, f1 0.8787999749183655
ep8_l4_test_time 3.061482868999974
gc 0
Train Epoch9 Acc 0.9737 (38948/40000), AUC 0.9935256242752075
ep9_train_time 91.9162045029999
Test Epoch9 layer0 Acc 0.8908, AUC 0.9527392387390137, avg_entr 0.1340513378381729, f1 0.8907999992370605
ep9_l0_test_time 0.5570171949998439
Test Epoch9 layer1 Acc 0.8808, AUC 0.9340319633483887, avg_entr 0.029955944046378136, f1 0.8808000087738037
ep9_l1_test_time 1.181721522000089
Test Epoch9 layer2 Acc 0.881, AUC 0.9436893463134766, avg_entr 0.021547552198171616, f1 0.8809999823570251
ep9_l2_test_time 1.800400358999923
Test Epoch9 layer3 Acc 0.8798, AUC 0.9444045424461365, avg_entr 0.0210659671574831, f1 0.879800021648407
ep9_l3_test_time 2.437144779999926
Test Epoch9 layer4 Acc 0.8804, AUC 0.9449290633201599, avg_entr 0.020976882427930832, f1 0.8804000020027161
ep9_l4_test_time 3.067201128999841
gc 0
Train Epoch10 Acc 0.975225 (39009/40000), AUC 0.9939947128295898
ep10_train_time 91.82680598800016
Test Epoch10 layer0 Acc 0.8878, AUC 0.9515863656997681, avg_entr 0.13123762607574463, f1 0.8877999782562256
ep10_l0_test_time 0.5481180650001534
Test Epoch10 layer1 Acc 0.8742, AUC 0.9289511442184448, avg_entr 0.028536072000861168, f1 0.8741999864578247
ep10_l1_test_time 1.182938644999922
Test Epoch10 layer2 Acc 0.8742, AUC 0.9408917427062988, avg_entr 0.020772574469447136, f1 0.8741999864578247
ep10_l2_test_time 1.8060479779999241
Test Epoch10 layer3 Acc 0.8738, AUC 0.9428539276123047, avg_entr 0.01947633922100067, f1 0.8737999796867371
ep10_l3_test_time 2.4317591290000564
Test Epoch10 layer4 Acc 0.8746, AUC 0.9433398246765137, avg_entr 0.019240692257881165, f1 0.8745999932289124
ep10_l4_test_time 3.0738014620001195
gc 0
Train Epoch11 Acc 0.976875 (39075/40000), AUC 0.9943380355834961
ep11_train_time 91.82883036000021
Test Epoch11 layer0 Acc 0.8822, AUC 0.9504601955413818, avg_entr 0.12558867037296295, f1 0.8822000026702881
ep11_l0_test_time 0.5484369119999428
Test Epoch11 layer1 Acc 0.8758, AUC 0.9273285269737244, avg_entr 0.027880331501364708, f1 0.8758000135421753
ep11_l1_test_time 1.182822879000014
Test Epoch11 layer2 Acc 0.8764, AUC 0.9390769004821777, avg_entr 0.019472867250442505, f1 0.8763999938964844
ep11_l2_test_time 1.8078624120000768
Test Epoch11 layer3 Acc 0.877, AUC 0.9421983957290649, avg_entr 0.018032364547252655, f1 0.8769999742507935
ep11_l3_test_time 2.46663334699997
Test Epoch11 layer4 Acc 0.8754, AUC 0.9426857233047485, avg_entr 0.01766764000058174, f1 0.8754000067710876
ep11_l4_test_time 3.111610146999965
gc 0
Train Epoch12 Acc 0.97735 (39094/40000), AUC 0.9947302937507629
ep12_train_time 91.98098211000001
Test Epoch12 layer0 Acc 0.8846, AUC 0.9494269490242004, avg_entr 0.12746219336986542, f1 0.8845999836921692
ep12_l0_test_time 0.5478602130001491
Test Epoch12 layer1 Acc 0.8716, AUC 0.9267864227294922, avg_entr 0.026813311502337456, f1 0.8715999722480774
ep12_l1_test_time 1.1913849539998864
Test Epoch12 layer2 Acc 0.8718, AUC 0.9379816055297852, avg_entr 0.01862461492419243, f1 0.8718000650405884
ep12_l2_test_time 1.8123216349999893
Test Epoch12 layer3 Acc 0.8712, AUC 0.9405773878097534, avg_entr 0.01742495596408844, f1 0.8712000846862793
ep12_l3_test_time 2.431871036000075
Test Epoch12 layer4 Acc 0.8714, AUC 0.9409684538841248, avg_entr 0.017157921567559242, f1 0.871399998664856
ep12_l4_test_time 3.0672767959999874
gc 0
Train Epoch13 Acc 0.97905 (39162/40000), AUC 0.9951459169387817
ep13_train_time 91.93889863799996
Test Epoch13 layer0 Acc 0.885, AUC 0.9488521814346313, avg_entr 0.12293210625648499, f1 0.8849999904632568
ep13_l0_test_time 0.5493962049999936
Test Epoch13 layer1 Acc 0.871, AUC 0.9246393442153931, avg_entr 0.025685522705316544, f1 0.8709999918937683
ep13_l1_test_time 1.1867509469998367
Test Epoch13 layer2 Acc 0.872, AUC 0.9362338185310364, avg_entr 0.017592569813132286, f1 0.871999979019165
ep13_l2_test_time 1.808151166000016
Test Epoch13 layer3 Acc 0.8712, AUC 0.9398984909057617, avg_entr 0.01622200384736061, f1 0.8712000846862793
ep13_l3_test_time 2.4328005509999002
Test Epoch13 layer4 Acc 0.8714, AUC 0.940376877784729, avg_entr 0.015931835398077965, f1 0.871399998664856
ep13_l4_test_time 3.066281278000133
gc 0
Train Epoch14 Acc 0.979925 (39197/40000), AUC 0.9952350854873657
ep14_train_time 91.82457914299994
Test Epoch14 layer0 Acc 0.8854, AUC 0.9482740163803101, avg_entr 0.12232182919979095, f1 0.8853999972343445
ep14_l0_test_time 0.5596366350000608
Test Epoch14 layer1 Acc 0.8714, AUC 0.9220229387283325, avg_entr 0.025324074551463127, f1 0.871399998664856
ep14_l1_test_time 1.1926316179999503
Test Epoch14 layer2 Acc 0.8704, AUC 0.9344751834869385, avg_entr 0.016994422301650047, f1 0.8704000115394592
ep14_l2_test_time 1.8078685660000247
Test Epoch14 layer3 Acc 0.8702, AUC 0.939412534236908, avg_entr 0.01563345640897751, f1 0.870199978351593
ep14_l3_test_time 2.432447617000207
Test Epoch14 layer4 Acc 0.8704, AUC 0.9398326873779297, avg_entr 0.015334954485297203, f1 0.8704000115394592
ep14_l4_test_time 3.066115093999997
gc 0
Train Epoch15 Acc 0.9799 (39196/40000), AUC 0.9954702854156494
ep15_train_time 91.96689852899999
Test Epoch15 layer0 Acc 0.8838, AUC 0.9475233554840088, avg_entr 0.12045978009700775, f1 0.8838000297546387
ep15_l0_test_time 0.5531754640001054
Test Epoch15 layer1 Acc 0.8682, AUC 0.9202511310577393, avg_entr 0.02359754778444767, f1 0.8682000041007996
ep15_l1_test_time 1.1872453329999644
Test Epoch15 layer2 Acc 0.8674, AUC 0.9320252537727356, avg_entr 0.01659041829407215, f1 0.8673999905586243
ep15_l2_test_time 1.8085319450001407
Test Epoch15 layer3 Acc 0.8672, AUC 0.9379266500473022, avg_entr 0.015616404823958874, f1 0.8672000169754028
ep15_l3_test_time 2.433834623000166
Test Epoch15 layer4 Acc 0.8672, AUC 0.9384586811065674, avg_entr 0.015002002008259296, f1 0.8672000169754028
ep15_l4_test_time 3.0746414519999234
gc 0
Train Epoch16 Acc 0.980425 (39217/40000), AUC 0.9955054521560669
ep16_train_time 91.887506433
Test Epoch16 layer0 Acc 0.8834, AUC 0.9471244812011719, avg_entr 0.11915317177772522, f1 0.8834000825881958
ep16_l0_test_time 0.551183169000069
Test Epoch16 layer1 Acc 0.8672, AUC 0.9209921360015869, avg_entr 0.024049200117588043, f1 0.8672000169754028
ep16_l1_test_time 1.18375088699986
Test Epoch16 layer2 Acc 0.8676, AUC 0.9323351383209229, avg_entr 0.01686590351164341, f1 0.8675999641418457
ep16_l2_test_time 1.8092079109999304
Test Epoch16 layer3 Acc 0.8672, AUC 0.9378463625907898, avg_entr 0.016094693914055824, f1 0.8672000169754028
ep16_l3_test_time 2.431587860000036
Test Epoch16 layer4 Acc 0.8674, AUC 0.9383544921875, avg_entr 0.015694720670580864, f1 0.8673999905586243
ep16_l4_test_time 3.0649710329998925
gc 0
Train Epoch17 Acc 0.9813 (39252/40000), AUC 0.9958350658416748
ep17_train_time 91.82279755699983
Test Epoch17 layer0 Acc 0.8818, AUC 0.9467200040817261, avg_entr 0.11896323412656784, f1 0.8818000555038452
ep17_l0_test_time 0.5473821150001186
Test Epoch17 layer1 Acc 0.8684, AUC 0.9198907017707825, avg_entr 0.024290086701512337, f1 0.868399977684021
ep17_l1_test_time 1.189351772000009
Test Epoch17 layer2 Acc 0.8682, AUC 0.9308621883392334, avg_entr 0.016300324350595474, f1 0.8682000041007996
ep17_l2_test_time 1.806124243000113
Test Epoch17 layer3 Acc 0.8676, AUC 0.9374605417251587, avg_entr 0.015358583070337772, f1 0.8675999641418457
ep17_l3_test_time 2.4315484180001476
Test Epoch17 layer4 Acc 0.8678, AUC 0.937918484210968, avg_entr 0.014995311386883259, f1 0.8677999973297119
ep17_l4_test_time 3.0691722390001814
gc 0
Train Epoch18 Acc 0.98125 (39250/40000), AUC 0.9958786964416504
ep18_train_time 91.85207850200004
Test Epoch18 layer0 Acc 0.8822, AUC 0.9464136958122253, avg_entr 0.11768282204866409, f1 0.8822000026702881
ep18_l0_test_time 0.5483092019999276
Test Epoch18 layer1 Acc 0.8668, AUC 0.9185740351676941, avg_entr 0.023061690852046013, f1 0.8668000102043152
ep18_l1_test_time 1.1856317169999784
Test Epoch18 layer2 Acc 0.8674, AUC 0.9300210475921631, avg_entr 0.01595117710530758, f1 0.8673999905586243
ep18_l2_test_time 1.804955781999979
Test Epoch18 layer3 Acc 0.8668, AUC 0.9362463355064392, avg_entr 0.015047618187963963, f1 0.8668000102043152
ep18_l3_test_time 2.4318795780000073
Test Epoch18 layer4 Acc 0.8668, AUC 0.937179684638977, avg_entr 0.014751006849110126, f1 0.8668000102043152
ep18_l4_test_time 3.0638385379998
gc 0
Train Epoch19 Acc 0.981925 (39277/40000), AUC 0.9959572553634644
ep19_train_time 91.82449945999997
Test Epoch19 layer0 Acc 0.8806, AUC 0.9461774826049805, avg_entr 0.1177564486861229, f1 0.8805999755859375
ep19_l0_test_time 0.5705118419998598
Test Epoch19 layer1 Acc 0.8672, AUC 0.91761314868927, avg_entr 0.022473851218819618, f1 0.8672000169754028
ep19_l1_test_time 1.1886392460000934
Test Epoch19 layer2 Acc 0.8668, AUC 0.9281269311904907, avg_entr 0.015498788096010685, f1 0.8668000102043152
ep19_l2_test_time 1.8073217060000388
Test Epoch19 layer3 Acc 0.8668, AUC 0.9360004663467407, avg_entr 0.014408023096621037, f1 0.8668000102043152
ep19_l3_test_time 2.432438469999852
Test Epoch19 layer4 Acc 0.8674, AUC 0.9368168115615845, avg_entr 0.014027186669409275, f1 0.8673999905586243
ep19_l4_test_time 3.068028868000056
gc 0
Train Epoch20 Acc 0.98205 (39282/40000), AUC 0.9961184859275818
ep20_train_time 91.8626613480003
Test Epoch20 layer0 Acc 0.88, AUC 0.9458377361297607, avg_entr 0.11668877303600311, f1 0.8799999952316284
ep20_l0_test_time 0.5496643239998775
Test Epoch20 layer1 Acc 0.8664, AUC 0.9178674817085266, avg_entr 0.022911788895726204, f1 0.8664000034332275
ep20_l1_test_time 1.183987519999846
Test Epoch20 layer2 Acc 0.866, AUC 0.9295647740364075, avg_entr 0.01607661135494709, f1 0.8659999966621399
ep20_l2_test_time 1.8088334750000286
Test Epoch20 layer3 Acc 0.866, AUC 0.9355627298355103, avg_entr 0.015029304660856724, f1 0.8659999966621399
ep20_l3_test_time 2.4312234219996753
Test Epoch20 layer4 Acc 0.8656, AUC 0.9368258714675903, avg_entr 0.014784266240894794, f1 0.8655999898910522
ep20_l4_test_time 3.066661968000062
gc 0
Train Epoch21 Acc 0.982375 (39295/40000), AUC 0.9960076808929443
ep21_train_time 91.89249690399993
Test Epoch21 layer0 Acc 0.8818, AUC 0.9456640481948853, avg_entr 0.11518003046512604, f1 0.8818000555038452
ep21_l0_test_time 0.5517040700001417
Test Epoch21 layer1 Acc 0.8652, AUC 0.9178417921066284, avg_entr 0.022962452843785286, f1 0.8652000427246094
ep21_l1_test_time 1.1860678660000303
Test Epoch21 layer2 Acc 0.8656, AUC 0.9284898638725281, avg_entr 0.015907054767012596, f1 0.8655999898910522
ep21_l2_test_time 1.8083719459996246
Test Epoch21 layer3 Acc 0.866, AUC 0.9354453086853027, avg_entr 0.015029264613986015, f1 0.8659999966621399
ep21_l3_test_time 2.4294415699996534
Test Epoch21 layer4 Acc 0.8654, AUC 0.9365619421005249, avg_entr 0.014636143110692501, f1 0.865399956703186
ep21_l4_test_time 3.0647105749999355
gc 0
Train Epoch22 Acc 0.98245 (39298/40000), AUC 0.9963486194610596
ep22_train_time 91.86921503699978
Test Epoch22 layer0 Acc 0.8816, AUC 0.9455269575119019, avg_entr 0.11466867476701736, f1 0.881600022315979
ep22_l0_test_time 0.5794998620003753
Test Epoch22 layer1 Acc 0.8662, AUC 0.916500449180603, avg_entr 0.0225216057151556, f1 0.8661999702453613
ep22_l1_test_time 1.1978995589997794
Test Epoch22 layer2 Acc 0.8668, AUC 0.9269486665725708, avg_entr 0.015561792068183422, f1 0.8668000102043152
ep22_l2_test_time 1.8126219880000463
Test Epoch22 layer3 Acc 0.8662, AUC 0.9343181848526001, avg_entr 0.014476696960628033, f1 0.8661999702453613
ep22_l3_test_time 2.4384769200000846
Test Epoch22 layer4 Acc 0.8658, AUC 0.9360637664794922, avg_entr 0.014184664934873581, f1 0.8658000826835632
ep22_l4_test_time 3.074037892999968
gc 0
Train Epoch23 Acc 0.9824 (39296/40000), AUC 0.9963078498840332
ep23_train_time 91.83590877300003
Test Epoch23 layer0 Acc 0.879, AUC 0.9453779458999634, avg_entr 0.11469699442386627, f1 0.8790000081062317
ep23_l0_test_time 0.5472319549999156
Test Epoch23 layer1 Acc 0.8652, AUC 0.9164673089981079, avg_entr 0.02209179475903511, f1 0.8652000427246094
ep23_l1_test_time 1.1858164169998417
Test Epoch23 layer2 Acc 0.8646, AUC 0.9262901544570923, avg_entr 0.01496074814349413, f1 0.8646000027656555
ep23_l2_test_time 1.8081561280000642
Test Epoch23 layer3 Acc 0.8644, AUC 0.9343585968017578, avg_entr 0.014090396463871002, f1 0.8644000291824341
ep23_l3_test_time 2.4374530899999627
Test Epoch23 layer4 Acc 0.8646, AUC 0.9359095096588135, avg_entr 0.013714264146983624, f1 0.8646000027656555
ep23_l4_test_time 3.0708655889998226
gc 0
Train Epoch24 Acc 0.982375 (39295/40000), AUC 0.9963395595550537
ep24_train_time 91.84881972699986
Test Epoch24 layer0 Acc 0.8802, AUC 0.9452133178710938, avg_entr 0.11284814774990082, f1 0.8802000284194946
ep24_l0_test_time 0.5568393980001929
Test Epoch24 layer1 Acc 0.8646, AUC 0.9161567687988281, avg_entr 0.02185695432126522, f1 0.8646000027656555
ep24_l1_test_time 1.1843875680001474
Test Epoch24 layer2 Acc 0.8644, AUC 0.9260162115097046, avg_entr 0.014840441755950451, f1 0.8644000291824341
ep24_l2_test_time 1.8155332229998749
Test Epoch24 layer3 Acc 0.8646, AUC 0.9340807199478149, avg_entr 0.013942941091954708, f1 0.8646000027656555
ep24_l3_test_time 2.435742931000277
Test Epoch24 layer4 Acc 0.864, AUC 0.9357399940490723, avg_entr 0.013584471307694912, f1 0.8640000224113464
ep24_l4_test_time 3.0681926959996417
gc 0
Train Epoch25 Acc 0.982875 (39315/40000), AUC 0.9965161681175232
ep25_train_time 92.00341061800009
Test Epoch25 layer0 Acc 0.8796, AUC 0.9451607465744019, avg_entr 0.11333878338336945, f1 0.8795999884605408
ep25_l0_test_time 0.5488154110003052
Test Epoch25 layer1 Acc 0.865, AUC 0.9157177209854126, avg_entr 0.021476985886693, f1 0.8650000095367432
ep25_l1_test_time 1.1872745939999731
Test Epoch25 layer2 Acc 0.8652, AUC 0.925104022026062, avg_entr 0.014537991024553776, f1 0.8652000427246094
ep25_l2_test_time 1.8094957610001074
Test Epoch25 layer3 Acc 0.865, AUC 0.933666467666626, avg_entr 0.013699902221560478, f1 0.8650000095367432
ep25_l3_test_time 2.4312376790003327
Test Epoch25 layer4 Acc 0.865, AUC 0.9354268312454224, avg_entr 0.013365604914724827, f1 0.8650000095367432
ep25_l4_test_time 3.065296996000143
gc 0
Train Epoch26 Acc 0.98275 (39310/40000), AUC 0.9963176250457764
ep26_train_time 91.93781714999977
Test Epoch26 layer0 Acc 0.8802, AUC 0.9451533555984497, avg_entr 0.11265379935503006, f1 0.8802000284194946
ep26_l0_test_time 0.5513841809997757
Test Epoch26 layer1 Acc 0.8644, AUC 0.9154959917068481, avg_entr 0.021224040538072586, f1 0.8644000291824341
ep26_l1_test_time 1.1847077090001221
Test Epoch26 layer2 Acc 0.8642, AUC 0.9241619110107422, avg_entr 0.014106927439570427, f1 0.8641999959945679
ep26_l2_test_time 1.8116527399997722
Test Epoch26 layer3 Acc 0.8646, AUC 0.9332495927810669, avg_entr 0.013205843977630138, f1 0.8646000027656555
ep26_l3_test_time 2.435586508000142
Test Epoch26 layer4 Acc 0.865, AUC 0.9350568056106567, avg_entr 0.012874777428805828, f1 0.8650000095367432
ep26_l4_test_time 3.0673570280000604
gc 0
Train Epoch27 Acc 0.983 (39320/40000), AUC 0.9962776899337769
ep27_train_time 91.85843791000025
Test Epoch27 layer0 Acc 0.8808, AUC 0.9450094699859619, avg_entr 0.11209671944379807, f1 0.8808000087738037
ep27_l0_test_time 0.5467895049996514
Test Epoch27 layer1 Acc 0.8644, AUC 0.9155967235565186, avg_entr 0.02133578062057495, f1 0.8644000291824341
ep27_l1_test_time 1.1829240949996347
Test Epoch27 layer2 Acc 0.8644, AUC 0.9244779348373413, avg_entr 0.01431746780872345, f1 0.8644000291824341
ep27_l2_test_time 1.8128539659996932
Test Epoch27 layer3 Acc 0.8646, AUC 0.9331991672515869, avg_entr 0.013501239009201527, f1 0.8646000027656555
ep27_l3_test_time 2.433042219000072
Test Epoch27 layer4 Acc 0.8646, AUC 0.9350851774215698, avg_entr 0.013171105645596981, f1 0.8646000027656555
ep27_l4_test_time 3.069951179999862
gc 0
Train Epoch28 Acc 0.983025 (39321/40000), AUC 0.9963541030883789
ep28_train_time 91.82801958800019
Test Epoch28 layer0 Acc 0.8788, AUC 0.9450294375419617, avg_entr 0.11215390264987946, f1 0.8787999749183655
ep28_l0_test_time 0.5495760169997084
Test Epoch28 layer1 Acc 0.8662, AUC 0.9154597520828247, avg_entr 0.02016547881066799, f1 0.8661999702453613
ep28_l1_test_time 1.1842895429999771
Test Epoch28 layer2 Acc 0.8654, AUC 0.9231149554252625, avg_entr 0.01380442176014185, f1 0.865399956703186
ep28_l2_test_time 1.8107717480002066
Test Epoch28 layer3 Acc 0.865, AUC 0.9324297904968262, avg_entr 0.012770097702741623, f1 0.8650000095367432
ep28_l3_test_time 2.4359153099999276
Test Epoch28 layer4 Acc 0.8654, AUC 0.934424877166748, avg_entr 0.012584115378558636, f1 0.865399956703186
ep28_l4_test_time 3.070315174999905
gc 0
Train Epoch29 Acc 0.9831 (39324/40000), AUC 0.9963364601135254
ep29_train_time 91.95890911800007
Test Epoch29 layer0 Acc 0.8794, AUC 0.9449875354766846, avg_entr 0.11155228316783905, f1 0.8794000148773193
ep29_l0_test_time 0.5493879569999081
Test Epoch29 layer1 Acc 0.864, AUC 0.9151699542999268, avg_entr 0.02084948681294918, f1 0.8640000224113464
ep29_l1_test_time 1.1842301570000018
Test Epoch29 layer2 Acc 0.864, AUC 0.9233536124229431, avg_entr 0.013807748444378376, f1 0.8640000224113464
ep29_l2_test_time 1.8075875499998801
Test Epoch29 layer3 Acc 0.8642, AUC 0.9325963258743286, avg_entr 0.012952947989106178, f1 0.8641999959945679
ep29_l3_test_time 2.43100269200022
Test Epoch29 layer4 Acc 0.8638, AUC 0.9345855712890625, avg_entr 0.012645222246646881, f1 0.8637999892234802
ep29_l4_test_time 3.067296365999937
Best AUC tensor(0.8974) 6 0
train_loss (2, 5, 30)
valid_acc (5, 30)
valid_AUC (5, 30)
train_acc (30,)
total_train+valid_time 3029.935382719
Start Testing
Load ckpt at ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt
Test layer0 Acc 0.889, AUC 0.952815055847168, avg_entr 0.14990957081317902, f1 0.8889999985694885
l0_test_time 0.5458983819999048
Test layer1 Acc 0.8806, AUC 0.9388761520385742, avg_entr 0.040109097957611084, f1 0.8805999755859375
l1_test_time 1.1845892569999705
Test layer2 Acc 0.8804, AUC 0.9480146169662476, avg_entr 0.028568105772137642, f1 0.8804000020027161
l2_test_time 1.8187974930001474
Test layer3 Acc 0.8814, AUC 0.9481375217437744, avg_entr 0.03010392375290394, f1 0.8813999891281128
l3_test_time 2.4284193589996903
Test layer4 Acc 0.881, AUC 0.9485574960708618, avg_entr 0.030228449031710625, f1 0.8809999823570251
l4_test_time 3.0655348640002558
