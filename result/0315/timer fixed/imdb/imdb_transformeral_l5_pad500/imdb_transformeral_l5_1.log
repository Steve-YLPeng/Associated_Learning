total count words 222751
vocab size 30000
train size 40000, valid size 5000, test size 5000
found 27937 words in glove
model: TransformerModelML(
  (layers): ModuleList(
    (0): EMBLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Embedding(30000, 300)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=2, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=2, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.weight 9000000
layers.0.ae.g.0.weight 256
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 256
layers.0.ae.h.0.bias 2
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
Total Trainable Params: 13671778
init_time 21.64800041
Start Training
gc 0
Train Epoch0 Acc 0.542225 (21689/40000), AUC 0.5711545348167419
ep0_train_time 92.11164758499999
Test Epoch0 layer0 Acc 0.804, AUC 0.882055938243866, avg_entr 0.5733949542045593, f1 0.8040000200271606
ep0_l0_test_time 0.5508176210000073
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 0
Test Epoch0 layer1 Acc 0.8214, AUC 0.9050968885421753, avg_entr 0.35567694902420044, f1 0.821399986743927
ep0_l1_test_time 1.186549001000003
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 0
Test Epoch0 layer2 Acc 0.8132, AUC 0.9093276262283325, avg_entr 0.4088326096534729, f1 0.8131999969482422
ep0_l2_test_time 1.8079689959999996
Test Epoch0 layer3 Acc 0.8192, AUC 0.9080488681793213, avg_entr 0.5591509342193604, f1 0.8191999197006226
ep0_l3_test_time 2.4299518979999988
Test Epoch0 layer4 Acc 0.7728, AUC 0.9065939784049988, avg_entr 0.6361967921257019, f1 0.7728000283241272
ep0_l4_test_time 3.063466817999995
gc 0
Train Epoch1 Acc 0.858825 (34353/40000), AUC 0.9314091801643372
ep1_train_time 91.78184546600001
Test Epoch1 layer0 Acc 0.8744, AUC 0.9453006982803345, avg_entr 0.28980886936187744, f1 0.8744000792503357
ep1_l0_test_time 0.5466472010000132
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 1
Test Epoch1 layer1 Acc 0.8852, AUC 0.9540099501609802, avg_entr 0.18724922835826874, f1 0.8852000832557678
ep1_l1_test_time 1.185779857
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 1
Test Epoch1 layer2 Acc 0.8882, AUC 0.954436182975769, avg_entr 0.16438816487789154, f1 0.888200044631958
ep1_l2_test_time 1.8086108979999835
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 1
Test Epoch1 layer3 Acc 0.8908, AUC 0.9546806812286377, avg_entr 0.14149294793605804, f1 0.8907999992370605
ep1_l3_test_time 2.434921934000016
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 1
Test Epoch1 layer4 Acc 0.8896, AUC 0.9547206163406372, avg_entr 0.12758763134479523, f1 0.8895999193191528
ep1_l4_test_time 3.069159453999987
gc 0
Train Epoch2 Acc 0.913025 (36521/40000), AUC 0.9685317277908325
ep2_train_time 91.80446210199997
Test Epoch2 layer0 Acc 0.8808, AUC 0.9550827741622925, avg_entr 0.22287610173225403, f1 0.8808000087738037
ep2_l0_test_time 0.553808078999964
Test Epoch2 layer1 Acc 0.8968, AUC 0.9582036137580872, avg_entr 0.1327933371067047, f1 0.8967999815940857
ep2_l1_test_time 1.1925841460000015
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 2
Test Epoch2 layer2 Acc 0.898, AUC 0.9582351446151733, avg_entr 0.08866748958826065, f1 0.8980000019073486
ep2_l2_test_time 1.8049033030000032
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 2
Test Epoch2 layer3 Acc 0.8978, AUC 0.9584386348724365, avg_entr 0.06312389671802521, f1 0.8978000283241272
ep2_l3_test_time 2.4321856690000345
Test Epoch2 layer4 Acc 0.8994, AUC 0.9598990678787231, avg_entr 0.05954775959253311, f1 0.8993999361991882
ep2_l4_test_time 3.065181912000014
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 2
gc 0
Train Epoch3 Acc 0.9398 (37592/40000), AUC 0.9802745580673218
ep3_train_time 91.783928527
Test Epoch3 layer0 Acc 0.8976, AUC 0.9585320949554443, avg_entr 0.19562725722789764, f1 0.897599995136261
ep3_l0_test_time 0.5460996369999975
Test Epoch3 layer1 Acc 0.8938, AUC 0.9552178382873535, avg_entr 0.07645613700151443, f1 0.8938000202178955
ep3_l1_test_time 1.1821722669999986
Test Epoch3 layer2 Acc 0.8934, AUC 0.9546232223510742, avg_entr 0.04374781996011734, f1 0.8934000134468079
ep3_l2_test_time 1.808481190000009
Test Epoch3 layer3 Acc 0.8934, AUC 0.955367922782898, avg_entr 0.04039442166686058, f1 0.8934000134468079
ep3_l3_test_time 2.4310218829999712
Test Epoch3 layer4 Acc 0.8924, AUC 0.9561861753463745, avg_entr 0.04037103429436684, f1 0.8923999667167664
ep3_l4_test_time 3.071935769999982
gc 0
Train Epoch4 Acc 0.95105 (38042/40000), AUC 0.985487699508667
ep4_train_time 91.79274720100005
Test Epoch4 layer0 Acc 0.8998, AUC 0.9590494632720947, avg_entr 0.17616680264472961, f1 0.8998000025749207
ep4_l0_test_time 0.5487460900000087
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 4
Test Epoch4 layer1 Acc 0.8866, AUC 0.9511695504188538, avg_entr 0.050753314048051834, f1 0.8866000771522522
ep4_l1_test_time 1.1844814119999683
Test Epoch4 layer2 Acc 0.8866, AUC 0.9530354738235474, avg_entr 0.035183344036340714, f1 0.8866000771522522
ep4_l2_test_time 1.8136115109999764
Test Epoch4 layer3 Acc 0.8866, AUC 0.9524185061454773, avg_entr 0.03612938150763512, f1 0.8866000771522522
ep4_l3_test_time 2.4316457060000403
Test Epoch4 layer4 Acc 0.8868, AUC 0.9530717134475708, avg_entr 0.03558886796236038, f1 0.8867999911308289
ep4_l4_test_time 3.064493005000031
gc 0
Train Epoch5 Acc 0.95765 (38306/40000), AUC 0.9874837398529053
ep5_train_time 91.77453461599998
Test Epoch5 layer0 Acc 0.9, AUC 0.9588010311126709, avg_entr 0.1619967371225357, f1 0.8999999761581421
ep5_l0_test_time 0.5458252790000415
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 5
Test Epoch5 layer1 Acc 0.8898, AUC 0.948698878288269, avg_entr 0.04487019032239914, f1 0.8898000121116638
ep5_l1_test_time 1.1825570199999902
Test Epoch5 layer2 Acc 0.89, AUC 0.9527219533920288, avg_entr 0.03271607309579849, f1 0.8899999856948853
ep5_l2_test_time 1.8071439579999833
Test Epoch5 layer3 Acc 0.89, AUC 0.9523173570632935, avg_entr 0.0335405096411705, f1 0.8899999856948853
ep5_l3_test_time 2.4282561149999538
Test Epoch5 layer4 Acc 0.8894, AUC 0.9529225826263428, avg_entr 0.032906532287597656, f1 0.8894000053405762
ep5_l4_test_time 3.0651702359999717
gc 0
Train Epoch6 Acc 0.963075 (38523/40000), AUC 0.988990306854248
ep6_train_time 91.81838535400004
Test Epoch6 layer0 Acc 0.8908, AUC 0.9574102163314819, avg_entr 0.15134413540363312, f1 0.8907999992370605
ep6_l0_test_time 0.5479591980000578
Test Epoch6 layer1 Acc 0.8812, AUC 0.9463362693786621, avg_entr 0.03979364037513733, f1 0.8812000155448914
ep6_l1_test_time 1.1902610299999878
Test Epoch6 layer2 Acc 0.8834, AUC 0.9511351585388184, avg_entr 0.028737075626850128, f1 0.8834000825881958
ep6_l2_test_time 1.8071859640000412
Test Epoch6 layer3 Acc 0.8824, AUC 0.9506927728652954, avg_entr 0.02825850248336792, f1 0.8823999762535095
ep6_l3_test_time 2.4293724369999836
Test Epoch6 layer4 Acc 0.8816, AUC 0.9510661363601685, avg_entr 0.027775289490818977, f1 0.881600022315979
ep6_l4_test_time 3.0655778259999806
gc 0
Train Epoch7 Acc 0.966575 (38663/40000), AUC 0.9907931089401245
ep7_train_time 91.81203546699999
Test Epoch7 layer0 Acc 0.8928, AUC 0.9564937353134155, avg_entr 0.14602752029895782, f1 0.892799973487854
ep7_l0_test_time 0.5854050719999577
Test Epoch7 layer1 Acc 0.8838, AUC 0.9390007853507996, avg_entr 0.03331492096185684, f1 0.8838000297546387
ep7_l1_test_time 1.1845331220000617
Test Epoch7 layer2 Acc 0.8832, AUC 0.948271632194519, avg_entr 0.023952452465891838, f1 0.8831999897956848
ep7_l2_test_time 1.8082985839999992
Test Epoch7 layer3 Acc 0.883, AUC 0.9475338459014893, avg_entr 0.02298278920352459, f1 0.8830000162124634
ep7_l3_test_time 2.4387057669999876
Test Epoch7 layer4 Acc 0.8832, AUC 0.9482843279838562, avg_entr 0.022233882918953896, f1 0.8831999897956848
ep7_l4_test_time 3.064543275999995
gc 0
Train Epoch8 Acc 0.9699 (38796/40000), AUC 0.9918593764305115
ep8_train_time 91.81987622300005
Test Epoch8 layer0 Acc 0.8946, AUC 0.95455002784729, avg_entr 0.13872423768043518, f1 0.894599974155426
ep8_l0_test_time 0.5527027399999724
Test Epoch8 layer1 Acc 0.8788, AUC 0.9374175071716309, avg_entr 0.03148102015256882, f1 0.8787999749183655
ep8_l1_test_time 1.1850577720000501
Test Epoch8 layer2 Acc 0.8794, AUC 0.9466098546981812, avg_entr 0.02265136130154133, f1 0.8794000148773193
ep8_l2_test_time 1.8081567829999585
Test Epoch8 layer3 Acc 0.8786, AUC 0.9463545083999634, avg_entr 0.02169322781264782, f1 0.878600001335144
ep8_l3_test_time 2.432993017000058
Test Epoch8 layer4 Acc 0.879, AUC 0.9467847347259521, avg_entr 0.020942728966474533, f1 0.8790000081062317
ep8_l4_test_time 3.0646093310000424
gc 0
Train Epoch9 Acc 0.97335 (38934/40000), AUC 0.9930956363677979
ep9_train_time 91.80807089500001
Test Epoch9 layer0 Acc 0.881, AUC 0.952791690826416, avg_entr 0.1335398256778717, f1 0.8809999823570251
ep9_l0_test_time 0.5540980089999721
Test Epoch9 layer1 Acc 0.8776, AUC 0.9341292381286621, avg_entr 0.0304531492292881, f1 0.8776000142097473
ep9_l1_test_time 1.1920939799999815
Test Epoch9 layer2 Acc 0.8788, AUC 0.9450078010559082, avg_entr 0.021655194461345673, f1 0.8787999749183655
ep9_l2_test_time 1.8051770739998574
Test Epoch9 layer3 Acc 0.8778, AUC 0.9450215101242065, avg_entr 0.02029370702803135, f1 0.8778000473976135
ep9_l3_test_time 2.4275925730000836
Test Epoch9 layer4 Acc 0.8778, AUC 0.9456057548522949, avg_entr 0.019446654245257378, f1 0.8778000473976135
ep9_l4_test_time 3.0623007990000133
gc 0
Train Epoch10 Acc 0.97435 (38974/40000), AUC 0.9929818511009216
ep10_train_time 92.06634398700021
Test Epoch10 layer0 Acc 0.889, AUC 0.9528076648712158, avg_entr 0.13013069331645966, f1 0.8889999985694885
ep10_l0_test_time 0.5470067139999628
Test Epoch10 layer1 Acc 0.8774, AUC 0.9324314594268799, avg_entr 0.029074370861053467, f1 0.8773999810218811
ep10_l1_test_time 1.1850036050000199
Test Epoch10 layer2 Acc 0.8786, AUC 0.9439808130264282, avg_entr 0.020889708772301674, f1 0.878600001335144
ep10_l2_test_time 1.8243667230001392
Test Epoch10 layer3 Acc 0.8784, AUC 0.9439977407455444, avg_entr 0.019513092935085297, f1 0.8784000277519226
ep10_l3_test_time 2.434430660000089
Test Epoch10 layer4 Acc 0.8784, AUC 0.944749116897583, avg_entr 0.018780965358018875, f1 0.8784000277519226
ep10_l4_test_time 3.065583290000177
gc 0
Train Epoch11 Acc 0.9758 (39032/40000), AUC 0.9940402507781982
ep11_train_time 91.9291046689998
Test Epoch11 layer0 Acc 0.8914, AUC 0.9517897367477417, avg_entr 0.12839965522289276, f1 0.8913999795913696
ep11_l0_test_time 0.5475313709998773
Test Epoch11 layer1 Acc 0.874, AUC 0.9292563199996948, avg_entr 0.028069179505109787, f1 0.8740000128746033
ep11_l1_test_time 1.1829903699999704
Test Epoch11 layer2 Acc 0.875, AUC 0.9408640265464783, avg_entr 0.02047213725745678, f1 0.875
ep11_l2_test_time 1.807185225000012
Test Epoch11 layer3 Acc 0.8746, AUC 0.9415522813796997, avg_entr 0.01915907673537731, f1 0.8745999932289124
ep11_l3_test_time 2.430209806999983
Test Epoch11 layer4 Acc 0.875, AUC 0.9427742958068848, avg_entr 0.01826636493206024, f1 0.875
ep11_l4_test_time 3.063467060999983
gc 0
Train Epoch12 Acc 0.97655 (39062/40000), AUC 0.9941087365150452
ep12_train_time 91.80695876899995
Test Epoch12 layer0 Acc 0.8884, AUC 0.9506264925003052, avg_entr 0.12810947000980377, f1 0.8884000182151794
ep12_l0_test_time 0.5462850950000302
Test Epoch12 layer1 Acc 0.8734, AUC 0.9267715215682983, avg_entr 0.0275461096316576, f1 0.8733999729156494
ep12_l1_test_time 1.1815301430001455
Test Epoch12 layer2 Acc 0.8726, AUC 0.9393310546875, avg_entr 0.020339785143733025, f1 0.8726000189781189
ep12_l2_test_time 1.808456736999915
Test Epoch12 layer3 Acc 0.873, AUC 0.9403347373008728, avg_entr 0.019201135262846947, f1 0.8730000257492065
ep12_l3_test_time 2.4336649440001565
Test Epoch12 layer4 Acc 0.8734, AUC 0.9418666362762451, avg_entr 0.018419278785586357, f1 0.8733999729156494
ep12_l4_test_time 3.067131561999986
gc 0
Train Epoch13 Acc 0.977375 (39095/40000), AUC 0.9946427345275879
ep13_train_time 91.90795943700004
Test Epoch13 layer0 Acc 0.8874, AUC 0.9498662948608398, avg_entr 0.12458265572786331, f1 0.8873999714851379
ep13_l0_test_time 0.5466736299999866
Test Epoch13 layer1 Acc 0.872, AUC 0.9245374798774719, avg_entr 0.026123136281967163, f1 0.871999979019165
ep13_l1_test_time 1.181365243000073
Test Epoch13 layer2 Acc 0.8728, AUC 0.936828076839447, avg_entr 0.019114181399345398, f1 0.8727999925613403
ep13_l2_test_time 1.8220320360001097
Test Epoch13 layer3 Acc 0.8724, AUC 0.9388712048530579, avg_entr 0.01824582740664482, f1 0.8723999857902527
ep13_l3_test_time 2.460590280999895
Test Epoch13 layer4 Acc 0.8714, AUC 0.941415548324585, avg_entr 0.017519313842058182, f1 0.871399998664856
ep13_l4_test_time 3.12426467399996
gc 0
Train Epoch14 Acc 0.978775 (39151/40000), AUC 0.9944308996200562
ep14_train_time 91.94789975700019
Test Epoch14 layer0 Acc 0.8874, AUC 0.9494411945343018, avg_entr 0.1243683248758316, f1 0.8873999714851379
ep14_l0_test_time 0.5473191759999736
Test Epoch14 layer1 Acc 0.8714, AUC 0.9255034923553467, avg_entr 0.02663024514913559, f1 0.871399998664856
ep14_l1_test_time 1.183607522000102
Test Epoch14 layer2 Acc 0.8718, AUC 0.9371484518051147, avg_entr 0.019704602658748627, f1 0.8718000650405884
ep14_l2_test_time 1.807603464000067
Test Epoch14 layer3 Acc 0.8718, AUC 0.9392638802528381, avg_entr 0.0189551692456007, f1 0.8718000650405884
ep14_l3_test_time 2.440951299000062
Test Epoch14 layer4 Acc 0.872, AUC 0.940974235534668, avg_entr 0.018237115815281868, f1 0.871999979019165
ep14_l4_test_time 3.0767933439999524
gc 0
Train Epoch15 Acc 0.979025 (39161/40000), AUC 0.9953059554100037
ep15_train_time 91.897253825
Test Epoch15 layer0 Acc 0.886, AUC 0.9487797617912292, avg_entr 0.12158133834600449, f1 0.8859999775886536
ep15_l0_test_time 0.5515295269999569
Test Epoch15 layer1 Acc 0.8712, AUC 0.9226810932159424, avg_entr 0.0246486384421587, f1 0.8712000846862793
ep15_l1_test_time 1.1869289190001382
Test Epoch15 layer2 Acc 0.8706, AUC 0.934086799621582, avg_entr 0.018021849915385246, f1 0.8705999851226807
ep15_l2_test_time 1.808098586999904
Test Epoch15 layer3 Acc 0.8712, AUC 0.9371229410171509, avg_entr 0.017191700637340546, f1 0.8712000846862793
ep15_l3_test_time 2.4352320440000312
Test Epoch15 layer4 Acc 0.8712, AUC 0.9401247501373291, avg_entr 0.0163822490721941, f1 0.8712000846862793
ep15_l4_test_time 3.065403106000076
gc 0
Train Epoch16 Acc 0.979575 (39183/40000), AUC 0.9954541921615601
ep16_train_time 92.02394932499988
Test Epoch16 layer0 Acc 0.8856, AUC 0.9481781721115112, avg_entr 0.12012768536806107, f1 0.8855999708175659
ep16_l0_test_time 0.5492136729999402
Test Epoch16 layer1 Acc 0.8712, AUC 0.9223371744155884, avg_entr 0.025958020240068436, f1 0.8712000846862793
ep16_l1_test_time 1.214417795000145
Test Epoch16 layer2 Acc 0.8696, AUC 0.9333109855651855, avg_entr 0.01923488639295101, f1 0.8695999383926392
ep16_l2_test_time 1.8073317829998814
Test Epoch16 layer3 Acc 0.8696, AUC 0.9368306398391724, avg_entr 0.018691901117563248, f1 0.8695999383926392
ep16_l3_test_time 2.4313926030001767
Test Epoch16 layer4 Acc 0.8698, AUC 0.9398081302642822, avg_entr 0.01802181638777256, f1 0.8697999715805054
ep16_l4_test_time 3.0666201779999938
gc 0
Train Epoch17 Acc 0.980475 (39219/40000), AUC 0.9957127571105957
ep17_train_time 91.9625377750001
Test Epoch17 layer0 Acc 0.8844, AUC 0.9478985071182251, avg_entr 0.11968476325273514, f1 0.8844000101089478
ep17_l0_test_time 0.5693178599999555
Test Epoch17 layer1 Acc 0.8712, AUC 0.9210693836212158, avg_entr 0.0253969244658947, f1 0.8712000846862793
ep17_l1_test_time 1.1971321859998625
Test Epoch17 layer2 Acc 0.8702, AUC 0.9299389123916626, avg_entr 0.01830272004008293, f1 0.870199978351593
ep17_l2_test_time 1.8478107889998228
Test Epoch17 layer3 Acc 0.8704, AUC 0.9338589906692505, avg_entr 0.017685387283563614, f1 0.8704000115394592
ep17_l3_test_time 2.4610259419998783
Test Epoch17 layer4 Acc 0.8704, AUC 0.9387563467025757, avg_entr 0.016949286684393883, f1 0.8704000115394592
ep17_l4_test_time 3.064578809000068
gc 0
Train Epoch18 Acc 0.980775 (39231/40000), AUC 0.9955509901046753
ep18_train_time 92.07918597000003
Test Epoch18 layer0 Acc 0.884, AUC 0.9475933909416199, avg_entr 0.11882831901311874, f1 0.8840000033378601
ep18_l0_test_time 0.5732765830000517
Test Epoch18 layer1 Acc 0.8698, AUC 0.9200930595397949, avg_entr 0.024250781163573265, f1 0.8697999715805054
ep18_l1_test_time 1.1963528890000816
Test Epoch18 layer2 Acc 0.8696, AUC 0.9293822646141052, avg_entr 0.01700129173696041, f1 0.8695999383926392
ep18_l2_test_time 1.8258114699999624
Test Epoch18 layer3 Acc 0.8692, AUC 0.93354332447052, avg_entr 0.016543686389923096, f1 0.8691999912261963
ep18_l3_test_time 2.4540196630000537
Test Epoch18 layer4 Acc 0.8692, AUC 0.9383230209350586, avg_entr 0.015946416184306145, f1 0.8691999912261963
ep18_l4_test_time 3.090853186000004
gc 0
Train Epoch19 Acc 0.981 (39240/40000), AUC 0.9956842660903931
ep19_train_time 98.78813501599984
Test Epoch19 layer0 Acc 0.884, AUC 0.9473209381103516, avg_entr 0.11790883541107178, f1 0.8840000033378601
ep19_l0_test_time 0.5999652180000794
Test Epoch19 layer1 Acc 0.8698, AUC 0.9197167754173279, avg_entr 0.02379833534359932, f1 0.8697999715805054
ep19_l1_test_time 1.3164035069999045
Test Epoch19 layer2 Acc 0.8696, AUC 0.9285646677017212, avg_entr 0.016601594164967537, f1 0.8695999383926392
ep19_l2_test_time 2.0600991069998145
Test Epoch19 layer3 Acc 0.8696, AUC 0.9328237771987915, avg_entr 0.01621195487678051, f1 0.8695999383926392
ep19_l3_test_time 2.7533619299999827
Test Epoch19 layer4 Acc 0.8694, AUC 0.9377607703208923, avg_entr 0.015613928437232971, f1 0.8694000244140625
ep19_l4_test_time 3.0980955169998197
gc 0
Train Epoch20 Acc 0.981375 (39255/40000), AUC 0.9959838390350342
ep20_train_time 112.42678867300037
Test Epoch20 layer0 Acc 0.8832, AUC 0.9470036029815674, avg_entr 0.11700118333101273, f1 0.8831999897956848
ep20_l0_test_time 0.6385028060003606
Test Epoch20 layer1 Acc 0.8686, AUC 0.9192412495613098, avg_entr 0.02405811846256256, f1 0.8686000108718872
ep20_l1_test_time 1.4849932730003275
Test Epoch20 layer2 Acc 0.8692, AUC 0.9277467727661133, avg_entr 0.017009112983942032, f1 0.8691999912261963
ep20_l2_test_time 2.3474865170001067
Test Epoch20 layer3 Acc 0.869, AUC 0.9328022599220276, avg_entr 0.016470415517687798, f1 0.8690000176429749
ep20_l3_test_time 3.1245078609999837
Test Epoch20 layer4 Acc 0.8688, AUC 0.9373775720596313, avg_entr 0.015818187966942787, f1 0.8687999844551086
ep20_l4_test_time 3.7473671160000777
gc 0
Train Epoch21 Acc 0.98165 (39266/40000), AUC 0.9956032633781433
ep21_train_time 110.2472168270001
Test Epoch21 layer0 Acc 0.8834, AUC 0.9468304514884949, avg_entr 0.11623493582010269, f1 0.8834000825881958
ep21_l0_test_time 0.6419914850002897
Test Epoch21 layer1 Acc 0.8678, AUC 0.9187721014022827, avg_entr 0.023557042703032494, f1 0.8677999973297119
ep21_l1_test_time 1.4733010790000662
Test Epoch21 layer2 Acc 0.8678, AUC 0.9265724420547485, avg_entr 0.016430694609880447, f1 0.8677999973297119
ep21_l2_test_time 2.3514606769999773
Test Epoch21 layer3 Acc 0.8678, AUC 0.930751621723175, avg_entr 0.015952028334140778, f1 0.8677999973297119
ep21_l3_test_time 3.142341331999887
Test Epoch21 layer4 Acc 0.8678, AUC 0.9368666410446167, avg_entr 0.015315159223973751, f1 0.8677999973297119
ep21_l4_test_time 4.006540747000145
gc 0
Train Epoch22 Acc 0.9814 (39256/40000), AUC 0.9959883689880371
ep22_train_time 110.28288730700024
Test Epoch22 layer0 Acc 0.8828, AUC 0.9466650485992432, avg_entr 0.11617324501276016, f1 0.8827999830245972
ep22_l0_test_time 0.6486876809999558
Test Epoch22 layer1 Acc 0.87, AUC 0.9186863899230957, avg_entr 0.02282785251736641, f1 0.8700000047683716
ep22_l1_test_time 1.489455323999664
Test Epoch22 layer2 Acc 0.869, AUC 0.9262403249740601, avg_entr 0.015654025599360466, f1 0.8690000176429749
ep22_l2_test_time 2.3222729990002335
Test Epoch22 layer3 Acc 0.8692, AUC 0.9300020933151245, avg_entr 0.015386254526674747, f1 0.8691999912261963
ep22_l3_test_time 3.154845860000023
Test Epoch22 layer4 Acc 0.869, AUC 0.9367544651031494, avg_entr 0.014740824699401855, f1 0.8690000176429749
ep22_l4_test_time 4.038155669999924
gc 0
Train Epoch23 Acc 0.982175 (39287/40000), AUC 0.9961643815040588
ep23_train_time 113.10724973199967
Test Epoch23 layer0 Acc 0.8822, AUC 0.9465624690055847, avg_entr 0.11516863852739334, f1 0.8822000026702881
ep23_l0_test_time 0.8105663240003196
Test Epoch23 layer1 Acc 0.8686, AUC 0.9173134565353394, avg_entr 0.02342708222568035, f1 0.8686000108718872
ep23_l1_test_time 1.4839603969999189
Test Epoch23 layer2 Acc 0.8692, AUC 0.9261167645454407, avg_entr 0.01664968952536583, f1 0.8691999912261963
ep23_l2_test_time 2.1026351890000115
Test Epoch23 layer3 Acc 0.8698, AUC 0.9310338497161865, avg_entr 0.016462400555610657, f1 0.8697999715805054
ep23_l3_test_time 2.727112231999854
Test Epoch23 layer4 Acc 0.8698, AUC 0.936436653137207, avg_entr 0.01586836576461792, f1 0.8697999715805054
ep23_l4_test_time 3.349581083999965
gc 0
Train Epoch24 Acc 0.982125 (39285/40000), AUC 0.9960700273513794
ep24_train_time 120.68256380899993
Test Epoch24 layer0 Acc 0.883, AUC 0.9464043378829956, avg_entr 0.11502964049577713, f1 0.8830000162124634
ep24_l0_test_time 0.6044720010004312
Test Epoch24 layer1 Acc 0.868, AUC 0.9179583787918091, avg_entr 0.022560596466064453, f1 0.8679999709129333
ep24_l1_test_time 1.305904486000145
Test Epoch24 layer2 Acc 0.8684, AUC 0.9245405197143555, avg_entr 0.015233371406793594, f1 0.868399977684021
ep24_l2_test_time 2.0427166269996633
Test Epoch24 layer3 Acc 0.8686, AUC 0.9284141063690186, avg_entr 0.01501984242349863, f1 0.8686000108718872
ep24_l3_test_time 2.644767475000208
Test Epoch24 layer4 Acc 0.8686, AUC 0.936087965965271, avg_entr 0.014400537125766277, f1 0.8686000108718872
ep24_l4_test_time 3.0796881080000276
gc 0
Train Epoch25 Acc 0.9821 (39284/40000), AUC 0.9962774515151978
ep25_train_time 113.88400437900009
Test Epoch25 layer0 Acc 0.8828, AUC 0.9463487863540649, avg_entr 0.11470572650432587, f1 0.8827999830245972
ep25_l0_test_time 0.6794841979999546
Test Epoch25 layer1 Acc 0.8674, AUC 0.9174724221229553, avg_entr 0.022415000945329666, f1 0.8673999905586243
ep25_l1_test_time 1.478908102999867
Test Epoch25 layer2 Acc 0.8684, AUC 0.9237264394760132, avg_entr 0.015061230398714542, f1 0.868399977684021
ep25_l2_test_time 2.3373044499999196
Test Epoch25 layer3 Acc 0.868, AUC 0.9278199672698975, avg_entr 0.014840063638985157, f1 0.8679999709129333
ep25_l3_test_time 3.1459315659999447
Test Epoch25 layer4 Acc 0.868, AUC 0.9359166622161865, avg_entr 0.014260847121477127, f1 0.8679999709129333
ep25_l4_test_time 4.017336882000109
gc 0
Train Epoch26 Acc 0.982275 (39291/40000), AUC 0.9961103200912476
ep26_train_time 108.85668411999995
Test Epoch26 layer0 Acc 0.8818, AUC 0.9462778568267822, avg_entr 0.11388655006885529, f1 0.8818000555038452
ep26_l0_test_time 0.6496156450002672
Test Epoch26 layer1 Acc 0.8664, AUC 0.9172660112380981, avg_entr 0.02237769030034542, f1 0.8664000034332275
ep26_l1_test_time 1.483754835000127
Test Epoch26 layer2 Acc 0.8662, AUC 0.9238952398300171, avg_entr 0.015197502449154854, f1 0.8661999702453613
ep26_l2_test_time 2.339819022000029
Test Epoch26 layer3 Acc 0.8666, AUC 0.928380012512207, avg_entr 0.014859477058053017, f1 0.866599977016449
ep26_l3_test_time 3.1343372669998644
Test Epoch26 layer4 Acc 0.8668, AUC 0.9358437061309814, avg_entr 0.01427480485290289, f1 0.8668000102043152
ep26_l4_test_time 4.033592151999983
gc 0
Train Epoch27 Acc 0.9824 (39296/40000), AUC 0.9961041212081909
ep27_train_time 109.45825113499995
Test Epoch27 layer0 Acc 0.882, AUC 0.9461948871612549, avg_entr 0.11324440687894821, f1 0.8820000290870667
ep27_l0_test_time 0.5461708389998421
Test Epoch27 layer1 Acc 0.8678, AUC 0.9171053171157837, avg_entr 0.022236144170165062, f1 0.8677999973297119
ep27_l1_test_time 1.183208211999954
Test Epoch27 layer2 Acc 0.8682, AUC 0.9241231083869934, avg_entr 0.015078422613441944, f1 0.8682000041007996
ep27_l2_test_time 1.8084504829998878
Test Epoch27 layer3 Acc 0.868, AUC 0.9287712574005127, avg_entr 0.014741163700819016, f1 0.8679999709129333
ep27_l3_test_time 2.4313320219998786
Test Epoch27 layer4 Acc 0.868, AUC 0.935705304145813, avg_entr 0.01418515294790268, f1 0.8679999709129333
ep27_l4_test_time 3.0680343949998132
gc 0
Train Epoch28 Acc 0.982175 (39287/40000), AUC 0.9961996078491211
ep28_train_time 91.78886292700008
Test Epoch28 layer0 Acc 0.8816, AUC 0.9461451768875122, avg_entr 0.11236944794654846, f1 0.881600022315979
ep28_l0_test_time 0.547405649999746
Test Epoch28 layer1 Acc 0.867, AUC 0.9169917106628418, avg_entr 0.021908100694417953, f1 0.8669999837875366
ep28_l1_test_time 1.1849797549998584
Test Epoch28 layer2 Acc 0.8664, AUC 0.9228742122650146, avg_entr 0.014693896286189556, f1 0.8664000034332275
ep28_l2_test_time 1.8084499960000358
Test Epoch28 layer3 Acc 0.8668, AUC 0.9270797967910767, avg_entr 0.014433279633522034, f1 0.8668000102043152
ep28_l3_test_time 2.429498744999819
Test Epoch28 layer4 Acc 0.8668, AUC 0.9354990720748901, avg_entr 0.01389849279075861, f1 0.8668000102043152
ep28_l4_test_time 3.062539271999867
gc 0
Train Epoch29 Acc 0.982525 (39301/40000), AUC 0.9963463544845581
ep29_train_time 91.7914742439998
Test Epoch29 layer0 Acc 0.881, AUC 0.9460976123809814, avg_entr 0.11218743771314621, f1 0.8809999823570251
ep29_l0_test_time 0.5457331259999592
Test Epoch29 layer1 Acc 0.8664, AUC 0.9170658588409424, avg_entr 0.021867485716938972, f1 0.8664000034332275
ep29_l1_test_time 1.1793931500001236
Test Epoch29 layer2 Acc 0.8668, AUC 0.9234659671783447, avg_entr 0.014618558809161186, f1 0.8668000102043152
ep29_l2_test_time 1.8074088870002925
Test Epoch29 layer3 Acc 0.8674, AUC 0.9280908107757568, avg_entr 0.014301766641438007, f1 0.8673999905586243
ep29_l3_test_time 2.435616298000241
Test Epoch29 layer4 Acc 0.8674, AUC 0.9353796243667603, avg_entr 0.013762008398771286, f1 0.8673999905586243
ep29_l4_test_time 3.0671119150001687
Best AUC tensor(0.9000) 5 0
train_loss (2, 5, 30)
valid_acc (5, 30)
valid_AUC (5, 30)
train_acc (30,)
total_train+valid_time 3216.499824766
Start Testing
Load ckpt at ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt
Test layer0 Acc 0.8904, AUC 0.9550764560699463, avg_entr 0.16186034679412842, f1 0.8903999924659729
l0_test_time 0.543793537000056
Test layer1 Acc 0.8852, AUC 0.9459689855575562, avg_entr 0.046491045504808426, f1 0.8852000832557678
l1_test_time 1.181094600999586
Test layer2 Acc 0.8854, AUC 0.9522758722305298, avg_entr 0.034188997000455856, f1 0.8853999972343445
l2_test_time 1.8101292480000666
Test layer3 Acc 0.8852, AUC 0.9515478014945984, avg_entr 0.035146214067935944, f1 0.8852000832557678
l3_test_time 2.4386644420001176
Test layer4 Acc 0.8858, AUC 0.9518257975578308, avg_entr 0.03446712717413902, f1 0.8858000040054321
l4_test_time 3.0714380710001024
