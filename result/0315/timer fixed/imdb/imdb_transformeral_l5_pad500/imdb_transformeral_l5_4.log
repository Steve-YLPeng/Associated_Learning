total count words 222751
vocab size 30000
train size 40000, valid size 5000, test size 5000
found 27937 words in glove
model: TransformerModelML(
  (layers): ModuleList(
    (0): EMBLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Embedding(30000, 300)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=2, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=2, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.weight 9000000
layers.0.ae.g.0.weight 256
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 256
layers.0.ae.h.0.bias 2
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
Total Trainable Params: 13671778
init_time 21.526390945000003
Start Training
gc 0
Train Epoch0 Acc 0.52425 (20970/40000), AUC 0.550391674041748
ep0_train_time 91.99628524600001
Test Epoch0 layer0 Acc 0.7842, AUC 0.8921124339103699, avg_entr 0.5466872453689575, f1 0.7842000126838684
ep0_l0_test_time 0.5502108359999909
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 0
Test Epoch0 layer1 Acc 0.8354, AUC 0.9141807556152344, avg_entr 0.3292553722858429, f1 0.8353999853134155
ep0_l1_test_time 1.176990743999994
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 0
Test Epoch0 layer2 Acc 0.8374, AUC 0.9147495031356812, avg_entr 0.3792521059513092, f1 0.8374000191688538
ep0_l2_test_time 1.7977318829999973
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 0
Test Epoch0 layer3 Acc 0.8258, AUC 0.9177291989326477, avg_entr 0.5211519598960876, f1 0.8258000016212463
ep0_l3_test_time 2.427463047999993
Test Epoch0 layer4 Acc 0.7338, AUC 0.9142714738845825, avg_entr 0.6332200169563293, f1 0.7338000535964966
ep0_l4_test_time 3.051378541999995
gc 0
Train Epoch1 Acc 0.85995 (34398/40000), AUC 0.9299468994140625
ep1_train_time 91.64199142500001
Test Epoch1 layer0 Acc 0.8734, AUC 0.9458820819854736, avg_entr 0.28138306736946106, f1 0.8733999729156494
ep1_l0_test_time 0.5430523020000066
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 1
Test Epoch1 layer1 Acc 0.8884, AUC 0.9536440372467041, avg_entr 0.18620388209819794, f1 0.8884000182151794
ep1_l1_test_time 1.1765346699999952
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 1
Test Epoch1 layer2 Acc 0.8914, AUC 0.9552764892578125, avg_entr 0.15586085617542267, f1 0.8913999795913696
ep1_l2_test_time 1.8044072359999745
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 1
Test Epoch1 layer3 Acc 0.8886, AUC 0.9554388523101807, avg_entr 0.1301901638507843, f1 0.8885999917984009
ep1_l3_test_time 2.4277582050000035
Test Epoch1 layer4 Acc 0.886, AUC 0.9553938508033752, avg_entr 0.11851063370704651, f1 0.8859999775886536
ep1_l4_test_time 3.0554735679999965
gc 0
Train Epoch2 Acc 0.91255 (36502/40000), AUC 0.9665275812149048
ep2_train_time 91.67698628500003
Test Epoch2 layer0 Acc 0.8916, AUC 0.9549641609191895, avg_entr 0.22520045936107635, f1 0.8916000127792358
ep2_l0_test_time 0.5451226079999856
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 2
Test Epoch2 layer1 Acc 0.8958, AUC 0.9591363668441772, avg_entr 0.13982614874839783, f1 0.895799994468689
ep2_l1_test_time 1.1847671269999864
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 2
Test Epoch2 layer2 Acc 0.8974, AUC 0.9582434892654419, avg_entr 0.08172360807657242, f1 0.8974000215530396
ep2_l2_test_time 1.8073933740000143
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 2
Test Epoch2 layer3 Acc 0.898, AUC 0.9600017666816711, avg_entr 0.06410104036331177, f1 0.8980000019073486
ep2_l3_test_time 2.432605265999996
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 2
Test Epoch2 layer4 Acc 0.8978, AUC 0.9597755670547485, avg_entr 0.05424921214580536, f1 0.8978000283241272
ep2_l4_test_time 3.060512973000016
gc 0
Train Epoch3 Acc 0.939825 (37593/40000), AUC 0.9800775647163391
ep3_train_time 91.69276389700002
Test Epoch3 layer0 Acc 0.8988, AUC 0.9582806825637817, avg_entr 0.1938706487417221, f1 0.8988000154495239
ep3_l0_test_time 0.5451572650000003
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 3
Test Epoch3 layer1 Acc 0.8924, AUC 0.9569204449653625, avg_entr 0.07421787828207016, f1 0.8923999667167664
ep3_l1_test_time 1.1774563099999682
Test Epoch3 layer2 Acc 0.8916, AUC 0.9559112787246704, avg_entr 0.04425959289073944, f1 0.8916000127792358
ep3_l2_test_time 1.797885602000008
Test Epoch3 layer3 Acc 0.8926, AUC 0.9570595026016235, avg_entr 0.042418017983436584, f1 0.8925999999046326
ep3_l3_test_time 2.4241831829999683
Test Epoch3 layer4 Acc 0.8926, AUC 0.956968367099762, avg_entr 0.03868837282061577, f1 0.8925999999046326
ep3_l4_test_time 3.053192303000003
gc 0
Train Epoch4 Acc 0.951925 (38077/40000), AUC 0.9851455688476562
ep4_train_time 91.71436151200004
Test Epoch4 layer0 Acc 0.8922, AUC 0.9589109420776367, avg_entr 0.17354212701320648, f1 0.8921999931335449
ep4_l0_test_time 0.544721318000029
Test Epoch4 layer1 Acc 0.8828, AUC 0.9517108201980591, avg_entr 0.05090388283133507, f1 0.8827999830245972
ep4_l1_test_time 1.1758753440000191
Test Epoch4 layer2 Acc 0.8834, AUC 0.9546473026275635, avg_entr 0.035979341715574265, f1 0.8834000825881958
ep4_l2_test_time 1.7965350289999833
Test Epoch4 layer3 Acc 0.8836, AUC 0.9551804065704346, avg_entr 0.03607436269521713, f1 0.8835999965667725
ep4_l3_test_time 2.425992721000057
Test Epoch4 layer4 Acc 0.8832, AUC 0.9551025629043579, avg_entr 0.0347057580947876, f1 0.8831999897956848
ep4_l4_test_time 3.0558091689999856
gc 0
Train Epoch5 Acc 0.958575 (38343/40000), AUC 0.9873538017272949
ep5_train_time 91.64147096099998
Test Epoch5 layer0 Acc 0.8922, AUC 0.9583914279937744, avg_entr 0.16130448877811432, f1 0.8921999931335449
ep5_l0_test_time 0.542066114000022
Test Epoch5 layer1 Acc 0.8872, AUC 0.9485207796096802, avg_entr 0.04469497501850128, f1 0.8871999979019165
ep5_l1_test_time 1.174107546000073
Test Epoch5 layer2 Acc 0.8888, AUC 0.9523183703422546, avg_entr 0.0332217775285244, f1 0.8888000249862671
ep5_l2_test_time 1.79403688900004
Test Epoch5 layer3 Acc 0.8886, AUC 0.9526419639587402, avg_entr 0.03283219784498215, f1 0.8885999917984009
ep5_l3_test_time 2.4230029389999572
Test Epoch5 layer4 Acc 0.8882, AUC 0.952703595161438, avg_entr 0.03191429376602173, f1 0.888200044631958
ep5_l4_test_time 3.051564338999924
gc 0
Train Epoch6 Acc 0.9621 (38484/40000), AUC 0.9889225363731384
ep6_train_time 91.695496972
Test Epoch6 layer0 Acc 0.887, AUC 0.9576122760772705, avg_entr 0.14925307035446167, f1 0.8870000243186951
ep6_l0_test_time 0.5424766419999969
Test Epoch6 layer1 Acc 0.876, AUC 0.9471914172172546, avg_entr 0.040502529591321945, f1 0.8760000467300415
ep6_l1_test_time 1.172147074999998
Test Epoch6 layer2 Acc 0.8792, AUC 0.9502289295196533, avg_entr 0.030843988060951233, f1 0.8791999816894531
ep6_l2_test_time 1.7955925059999345
Test Epoch6 layer3 Acc 0.8786, AUC 0.9505264759063721, avg_entr 0.030062764883041382, f1 0.878600001335144
ep6_l3_test_time 2.427490325000008
Test Epoch6 layer4 Acc 0.8768, AUC 0.9505008459091187, avg_entr 0.029893163591623306, f1 0.876800000667572
ep6_l4_test_time 3.0534539110000196
gc 0
Train Epoch7 Acc 0.965825 (38633/40000), AUC 0.9911433458328247
ep7_train_time 91.70956807800007
Test Epoch7 layer0 Acc 0.8904, AUC 0.9561405181884766, avg_entr 0.14125660061836243, f1 0.8903999924659729
ep7_l0_test_time 0.5430856909999875
Test Epoch7 layer1 Acc 0.8802, AUC 0.9417188763618469, avg_entr 0.03360893577337265, f1 0.8802000284194946
ep7_l1_test_time 1.1754978120000033
Test Epoch7 layer2 Acc 0.8806, AUC 0.9486376047134399, avg_entr 0.025827962905168533, f1 0.8805999755859375
ep7_l2_test_time 1.795864875999996
Test Epoch7 layer3 Acc 0.8802, AUC 0.9488555788993835, avg_entr 0.025209488347172737, f1 0.8802000284194946
ep7_l3_test_time 2.422617392999996
Test Epoch7 layer4 Acc 0.8814, AUC 0.9489874839782715, avg_entr 0.024704905226826668, f1 0.8813999891281128
ep7_l4_test_time 3.0526223120000395
gc 0
Train Epoch8 Acc 0.969075 (38763/40000), AUC 0.9920336008071899
ep8_train_time 91.64066837099995
Test Epoch8 layer0 Acc 0.882, AUC 0.9542720317840576, avg_entr 0.13498225808143616, f1 0.8820000290870667
ep8_l0_test_time 0.5593411759999753
Test Epoch8 layer1 Acc 0.8794, AUC 0.9407882690429688, avg_entr 0.032912466675043106, f1 0.8794000148773193
ep8_l1_test_time 1.1741921249999905
Test Epoch8 layer2 Acc 0.8796, AUC 0.9473730325698853, avg_entr 0.024348128587007523, f1 0.8795999884605408
ep8_l2_test_time 1.79793534800001
Test Epoch8 layer3 Acc 0.8798, AUC 0.9476568698883057, avg_entr 0.022987890988588333, f1 0.879800021648407
ep8_l3_test_time 2.4248286190000954
Test Epoch8 layer4 Acc 0.8792, AUC 0.9478023052215576, avg_entr 0.022748315706849098, f1 0.8791999816894531
ep8_l4_test_time 3.0582587869999998
gc 0
Train Epoch9 Acc 0.9733 (38932/40000), AUC 0.9933356642723083
ep9_train_time 91.71153158499999
Test Epoch9 layer0 Acc 0.8918, AUC 0.9535915851593018, avg_entr 0.13360024988651276, f1 0.8917999863624573
ep9_l0_test_time 0.5423575850001043
Test Epoch9 layer1 Acc 0.876, AUC 0.9338724613189697, avg_entr 0.02933603525161743, f1 0.8760000467300415
ep9_l1_test_time 1.1760015359999443
Test Epoch9 layer2 Acc 0.8762, AUC 0.945569634437561, avg_entr 0.021041106432676315, f1 0.8762000203132629
ep9_l2_test_time 1.8072431780000215
Test Epoch9 layer3 Acc 0.8768, AUC 0.9462957978248596, avg_entr 0.01958010345697403, f1 0.876800000667572
ep9_l3_test_time 2.4260407790000045
Test Epoch9 layer4 Acc 0.8766, AUC 0.9465816020965576, avg_entr 0.01910104975104332, f1 0.8766000270843506
ep9_l4_test_time 3.0574693369999295
gc 0
Train Epoch10 Acc 0.97435 (38974/40000), AUC 0.9930946230888367
ep10_train_time 91.70626341699995
Test Epoch10 layer0 Acc 0.8926, AUC 0.9523982405662537, avg_entr 0.1300450563430786, f1 0.8925999999046326
ep10_l0_test_time 0.5418726089999382
Test Epoch10 layer1 Acc 0.8746, AUC 0.9303169846534729, avg_entr 0.02842506393790245, f1 0.8745999932289124
ep10_l1_test_time 1.1698402629999691
Test Epoch10 layer2 Acc 0.875, AUC 0.9436129927635193, avg_entr 0.02125433087348938, f1 0.875
ep10_l2_test_time 1.7955664800001614
Test Epoch10 layer3 Acc 0.875, AUC 0.9445990324020386, avg_entr 0.01971566677093506, f1 0.875
ep10_l3_test_time 2.42424178400006
Test Epoch10 layer4 Acc 0.8748, AUC 0.9450442790985107, avg_entr 0.01940154656767845, f1 0.8748000264167786
ep10_l4_test_time 3.0531161769999926
gc 0
Train Epoch11 Acc 0.97595 (39038/40000), AUC 0.9942623376846313
ep11_train_time 91.7046314690001
Test Epoch11 layer0 Acc 0.887, AUC 0.9513431787490845, avg_entr 0.12634792923927307, f1 0.8870000243186951
ep11_l0_test_time 0.5433626359999835
Test Epoch11 layer1 Acc 0.8744, AUC 0.9307283163070679, avg_entr 0.027706820517778397, f1 0.8744000792503357
ep11_l1_test_time 1.1726649260001523
Test Epoch11 layer2 Acc 0.8752, AUC 0.9433732032775879, avg_entr 0.020611800253391266, f1 0.8751999735832214
ep11_l2_test_time 1.7961887699998442
Test Epoch11 layer3 Acc 0.8752, AUC 0.9441519379615784, avg_entr 0.01911088638007641, f1 0.8751999735832214
ep11_l3_test_time 2.422298542999897
Test Epoch11 layer4 Acc 0.8748, AUC 0.944631814956665, avg_entr 0.01889476552605629, f1 0.8748000264167786
ep11_l4_test_time 3.0527532710000287
gc 0
Train Epoch12 Acc 0.976575 (39063/40000), AUC 0.9944511651992798
ep12_train_time 91.71565539699986
Test Epoch12 layer0 Acc 0.8882, AUC 0.9500846862792969, avg_entr 0.12602919340133667, f1 0.888200044631958
ep12_l0_test_time 0.5406408620001457
Test Epoch12 layer1 Acc 0.8738, AUC 0.930692195892334, avg_entr 0.028230419382452965, f1 0.8737999796867371
ep12_l1_test_time 1.1747421869999926
Test Epoch12 layer2 Acc 0.8742, AUC 0.9414879083633423, avg_entr 0.021294599398970604, f1 0.8741999864578247
ep12_l2_test_time 1.8041441289999511
Test Epoch12 layer3 Acc 0.8742, AUC 0.9430804252624512, avg_entr 0.019146863371133804, f1 0.8741999864578247
ep12_l3_test_time 2.4312640050000027
Test Epoch12 layer4 Acc 0.8744, AUC 0.9435915946960449, avg_entr 0.01890558935701847, f1 0.8744000792503357
ep12_l4_test_time 3.0595084509998287
gc 0
Train Epoch13 Acc 0.978375 (39135/40000), AUC 0.9948949813842773
ep13_train_time 91.7250055500001
Test Epoch13 layer0 Acc 0.8872, AUC 0.949674665927887, avg_entr 0.12476694583892822, f1 0.8871999979019165
ep13_l0_test_time 0.5478113400001803
Test Epoch13 layer1 Acc 0.8714, AUC 0.9278770685195923, avg_entr 0.027368389070034027, f1 0.871399998664856
ep13_l1_test_time 1.1768234080000184
Test Epoch13 layer2 Acc 0.8712, AUC 0.938615083694458, avg_entr 0.020202307030558586, f1 0.8712000846862793
ep13_l2_test_time 1.796290018000036
Test Epoch13 layer3 Acc 0.8714, AUC 0.9421683549880981, avg_entr 0.01828359067440033, f1 0.871399998664856
ep13_l3_test_time 2.4244594179999694
Test Epoch13 layer4 Acc 0.8714, AUC 0.9429917335510254, avg_entr 0.01810271292924881, f1 0.871399998664856
ep13_l4_test_time 3.058256250000113
gc 0
Train Epoch14 Acc 0.97885 (39154/40000), AUC 0.9948180913925171
ep14_train_time 91.69194496
Test Epoch14 layer0 Acc 0.8872, AUC 0.949015200138092, avg_entr 0.12283338606357574, f1 0.8871999979019165
ep14_l0_test_time 0.5419465270001638
Test Epoch14 layer1 Acc 0.8722, AUC 0.9267330765724182, avg_entr 0.026879101991653442, f1 0.872200071811676
ep14_l1_test_time 1.1767097580000154
Test Epoch14 layer2 Acc 0.8732, AUC 0.9373471736907959, avg_entr 0.020120514556765556, f1 0.873199999332428
ep14_l2_test_time 1.7945162739999887
Test Epoch14 layer3 Acc 0.8734, AUC 0.9416704177856445, avg_entr 0.018150612711906433, f1 0.8733999729156494
ep14_l3_test_time 2.424081311000009
Test Epoch14 layer4 Acc 0.8732, AUC 0.9427562355995178, avg_entr 0.017823470756411552, f1 0.873199999332428
ep14_l4_test_time 3.0511471439999696
gc 0
Train Epoch15 Acc 0.97935 (39174/40000), AUC 0.9952062368392944
ep15_train_time 91.75434205600004
Test Epoch15 layer0 Acc 0.885, AUC 0.9484169483184814, avg_entr 0.12187473475933075, f1 0.8849999904632568
ep15_l0_test_time 0.5409237999999732
Test Epoch15 layer1 Acc 0.8686, AUC 0.9241424202919006, avg_entr 0.02635125443339348, f1 0.8686000108718872
ep15_l1_test_time 1.1728908319998936
Test Epoch15 layer2 Acc 0.8702, AUC 0.9361560940742493, avg_entr 0.01955128088593483, f1 0.870199978351593
ep15_l2_test_time 1.794256667999889
Test Epoch15 layer3 Acc 0.8696, AUC 0.9405242204666138, avg_entr 0.017679080367088318, f1 0.8695999383926392
ep15_l3_test_time 2.421751623999853
Test Epoch15 layer4 Acc 0.8696, AUC 0.9417752623558044, avg_entr 0.01754874177277088, f1 0.8695999383926392
ep15_l4_test_time 3.0506486340000265
gc 0
Train Epoch16 Acc 0.979875 (39195/40000), AUC 0.9952480792999268
ep16_train_time 91.63452383000003
Test Epoch16 layer0 Acc 0.8854, AUC 0.9479336142539978, avg_entr 0.12097014486789703, f1 0.8853999972343445
ep16_l0_test_time 0.5412216089998765
Test Epoch16 layer1 Acc 0.8698, AUC 0.92377769947052, avg_entr 0.026300903409719467, f1 0.8697999715805054
ep16_l1_test_time 1.1734811960000116
Test Epoch16 layer2 Acc 0.8694, AUC 0.9340778589248657, avg_entr 0.019803741946816444, f1 0.8694000244140625
ep16_l2_test_time 1.7939786920001097
Test Epoch16 layer3 Acc 0.87, AUC 0.9398223161697388, avg_entr 0.017928721383213997, f1 0.8700000047683716
ep16_l3_test_time 2.423566974000096
Test Epoch16 layer4 Acc 0.8702, AUC 0.9414709806442261, avg_entr 0.01787000149488449, f1 0.870199978351593
ep16_l4_test_time 3.053070684999966
gc 0
Train Epoch17 Acc 0.9806 (39224/40000), AUC 0.9953973293304443
ep17_train_time 91.62422361499989
Test Epoch17 layer0 Acc 0.8848, AUC 0.9476573467254639, avg_entr 0.11856788396835327, f1 0.8848000168800354
ep17_l0_test_time 0.5417203550000522
Test Epoch17 layer1 Acc 0.8682, AUC 0.9222086668014526, avg_entr 0.0250090304762125, f1 0.8682000041007996
ep17_l1_test_time 1.1762871310002083
Test Epoch17 layer2 Acc 0.8678, AUC 0.9324038028717041, avg_entr 0.018579209223389626, f1 0.8677999973297119
ep17_l2_test_time 1.7981732569999167
Test Epoch17 layer3 Acc 0.8688, AUC 0.9389137625694275, avg_entr 0.0167531818151474, f1 0.8687999844551086
ep17_l3_test_time 2.424501864000149
Test Epoch17 layer4 Acc 0.8684, AUC 0.9409751892089844, avg_entr 0.016765693202614784, f1 0.868399977684021
ep17_l4_test_time 3.0519204239999453
gc 0
Train Epoch18 Acc 0.9808 (39232/40000), AUC 0.9955235123634338
ep18_train_time 91.63267454700008
Test Epoch18 layer0 Acc 0.8844, AUC 0.9473515152931213, avg_entr 0.11849810928106308, f1 0.8844000101089478
ep18_l0_test_time 0.5418200799999795
Test Epoch18 layer1 Acc 0.8702, AUC 0.9221711158752441, avg_entr 0.02576417289674282, f1 0.870199978351593
ep18_l1_test_time 1.1752616119999857
Test Epoch18 layer2 Acc 0.8702, AUC 0.9331099987030029, avg_entr 0.01960204541683197, f1 0.870199978351593
ep18_l2_test_time 1.7949198690000685
Test Epoch18 layer3 Acc 0.8698, AUC 0.9387288689613342, avg_entr 0.018002556636929512, f1 0.8697999715805054
ep18_l3_test_time 2.423284457000136
Test Epoch18 layer4 Acc 0.8696, AUC 0.940753698348999, avg_entr 0.018097972497344017, f1 0.8695999383926392
ep18_l4_test_time 3.0504381650000596
gc 0
Train Epoch19 Acc 0.981375 (39255/40000), AUC 0.9959989786148071
ep19_train_time 91.61975038000014
Test Epoch19 layer0 Acc 0.8842, AUC 0.9469467401504517, avg_entr 0.11610917001962662, f1 0.8842000365257263
ep19_l0_test_time 0.5484387589999642
Test Epoch19 layer1 Acc 0.869, AUC 0.9207830429077148, avg_entr 0.025574900209903717, f1 0.8690000176429749
ep19_l1_test_time 1.1762295930000164
Test Epoch19 layer2 Acc 0.8686, AUC 0.9306269288063049, avg_entr 0.01927296631038189, f1 0.8686000108718872
ep19_l2_test_time 1.7952180429999771
Test Epoch19 layer3 Acc 0.8692, AUC 0.9377937316894531, avg_entr 0.017434008419513702, f1 0.8691999912261963
ep19_l3_test_time 2.433235565000132
Test Epoch19 layer4 Acc 0.869, AUC 0.9404963254928589, avg_entr 0.0175396129488945, f1 0.8690000176429749
ep19_l4_test_time 3.0535995810000713
gc 0
Train Epoch20 Acc 0.981225 (39249/40000), AUC 0.9957022070884705
ep20_train_time 91.64615485800005
Test Epoch20 layer0 Acc 0.8836, AUC 0.9467402696609497, avg_entr 0.11639803647994995, f1 0.8835999965667725
ep20_l0_test_time 0.5512000929998067
Test Epoch20 layer1 Acc 0.8684, AUC 0.9198054075241089, avg_entr 0.024808358401060104, f1 0.868399977684021
ep20_l1_test_time 1.177187412999956
Test Epoch20 layer2 Acc 0.8688, AUC 0.9311561584472656, avg_entr 0.018760913982987404, f1 0.8687999844551086
ep20_l2_test_time 1.7997170420003386
Test Epoch20 layer3 Acc 0.8686, AUC 0.9376474022865295, avg_entr 0.016821950674057007, f1 0.8686000108718872
ep20_l3_test_time 2.4272890350002854
Test Epoch20 layer4 Acc 0.8686, AUC 0.9402898550033569, avg_entr 0.016819601878523827, f1 0.8686000108718872
ep20_l4_test_time 3.0587545110001884
gc 0
Train Epoch21 Acc 0.9817 (39268/40000), AUC 0.9956888556480408
ep21_train_time 91.71323501699999
Test Epoch21 layer0 Acc 0.8838, AUC 0.9465851187705994, avg_entr 0.11553588509559631, f1 0.8838000297546387
ep21_l0_test_time 0.5391761670002779
Test Epoch21 layer1 Acc 0.8678, AUC 0.9200280904769897, avg_entr 0.023816894739866257, f1 0.8677999973297119
ep21_l1_test_time 1.1749871449997045
Test Epoch21 layer2 Acc 0.867, AUC 0.92948979139328, avg_entr 0.017399046570062637, f1 0.8669999837875366
ep21_l2_test_time 1.7946476479996818
Test Epoch21 layer3 Acc 0.8676, AUC 0.9367292523384094, avg_entr 0.015388055704534054, f1 0.8675999641418457
ep21_l3_test_time 2.422178876000089
Test Epoch21 layer4 Acc 0.8676, AUC 0.9401496648788452, avg_entr 0.015322097577154636, f1 0.8675999641418457
ep21_l4_test_time 3.048442143000102
gc 0
Train Epoch22 Acc 0.981675 (39267/40000), AUC 0.9958984851837158
ep22_train_time 91.69315036000035
Test Epoch22 layer0 Acc 0.8836, AUC 0.9464454650878906, avg_entr 0.11464941501617432, f1 0.8835999965667725
ep22_l0_test_time 0.5421898750000764
Test Epoch22 layer1 Acc 0.8676, AUC 0.9190570712089539, avg_entr 0.024003369733691216, f1 0.8675999641418457
ep22_l1_test_time 1.1772305029999188
Test Epoch22 layer2 Acc 0.8668, AUC 0.9290072321891785, avg_entr 0.017858661711215973, f1 0.8668000102043152
ep22_l2_test_time 1.7945577000000412
Test Epoch22 layer3 Acc 0.8674, AUC 0.9364117383956909, avg_entr 0.01612127013504505, f1 0.8673999905586243
ep22_l3_test_time 2.4245464539999375
Test Epoch22 layer4 Acc 0.8678, AUC 0.9399048686027527, avg_entr 0.016108255833387375, f1 0.8677999973297119
ep22_l4_test_time 3.0500208859998565
gc 0
Train Epoch23 Acc 0.98195 (39278/40000), AUC 0.9959884881973267
ep23_train_time 91.64059915200005
Test Epoch23 layer0 Acc 0.882, AUC 0.9462825059890747, avg_entr 0.1143457293510437, f1 0.8820000290870667
ep23_l0_test_time 0.554862908999894
Test Epoch23 layer1 Acc 0.8686, AUC 0.9188166856765747, avg_entr 0.02431381866335869, f1 0.8686000108718872
ep23_l1_test_time 1.1720995890000268
Test Epoch23 layer2 Acc 0.8678, AUC 0.9288896322250366, avg_entr 0.018248271197080612, f1 0.8677999973297119
ep23_l2_test_time 1.7961837429998013
Test Epoch23 layer3 Acc 0.8684, AUC 0.936384916305542, avg_entr 0.016450801864266396, f1 0.868399977684021
ep23_l3_test_time 2.424225662000026
Test Epoch23 layer4 Acc 0.8676, AUC 0.939854621887207, avg_entr 0.016400562599301338, f1 0.8675999641418457
ep23_l4_test_time 3.052940425000088
gc 0
Train Epoch24 Acc 0.98205 (39282/40000), AUC 0.995927095413208
ep24_train_time 91.66602534399999
Test Epoch24 layer0 Acc 0.8832, AUC 0.9461379051208496, avg_entr 0.11358828097581863, f1 0.8831999897956848
ep24_l0_test_time 0.540441772000122
Test Epoch24 layer1 Acc 0.8686, AUC 0.9187085628509521, avg_entr 0.02421078272163868, f1 0.8686000108718872
ep24_l1_test_time 1.1772721349998392
Test Epoch24 layer2 Acc 0.868, AUC 0.9287928342819214, avg_entr 0.01813877932727337, f1 0.8679999709129333
ep24_l2_test_time 1.795361843000137
Test Epoch24 layer3 Acc 0.8676, AUC 0.9361039996147156, avg_entr 0.01634463481605053, f1 0.8675999641418457
ep24_l3_test_time 2.423882821999996
Test Epoch24 layer4 Acc 0.868, AUC 0.93966144323349, avg_entr 0.016280239447951317, f1 0.8679999709129333
ep24_l4_test_time 3.0500915170000553
gc 0
Train Epoch25 Acc 0.982075 (39283/40000), AUC 0.9959293603897095
ep25_train_time 91.6497392489996
Test Epoch25 layer0 Acc 0.8836, AUC 0.9460679292678833, avg_entr 0.11275207251310349, f1 0.8835999965667725
ep25_l0_test_time 0.5414804699998967
Test Epoch25 layer1 Acc 0.868, AUC 0.9187355637550354, avg_entr 0.02371177077293396, f1 0.8679999709129333
ep25_l1_test_time 1.1732488709999416
Test Epoch25 layer2 Acc 0.8684, AUC 0.928921103477478, avg_entr 0.017134295776486397, f1 0.868399977684021
ep25_l2_test_time 1.7945557689999987
Test Epoch25 layer3 Acc 0.8684, AUC 0.9362622499465942, avg_entr 0.015410064719617367, f1 0.868399977684021
ep25_l3_test_time 2.421802308000224
Test Epoch25 layer4 Acc 0.8678, AUC 0.9397749304771423, avg_entr 0.015351592563092709, f1 0.8677999973297119
ep25_l4_test_time 3.0497884289998183
gc 0
Train Epoch26 Acc 0.98225 (39290/40000), AUC 0.996027946472168
ep26_train_time 91.63048760299989
Test Epoch26 layer0 Acc 0.8822, AUC 0.9460059404373169, avg_entr 0.11318088322877884, f1 0.8822000026702881
ep26_l0_test_time 0.5516120309998769
Test Epoch26 layer1 Acc 0.868, AUC 0.918155312538147, avg_entr 0.023687055334448814, f1 0.8679999709129333
ep26_l1_test_time 1.1729832010000791
Test Epoch26 layer2 Acc 0.8678, AUC 0.928281307220459, avg_entr 0.017735164612531662, f1 0.8677999973297119
ep26_l2_test_time 1.805118296999808
Test Epoch26 layer3 Acc 0.8678, AUC 0.9355816841125488, avg_entr 0.015910878777503967, f1 0.8677999973297119
ep26_l3_test_time 2.428785869999956
Test Epoch26 layer4 Acc 0.8676, AUC 0.9395333528518677, avg_entr 0.015851391479372978, f1 0.8675999641418457
ep26_l4_test_time 3.0522337969996443
gc 0
Train Epoch27 Acc 0.982475 (39299/40000), AUC 0.9960765838623047
ep27_train_time 91.64832234000005
Test Epoch27 layer0 Acc 0.8818, AUC 0.945928156375885, avg_entr 0.11155012249946594, f1 0.8818000555038452
ep27_l0_test_time 0.5424957760001234
Test Epoch27 layer1 Acc 0.8678, AUC 0.9182097911834717, avg_entr 0.023639308288693428, f1 0.8677999973297119
ep27_l1_test_time 1.173970470000313
Test Epoch27 layer2 Acc 0.868, AUC 0.9283586740493774, avg_entr 0.01725916378200054, f1 0.8679999709129333
ep27_l2_test_time 1.7951791199998297
Test Epoch27 layer3 Acc 0.8676, AUC 0.9357391595840454, avg_entr 0.015473460778594017, f1 0.8675999641418457
ep27_l3_test_time 2.4203904930000135
Test Epoch27 layer4 Acc 0.8676, AUC 0.9395605325698853, avg_entr 0.015455282293260098, f1 0.8675999641418457
ep27_l4_test_time 3.054532137000024
gc 0
Train Epoch28 Acc 0.98235 (39294/40000), AUC 0.9958219528198242
ep28_train_time 91.6550713470001
Test Epoch28 layer0 Acc 0.8824, AUC 0.945884108543396, avg_entr 0.11191767454147339, f1 0.8823999762535095
ep28_l0_test_time 0.543089787000099
Test Epoch28 layer1 Acc 0.8678, AUC 0.9178997278213501, avg_entr 0.023512570187449455, f1 0.8677999973297119
ep28_l1_test_time 1.1744053219999842
Test Epoch28 layer2 Acc 0.868, AUC 0.9279205203056335, avg_entr 0.017583686858415604, f1 0.8679999709129333
ep28_l2_test_time 1.7961254369997732
Test Epoch28 layer3 Acc 0.8674, AUC 0.9351341724395752, avg_entr 0.015835152938961983, f1 0.8673999905586243
ep28_l3_test_time 2.421375610999803
Test Epoch28 layer4 Acc 0.8674, AUC 0.9393306970596313, avg_entr 0.015784021466970444, f1 0.8673999905586243
ep28_l4_test_time 3.0487800079999943
gc 0
Train Epoch29 Acc 0.98245 (39298/40000), AUC 0.996073305606842
ep29_train_time 91.80720612000005
Test Epoch29 layer0 Acc 0.882, AUC 0.9458589553833008, avg_entr 0.1111053079366684, f1 0.8820000290870667
ep29_l0_test_time 0.5402629390000584
Test Epoch29 layer1 Acc 0.8672, AUC 0.9178677201271057, avg_entr 0.023222962394356728, f1 0.8672000169754028
ep29_l1_test_time 1.17392365600017
Test Epoch29 layer2 Acc 0.8686, AUC 0.9280374050140381, avg_entr 0.016780834645032883, f1 0.8686000108718872
ep29_l2_test_time 1.7947430919998624
Test Epoch29 layer3 Acc 0.8674, AUC 0.9353272318840027, avg_entr 0.015111846849322319, f1 0.8673999905586243
ep29_l3_test_time 2.428311139000016
Test Epoch29 layer4 Acc 0.868, AUC 0.9393471479415894, avg_entr 0.01510856207460165, f1 0.8679999709129333
ep29_l4_test_time 3.0546253339998657
Best AUC tensor(0.8988) 3 0
train_loss (2, 5, 30)
valid_acc (5, 30)
valid_AUC (5, 30)
train_acc (30,)
total_train+valid_time 3022.5254028950003
Start Testing
Load ckpt at ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt
Test layer0 Acc 0.898, AUC 0.9562665224075317, avg_entr 0.19339245557785034, f1 0.8980000019073486
l0_test_time 0.5811439029998837
Test layer1 Acc 0.8912, AUC 0.9561864733695984, avg_entr 0.0776103064417839, f1 0.8912000060081482
l1_test_time 1.2127013119998082
Test layer2 Acc 0.8908, AUC 0.9552558064460754, avg_entr 0.04681750014424324, f1 0.8907999992370605
l2_test_time 1.8211417640000036
Test layer3 Acc 0.8902, AUC 0.9560953378677368, avg_entr 0.04530852660536766, f1 0.8902000188827515
l3_test_time 2.4319225620001816
Test layer4 Acc 0.8902, AUC 0.95594722032547, avg_entr 0.04173257201910019, f1 0.8902000188827515
l4_test_time 3.0544890930000292
