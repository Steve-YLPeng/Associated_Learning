total count words 222751
vocab size 30000
train size 40000, valid size 5000, test size 5000
found 27937 words in glove
model: TransformerModelML(
  (layers): ModuleList(
    (0): EMBLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Embedding(30000, 300)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=2, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=2, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.weight 9000000
layers.0.ae.g.0.weight 256
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 256
layers.0.ae.h.0.bias 2
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
Total Trainable Params: 13671778
init_time 22.220433496
Start Training
gc 0
Train Epoch0 Acc 0.546075 (21843/40000), AUC 0.5730282068252563
ep0_train_time 108.95739962899998
Test Epoch0 layer0 Acc 0.8238, AUC 0.8967260122299194, avg_entr 0.5312368869781494, f1 0.8238000273704529
ep0_l0_test_time 0.6371130389999848
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 0
Test Epoch0 layer1 Acc 0.8268, AUC 0.9159016609191895, avg_entr 0.3254241645336151, f1 0.8267999887466431
ep0_l1_test_time 1.4640312870000116
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 0
Test Epoch0 layer2 Acc 0.8302, AUC 0.9155217409133911, avg_entr 0.45447203516960144, f1 0.8302000164985657
ep0_l2_test_time 2.311156600000004
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 0
Test Epoch0 layer3 Acc 0.8338, AUC 0.9169034957885742, avg_entr 0.5408942103385925, f1 0.8338000774383545
ep0_l3_test_time 3.0935534820000044
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 0
Test Epoch0 layer4 Acc 0.8354, AUC 0.9122183322906494, avg_entr 0.6126475930213928, f1 0.8353999853134155
ep0_l4_test_time 3.959776148000003
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 0
gc 0
Train Epoch1 Acc 0.860125 (34405/40000), AUC 0.9327298402786255
ep1_train_time 108.67911473500001
Test Epoch1 layer0 Acc 0.8838, AUC 0.9476048946380615, avg_entr 0.2811039686203003, f1 0.8838000297546387
ep1_l0_test_time 0.6513913059999936
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 1
Test Epoch1 layer1 Acc 0.893, AUC 0.9556732773780823, avg_entr 0.1835077702999115, f1 0.8930000066757202
ep1_l1_test_time 1.4807132459999934
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 1
Test Epoch1 layer2 Acc 0.892, AUC 0.956437885761261, avg_entr 0.16646991670131683, f1 0.8920000195503235
ep1_l2_test_time 2.306988060000009
Test Epoch1 layer3 Acc 0.8884, AUC 0.956135630607605, avg_entr 0.14166919887065887, f1 0.8884000182151794
ep1_l3_test_time 3.0930275759999972
Test Epoch1 layer4 Acc 0.8822, AUC 0.9561976790428162, avg_entr 0.1369318664073944, f1 0.8822000026702881
ep1_l4_test_time 3.9572373809999704
gc 0
Train Epoch2 Acc 0.914375 (36575/40000), AUC 0.9682852029800415
ep2_train_time 108.805847427
Test Epoch2 layer0 Acc 0.893, AUC 0.9560060501098633, avg_entr 0.22639887034893036, f1 0.8930000066757202
ep2_l0_test_time 0.6218869119999795
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 2
Test Epoch2 layer1 Acc 0.8946, AUC 0.9593298435211182, avg_entr 0.13221141695976257, f1 0.894599974155426
ep2_l1_test_time 1.4838901129999726
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 2
Test Epoch2 layer2 Acc 0.8968, AUC 0.9579685926437378, avg_entr 0.07996770739555359, f1 0.8967999815940857
ep2_l2_test_time 2.308375319999982
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 2
Test Epoch2 layer3 Acc 0.8952, AUC 0.9598197937011719, avg_entr 0.05948259308934212, f1 0.8952000141143799
ep2_l3_test_time 3.1063724190000244
Test Epoch2 layer4 Acc 0.8948, AUC 0.9601082801818848, avg_entr 0.05606517195701599, f1 0.8948000073432922
ep2_l4_test_time 3.9504788499999677
gc 0
Train Epoch3 Acc 0.940675 (37627/40000), AUC 0.9813584089279175
ep3_train_time 114.279550083
Test Epoch3 layer0 Acc 0.887, AUC 0.9587131142616272, avg_entr 0.19306230545043945, f1 0.8870000243186951
ep3_l0_test_time 0.6944933590000346
Test Epoch3 layer1 Acc 0.8916, AUC 0.9557110667228699, avg_entr 0.08201802521944046, f1 0.8916000127792358
ep3_l1_test_time 1.4859333769999807
Test Epoch3 layer2 Acc 0.89, AUC 0.9553705453872681, avg_entr 0.04589800164103508, f1 0.8899999856948853
ep3_l2_test_time 2.1095915539999623
Test Epoch3 layer3 Acc 0.891, AUC 0.9565180540084839, avg_entr 0.03936324268579483, f1 0.890999972820282
ep3_l3_test_time 2.7683826530000033
Test Epoch3 layer4 Acc 0.8912, AUC 0.95710289478302, avg_entr 0.03855739161372185, f1 0.8912000060081482
ep3_l4_test_time 3.5187983120000013
gc 0
Train Epoch4 Acc 0.951175 (38047/40000), AUC 0.9853429198265076
ep4_train_time 116.82017670199997
Test Epoch4 layer0 Acc 0.8998, AUC 0.9594541788101196, avg_entr 0.17618267238140106, f1 0.8998000025749207
ep4_l0_test_time 0.6184838389999641
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 4
Test Epoch4 layer1 Acc 0.8848, AUC 0.9501495361328125, avg_entr 0.053454283624887466, f1 0.8848000168800354
ep4_l1_test_time 1.4508936189999986
Test Epoch4 layer2 Acc 0.8834, AUC 0.952910304069519, avg_entr 0.037246186286211014, f1 0.8834000825881958
ep4_l2_test_time 2.2947161499999993
Test Epoch4 layer3 Acc 0.8832, AUC 0.9537512063980103, avg_entr 0.03364415839314461, f1 0.8831999897956848
ep4_l3_test_time 2.7884464109999954
Test Epoch4 layer4 Acc 0.8832, AUC 0.9541636109352112, avg_entr 0.03310307487845421, f1 0.8831999897956848
ep4_l4_test_time 3.345563101000039
gc 0
Train Epoch5 Acc 0.95945 (38378/40000), AUC 0.9884352684020996
ep5_train_time 109.72603118199993
Test Epoch5 layer0 Acc 0.8974, AUC 0.9584288001060486, avg_entr 0.1618560254573822, f1 0.8974000215530396
ep5_l0_test_time 0.6324555830000236
Test Epoch5 layer1 Acc 0.8856, AUC 0.9470744132995605, avg_entr 0.04144437611103058, f1 0.8855999708175659
ep5_l1_test_time 1.4662934389999691
Test Epoch5 layer2 Acc 0.8864, AUC 0.9512025117874146, avg_entr 0.03068963624536991, f1 0.8863999843597412
ep5_l2_test_time 2.2964956209999627
Test Epoch5 layer3 Acc 0.8872, AUC 0.9518129229545593, avg_entr 0.02897470071911812, f1 0.8871999979019165
ep5_l3_test_time 3.0806948130000364
Test Epoch5 layer4 Acc 0.887, AUC 0.9519546031951904, avg_entr 0.028206298127770424, f1 0.8870000243186951
ep5_l4_test_time 3.4816353149999486
gc 0
Train Epoch6 Acc 0.963325 (38533/40000), AUC 0.989249587059021
ep6_train_time 108.48956147799993
Test Epoch6 layer0 Acc 0.8966, AUC 0.9574052095413208, avg_entr 0.15234018862247467, f1 0.8966000080108643
ep6_l0_test_time 0.6187929960000247
Test Epoch6 layer1 Acc 0.885, AUC 0.9430578947067261, avg_entr 0.038918811827898026, f1 0.8849999904632568
ep6_l1_test_time 1.4694754569999304
Test Epoch6 layer2 Acc 0.8858, AUC 0.9493493437767029, avg_entr 0.028596777468919754, f1 0.8858000040054321
ep6_l2_test_time 2.2746758110000656
Test Epoch6 layer3 Acc 0.8858, AUC 0.9507876634597778, avg_entr 0.02701052464544773, f1 0.8858000040054321
ep6_l3_test_time 3.0797016409999287
Test Epoch6 layer4 Acc 0.8858, AUC 0.9508266448974609, avg_entr 0.026261962950229645, f1 0.8858000040054321
ep6_l4_test_time 3.9457595410000295
gc 0
Train Epoch7 Acc 0.96775 (38710/40000), AUC 0.9918117523193359
ep7_train_time 111.20920130500008
Test Epoch7 layer0 Acc 0.8932, AUC 0.9558030366897583, avg_entr 0.14342191815376282, f1 0.8931999802589417
ep7_l0_test_time 0.7997032369999033
Test Epoch7 layer1 Acc 0.8798, AUC 0.9393801689147949, avg_entr 0.030866028741002083, f1 0.879800021648407
ep7_l1_test_time 1.4921208189999788
Test Epoch7 layer2 Acc 0.8806, AUC 0.9470781087875366, avg_entr 0.021824898198246956, f1 0.8805999755859375
ep7_l2_test_time 2.1163537799999403
Test Epoch7 layer3 Acc 0.881, AUC 0.9476736783981323, avg_entr 0.020349055528640747, f1 0.8809999823570251
ep7_l3_test_time 2.6799988939999366
Test Epoch7 layer4 Acc 0.8808, AUC 0.9476548433303833, avg_entr 0.019851477816700935, f1 0.8808000087738037
ep7_l4_test_time 3.450431475000073
gc 0
Train Epoch8 Acc 0.970725 (38829/40000), AUC 0.9924851655960083
ep8_train_time 118.39011429300001
Test Epoch8 layer0 Acc 0.8882, AUC 0.9538573026657104, avg_entr 0.13662652671337128, f1 0.888200044631958
ep8_l0_test_time 0.6084571460000916
Test Epoch8 layer1 Acc 0.8792, AUC 0.9350912570953369, avg_entr 0.030387066304683685, f1 0.8791999816894531
ep8_l1_test_time 1.2982534070001748
Test Epoch8 layer2 Acc 0.8784, AUC 0.9438924789428711, avg_entr 0.02166234515607357, f1 0.8784000277519226
ep8_l2_test_time 2.0265261190002093
Test Epoch8 layer3 Acc 0.8782, AUC 0.9457762241363525, avg_entr 0.020119955763220787, f1 0.8781999945640564
ep8_l3_test_time 2.4723085040000115
Test Epoch8 layer4 Acc 0.8782, AUC 0.9457643032073975, avg_entr 0.01960272714495659, f1 0.8781999945640564
ep8_l4_test_time 3.080819926999993
gc 0
Train Epoch9 Acc 0.974175 (38967/40000), AUC 0.993484616279602
ep9_train_time 112.65938250500017
Test Epoch9 layer0 Acc 0.8884, AUC 0.9530378580093384, avg_entr 0.13331571221351624, f1 0.8884000182151794
ep9_l0_test_time 0.6278448190000745
Test Epoch9 layer1 Acc 0.8768, AUC 0.9334970712661743, avg_entr 0.027438079938292503, f1 0.876800000667572
ep9_l1_test_time 1.466538276999927
Test Epoch9 layer2 Acc 0.878, AUC 0.941989541053772, avg_entr 0.019013628363609314, f1 0.878000020980835
ep9_l2_test_time 2.2646564280000803
Test Epoch9 layer3 Acc 0.8782, AUC 0.944306492805481, avg_entr 0.017462480813264847, f1 0.8781999945640564
ep9_l3_test_time 3.092874970999901
Test Epoch9 layer4 Acc 0.8782, AUC 0.944314181804657, avg_entr 0.01682918518781662, f1 0.8781999945640564
ep9_l4_test_time 3.918131406999919
gc 0
Train Epoch10 Acc 0.975675 (39027/40000), AUC 0.9942172765731812
ep10_train_time 108.59324756000001
Test Epoch10 layer0 Acc 0.8846, AUC 0.951743483543396, avg_entr 0.13453833758831024, f1 0.8845999836921692
ep10_l0_test_time 0.6208076160000928
Test Epoch10 layer1 Acc 0.8764, AUC 0.9295459389686584, avg_entr 0.026476779952645302, f1 0.8763999938964844
ep10_l1_test_time 1.4886278259998562
Test Epoch10 layer2 Acc 0.8754, AUC 0.9374856352806091, avg_entr 0.01835675910115242, f1 0.8754000067710876
ep10_l2_test_time 2.2808854570000676
Test Epoch10 layer3 Acc 0.875, AUC 0.9424186944961548, avg_entr 0.016749301925301552, f1 0.875
ep10_l3_test_time 3.079176807999829
Test Epoch10 layer4 Acc 0.8756, AUC 0.942631721496582, avg_entr 0.016125991940498352, f1 0.8756000399589539
ep10_l4_test_time 3.965322471000036
gc 0
Train Epoch11 Acc 0.976325 (39053/40000), AUC 0.9947706460952759
ep11_train_time 107.87144962000002
Test Epoch11 layer0 Acc 0.8808, AUC 0.9508481025695801, avg_entr 0.12721329927444458, f1 0.8808000087738037
ep11_l0_test_time 0.6207089419999647
Test Epoch11 layer1 Acc 0.874, AUC 0.9278512597084045, avg_entr 0.027721136808395386, f1 0.8740000128746033
ep11_l1_test_time 1.4645830990000377
Test Epoch11 layer2 Acc 0.874, AUC 0.936320424079895, avg_entr 0.01971118152141571, f1 0.8740000128746033
ep11_l2_test_time 2.2754351280000265
Test Epoch11 layer3 Acc 0.8738, AUC 0.9418126344680786, avg_entr 0.01837783493101597, f1 0.8737999796867371
ep11_l3_test_time 3.0471120489999066
Test Epoch11 layer4 Acc 0.874, AUC 0.9421095848083496, avg_entr 0.017963014543056488, f1 0.8740000128746033
ep11_l4_test_time 3.9543965739999294
gc 0
Train Epoch12 Acc 0.977475 (39099/40000), AUC 0.9949601888656616
ep12_train_time 114.85456887199985
Test Epoch12 layer0 Acc 0.8856, AUC 0.9493818283081055, avg_entr 0.12640655040740967, f1 0.8855999708175659
ep12_l0_test_time 0.8062973599999168
Test Epoch12 layer1 Acc 0.8742, AUC 0.9256906509399414, avg_entr 0.024977104738354683, f1 0.8741999864578247
ep12_l1_test_time 1.4016214119999404
Test Epoch12 layer2 Acc 0.8732, AUC 0.9324498176574707, avg_entr 0.017186662182211876, f1 0.873199999332428
ep12_l2_test_time 2.110712410999895
Test Epoch12 layer3 Acc 0.8728, AUC 0.9399411678314209, avg_entr 0.016161363571882248, f1 0.8727999925613403
ep12_l3_test_time 2.777701411999942
Test Epoch12 layer4 Acc 0.8728, AUC 0.9404647350311279, avg_entr 0.01569426991045475, f1 0.8727999925613403
ep12_l4_test_time 3.3292510890000813
gc 0
Train Epoch13 Acc 0.978775 (39151/40000), AUC 0.995303213596344
ep13_train_time 117.19642077899994
Test Epoch13 layer0 Acc 0.8848, AUC 0.9490073919296265, avg_entr 0.12471885979175568, f1 0.8848000168800354
ep13_l0_test_time 0.6165245550000691
Test Epoch13 layer1 Acc 0.8722, AUC 0.925603985786438, avg_entr 0.026450499892234802, f1 0.872200071811676
ep13_l1_test_time 1.4795428909999373
Test Epoch13 layer2 Acc 0.8714, AUC 0.9308695793151855, avg_entr 0.019200025126338005, f1 0.871399998664856
ep13_l2_test_time 2.0319176099999368
Test Epoch13 layer3 Acc 0.8712, AUC 0.939789354801178, avg_entr 0.018084818497300148, f1 0.8712000846862793
ep13_l3_test_time 2.7117236649999086
Test Epoch13 layer4 Acc 0.8718, AUC 0.9404201507568359, avg_entr 0.01756436936557293, f1 0.8718000650405884
ep13_l4_test_time 3.293974247000051
gc 0
Train Epoch14 Acc 0.979275 (39171/40000), AUC 0.9952914118766785
ep14_train_time 109.00810632899993
Test Epoch14 layer0 Acc 0.8836, AUC 0.9482492208480835, avg_entr 0.12277942150831223, f1 0.8835999965667725
ep14_l0_test_time 0.642140844000096
Test Epoch14 layer1 Acc 0.874, AUC 0.9240649938583374, avg_entr 0.02415132336318493, f1 0.8740000128746033
ep14_l1_test_time 1.475526053000067
Test Epoch14 layer2 Acc 0.8734, AUC 0.9297949075698853, avg_entr 0.016969485208392143, f1 0.8733999729156494
ep14_l2_test_time 2.3079844239998693
Test Epoch14 layer3 Acc 0.8732, AUC 0.9381662011146545, avg_entr 0.016060320660471916, f1 0.873199999332428
ep14_l3_test_time 3.055024911999908
Test Epoch14 layer4 Acc 0.8728, AUC 0.9395662546157837, avg_entr 0.015695292502641678, f1 0.8727999925613403
ep14_l4_test_time 3.9358863439999823
gc 0
Train Epoch15 Acc 0.979725 (39189/40000), AUC 0.9956110715866089
ep15_train_time 108.75554994899994
Test Epoch15 layer0 Acc 0.8828, AUC 0.9475767612457275, avg_entr 0.12246432155370712, f1 0.8827999830245972
ep15_l0_test_time 0.6164900330002183
Test Epoch15 layer1 Acc 0.8696, AUC 0.9224931001663208, avg_entr 0.024644998833537102, f1 0.8695999383926392
ep15_l1_test_time 1.4762297509998916
Test Epoch15 layer2 Acc 0.8686, AUC 0.927780032157898, avg_entr 0.017727529630064964, f1 0.8686000108718872
ep15_l2_test_time 2.3040148310001314
Test Epoch15 layer3 Acc 0.8684, AUC 0.9369608163833618, avg_entr 0.016886882483959198, f1 0.868399977684021
ep15_l3_test_time 3.099435480000011
Test Epoch15 layer4 Acc 0.8682, AUC 0.9387006759643555, avg_entr 0.01653161644935608, f1 0.8682000041007996
ep15_l4_test_time 3.9352291289999357
gc 0
Train Epoch16 Acc 0.980525 (39221/40000), AUC 0.9957031011581421
ep16_train_time 110.69488826200018
Test Epoch16 layer0 Acc 0.8816, AUC 0.9469740390777588, avg_entr 0.12074561417102814, f1 0.881600022315979
ep16_l0_test_time 0.8641586179996921
Test Epoch16 layer1 Acc 0.87, AUC 0.9206438660621643, avg_entr 0.02273484319448471, f1 0.8700000047683716
ep16_l1_test_time 1.4650757529998373
Test Epoch16 layer2 Acc 0.8696, AUC 0.9258910417556763, avg_entr 0.016348564997315407, f1 0.8695999383926392
ep16_l2_test_time 2.111715305999951
Test Epoch16 layer3 Acc 0.87, AUC 0.9357631206512451, avg_entr 0.015530302189290524, f1 0.8700000047683716
ep16_l3_test_time 2.7511448970003585
Test Epoch16 layer4 Acc 0.8704, AUC 0.9378980398178101, avg_entr 0.015200858935713768, f1 0.8704000115394592
ep16_l4_test_time 3.9522374449998097
gc 0
Train Epoch17 Acc 0.981475 (39259/40000), AUC 0.9957115054130554
ep17_train_time 117.637733476
Test Epoch17 layer0 Acc 0.8826, AUC 0.9466012716293335, avg_entr 0.11958061158657074, f1 0.8826000094413757
ep17_l0_test_time 0.58623180900031
Test Epoch17 layer1 Acc 0.8732, AUC 0.9200679063796997, avg_entr 0.021644458174705505, f1 0.873199999332428
ep17_l1_test_time 1.2961022279996541
Test Epoch17 layer2 Acc 0.8728, AUC 0.927026629447937, avg_entr 0.015396969392895699, f1 0.8727999925613403
ep17_l2_test_time 1.8098080810000283
Test Epoch17 layer3 Acc 0.8728, AUC 0.9363231658935547, avg_entr 0.014538480900228024, f1 0.8727999925613403
ep17_l3_test_time 2.4353631750000204
Test Epoch17 layer4 Acc 0.8726, AUC 0.9379345774650574, avg_entr 0.014111132360994816, f1 0.8726000189781189
ep17_l4_test_time 3.072048237999752
gc 0
Train Epoch18 Acc 0.9813 (39252/40000), AUC 0.9959843158721924
ep18_train_time 113.37688593500025
Test Epoch18 layer0 Acc 0.8814, AUC 0.9463178515434265, avg_entr 0.11873466521501541, f1 0.8813999891281128
ep18_l0_test_time 0.6224202920002426
Test Epoch18 layer1 Acc 0.8718, AUC 0.9191693663597107, avg_entr 0.021238092333078384, f1 0.8718000650405884
ep18_l1_test_time 1.46893292499999
Test Epoch18 layer2 Acc 0.8724, AUC 0.9243373870849609, avg_entr 0.014749713242053986, f1 0.8723999857902527
ep18_l2_test_time 2.3112121139997726
Test Epoch18 layer3 Acc 0.8726, AUC 0.9348241090774536, avg_entr 0.013979635201394558, f1 0.8726000189781189
ep18_l3_test_time 3.0807649599996694
Test Epoch18 layer4 Acc 0.8726, AUC 0.9369429349899292, avg_entr 0.013485177420079708, f1 0.8726000189781189
ep18_l4_test_time 3.960143741999673
gc 0
Train Epoch19 Acc 0.9814 (39256/40000), AUC 0.9959996938705444
ep19_train_time 108.76421935400003
Test Epoch19 layer0 Acc 0.8792, AUC 0.946076512336731, avg_entr 0.1193452998995781, f1 0.8791999816894531
ep19_l0_test_time 0.6199325509996925
Test Epoch19 layer1 Acc 0.8714, AUC 0.9194643497467041, avg_entr 0.02108805999159813, f1 0.871399998664856
ep19_l1_test_time 1.4115780649999579
Test Epoch19 layer2 Acc 0.8724, AUC 0.9241082668304443, avg_entr 0.01451628003269434, f1 0.8723999857902527
ep19_l2_test_time 1.849297745000058
Test Epoch19 layer3 Acc 0.8722, AUC 0.9343307614326477, avg_entr 0.013658024370670319, f1 0.872200071811676
ep19_l3_test_time 2.7507380289998764
Test Epoch19 layer4 Acc 0.8718, AUC 0.9368158578872681, avg_entr 0.01310740690678358, f1 0.8718000650405884
ep19_l4_test_time 3.941482927999914
gc 0
Train Epoch20 Acc 0.98205 (39282/40000), AUC 0.9961463212966919
ep20_train_time 108.6143603139999
Test Epoch20 layer0 Acc 0.8794, AUC 0.945556640625, avg_entr 0.11593741178512573, f1 0.8794000148773193
ep20_l0_test_time 0.6331548779999139
Test Epoch20 layer1 Acc 0.8706, AUC 0.9182050228118896, avg_entr 0.020658200606703758, f1 0.8705999851226807
ep20_l1_test_time 1.4634239360002539
Test Epoch20 layer2 Acc 0.871, AUC 0.9216307401657104, avg_entr 0.013397421687841415, f1 0.8709999918937683
ep20_l2_test_time 2.31233373699979
Test Epoch20 layer3 Acc 0.8712, AUC 0.9330980181694031, avg_entr 0.012494122609496117, f1 0.8712000846862793
ep20_l3_test_time 3.0916477519999717
Test Epoch20 layer4 Acc 0.8712, AUC 0.9360450506210327, avg_entr 0.011961221694946289, f1 0.8712000846862793
ep20_l4_test_time 3.9602601630003846
gc 0
Train Epoch21 Acc 0.982625 (39305/40000), AUC 0.9964327216148376
ep21_train_time 114.32245306300001
Test Epoch21 layer0 Acc 0.8798, AUC 0.9455224871635437, avg_entr 0.11685220897197723, f1 0.879800021648407
ep21_l0_test_time 0.8289964450000298
Test Epoch21 layer1 Acc 0.87, AUC 0.9181959629058838, avg_entr 0.020976662635803223, f1 0.8700000047683716
ep21_l1_test_time 1.4550290679999307
Test Epoch21 layer2 Acc 0.8706, AUC 0.920488715171814, avg_entr 0.013645380735397339, f1 0.8705999851226807
ep21_l2_test_time 2.1339736689997153
Test Epoch21 layer3 Acc 0.87, AUC 0.9323985576629639, avg_entr 0.012627139687538147, f1 0.8700000047683716
ep21_l3_test_time 2.7855539520001003
Test Epoch21 layer4 Acc 0.8702, AUC 0.9359389543533325, avg_entr 0.012202185578644276, f1 0.870199978351593
ep21_l4_test_time 3.2997772150001765
gc 0
Train Epoch22 Acc 0.982225 (39289/40000), AUC 0.9960488677024841
ep22_train_time 116.7849645880001
Test Epoch22 layer0 Acc 0.8812, AUC 0.9453486800193787, avg_entr 0.11525376886129379, f1 0.8812000155448914
ep22_l0_test_time 0.6151423410001371
Test Epoch22 layer1 Acc 0.8702, AUC 0.9182511568069458, avg_entr 0.02150932140648365, f1 0.870199978351593
ep22_l1_test_time 1.477628642000127
Test Epoch22 layer2 Acc 0.8706, AUC 0.9200077056884766, avg_entr 0.014403438195586205, f1 0.8705999851226807
ep22_l2_test_time 2.2139171550002175
Test Epoch22 layer3 Acc 0.8708, AUC 0.9319967031478882, avg_entr 0.013403715565800667, f1 0.8708000183105469
ep22_l3_test_time 2.7276246790002006
Test Epoch22 layer4 Acc 0.8708, AUC 0.9356237649917603, avg_entr 0.012968432158231735, f1 0.8708000183105469
ep22_l4_test_time 3.3473732010002095
gc 0
Train Epoch23 Acc 0.982775 (39311/40000), AUC 0.9963546991348267
ep23_train_time 109.81462964999992
Test Epoch23 layer0 Acc 0.8776, AUC 0.9452489614486694, avg_entr 0.11619919538497925, f1 0.8776000142097473
ep23_l0_test_time 0.6274662709997756
Test Epoch23 layer1 Acc 0.8696, AUC 0.9176110029220581, avg_entr 0.021071437746286392, f1 0.8695999383926392
ep23_l1_test_time 1.466169491999608
Test Epoch23 layer2 Acc 0.8688, AUC 0.9184633493423462, avg_entr 0.014543545432388783, f1 0.8687999844551086
ep23_l2_test_time 2.322708496999894
Test Epoch23 layer3 Acc 0.8692, AUC 0.9310973882675171, avg_entr 0.01373257301747799, f1 0.8691999912261963
ep23_l3_test_time 3.067628361000061
Test Epoch23 layer4 Acc 0.8688, AUC 0.9352789521217346, avg_entr 0.013400967232882977, f1 0.8687999844551086
ep23_l4_test_time 3.9336322579997614
gc 0
Train Epoch24 Acc 0.98305 (39322/40000), AUC 0.9963943958282471
ep24_train_time 108.99390383599984
Test Epoch24 layer0 Acc 0.8796, AUC 0.9450762271881104, avg_entr 0.11454052478075027, f1 0.8795999884605408
ep24_l0_test_time 0.6306629059999977
Test Epoch24 layer1 Acc 0.87, AUC 0.9174466729164124, avg_entr 0.020353784784674644, f1 0.8700000047683716
ep24_l1_test_time 1.4637579660002302
Test Epoch24 layer2 Acc 0.8704, AUC 0.9185686707496643, avg_entr 0.013012773357331753, f1 0.8704000115394592
ep24_l2_test_time 2.2619975460002024
Test Epoch24 layer3 Acc 0.87, AUC 0.9307688474655151, avg_entr 0.012067691422998905, f1 0.8700000047683716
ep24_l3_test_time 3.055006597999636
Test Epoch24 layer4 Acc 0.8698, AUC 0.9348814487457275, avg_entr 0.011613321490585804, f1 0.8697999715805054
ep24_l4_test_time 3.9214640400000462
gc 0
Train Epoch25 Acc 0.982775 (39311/40000), AUC 0.9963181018829346
ep25_train_time 109.88519364500007
Test Epoch25 layer0 Acc 0.8794, AUC 0.9450140595436096, avg_entr 0.11429062485694885, f1 0.8794000148773193
ep25_l0_test_time 0.837952544000018
Test Epoch25 layer1 Acc 0.8712, AUC 0.9174227714538574, avg_entr 0.02056998200714588, f1 0.8712000846862793
ep25_l1_test_time 1.4841101520000848
Test Epoch25 layer2 Acc 0.8712, AUC 0.9187324047088623, avg_entr 0.01381517294794321, f1 0.8712000846862793
ep25_l2_test_time 2.0974020949997794
Test Epoch25 layer3 Acc 0.8714, AUC 0.9310916662216187, avg_entr 0.012846516445279121, f1 0.871399998664856
ep25_l3_test_time 2.7582513919996927
Test Epoch25 layer4 Acc 0.8714, AUC 0.9348987936973572, avg_entr 0.012373637408018112, f1 0.871399998664856
ep25_l4_test_time 3.1514781920000132
gc 0
Train Epoch26 Acc 0.9831 (39324/40000), AUC 0.9965125918388367
ep26_train_time 91.72887924799988
Test Epoch26 layer0 Acc 0.879, AUC 0.9449442625045776, avg_entr 0.11383061856031418, f1 0.8790000081062317
ep26_l0_test_time 0.5496263029999682
Test Epoch26 layer1 Acc 0.8676, AUC 0.9168229103088379, avg_entr 0.020738478749990463, f1 0.8675999641418457
ep26_l1_test_time 1.179714118999982
Test Epoch26 layer2 Acc 0.868, AUC 0.9174783229827881, avg_entr 0.014493980444967747, f1 0.8679999709129333
ep26_l2_test_time 1.7996448859998964
Test Epoch26 layer3 Acc 0.868, AUC 0.9298248291015625, avg_entr 0.013704204931855202, f1 0.8679999709129333
ep26_l3_test_time 2.4266185390001738
Test Epoch26 layer4 Acc 0.8676, AUC 0.9346966743469238, avg_entr 0.013337268494069576, f1 0.8675999641418457
ep26_l4_test_time 3.063470905000031
gc 0
Train Epoch27 Acc 0.9832 (39328/40000), AUC 0.996457576751709
ep27_train_time 91.7272465269998
Test Epoch27 layer0 Acc 0.879, AUC 0.9448866844177246, avg_entr 0.11348287016153336, f1 0.8790000081062317
ep27_l0_test_time 0.5474868480000623
Test Epoch27 layer1 Acc 0.869, AUC 0.9166867733001709, avg_entr 0.02028493396937847, f1 0.8690000176429749
ep27_l1_test_time 1.1772631669996372
Test Epoch27 layer2 Acc 0.8694, AUC 0.9169392585754395, avg_entr 0.013713669031858444, f1 0.8694000244140625
ep27_l2_test_time 1.802915720999863
Test Epoch27 layer3 Acc 0.8694, AUC 0.9295251369476318, avg_entr 0.012800965458154678, f1 0.8694000244140625
ep27_l3_test_time 2.4287840680003683
Test Epoch27 layer4 Acc 0.8692, AUC 0.9344873428344727, avg_entr 0.012448334135115147, f1 0.8691999912261963
ep27_l4_test_time 3.063734220999777
gc 0
Train Epoch28 Acc 0.9829 (39316/40000), AUC 0.9965226054191589
ep28_train_time 91.92379702500011
Test Epoch28 layer0 Acc 0.8774, AUC 0.9448392987251282, avg_entr 0.11325190216302872, f1 0.8773999810218811
ep28_l0_test_time 0.5576153240003805
Test Epoch28 layer1 Acc 0.8698, AUC 0.916598916053772, avg_entr 0.020198382437229156, f1 0.8697999715805054
ep28_l1_test_time 1.1803905380002107
Test Epoch28 layer2 Acc 0.8696, AUC 0.9167540669441223, avg_entr 0.013430183753371239, f1 0.8695999383926392
ep28_l2_test_time 1.8136674890001814
Test Epoch28 layer3 Acc 0.8696, AUC 0.9297829866409302, avg_entr 0.012432725168764591, f1 0.8695999383926392
ep28_l3_test_time 2.4248694989996693
Test Epoch28 layer4 Acc 0.8696, AUC 0.9344090223312378, avg_entr 0.012043406255543232, f1 0.8695999383926392
ep28_l4_test_time 3.0601458010000897
gc 0
Train Epoch29 Acc 0.98325 (39330/40000), AUC 0.9966351985931396
ep29_train_time 91.8213747499999
Test Epoch29 layer0 Acc 0.8788, AUC 0.944799542427063, avg_entr 0.1126873567700386, f1 0.8787999749183655
ep29_l0_test_time 0.5435177340000337
Test Epoch29 layer1 Acc 0.869, AUC 0.9165980815887451, avg_entr 0.020116271451115608, f1 0.8690000176429749
ep29_l1_test_time 1.1765626759997758
Test Epoch29 layer2 Acc 0.87, AUC 0.916266679763794, avg_entr 0.01350142527371645, f1 0.8700000047683716
ep29_l2_test_time 1.8023820099997465
Test Epoch29 layer3 Acc 0.8698, AUC 0.9291421175003052, avg_entr 0.012481589801609516, f1 0.8697999715805054
ep29_l3_test_time 2.4267589239998415
Test Epoch29 layer4 Acc 0.8696, AUC 0.9342483878135681, avg_entr 0.012116526253521442, f1 0.8695999383926392
ep29_l4_test_time 3.064502872999583
Best AUC tensor(0.8998) 4 0
train_loss (2, 5, 30)
valid_acc (5, 30)
valid_AUC (5, 30)
train_acc (30,)
total_train+valid_time 3592.013414353
Start Testing
Load ckpt at ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt
Test layer0 Acc 0.8964, AUC 0.9549248218536377, avg_entr 0.1767827719449997, f1 0.896399974822998
l0_test_time 0.5402717950000806
Test layer1 Acc 0.8764, AUC 0.9450736045837402, avg_entr 0.05623066425323486, f1 0.8763999938964844
l1_test_time 1.1751555770001687
Test layer2 Acc 0.878, AUC 0.9492079019546509, avg_entr 0.03912249952554703, f1 0.878000020980835
l2_test_time 1.812123843000336
Test layer3 Acc 0.8776, AUC 0.9498087167739868, avg_entr 0.03547519072890282, f1 0.8776000142097473
l3_test_time 2.4304791449999357
Test layer4 Acc 0.8778, AUC 0.9505972862243652, avg_entr 0.03492916747927666, f1 0.8778000473976135
l4_test_time 3.062612213000193
