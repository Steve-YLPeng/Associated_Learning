total count words 222751
vocab size 30000
train size 40000, valid size 5000, test size 5000
found 27937 words in glove
model: TransformerModelML(
  (layers): ModuleList(
    (0): EMBLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Embedding(30000, 300)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=2, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=2, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.weight 9000000
layers.0.ae.g.0.weight 256
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 256
layers.0.ae.h.0.bias 2
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
Total Trainable Params: 13671778
init_time 22.462839603424072
Start Training
gc 0
Train Epoch0 Acc 0.51375 (20550/40000), AUC 0.519271731376648
ep0_train_time 184.82302594184875
Test Epoch0 layer0 Acc 0.7952, AUC 0.8894643783569336, avg_entr 0.5631939768791199, f1 0.7952000498771667
ep0_l0_test_time 0.7641162872314453
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 0
Test Epoch0 layer1 Acc 0.8316, AUC 0.9114997386932373, avg_entr 0.3551657795906067, f1 0.83160001039505
ep0_l1_test_time 2.0524797439575195
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 0
Test Epoch0 layer2 Acc 0.8236, AUC 0.9106965065002441, avg_entr 0.48644500970840454, f1 0.8235999941825867
ep0_l2_test_time 3.2762129306793213
Test Epoch0 layer3 Acc 0.725, AUC 0.9095731973648071, avg_entr 0.6346161961555481, f1 0.7250000238418579
ep0_l3_test_time 4.526016473770142
Test Epoch0 layer4 Acc 0.5224, AUC 0.9000228643417358, avg_entr 0.6557907462120056, f1 0.5224000215530396
ep0_l4_test_time 5.846903085708618
gc 0
Train Epoch1 Acc 0.854475 (34179/40000), AUC 0.9233526587486267
ep1_train_time 183.74920654296875
Test Epoch1 layer0 Acc 0.8782, AUC 0.9448487162590027, avg_entr 0.29061734676361084, f1 0.8781999945640564
ep1_l0_test_time 0.7735412120819092
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 1
Test Epoch1 layer1 Acc 0.8902, AUC 0.9547497630119324, avg_entr 0.1935681253671646, f1 0.8902000188827515
ep1_l1_test_time 2.0628511905670166
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 1
Test Epoch1 layer2 Acc 0.886, AUC 0.9555665254592896, avg_entr 0.1493193358182907, f1 0.8859999775886536
ep1_l2_test_time 3.2711145877838135
Test Epoch1 layer3 Acc 0.8884, AUC 0.9551283717155457, avg_entr 0.13556011021137238, f1 0.8884000182151794
ep1_l3_test_time 4.581262588500977
Test Epoch1 layer4 Acc 0.889, AUC 0.9551064968109131, avg_entr 0.12265349179506302, f1 0.8889999985694885
ep1_l4_test_time 5.80449366569519
gc 0
Train Epoch2 Acc 0.91395 (36558/40000), AUC 0.9685142040252686
ep2_train_time 183.8010561466217
Test Epoch2 layer0 Acc 0.8898, AUC 0.9549952745437622, avg_entr 0.2312159240245819, f1 0.8898000121116638
ep2_l0_test_time 0.7841479778289795
Test Epoch2 layer1 Acc 0.886, AUC 0.9581955075263977, avg_entr 0.15113183856010437, f1 0.8859999775886536
ep2_l1_test_time 2.0694215297698975
Test Epoch2 layer2 Acc 0.8856, AUC 0.955952525138855, avg_entr 0.08387202024459839, f1 0.8855999708175659
ep2_l2_test_time 3.2709176540374756
Test Epoch2 layer3 Acc 0.8844, AUC 0.9590517282485962, avg_entr 0.0714620053768158, f1 0.8844000101089478
ep2_l3_test_time 4.566142320632935
Test Epoch2 layer4 Acc 0.8842, AUC 0.9591317772865295, avg_entr 0.06574239581823349, f1 0.8842000365257263
ep2_l4_test_time 5.906851291656494
gc 0
Train Epoch3 Acc 0.9411 (37644/40000), AUC 0.9806635975837708
ep3_train_time 183.775132894516
Test Epoch3 layer0 Acc 0.8952, AUC 0.9575004577636719, avg_entr 0.19975654780864716, f1 0.8952000141143799
ep3_l0_test_time 0.7958400249481201
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 3
Test Epoch3 layer1 Acc 0.891, AUC 0.9552774429321289, avg_entr 0.09264318645000458, f1 0.890999972820282
ep3_l1_test_time 2.052701950073242
Test Epoch3 layer2 Acc 0.8888, AUC 0.9554414749145508, avg_entr 0.047330424189567566, f1 0.8888000249862671
ep3_l2_test_time 3.283463954925537
Test Epoch3 layer3 Acc 0.8888, AUC 0.9560160040855408, avg_entr 0.044554661959409714, f1 0.8888000249862671
ep3_l3_test_time 4.592242002487183
Test Epoch3 layer4 Acc 0.889, AUC 0.9562061429023743, avg_entr 0.04181535169482231, f1 0.8889999985694885
ep3_l4_test_time 5.889965534210205
gc 0
Train Epoch4 Acc 0.95095 (38038/40000), AUC 0.9856525659561157
ep4_train_time 183.85962510108948
Test Epoch4 layer0 Acc 0.8992, AUC 0.9587090015411377, avg_entr 0.1781173199415207, f1 0.8992000222206116
ep4_l0_test_time 0.7706513404846191
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 4
Test Epoch4 layer1 Acc 0.8798, AUC 0.9516525864601135, avg_entr 0.05283366143703461, f1 0.879800021648407
ep4_l1_test_time 2.061704397201538
Test Epoch4 layer2 Acc 0.8806, AUC 0.9531118273735046, avg_entr 0.037814922630786896, f1 0.8805999755859375
ep4_l2_test_time 3.2645294666290283
Test Epoch4 layer3 Acc 0.8808, AUC 0.9529923796653748, avg_entr 0.037505604326725006, f1 0.8808000087738037
ep4_l3_test_time 4.582442760467529
Test Epoch4 layer4 Acc 0.8802, AUC 0.9532079100608826, avg_entr 0.03526001423597336, f1 0.8802000284194946
ep4_l4_test_time 5.829292058944702
gc 0
Train Epoch5 Acc 0.956575 (38263/40000), AUC 0.9867492914199829
ep5_train_time 183.96334838867188
Test Epoch5 layer0 Acc 0.8968, AUC 0.9578953385353088, avg_entr 0.1623384654521942, f1 0.8967999815940857
ep5_l0_test_time 0.7686350345611572
Test Epoch5 layer1 Acc 0.8836, AUC 0.9470614194869995, avg_entr 0.04087287560105324, f1 0.8835999965667725
ep5_l1_test_time 2.053619384765625
Test Epoch5 layer2 Acc 0.8846, AUC 0.951583743095398, avg_entr 0.03012344427406788, f1 0.8845999836921692
ep5_l2_test_time 3.2233738899230957
Test Epoch5 layer3 Acc 0.8842, AUC 0.9512031078338623, avg_entr 0.030147390440106392, f1 0.8842000365257263
ep5_l3_test_time 4.605208873748779
Test Epoch5 layer4 Acc 0.8846, AUC 0.9514813423156738, avg_entr 0.02813488245010376, f1 0.8845999836921692
ep5_l4_test_time 5.881525754928589
gc 0
Train Epoch6 Acc 0.962025 (38481/40000), AUC 0.9888434410095215
ep6_train_time 155.68699598312378
Test Epoch6 layer0 Acc 0.8924, AUC 0.9567623734474182, avg_entr 0.15593643486499786, f1 0.8923999667167664
ep6_l0_test_time 0.7143862247467041
Test Epoch6 layer1 Acc 0.8756, AUC 0.9396415948867798, avg_entr 0.03738526627421379, f1 0.8756000399589539
ep6_l1_test_time 1.8868837356567383
Test Epoch6 layer2 Acc 0.8766, AUC 0.9496723413467407, avg_entr 0.028170572593808174, f1 0.8766000270843506
ep6_l2_test_time 3.1953184604644775
Test Epoch6 layer3 Acc 0.8762, AUC 0.9492387771606445, avg_entr 0.027596771717071533, f1 0.8762000203132629
ep6_l3_test_time 4.2911536693573
Test Epoch6 layer4 Acc 0.8762, AUC 0.949474036693573, avg_entr 0.025791464373469353, f1 0.8762000203132629
ep6_l4_test_time 5.497923374176025
gc 0
Train Epoch7 Acc 0.96715 (38686/40000), AUC 0.9909358024597168
ep7_train_time 171.3470983505249
Test Epoch7 layer0 Acc 0.895, AUC 0.9554172158241272, avg_entr 0.14390884339809418, f1 0.8949999809265137
ep7_l0_test_time 0.7172338962554932
Test Epoch7 layer1 Acc 0.879, AUC 0.9397757649421692, avg_entr 0.03255213797092438, f1 0.8790000081062317
ep7_l1_test_time 1.8798229694366455
Test Epoch7 layer2 Acc 0.8792, AUC 0.9480855464935303, avg_entr 0.02666790410876274, f1 0.8791999816894531
ep7_l2_test_time 3.2361769676208496
Test Epoch7 layer3 Acc 0.8786, AUC 0.9475173950195312, avg_entr 0.026347534731030464, f1 0.878600001335144
ep7_l3_test_time 4.2817370891571045
Test Epoch7 layer4 Acc 0.8782, AUC 0.9479647278785706, avg_entr 0.024737052619457245, f1 0.8781999945640564
ep7_l4_test_time 5.4818878173828125
gc 0
Train Epoch8 Acc 0.96935 (38774/40000), AUC 0.9924954175949097
ep8_train_time 158.93714594841003
Test Epoch8 layer0 Acc 0.8918, AUC 0.9537644982337952, avg_entr 0.1385214775800705, f1 0.8917999863624573
ep8_l0_test_time 0.7080292701721191
Test Epoch8 layer1 Acc 0.8732, AUC 0.9357103109359741, avg_entr 0.028630506247282028, f1 0.873199999332428
ep8_l1_test_time 1.918607473373413
Test Epoch8 layer2 Acc 0.873, AUC 0.945480227470398, avg_entr 0.023981265723705292, f1 0.8730000257492065
ep8_l2_test_time 3.0702664852142334
Test Epoch8 layer3 Acc 0.873, AUC 0.9449007511138916, avg_entr 0.02419533021748066, f1 0.8730000257492065
ep8_l3_test_time 4.223254919052124
Test Epoch8 layer4 Acc 0.8726, AUC 0.9453655481338501, avg_entr 0.02307363972067833, f1 0.8726000189781189
ep8_l4_test_time 5.501111745834351
gc 0
Train Epoch9 Acc 0.973075 (38923/40000), AUC 0.9933617115020752
ep9_train_time 165.88028240203857
Test Epoch9 layer0 Acc 0.8896, AUC 0.9526826739311218, avg_entr 0.13526380062103271, f1 0.8895999193191528
ep9_l0_test_time 0.5623624324798584
Test Epoch9 layer1 Acc 0.8758, AUC 0.9325066208839417, avg_entr 0.027373459190130234, f1 0.8758000135421753
ep9_l1_test_time 1.2130868434906006
Test Epoch9 layer2 Acc 0.875, AUC 0.9437879920005798, avg_entr 0.021585894748568535, f1 0.875
ep9_l2_test_time 1.8399415016174316
Test Epoch9 layer3 Acc 0.8744, AUC 0.9434195756912231, avg_entr 0.021172840148210526, f1 0.8744000792503357
ep9_l3_test_time 2.458360195159912
Test Epoch9 layer4 Acc 0.8744, AUC 0.9441906213760376, avg_entr 0.02001306414604187, f1 0.8744000792503357
ep9_l4_test_time 4.276102542877197
gc 0
Train Epoch10 Acc 0.974325 (38973/40000), AUC 0.9934168457984924
ep10_train_time 170.85425996780396
Test Epoch10 layer0 Acc 0.8894, AUC 0.9517754912376404, avg_entr 0.1320989727973938, f1 0.8894000053405762
ep10_l0_test_time 0.7152678966522217
Test Epoch10 layer1 Acc 0.8768, AUC 0.9303208589553833, avg_entr 0.025818338617682457, f1 0.876800000667572
ep10_l1_test_time 1.920644760131836
Test Epoch10 layer2 Acc 0.8764, AUC 0.9419353008270264, avg_entr 0.019359607249498367, f1 0.8763999938964844
ep10_l2_test_time 3.0783023834228516
Test Epoch10 layer3 Acc 0.876, AUC 0.9425210952758789, avg_entr 0.018872998654842377, f1 0.8760000467300415
ep10_l3_test_time 4.2755446434021
Test Epoch10 layer4 Acc 0.8762, AUC 0.9439105987548828, avg_entr 0.017574837431311607, f1 0.8762000203132629
ep10_l4_test_time 5.415003776550293
gc 0
Train Epoch11 Acc 0.9753 (39012/40000), AUC 0.994019627571106
ep11_train_time 158.9804995059967
Test Epoch11 layer0 Acc 0.8892, AUC 0.9506887197494507, avg_entr 0.13130876421928406, f1 0.88919997215271
ep11_l0_test_time 0.7112469673156738
Test Epoch11 layer1 Acc 0.8716, AUC 0.9280228018760681, avg_entr 0.024476423859596252, f1 0.8715999722480774
ep11_l1_test_time 1.913938045501709
Test Epoch11 layer2 Acc 0.872, AUC 0.9396204948425293, avg_entr 0.018562953919172287, f1 0.871999979019165
ep11_l2_test_time 3.1481893062591553
Test Epoch11 layer3 Acc 0.8716, AUC 0.9403132200241089, avg_entr 0.01819094642996788, f1 0.8715999722480774
ep11_l3_test_time 4.276427745819092
Test Epoch11 layer4 Acc 0.8718, AUC 0.9420324563980103, avg_entr 0.017176553606987, f1 0.8718000650405884
ep11_l4_test_time 5.483954668045044
gc 0
Train Epoch12 Acc 0.976725 (39069/40000), AUC 0.9946113228797913
ep12_train_time 170.8106391429901
Test Epoch12 layer0 Acc 0.8862, AUC 0.9496402740478516, avg_entr 0.12805494666099548, f1 0.8862000107765198
ep12_l0_test_time 0.7140886783599854
Test Epoch12 layer1 Acc 0.8704, AUC 0.9260910153388977, avg_entr 0.02319309487938881, f1 0.8704000115394592
ep12_l1_test_time 1.9054546356201172
Test Epoch12 layer2 Acc 0.8694, AUC 0.936008632183075, avg_entr 0.016668301075696945, f1 0.8694000244140625
ep12_l2_test_time 3.0672719478607178
Test Epoch12 layer3 Acc 0.8696, AUC 0.9370149374008179, avg_entr 0.016174500808119774, f1 0.8695999383926392
ep12_l3_test_time 4.269942283630371
Test Epoch12 layer4 Acc 0.8702, AUC 0.9398400783538818, avg_entr 0.015286844223737717, f1 0.870199978351593
ep12_l4_test_time 5.462157487869263
gc 0
Train Epoch13 Acc 0.97765 (39106/40000), AUC 0.9946269989013672
ep13_train_time 158.94838571548462
Test Epoch13 layer0 Acc 0.8868, AUC 0.9489843845367432, avg_entr 0.12658263742923737, f1 0.8867999911308289
ep13_l0_test_time 0.716132402420044
Test Epoch13 layer1 Acc 0.8688, AUC 0.9250165224075317, avg_entr 0.02152257412672043, f1 0.8687999844551086
ep13_l1_test_time 1.9268689155578613
Test Epoch13 layer2 Acc 0.8682, AUC 0.9338452816009521, avg_entr 0.01646992191672325, f1 0.8682000041007996
ep13_l2_test_time 3.126782178878784
Test Epoch13 layer3 Acc 0.8676, AUC 0.9358178377151489, avg_entr 0.015971940010786057, f1 0.8675999641418457
ep13_l3_test_time 4.2705771923065186
Test Epoch13 layer4 Acc 0.8678, AUC 0.9388227462768555, avg_entr 0.015012490563094616, f1 0.8677999973297119
ep13_l4_test_time 5.464277982711792
gc 0
Train Epoch14 Acc 0.979025 (39161/40000), AUC 0.9950916171073914
ep14_train_time 170.8400056362152
Test Epoch14 layer0 Acc 0.8854, AUC 0.9483290910720825, avg_entr 0.12537313997745514, f1 0.8853999972343445
ep14_l0_test_time 0.7172060012817383
Test Epoch14 layer1 Acc 0.8668, AUC 0.9244919419288635, avg_entr 0.021183060482144356, f1 0.8668000102043152
ep14_l1_test_time 1.886204719543457
Test Epoch14 layer2 Acc 0.8666, AUC 0.9347089529037476, avg_entr 0.016463713720440865, f1 0.866599977016449
ep14_l2_test_time 3.1137449741363525
Test Epoch14 layer3 Acc 0.8666, AUC 0.9353641271591187, avg_entr 0.01601576618850231, f1 0.866599977016449
ep14_l3_test_time 4.270060777664185
Test Epoch14 layer4 Acc 0.866, AUC 0.9381977319717407, avg_entr 0.015071330592036247, f1 0.8659999966621399
ep14_l4_test_time 5.470840215682983
gc 0
Train Epoch15 Acc 0.979275 (39171/40000), AUC 0.9953428506851196
ep15_train_time 119.21271109580994
Test Epoch15 layer0 Acc 0.884, AUC 0.9478374719619751, avg_entr 0.12302004545927048, f1 0.8840000033378601
ep15_l0_test_time 0.5631186962127686
Test Epoch15 layer1 Acc 0.868, AUC 0.9222311973571777, avg_entr 0.021171195432543755, f1 0.8679999709129333
ep15_l1_test_time 1.199399709701538
Test Epoch15 layer2 Acc 0.8674, AUC 0.9301039576530457, avg_entr 0.01582755520939827, f1 0.8673999905586243
ep15_l2_test_time 1.8203351497650146
Test Epoch15 layer3 Acc 0.8678, AUC 0.9326999187469482, avg_entr 0.01518150046467781, f1 0.8677999973297119
ep15_l3_test_time 2.4507155418395996
Test Epoch15 layer4 Acc 0.8676, AUC 0.9369432926177979, avg_entr 0.014386268332600594, f1 0.8675999641418457
ep15_l4_test_time 3.07444167137146
gc 0
Train Epoch16 Acc 0.9798 (39192/40000), AUC 0.9955352544784546
ep16_train_time 137.11449527740479
Test Epoch16 layer0 Acc 0.8838, AUC 0.9470608830451965, avg_entr 0.12284292280673981, f1 0.8838000297546387
ep16_l0_test_time 0.7715921401977539
Test Epoch16 layer1 Acc 0.867, AUC 0.9214398860931396, avg_entr 0.021844955161213875, f1 0.8669999837875366
ep16_l1_test_time 2.0608530044555664
Test Epoch16 layer2 Acc 0.8654, AUC 0.9288038015365601, avg_entr 0.015849733725190163, f1 0.865399956703186
ep16_l2_test_time 3.2615325450897217
Test Epoch16 layer3 Acc 0.866, AUC 0.9308249950408936, avg_entr 0.015314451418817043, f1 0.8659999966621399
ep16_l3_test_time 4.554716348648071
Test Epoch16 layer4 Acc 0.866, AUC 0.9358689188957214, avg_entr 0.014492661692202091, f1 0.8659999966621399
ep16_l4_test_time 5.805402994155884
gc 0
Train Epoch17 Acc 0.980775 (39231/40000), AUC 0.9958813190460205
ep17_train_time 182.4699227809906
Test Epoch17 layer0 Acc 0.883, AUC 0.9468789100646973, avg_entr 0.12205935269594193, f1 0.8830000162124634
ep17_l0_test_time 0.7578980922698975
Test Epoch17 layer1 Acc 0.8668, AUC 0.9196780920028687, avg_entr 0.02096509374678135, f1 0.8668000102043152
ep17_l1_test_time 2.057725429534912
Test Epoch17 layer2 Acc 0.8654, AUC 0.9268922209739685, avg_entr 0.015422933734953403, f1 0.865399956703186
ep17_l2_test_time 3.2631571292877197
Test Epoch17 layer3 Acc 0.8652, AUC 0.9299901127815247, avg_entr 0.01476526353508234, f1 0.8652000427246094
ep17_l3_test_time 4.509203195571899
Test Epoch17 layer4 Acc 0.8652, AUC 0.9347077012062073, avg_entr 0.013987349346280098, f1 0.8652000427246094
ep17_l4_test_time 5.72783899307251
gc 0
Train Epoch18 Acc 0.980875 (39235/40000), AUC 0.9958115816116333
ep18_train_time 182.6341004371643
Test Epoch18 layer0 Acc 0.8828, AUC 0.9465012550354004, avg_entr 0.12042421847581863, f1 0.8827999830245972
ep18_l0_test_time 0.7628452777862549
Test Epoch18 layer1 Acc 0.865, AUC 0.9199782013893127, avg_entr 0.020953424274921417, f1 0.8650000095367432
ep18_l1_test_time 2.0762476921081543
Test Epoch18 layer2 Acc 0.8646, AUC 0.9240032434463501, avg_entr 0.015240751206874847, f1 0.8646000027656555
ep18_l2_test_time 3.2770304679870605
Test Epoch18 layer3 Acc 0.8648, AUC 0.9264104962348938, avg_entr 0.014525097794830799, f1 0.8648000359535217
ep18_l3_test_time 4.575510263442993
Test Epoch18 layer4 Acc 0.865, AUC 0.9322257041931152, avg_entr 0.013755357824265957, f1 0.8650000095367432
ep18_l4_test_time 5.7805657386779785
gc 0
Train Epoch19 Acc 0.9812 (39248/40000), AUC 0.9961156845092773
ep19_train_time 182.48123788833618
Test Epoch19 layer0 Acc 0.882, AUC 0.9461835622787476, avg_entr 0.11955576390028, f1 0.8820000290870667
ep19_l0_test_time 0.7887153625488281
Test Epoch19 layer1 Acc 0.8654, AUC 0.9194400310516357, avg_entr 0.020604556426405907, f1 0.865399956703186
ep19_l1_test_time 2.0554800033569336
Test Epoch19 layer2 Acc 0.8654, AUC 0.9255440831184387, avg_entr 0.015434255823493004, f1 0.865399956703186
ep19_l2_test_time 3.26251482963562
Test Epoch19 layer3 Acc 0.8652, AUC 0.9278403520584106, avg_entr 0.014754803851246834, f1 0.8652000427246094
ep19_l3_test_time 4.561279535293579
Test Epoch19 layer4 Acc 0.8656, AUC 0.9329204559326172, avg_entr 0.013993915170431137, f1 0.8655999898910522
ep19_l4_test_time 5.790838956832886
gc 0
Train Epoch20 Acc 0.981125 (39245/40000), AUC 0.9957233667373657
ep20_train_time 182.501540184021
Test Epoch20 layer0 Acc 0.8812, AUC 0.945884108543396, avg_entr 0.11732452362775803, f1 0.8812000155448914
ep20_l0_test_time 0.7619574069976807
Test Epoch20 layer1 Acc 0.8676, AUC 0.9166080951690674, avg_entr 0.01963423378765583, f1 0.8675999641418457
ep20_l1_test_time 2.062011241912842
Test Epoch20 layer2 Acc 0.8666, AUC 0.9231077432632446, avg_entr 0.014430239796638489, f1 0.866599977016449
ep20_l2_test_time 3.235386371612549
Test Epoch20 layer3 Acc 0.8666, AUC 0.9272618293762207, avg_entr 0.013699701055884361, f1 0.866599977016449
ep20_l3_test_time 4.502098560333252
Test Epoch20 layer4 Acc 0.8668, AUC 0.9321617484092712, avg_entr 0.012957241386175156, f1 0.8668000102043152
ep20_l4_test_time 5.828060150146484
gc 0
Train Epoch21 Acc 0.9816 (39264/40000), AUC 0.9958382248878479
ep21_train_time 182.59556579589844
Test Epoch21 layer0 Acc 0.8822, AUC 0.9457790851593018, avg_entr 0.11819108575582504, f1 0.8822000026702881
ep21_l0_test_time 0.7655935287475586
Test Epoch21 layer1 Acc 0.8644, AUC 0.9177690744400024, avg_entr 0.02039945311844349, f1 0.8644000291824341
ep21_l1_test_time 2.0481009483337402
Test Epoch21 layer2 Acc 0.8632, AUC 0.9220669865608215, avg_entr 0.014238525182008743, f1 0.8632000088691711
ep21_l2_test_time 3.279496908187866
Test Epoch21 layer3 Acc 0.8634, AUC 0.9247494339942932, avg_entr 0.01345454715192318, f1 0.8634000420570374
ep21_l3_test_time 4.548696994781494
Test Epoch21 layer4 Acc 0.8634, AUC 0.9305495619773865, avg_entr 0.01268584094941616, f1 0.8634000420570374
ep21_l4_test_time 5.81778883934021
gc 0
Train Epoch22 Acc 0.98185 (39274/40000), AUC 0.9961627721786499
ep22_train_time 182.49446320533752
Test Epoch22 layer0 Acc 0.8802, AUC 0.945612907409668, avg_entr 0.11704802513122559, f1 0.8802000284194946
ep22_l0_test_time 0.7745730876922607
Test Epoch22 layer1 Acc 0.864, AUC 0.9179220199584961, avg_entr 0.01993979886174202, f1 0.8640000224113464
ep22_l1_test_time 2.0769801139831543
Test Epoch22 layer2 Acc 0.8636, AUC 0.9213875532150269, avg_entr 0.013781262561678886, f1 0.8636000156402588
ep22_l2_test_time 3.2780067920684814
Test Epoch22 layer3 Acc 0.8636, AUC 0.9239701628684998, avg_entr 0.013105268590152264, f1 0.8636000156402588
ep22_l3_test_time 4.558475494384766
Test Epoch22 layer4 Acc 0.8632, AUC 0.929877519607544, avg_entr 0.012424067594110966, f1 0.8632000088691711
ep22_l4_test_time 5.773488759994507
gc 0
Train Epoch23 Acc 0.982 (39280/40000), AUC 0.9961408972740173
ep23_train_time 182.50182700157166
Test Epoch23 layer0 Acc 0.8816, AUC 0.9454825520515442, avg_entr 0.11693651229143143, f1 0.881600022315979
ep23_l0_test_time 0.7660126686096191
Test Epoch23 layer1 Acc 0.864, AUC 0.9176040887832642, avg_entr 0.02000497654080391, f1 0.8640000224113464
ep23_l1_test_time 2.045830249786377
Test Epoch23 layer2 Acc 0.863, AUC 0.9208771586418152, avg_entr 0.01404622197151184, f1 0.8629999756813049
ep23_l2_test_time 3.299924612045288
Test Epoch23 layer3 Acc 0.8632, AUC 0.9224861860275269, avg_entr 0.013293766416609287, f1 0.8632000088691711
ep23_l3_test_time 4.512042760848999
Test Epoch23 layer4 Acc 0.863, AUC 0.9291088581085205, avg_entr 0.012593676336109638, f1 0.8629999756813049
ep23_l4_test_time 5.80378532409668
gc 0
Train Epoch24 Acc 0.98195 (39278/40000), AUC 0.9960817098617554
ep24_train_time 182.63136553764343
Test Epoch24 layer0 Acc 0.881, AUC 0.9452857971191406, avg_entr 0.11571075767278671, f1 0.8809999823570251
ep24_l0_test_time 0.7855648994445801
Test Epoch24 layer1 Acc 0.865, AUC 0.9167840480804443, avg_entr 0.02011524885892868, f1 0.8650000095367432
ep24_l1_test_time 2.0555615425109863
Test Epoch24 layer2 Acc 0.866, AUC 0.9208495616912842, avg_entr 0.01461515761911869, f1 0.8659999966621399
ep24_l2_test_time 3.27095627784729
Test Epoch24 layer3 Acc 0.8662, AUC 0.9238653779029846, avg_entr 0.013809256255626678, f1 0.8661999702453613
ep24_l3_test_time 4.576660394668579
Test Epoch24 layer4 Acc 0.8662, AUC 0.9295225143432617, avg_entr 0.013165609911084175, f1 0.8661999702453613
ep24_l4_test_time 5.806220054626465
gc 0
Train Epoch25 Acc 0.982025 (39281/40000), AUC 0.996172308921814
ep25_train_time 174.0579149723053
Test Epoch25 layer0 Acc 0.8796, AUC 0.9452565908432007, avg_entr 0.11535612493753433, f1 0.8795999884605408
ep25_l0_test_time 0.5605673789978027
Test Epoch25 layer1 Acc 0.8662, AUC 0.9161523580551147, avg_entr 0.01978389546275139, f1 0.8661999702453613
ep25_l1_test_time 1.215137004852295
Test Epoch25 layer2 Acc 0.8652, AUC 0.9200106859207153, avg_entr 0.014305401593446732, f1 0.8652000427246094
ep25_l2_test_time 1.840561866760254
Test Epoch25 layer3 Acc 0.8652, AUC 0.9236843585968018, avg_entr 0.013394095003604889, f1 0.8652000427246094
ep25_l3_test_time 2.470224142074585
Test Epoch25 layer4 Acc 0.865, AUC 0.9291001558303833, avg_entr 0.012700891122221947, f1 0.8650000095367432
ep25_l4_test_time 3.0874533653259277
gc 0
Train Epoch26 Acc 0.982475 (39299/40000), AUC 0.9960092306137085
ep26_train_time 181.79828333854675
Test Epoch26 layer0 Acc 0.8812, AUC 0.9451839923858643, avg_entr 0.1152983158826828, f1 0.8812000155448914
ep26_l0_test_time 0.772144079208374
Test Epoch26 layer1 Acc 0.8644, AUC 0.9168577194213867, avg_entr 0.019392279908061028, f1 0.8644000291824341
ep26_l1_test_time 2.05999755859375
Test Epoch26 layer2 Acc 0.8628, AUC 0.9186015129089355, avg_entr 0.013549456372857094, f1 0.8628000020980835
ep26_l2_test_time 3.187361240386963
Test Epoch26 layer3 Acc 0.8626, AUC 0.9201734066009521, avg_entr 0.01278518233448267, f1 0.8626000285148621
ep26_l3_test_time 4.543411493301392
Test Epoch26 layer4 Acc 0.8624, AUC 0.9274932742118835, avg_entr 0.012133387848734856, f1 0.8623999953269958
ep26_l4_test_time 5.788875341415405
gc 0
Train Epoch27 Acc 0.982225 (39289/40000), AUC 0.996393084526062
ep27_train_time 182.562162399292
Test Epoch27 layer0 Acc 0.8808, AUC 0.9451165199279785, avg_entr 0.11471418291330338, f1 0.8808000087738037
ep27_l0_test_time 0.771519660949707
Test Epoch27 layer1 Acc 0.8634, AUC 0.9168031811714172, avg_entr 0.01905439980328083, f1 0.8634000420570374
ep27_l1_test_time 2.0516607761383057
Test Epoch27 layer2 Acc 0.862, AUC 0.9179164171218872, avg_entr 0.01288573257625103, f1 0.8619999885559082
ep27_l2_test_time 3.274016857147217
Test Epoch27 layer3 Acc 0.8622, AUC 0.919284999370575, avg_entr 0.012118654325604439, f1 0.8622000217437744
ep27_l3_test_time 4.556406259536743
Test Epoch27 layer4 Acc 0.8624, AUC 0.9267048835754395, avg_entr 0.011446419171988964, f1 0.8623999953269958
ep27_l4_test_time 5.791973829269409
gc 0
Train Epoch28 Acc 0.982325 (39293/40000), AUC 0.9966174960136414
ep28_train_time 182.5901174545288
Test Epoch28 layer0 Acc 0.8816, AUC 0.9450463056564331, avg_entr 0.11457065492868423, f1 0.881600022315979
ep28_l0_test_time 0.765998363494873
Test Epoch28 layer1 Acc 0.8636, AUC 0.9162834882736206, avg_entr 0.019522830843925476, f1 0.8636000156402588
ep28_l1_test_time 2.065617799758911
Test Epoch28 layer2 Acc 0.8628, AUC 0.9190555810928345, avg_entr 0.013545258902013302, f1 0.8628000020980835
ep28_l2_test_time 3.2585196495056152
Test Epoch28 layer3 Acc 0.8624, AUC 0.9208358526229858, avg_entr 0.01275354903191328, f1 0.8623999953269958
ep28_l3_test_time 4.543814182281494
Test Epoch28 layer4 Acc 0.8624, AUC 0.9273679852485657, avg_entr 0.012102095410227776, f1 0.8623999953269958
ep28_l4_test_time 5.754924297332764
gc 0
Train Epoch29 Acc 0.9824 (39296/40000), AUC 0.996289074420929
ep29_train_time 182.57755064964294
Test Epoch29 layer0 Acc 0.8808, AUC 0.9449917078018188, avg_entr 0.11386416107416153, f1 0.8808000087738037
ep29_l0_test_time 0.7835311889648438
Test Epoch29 layer1 Acc 0.8634, AUC 0.9163291454315186, avg_entr 0.019359786063432693, f1 0.8634000420570374
ep29_l1_test_time 2.056882381439209
Test Epoch29 layer2 Acc 0.8612, AUC 0.9192941188812256, avg_entr 0.013423752039670944, f1 0.8612000346183777
ep29_l2_test_time 3.18581223487854
Test Epoch29 layer3 Acc 0.8612, AUC 0.9209126830101013, avg_entr 0.012619838118553162, f1 0.8612000346183777
ep29_l3_test_time 4.568541765213013
Test Epoch29 layer4 Acc 0.8612, AUC 0.9274119138717651, avg_entr 0.011968474835157394, f1 0.8612000346183777
ep29_l4_test_time 5.812487602233887
Best AUC tensor(0.8992) 4 0
train_loss (2, 5, 30)
valid_acc (5, 30)
valid_AUC (5, 30)
train_acc (30,)
total_train+valid_time 5673.569623947144
Start Testing
Load ckpt at ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt
Test layer0 Acc 0.8952, AUC 0.9558722972869873, avg_entr 0.17894193530082703, f1 0.8952000141143799
l0_test_time 0.7740645408630371
Test layer1 Acc 0.8802, AUC 0.9485803842544556, avg_entr 0.05639229714870453, f1 0.8802000284194946
l1_test_time 2.0532681941986084
Test layer2 Acc 0.8816, AUC 0.9513163566589355, avg_entr 0.040660060942173004, f1 0.881600022315979
l2_test_time 3.282111167907715
Test layer3 Acc 0.8814, AUC 0.9514083862304688, avg_entr 0.04028838500380516, f1 0.8813999891281128
l3_test_time 4.54953670501709
Test layer4 Acc 0.8812, AUC 0.9517313838005066, avg_entr 0.038082223385572433, f1 0.8812000155448914
l4_test_time 5.751365423202515
