total count words 222751
vocab size 30000
train size 40000, valid size 5000, test size 5000
found 27937 words in glove
model: TransformerModelML(
  (layers): ModuleList(
    (0): EMBLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Embedding(30000, 300)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=2, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=2, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.weight 9000000
layers.0.ae.g.0.weight 256
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 256
layers.0.ae.h.0.bias 2
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
Total Trainable Params: 13671778
init_time 22.268208742141724
Start Training
gc 0
Train Epoch0 Acc 0.50955 (20382/40000), AUC 0.5113769173622131
ep0_train_time 115.00674343109131
Test Epoch0 layer0 Acc 0.8144, AUC 0.8853586912155151, avg_entr 0.5852855443954468, f1 0.8144000172615051
ep0_l0_test_time 0.6470823287963867
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 0
Test Epoch0 layer1 Acc 0.8114, AUC 0.899631679058075, avg_entr 0.3700447678565979, f1 0.8113999962806702
ep0_l1_test_time 1.5398001670837402
Test Epoch0 layer2 Acc 0.8038, AUC 0.8989025354385376, avg_entr 0.6321037411689758, f1 0.8037999868392944
ep0_l2_test_time 2.433342933654785
Test Epoch0 layer3 Acc 0.7304, AUC 0.8503334522247314, avg_entr 0.6830753087997437, f1 0.7304000854492188
ep0_l3_test_time 3.2123732566833496
Test Epoch0 layer4 Acc 0.6844, AUC 0.8750423192977905, avg_entr 0.6838696599006653, f1 0.6844000220298767
ep0_l4_test_time 4.149107217788696
gc 0
Train Epoch1 Acc 0.84395 (33758/40000), AUC 0.9129811525344849
ep1_train_time 114.43607497215271
Test Epoch1 layer0 Acc 0.875, AUC 0.9443540573120117, avg_entr 0.30078351497650146, f1 0.875
ep1_l0_test_time 0.6414988040924072
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 1
Test Epoch1 layer1 Acc 0.8824, AUC 0.9528413414955139, avg_entr 0.19699829816818237, f1 0.8823999762535095
ep1_l1_test_time 1.5358738899230957
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 1
Test Epoch1 layer2 Acc 0.8772, AUC 0.9539885520935059, avg_entr 0.18386705219745636, f1 0.8772000074386597
ep1_l2_test_time 2.398488998413086
Test Epoch1 layer3 Acc 0.8608, AUC 0.9540455937385559, avg_entr 0.1638469249010086, f1 0.86080002784729
ep1_l3_test_time 3.242377281188965
Test Epoch1 layer4 Acc 0.85, AUC 0.9546469449996948, avg_entr 0.15391594171524048, f1 0.8500000238418579
ep1_l4_test_time 4.139167070388794
gc 0
Train Epoch2 Acc 0.909575 (36383/40000), AUC 0.9663797616958618
ep2_train_time 114.96037673950195
Test Epoch2 layer0 Acc 0.893, AUC 0.9544918537139893, avg_entr 0.23072956502437592, f1 0.8930000066757202
ep2_l0_test_time 0.7551300525665283
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 2
Test Epoch2 layer1 Acc 0.8918, AUC 0.9573628306388855, avg_entr 0.15338493883609772, f1 0.8917999863624573
ep2_l1_test_time 1.4381189346313477
Test Epoch2 layer2 Acc 0.8934, AUC 0.9587563276290894, avg_entr 0.09918741136789322, f1 0.8934000134468079
ep2_l2_test_time 2.0269277095794678
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 2
Test Epoch2 layer3 Acc 0.89, AUC 0.9599159359931946, avg_entr 0.06726787984371185, f1 0.8899999856948853
ep2_l3_test_time 2.5656752586364746
Test Epoch2 layer4 Acc 0.8882, AUC 0.9595166444778442, avg_entr 0.06498342752456665, f1 0.888200044631958
ep2_l4_test_time 3.0628368854522705
gc 0
Train Epoch3 Acc 0.937775 (37511/40000), AUC 0.9801127910614014
ep3_train_time 91.95985555648804
Test Epoch3 layer0 Acc 0.8936, AUC 0.9581146240234375, avg_entr 0.19877387583255768, f1 0.8935999870300293
ep3_l0_test_time 0.5513119697570801
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 3
Test Epoch3 layer1 Acc 0.888, AUC 0.9552600383758545, avg_entr 0.09542656689882278, f1 0.8880000114440918
ep3_l1_test_time 1.1825084686279297
Test Epoch3 layer2 Acc 0.8922, AUC 0.9532227516174316, avg_entr 0.05119370296597481, f1 0.8921999931335449
ep3_l2_test_time 1.8053152561187744
Test Epoch3 layer3 Acc 0.893, AUC 0.9559751749038696, avg_entr 0.048045266419649124, f1 0.8930000066757202
ep3_l3_test_time 2.4342031478881836
Test Epoch3 layer4 Acc 0.8932, AUC 0.9557676911354065, avg_entr 0.04776163771748543, f1 0.8931999802589417
ep3_l4_test_time 3.063447952270508
gc 0
Train Epoch4 Acc 0.949475 (37979/40000), AUC 0.985228419303894
ep4_train_time 92.13648843765259
Test Epoch4 layer0 Acc 0.8998, AUC 0.9589855670928955, avg_entr 0.1794174313545227, f1 0.8998000025749207
ep4_l0_test_time 0.549546480178833
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 4
Test Epoch4 layer1 Acc 0.8862, AUC 0.9526984691619873, avg_entr 0.05497313290834427, f1 0.8862000107765198
ep4_l1_test_time 1.192765474319458
Test Epoch4 layer2 Acc 0.8874, AUC 0.9542789459228516, avg_entr 0.03674740716814995, f1 0.8873999714851379
ep4_l2_test_time 1.8088111877441406
Test Epoch4 layer3 Acc 0.8868, AUC 0.9540584087371826, avg_entr 0.03684384003281593, f1 0.8867999911308289
ep4_l3_test_time 2.434058904647827
Test Epoch4 layer4 Acc 0.887, AUC 0.9538196325302124, avg_entr 0.03549458459019661, f1 0.8870000243186951
ep4_l4_test_time 3.0611400604248047
gc 0
Train Epoch5 Acc 0.957525 (38301/40000), AUC 0.9867485165596008
ep5_train_time 91.96115732192993
Test Epoch5 layer0 Acc 0.8952, AUC 0.9588602185249329, avg_entr 0.1653575599193573, f1 0.8952000141143799
ep5_l0_test_time 0.550950288772583
Test Epoch5 layer1 Acc 0.8772, AUC 0.9483221769332886, avg_entr 0.045316822826862335, f1 0.8772000074386597
ep5_l1_test_time 1.185455322265625
Test Epoch5 layer2 Acc 0.8752, AUC 0.9524376392364502, avg_entr 0.032031212002038956, f1 0.8751999735832214
ep5_l2_test_time 1.8086364269256592
Test Epoch5 layer3 Acc 0.8738, AUC 0.9522355794906616, avg_entr 0.0316111259162426, f1 0.8737999796867371
ep5_l3_test_time 2.4371626377105713
Test Epoch5 layer4 Acc 0.8738, AUC 0.9520143270492554, avg_entr 0.03016776405274868, f1 0.8737999796867371
ep5_l4_test_time 3.0682308673858643
gc 0
Train Epoch6 Acc 0.962225 (38489/40000), AUC 0.9897293448448181
ep6_train_time 92.17711973190308
Test Epoch6 layer0 Acc 0.883, AUC 0.9575759172439575, avg_entr 0.15451949834823608, f1 0.8830000162124634
ep6_l0_test_time 0.5956470966339111
Test Epoch6 layer1 Acc 0.8784, AUC 0.9443010091781616, avg_entr 0.04148584231734276, f1 0.8784000277519226
ep6_l1_test_time 1.1815431118011475
Test Epoch6 layer2 Acc 0.8796, AUC 0.9507008790969849, avg_entr 0.03187824413180351, f1 0.8795999884605408
ep6_l2_test_time 1.8055675029754639
Test Epoch6 layer3 Acc 0.8786, AUC 0.95029616355896, avg_entr 0.03243671730160713, f1 0.878600001335144
ep6_l3_test_time 2.436721086502075
Test Epoch6 layer4 Acc 0.8792, AUC 0.9501193761825562, avg_entr 0.03162701427936554, f1 0.8791999816894531
ep6_l4_test_time 3.0704777240753174
gc 0
Train Epoch7 Acc 0.966275 (38651/40000), AUC 0.9914869070053101
ep7_train_time 92.23664116859436
Test Epoch7 layer0 Acc 0.89, AUC 0.9553593397140503, avg_entr 0.15098993480205536, f1 0.8899999856948853
ep7_l0_test_time 0.552325963973999
Test Epoch7 layer1 Acc 0.8768, AUC 0.93768709897995, avg_entr 0.0384366475045681, f1 0.876800000667572
ep7_l1_test_time 1.2057397365570068
Test Epoch7 layer2 Acc 0.8756, AUC 0.9494039416313171, avg_entr 0.02861003950238228, f1 0.8756000399589539
ep7_l2_test_time 1.8091490268707275
Test Epoch7 layer3 Acc 0.8752, AUC 0.9497443437576294, avg_entr 0.02881394699215889, f1 0.8751999735832214
ep7_l3_test_time 2.4364967346191406
Test Epoch7 layer4 Acc 0.874, AUC 0.9498382210731506, avg_entr 0.02799423784017563, f1 0.8740000128746033
ep7_l4_test_time 3.066983222961426
gc 0
Train Epoch8 Acc 0.969175 (38767/40000), AUC 0.9924021363258362
ep8_train_time 92.2663345336914
Test Epoch8 layer0 Acc 0.8934, AUC 0.9542478322982788, avg_entr 0.1415727734565735, f1 0.8934000134468079
ep8_l0_test_time 0.5492722988128662
Test Epoch8 layer1 Acc 0.8808, AUC 0.9395495653152466, avg_entr 0.0321228988468647, f1 0.8808000087738037
ep8_l1_test_time 1.182551622390747
Test Epoch8 layer2 Acc 0.8804, AUC 0.9478911757469177, avg_entr 0.023206260055303574, f1 0.8804000020027161
ep8_l2_test_time 1.807859182357788
Test Epoch8 layer3 Acc 0.8808, AUC 0.9475117325782776, avg_entr 0.022840330377221107, f1 0.8808000087738037
ep8_l3_test_time 2.4392552375793457
Test Epoch8 layer4 Acc 0.8804, AUC 0.947484016418457, avg_entr 0.02222268097102642, f1 0.8804000020027161
ep8_l4_test_time 3.065150499343872
gc 0
Train Epoch9 Acc 0.97255 (38902/40000), AUC 0.9933739900588989
ep9_train_time 92.37164664268494
Test Epoch9 layer0 Acc 0.8896, AUC 0.9535925388336182, avg_entr 0.1402936577796936, f1 0.8895999193191528
ep9_l0_test_time 0.5506711006164551
Test Epoch9 layer1 Acc 0.8768, AUC 0.931613564491272, avg_entr 0.03168792650103569, f1 0.876800000667572
ep9_l1_test_time 1.1843576431274414
Test Epoch9 layer2 Acc 0.8778, AUC 0.9442729949951172, avg_entr 0.023335304111242294, f1 0.8778000473976135
ep9_l2_test_time 1.8065335750579834
Test Epoch9 layer3 Acc 0.8794, AUC 0.9464369416236877, avg_entr 0.022772304713726044, f1 0.8794000148773193
ep9_l3_test_time 2.4363393783569336
Test Epoch9 layer4 Acc 0.878, AUC 0.9466325044631958, avg_entr 0.02189587615430355, f1 0.878000020980835
ep9_l4_test_time 3.065539836883545
gc 0
Train Epoch10 Acc 0.974125 (38965/40000), AUC 0.993322491645813
ep10_train_time 92.19805550575256
Test Epoch10 layer0 Acc 0.8898, AUC 0.9524980783462524, avg_entr 0.13530807197093964, f1 0.8898000121116638
ep10_l0_test_time 0.5512564182281494
Test Epoch10 layer1 Acc 0.8784, AUC 0.9320424795150757, avg_entr 0.03080153279006481, f1 0.8784000277519226
ep10_l1_test_time 1.1838903427124023
Test Epoch10 layer2 Acc 0.8782, AUC 0.9440113306045532, avg_entr 0.023252347484230995, f1 0.8781999945640564
ep10_l2_test_time 1.8068807125091553
Test Epoch10 layer3 Acc 0.879, AUC 0.9456075429916382, avg_entr 0.023119580000638962, f1 0.8790000081062317
ep10_l3_test_time 2.437971830368042
Test Epoch10 layer4 Acc 0.877, AUC 0.9458407163619995, avg_entr 0.02273092046380043, f1 0.8769999742507935
ep10_l4_test_time 3.066824197769165
gc 0
Train Epoch11 Acc 0.975 (39000/40000), AUC 0.9942985773086548
ep11_train_time 92.31325221061707
Test Epoch11 layer0 Acc 0.8866, AUC 0.9512622356414795, avg_entr 0.13122618198394775, f1 0.8866000771522522
ep11_l0_test_time 0.5506436824798584
Test Epoch11 layer1 Acc 0.8764, AUC 0.9311679005622864, avg_entr 0.028852984309196472, f1 0.8763999938964844
ep11_l1_test_time 1.1818184852600098
Test Epoch11 layer2 Acc 0.8764, AUC 0.9426790475845337, avg_entr 0.020574035122990608, f1 0.8763999938964844
ep11_l2_test_time 1.8066191673278809
Test Epoch11 layer3 Acc 0.8762, AUC 0.9445133209228516, avg_entr 0.01917436346411705, f1 0.8762000203132629
ep11_l3_test_time 2.434598922729492
Test Epoch11 layer4 Acc 0.8766, AUC 0.9447755813598633, avg_entr 0.018546534702181816, f1 0.8766000270843506
ep11_l4_test_time 3.0657801628112793
gc 0
Train Epoch12 Acc 0.9762 (39048/40000), AUC 0.9944521188735962
ep12_train_time 92.25860595703125
Test Epoch12 layer0 Acc 0.8872, AUC 0.9501999616622925, avg_entr 0.1303493082523346, f1 0.8871999979019165
ep12_l0_test_time 0.550391674041748
Test Epoch12 layer1 Acc 0.8764, AUC 0.9283452033996582, avg_entr 0.029119715094566345, f1 0.8763999938964844
ep12_l1_test_time 1.182485580444336
Test Epoch12 layer2 Acc 0.876, AUC 0.940294086933136, avg_entr 0.021415604278445244, f1 0.8760000467300415
ep12_l2_test_time 1.80934476852417
Test Epoch12 layer3 Acc 0.876, AUC 0.9436560869216919, avg_entr 0.020307153463363647, f1 0.8760000467300415
ep12_l3_test_time 2.434950828552246
Test Epoch12 layer4 Acc 0.8758, AUC 0.9440634250640869, avg_entr 0.01969052106142044, f1 0.8758000135421753
ep12_l4_test_time 3.0663278102874756
gc 0
Train Epoch13 Acc 0.97765 (39106/40000), AUC 0.994828462600708
ep13_train_time 92.27009320259094
Test Epoch13 layer0 Acc 0.8866, AUC 0.9496272802352905, avg_entr 0.12866508960723877, f1 0.8866000771522522
ep13_l0_test_time 0.552255392074585
Test Epoch13 layer1 Acc 0.874, AUC 0.9277699589729309, avg_entr 0.028381094336509705, f1 0.8740000128746033
ep13_l1_test_time 1.1811602115631104
Test Epoch13 layer2 Acc 0.8744, AUC 0.9390026330947876, avg_entr 0.020864073187112808, f1 0.8744000792503357
ep13_l2_test_time 1.807910442352295
Test Epoch13 layer3 Acc 0.874, AUC 0.9428104758262634, avg_entr 0.020043445751070976, f1 0.8740000128746033
ep13_l3_test_time 2.4361934661865234
Test Epoch13 layer4 Acc 0.874, AUC 0.9432412981987, avg_entr 0.0195601899176836, f1 0.8740000128746033
ep13_l4_test_time 3.0665552616119385
gc 0
Train Epoch14 Acc 0.978525 (39141/40000), AUC 0.9949178695678711
ep14_train_time 92.21321105957031
Test Epoch14 layer0 Acc 0.885, AUC 0.948993444442749, avg_entr 0.12591321766376495, f1 0.8849999904632568
ep14_l0_test_time 0.5514774322509766
Test Epoch14 layer1 Acc 0.8732, AUC 0.9262003898620605, avg_entr 0.02802322618663311, f1 0.873199999332428
ep14_l1_test_time 1.1833772659301758
Test Epoch14 layer2 Acc 0.8738, AUC 0.9356080889701843, avg_entr 0.020469576120376587, f1 0.8737999796867371
ep14_l2_test_time 1.8058762550354004
Test Epoch14 layer3 Acc 0.873, AUC 0.9422770738601685, avg_entr 0.01921515166759491, f1 0.8730000257492065
ep14_l3_test_time 2.4356801509857178
Test Epoch14 layer4 Acc 0.8734, AUC 0.9430109262466431, avg_entr 0.018577545881271362, f1 0.8733999729156494
ep14_l4_test_time 3.066875696182251
gc 0
Train Epoch15 Acc 0.97895 (39158/40000), AUC 0.9951150417327881
ep15_train_time 92.1493706703186
Test Epoch15 layer0 Acc 0.8844, AUC 0.9482457041740417, avg_entr 0.12420898675918579, f1 0.8844000101089478
ep15_l0_test_time 0.5554301738739014
Test Epoch15 layer1 Acc 0.874, AUC 0.9241844415664673, avg_entr 0.02681582048535347, f1 0.8740000128746033
ep15_l1_test_time 1.179262399673462
Test Epoch15 layer2 Acc 0.873, AUC 0.9368305206298828, avg_entr 0.01935606077313423, f1 0.8730000257492065
ep15_l2_test_time 1.8058302402496338
Test Epoch15 layer3 Acc 0.873, AUC 0.9415779113769531, avg_entr 0.01808319240808487, f1 0.8730000257492065
ep15_l3_test_time 2.433579444885254
Test Epoch15 layer4 Acc 0.8726, AUC 0.9423004388809204, avg_entr 0.017407260835170746, f1 0.8726000189781189
ep15_l4_test_time 3.0667731761932373
gc 0
Train Epoch16 Acc 0.979375 (39175/40000), AUC 0.9953397512435913
ep16_train_time 92.19702410697937
Test Epoch16 layer0 Acc 0.8828, AUC 0.9476801753044128, avg_entr 0.12265139073133469, f1 0.8827999830245972
ep16_l0_test_time 0.5532479286193848
Test Epoch16 layer1 Acc 0.8722, AUC 0.9235968589782715, avg_entr 0.026794614270329475, f1 0.872200071811676
ep16_l1_test_time 1.1813600063323975
Test Epoch16 layer2 Acc 0.8726, AUC 0.9361523389816284, avg_entr 0.019299985840916634, f1 0.8726000189781189
ep16_l2_test_time 1.8072197437286377
Test Epoch16 layer3 Acc 0.8722, AUC 0.9412204027175903, avg_entr 0.017922168597579002, f1 0.872200071811676
ep16_l3_test_time 2.437363624572754
Test Epoch16 layer4 Acc 0.8722, AUC 0.9418714046478271, avg_entr 0.017215799540281296, f1 0.872200071811676
ep16_l4_test_time 3.066380739212036
gc 0
Train Epoch17 Acc 0.980225 (39209/40000), AUC 0.9955139756202698
ep17_train_time 92.17633199691772
Test Epoch17 layer0 Acc 0.8846, AUC 0.9473932981491089, avg_entr 0.12232568114995956, f1 0.8845999836921692
ep17_l0_test_time 0.5518591403961182
Test Epoch17 layer1 Acc 0.8714, AUC 0.9232736825942993, avg_entr 0.02539750374853611, f1 0.871399998664856
ep17_l1_test_time 1.1815402507781982
Test Epoch17 layer2 Acc 0.8704, AUC 0.9359409809112549, avg_entr 0.01808874122798443, f1 0.8704000115394592
ep17_l2_test_time 1.8071420192718506
Test Epoch17 layer3 Acc 0.871, AUC 0.9409939050674438, avg_entr 0.01681974157691002, f1 0.8709999918937683
ep17_l3_test_time 2.4341344833374023
Test Epoch17 layer4 Acc 0.8704, AUC 0.9418083429336548, avg_entr 0.016289111226797104, f1 0.8704000115394592
ep17_l4_test_time 3.0628650188446045
gc 0
Train Epoch18 Acc 0.98055 (39222/40000), AUC 0.9959769248962402
ep18_train_time 92.24427103996277
Test Epoch18 layer0 Acc 0.8836, AUC 0.9469684362411499, avg_entr 0.12130901217460632, f1 0.8835999965667725
ep18_l0_test_time 0.5592458248138428
Test Epoch18 layer1 Acc 0.8694, AUC 0.9208472967147827, avg_entr 0.02469301037490368, f1 0.8694000244140625
ep18_l1_test_time 1.1844773292541504
Test Epoch18 layer2 Acc 0.8692, AUC 0.9347078800201416, avg_entr 0.017102181911468506, f1 0.8691999912261963
ep18_l2_test_time 1.8080403804779053
Test Epoch18 layer3 Acc 0.8694, AUC 0.9402874708175659, avg_entr 0.015688953921198845, f1 0.8694000244140625
ep18_l3_test_time 2.4360477924346924
Test Epoch18 layer4 Acc 0.8694, AUC 0.9411327838897705, avg_entr 0.015065260231494904, f1 0.8694000244140625
ep18_l4_test_time 3.070612907409668
gc 0
Train Epoch19 Acc 0.9808 (39232/40000), AUC 0.995803952217102
ep19_train_time 92.1893036365509
Test Epoch19 layer0 Acc 0.884, AUC 0.9467220306396484, avg_entr 0.12035293132066727, f1 0.8840000033378601
ep19_l0_test_time 0.5537328720092773
Test Epoch19 layer1 Acc 0.8714, AUC 0.9212194085121155, avg_entr 0.02547735907137394, f1 0.871399998664856
ep19_l1_test_time 1.1912858486175537
Test Epoch19 layer2 Acc 0.872, AUC 0.9328787922859192, avg_entr 0.018096519634127617, f1 0.871999979019165
ep19_l2_test_time 1.8090529441833496
Test Epoch19 layer3 Acc 0.8722, AUC 0.9400847554206848, avg_entr 0.016591202467679977, f1 0.872200071811676
ep19_l3_test_time 2.441896915435791
Test Epoch19 layer4 Acc 0.872, AUC 0.9411511421203613, avg_entr 0.016002744436264038, f1 0.871999979019165
ep19_l4_test_time 3.066654920578003
gc 0
Train Epoch20 Acc 0.98115 (39246/40000), AUC 0.9957507848739624
ep20_train_time 92.26076245307922
Test Epoch20 layer0 Acc 0.8834, AUC 0.9464085102081299, avg_entr 0.11969596892595291, f1 0.8834000825881958
ep20_l0_test_time 0.5726175308227539
Test Epoch20 layer1 Acc 0.869, AUC 0.9208633303642273, avg_entr 0.024691985920071602, f1 0.8690000176429749
ep20_l1_test_time 1.1823408603668213
Test Epoch20 layer2 Acc 0.8702, AUC 0.9328161478042603, avg_entr 0.017053963616490364, f1 0.870199978351593
ep20_l2_test_time 1.8084776401519775
Test Epoch20 layer3 Acc 0.8702, AUC 0.9398477077484131, avg_entr 0.015958508476614952, f1 0.870199978351593
ep20_l3_test_time 2.442669630050659
Test Epoch20 layer4 Acc 0.8702, AUC 0.9409348964691162, avg_entr 0.015339605510234833, f1 0.870199978351593
ep20_l4_test_time 3.066164970397949
gc 0
Train Epoch21 Acc 0.98125 (39250/40000), AUC 0.9959897994995117
ep21_train_time 92.21271133422852
Test Epoch21 layer0 Acc 0.883, AUC 0.9462805986404419, avg_entr 0.11907433718442917, f1 0.8830000162124634
ep21_l0_test_time 0.5486471652984619
Test Epoch21 layer1 Acc 0.8712, AUC 0.9201774001121521, avg_entr 0.024472584947943687, f1 0.8712000846862793
ep21_l1_test_time 1.1819109916687012
Test Epoch21 layer2 Acc 0.8708, AUC 0.9319155812263489, avg_entr 0.01697680726647377, f1 0.8708000183105469
ep21_l2_test_time 1.80501389503479
Test Epoch21 layer3 Acc 0.8702, AUC 0.9397598505020142, avg_entr 0.015625150874257088, f1 0.870199978351593
ep21_l3_test_time 2.4347212314605713
Test Epoch21 layer4 Acc 0.8704, AUC 0.9409018158912659, avg_entr 0.015067709609866142, f1 0.8704000115394592
ep21_l4_test_time 3.062852621078491
gc 0
Train Epoch22 Acc 0.98125 (39250/40000), AUC 0.9958288669586182
ep22_train_time 92.20311641693115
Test Epoch22 layer0 Acc 0.8814, AUC 0.9461275339126587, avg_entr 0.11784200370311737, f1 0.8813999891281128
ep22_l0_test_time 0.5517911911010742
Test Epoch22 layer1 Acc 0.8692, AUC 0.920594334602356, avg_entr 0.02391560934484005, f1 0.8691999912261963
ep22_l1_test_time 1.1848208904266357
Test Epoch22 layer2 Acc 0.869, AUC 0.9325006604194641, avg_entr 0.016117462888360023, f1 0.8690000176429749
ep22_l2_test_time 1.8052995204925537
Test Epoch22 layer3 Acc 0.869, AUC 0.9393693804740906, avg_entr 0.015107615850865841, f1 0.8690000176429749
ep22_l3_test_time 2.4356181621551514
Test Epoch22 layer4 Acc 0.8692, AUC 0.9406006336212158, avg_entr 0.014447085559368134, f1 0.8691999912261963
ep22_l4_test_time 3.0626442432403564
gc 0
Train Epoch23 Acc 0.981725 (39269/40000), AUC 0.996117353439331
ep23_train_time 92.21241998672485
Test Epoch23 layer0 Acc 0.8832, AUC 0.9459363222122192, avg_entr 0.11734907329082489, f1 0.8831999897956848
ep23_l0_test_time 0.565920352935791
Test Epoch23 layer1 Acc 0.8688, AUC 0.9200437664985657, avg_entr 0.02381059341132641, f1 0.8687999844551086
ep23_l1_test_time 1.1900184154510498
Test Epoch23 layer2 Acc 0.869, AUC 0.9319989085197449, avg_entr 0.016099942848086357, f1 0.8690000176429749
ep23_l2_test_time 1.810746431350708
Test Epoch23 layer3 Acc 0.8688, AUC 0.9394774436950684, avg_entr 0.014875457622110844, f1 0.8687999844551086
ep23_l3_test_time 2.4370779991149902
Test Epoch23 layer4 Acc 0.8686, AUC 0.9406214952468872, avg_entr 0.01429565716534853, f1 0.8686000108718872
ep23_l4_test_time 3.0754518508911133
gc 0
Train Epoch24 Acc 0.98195 (39278/40000), AUC 0.9960005283355713
ep24_train_time 92.17479133605957
Test Epoch24 layer0 Acc 0.8826, AUC 0.9458504915237427, avg_entr 0.11659124493598938, f1 0.8826000094413757
ep24_l0_test_time 0.5500006675720215
Test Epoch24 layer1 Acc 0.8686, AUC 0.919614315032959, avg_entr 0.023361388593912125, f1 0.8686000108718872
ep24_l1_test_time 1.1822540760040283
Test Epoch24 layer2 Acc 0.8692, AUC 0.9310784339904785, avg_entr 0.01563720963895321, f1 0.8691999912261963
ep24_l2_test_time 1.8073704242706299
Test Epoch24 layer3 Acc 0.8686, AUC 0.938917338848114, avg_entr 0.01451278105378151, f1 0.8686000108718872
ep24_l3_test_time 2.4349732398986816
Test Epoch24 layer4 Acc 0.8688, AUC 0.94029700756073, avg_entr 0.013889052905142307, f1 0.8687999844551086
ep24_l4_test_time 3.071352481842041
gc 0
Train Epoch25 Acc 0.981725 (39269/40000), AUC 0.9960534572601318
ep25_train_time 92.20951104164124
Test Epoch25 layer0 Acc 0.8828, AUC 0.9457600116729736, avg_entr 0.11619172245264053, f1 0.8827999830245972
ep25_l0_test_time 0.5510299205780029
Test Epoch25 layer1 Acc 0.8686, AUC 0.9193981289863586, avg_entr 0.023207353428006172, f1 0.8686000108718872
ep25_l1_test_time 1.181922197341919
Test Epoch25 layer2 Acc 0.869, AUC 0.9312009811401367, avg_entr 0.015441110357642174, f1 0.8690000176429749
ep25_l2_test_time 1.8066930770874023
Test Epoch25 layer3 Acc 0.8688, AUC 0.9389342069625854, avg_entr 0.014221501536667347, f1 0.8687999844551086
ep25_l3_test_time 2.4368982315063477
Test Epoch25 layer4 Acc 0.8688, AUC 0.9403026700019836, avg_entr 0.01362645998597145, f1 0.8687999844551086
ep25_l4_test_time 3.065904140472412
gc 0
Train Epoch26 Acc 0.981975 (39279/40000), AUC 0.9961522817611694
ep26_train_time 92.1686384677887
Test Epoch26 layer0 Acc 0.8808, AUC 0.9456894397735596, avg_entr 0.11673356592655182, f1 0.8808000087738037
ep26_l0_test_time 0.5547471046447754
Test Epoch26 layer1 Acc 0.8702, AUC 0.918673574924469, avg_entr 0.02363370917737484, f1 0.870199978351593
ep26_l1_test_time 1.182652235031128
Test Epoch26 layer2 Acc 0.8698, AUC 0.92934250831604, avg_entr 0.01630614511668682, f1 0.8697999715805054
ep26_l2_test_time 1.8158090114593506
Test Epoch26 layer3 Acc 0.87, AUC 0.9385768175125122, avg_entr 0.015157924965023994, f1 0.8700000047683716
ep26_l3_test_time 2.4362967014312744
Test Epoch26 layer4 Acc 0.87, AUC 0.9402737617492676, avg_entr 0.014790745452046394, f1 0.8700000047683716
ep26_l4_test_time 3.069730520248413
gc 0
Train Epoch27 Acc 0.981775 (39271/40000), AUC 0.9963053464889526
ep27_train_time 92.20399594306946
Test Epoch27 layer0 Acc 0.883, AUC 0.9456291198730469, avg_entr 0.1159023568034172, f1 0.8830000162124634
ep27_l0_test_time 0.5540838241577148
Test Epoch27 layer1 Acc 0.8696, AUC 0.9189929962158203, avg_entr 0.02312372997403145, f1 0.8695999383926392
ep27_l1_test_time 1.1805484294891357
Test Epoch27 layer2 Acc 0.8698, AUC 0.9297425746917725, avg_entr 0.015564399771392345, f1 0.8697999715805054
ep27_l2_test_time 1.8091909885406494
Test Epoch27 layer3 Acc 0.8698, AUC 0.9385483264923096, avg_entr 0.014389538206160069, f1 0.8697999715805054
ep27_l3_test_time 2.434542417526245
Test Epoch27 layer4 Acc 0.8696, AUC 0.9401684999465942, avg_entr 0.013892178423702717, f1 0.8695999383926392
ep27_l4_test_time 3.0682003498077393
gc 0
Train Epoch28 Acc 0.9821 (39284/40000), AUC 0.9962863326072693
ep28_train_time 92.21563839912415
Test Epoch28 layer0 Acc 0.8808, AUC 0.9455500841140747, avg_entr 0.11551529169082642, f1 0.8808000087738037
ep28_l0_test_time 0.5545613765716553
Test Epoch28 layer1 Acc 0.8688, AUC 0.9190038442611694, avg_entr 0.022840553894639015, f1 0.8687999844551086
ep28_l1_test_time 1.1862390041351318
Test Epoch28 layer2 Acc 0.8694, AUC 0.9298919439315796, avg_entr 0.015233615413308144, f1 0.8694000244140625
ep28_l2_test_time 1.8077118396759033
Test Epoch28 layer3 Acc 0.8694, AUC 0.9384452104568481, avg_entr 0.01408441737294197, f1 0.8694000244140625
ep28_l3_test_time 2.4372100830078125
Test Epoch28 layer4 Acc 0.8696, AUC 0.9401037693023682, avg_entr 0.013572241179645061, f1 0.8695999383926392
ep28_l4_test_time 3.0719411373138428
gc 0
Train Epoch29 Acc 0.98225 (39290/40000), AUC 0.9961355924606323
ep29_train_time 92.27444505691528
Test Epoch29 layer0 Acc 0.8826, AUC 0.9455130696296692, avg_entr 0.11508433520793915, f1 0.8826000094413757
ep29_l0_test_time 0.5527360439300537
Test Epoch29 layer1 Acc 0.8684, AUC 0.918969452381134, avg_entr 0.022569043561816216, f1 0.868399977684021
ep29_l1_test_time 1.187013864517212
Test Epoch29 layer2 Acc 0.8688, AUC 0.9300607442855835, avg_entr 0.014815660193562508, f1 0.8687999844551086
ep29_l2_test_time 1.8095111846923828
Test Epoch29 layer3 Acc 0.868, AUC 0.9384465217590332, avg_entr 0.013798694126307964, f1 0.8679999709129333
ep29_l3_test_time 2.435985565185547
Test Epoch29 layer4 Acc 0.8686, AUC 0.9400131106376648, avg_entr 0.013196505606174469, f1 0.8686000108718872
ep29_l4_test_time 3.064040422439575
Best AUC tensor(0.8998) 4 0
train_loss (2, 5, 30)
valid_acc (5, 30)
valid_AUC (5, 30)
train_acc (30,)
total_train+valid_time 3113.938087463379
Start Testing
Load ckpt at ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt
Test layer0 Acc 0.898, AUC 0.9557527303695679, avg_entr 0.1801348179578781, f1 0.8980000019073486
l0_test_time 0.5597360134124756
Test layer1 Acc 0.8838, AUC 0.9515752792358398, avg_entr 0.05582123249769211, f1 0.8838000297546387
l1_test_time 1.1759605407714844
Test layer2 Acc 0.8834, AUC 0.9536749124526978, avg_entr 0.03609631210565567, f1 0.8834000825881958
l2_test_time 1.7990243434906006
Test layer3 Acc 0.8826, AUC 0.953220784664154, avg_entr 0.03599588945508003, f1 0.8826000094413757
l3_test_time 2.429631233215332
Test layer4 Acc 0.8834, AUC 0.9534205794334412, avg_entr 0.0347287580370903, f1 0.8834000825881958
l4_test_time 3.056732177734375
