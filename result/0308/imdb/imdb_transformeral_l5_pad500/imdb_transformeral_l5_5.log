total count words 222751
vocab size 30000
train size 40000, valid size 5000, test size 5000
found 27937 words in glove
model: TransformerModelML(
  (layers): ModuleList(
    (0): EMBLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Embedding(30000, 300)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=2, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=2, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.weight 9000000
layers.0.ae.g.0.weight 256
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 256
layers.0.ae.h.0.bias 2
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
Total Trainable Params: 13671778
init_time 22.434877634048462
Start Training
gc 0
Train Epoch0 Acc 0.487025 (19481/40000), AUC 0.48507630825042725
ep0_train_time 114.65823340415955
Test Epoch0 layer0 Acc 0.7694, AUC 0.8896116018295288, avg_entr 0.5489886999130249, f1 0.7694000005722046
ep0_l0_test_time 0.6402015686035156
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 0
Test Epoch0 layer1 Acc 0.8178, AUC 0.8963745832443237, avg_entr 0.3718588352203369, f1 0.817799985408783
ep0_l1_test_time 1.495332956314087
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 0
Test Epoch0 layer2 Acc 0.787, AUC 0.8902260661125183, avg_entr 0.6172608137130737, f1 0.7870000600814819
ep0_l2_test_time 2.3889927864074707
Test Epoch0 layer3 Acc 0.7098, AUC 0.8875056505203247, avg_entr 0.6511218547821045, f1 0.7098000049591064
ep0_l3_test_time 3.2800133228302
Test Epoch0 layer4 Acc 0.503, AUC 0.550194501876831, avg_entr 0.6910704970359802, f1 0.503000020980835
ep0_l4_test_time 4.172937870025635
gc 0
Train Epoch1 Acc 0.83225 (33290/40000), AUC 0.9138311147689819
ep1_train_time 114.36865878105164
Test Epoch1 layer0 Acc 0.8808, AUC 0.9448801279067993, avg_entr 0.28689202666282654, f1 0.8808000087738037
ep1_l0_test_time 0.6497316360473633
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 1
Test Epoch1 layer1 Acc 0.8876, AUC 0.9529252052307129, avg_entr 0.19426937401294708, f1 0.8876000046730042
ep1_l1_test_time 1.5465054512023926
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 1
Test Epoch1 layer2 Acc 0.8906, AUC 0.9542420506477356, avg_entr 0.1757017821073532, f1 0.8906000256538391
ep1_l2_test_time 2.417154312133789
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 1
Test Epoch1 layer3 Acc 0.8882, AUC 0.9543195962905884, avg_entr 0.15501315891742706, f1 0.888200044631958
ep1_l3_test_time 3.229510545730591
Test Epoch1 layer4 Acc 0.889, AUC 0.9534459114074707, avg_entr 0.14664243161678314, f1 0.8889999985694885
ep1_l4_test_time 4.121257781982422
gc 0
Train Epoch2 Acc 0.905825 (36233/40000), AUC 0.9632986187934875
ep2_train_time 121.86904120445251
Test Epoch2 layer0 Acc 0.8772, AUC 0.9531562328338623, avg_entr 0.2278505563735962, f1 0.8772000074386597
ep2_l0_test_time 0.5586342811584473
Test Epoch2 layer1 Acc 0.8928, AUC 0.9578149318695068, avg_entr 0.15264485776424408, f1 0.892799973487854
ep2_l1_test_time 1.5176210403442383
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 2
Test Epoch2 layer2 Acc 0.8956, AUC 0.9587059617042542, avg_entr 0.09209679812192917, f1 0.8956000208854675
ep2_l2_test_time 2.167724370956421
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 2
Test Epoch2 layer3 Acc 0.8978, AUC 0.9596430063247681, avg_entr 0.06414192169904709, f1 0.8978000283241272
ep2_l3_test_time 2.707048177719116
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 2
Test Epoch2 layer4 Acc 0.8994, AUC 0.9595233201980591, avg_entr 0.0616549551486969, f1 0.8993999361991882
ep2_l4_test_time 3.371187448501587
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 2
gc 0
Train Epoch3 Acc 0.936775 (37471/40000), AUC 0.9797626733779907
ep3_train_time 124.14153099060059
Test Epoch3 layer0 Acc 0.8964, AUC 0.95796138048172, avg_entr 0.1991162896156311, f1 0.896399974822998
ep3_l0_test_time 0.6394340991973877
Test Epoch3 layer1 Acc 0.8906, AUC 0.9552112221717834, avg_entr 0.11364902555942535, f1 0.8906000256538391
ep3_l1_test_time 1.5212886333465576
Test Epoch3 layer2 Acc 0.8926, AUC 0.9544602632522583, avg_entr 0.05461801216006279, f1 0.8925999999046326
ep3_l2_test_time 2.216920852661133
Test Epoch3 layer3 Acc 0.893, AUC 0.9561047554016113, avg_entr 0.0444677472114563, f1 0.8930000066757202
ep3_l3_test_time 2.8797457218170166
Test Epoch3 layer4 Acc 0.894, AUC 0.9563159346580505, avg_entr 0.04409259185194969, f1 0.8939999938011169
ep3_l4_test_time 3.5755555629730225
gc 0
Train Epoch4 Acc 0.948675 (37947/40000), AUC 0.9846568703651428
ep4_train_time 115.8242917060852
Test Epoch4 layer0 Acc 0.8916, AUC 0.9588576555252075, avg_entr 0.17400924861431122, f1 0.8916000127792358
ep4_l0_test_time 0.7018444538116455
Test Epoch4 layer1 Acc 0.8866, AUC 0.9521016478538513, avg_entr 0.06764037162065506, f1 0.8866000771522522
ep4_l1_test_time 1.5152678489685059
Test Epoch4 layer2 Acc 0.8878, AUC 0.9524306654930115, avg_entr 0.03903522342443466, f1 0.8877999782562256
ep4_l2_test_time 2.416031837463379
Test Epoch4 layer3 Acc 0.8866, AUC 0.9532080888748169, avg_entr 0.03595015034079552, f1 0.8866000771522522
ep4_l3_test_time 2.8078057765960693
Test Epoch4 layer4 Acc 0.8864, AUC 0.953242301940918, avg_entr 0.03563809394836426, f1 0.8863999843597412
ep4_l4_test_time 3.566253662109375
gc 0
Train Epoch5 Acc 0.95775 (38310/40000), AUC 0.9870966672897339
ep5_train_time 114.45513391494751
Test Epoch5 layer0 Acc 0.8968, AUC 0.958199143409729, avg_entr 0.1626446396112442, f1 0.8967999815940857
ep5_l0_test_time 0.6605560779571533
Test Epoch5 layer1 Acc 0.8868, AUC 0.9484165906906128, avg_entr 0.05112173780798912, f1 0.8867999911308289
ep5_l1_test_time 1.501347541809082
Test Epoch5 layer2 Acc 0.8864, AUC 0.9509608745574951, avg_entr 0.03364820033311844, f1 0.8863999843597412
ep5_l2_test_time 2.3770484924316406
Test Epoch5 layer3 Acc 0.8862, AUC 0.9511235952377319, avg_entr 0.032441310584545135, f1 0.8862000107765198
ep5_l3_test_time 3.257575273513794
Test Epoch5 layer4 Acc 0.888, AUC 0.9511728286743164, avg_entr 0.031762562692165375, f1 0.8880000114440918
ep5_l4_test_time 4.158276081085205
gc 0
Train Epoch6 Acc 0.961675 (38467/40000), AUC 0.9886078834533691
ep6_train_time 114.12695789337158
Test Epoch6 layer0 Acc 0.8962, AUC 0.9573057889938354, avg_entr 0.15157672762870789, f1 0.8962000012397766
ep6_l0_test_time 0.6605410575866699
Test Epoch6 layer1 Acc 0.8786, AUC 0.9438914060592651, avg_entr 0.04173972085118294, f1 0.878600001335144
ep6_l1_test_time 1.5348758697509766
Test Epoch6 layer2 Acc 0.8816, AUC 0.9485111832618713, avg_entr 0.030800992622971535, f1 0.881600022315979
ep6_l2_test_time 2.3989531993865967
Test Epoch6 layer3 Acc 0.8806, AUC 0.9493808746337891, avg_entr 0.029829727485775948, f1 0.8805999755859375
ep6_l3_test_time 3.2586944103240967
Test Epoch6 layer4 Acc 0.8804, AUC 0.949561357498169, avg_entr 0.029340777546167374, f1 0.8804000020027161
ep6_l4_test_time 4.124739646911621
gc 0
Train Epoch7 Acc 0.9658 (38632/40000), AUC 0.9906855821609497
ep7_train_time 123.06648993492126
Test Epoch7 layer0 Acc 0.8918, AUC 0.9559414386749268, avg_entr 0.14270466566085815, f1 0.8917999863624573
ep7_l0_test_time 0.5537774562835693
Test Epoch7 layer1 Acc 0.8794, AUC 0.9392616748809814, avg_entr 0.03486383706331253, f1 0.8794000148773193
ep7_l1_test_time 1.185436487197876
Test Epoch7 layer2 Acc 0.8802, AUC 0.9463003873825073, avg_entr 0.026871779933571815, f1 0.8802000284194946
ep7_l2_test_time 1.8179428577423096
Test Epoch7 layer3 Acc 0.8796, AUC 0.9463404417037964, avg_entr 0.026295248419046402, f1 0.8795999884605408
ep7_l3_test_time 2.733262062072754
Test Epoch7 layer4 Acc 0.8792, AUC 0.946593701839447, avg_entr 0.025416135787963867, f1 0.8791999816894531
ep7_l4_test_time 3.344669818878174
gc 0
Train Epoch8 Acc 0.96885 (38754/40000), AUC 0.991666853427887
ep8_train_time 123.18386936187744
Test Epoch8 layer0 Acc 0.8918, AUC 0.9538959860801697, avg_entr 0.1408098340034485, f1 0.8917999863624573
ep8_l0_test_time 0.6331579685211182
Test Epoch8 layer1 Acc 0.871, AUC 0.9340213537216187, avg_entr 0.03275241330265999, f1 0.8709999918937683
ep8_l1_test_time 1.5141806602478027
Test Epoch8 layer2 Acc 0.8716, AUC 0.9430919885635376, avg_entr 0.024039635434746742, f1 0.8715999722480774
ep8_l2_test_time 2.4002645015716553
Test Epoch8 layer3 Acc 0.8708, AUC 0.9430672526359558, avg_entr 0.023435572162270546, f1 0.8708000183105469
ep8_l3_test_time 3.1999030113220215
Test Epoch8 layer4 Acc 0.8712, AUC 0.944125771522522, avg_entr 0.02345466986298561, f1 0.8712000846862793
ep8_l4_test_time 3.6140236854553223
gc 0
Train Epoch9 Acc 0.97345 (38938/40000), AUC 0.9928833246231079
ep9_train_time 115.24193000793457
Test Epoch9 layer0 Acc 0.8912, AUC 0.9530723690986633, avg_entr 0.1335342824459076, f1 0.8912000060081482
ep9_l0_test_time 0.6484050750732422
Test Epoch9 layer1 Acc 0.8768, AUC 0.9332811832427979, avg_entr 0.02958539128303528, f1 0.876800000667572
ep9_l1_test_time 1.526991367340088
Test Epoch9 layer2 Acc 0.877, AUC 0.9426751136779785, avg_entr 0.02194197289645672, f1 0.8769999742507935
ep9_l2_test_time 2.4121530055999756
Test Epoch9 layer3 Acc 0.8772, AUC 0.9424246549606323, avg_entr 0.021404946222901344, f1 0.8772000074386597
ep9_l3_test_time 3.2383933067321777
Test Epoch9 layer4 Acc 0.8774, AUC 0.9429856538772583, avg_entr 0.02087283693253994, f1 0.8773999810218811
ep9_l4_test_time 4.101059675216675
gc 0
Train Epoch10 Acc 0.974175 (38967/40000), AUC 0.9930274486541748
ep10_train_time 113.41086602210999
Test Epoch10 layer0 Acc 0.8898, AUC 0.9519495368003845, avg_entr 0.13300012052059174, f1 0.8898000121116638
ep10_l0_test_time 0.6359167098999023
Test Epoch10 layer1 Acc 0.8752, AUC 0.9306252002716064, avg_entr 0.030316095799207687, f1 0.8751999735832214
ep10_l1_test_time 1.5381910800933838
Test Epoch10 layer2 Acc 0.8756, AUC 0.9405934810638428, avg_entr 0.0216270349919796, f1 0.8756000399589539
ep10_l2_test_time 2.3968889713287354
Test Epoch10 layer3 Acc 0.8754, AUC 0.9409410953521729, avg_entr 0.020770303905010223, f1 0.8754000067710876
ep10_l3_test_time 3.206705331802368
Test Epoch10 layer4 Acc 0.8754, AUC 0.9415346384048462, avg_entr 0.020074838772416115, f1 0.8754000067710876
ep10_l4_test_time 4.205976724624634
gc 0
Train Epoch11 Acc 0.976225 (39049/40000), AUC 0.9940404891967773
ep11_train_time 115.93641352653503
Test Epoch11 layer0 Acc 0.8876, AUC 0.9507949352264404, avg_entr 0.12888465821743011, f1 0.8876000046730042
ep11_l0_test_time 0.863924503326416
Test Epoch11 layer1 Acc 0.8738, AUC 0.9285961985588074, avg_entr 0.02790774591267109, f1 0.8737999796867371
ep11_l1_test_time 1.4857418537139893
Test Epoch11 layer2 Acc 0.8746, AUC 0.9389083385467529, avg_entr 0.02020503580570221, f1 0.8745999932289124
ep11_l2_test_time 2.2846899032592773
Test Epoch11 layer3 Acc 0.8744, AUC 0.9392502307891846, avg_entr 0.01954534277319908, f1 0.8744000792503357
ep11_l3_test_time 3.2105541229248047
Test Epoch11 layer4 Acc 0.8742, AUC 0.9402822256088257, avg_entr 0.018922077491879463, f1 0.8741999864578247
ep11_l4_test_time 4.147091388702393
gc 0
Train Epoch12 Acc 0.9771 (39084/40000), AUC 0.9943147301673889
ep12_train_time 124.20435881614685
Test Epoch12 layer0 Acc 0.8872, AUC 0.9498171806335449, avg_entr 0.1279105395078659, f1 0.8871999979019165
ep12_l0_test_time 0.5531957149505615
Test Epoch12 layer1 Acc 0.8728, AUC 0.9276221990585327, avg_entr 0.025807645171880722, f1 0.8727999925613403
ep12_l1_test_time 1.188957691192627
Test Epoch12 layer2 Acc 0.8722, AUC 0.9377186894416809, avg_entr 0.018498430028557777, f1 0.872200071811676
ep12_l2_test_time 1.8126816749572754
Test Epoch12 layer3 Acc 0.8718, AUC 0.9377962350845337, avg_entr 0.01782349683344364, f1 0.8718000650405884
ep12_l3_test_time 2.442474603652954
Test Epoch12 layer4 Acc 0.8716, AUC 0.9389103651046753, avg_entr 0.017415834590792656, f1 0.8715999722480774
ep12_l4_test_time 3.458799123764038
gc 0
Train Epoch13 Acc 0.97795 (39118/40000), AUC 0.9946733713150024
ep13_train_time 122.01846718788147
Test Epoch13 layer0 Acc 0.8844, AUC 0.9491615295410156, avg_entr 0.12480659037828445, f1 0.8844000101089478
ep13_l0_test_time 0.6580979824066162
Test Epoch13 layer1 Acc 0.872, AUC 0.925247848033905, avg_entr 0.02620466612279415, f1 0.871999979019165
ep13_l1_test_time 1.5316660404205322
Test Epoch13 layer2 Acc 0.8726, AUC 0.934691846370697, avg_entr 0.01876598782837391, f1 0.8726000189781189
ep13_l2_test_time 2.4198617935180664
Test Epoch13 layer3 Acc 0.8734, AUC 0.9360913038253784, avg_entr 0.018354889005422592, f1 0.8733999729156494
ep13_l3_test_time 3.2065236568450928
Test Epoch13 layer4 Acc 0.8726, AUC 0.9374208450317383, avg_entr 0.017943914979696274, f1 0.8726000189781189
ep13_l4_test_time 4.185154676437378
gc 0
Train Epoch14 Acc 0.978425 (39137/40000), AUC 0.9946091175079346
ep14_train_time 114.75951647758484
Test Epoch14 layer0 Acc 0.8842, AUC 0.9484999179840088, avg_entr 0.12535490095615387, f1 0.8842000365257263
ep14_l0_test_time 0.6402480602264404
Test Epoch14 layer1 Acc 0.8694, AUC 0.9246562719345093, avg_entr 0.02499490976333618, f1 0.8694000244140625
ep14_l1_test_time 1.5252032279968262
Test Epoch14 layer2 Acc 0.87, AUC 0.9345538020133972, avg_entr 0.018494972959160805, f1 0.8700000047683716
ep14_l2_test_time 2.417438507080078
Test Epoch14 layer3 Acc 0.87, AUC 0.9356604814529419, avg_entr 0.017637591809034348, f1 0.8700000047683716
ep14_l3_test_time 3.233222246170044
Test Epoch14 layer4 Acc 0.8698, AUC 0.9373006820678711, avg_entr 0.01703939214348793, f1 0.8697999715805054
ep14_l4_test_time 4.174985647201538
gc 0
Train Epoch15 Acc 0.979525 (39181/40000), AUC 0.995101809501648
ep15_train_time 113.4235851764679
Test Epoch15 layer0 Acc 0.8854, AUC 0.9478675127029419, avg_entr 0.12322395294904709, f1 0.8853999972343445
ep15_l0_test_time 0.6754705905914307
Test Epoch15 layer1 Acc 0.8702, AUC 0.9229804277420044, avg_entr 0.023616312071681023, f1 0.870199978351593
ep15_l1_test_time 1.5556504726409912
Test Epoch15 layer2 Acc 0.87, AUC 0.929775595664978, avg_entr 0.01629149541258812, f1 0.8700000047683716
ep15_l2_test_time 2.3840596675872803
Test Epoch15 layer3 Acc 0.8704, AUC 0.9338845014572144, avg_entr 0.01564471423625946, f1 0.8704000115394592
ep15_l3_test_time 3.243464231491089
Test Epoch15 layer4 Acc 0.8696, AUC 0.9361882209777832, avg_entr 0.015087884850800037, f1 0.8695999383926392
ep15_l4_test_time 4.106450080871582
gc 0
Train Epoch16 Acc 0.9797 (39188/40000), AUC 0.9950894117355347
ep16_train_time 116.89530181884766
Test Epoch16 layer0 Acc 0.8806, AUC 0.9472741484642029, avg_entr 0.1221577376127243, f1 0.8805999755859375
ep16_l0_test_time 0.8187847137451172
Test Epoch16 layer1 Acc 0.8698, AUC 0.9228171110153198, avg_entr 0.02334750071167946, f1 0.8697999715805054
ep16_l1_test_time 1.544776201248169
Test Epoch16 layer2 Acc 0.8706, AUC 0.930917501449585, avg_entr 0.016793645918369293, f1 0.8705999851226807
ep16_l2_test_time 1.9993855953216553
Test Epoch16 layer3 Acc 0.8708, AUC 0.9338933229446411, avg_entr 0.016315005719661713, f1 0.8708000183105469
ep16_l3_test_time 2.7446115016937256
Test Epoch16 layer4 Acc 0.8702, AUC 0.9357568025588989, avg_entr 0.01581486500799656, f1 0.870199978351593
ep16_l4_test_time 4.185433626174927
gc 0
Train Epoch17 Acc 0.981025 (39241/40000), AUC 0.9952670931816101
ep17_train_time 125.81154489517212
Test Epoch17 layer0 Acc 0.8814, AUC 0.9470008611679077, avg_entr 0.12040752172470093, f1 0.8813999891281128
ep17_l0_test_time 0.618187665939331
Test Epoch17 layer1 Acc 0.8694, AUC 0.9210095405578613, avg_entr 0.02463541366159916, f1 0.8694000244140625
ep17_l1_test_time 1.1890530586242676
Test Epoch17 layer2 Acc 0.8688, AUC 0.9265004396438599, avg_entr 0.018081650137901306, f1 0.8687999844551086
ep17_l2_test_time 1.815774917602539
Test Epoch17 layer3 Acc 0.8682, AUC 0.9309889078140259, avg_entr 0.01725608855485916, f1 0.8682000041007996
ep17_l3_test_time 2.448460817337036
Test Epoch17 layer4 Acc 0.868, AUC 0.933641791343689, avg_entr 0.016742244362831116, f1 0.8679999709129333
ep17_l4_test_time 3.0757663249969482
gc 0
Train Epoch18 Acc 0.980925 (39237/40000), AUC 0.9953861236572266
ep18_train_time 120.58687353134155
Test Epoch18 layer0 Acc 0.8818, AUC 0.9467214345932007, avg_entr 0.1199311763048172, f1 0.8818000555038452
ep18_l0_test_time 0.6344728469848633
Test Epoch18 layer1 Acc 0.8684, AUC 0.9210799336433411, avg_entr 0.021976513788104057, f1 0.868399977684021
ep18_l1_test_time 1.532322883605957
Test Epoch18 layer2 Acc 0.8692, AUC 0.9251632690429688, avg_entr 0.014445843175053596, f1 0.8691999912261963
ep18_l2_test_time 2.3831043243408203
Test Epoch18 layer3 Acc 0.8696, AUC 0.9305448532104492, avg_entr 0.013787060976028442, f1 0.8695999383926392
ep18_l3_test_time 3.2531235218048096
Test Epoch18 layer4 Acc 0.8698, AUC 0.9339357614517212, avg_entr 0.013284710235893726, f1 0.8697999715805054
ep18_l4_test_time 4.123738527297974
gc 0
Train Epoch19 Acc 0.9814 (39256/40000), AUC 0.9956250786781311
ep19_train_time 114.79865574836731
Test Epoch19 layer0 Acc 0.8816, AUC 0.9464201331138611, avg_entr 0.11916463822126389, f1 0.881600022315979
ep19_l0_test_time 0.6554460525512695
Test Epoch19 layer1 Acc 0.8682, AUC 0.9202234745025635, avg_entr 0.022245150059461594, f1 0.8682000041007996
ep19_l1_test_time 1.5279991626739502
Test Epoch19 layer2 Acc 0.8696, AUC 0.9235033392906189, avg_entr 0.014725372195243835, f1 0.8695999383926392
ep19_l2_test_time 2.4044182300567627
Test Epoch19 layer3 Acc 0.8698, AUC 0.9294718503952026, avg_entr 0.01421381812542677, f1 0.8697999715805054
ep19_l3_test_time 3.253748893737793
Test Epoch19 layer4 Acc 0.8692, AUC 0.932770848274231, avg_entr 0.013791006058454514, f1 0.8691999912261963
ep19_l4_test_time 4.189146280288696
gc 0
Train Epoch20 Acc 0.981975 (39279/40000), AUC 0.9956376552581787
ep20_train_time 113.38470959663391
Test Epoch20 layer0 Acc 0.8806, AUC 0.9461702108383179, avg_entr 0.11831969022750854, f1 0.8805999755859375
ep20_l0_test_time 0.6419928073883057
Test Epoch20 layer1 Acc 0.8688, AUC 0.920177698135376, avg_entr 0.021620333194732666, f1 0.8687999844551086
ep20_l1_test_time 1.546416997909546
Test Epoch20 layer2 Acc 0.8694, AUC 0.9219011068344116, avg_entr 0.013943791389465332, f1 0.8694000244140625
ep20_l2_test_time 2.457618474960327
Test Epoch20 layer3 Acc 0.869, AUC 0.928741455078125, avg_entr 0.013386845588684082, f1 0.8690000176429749
ep20_l3_test_time 3.2280704975128174
Test Epoch20 layer4 Acc 0.869, AUC 0.932496190071106, avg_entr 0.012951129116117954, f1 0.8690000176429749
ep20_l4_test_time 4.166559219360352
gc 0
Train Epoch21 Acc 0.982025 (39281/40000), AUC 0.9957811832427979
ep21_train_time 118.1288993358612
Test Epoch21 layer0 Acc 0.8806, AUC 0.9459907412528992, avg_entr 0.11857687681913376, f1 0.8805999755859375
ep21_l0_test_time 0.8213651180267334
Test Epoch21 layer1 Acc 0.8694, AUC 0.9208070039749146, avg_entr 0.02173900604248047, f1 0.8694000244140625
ep21_l1_test_time 1.5200371742248535
Test Epoch21 layer2 Acc 0.8688, AUC 0.921917200088501, avg_entr 0.015493321232497692, f1 0.8687999844551086
ep21_l2_test_time 2.046142339706421
Test Epoch21 layer3 Acc 0.8688, AUC 0.9292739033699036, avg_entr 0.014968695119023323, f1 0.8687999844551086
ep21_l3_test_time 2.756258726119995
Test Epoch21 layer4 Acc 0.869, AUC 0.932356595993042, avg_entr 0.014485226012766361, f1 0.8690000176429749
ep21_l4_test_time 3.330322265625
gc 0
Train Epoch22 Acc 0.9817 (39268/40000), AUC 0.995700478553772
ep22_train_time 126.25280785560608
Test Epoch22 layer0 Acc 0.8804, AUC 0.9458266496658325, avg_entr 0.11669021099805832, f1 0.8804000020027161
ep22_l0_test_time 0.6225557327270508
Test Epoch22 layer1 Acc 0.868, AUC 0.91845703125, avg_entr 0.02163814939558506, f1 0.8679999709129333
ep22_l1_test_time 1.3486297130584717
Test Epoch22 layer2 Acc 0.869, AUC 0.9201533794403076, avg_entr 0.013888009823858738, f1 0.8690000176429749
ep22_l2_test_time 2.0661253929138184
Test Epoch22 layer3 Acc 0.8686, AUC 0.927478551864624, avg_entr 0.01338789239525795, f1 0.8686000108718872
ep22_l3_test_time 2.5072107315063477
Test Epoch22 layer4 Acc 0.869, AUC 0.9310607314109802, avg_entr 0.013028811663389206, f1 0.8690000176429749
ep22_l4_test_time 3.073817014694214
gc 0
Train Epoch23 Acc 0.982125 (39285/40000), AUC 0.9958266019821167
ep23_train_time 119.25482940673828
Test Epoch23 layer0 Acc 0.8802, AUC 0.9456929564476013, avg_entr 0.11684074997901917, f1 0.8802000284194946
ep23_l0_test_time 0.6407668590545654
Test Epoch23 layer1 Acc 0.8682, AUC 0.9185503721237183, avg_entr 0.021491022780537605, f1 0.8682000041007996
ep23_l1_test_time 1.5567617416381836
Test Epoch23 layer2 Acc 0.8688, AUC 0.9198116064071655, avg_entr 0.014052532613277435, f1 0.8687999844551086
ep23_l2_test_time 2.3908233642578125
Test Epoch23 layer3 Acc 0.8686, AUC 0.9275257587432861, avg_entr 0.013654348440468311, f1 0.8686000108718872
ep23_l3_test_time 3.214282751083374
Test Epoch23 layer4 Acc 0.8686, AUC 0.9310792684555054, avg_entr 0.013255475088953972, f1 0.8686000108718872
ep23_l4_test_time 4.18251633644104
gc 0
Train Epoch24 Acc 0.982475 (39299/40000), AUC 0.9960983395576477
ep24_train_time 114.57435250282288
Test Epoch24 layer0 Acc 0.88, AUC 0.945436954498291, avg_entr 0.11524871736764908, f1 0.8799999952316284
ep24_l0_test_time 0.6564793586730957
Test Epoch24 layer1 Acc 0.8682, AUC 0.919090211391449, avg_entr 0.02090625651180744, f1 0.8682000041007996
ep24_l1_test_time 1.5133435726165771
Test Epoch24 layer2 Acc 0.8676, AUC 0.9192354679107666, avg_entr 0.014388954266905785, f1 0.8675999641418457
ep24_l2_test_time 2.404851198196411
Test Epoch24 layer3 Acc 0.8678, AUC 0.9272208213806152, avg_entr 0.013828454539179802, f1 0.8677999973297119
ep24_l3_test_time 3.2057993412017822
Test Epoch24 layer4 Acc 0.8682, AUC 0.9312505722045898, avg_entr 0.013368252664804459, f1 0.8682000041007996
ep24_l4_test_time 4.200087308883667
gc 0
Train Epoch25 Acc 0.982875 (39315/40000), AUC 0.9963026642799377
ep25_train_time 113.4608461856842
Test Epoch25 layer0 Acc 0.8796, AUC 0.9454748034477234, avg_entr 0.11598224937915802, f1 0.8795999884605408
ep25_l0_test_time 0.6357884407043457
Test Epoch25 layer1 Acc 0.8676, AUC 0.9188328981399536, avg_entr 0.020740419626235962, f1 0.8675999641418457
ep25_l1_test_time 1.5136818885803223
Test Epoch25 layer2 Acc 0.8686, AUC 0.9188000559806824, avg_entr 0.013552287593483925, f1 0.8686000108718872
ep25_l2_test_time 2.4222028255462646
Test Epoch25 layer3 Acc 0.869, AUC 0.9265550971031189, avg_entr 0.012861160561442375, f1 0.8690000176429749
ep25_l3_test_time 3.292621374130249
Test Epoch25 layer4 Acc 0.869, AUC 0.9309399127960205, avg_entr 0.01244885753840208, f1 0.8690000176429749
ep25_l4_test_time 4.149455308914185
gc 0
Train Epoch26 Acc 0.982575 (39303/40000), AUC 0.9959662556648254
ep26_train_time 119.232919216156
Test Epoch26 layer0 Acc 0.8792, AUC 0.9453557729721069, avg_entr 0.11477288603782654, f1 0.8791999816894531
ep26_l0_test_time 0.8097991943359375
Test Epoch26 layer1 Acc 0.8686, AUC 0.9178783297538757, avg_entr 0.020825594663619995, f1 0.8686000108718872
ep26_l1_test_time 1.4205620288848877
Test Epoch26 layer2 Acc 0.8692, AUC 0.916781485080719, avg_entr 0.013335374183952808, f1 0.8691999912261963
ep26_l2_test_time 2.1249306201934814
Test Epoch26 layer3 Acc 0.8688, AUC 0.9253202676773071, avg_entr 0.012753288261592388, f1 0.8687999844551086
ep26_l3_test_time 2.70727801322937
Test Epoch26 layer4 Acc 0.869, AUC 0.9300183653831482, avg_entr 0.012381184846162796, f1 0.8690000176429749
ep26_l4_test_time 3.372206926345825
gc 0
Train Epoch27 Acc 0.9828 (39312/40000), AUC 0.9961625933647156
ep27_train_time 125.79228281974792
Test Epoch27 layer0 Acc 0.8792, AUC 0.9453011751174927, avg_entr 0.11437564343214035, f1 0.8791999816894531
ep27_l0_test_time 0.6048099994659424
Test Epoch27 layer1 Acc 0.8682, AUC 0.9180117845535278, avg_entr 0.02060328982770443, f1 0.8682000041007996
ep27_l1_test_time 1.349975347518921
Test Epoch27 layer2 Acc 0.8692, AUC 0.916864275932312, avg_entr 0.013465718366205692, f1 0.8691999912261963
ep27_l2_test_time 2.089348077774048
Test Epoch27 layer3 Acc 0.8684, AUC 0.9255096912384033, avg_entr 0.013012577779591084, f1 0.868399977684021
ep27_l3_test_time 2.847407341003418
Test Epoch27 layer4 Acc 0.8686, AUC 0.9300419092178345, avg_entr 0.012612555176019669, f1 0.8686000108718872
ep27_l4_test_time 3.2446529865264893
gc 0
Train Epoch28 Acc 0.98295 (39318/40000), AUC 0.9960330724716187
ep28_train_time 118.14382982254028
Test Epoch28 layer0 Acc 0.88, AUC 0.9451892375946045, avg_entr 0.1136031523346901, f1 0.8799999952316284
ep28_l0_test_time 0.6422955989837646
Test Epoch28 layer1 Acc 0.8678, AUC 0.9179090261459351, avg_entr 0.021191272884607315, f1 0.8677999973297119
ep28_l1_test_time 1.536947250366211
Test Epoch28 layer2 Acc 0.8686, AUC 0.9164589047431946, avg_entr 0.013814018107950687, f1 0.8686000108718872
ep28_l2_test_time 2.415635585784912
Test Epoch28 layer3 Acc 0.8684, AUC 0.9248009324073792, avg_entr 0.013224157504737377, f1 0.868399977684021
ep28_l3_test_time 3.2496237754821777
Test Epoch28 layer4 Acc 0.8682, AUC 0.9295533299446106, avg_entr 0.012891052290797234, f1 0.8682000041007996
ep28_l4_test_time 4.176447629928589
gc 0
Train Epoch29 Acc 0.982975 (39319/40000), AUC 0.9961997270584106
ep29_train_time 114.91037559509277
Test Epoch29 layer0 Acc 0.8792, AUC 0.945212185382843, avg_entr 0.11384391784667969, f1 0.8791999816894531
ep29_l0_test_time 0.6575133800506592
Test Epoch29 layer1 Acc 0.8682, AUC 0.9180083870887756, avg_entr 0.020492415875196457, f1 0.8682000041007996
ep29_l1_test_time 1.5053949356079102
Test Epoch29 layer2 Acc 0.8692, AUC 0.916248619556427, avg_entr 0.01338501088321209, f1 0.8691999912261963
ep29_l2_test_time 2.412616729736328
Test Epoch29 layer3 Acc 0.8692, AUC 0.9247231483459473, avg_entr 0.012753927148878574, f1 0.8691999912261963
ep29_l3_test_time 3.235710859298706
Test Epoch29 layer4 Acc 0.869, AUC 0.9296889901161194, avg_entr 0.012352023273706436, f1 0.8690000176429749
ep29_l4_test_time 4.1534833908081055
Best AUC tensor(0.8994) 2 4
train_loss (2, 5, 30)
valid_acc (5, 30)
valid_AUC (5, 30)
train_acc (30,)
total_train+valid_time 3887.6480576992035
Start Testing
Load ckpt at ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt
Test layer0 Acc 0.8762, AUC 0.9517302513122559, avg_entr 0.22787384688854218, f1 0.8762000203132629
l0_test_time 0.6565003395080566
Test layer1 Acc 0.889, AUC 0.9564986228942871, avg_entr 0.16045697033405304, f1 0.8889999985694885
l1_test_time 1.5323185920715332
Test layer2 Acc 0.8922, AUC 0.9568904638290405, avg_entr 0.09816527366638184, f1 0.8921999931335449
l2_test_time 2.4165918827056885
Test layer3 Acc 0.8932, AUC 0.956529974937439, avg_entr 0.06913933157920837, f1 0.8931999802589417
l3_test_time 3.232764959335327
Test layer4 Acc 0.8942, AUC 0.9563716650009155, avg_entr 0.06594671308994293, f1 0.8942000269889832
l4_test_time 4.107933044433594
