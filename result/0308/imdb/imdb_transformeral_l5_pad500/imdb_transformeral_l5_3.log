total count words 222751
vocab size 30000
train size 40000, valid size 5000, test size 5000
found 27937 words in glove
model: TransformerModelML(
  (layers): ModuleList(
    (0): EMBLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Embedding(30000, 300)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=2, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=2, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.weight 9000000
layers.0.ae.g.0.weight 256
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 256
layers.0.ae.h.0.bias 2
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
Total Trainable Params: 13671778
init_time 23.14936137199402
Start Training
gc 0
Train Epoch0 Acc 0.54585 (21834/40000), AUC 0.5566208958625793
ep0_train_time 168.00858974456787
Test Epoch0 layer0 Acc 0.8132, AUC 0.8929932117462158, avg_entr 0.5556262731552124, f1 0.8131999969482422
ep0_l0_test_time 0.7134349346160889
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 0
Test Epoch0 layer1 Acc 0.8304, AUC 0.9107164144515991, avg_entr 0.36381828784942627, f1 0.8304000496864319
ep0_l1_test_time 1.8666503429412842
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 0
Test Epoch0 layer2 Acc 0.8366, AUC 0.9138383865356445, avg_entr 0.4518745541572571, f1 0.8366000056266785
ep0_l2_test_time 3.037510871887207
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 0
Test Epoch0 layer3 Acc 0.8312, AUC 0.9112251996994019, avg_entr 0.5371276140213013, f1 0.8312000632286072
ep0_l3_test_time 4.183213710784912
Test Epoch0 layer4 Acc 0.8102, AUC 0.9136252403259277, avg_entr 0.6743760704994202, f1 0.8101999759674072
ep0_l4_test_time 5.3110339641571045
gc 0
Train Epoch1 Acc 0.862325 (34493/40000), AUC 0.9294276237487793
ep1_train_time 155.7310290336609
Test Epoch1 layer0 Acc 0.8784, AUC 0.9460932612419128, avg_entr 0.289434015750885, f1 0.8784000277519226
ep1_l0_test_time 0.7117743492126465
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 1
Test Epoch1 layer1 Acc 0.8898, AUC 0.9539467692375183, avg_entr 0.18995808064937592, f1 0.8898000121116638
ep1_l1_test_time 1.8767945766448975
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 1
Test Epoch1 layer2 Acc 0.8888, AUC 0.9545227289199829, avg_entr 0.1643148809671402, f1 0.8888000249862671
ep1_l2_test_time 3.045938730239868
Test Epoch1 layer3 Acc 0.8868, AUC 0.953535258769989, avg_entr 0.152247816324234, f1 0.8867999911308289
ep1_l3_test_time 4.225831508636475
Test Epoch1 layer4 Acc 0.8848, AUC 0.9539819955825806, avg_entr 0.1415158063173294, f1 0.8848000168800354
ep1_l4_test_time 5.319497585296631
gc 0
Train Epoch2 Acc 0.912425 (36497/40000), AUC 0.9680061340332031
ep2_train_time 167.35964584350586
Test Epoch2 layer0 Acc 0.882, AUC 0.9548808932304382, avg_entr 0.22189189493656158, f1 0.8820000290870667
ep2_l0_test_time 0.7293655872344971
Test Epoch2 layer1 Acc 0.8968, AUC 0.9587361812591553, avg_entr 0.1412266045808792, f1 0.8967999815940857
ep2_l1_test_time 1.889753818511963
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 2
Test Epoch2 layer2 Acc 0.899, AUC 0.9592269659042358, avg_entr 0.09601730108261108, f1 0.8989999890327454
ep2_l2_test_time 3.0205612182617188
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 2
Test Epoch2 layer3 Acc 0.899, AUC 0.9594993591308594, avg_entr 0.06511495262384415, f1 0.8989999890327454
ep2_l3_test_time 4.217762470245361
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 2
Test Epoch2 layer4 Acc 0.8986, AUC 0.9593725800514221, avg_entr 0.06741365045309067, f1 0.8985999822616577
ep2_l4_test_time 5.306556224822998
gc 0
Train Epoch3 Acc 0.940725 (37629/40000), AUC 0.9815455675125122
ep3_train_time 155.8648281097412
Test Epoch3 layer0 Acc 0.8852, AUC 0.9584355354309082, avg_entr 0.19273801147937775, f1 0.8852000832557678
ep3_l0_test_time 0.7105503082275391
Test Epoch3 layer1 Acc 0.8912, AUC 0.9556750059127808, avg_entr 0.09347528964281082, f1 0.8912000060081482
ep3_l1_test_time 1.8806889057159424
Test Epoch3 layer2 Acc 0.8932, AUC 0.9552838802337646, avg_entr 0.04779360815882683, f1 0.8931999802589417
ep3_l2_test_time 3.0329513549804688
Test Epoch3 layer3 Acc 0.8936, AUC 0.9559949040412903, avg_entr 0.040251072496175766, f1 0.8935999870300293
ep3_l3_test_time 4.178380250930786
Test Epoch3 layer4 Acc 0.8932, AUC 0.9563592076301575, avg_entr 0.04084879159927368, f1 0.8931999802589417
ep3_l4_test_time 5.4011523723602295
gc 0
Train Epoch4 Acc 0.950275 (38011/40000), AUC 0.98558509349823
ep4_train_time 167.16986775398254
Test Epoch4 layer0 Acc 0.8944, AUC 0.9592032432556152, avg_entr 0.17531917989253998, f1 0.8944000005722046
ep4_l0_test_time 0.7151501178741455
Test Epoch4 layer1 Acc 0.8902, AUC 0.9521718621253967, avg_entr 0.06021525338292122, f1 0.8902000188827515
ep4_l1_test_time 1.8536345958709717
Test Epoch4 layer2 Acc 0.891, AUC 0.9529529809951782, avg_entr 0.04011382907629013, f1 0.890999972820282
ep4_l2_test_time 3.0234503746032715
Test Epoch4 layer3 Acc 0.8908, AUC 0.9535635709762573, avg_entr 0.0384017713367939, f1 0.8907999992370605
ep4_l3_test_time 4.214616060256958
Test Epoch4 layer4 Acc 0.8906, AUC 0.9536381959915161, avg_entr 0.03845023363828659, f1 0.8906000256538391
ep4_l4_test_time 3.8452882766723633
gc 0
Train Epoch5 Acc 0.957675 (38307/40000), AUC 0.9874744415283203
ep5_train_time 161.23069310188293
Test Epoch5 layer0 Acc 0.8986, AUC 0.9588724374771118, avg_entr 0.1641460508108139, f1 0.8985999822616577
ep5_l0_test_time 0.7574667930603027
Test Epoch5 layer1 Acc 0.8826, AUC 0.9482738971710205, avg_entr 0.04685382544994354, f1 0.8826000094413757
ep5_l1_test_time 2.0886905193328857
Test Epoch5 layer2 Acc 0.885, AUC 0.9518742561340332, avg_entr 0.03633367270231247, f1 0.8849999904632568
ep5_l2_test_time 3.321601152420044
Test Epoch5 layer3 Acc 0.8838, AUC 0.9522467851638794, avg_entr 0.035687871277332306, f1 0.8838000297546387
ep5_l3_test_time 4.62604022026062
Test Epoch5 layer4 Acc 0.8818, AUC 0.9520114660263062, avg_entr 0.03433476388454437, f1 0.8818000555038452
ep5_l4_test_time 5.963052988052368
gc 0
Train Epoch6 Acc 0.96125 (38450/40000), AUC 0.9885510802268982
ep6_train_time 185.78529858589172
Test Epoch6 layer0 Acc 0.8984, AUC 0.9577454328536987, avg_entr 0.1553555130958557, f1 0.8984000086784363
ep6_l0_test_time 0.7598388195037842
Test Epoch6 layer1 Acc 0.887, AUC 0.9422873854637146, avg_entr 0.036287300288677216, f1 0.8870000243186951
ep6_l1_test_time 2.0865330696105957
Test Epoch6 layer2 Acc 0.8874, AUC 0.949481189250946, avg_entr 0.027565179392695427, f1 0.8873999714851379
ep6_l2_test_time 3.299452304840088
Test Epoch6 layer3 Acc 0.8868, AUC 0.9493551254272461, avg_entr 0.02657439559698105, f1 0.8867999911308289
ep6_l3_test_time 4.63105583190918
Test Epoch6 layer4 Acc 0.887, AUC 0.9499784708023071, avg_entr 0.026065664365887642, f1 0.8870000243186951
ep6_l4_test_time 5.947898626327515
gc 0
Train Epoch7 Acc 0.96705 (38682/40000), AUC 0.9909507036209106
ep7_train_time 185.93317317962646
Test Epoch7 layer0 Acc 0.8886, AUC 0.9558898210525513, avg_entr 0.14376355707645416, f1 0.8885999917984009
ep7_l0_test_time 0.7668533325195312
Test Epoch7 layer1 Acc 0.8854, AUC 0.9409136772155762, avg_entr 0.03499694541096687, f1 0.8853999972343445
ep7_l1_test_time 2.0859124660491943
Test Epoch7 layer2 Acc 0.8852, AUC 0.9476090669631958, avg_entr 0.026558605954051018, f1 0.8852000832557678
ep7_l2_test_time 3.3222784996032715
Test Epoch7 layer3 Acc 0.8868, AUC 0.9480572938919067, avg_entr 0.02610635943710804, f1 0.8867999911308289
ep7_l3_test_time 4.63337516784668
Test Epoch7 layer4 Acc 0.8852, AUC 0.9482295513153076, avg_entr 0.025137770920991898, f1 0.8852000832557678
ep7_l4_test_time 5.995490550994873
gc 0
Train Epoch8 Acc 0.969175 (38767/40000), AUC 0.9922776222229004
ep8_train_time 186.3210847377777
Test Epoch8 layer0 Acc 0.8886, AUC 0.9542744159698486, avg_entr 0.1363396793603897, f1 0.8885999917984009
ep8_l0_test_time 0.7655351161956787
Test Epoch8 layer1 Acc 0.8808, AUC 0.9363452196121216, avg_entr 0.03427501395344734, f1 0.8808000087738037
ep8_l1_test_time 2.0618879795074463
Test Epoch8 layer2 Acc 0.8824, AUC 0.9442209005355835, avg_entr 0.027171270921826363, f1 0.8823999762535095
ep8_l2_test_time 3.3456265926361084
Test Epoch8 layer3 Acc 0.8828, AUC 0.9455361366271973, avg_entr 0.026988059282302856, f1 0.8827999830245972
ep8_l3_test_time 4.64304256439209
Test Epoch8 layer4 Acc 0.8826, AUC 0.9458798766136169, avg_entr 0.026386257261037827, f1 0.8826000094413757
ep8_l4_test_time 5.952329397201538
gc 0
Train Epoch9 Acc 0.97325 (38930/40000), AUC 0.9933180809020996
ep9_train_time 186.2329294681549
Test Epoch9 layer0 Acc 0.8878, AUC 0.9531028866767883, avg_entr 0.13311922550201416, f1 0.8877999782562256
ep9_l0_test_time 0.7586629390716553
Test Epoch9 layer1 Acc 0.8798, AUC 0.9335436820983887, avg_entr 0.031043902039527893, f1 0.879800021648407
ep9_l1_test_time 2.0982229709625244
Test Epoch9 layer2 Acc 0.8798, AUC 0.942827582359314, avg_entr 0.023567356169223785, f1 0.879800021648407
ep9_l2_test_time 3.316121816635132
Test Epoch9 layer3 Acc 0.8798, AUC 0.9439257383346558, avg_entr 0.02288825996220112, f1 0.879800021648407
ep9_l3_test_time 4.642490863800049
Test Epoch9 layer4 Acc 0.8798, AUC 0.9441533088684082, avg_entr 0.02223043330013752, f1 0.879800021648407
ep9_l4_test_time 5.928379774093628
gc 0
Train Epoch10 Acc 0.9748 (38992/40000), AUC 0.9937306046485901
ep10_train_time 186.14660668373108
Test Epoch10 layer0 Acc 0.89, AUC 0.9519075155258179, avg_entr 0.1343788206577301, f1 0.8899999856948853
ep10_l0_test_time 0.7635822296142578
Test Epoch10 layer1 Acc 0.8778, AUC 0.9310280084609985, avg_entr 0.029690375551581383, f1 0.8778000473976135
ep10_l1_test_time 2.0894458293914795
Test Epoch10 layer2 Acc 0.8784, AUC 0.940768837928772, avg_entr 0.022742215543985367, f1 0.8784000277519226
ep10_l2_test_time 3.324491500854492
Test Epoch10 layer3 Acc 0.878, AUC 0.9426490068435669, avg_entr 0.02211366593837738, f1 0.878000020980835
ep10_l3_test_time 4.644946098327637
Test Epoch10 layer4 Acc 0.879, AUC 0.943093478679657, avg_entr 0.021352313458919525, f1 0.8790000081062317
ep10_l4_test_time 5.928865671157837
gc 0
Train Epoch11 Acc 0.975925 (39037/40000), AUC 0.9940178394317627
ep11_train_time 185.8715307712555
Test Epoch11 layer0 Acc 0.8894, AUC 0.9509181976318359, avg_entr 0.13092084228992462, f1 0.8894000053405762
ep11_l0_test_time 0.7699055671691895
Test Epoch11 layer1 Acc 0.875, AUC 0.9285281896591187, avg_entr 0.027347879484295845, f1 0.875
ep11_l1_test_time 2.054560422897339
Test Epoch11 layer2 Acc 0.8754, AUC 0.9365862607955933, avg_entr 0.019836556166410446, f1 0.8754000067710876
ep11_l2_test_time 3.3371853828430176
Test Epoch11 layer3 Acc 0.8748, AUC 0.9407072067260742, avg_entr 0.019368890672922134, f1 0.8748000264167786
ep11_l3_test_time 4.622373342514038
Test Epoch11 layer4 Acc 0.8748, AUC 0.9417862892150879, avg_entr 0.01882815919816494, f1 0.8748000264167786
ep11_l4_test_time 5.9576685428619385
gc 0
Train Epoch12 Acc 0.977125 (39085/40000), AUC 0.9946259260177612
ep12_train_time 186.0602469444275
Test Epoch12 layer0 Acc 0.8888, AUC 0.9495415687561035, avg_entr 0.12767110764980316, f1 0.8888000249862671
ep12_l0_test_time 0.7617454528808594
Test Epoch12 layer1 Acc 0.8746, AUC 0.9274584650993347, avg_entr 0.025730576366186142, f1 0.8745999932289124
ep12_l1_test_time 2.082338809967041
Test Epoch12 layer2 Acc 0.8744, AUC 0.9349939823150635, avg_entr 0.018695052713155746, f1 0.8744000792503357
ep12_l2_test_time 3.3360328674316406
Test Epoch12 layer3 Acc 0.874, AUC 0.9396381378173828, avg_entr 0.018353601917624474, f1 0.8740000128746033
ep12_l3_test_time 4.6263439655303955
Test Epoch12 layer4 Acc 0.8742, AUC 0.9406201839447021, avg_entr 0.018026020377874374, f1 0.8741999864578247
ep12_l4_test_time 5.955916404724121
gc 0
Train Epoch13 Acc 0.979025 (39161/40000), AUC 0.9950029253959656
ep13_train_time 185.93596172332764
Test Epoch13 layer0 Acc 0.8874, AUC 0.9491573572158813, avg_entr 0.12566860020160675, f1 0.8873999714851379
ep13_l0_test_time 0.7871172428131104
Test Epoch13 layer1 Acc 0.8726, AUC 0.92537522315979, avg_entr 0.025017578154802322, f1 0.8726000189781189
ep13_l1_test_time 2.08778715133667
Test Epoch13 layer2 Acc 0.8742, AUC 0.9325780868530273, avg_entr 0.017800932750105858, f1 0.8741999864578247
ep13_l2_test_time 3.315303325653076
Test Epoch13 layer3 Acc 0.874, AUC 0.9388185143470764, avg_entr 0.017193904146552086, f1 0.8740000128746033
ep13_l3_test_time 4.594928979873657
Test Epoch13 layer4 Acc 0.8732, AUC 0.9402517080307007, avg_entr 0.01666831411421299, f1 0.873199999332428
ep13_l4_test_time 5.8998095989227295
gc 0
Train Epoch14 Acc 0.979075 (39163/40000), AUC 0.9951863288879395
ep14_train_time 185.95717668533325
Test Epoch14 layer0 Acc 0.8868, AUC 0.9484987258911133, avg_entr 0.12487288564443588, f1 0.8867999911308289
ep14_l0_test_time 0.7654953002929688
Test Epoch14 layer1 Acc 0.8728, AUC 0.9228591918945312, avg_entr 0.025026917457580566, f1 0.8727999925613403
ep14_l1_test_time 2.0904157161712646
Test Epoch14 layer2 Acc 0.8724, AUC 0.9258410930633545, avg_entr 0.01688976213335991, f1 0.8723999857902527
ep14_l2_test_time 3.3166682720184326
Test Epoch14 layer3 Acc 0.8722, AUC 0.9364588260650635, avg_entr 0.016722463071346283, f1 0.872200071811676
ep14_l3_test_time 4.641199111938477
Test Epoch14 layer4 Acc 0.8724, AUC 0.9384230375289917, avg_entr 0.016225671395659447, f1 0.8723999857902527
ep14_l4_test_time 5.650188446044922
gc 0
Train Epoch15 Acc 0.9798 (39192/40000), AUC 0.9952784776687622
ep15_train_time 186.475665807724
Test Epoch15 layer0 Acc 0.8848, AUC 0.9479836225509644, avg_entr 0.12370210140943527, f1 0.8848000168800354
ep15_l0_test_time 0.7610695362091064
Test Epoch15 layer1 Acc 0.8708, AUC 0.922810435295105, avg_entr 0.02404884435236454, f1 0.8708000183105469
ep15_l1_test_time 2.0855276584625244
Test Epoch15 layer2 Acc 0.8716, AUC 0.9291225671768188, avg_entr 0.01668396219611168, f1 0.8715999722480774
ep15_l2_test_time 3.310253381729126
Test Epoch15 layer3 Acc 0.8716, AUC 0.9365947246551514, avg_entr 0.01610509119927883, f1 0.8715999722480774
ep15_l3_test_time 4.34534215927124
Test Epoch15 layer4 Acc 0.8712, AUC 0.9383915662765503, avg_entr 0.0156486164778471, f1 0.8712000846862793
ep15_l4_test_time 5.985125780105591
gc 0
Train Epoch16 Acc 0.980525 (39221/40000), AUC 0.9956280589103699
ep16_train_time 186.3756878376007
Test Epoch16 layer0 Acc 0.8822, AUC 0.9472305774688721, avg_entr 0.12408376485109329, f1 0.8822000026702881
ep16_l0_test_time 0.7597806453704834
Test Epoch16 layer1 Acc 0.87, AUC 0.9213688373565674, avg_entr 0.024666691198945045, f1 0.8700000047683716
ep16_l1_test_time 2.0501067638397217
Test Epoch16 layer2 Acc 0.8698, AUC 0.9281010031700134, avg_entr 0.017557794228196144, f1 0.8697999715805054
ep16_l2_test_time 3.0884132385253906
Test Epoch16 layer3 Acc 0.87, AUC 0.9357089996337891, avg_entr 0.01706676557660103, f1 0.8700000047683716
ep16_l3_test_time 4.610121965408325
Test Epoch16 layer4 Acc 0.8696, AUC 0.9368996620178223, avg_entr 0.016655663028359413, f1 0.8695999383926392
ep16_l4_test_time 5.96155047416687
gc 0
Train Epoch17 Acc 0.98125 (39250/40000), AUC 0.9956222772598267
ep17_train_time 186.10698223114014
Test Epoch17 layer0 Acc 0.8836, AUC 0.9468820691108704, avg_entr 0.12032262235879898, f1 0.8835999965667725
ep17_l0_test_time 0.7352194786071777
Test Epoch17 layer1 Acc 0.8718, AUC 0.921193540096283, avg_entr 0.02319510467350483, f1 0.8718000650405884
ep17_l1_test_time 1.9958138465881348
Test Epoch17 layer2 Acc 0.8716, AUC 0.9268185496330261, avg_entr 0.016351619735360146, f1 0.8715999722480774
ep17_l2_test_time 3.3408238887786865
Test Epoch17 layer3 Acc 0.872, AUC 0.9351165294647217, avg_entr 0.01548810862004757, f1 0.871999979019165
ep17_l3_test_time 4.621721029281616
Test Epoch17 layer4 Acc 0.8712, AUC 0.9366710186004639, avg_entr 0.01487007737159729, f1 0.8712000846862793
ep17_l4_test_time 5.933974266052246
gc 0
Train Epoch18 Acc 0.9811 (39244/40000), AUC 0.9953666925430298
ep18_train_time 186.15787720680237
Test Epoch18 layer0 Acc 0.8848, AUC 0.9466678500175476, avg_entr 0.11983763426542282, f1 0.8848000168800354
ep18_l0_test_time 0.7724664211273193
Test Epoch18 layer1 Acc 0.8712, AUC 0.9213208556175232, avg_entr 0.023607173934578896, f1 0.8712000846862793
ep18_l1_test_time 2.0895602703094482
Test Epoch18 layer2 Acc 0.8718, AUC 0.927081823348999, avg_entr 0.016488904133439064, f1 0.8718000650405884
ep18_l2_test_time 3.3265817165374756
Test Epoch18 layer3 Acc 0.8714, AUC 0.9346645474433899, avg_entr 0.0156916081905365, f1 0.871399998664856
ep18_l3_test_time 4.621890544891357
Test Epoch18 layer4 Acc 0.8716, AUC 0.9365160465240479, avg_entr 0.015277688391506672, f1 0.8715999722480774
ep18_l4_test_time 5.983314752578735
gc 0
Train Epoch19 Acc 0.98175 (39270/40000), AUC 0.9958114624023438
ep19_train_time 186.0935559272766
Test Epoch19 layer0 Acc 0.8818, AUC 0.9463651180267334, avg_entr 0.11998067796230316, f1 0.8818000555038452
ep19_l0_test_time 0.7670300006866455
Test Epoch19 layer1 Acc 0.87, AUC 0.9197264909744263, avg_entr 0.023228485137224197, f1 0.8700000047683716
ep19_l1_test_time 2.0892443656921387
Test Epoch19 layer2 Acc 0.8702, AUC 0.9236317873001099, avg_entr 0.016167622059583664, f1 0.870199978351593
ep19_l2_test_time 3.3173727989196777
Test Epoch19 layer3 Acc 0.8698, AUC 0.9337629079818726, avg_entr 0.015266821719706059, f1 0.8697999715805054
ep19_l3_test_time 4.643416404724121
Test Epoch19 layer4 Acc 0.8698, AUC 0.9357547760009766, avg_entr 0.014828392304480076, f1 0.8697999715805054
ep19_l4_test_time 5.979373931884766
gc 0
Train Epoch20 Acc 0.9817 (39268/40000), AUC 0.9959986209869385
ep20_train_time 186.08921909332275
Test Epoch20 layer0 Acc 0.8832, AUC 0.9459840059280396, avg_entr 0.11776459962129593, f1 0.8831999897956848
ep20_l0_test_time 0.7698400020599365
Test Epoch20 layer1 Acc 0.8684, AUC 0.9184783101081848, avg_entr 0.023431207984685898, f1 0.868399977684021
ep20_l1_test_time 2.070722818374634
Test Epoch20 layer2 Acc 0.8682, AUC 0.9222003817558289, avg_entr 0.016204627230763435, f1 0.8682000041007996
ep20_l2_test_time 3.3309690952301025
Test Epoch20 layer3 Acc 0.8684, AUC 0.932942271232605, avg_entr 0.015475461259484291, f1 0.868399977684021
ep20_l3_test_time 4.639069318771362
Test Epoch20 layer4 Acc 0.8684, AUC 0.9345976114273071, avg_entr 0.015042242594063282, f1 0.868399977684021
ep20_l4_test_time 5.9217002391815186
gc 0
Train Epoch21 Acc 0.9819 (39276/40000), AUC 0.9960888624191284
ep21_train_time 166.43092727661133
Test Epoch21 layer0 Acc 0.8818, AUC 0.9459107518196106, avg_entr 0.11884116381406784, f1 0.8818000555038452
ep21_l0_test_time 0.7689492702484131
Test Epoch21 layer1 Acc 0.869, AUC 0.9188635349273682, avg_entr 0.023109829053282738, f1 0.8690000176429749
ep21_l1_test_time 2.0846147537231445
Test Epoch21 layer2 Acc 0.8696, AUC 0.9224605560302734, avg_entr 0.016068067401647568, f1 0.8695999383926392
ep21_l2_test_time 3.3142971992492676
Test Epoch21 layer3 Acc 0.8692, AUC 0.9326497912406921, avg_entr 0.015283091925084591, f1 0.8691999912261963
ep21_l3_test_time 4.623793840408325
Test Epoch21 layer4 Acc 0.8696, AUC 0.9348642826080322, avg_entr 0.014798052608966827, f1 0.8695999383926392
ep21_l4_test_time 5.947343587875366
gc 0
Train Epoch22 Acc 0.982475 (39299/40000), AUC 0.9960622787475586
ep22_train_time 185.94848322868347
Test Epoch22 layer0 Acc 0.8826, AUC 0.945733904838562, avg_entr 0.11690528690814972, f1 0.8826000094413757
ep22_l0_test_time 0.7633063793182373
Test Epoch22 layer1 Acc 0.8684, AUC 0.9173990488052368, avg_entr 0.02272554486989975, f1 0.868399977684021
ep22_l1_test_time 2.0776853561401367
Test Epoch22 layer2 Acc 0.8678, AUC 0.9204280376434326, avg_entr 0.01587320864200592, f1 0.8677999973297119
ep22_l2_test_time 3.308986186981201
Test Epoch22 layer3 Acc 0.8682, AUC 0.9318376779556274, avg_entr 0.015077749267220497, f1 0.8682000041007996
ep22_l3_test_time 4.649181842803955
Test Epoch22 layer4 Acc 0.8678, AUC 0.9341930150985718, avg_entr 0.014595662243664265, f1 0.8677999973297119
ep22_l4_test_time 5.6166298389434814
gc 0
Train Epoch23 Acc 0.98235 (39294/40000), AUC 0.9961586594581604
ep23_train_time 186.1296901702881
Test Epoch23 layer0 Acc 0.8826, AUC 0.9456008672714233, avg_entr 0.11616256088018417, f1 0.8826000094413757
ep23_l0_test_time 0.7570619583129883
Test Epoch23 layer1 Acc 0.8684, AUC 0.9169768691062927, avg_entr 0.022005053237080574, f1 0.868399977684021
ep23_l1_test_time 2.07989501953125
Test Epoch23 layer2 Acc 0.8682, AUC 0.9224836826324463, avg_entr 0.015459106303751469, f1 0.8682000041007996
ep23_l2_test_time 3.3280930519104004
Test Epoch23 layer3 Acc 0.869, AUC 0.9322746992111206, avg_entr 0.014856066554784775, f1 0.8690000176429749
ep23_l3_test_time 4.363183975219727
Test Epoch23 layer4 Acc 0.8684, AUC 0.9337189197540283, avg_entr 0.01430067140609026, f1 0.868399977684021
ep23_l4_test_time 5.944953918457031
gc 0
Train Epoch24 Acc 0.98265 (39306/40000), AUC 0.9960507154464722
ep24_train_time 186.2264301776886
Test Epoch24 layer0 Acc 0.8832, AUC 0.9455013275146484, avg_entr 0.11558335274457932, f1 0.8831999897956848
ep24_l0_test_time 0.7572813034057617
Test Epoch24 layer1 Acc 0.869, AUC 0.917426347732544, avg_entr 0.022656649351119995, f1 0.8690000176429749
ep24_l1_test_time 2.0172035694122314
Test Epoch24 layer2 Acc 0.8706, AUC 0.9175124764442444, avg_entr 0.015342523343861103, f1 0.8705999851226807
ep24_l2_test_time 3.105329751968384
Test Epoch24 layer3 Acc 0.8698, AUC 0.9307444095611572, avg_entr 0.0145355723798275, f1 0.8697999715805054
ep24_l3_test_time 4.610619306564331
Test Epoch24 layer4 Acc 0.87, AUC 0.9338436126708984, avg_entr 0.014150184579193592, f1 0.8700000047683716
ep24_l4_test_time 5.991943597793579
gc 0
Train Epoch25 Acc 0.98295 (39318/40000), AUC 0.9961652755737305
ep25_train_time 186.06066966056824
Test Epoch25 layer0 Acc 0.883, AUC 0.945412278175354, avg_entr 0.1151290163397789, f1 0.8830000162124634
ep25_l0_test_time 0.725395917892456
Test Epoch25 layer1 Acc 0.8678, AUC 0.9168983697891235, avg_entr 0.022399410605430603, f1 0.8677999973297119
ep25_l1_test_time 1.9699177742004395
Test Epoch25 layer2 Acc 0.8692, AUC 0.918376624584198, avg_entr 0.015531676821410656, f1 0.8691999912261963
ep25_l2_test_time 3.2874529361724854
Test Epoch25 layer3 Acc 0.8696, AUC 0.9304736852645874, avg_entr 0.01474336814135313, f1 0.8695999383926392
ep25_l3_test_time 4.646831274032593
Test Epoch25 layer4 Acc 0.8694, AUC 0.9330889582633972, avg_entr 0.014235653914511204, f1 0.8694000244140625
ep25_l4_test_time 5.978286027908325
gc 0
Train Epoch26 Acc 0.982825 (39313/40000), AUC 0.9960939884185791
ep26_train_time 185.84533381462097
Test Epoch26 layer0 Acc 0.8828, AUC 0.9453341364860535, avg_entr 0.11436131596565247, f1 0.8827999830245972
ep26_l0_test_time 0.7750296592712402
Test Epoch26 layer1 Acc 0.868, AUC 0.9163788557052612, avg_entr 0.02172931283712387, f1 0.8679999709129333
ep26_l1_test_time 2.0821022987365723
Test Epoch26 layer2 Acc 0.8672, AUC 0.9189759492874146, avg_entr 0.015229585580527782, f1 0.8672000169754028
ep26_l2_test_time 3.3086752891540527
Test Epoch26 layer3 Acc 0.8672, AUC 0.930359959602356, avg_entr 0.014552004635334015, f1 0.8672000169754028
ep26_l3_test_time 4.630614757537842
Test Epoch26 layer4 Acc 0.867, AUC 0.9325934648513794, avg_entr 0.014040826819837093, f1 0.8669999837875366
ep26_l4_test_time 5.984302520751953
gc 0
Train Epoch27 Acc 0.982675 (39307/40000), AUC 0.9960887432098389
ep27_train_time 185.70641708374023
Test Epoch27 layer0 Acc 0.8818, AUC 0.9452738761901855, avg_entr 0.11476070433855057, f1 0.8818000555038452
ep27_l0_test_time 0.7530219554901123
Test Epoch27 layer1 Acc 0.8688, AUC 0.916535496711731, avg_entr 0.0223520677536726, f1 0.8687999844551086
ep27_l1_test_time 2.085986852645874
Test Epoch27 layer2 Acc 0.8696, AUC 0.9174115657806396, avg_entr 0.015612296760082245, f1 0.8695999383926392
ep27_l2_test_time 3.315032720565796
Test Epoch27 layer3 Acc 0.8706, AUC 0.9297517538070679, avg_entr 0.014833183027803898, f1 0.8705999851226807
ep27_l3_test_time 4.628024339675903
Test Epoch27 layer4 Acc 0.87, AUC 0.9327530860900879, avg_entr 0.014360427856445312, f1 0.8700000047683716
ep27_l4_test_time 5.982106685638428
gc 0
Train Epoch28 Acc 0.983 (39320/40000), AUC 0.9963279962539673
ep28_train_time 185.89023447036743
Test Epoch28 layer0 Acc 0.8828, AUC 0.9452049732208252, avg_entr 0.11321265995502472, f1 0.8827999830245972
ep28_l0_test_time 0.7691860198974609
Test Epoch28 layer1 Acc 0.8672, AUC 0.9158872365951538, avg_entr 0.021777519956231117, f1 0.8672000169754028
ep28_l1_test_time 2.0567944049835205
Test Epoch28 layer2 Acc 0.8664, AUC 0.9180659651756287, avg_entr 0.015057489275932312, f1 0.8664000034332275
ep28_l2_test_time 3.3525173664093018
Test Epoch28 layer3 Acc 0.8672, AUC 0.9298336505889893, avg_entr 0.014321006834506989, f1 0.8672000169754028
ep28_l3_test_time 4.627298593521118
Test Epoch28 layer4 Acc 0.867, AUC 0.9321860671043396, avg_entr 0.013794517144560814, f1 0.8669999837875366
ep28_l4_test_time 5.953891277313232
gc 0
Train Epoch29 Acc 0.982925 (39317/40000), AUC 0.9962326288223267
ep29_train_time 186.02991843223572
Test Epoch29 layer0 Acc 0.8824, AUC 0.9451506733894348, avg_entr 0.11332785338163376, f1 0.8823999762535095
ep29_l0_test_time 0.7576627731323242
Test Epoch29 layer1 Acc 0.8678, AUC 0.9160491228103638, avg_entr 0.021986527368426323, f1 0.8677999973297119
ep29_l1_test_time 2.096047878265381
Test Epoch29 layer2 Acc 0.869, AUC 0.9170783162117004, avg_entr 0.015281285159289837, f1 0.8690000176429749
ep29_l2_test_time 3.3097190856933594
Test Epoch29 layer3 Acc 0.8686, AUC 0.9293668270111084, avg_entr 0.014477969147264957, f1 0.8686000108718872
ep29_l3_test_time 4.644214630126953
Test Epoch29 layer4 Acc 0.8684, AUC 0.9321987628936768, avg_entr 0.013968558050692081, f1 0.868399977684021
ep29_l4_test_time 5.914169549942017
Best AUC tensor(0.8990) 2 3
train_loss (2, 5, 30)
valid_acc (5, 30)
valid_AUC (5, 30)
train_acc (30,)
total_train+valid_time 5914.400750160217
Start Testing
Load ckpt at ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt
Test layer0 Acc 0.8792, AUC 0.9528779983520508, avg_entr 0.21885976195335388, f1 0.8791999816894531
l0_test_time 0.7756922245025635
Test layer1 Acc 0.8936, AUC 0.9584781527519226, avg_entr 0.14417648315429688, f1 0.8935999870300293
l1_test_time 2.0629827976226807
Test layer2 Acc 0.897, AUC 0.958885133266449, avg_entr 0.09908945858478546, f1 0.8970000147819519
l2_test_time 3.3236756324768066
Test layer3 Acc 0.8972, AUC 0.9591260552406311, avg_entr 0.06673604995012283, f1 0.8971999883651733
l3_test_time 4.639891624450684
Test layer4 Acc 0.8972, AUC 0.9590002298355103, avg_entr 0.06923343241214752, f1 0.8971999883651733
l4_test_time 5.968439340591431
