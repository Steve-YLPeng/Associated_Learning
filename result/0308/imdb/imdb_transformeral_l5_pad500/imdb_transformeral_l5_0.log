total count words 222751
vocab size 30000
train size 40000, valid size 5000, test size 5000
found 27937 words in glove
model: TransformerModelML(
  (layers): ModuleList(
    (0): EMBLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Embedding(30000, 300)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=2, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=2, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.weight 9000000
layers.0.ae.g.0.weight 256
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 256
layers.0.ae.h.0.bias 2
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
Total Trainable Params: 13671778
init_time 22.266507863998413
Start Training
gc 0
Train Epoch0 Acc 0.460775 (18431/40000), AUC 0.45712992548942566
ep0_train_time 183.0260350704193
Test Epoch0 layer0 Acc 0.8108, AUC 0.883790910243988, avg_entr 0.5803326368331909, f1 0.8108000159263611
ep0_l0_test_time 0.779315710067749
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 0
Test Epoch0 layer1 Acc 0.8138, AUC 0.894606351852417, avg_entr 0.3730073571205139, f1 0.8137999773025513
ep0_l1_test_time 2.0623321533203125
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 0
Test Epoch0 layer2 Acc 0.8148, AUC 0.8979592323303223, avg_entr 0.4430893659591675, f1 0.8148000240325928
ep0_l2_test_time 3.264383316040039
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 0
Test Epoch0 layer3 Acc 0.8032, AUC 0.8953805565834045, avg_entr 0.6178276538848877, f1 0.8032000660896301
ep0_l3_test_time 4.522695541381836
Test Epoch0 layer4 Acc 0.661, AUC 0.8620021343231201, avg_entr 0.6873273849487305, f1 0.6610000133514404
ep0_l4_test_time 5.864643335342407
gc 0
Train Epoch1 Acc 0.85345 (34138/40000), AUC 0.9237744808197021
ep1_train_time 182.96956133842468
Test Epoch1 layer0 Acc 0.881, AUC 0.9443897008895874, avg_entr 0.2933632731437683, f1 0.8809999823570251
ep1_l0_test_time 0.7689633369445801
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 1
Test Epoch1 layer1 Acc 0.886, AUC 0.9526137113571167, avg_entr 0.19872982800006866, f1 0.8859999775886536
ep1_l1_test_time 2.020660638809204
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 1
Test Epoch1 layer2 Acc 0.8908, AUC 0.9543485045433044, avg_entr 0.17673476040363312, f1 0.8907999992370605
ep1_l2_test_time 3.2600810527801514
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 1
Test Epoch1 layer3 Acc 0.8882, AUC 0.9540663957595825, avg_entr 0.14624285697937012, f1 0.888200044631958
ep1_l3_test_time 4.7467052936553955
Test Epoch1 layer4 Acc 0.8886, AUC 0.9537216424942017, avg_entr 0.14132557809352875, f1 0.8885999917984009
ep1_l4_test_time 5.746856451034546
gc 0
Train Epoch2 Acc 0.9127 (36508/40000), AUC 0.9681525826454163
ep2_train_time 165.31225156784058
Test Epoch2 layer0 Acc 0.893, AUC 0.954470694065094, avg_entr 0.2301097959280014, f1 0.8930000066757202
ep2_l0_test_time 0.7508227825164795
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 2
Test Epoch2 layer1 Acc 0.8962, AUC 0.9578049182891846, avg_entr 0.15456494688987732, f1 0.8962000012397766
ep2_l1_test_time 2.059372663497925
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 2
Test Epoch2 layer2 Acc 0.8972, AUC 0.9584490060806274, avg_entr 0.09219875186681747, f1 0.8971999883651733
ep2_l2_test_time 3.2806782722473145
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 2
Test Epoch2 layer3 Acc 0.8966, AUC 0.958497166633606, avg_entr 0.06207425519824028, f1 0.8966000080108643
ep2_l3_test_time 4.5273354053497314
Test Epoch2 layer4 Acc 0.8968, AUC 0.9589342474937439, avg_entr 0.05938481539487839, f1 0.8967999815940857
ep2_l4_test_time 5.838898420333862
gc 0
Train Epoch3 Acc 0.937625 (37505/40000), AUC 0.979052722454071
ep3_train_time 182.52541828155518
Test Epoch3 layer0 Acc 0.8828, AUC 0.9574845433235168, avg_entr 0.20563222467899323, f1 0.8827999830245972
ep3_l0_test_time 0.766273021697998
Test Epoch3 layer1 Acc 0.8842, AUC 0.9538838863372803, avg_entr 0.10886803269386292, f1 0.8842000365257263
ep3_l1_test_time 2.0322701930999756
Test Epoch3 layer2 Acc 0.8842, AUC 0.9506237506866455, avg_entr 0.04972397908568382, f1 0.8842000365257263
ep3_l2_test_time 3.2878215312957764
Test Epoch3 layer3 Acc 0.8834, AUC 0.9538449645042419, avg_entr 0.042539093643426895, f1 0.8834000825881958
ep3_l3_test_time 4.667439222335815
Test Epoch3 layer4 Acc 0.8834, AUC 0.9543818831443787, avg_entr 0.0419297032058239, f1 0.8834000825881958
ep3_l4_test_time 5.806499004364014
gc 0
Train Epoch4 Acc 0.9496 (37984/40000), AUC 0.9849658012390137
ep4_train_time 182.84720468521118
Test Epoch4 layer0 Acc 0.9004, AUC 0.9585427045822144, avg_entr 0.17918959259986877, f1 0.9003999829292297
ep4_l0_test_time 0.7592141628265381
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 4
Test Epoch4 layer1 Acc 0.8858, AUC 0.9509182572364807, avg_entr 0.05768610164523125, f1 0.8858000040054321
ep4_l1_test_time 2.042487382888794
Test Epoch4 layer2 Acc 0.8882, AUC 0.9526678323745728, avg_entr 0.03698605298995972, f1 0.888200044631958
ep4_l2_test_time 3.284273386001587
Test Epoch4 layer3 Acc 0.888, AUC 0.9520561695098877, avg_entr 0.03688860312104225, f1 0.8880000114440918
ep4_l3_test_time 4.52385950088501
Test Epoch4 layer4 Acc 0.8878, AUC 0.9527205228805542, avg_entr 0.03668573126196861, f1 0.8877999782562256
ep4_l4_test_time 5.873953819274902
gc 0
Train Epoch5 Acc 0.95705 (38282/40000), AUC 0.9877781867980957
ep5_train_time 183.05432987213135
Test Epoch5 layer0 Acc 0.8956, AUC 0.9575880169868469, avg_entr 0.16455243527889252, f1 0.8956000208854675
ep5_l0_test_time 0.7715740203857422
Test Epoch5 layer1 Acc 0.8834, AUC 0.9470939040184021, avg_entr 0.045858874917030334, f1 0.8834000825881958
ep5_l1_test_time 2.078911304473877
Test Epoch5 layer2 Acc 0.8844, AUC 0.950823187828064, avg_entr 0.03341617435216904, f1 0.8844000101089478
ep5_l2_test_time 3.259636402130127
Test Epoch5 layer3 Acc 0.8838, AUC 0.9502648711204529, avg_entr 0.03364355489611626, f1 0.8838000297546387
ep5_l3_test_time 4.635841369628906
Test Epoch5 layer4 Acc 0.884, AUC 0.9508095979690552, avg_entr 0.032859306782484055, f1 0.8840000033378601
ep5_l4_test_time 5.896378040313721
gc 0
Train Epoch6 Acc 0.9618 (38472/40000), AUC 0.9886059761047363
ep6_train_time 182.4165198802948
Test Epoch6 layer0 Acc 0.8852, AUC 0.9566794633865356, avg_entr 0.15444940328598022, f1 0.8852000832557678
ep6_l0_test_time 0.771775484085083
Test Epoch6 layer1 Acc 0.8736, AUC 0.9425697326660156, avg_entr 0.04164241999387741, f1 0.8736000061035156
ep6_l1_test_time 2.0180160999298096
Test Epoch6 layer2 Acc 0.8726, AUC 0.948920726776123, avg_entr 0.031878672540187836, f1 0.8726000189781189
ep6_l2_test_time 3.2711305618286133
Test Epoch6 layer3 Acc 0.8728, AUC 0.9482518434524536, avg_entr 0.03190401569008827, f1 0.8727999925613403
ep6_l3_test_time 4.577500343322754
Test Epoch6 layer4 Acc 0.8704, AUC 0.9488124847412109, avg_entr 0.03143405541777611, f1 0.8704000115394592
ep6_l4_test_time 5.875365972518921
gc 0
Train Epoch7 Acc 0.966375 (38655/40000), AUC 0.9913475513458252
ep7_train_time 183.18180775642395
Test Epoch7 layer0 Acc 0.8848, AUC 0.9551559686660767, avg_entr 0.14443913102149963, f1 0.8848000168800354
ep7_l0_test_time 0.7479856014251709
Test Epoch7 layer1 Acc 0.8808, AUC 0.9403859972953796, avg_entr 0.03550361469388008, f1 0.8808000087738037
ep7_l1_test_time 2.021472454071045
Test Epoch7 layer2 Acc 0.8808, AUC 0.947425365447998, avg_entr 0.02548195794224739, f1 0.8808000087738037
ep7_l2_test_time 3.2785701751708984
Test Epoch7 layer3 Acc 0.881, AUC 0.9472173452377319, avg_entr 0.02473839372396469, f1 0.8809999823570251
ep7_l3_test_time 4.562185287475586
Test Epoch7 layer4 Acc 0.881, AUC 0.9476510286331177, avg_entr 0.02389531396329403, f1 0.8809999823570251
ep7_l4_test_time 5.859412431716919
gc 0
Train Epoch8 Acc 0.969725 (38789/40000), AUC 0.9922924041748047
ep8_train_time 183.10622644424438
Test Epoch8 layer0 Acc 0.8888, AUC 0.9539370536804199, avg_entr 0.1425710767507553, f1 0.8888000249862671
ep8_l0_test_time 0.7643091678619385
Test Epoch8 layer1 Acc 0.8792, AUC 0.9368020296096802, avg_entr 0.033140696585178375, f1 0.8791999816894531
ep8_l1_test_time 2.021780014038086
Test Epoch8 layer2 Acc 0.8804, AUC 0.9455875158309937, avg_entr 0.023532584309577942, f1 0.8804000020027161
ep8_l2_test_time 3.2680752277374268
Test Epoch8 layer3 Acc 0.8802, AUC 0.9458810687065125, avg_entr 0.02283857762813568, f1 0.8802000284194946
ep8_l3_test_time 4.495966672897339
Test Epoch8 layer4 Acc 0.8802, AUC 0.9462733268737793, avg_entr 0.022086840122938156, f1 0.8802000284194946
ep8_l4_test_time 5.865346193313599
gc 0
Train Epoch9 Acc 0.972375 (38895/40000), AUC 0.9926121234893799
ep9_train_time 183.1701056957245
Test Epoch9 layer0 Acc 0.8892, AUC 0.952346682548523, avg_entr 0.13811638951301575, f1 0.88919997215271
ep9_l0_test_time 0.7822604179382324
Test Epoch9 layer1 Acc 0.8806, AUC 0.9335058927536011, avg_entr 0.03249707445502281, f1 0.8805999755859375
ep9_l1_test_time 2.0418624877929688
Test Epoch9 layer2 Acc 0.8804, AUC 0.9416378736495972, avg_entr 0.02248665690422058, f1 0.8804000020027161
ep9_l2_test_time 3.2691140174865723
Test Epoch9 layer3 Acc 0.8802, AUC 0.943529486656189, avg_entr 0.021720536053180695, f1 0.8802000284194946
ep9_l3_test_time 4.527249336242676
Test Epoch9 layer4 Acc 0.8802, AUC 0.9440656304359436, avg_entr 0.02114274352788925, f1 0.8802000284194946
ep9_l4_test_time 5.86211371421814
gc 0
Train Epoch10 Acc 0.97385 (38954/40000), AUC 0.9937812089920044
ep10_train_time 158.6037392616272
Test Epoch10 layer0 Acc 0.8884, AUC 0.951363742351532, avg_entr 0.1331622302532196, f1 0.8884000182151794
ep10_l0_test_time 0.7216644287109375
Test Epoch10 layer1 Acc 0.8744, AUC 0.9293190240859985, avg_entr 0.030201828107237816, f1 0.8744000792503357
ep10_l1_test_time 1.993607997894287
Test Epoch10 layer2 Acc 0.8754, AUC 0.93793123960495, avg_entr 0.02024732157588005, f1 0.8754000067710876
ep10_l2_test_time 3.131601572036743
Test Epoch10 layer3 Acc 0.8754, AUC 0.9418455362319946, avg_entr 0.01901484653353691, f1 0.8754000067710876
ep10_l3_test_time 4.381488561630249
Test Epoch10 layer4 Acc 0.8754, AUC 0.9425249695777893, avg_entr 0.01856495812535286, f1 0.8754000067710876
ep10_l4_test_time 5.54412055015564
gc 0
Train Epoch11 Acc 0.9755 (39020/40000), AUC 0.9944184422492981
ep11_train_time 162.42448592185974
Test Epoch11 layer0 Acc 0.8872, AUC 0.9503309726715088, avg_entr 0.13100256025791168, f1 0.8871999979019165
ep11_l0_test_time 0.6013913154602051
Test Epoch11 layer1 Acc 0.8772, AUC 0.9282128810882568, avg_entr 0.029995333403348923, f1 0.8772000074386597
ep11_l1_test_time 1.2487475872039795
Test Epoch11 layer2 Acc 0.8778, AUC 0.9370228052139282, avg_entr 0.02077334187924862, f1 0.8778000473976135
ep11_l2_test_time 3.0074985027313232
Test Epoch11 layer3 Acc 0.8772, AUC 0.9413831233978271, avg_entr 0.01975243166089058, f1 0.8772000074386597
ep11_l3_test_time 4.37080192565918
Test Epoch11 layer4 Acc 0.877, AUC 0.9419955015182495, avg_entr 0.019198395311832428, f1 0.8769999742507935
ep11_l4_test_time 5.5545454025268555
gc 0
Train Epoch12 Acc 0.976125 (39045/40000), AUC 0.9944928884506226
ep12_train_time 173.44408416748047
Test Epoch12 layer0 Acc 0.8858, AUC 0.9494545459747314, avg_entr 0.13142667710781097, f1 0.8858000040054321
ep12_l0_test_time 0.7323036193847656
Test Epoch12 layer1 Acc 0.8726, AUC 0.925828218460083, avg_entr 0.02859957329928875, f1 0.8726000189781189
ep12_l1_test_time 1.9826076030731201
Test Epoch12 layer2 Acc 0.8744, AUC 0.935024619102478, avg_entr 0.02004137821495533, f1 0.8744000792503357
ep12_l2_test_time 3.0959038734436035
Test Epoch12 layer3 Acc 0.874, AUC 0.940032958984375, avg_entr 0.019026577472686768, f1 0.8740000128746033
ep12_l3_test_time 4.34970498085022
Test Epoch12 layer4 Acc 0.8742, AUC 0.9408375024795532, avg_entr 0.018526026979088783, f1 0.8741999864578247
ep12_l4_test_time 5.587806463241577
gc 0
Train Epoch13 Acc 0.977975 (39119/40000), AUC 0.9950807094573975
ep13_train_time 210.53272795677185
Test Epoch13 layer0 Acc 0.885, AUC 0.9487179517745972, avg_entr 0.12759125232696533, f1 0.8849999904632568
ep13_l0_test_time 0.9380197525024414
Test Epoch13 layer1 Acc 0.8702, AUC 0.9224007725715637, avg_entr 0.027500569820404053, f1 0.870199978351593
ep13_l1_test_time 2.6393494606018066
Test Epoch13 layer2 Acc 0.8718, AUC 0.9316771030426025, avg_entr 0.01899840123951435, f1 0.8718000650405884
ep13_l2_test_time 4.182196617126465
Test Epoch13 layer3 Acc 0.8718, AUC 0.937985360622406, avg_entr 0.017826950177550316, f1 0.8718000650405884
ep13_l3_test_time 6.0375590324401855
Test Epoch13 layer4 Acc 0.872, AUC 0.9390882253646851, avg_entr 0.017284872010350227, f1 0.871999979019165
ep13_l4_test_time 7.5625
gc 0
Train Epoch14 Acc 0.97825 (39130/40000), AUC 0.9949359893798828
ep14_train_time 211.0285131931305
Test Epoch14 layer0 Acc 0.8848, AUC 0.948095440864563, avg_entr 0.12676571309566498, f1 0.8848000168800354
ep14_l0_test_time 0.728740930557251
Test Epoch14 layer1 Acc 0.8724, AUC 0.9214544296264648, avg_entr 0.02680864930152893, f1 0.8723999857902527
ep14_l1_test_time 1.9919018745422363
Test Epoch14 layer2 Acc 0.8736, AUC 0.9310104250907898, avg_entr 0.018514372408390045, f1 0.8736000061035156
ep14_l2_test_time 3.158684730529785
Test Epoch14 layer3 Acc 0.8734, AUC 0.9384258389472961, avg_entr 0.017388638108968735, f1 0.8733999729156494
ep14_l3_test_time 4.325379371643066
Test Epoch14 layer4 Acc 0.8738, AUC 0.9396111965179443, avg_entr 0.016936538740992546, f1 0.8737999796867371
ep14_l4_test_time 5.588888883590698
gc 0
Train Epoch15 Acc 0.9792 (39168/40000), AUC 0.9953389167785645
ep15_train_time 171.46153378486633
Test Epoch15 layer0 Acc 0.883, AUC 0.9477621912956238, avg_entr 0.12544237077236176, f1 0.8830000162124634
ep15_l0_test_time 0.7903072834014893
Test Epoch15 layer1 Acc 0.8728, AUC 0.9213013052940369, avg_entr 0.025828460231423378, f1 0.8727999925613403
ep15_l1_test_time 2.3017194271087646
Test Epoch15 layer2 Acc 0.873, AUC 0.9303652048110962, avg_entr 0.01736360788345337, f1 0.8730000257492065
ep15_l2_test_time 4.226602554321289
Test Epoch15 layer3 Acc 0.8728, AUC 0.9376775026321411, avg_entr 0.01606271043419838, f1 0.8727999925613403
ep15_l3_test_time 5.997540473937988
Test Epoch15 layer4 Acc 0.8726, AUC 0.9390535354614258, avg_entr 0.015465205535292625, f1 0.8726000189781189
ep15_l4_test_time 6.051538467407227
gc 0
Train Epoch16 Acc 0.97895 (39158/40000), AUC 0.9951399564743042
ep16_train_time 173.50397634506226
Test Epoch16 layer0 Acc 0.8848, AUC 0.9469389915466309, avg_entr 0.12259622663259506, f1 0.8848000168800354
ep16_l0_test_time 0.7353792190551758
Test Epoch16 layer1 Acc 0.8718, AUC 0.9201042652130127, avg_entr 0.025493618100881577, f1 0.8718000650405884
ep16_l1_test_time 1.957106351852417
Test Epoch16 layer2 Acc 0.872, AUC 0.9292504787445068, avg_entr 0.01725144311785698, f1 0.871999979019165
ep16_l2_test_time 3.1160919666290283
Test Epoch16 layer3 Acc 0.8718, AUC 0.936683177947998, avg_entr 0.016062812879681587, f1 0.8718000650405884
ep16_l3_test_time 4.3388543128967285
Test Epoch16 layer4 Acc 0.872, AUC 0.9380964040756226, avg_entr 0.015599004924297333, f1 0.871999979019165
ep16_l4_test_time 5.553070068359375
gc 0
Train Epoch17 Acc 0.979975 (39199/40000), AUC 0.995381772518158
ep17_train_time 107.87164878845215
Test Epoch17 layer0 Acc 0.8842, AUC 0.9466668963432312, avg_entr 0.1217607781291008, f1 0.8842000365257263
ep17_l0_test_time 0.5675251483917236
Test Epoch17 layer1 Acc 0.8724, AUC 0.920015275478363, avg_entr 0.02528427354991436, f1 0.8723999857902527
ep17_l1_test_time 1.216459035873413
Test Epoch17 layer2 Acc 0.872, AUC 0.9287527799606323, avg_entr 0.017410555854439735, f1 0.871999979019165
ep17_l2_test_time 1.8645093441009521
Test Epoch17 layer3 Acc 0.872, AUC 0.9362069368362427, avg_entr 0.01659330166876316, f1 0.871999979019165
ep17_l3_test_time 2.4848811626434326
Test Epoch17 layer4 Acc 0.8722, AUC 0.9376329183578491, avg_entr 0.016156774014234543, f1 0.872200071811676
ep17_l4_test_time 3.105400800704956
gc 0
Train Epoch18 Acc 0.9803 (39212/40000), AUC 0.9958750009536743
ep18_train_time 148.39385080337524
Test Epoch18 layer0 Acc 0.8826, AUC 0.94636470079422, avg_entr 0.12212596088647842, f1 0.8826000094413757
ep18_l0_test_time 0.78387451171875
Test Epoch18 layer1 Acc 0.8694, AUC 0.917784571647644, avg_entr 0.024617621675133705, f1 0.8694000244140625
ep18_l1_test_time 2.0747733116149902
Test Epoch18 layer2 Acc 0.8696, AUC 0.9248766899108887, avg_entr 0.016414346173405647, f1 0.8695999383926392
ep18_l2_test_time 3.2992238998413086
Test Epoch18 layer3 Acc 0.869, AUC 0.9355900287628174, avg_entr 0.015071708709001541, f1 0.8690000176429749
ep18_l3_test_time 4.534539222717285
Test Epoch18 layer4 Acc 0.869, AUC 0.9372531175613403, avg_entr 0.014642015099525452, f1 0.8690000176429749
ep18_l4_test_time 5.847273111343384
gc 0
Train Epoch19 Acc 0.98075 (39230/40000), AUC 0.9957466125488281
ep19_train_time 182.78378987312317
Test Epoch19 layer0 Acc 0.8838, AUC 0.9459879994392395, avg_entr 0.12046406418085098, f1 0.8838000297546387
ep19_l0_test_time 0.7707583904266357
Test Epoch19 layer1 Acc 0.8714, AUC 0.9173222780227661, avg_entr 0.024344727396965027, f1 0.871399998664856
ep19_l1_test_time 2.0390806198120117
Test Epoch19 layer2 Acc 0.8708, AUC 0.9255306124687195, avg_entr 0.016122087836265564, f1 0.8708000183105469
ep19_l2_test_time 3.287921905517578
Test Epoch19 layer3 Acc 0.8708, AUC 0.9353110790252686, avg_entr 0.015021995641291142, f1 0.8708000183105469
ep19_l3_test_time 4.548711776733398
Test Epoch19 layer4 Acc 0.8706, AUC 0.9368587732315063, avg_entr 0.014533580280840397, f1 0.8705999851226807
ep19_l4_test_time 5.748968839645386
gc 0
Train Epoch20 Acc 0.9812 (39248/40000), AUC 0.995979905128479
ep20_train_time 181.2974226474762
Test Epoch20 layer0 Acc 0.8804, AUC 0.9456835985183716, avg_entr 0.11814958602190018, f1 0.8804000020027161
ep20_l0_test_time 0.793287992477417
Test Epoch20 layer1 Acc 0.8712, AUC 0.9173094034194946, avg_entr 0.024095658212900162, f1 0.8712000846862793
ep20_l1_test_time 2.0916218757629395
Test Epoch20 layer2 Acc 0.871, AUC 0.9257178902626038, avg_entr 0.01590711437165737, f1 0.8709999918937683
ep20_l2_test_time 3.3174970149993896
Test Epoch20 layer3 Acc 0.8712, AUC 0.9349592924118042, avg_entr 0.01492643728852272, f1 0.8712000846862793
ep20_l3_test_time 4.622745752334595
Test Epoch20 layer4 Acc 0.871, AUC 0.9365861415863037, avg_entr 0.014379383996129036, f1 0.8709999918937683
ep20_l4_test_time 5.990039348602295
gc 0
Train Epoch21 Acc 0.981575 (39263/40000), AUC 0.995987057685852
ep21_train_time 182.94103622436523
Test Epoch21 layer0 Acc 0.8832, AUC 0.9455651044845581, avg_entr 0.11889837682247162, f1 0.8831999897956848
ep21_l0_test_time 0.7872238159179688
Test Epoch21 layer1 Acc 0.8712, AUC 0.9166420698165894, avg_entr 0.023663977161049843, f1 0.8712000846862793
ep21_l1_test_time 2.0749504566192627
Test Epoch21 layer2 Acc 0.8708, AUC 0.9247360825538635, avg_entr 0.015831317752599716, f1 0.8708000183105469
ep21_l2_test_time 3.278503656387329
Test Epoch21 layer3 Acc 0.871, AUC 0.9346821308135986, avg_entr 0.014847668819129467, f1 0.8709999918937683
ep21_l3_test_time 4.555572509765625
Test Epoch21 layer4 Acc 0.871, AUC 0.9362692832946777, avg_entr 0.014285912737250328, f1 0.8709999918937683
ep21_l4_test_time 5.875323295593262
gc 0
Train Epoch22 Acc 0.981425 (39257/40000), AUC 0.9960082769393921
ep22_train_time 182.95398116111755
Test Epoch22 layer0 Acc 0.8838, AUC 0.9453815817832947, avg_entr 0.11835359036922455, f1 0.8838000297546387
ep22_l0_test_time 0.7862813472747803
Test Epoch22 layer1 Acc 0.8694, AUC 0.9159331917762756, avg_entr 0.023035936057567596, f1 0.8694000244140625
ep22_l1_test_time 2.050614595413208
Test Epoch22 layer2 Acc 0.87, AUC 0.9237055778503418, avg_entr 0.014593822881579399, f1 0.8700000047683716
ep22_l2_test_time 3.264430284500122
Test Epoch22 layer3 Acc 0.8708, AUC 0.9344618320465088, avg_entr 0.013461017981171608, f1 0.8708000183105469
ep22_l3_test_time 4.509599208831787
Test Epoch22 layer4 Acc 0.8704, AUC 0.9361916780471802, avg_entr 0.012952911667525768, f1 0.8704000115394592
ep22_l4_test_time 5.829752445220947
gc 0
Train Epoch23 Acc 0.98145 (39258/40000), AUC 0.9961763620376587
ep23_train_time 182.69751834869385
Test Epoch23 layer0 Acc 0.8816, AUC 0.9452568292617798, avg_entr 0.11807221919298172, f1 0.881600022315979
ep23_l0_test_time 0.7871780395507812
Test Epoch23 layer1 Acc 0.8698, AUC 0.9152978658676147, avg_entr 0.02294243313372135, f1 0.8697999715805054
ep23_l1_test_time 2.0572550296783447
Test Epoch23 layer2 Acc 0.87, AUC 0.9227330684661865, avg_entr 0.0147874616086483, f1 0.8700000047683716
ep23_l2_test_time 3.303618907928467
Test Epoch23 layer3 Acc 0.8706, AUC 0.9338916540145874, avg_entr 0.013711001724004745, f1 0.8705999851226807
ep23_l3_test_time 4.512080907821655
Test Epoch23 layer4 Acc 0.87, AUC 0.9358069896697998, avg_entr 0.013135693036019802, f1 0.8700000047683716
ep23_l4_test_time 5.880542993545532
gc 0
Train Epoch24 Acc 0.981675 (39267/40000), AUC 0.995955228805542
ep24_train_time 182.9620885848999
Test Epoch24 layer0 Acc 0.88, AUC 0.9451779127120972, avg_entr 0.11835026741027832, f1 0.8799999952316284
ep24_l0_test_time 0.7810783386230469
Test Epoch24 layer1 Acc 0.8674, AUC 0.9153719544410706, avg_entr 0.022705810144543648, f1 0.8673999905586243
ep24_l1_test_time 2.052410125732422
Test Epoch24 layer2 Acc 0.868, AUC 0.9220062494277954, avg_entr 0.014520570635795593, f1 0.8679999709129333
ep24_l2_test_time 3.2983243465423584
Test Epoch24 layer3 Acc 0.868, AUC 0.9337848424911499, avg_entr 0.013421362265944481, f1 0.8679999709129333
ep24_l3_test_time 4.586374759674072
Test Epoch24 layer4 Acc 0.8676, AUC 0.9357495903968811, avg_entr 0.013042992912232876, f1 0.8675999641418457
ep24_l4_test_time 5.878354072570801
gc 0
Train Epoch25 Acc 0.981775 (39271/40000), AUC 0.9962373971939087
ep25_train_time 183.15197777748108
Test Epoch25 layer0 Acc 0.8808, AUC 0.9450830221176147, avg_entr 0.11680241674184799, f1 0.8808000087738037
ep25_l0_test_time 0.7749004364013672
Test Epoch25 layer1 Acc 0.8702, AUC 0.9153635501861572, avg_entr 0.022396041080355644, f1 0.870199978351593
ep25_l1_test_time 2.092578172683716
Test Epoch25 layer2 Acc 0.87, AUC 0.9226332902908325, avg_entr 0.014234527945518494, f1 0.8700000047683716
ep25_l2_test_time 3.2921760082244873
Test Epoch25 layer3 Acc 0.87, AUC 0.9337242841720581, avg_entr 0.013070185668766499, f1 0.8700000047683716
ep25_l3_test_time 4.61706805229187
Test Epoch25 layer4 Acc 0.8702, AUC 0.9356458187103271, avg_entr 0.012500294484198093, f1 0.870199978351593
ep25_l4_test_time 6.026077747344971
gc 0
Train Epoch26 Acc 0.98185 (39274/40000), AUC 0.9965245723724365
ep26_train_time 182.6061201095581
Test Epoch26 layer0 Acc 0.8816, AUC 0.94500732421875, avg_entr 0.11664380133152008, f1 0.881600022315979
ep26_l0_test_time 0.7706902027130127
Test Epoch26 layer1 Acc 0.8706, AUC 0.9152549505233765, avg_entr 0.02250009961426258, f1 0.8705999851226807
ep26_l1_test_time 2.058011293411255
Test Epoch26 layer2 Acc 0.8698, AUC 0.9222252368927002, avg_entr 0.014359324239194393, f1 0.8697999715805054
ep26_l2_test_time 3.283083915710449
Test Epoch26 layer3 Acc 0.8706, AUC 0.9335294365882874, avg_entr 0.013280685059726238, f1 0.8705999851226807
ep26_l3_test_time 4.620831727981567
Test Epoch26 layer4 Acc 0.87, AUC 0.9355152249336243, avg_entr 0.012682671658694744, f1 0.8700000047683716
ep26_l4_test_time 5.974700927734375
gc 0
Train Epoch27 Acc 0.982225 (39289/40000), AUC 0.9962249994277954
ep27_train_time 244.85236358642578
Test Epoch27 layer0 Acc 0.8788, AUC 0.9449657201766968, avg_entr 0.11679191887378693, f1 0.8787999749183655
ep27_l0_test_time 0.9936320781707764
Test Epoch27 layer1 Acc 0.8694, AUC 0.9150223135948181, avg_entr 0.022222962230443954, f1 0.8694000244140625
ep27_l1_test_time 2.8754615783691406
Test Epoch27 layer2 Acc 0.8696, AUC 0.9216466546058655, avg_entr 0.013980342075228691, f1 0.8695999383926392
ep27_l2_test_time 4.345103740692139
Test Epoch27 layer3 Acc 0.8698, AUC 0.9334235191345215, avg_entr 0.012855958193540573, f1 0.8697999715805054
ep27_l3_test_time 6.177833318710327
Test Epoch27 layer4 Acc 0.8698, AUC 0.9354349374771118, avg_entr 0.012336339801549911, f1 0.8697999715805054
ep27_l4_test_time 7.7691357135772705
gc 0
Train Epoch28 Acc 0.982275 (39291/40000), AUC 0.9963637590408325
ep28_train_time 234.88754224777222
Test Epoch28 layer0 Acc 0.8826, AUC 0.9448634386062622, avg_entr 0.11492994427680969, f1 0.8826000094413757
ep28_l0_test_time 0.9596080780029297
Test Epoch28 layer1 Acc 0.8712, AUC 0.9148245453834534, avg_entr 0.02255021221935749, f1 0.8712000846862793
ep28_l1_test_time 2.7724649906158447
Test Epoch28 layer2 Acc 0.8704, AUC 0.9218002557754517, avg_entr 0.014828134328126907, f1 0.8704000115394592
ep28_l2_test_time 4.302027702331543
Test Epoch28 layer3 Acc 0.8698, AUC 0.9332518577575684, avg_entr 0.013891361653804779, f1 0.8697999715805054
ep28_l3_test_time 6.13730525970459
Test Epoch28 layer4 Acc 0.87, AUC 0.9352554082870483, avg_entr 0.013292175717651844, f1 0.8700000047683716
ep28_l4_test_time 7.77274489402771
gc 0
Train Epoch29 Acc 0.982225 (39289/40000), AUC 0.9963400363922119
ep29_train_time 246.42460775375366
Test Epoch29 layer0 Acc 0.8816, AUC 0.9448484182357788, avg_entr 0.11526709049940109, f1 0.881600022315979
ep29_l0_test_time 0.9893138408660889
Test Epoch29 layer1 Acc 0.8676, AUC 0.914472222328186, avg_entr 0.022018665447831154, f1 0.8675999641418457
ep29_l1_test_time 2.7997753620147705
Test Epoch29 layer2 Acc 0.868, AUC 0.9205667972564697, avg_entr 0.013759423978626728, f1 0.8679999709129333
ep29_l2_test_time 4.459533214569092
Test Epoch29 layer3 Acc 0.8682, AUC 0.9329192638397217, avg_entr 0.012652887962758541, f1 0.8682000041007996
ep29_l3_test_time 6.305321216583252
Test Epoch29 layer4 Acc 0.8678, AUC 0.9351060390472412, avg_entr 0.012232580222189426, f1 0.8677999973297119
ep29_l4_test_time 8.096812009811401
Best AUC tensor(0.9004) 4 0
train_loss (2, 5, 30)
valid_acc (5, 30)
valid_AUC (5, 30)
train_acc (30,)
total_train+valid_time 6027.663151979446
Start Testing
Load ckpt at ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt
Test layer0 Acc 0.8964, AUC 0.9557414650917053, avg_entr 0.1797855645418167, f1 0.896399974822998
l0_test_time 0.9972481727600098
Test layer1 Acc 0.8824, AUC 0.9497265219688416, avg_entr 0.06181538105010986, f1 0.8823999762535095
l1_test_time 2.7770779132843018
Test layer2 Acc 0.8832, AUC 0.9519550800323486, avg_entr 0.04059600457549095, f1 0.8831999897956848
l2_test_time 4.356143236160278
Test layer3 Acc 0.8834, AUC 0.9522919058799744, avg_entr 0.04043379798531532, f1 0.8834000825881958
l3_test_time 6.306933403015137
Test layer4 Acc 0.8824, AUC 0.9528747200965881, avg_entr 0.04016931354999542, f1 0.8823999762535095
l4_test_time 8.122187614440918
