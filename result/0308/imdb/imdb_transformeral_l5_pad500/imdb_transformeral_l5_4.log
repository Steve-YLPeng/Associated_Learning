total count words 222751
vocab size 30000
train size 40000, valid size 5000, test size 5000
found 27937 words in glove
model: TransformerModelML(
  (layers): ModuleList(
    (0): EMBLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Embedding(30000, 300)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=2, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=2, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.weight 9000000
layers.0.ae.g.0.weight 256
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 256
layers.0.ae.h.0.bias 2
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
Total Trainable Params: 13671778
init_time 22.969146966934204
Start Training
gc 0
Train Epoch0 Acc 0.527675 (21107/40000), AUC 0.5482847690582275
ep0_train_time 187.13634204864502
Test Epoch0 layer0 Acc 0.7926, AUC 0.8891995549201965, avg_entr 0.532658040523529, f1 0.7925999760627747
ep0_l0_test_time 0.7637639045715332
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 0
Test Epoch0 layer1 Acc 0.8364, AUC 0.9144562482833862, avg_entr 0.34422576427459717, f1 0.8363999724388123
ep0_l1_test_time 2.0774099826812744
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 0
Test Epoch0 layer2 Acc 0.8298, AUC 0.9151045680046082, avg_entr 0.5420814156532288, f1 0.829800009727478
ep0_l2_test_time 3.3267641067504883
Test Epoch0 layer3 Acc 0.8294, AUC 0.9149608612060547, avg_entr 0.6164037585258484, f1 0.8294000029563904
ep0_l3_test_time 4.6196582317352295
Test Epoch0 layer4 Acc 0.7524, AUC 0.9041736125946045, avg_entr 0.6782474517822266, f1 0.7523999810218811
ep0_l4_test_time 5.941694736480713
gc 0
Train Epoch1 Acc 0.86285 (34514/40000), AUC 0.9316923022270203
ep1_train_time 186.03068661689758
Test Epoch1 layer0 Acc 0.883, AUC 0.9460233449935913, avg_entr 0.28599783778190613, f1 0.8830000162124634
ep1_l0_test_time 0.7640812397003174
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 1
Test Epoch1 layer1 Acc 0.8776, AUC 0.9558624625205994, avg_entr 0.19296802580356598, f1 0.8776000142097473
ep1_l1_test_time 2.065331220626831
Test Epoch1 layer2 Acc 0.8654, AUC 0.9568567276000977, avg_entr 0.17783227562904358, f1 0.865399956703186
ep1_l2_test_time 3.3003013134002686
Test Epoch1 layer3 Acc 0.8638, AUC 0.9567009210586548, avg_entr 0.16177088022232056, f1 0.8637999892234802
ep1_l3_test_time 4.655713796615601
Test Epoch1 layer4 Acc 0.8606, AUC 0.9566841125488281, avg_entr 0.15371455252170563, f1 0.8605999946594238
ep1_l4_test_time 5.953625917434692
gc 0
Train Epoch2 Acc 0.918125 (36725/40000), AUC 0.9705462455749512
ep2_train_time 185.7313756942749
Test Epoch2 layer0 Acc 0.8918, AUC 0.955223560333252, avg_entr 0.223678320646286, f1 0.8917999863624573
ep2_l0_test_time 0.7642679214477539
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 2
Test Epoch2 layer1 Acc 0.8988, AUC 0.9586719274520874, avg_entr 0.14939738810062408, f1 0.8988000154495239
ep2_l1_test_time 2.1032769680023193
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 2
Test Epoch2 layer2 Acc 0.8954, AUC 0.9583157300949097, avg_entr 0.0904388278722763, f1 0.8953999876976013
ep2_l2_test_time 3.316319465637207
Test Epoch2 layer3 Acc 0.8948, AUC 0.9581683278083801, avg_entr 0.0615701898932457, f1 0.8948000073432922
ep2_l3_test_time 4.628059148788452
Test Epoch2 layer4 Acc 0.8944, AUC 0.9583088755607605, avg_entr 0.05394138768315315, f1 0.8944000005722046
ep2_l4_test_time 5.9532201290130615
gc 0
Train Epoch3 Acc 0.94055 (37622/40000), AUC 0.9806303381919861
ep3_train_time 186.02847170829773
Test Epoch3 layer0 Acc 0.8938, AUC 0.9581344723701477, avg_entr 0.19353413581848145, f1 0.8938000202178955
ep3_l0_test_time 0.7649009227752686
Test Epoch3 layer1 Acc 0.8962, AUC 0.9565552473068237, avg_entr 0.11793609708547592, f1 0.8962000012397766
ep3_l1_test_time 2.0672850608825684
Test Epoch3 layer2 Acc 0.8954, AUC 0.9554868936538696, avg_entr 0.053461167961359024, f1 0.8953999876976013
ep3_l2_test_time 3.3347585201263428
Test Epoch3 layer3 Acc 0.896, AUC 0.9554786682128906, avg_entr 0.04246436059474945, f1 0.8960000276565552
ep3_l3_test_time 4.625315189361572
Test Epoch3 layer4 Acc 0.8958, AUC 0.9566513299942017, avg_entr 0.04144008457660675, f1 0.895799994468689
ep3_l4_test_time 5.922630310058594
gc 0
Train Epoch4 Acc 0.95135 (38054/40000), AUC 0.9856972694396973
ep4_train_time 186.1378264427185
Test Epoch4 layer0 Acc 0.8976, AUC 0.9580382704734802, avg_entr 0.1748574674129486, f1 0.897599995136261
ep4_l0_test_time 0.7661335468292236
Test Epoch4 layer1 Acc 0.89, AUC 0.9538270831108093, avg_entr 0.06584616005420685, f1 0.8899999856948853
ep4_l1_test_time 2.0720481872558594
Test Epoch4 layer2 Acc 0.8902, AUC 0.9533414840698242, avg_entr 0.03594323247671127, f1 0.8902000188827515
ep4_l2_test_time 3.3076438903808594
Test Epoch4 layer3 Acc 0.8906, AUC 0.9534426927566528, avg_entr 0.03471461310982704, f1 0.8906000256538391
ep4_l3_test_time 4.595967054367065
Test Epoch4 layer4 Acc 0.891, AUC 0.9536329507827759, avg_entr 0.03558554872870445, f1 0.890999972820282
ep4_l4_test_time 5.935062408447266
gc 0
Train Epoch5 Acc 0.958 (38320/40000), AUC 0.9876111149787903
ep5_train_time 186.03379368782043
Test Epoch5 layer0 Acc 0.8962, AUC 0.9579658508300781, avg_entr 0.16188791394233704, f1 0.8962000012397766
ep5_l0_test_time 0.771207332611084
Test Epoch5 layer1 Acc 0.8848, AUC 0.9506978988647461, avg_entr 0.05093246325850487, f1 0.8848000168800354
ep5_l1_test_time 2.085308074951172
Test Epoch5 layer2 Acc 0.8858, AUC 0.9516434073448181, avg_entr 0.03233908861875534, f1 0.8858000040054321
ep5_l2_test_time 3.307882070541382
Test Epoch5 layer3 Acc 0.885, AUC 0.9521218538284302, avg_entr 0.03231671452522278, f1 0.8849999904632568
ep5_l3_test_time 4.649837017059326
Test Epoch5 layer4 Acc 0.8844, AUC 0.9520216584205627, avg_entr 0.0332520492374897, f1 0.8844000101089478
ep5_l4_test_time 5.979872941970825
gc 0
Train Epoch6 Acc 0.961875 (38475/40000), AUC 0.9881744980812073
ep6_train_time 185.98366022109985
Test Epoch6 layer0 Acc 0.896, AUC 0.9568384885787964, avg_entr 0.15556325018405914, f1 0.8960000276565552
ep6_l0_test_time 0.7709143161773682
Test Epoch6 layer1 Acc 0.8862, AUC 0.9468008279800415, avg_entr 0.04343932494521141, f1 0.8862000107765198
ep6_l1_test_time 2.0901055335998535
Test Epoch6 layer2 Acc 0.887, AUC 0.9500361680984497, avg_entr 0.029993148520588875, f1 0.8870000243186951
ep6_l2_test_time 3.3307430744171143
Test Epoch6 layer3 Acc 0.886, AUC 0.9504382014274597, avg_entr 0.030707042664289474, f1 0.8859999775886536
ep6_l3_test_time 4.6398985385894775
Test Epoch6 layer4 Acc 0.886, AUC 0.9502645134925842, avg_entr 0.030562780797481537, f1 0.8859999775886536
ep6_l4_test_time 5.7737812995910645
gc 0
Train Epoch7 Acc 0.966625 (38665/40000), AUC 0.9913680553436279
ep7_train_time 186.19221997261047
Test Epoch7 layer0 Acc 0.8886, AUC 0.9547874927520752, avg_entr 0.14353683590888977, f1 0.8885999917984009
ep7_l0_test_time 0.7653985023498535
Test Epoch7 layer1 Acc 0.8822, AUC 0.9437522888183594, avg_entr 0.03760416805744171, f1 0.8822000026702881
ep7_l1_test_time 2.0635673999786377
Test Epoch7 layer2 Acc 0.883, AUC 0.9475761651992798, avg_entr 0.025890517979860306, f1 0.8830000162124634
ep7_l2_test_time 3.347240447998047
Test Epoch7 layer3 Acc 0.8828, AUC 0.9484400749206543, avg_entr 0.025822525843977928, f1 0.8827999830245972
ep7_l3_test_time 4.548875331878662
Test Epoch7 layer4 Acc 0.8828, AUC 0.948119580745697, avg_entr 0.025576557964086533, f1 0.8827999830245972
ep7_l4_test_time 5.712599754333496
gc 0
Train Epoch8 Acc 0.96985 (38794/40000), AUC 0.9927897453308105
ep8_train_time 186.10697054862976
Test Epoch8 layer0 Acc 0.8898, AUC 0.9541943073272705, avg_entr 0.14079193770885468, f1 0.8898000121116638
ep8_l0_test_time 0.7708544731140137
Test Epoch8 layer1 Acc 0.881, AUC 0.9408286809921265, avg_entr 0.03474171832203865, f1 0.8809999823570251
ep8_l1_test_time 2.055471897125244
Test Epoch8 layer2 Acc 0.8814, AUC 0.9469382762908936, avg_entr 0.023369912058115005, f1 0.8813999891281128
ep8_l2_test_time 3.240642547607422
Test Epoch8 layer3 Acc 0.8812, AUC 0.9481195211410522, avg_entr 0.022937363013625145, f1 0.8812000155448914
ep8_l3_test_time 4.406508445739746
Test Epoch8 layer4 Acc 0.8816, AUC 0.9478090405464172, avg_entr 0.02279193326830864, f1 0.881600022315979
ep8_l4_test_time 5.950739860534668
gc 0
Train Epoch9 Acc 0.971825 (38873/40000), AUC 0.9928827285766602
ep9_train_time 166.2397174835205
Test Epoch9 layer0 Acc 0.8888, AUC 0.9530644416809082, avg_entr 0.13643401861190796, f1 0.8888000249862671
ep9_l0_test_time 0.7667672634124756
Test Epoch9 layer1 Acc 0.88, AUC 0.9383240938186646, avg_entr 0.03140128776431084, f1 0.8799999952316284
ep9_l1_test_time 2.0605127811431885
Test Epoch9 layer2 Acc 0.8798, AUC 0.9454963207244873, avg_entr 0.02097402885556221, f1 0.879800021648407
ep9_l2_test_time 3.3107166290283203
Test Epoch9 layer3 Acc 0.8798, AUC 0.9470848441123962, avg_entr 0.020818792283535004, f1 0.879800021648407
ep9_l3_test_time 4.616109132766724
Test Epoch9 layer4 Acc 0.8798, AUC 0.9466370344161987, avg_entr 0.020538216456770897, f1 0.879800021648407
ep9_l4_test_time 5.9563329219818115
gc 0
Train Epoch10 Acc 0.972975 (38919/40000), AUC 0.9929168224334717
ep10_train_time 185.68952679634094
Test Epoch10 layer0 Acc 0.8878, AUC 0.9519011378288269, avg_entr 0.13555209338665009, f1 0.8877999782562256
ep10_l0_test_time 0.767127513885498
Test Epoch10 layer1 Acc 0.876, AUC 0.9348347187042236, avg_entr 0.031888026744127274, f1 0.8760000467300415
ep10_l1_test_time 2.0859169960021973
Test Epoch10 layer2 Acc 0.8758, AUC 0.9435216188430786, avg_entr 0.022414421662688255, f1 0.8758000135421753
ep10_l2_test_time 3.3148717880249023
Test Epoch10 layer3 Acc 0.8764, AUC 0.9455596804618835, avg_entr 0.021783266216516495, f1 0.8763999938964844
ep10_l3_test_time 4.61096453666687
Test Epoch10 layer4 Acc 0.876, AUC 0.945091724395752, avg_entr 0.021593621000647545, f1 0.8760000467300415
ep10_l4_test_time 5.94965672492981
gc 0
Train Epoch11 Acc 0.97455 (38982/40000), AUC 0.9942495822906494
ep11_train_time 185.79986929893494
Test Epoch11 layer0 Acc 0.888, AUC 0.9509443044662476, avg_entr 0.13149763643741608, f1 0.8880000114440918
ep11_l0_test_time 0.7669575214385986
Test Epoch11 layer1 Acc 0.8736, AUC 0.9332441687583923, avg_entr 0.02909298986196518, f1 0.8736000061035156
ep11_l1_test_time 2.0833544731140137
Test Epoch11 layer2 Acc 0.873, AUC 0.9422144889831543, avg_entr 0.020281214267015457, f1 0.8730000257492065
ep11_l2_test_time 3.348776340484619
Test Epoch11 layer3 Acc 0.8722, AUC 0.9444341063499451, avg_entr 0.019912434741854668, f1 0.872200071811676
ep11_l3_test_time 4.655660152435303
Test Epoch11 layer4 Acc 0.872, AUC 0.9441238641738892, avg_entr 0.01961608976125717, f1 0.871999979019165
ep11_l4_test_time 5.947460174560547
gc 0
Train Epoch12 Acc 0.97595 (39038/40000), AUC 0.9943273067474365
ep12_train_time 185.8395094871521
Test Epoch12 layer0 Acc 0.8878, AUC 0.95045006275177, avg_entr 0.12900468707084656, f1 0.8877999782562256
ep12_l0_test_time 0.7580101490020752
Test Epoch12 layer1 Acc 0.8706, AUC 0.9326353073120117, avg_entr 0.029166894033551216, f1 0.8705999851226807
ep12_l1_test_time 2.084235191345215
Test Epoch12 layer2 Acc 0.8706, AUC 0.9410624504089355, avg_entr 0.020339462906122208, f1 0.8705999851226807
ep12_l2_test_time 3.3224234580993652
Test Epoch12 layer3 Acc 0.8706, AUC 0.9441172480583191, avg_entr 0.019835982471704483, f1 0.8705999851226807
ep12_l3_test_time 4.64486026763916
Test Epoch12 layer4 Acc 0.8706, AUC 0.9438713788986206, avg_entr 0.01951303333044052, f1 0.8705999851226807
ep12_l4_test_time 6.022373676300049
gc 0
Train Epoch13 Acc 0.9767 (39068/40000), AUC 0.9945673942565918
ep13_train_time 186.04764890670776
Test Epoch13 layer0 Acc 0.8864, AUC 0.9498279094696045, avg_entr 0.12893322110176086, f1 0.8863999843597412
ep13_l0_test_time 0.7686479091644287
Test Epoch13 layer1 Acc 0.876, AUC 0.9315994381904602, avg_entr 0.026046276092529297, f1 0.8760000467300415
ep13_l1_test_time 2.072300434112549
Test Epoch13 layer2 Acc 0.876, AUC 0.9402716755867004, avg_entr 0.01750420220196247, f1 0.8760000467300415
ep13_l2_test_time 3.325554847717285
Test Epoch13 layer3 Acc 0.8758, AUC 0.9440674781799316, avg_entr 0.01718563586473465, f1 0.8758000135421753
ep13_l3_test_time 4.610463619232178
Test Epoch13 layer4 Acc 0.8758, AUC 0.9437966346740723, avg_entr 0.01714698038995266, f1 0.8758000135421753
ep13_l4_test_time 5.969548225402832
gc 0
Train Epoch14 Acc 0.977325 (39093/40000), AUC 0.9948025941848755
ep14_train_time 185.78697538375854
Test Epoch14 layer0 Acc 0.8878, AUC 0.949291467666626, avg_entr 0.12704741954803467, f1 0.8877999782562256
ep14_l0_test_time 0.7711648941040039
Test Epoch14 layer1 Acc 0.8748, AUC 0.9304549694061279, avg_entr 0.02515660785138607, f1 0.8748000264167786
ep14_l1_test_time 2.0954153537750244
Test Epoch14 layer2 Acc 0.8742, AUC 0.9398211240768433, avg_entr 0.016728540882468224, f1 0.8741999864578247
ep14_l2_test_time 3.2998831272125244
Test Epoch14 layer3 Acc 0.875, AUC 0.943112850189209, avg_entr 0.016478341072797775, f1 0.875
ep14_l3_test_time 4.654124736785889
Test Epoch14 layer4 Acc 0.874, AUC 0.9429517984390259, avg_entr 0.016089100390672684, f1 0.8740000128746033
ep14_l4_test_time 5.707761764526367
gc 0
Train Epoch15 Acc 0.977825 (39113/40000), AUC 0.9950858354568481
ep15_train_time 186.0847511291504
Test Epoch15 layer0 Acc 0.8828, AUC 0.9483853578567505, avg_entr 0.12407343834638596, f1 0.8827999830245972
ep15_l0_test_time 0.7812168598175049
Test Epoch15 layer1 Acc 0.8726, AUC 0.9277880191802979, avg_entr 0.024317855015397072, f1 0.8726000189781189
ep15_l1_test_time 2.0827009677886963
Test Epoch15 layer2 Acc 0.873, AUC 0.9359883069992065, avg_entr 0.01636904664337635, f1 0.8730000257492065
ep15_l2_test_time 3.3096137046813965
Test Epoch15 layer3 Acc 0.8726, AUC 0.9417683482170105, avg_entr 0.015864431858062744, f1 0.8726000189781189
ep15_l3_test_time 4.410919427871704
Test Epoch15 layer4 Acc 0.8734, AUC 0.9421154856681824, avg_entr 0.015710007399320602, f1 0.8733999729156494
ep15_l4_test_time 5.85279655456543
gc 0
Train Epoch16 Acc 0.978925 (39157/40000), AUC 0.99546217918396
ep16_train_time 186.08286237716675
Test Epoch16 layer0 Acc 0.8838, AUC 0.9482840299606323, avg_entr 0.1235121563076973, f1 0.8838000297546387
ep16_l0_test_time 0.7693428993225098
Test Epoch16 layer1 Acc 0.8734, AUC 0.9266434907913208, avg_entr 0.024168068543076515, f1 0.8733999729156494
ep16_l1_test_time 2.0913760662078857
Test Epoch16 layer2 Acc 0.8724, AUC 0.9367449283599854, avg_entr 0.01595452055335045, f1 0.8723999857902527
ep16_l2_test_time 3.1175143718719482
Test Epoch16 layer3 Acc 0.8726, AUC 0.9417880773544312, avg_entr 0.015529317781329155, f1 0.8726000189781189
ep16_l3_test_time 4.54194974899292
Test Epoch16 layer4 Acc 0.8724, AUC 0.9419685006141663, avg_entr 0.015088076703250408, f1 0.8723999857902527
ep16_l4_test_time 5.961912393569946
gc 0
Train Epoch17 Acc 0.9789 (39156/40000), AUC 0.9955668449401855
ep17_train_time 186.1963164806366
Test Epoch17 layer0 Acc 0.8836, AUC 0.948055624961853, avg_entr 0.12451491504907608, f1 0.8835999965667725
ep17_l0_test_time 0.7274539470672607
Test Epoch17 layer1 Acc 0.8728, AUC 0.9275394678115845, avg_entr 0.023179566487669945, f1 0.8727999925613403
ep17_l1_test_time 1.9438672065734863
Test Epoch17 layer2 Acc 0.8734, AUC 0.9358210563659668, avg_entr 0.015195182524621487, f1 0.8733999729156494
ep17_l2_test_time 3.3081696033477783
Test Epoch17 layer3 Acc 0.8728, AUC 0.9413851499557495, avg_entr 0.014530667103827, f1 0.8727999925613403
ep17_l3_test_time 4.651636362075806
Test Epoch17 layer4 Acc 0.873, AUC 0.9420055747032166, avg_entr 0.014406543225049973, f1 0.8730000257492065
ep17_l4_test_time 5.985350131988525
gc 0
Train Epoch18 Acc 0.97955 (39182/40000), AUC 0.9954904913902283
ep18_train_time 185.9445719718933
Test Epoch18 layer0 Acc 0.8826, AUC 0.9476784467697144, avg_entr 0.12306445091962814, f1 0.8826000094413757
ep18_l0_test_time 0.7511632442474365
Test Epoch18 layer1 Acc 0.872, AUC 0.9253208637237549, avg_entr 0.02332228794693947, f1 0.871999979019165
ep18_l1_test_time 2.0884151458740234
Test Epoch18 layer2 Acc 0.8702, AUC 0.9332307577133179, avg_entr 0.01562411803752184, f1 0.870199978351593
ep18_l2_test_time 3.3413376808166504
Test Epoch18 layer3 Acc 0.8708, AUC 0.9393818378448486, avg_entr 0.014521464705467224, f1 0.8708000183105469
ep18_l3_test_time 4.63918137550354
Test Epoch18 layer4 Acc 0.8708, AUC 0.9410314559936523, avg_entr 0.014308796264231205, f1 0.8708000183105469
ep18_l4_test_time 5.9377830028533936
gc 0
Train Epoch19 Acc 0.9795 (39180/40000), AUC 0.9956508278846741
ep19_train_time 185.9607391357422
Test Epoch19 layer0 Acc 0.8832, AUC 0.9474439024925232, avg_entr 0.12090995162725449, f1 0.8831999897956848
ep19_l0_test_time 0.7571227550506592
Test Epoch19 layer1 Acc 0.872, AUC 0.9243643283843994, avg_entr 0.02268621139228344, f1 0.871999979019165
ep19_l1_test_time 2.0953474044799805
Test Epoch19 layer2 Acc 0.8722, AUC 0.9339039921760559, avg_entr 0.015191244892776012, f1 0.872200071811676
ep19_l2_test_time 3.306079626083374
Test Epoch19 layer3 Acc 0.872, AUC 0.9394078254699707, avg_entr 0.014356663450598717, f1 0.871999979019165
ep19_l3_test_time 4.638891696929932
Test Epoch19 layer4 Acc 0.872, AUC 0.9408296346664429, avg_entr 0.014082636684179306, f1 0.871999979019165
ep19_l4_test_time 5.924001693725586
gc 0
Train Epoch20 Acc 0.98 (39200/40000), AUC 0.9958707094192505
ep20_train_time 185.98459911346436
Test Epoch20 layer0 Acc 0.8808, AUC 0.9472549557685852, avg_entr 0.11970338225364685, f1 0.8808000087738037
ep20_l0_test_time 0.7670090198516846
Test Epoch20 layer1 Acc 0.8716, AUC 0.9238476753234863, avg_entr 0.0228769201785326, f1 0.8715999722480774
ep20_l1_test_time 2.0715975761413574
Test Epoch20 layer2 Acc 0.8714, AUC 0.933678388595581, avg_entr 0.014834867790341377, f1 0.871399998664856
ep20_l2_test_time 3.303379774093628
Test Epoch20 layer3 Acc 0.8722, AUC 0.9390676617622375, avg_entr 0.014378399588167667, f1 0.872200071811676
ep20_l3_test_time 4.646311044692993
Test Epoch20 layer4 Acc 0.8718, AUC 0.9406326413154602, avg_entr 0.013878250494599342, f1 0.8718000650405884
ep20_l4_test_time 5.923338890075684
gc 0
Train Epoch21 Acc 0.980175 (39207/40000), AUC 0.996192455291748
ep21_train_time 185.92689990997314
Test Epoch21 layer0 Acc 0.884, AUC 0.9471104145050049, avg_entr 0.12011183053255081, f1 0.8840000033378601
ep21_l0_test_time 0.7643978595733643
Test Epoch21 layer1 Acc 0.8722, AUC 0.9236275553703308, avg_entr 0.022126438096165657, f1 0.872200071811676
ep21_l1_test_time 2.0602681636810303
Test Epoch21 layer2 Acc 0.8724, AUC 0.9316249489784241, avg_entr 0.014501619152724743, f1 0.8723999857902527
ep21_l2_test_time 3.350205421447754
Test Epoch21 layer3 Acc 0.8716, AUC 0.9375998973846436, avg_entr 0.013677160255610943, f1 0.8715999722480774
ep21_l3_test_time 4.642794847488403
Test Epoch21 layer4 Acc 0.8716, AUC 0.9402443170547485, avg_entr 0.013429299928247929, f1 0.8715999722480774
ep21_l4_test_time 5.965457439422607
gc 0
Train Epoch22 Acc 0.98015 (39206/40000), AUC 0.9957900047302246
ep22_train_time 185.89856386184692
Test Epoch22 layer0 Acc 0.8836, AUC 0.9469341039657593, avg_entr 0.11894212663173676, f1 0.8835999965667725
ep22_l0_test_time 0.7731704711914062
Test Epoch22 layer1 Acc 0.8718, AUC 0.9231802225112915, avg_entr 0.02198720909655094, f1 0.8718000650405884
ep22_l1_test_time 2.0884361267089844
Test Epoch22 layer2 Acc 0.8712, AUC 0.9314875602722168, avg_entr 0.014648092910647392, f1 0.8712000846862793
ep22_l2_test_time 3.3124892711639404
Test Epoch22 layer3 Acc 0.8712, AUC 0.9376389384269714, avg_entr 0.013980729505419731, f1 0.8712000846862793
ep22_l3_test_time 4.632333040237427
Test Epoch22 layer4 Acc 0.8714, AUC 0.9400460720062256, avg_entr 0.013745575211942196, f1 0.871399998664856
ep22_l4_test_time 5.955416202545166
gc 0
Train Epoch23 Acc 0.98015 (39206/40000), AUC 0.9961787462234497
ep23_train_time 186.04775404930115
Test Epoch23 layer0 Acc 0.8832, AUC 0.9468166828155518, avg_entr 0.11859084665775299, f1 0.8831999897956848
ep23_l0_test_time 0.7792680263519287
Test Epoch23 layer1 Acc 0.871, AUC 0.9227653741836548, avg_entr 0.02195507474243641, f1 0.8709999918937683
ep23_l1_test_time 2.0675201416015625
Test Epoch23 layer2 Acc 0.8716, AUC 0.9318176507949829, avg_entr 0.014250076375901699, f1 0.8715999722480774
ep23_l2_test_time 3.314544439315796
Test Epoch23 layer3 Acc 0.8718, AUC 0.9376242160797119, avg_entr 0.01377545204013586, f1 0.8718000650405884
ep23_l3_test_time 4.635133743286133
Test Epoch23 layer4 Acc 0.872, AUC 0.9399373531341553, avg_entr 0.013252406381070614, f1 0.871999979019165
ep23_l4_test_time 5.961566686630249
gc 0
Train Epoch24 Acc 0.980525 (39221/40000), AUC 0.9961823225021362
ep24_train_time 185.74216866493225
Test Epoch24 layer0 Acc 0.8832, AUC 0.9467432498931885, avg_entr 0.11810438334941864, f1 0.8831999897956848
ep24_l0_test_time 0.771552324295044
Test Epoch24 layer1 Acc 0.8708, AUC 0.922639787197113, avg_entr 0.021484877914190292, f1 0.8708000183105469
ep24_l1_test_time 2.0642552375793457
Test Epoch24 layer2 Acc 0.8716, AUC 0.9307156801223755, avg_entr 0.013913139700889587, f1 0.8715999722480774
ep24_l2_test_time 3.307039499282837
Test Epoch24 layer3 Acc 0.8722, AUC 0.936734676361084, avg_entr 0.013391105458140373, f1 0.872200071811676
ep24_l3_test_time 4.642286062240601
Test Epoch24 layer4 Acc 0.872, AUC 0.9397475719451904, avg_entr 0.012965837493538857, f1 0.871999979019165
ep24_l4_test_time 5.957448244094849
gc 0
Train Epoch25 Acc 0.9807 (39228/40000), AUC 0.9961431622505188
ep25_train_time 185.79307770729065
Test Epoch25 layer0 Acc 0.8822, AUC 0.9466947317123413, avg_entr 0.11757128685712814, f1 0.8822000026702881
ep25_l0_test_time 0.7708508968353271
Test Epoch25 layer1 Acc 0.8712, AUC 0.9227088689804077, avg_entr 0.021689098328351974, f1 0.8712000846862793
ep25_l1_test_time 2.0604400634765625
Test Epoch25 layer2 Acc 0.8716, AUC 0.9306513071060181, avg_entr 0.013891229405999184, f1 0.8715999722480774
ep25_l2_test_time 3.34724497795105
Test Epoch25 layer3 Acc 0.872, AUC 0.9365321397781372, avg_entr 0.013394351117312908, f1 0.871999979019165
ep25_l3_test_time 4.63611364364624
Test Epoch25 layer4 Acc 0.8716, AUC 0.9396522045135498, avg_entr 0.012882882729172707, f1 0.8715999722480774
ep25_l4_test_time 6.020725727081299
gc 0
Train Epoch26 Acc 0.98075 (39230/40000), AUC 0.9961168766021729
ep26_train_time 118.74690961837769
Test Epoch26 layer0 Acc 0.8834, AUC 0.9465782642364502, avg_entr 0.11685861647129059, f1 0.8834000825881958
ep26_l0_test_time 0.7676501274108887
Test Epoch26 layer1 Acc 0.871, AUC 0.9235378503799438, avg_entr 0.02152092754840851, f1 0.8709999918937683
ep26_l1_test_time 2.072030544281006
Test Epoch26 layer2 Acc 0.8716, AUC 0.9313837289810181, avg_entr 0.014337071217596531, f1 0.8715999722480774
ep26_l2_test_time 3.2881381511688232
Test Epoch26 layer3 Acc 0.872, AUC 0.9363000392913818, avg_entr 0.013772416859865189, f1 0.871999979019165
ep26_l3_test_time 4.6103925704956055
Test Epoch26 layer4 Acc 0.8718, AUC 0.9394683837890625, avg_entr 0.013247353956103325, f1 0.8718000650405884
ep26_l4_test_time 5.9160990715026855
gc 0
Train Epoch27 Acc 0.980775 (39231/40000), AUC 0.9960153102874756
ep27_train_time 185.19788336753845
Test Epoch27 layer0 Acc 0.8828, AUC 0.9465388655662537, avg_entr 0.11650358885526657, f1 0.8827999830245972
ep27_l0_test_time 0.7698049545288086
Test Epoch27 layer1 Acc 0.871, AUC 0.9220786690711975, avg_entr 0.021045023575425148, f1 0.8709999918937683
ep27_l1_test_time 2.0868518352508545
Test Epoch27 layer2 Acc 0.8718, AUC 0.929526686668396, avg_entr 0.01350135076791048, f1 0.8718000650405884
ep27_l2_test_time 3.275390863418579
Test Epoch27 layer3 Acc 0.8722, AUC 0.9354844689369202, avg_entr 0.012974602170288563, f1 0.872200071811676
ep27_l3_test_time 4.595853805541992
Test Epoch27 layer4 Acc 0.872, AUC 0.9393008947372437, avg_entr 0.012553190812468529, f1 0.871999979019165
ep27_l4_test_time 5.895289897918701
gc 0
Train Epoch28 Acc 0.980725 (39229/40000), AUC 0.9962729215621948
ep28_train_time 185.26039481163025
Test Epoch28 layer0 Acc 0.883, AUC 0.9465124607086182, avg_entr 0.1166301742196083, f1 0.8830000162124634
ep28_l0_test_time 0.7589926719665527
Test Epoch28 layer1 Acc 0.8726, AUC 0.9220160245895386, avg_entr 0.02091691642999649, f1 0.8726000189781189
ep28_l1_test_time 2.0493974685668945
Test Epoch28 layer2 Acc 0.872, AUC 0.9293606877326965, avg_entr 0.013571078889071941, f1 0.871999979019165
ep28_l2_test_time 3.305147647857666
Test Epoch28 layer3 Acc 0.872, AUC 0.9354193210601807, avg_entr 0.012969834730029106, f1 0.871999979019165
ep28_l3_test_time 4.591708183288574
Test Epoch28 layer4 Acc 0.8718, AUC 0.939228892326355, avg_entr 0.012579535134136677, f1 0.8718000650405884
ep28_l4_test_time 5.901407957077026
gc 0
Train Epoch29 Acc 0.9809 (39236/40000), AUC 0.9964035749435425
ep29_train_time 185.76656651496887
Test Epoch29 layer0 Acc 0.8824, AUC 0.9464812874794006, avg_entr 0.11626517027616501, f1 0.8823999762535095
ep29_l0_test_time 0.7780783176422119
Test Epoch29 layer1 Acc 0.8716, AUC 0.921756386756897, avg_entr 0.02098093554377556, f1 0.8715999722480774
ep29_l1_test_time 2.0261070728302
Test Epoch29 layer2 Acc 0.8716, AUC 0.9294660091400146, avg_entr 0.013511610217392445, f1 0.8715999722480774
ep29_l2_test_time 3.2640221118927
Test Epoch29 layer3 Acc 0.8722, AUC 0.9353910684585571, avg_entr 0.013056361116468906, f1 0.872200071811676
ep29_l3_test_time 4.393759489059448
Test Epoch29 layer4 Acc 0.8722, AUC 0.939231276512146, avg_entr 0.012620611116290092, f1 0.872200071811676
ep29_l4_test_time 5.84307336807251
Best AUC tensor(0.8988) 2 1
train_loss (2, 5, 30)
valid_acc (5, 30)
valid_AUC (5, 30)
train_acc (30,)
total_train+valid_time 5993.213824748993
Start Testing
Load ckpt at ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt
Test layer0 Acc 0.885, AUC 0.9541571140289307, avg_entr 0.22313061356544495, f1 0.8849999904632568
l0_test_time 0.7651166915893555
Test layer1 Acc 0.8916, AUC 0.9564322829246521, avg_entr 0.15567557513713837, f1 0.8916000127792358
l1_test_time 2.057391881942749
Test layer2 Acc 0.8908, AUC 0.9560494422912598, avg_entr 0.0934298112988472, f1 0.8907999992370605
l2_test_time 3.2875547409057617
Test layer3 Acc 0.8916, AUC 0.9560673236846924, avg_entr 0.06310445815324783, f1 0.8916000127792358
l3_test_time 4.623639345169067
Test layer4 Acc 0.8902, AUC 0.956134557723999, avg_entr 0.05481560528278351, f1 0.8902000188827515
l4_test_time 5.908146381378174
