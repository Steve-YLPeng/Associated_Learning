total count words 222751
vocab size 30000
train size 40000, valid size 5000, test size 5000
found 27937 words in glove
model: TransformerModelML(
  (layers): ModuleList(
    (0): EMBLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Embedding(30000, 300)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=2, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=2, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.weight 9000000
layers.0.ae.g.0.weight 256
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 256
layers.0.ae.h.0.bias 2
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
Total Trainable Params: 13671778
init_time 22.521484375
Start Training
gc 0
Train Epoch0 Acc 0.513425 (20537/40000), AUC 0.5240705013275146
ep0_train_time 183.6501441001892
Test Epoch0 layer0 Acc 0.8124, AUC 0.88950514793396, avg_entr 0.5584216117858887, f1 0.8123999238014221
ep0_l0_test_time 0.770768404006958
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 0
Test Epoch0 layer1 Acc 0.8308, AUC 0.9124366044998169, avg_entr 0.35113102197647095, f1 0.8307999968528748
ep0_l1_test_time 2.0488338470458984
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 0
Test Epoch0 layer2 Acc 0.833, AUC 0.9163208603858948, avg_entr 0.45611411333084106, f1 0.8330000042915344
ep0_l2_test_time 3.2171850204467773
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 0
Test Epoch0 layer3 Acc 0.8372, AUC 0.9148489832878113, avg_entr 0.582383394241333, f1 0.8371999859809875
ep0_l3_test_time 4.561676025390625
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 0
Test Epoch0 layer4 Acc 0.841, AUC 0.915209949016571, avg_entr 0.6621020436286926, f1 0.8410000205039978
ep0_l4_test_time 5.7800750732421875
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 0
gc 0
Train Epoch1 Acc 0.866375 (34655/40000), AUC 0.9339574575424194
ep1_train_time 182.51994729042053
Test Epoch1 layer0 Acc 0.8624, AUC 0.9449432492256165, avg_entr 0.28708916902542114, f1 0.8623999953269958
ep1_l0_test_time 0.7712593078613281
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 1
Test Epoch1 layer1 Acc 0.8786, AUC 0.9549136161804199, avg_entr 0.1931295245885849, f1 0.878600001335144
ep1_l1_test_time 2.0532925128936768
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 1
Test Epoch1 layer2 Acc 0.8812, AUC 0.9558756351470947, avg_entr 0.17121575772762299, f1 0.8812000155448914
ep1_l2_test_time 3.2099125385284424
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 1
Test Epoch1 layer3 Acc 0.8846, AUC 0.9554789066314697, avg_entr 0.1477814018726349, f1 0.8845999836921692
ep1_l3_test_time 4.563072204589844
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 1
Test Epoch1 layer4 Acc 0.8888, AUC 0.9554687738418579, avg_entr 0.12349129468202591, f1 0.8888000249862671
ep1_l4_test_time 5.787951707839966
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 1
gc 0
Train Epoch2 Acc 0.9139 (36556/40000), AUC 0.966722309589386
ep2_train_time 162.45203638076782
Test Epoch2 layer0 Acc 0.8892, AUC 0.9554386138916016, avg_entr 0.22534829378128052, f1 0.88919997215271
ep2_l0_test_time 0.7067315578460693
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 2
Test Epoch2 layer1 Acc 0.8788, AUC 0.958764374256134, avg_entr 0.1362002193927765, f1 0.8787999749183655
ep2_l1_test_time 1.8932504653930664
Test Epoch2 layer2 Acc 0.8776, AUC 0.958506166934967, avg_entr 0.09117813408374786, f1 0.8776000142097473
ep2_l2_test_time 3.0132522583007812
Test Epoch2 layer3 Acc 0.876, AUC 0.9582219123840332, avg_entr 0.07482859492301941, f1 0.8760000467300415
ep2_l3_test_time 4.190749645233154
Test Epoch2 layer4 Acc 0.874, AUC 0.9585118293762207, avg_entr 0.059373728930950165, f1 0.8740000128746033
ep2_l4_test_time 5.38205885887146
gc 0
Train Epoch3 Acc 0.94115 (37646/40000), AUC 0.9792963266372681
ep3_train_time 167.81550645828247
Test Epoch3 layer0 Acc 0.891, AUC 0.9582339525222778, avg_entr 0.1926826387643814, f1 0.890999972820282
ep3_l0_test_time 0.7173686027526855
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 3
Test Epoch3 layer1 Acc 0.89, AUC 0.9564090371131897, avg_entr 0.0764087662100792, f1 0.8899999856948853
ep3_l1_test_time 1.8896820545196533
Test Epoch3 layer2 Acc 0.8926, AUC 0.9547500610351562, avg_entr 0.04511770233511925, f1 0.8925999999046326
ep3_l2_test_time 3.011164665222168
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 3
Test Epoch3 layer3 Acc 0.8932, AUC 0.9565757513046265, avg_entr 0.03923361375927925, f1 0.8931999802589417
ep3_l3_test_time 4.157238245010376
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 3
Test Epoch3 layer4 Acc 0.8926, AUC 0.9569041728973389, avg_entr 0.033972643315792084, f1 0.8925999999046326
ep3_l4_test_time 5.381832599639893
gc 0
Train Epoch4 Acc 0.95005 (38002/40000), AUC 0.9831125140190125
ep4_train_time 155.96238613128662
Test Epoch4 layer0 Acc 0.8878, AUC 0.9586394429206848, avg_entr 0.17676973342895508, f1 0.8877999782562256
ep4_l0_test_time 0.7050199508666992
Test Epoch4 layer1 Acc 0.8866, AUC 0.9522730112075806, avg_entr 0.05252096429467201, f1 0.8866000771522522
ep4_l1_test_time 1.8923897743225098
Test Epoch4 layer2 Acc 0.8872, AUC 0.9527567028999329, avg_entr 0.03626873344182968, f1 0.8871999979019165
ep4_l2_test_time 2.9563426971435547
Test Epoch4 layer3 Acc 0.8858, AUC 0.9550491571426392, avg_entr 0.03140611946582794, f1 0.8858000040054321
ep4_l3_test_time 4.192598819732666
Test Epoch4 layer4 Acc 0.886, AUC 0.9550350904464722, avg_entr 0.02815008908510208, f1 0.8859999775886536
ep4_l4_test_time 5.389025449752808
gc 0
Train Epoch5 Acc 0.9574 (38296/40000), AUC 0.9854826331138611
ep5_train_time 167.79686284065247
Test Epoch5 layer0 Acc 0.8958, AUC 0.9580100774765015, avg_entr 0.16761796176433563, f1 0.895799994468689
ep5_l0_test_time 0.6956181526184082
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 5
Test Epoch5 layer1 Acc 0.886, AUC 0.945651650428772, avg_entr 0.04349718987941742, f1 0.8859999775886536
ep5_l1_test_time 1.8929402828216553
Test Epoch5 layer2 Acc 0.8868, AUC 0.9500025510787964, avg_entr 0.03300850838422775, f1 0.8867999911308289
ep5_l2_test_time 2.9702868461608887
Test Epoch5 layer3 Acc 0.8854, AUC 0.9521301984786987, avg_entr 0.030621597543358803, f1 0.8853999972343445
ep5_l3_test_time 4.210878610610962
Test Epoch5 layer4 Acc 0.8856, AUC 0.9519443511962891, avg_entr 0.028604241088032722, f1 0.8855999708175659
ep5_l4_test_time 5.386654376983643
gc 0
Train Epoch6 Acc 0.963475 (38539/40000), AUC 0.9883815050125122
ep6_train_time 156.25581049919128
Test Epoch6 layer0 Acc 0.8968, AUC 0.9570431709289551, avg_entr 0.15343718230724335, f1 0.8967999815940857
ep6_l0_test_time 0.6647276878356934
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 6
Test Epoch6 layer1 Acc 0.8824, AUC 0.9454952478408813, avg_entr 0.03767421841621399, f1 0.8823999762535095
ep6_l1_test_time 1.8957664966583252
Test Epoch6 layer2 Acc 0.8828, AUC 0.9500899910926819, avg_entr 0.029314810410141945, f1 0.8827999830245972
ep6_l2_test_time 2.9966609477996826
Test Epoch6 layer3 Acc 0.883, AUC 0.9510557651519775, avg_entr 0.027511296793818474, f1 0.8830000162124634
ep6_l3_test_time 4.230757236480713
Test Epoch6 layer4 Acc 0.883, AUC 0.9507961273193359, avg_entr 0.025318969041109085, f1 0.8830000162124634
ep6_l4_test_time 5.337216377258301
gc 0
Train Epoch7 Acc 0.9667 (38668/40000), AUC 0.9904969930648804
ep7_train_time 167.75183081626892
Test Epoch7 layer0 Acc 0.8908, AUC 0.9553393125534058, avg_entr 0.1499296873807907, f1 0.8907999992370605
ep7_l0_test_time 0.6795358657836914
Test Epoch7 layer1 Acc 0.8788, AUC 0.9373624920845032, avg_entr 0.03325432911515236, f1 0.8787999749183655
ep7_l1_test_time 1.8954017162322998
Test Epoch7 layer2 Acc 0.881, AUC 0.9471412301063538, avg_entr 0.025876883417367935, f1 0.8809999823570251
ep7_l2_test_time 2.9986507892608643
Test Epoch7 layer3 Acc 0.8804, AUC 0.9483463764190674, avg_entr 0.024431688711047173, f1 0.8804000020027161
ep7_l3_test_time 4.206995487213135
Test Epoch7 layer4 Acc 0.88, AUC 0.9482027292251587, avg_entr 0.022867493331432343, f1 0.8799999952316284
ep7_l4_test_time 5.392755746841431
gc 0
Train Epoch8 Acc 0.96925 (38770/40000), AUC 0.9911636114120483
ep8_train_time 156.09087872505188
Test Epoch8 layer0 Acc 0.8938, AUC 0.9538690447807312, avg_entr 0.139541357755661, f1 0.8938000202178955
ep8_l0_test_time 0.7052972316741943
Test Epoch8 layer1 Acc 0.8776, AUC 0.9366096258163452, avg_entr 0.029293084517121315, f1 0.8776000142097473
ep8_l1_test_time 1.894190788269043
Test Epoch8 layer2 Acc 0.8786, AUC 0.945350706577301, avg_entr 0.023064574226737022, f1 0.878600001335144
ep8_l2_test_time 3.0178775787353516
Test Epoch8 layer3 Acc 0.8782, AUC 0.9463241100311279, avg_entr 0.02186519093811512, f1 0.8781999945640564
ep8_l3_test_time 4.211289644241333
Test Epoch8 layer4 Acc 0.8786, AUC 0.9460475444793701, avg_entr 0.020206118002533913, f1 0.878600001335144
ep8_l4_test_time 5.390051364898682
gc 0
Train Epoch9 Acc 0.9728 (38912/40000), AUC 0.9928185939788818
ep9_train_time 167.77945017814636
Test Epoch9 layer0 Acc 0.8896, AUC 0.9526991248130798, avg_entr 0.13803084194660187, f1 0.8895999193191528
ep9_l0_test_time 0.6925954818725586
Test Epoch9 layer1 Acc 0.8778, AUC 0.9350396394729614, avg_entr 0.028084801509976387, f1 0.8778000473976135
ep9_l1_test_time 1.8645198345184326
Test Epoch9 layer2 Acc 0.878, AUC 0.9442195296287537, avg_entr 0.021543970331549644, f1 0.878000020980835
ep9_l2_test_time 2.991734504699707
Test Epoch9 layer3 Acc 0.8778, AUC 0.9453327655792236, avg_entr 0.020534617826342583, f1 0.8778000473976135
ep9_l3_test_time 4.1950719356536865
Test Epoch9 layer4 Acc 0.878, AUC 0.9450125694274902, avg_entr 0.019235724583268166, f1 0.878000020980835
ep9_l4_test_time 5.407074928283691
gc 0
Train Epoch10 Acc 0.973725 (38949/40000), AUC 0.9920468330383301
ep10_train_time 156.21375107765198
Test Epoch10 layer0 Acc 0.8874, AUC 0.9517196416854858, avg_entr 0.13697311282157898, f1 0.8873999714851379
ep10_l0_test_time 0.6897859573364258
Test Epoch10 layer1 Acc 0.8746, AUC 0.9325659275054932, avg_entr 0.027713607996702194, f1 0.8745999932289124
ep10_l1_test_time 1.890754222869873
Test Epoch10 layer2 Acc 0.8744, AUC 0.9434431791305542, avg_entr 0.02171136438846588, f1 0.8744000792503357
ep10_l2_test_time 3.0096919536590576
Test Epoch10 layer3 Acc 0.8754, AUC 0.9442781209945679, avg_entr 0.01991286501288414, f1 0.8754000067710876
ep10_l3_test_time 4.187964916229248
Test Epoch10 layer4 Acc 0.8744, AUC 0.9439680576324463, avg_entr 0.018478449434041977, f1 0.8744000792503357
ep10_l4_test_time 5.3371899127960205
gc 0
Train Epoch11 Acc 0.975275 (39011/40000), AUC 0.9937138557434082
ep11_train_time 167.75992941856384
Test Epoch11 layer0 Acc 0.8868, AUC 0.9504058957099915, avg_entr 0.13235244154930115, f1 0.8867999911308289
ep11_l0_test_time 0.6823503971099854
Test Epoch11 layer1 Acc 0.8734, AUC 0.9303529262542725, avg_entr 0.026244308799505234, f1 0.8733999729156494
ep11_l1_test_time 1.8889248371124268
Test Epoch11 layer2 Acc 0.874, AUC 0.9408676028251648, avg_entr 0.020167117938399315, f1 0.8740000128746033
ep11_l2_test_time 3.007850408554077
Test Epoch11 layer3 Acc 0.874, AUC 0.9428982138633728, avg_entr 0.019076161086559296, f1 0.8740000128746033
ep11_l3_test_time 4.208445072174072
Test Epoch11 layer4 Acc 0.8738, AUC 0.942560076713562, avg_entr 0.01792098395526409, f1 0.8737999796867371
ep11_l4_test_time 5.391583442687988
gc 0
Train Epoch12 Acc 0.97675 (39070/40000), AUC 0.994098424911499
ep12_train_time 124.19157361984253
Test Epoch12 layer0 Acc 0.8864, AUC 0.9492212533950806, avg_entr 0.12895920872688293, f1 0.8863999843597412
ep12_l0_test_time 0.5596404075622559
Test Epoch12 layer1 Acc 0.873, AUC 0.927773654460907, avg_entr 0.024587435647845268, f1 0.8730000257492065
ep12_l1_test_time 1.1908140182495117
Test Epoch12 layer2 Acc 0.8736, AUC 0.9385777711868286, avg_entr 0.01816564053297043, f1 0.8736000061035156
ep12_l2_test_time 1.8162007331848145
Test Epoch12 layer3 Acc 0.8734, AUC 0.9415667057037354, avg_entr 0.017261497676372528, f1 0.8733999729156494
ep12_l3_test_time 2.443498134613037
Test Epoch12 layer4 Acc 0.8736, AUC 0.9410718083381653, avg_entr 0.01607692427933216, f1 0.8736000061035156
ep12_l4_test_time 3.0751612186431885
gc 0
Train Epoch13 Acc 0.9778 (39112/40000), AUC 0.9944791197776794
ep13_train_time 131.61004114151
Test Epoch13 layer0 Acc 0.8854, AUC 0.9487365484237671, avg_entr 0.1279875934123993, f1 0.8853999972343445
ep13_l0_test_time 0.7769870758056641
Test Epoch13 layer1 Acc 0.8722, AUC 0.9272988438606262, avg_entr 0.02552741765975952, f1 0.872200071811676
ep13_l1_test_time 2.0379674434661865
Test Epoch13 layer2 Acc 0.8716, AUC 0.938431441783905, avg_entr 0.018789878115057945, f1 0.8715999722480774
ep13_l2_test_time 3.272310495376587
Test Epoch13 layer3 Acc 0.8716, AUC 0.9412288665771484, avg_entr 0.018118474632501602, f1 0.8715999722480774
ep13_l3_test_time 4.563684701919556
Test Epoch13 layer4 Acc 0.8716, AUC 0.9409311413764954, avg_entr 0.01696651242673397, f1 0.8715999722480774
ep13_l4_test_time 5.794400691986084
gc 0
Train Epoch14 Acc 0.978375 (39135/40000), AUC 0.9945487976074219
ep14_train_time 183.35730457305908
Test Epoch14 layer0 Acc 0.8854, AUC 0.9481096863746643, avg_entr 0.12638556957244873, f1 0.8853999972343445
ep14_l0_test_time 0.7540218830108643
Test Epoch14 layer1 Acc 0.8732, AUC 0.9244503974914551, avg_entr 0.023319978266954422, f1 0.873199999332428
ep14_l1_test_time 2.032712459564209
Test Epoch14 layer2 Acc 0.8732, AUC 0.9342951774597168, avg_entr 0.01634596288204193, f1 0.873199999332428
ep14_l2_test_time 3.2750511169433594
Test Epoch14 layer3 Acc 0.8728, AUC 0.9403220415115356, avg_entr 0.01553525123745203, f1 0.8727999925613403
ep14_l3_test_time 4.55277156829834
Test Epoch14 layer4 Acc 0.873, AUC 0.9394095540046692, avg_entr 0.014703418128192425, f1 0.8730000257492065
ep14_l4_test_time 5.814674139022827
gc 0
Train Epoch15 Acc 0.978775 (39151/40000), AUC 0.9949963092803955
ep15_train_time 183.309223651886
Test Epoch15 layer0 Acc 0.8838, AUC 0.9475827217102051, avg_entr 0.12511804699897766, f1 0.8838000297546387
ep15_l0_test_time 0.7265186309814453
Test Epoch15 layer1 Acc 0.8706, AUC 0.9246277809143066, avg_entr 0.023458197712898254, f1 0.8705999851226807
ep15_l1_test_time 2.0479469299316406
Test Epoch15 layer2 Acc 0.8712, AUC 0.9352484941482544, avg_entr 0.01745029352605343, f1 0.8712000846862793
ep15_l2_test_time 3.297600746154785
Test Epoch15 layer3 Acc 0.871, AUC 0.9393758177757263, avg_entr 0.016872648149728775, f1 0.8709999918937683
ep15_l3_test_time 4.55555272102356
Test Epoch15 layer4 Acc 0.871, AUC 0.9388782978057861, avg_entr 0.01596064679324627, f1 0.8709999918937683
ep15_l4_test_time 5.863656044006348
gc 0
Train Epoch16 Acc 0.979475 (39179/40000), AUC 0.9950927495956421
ep16_train_time 183.35890221595764
Test Epoch16 layer0 Acc 0.8836, AUC 0.9468836188316345, avg_entr 0.1224399283528328, f1 0.8835999965667725
ep16_l0_test_time 0.7244336605072021
Test Epoch16 layer1 Acc 0.8694, AUC 0.9223728179931641, avg_entr 0.02351090870797634, f1 0.8694000244140625
ep16_l1_test_time 2.0525317192077637
Test Epoch16 layer2 Acc 0.8684, AUC 0.9333671927452087, avg_entr 0.018713844940066338, f1 0.868399977684021
ep16_l2_test_time 3.281942367553711
Test Epoch16 layer3 Acc 0.8688, AUC 0.9388813972473145, avg_entr 0.017886299639940262, f1 0.8687999844551086
ep16_l3_test_time 4.555079936981201
Test Epoch16 layer4 Acc 0.8684, AUC 0.9382076263427734, avg_entr 0.016977792605757713, f1 0.868399977684021
ep16_l4_test_time 5.87184476852417
gc 0
Train Epoch17 Acc 0.980125 (39205/40000), AUC 0.9952961802482605
ep17_train_time 183.16007804870605
Test Epoch17 layer0 Acc 0.883, AUC 0.9465932250022888, avg_entr 0.12264976650476456, f1 0.8830000162124634
ep17_l0_test_time 0.7526319026947021
Test Epoch17 layer1 Acc 0.8704, AUC 0.9221701622009277, avg_entr 0.021801287308335304, f1 0.8704000115394592
ep17_l1_test_time 2.039649486541748
Test Epoch17 layer2 Acc 0.8706, AUC 0.9294384717941284, avg_entr 0.014935060404241085, f1 0.8705999851226807
ep17_l2_test_time 3.2714016437530518
Test Epoch17 layer3 Acc 0.8706, AUC 0.9379913806915283, avg_entr 0.014356684871017933, f1 0.8705999851226807
ep17_l3_test_time 4.55795693397522
Test Epoch17 layer4 Acc 0.871, AUC 0.9374222159385681, avg_entr 0.01329053845256567, f1 0.8709999918937683
ep17_l4_test_time 5.847350358963013
gc 0
Train Epoch18 Acc 0.980275 (39211/40000), AUC 0.9953891038894653
ep18_train_time 183.19261288642883
Test Epoch18 layer0 Acc 0.8828, AUC 0.9463192820549011, avg_entr 0.12111243605613708, f1 0.8827999830245972
ep18_l0_test_time 0.7631525993347168
Test Epoch18 layer1 Acc 0.8708, AUC 0.9207742214202881, avg_entr 0.020990148186683655, f1 0.8708000183105469
ep18_l1_test_time 2.033188581466675
Test Epoch18 layer2 Acc 0.87, AUC 0.9266649484634399, avg_entr 0.014347158372402191, f1 0.8700000047683716
ep18_l2_test_time 3.257392406463623
Test Epoch18 layer3 Acc 0.8704, AUC 0.937204122543335, avg_entr 0.013720780611038208, f1 0.8704000115394592
ep18_l3_test_time 4.563486576080322
Test Epoch18 layer4 Acc 0.8696, AUC 0.9365018606185913, avg_entr 0.01276128739118576, f1 0.8695999383926392
ep18_l4_test_time 5.8481316566467285
gc 0
Train Epoch19 Acc 0.98075 (39230/40000), AUC 0.9955470561981201
ep19_train_time 183.19981980323792
Test Epoch19 layer0 Acc 0.8828, AUC 0.9460009932518005, avg_entr 0.12010587006807327, f1 0.8827999830245972
ep19_l0_test_time 0.7625412940979004
Test Epoch19 layer1 Acc 0.8692, AUC 0.921277642250061, avg_entr 0.02101529948413372, f1 0.8691999912261963
ep19_l1_test_time 2.020693302154541
Test Epoch19 layer2 Acc 0.8692, AUC 0.9281772375106812, avg_entr 0.014974397607147694, f1 0.8691999912261963
ep19_l2_test_time 3.264200210571289
Test Epoch19 layer3 Acc 0.8682, AUC 0.9368665218353271, avg_entr 0.014332229271531105, f1 0.8682000041007996
ep19_l3_test_time 4.560381889343262
Test Epoch19 layer4 Acc 0.8684, AUC 0.9366730451583862, avg_entr 0.013450060971081257, f1 0.868399977684021
ep19_l4_test_time 5.871619939804077
gc 0
Train Epoch20 Acc 0.9811 (39244/40000), AUC 0.9955633878707886
ep20_train_time 183.21650385856628
Test Epoch20 layer0 Acc 0.8788, AUC 0.945615291595459, avg_entr 0.11841677129268646, f1 0.8787999749183655
ep20_l0_test_time 0.7313442230224609
Test Epoch20 layer1 Acc 0.8696, AUC 0.9205211400985718, avg_entr 0.02114574797451496, f1 0.8695999383926392
ep20_l1_test_time 2.02718448638916
Test Epoch20 layer2 Acc 0.8696, AUC 0.9284195899963379, avg_entr 0.015362164005637169, f1 0.8695999383926392
ep20_l2_test_time 3.2853055000305176
Test Epoch20 layer3 Acc 0.869, AUC 0.9365111589431763, avg_entr 0.014871337451040745, f1 0.8690000176429749
ep20_l3_test_time 4.583613872528076
Test Epoch20 layer4 Acc 0.8692, AUC 0.9361162185668945, avg_entr 0.014100631698966026, f1 0.8691999912261963
ep20_l4_test_time 5.87483286857605
gc 0
Train Epoch21 Acc 0.9813 (39252/40000), AUC 0.9957131147384644
ep21_train_time 183.10795879364014
Test Epoch21 layer0 Acc 0.8802, AUC 0.9454750418663025, avg_entr 0.11813825368881226, f1 0.8802000284194946
ep21_l0_test_time 0.7590842247009277
Test Epoch21 layer1 Acc 0.8692, AUC 0.9205533266067505, avg_entr 0.021743111312389374, f1 0.8691999912261963
ep21_l1_test_time 2.0403025150299072
Test Epoch21 layer2 Acc 0.8684, AUC 0.9269222617149353, avg_entr 0.015638498589396477, f1 0.868399977684021
ep21_l2_test_time 3.2933895587921143
Test Epoch21 layer3 Acc 0.8686, AUC 0.9359925389289856, avg_entr 0.015093497931957245, f1 0.8686000108718872
ep21_l3_test_time 4.548931121826172
Test Epoch21 layer4 Acc 0.8684, AUC 0.9355243444442749, avg_entr 0.014309046790003777, f1 0.868399977684021
ep21_l4_test_time 5.847379922866821
gc 0
Train Epoch22 Acc 0.98185 (39274/40000), AUC 0.995567798614502
ep22_train_time 164.8875002861023
Test Epoch22 layer0 Acc 0.881, AUC 0.9453138113021851, avg_entr 0.11771978437900543, f1 0.8809999823570251
ep22_l0_test_time 0.7397809028625488
Test Epoch22 layer1 Acc 0.8678, AUC 0.9193359613418579, avg_entr 0.02114560827612877, f1 0.8677999973297119
ep22_l1_test_time 2.035524606704712
Test Epoch22 layer2 Acc 0.8678, AUC 0.9252005219459534, avg_entr 0.015174641273915768, f1 0.8677999973297119
ep22_l2_test_time 3.2285871505737305
Test Epoch22 layer3 Acc 0.8678, AUC 0.935289740562439, avg_entr 0.014638712629675865, f1 0.8677999973297119
ep22_l3_test_time 4.5686352252960205
Test Epoch22 layer4 Acc 0.868, AUC 0.934766411781311, avg_entr 0.013870456255972385, f1 0.8679999709129333
ep22_l4_test_time 5.878522634506226
gc 0
Train Epoch23 Acc 0.981575 (39263/40000), AUC 0.9961550235748291
ep23_train_time 183.3663067817688
Test Epoch23 layer0 Acc 0.88, AUC 0.9451979398727417, avg_entr 0.11747003346681595, f1 0.8799999952316284
ep23_l0_test_time 0.7317655086517334
Test Epoch23 layer1 Acc 0.8686, AUC 0.9192402362823486, avg_entr 0.020590491592884064, f1 0.8686000108718872
ep23_l1_test_time 2.0154035091400146
Test Epoch23 layer2 Acc 0.8688, AUC 0.9240159392356873, avg_entr 0.014434415847063065, f1 0.8687999844551086
ep23_l2_test_time 3.282747268676758
Test Epoch23 layer3 Acc 0.8684, AUC 0.93496173620224, avg_entr 0.01387057825922966, f1 0.868399977684021
ep23_l3_test_time 4.551136255264282
Test Epoch23 layer4 Acc 0.8684, AUC 0.9347113966941833, avg_entr 0.013004415668547153, f1 0.868399977684021
ep23_l4_test_time 5.875251054763794
gc 0
Train Epoch24 Acc 0.982 (39280/40000), AUC 0.9956670999526978
ep24_train_time 183.3562159538269
Test Epoch24 layer0 Acc 0.879, AUC 0.945041298866272, avg_entr 0.11777465790510178, f1 0.8790000081062317
ep24_l0_test_time 0.7561323642730713
Test Epoch24 layer1 Acc 0.8682, AUC 0.9188774824142456, avg_entr 0.02021348848938942, f1 0.8682000041007996
ep24_l1_test_time 1.9873037338256836
Test Epoch24 layer2 Acc 0.8674, AUC 0.9223470091819763, avg_entr 0.013593876734375954, f1 0.8673999905586243
ep24_l2_test_time 3.300635576248169
Test Epoch24 layer3 Acc 0.8674, AUC 0.9342432022094727, avg_entr 0.012918046675622463, f1 0.8673999905586243
ep24_l3_test_time 4.565915584564209
Test Epoch24 layer4 Acc 0.8672, AUC 0.9340327978134155, avg_entr 0.01207359042018652, f1 0.8672000169754028
ep24_l4_test_time 5.865525245666504
gc 0
Train Epoch25 Acc 0.982 (39280/40000), AUC 0.9959303140640259
ep25_train_time 183.28688192367554
Test Epoch25 layer0 Acc 0.8808, AUC 0.9449567794799805, avg_entr 0.11637220531702042, f1 0.8808000087738037
ep25_l0_test_time 0.7340202331542969
Test Epoch25 layer1 Acc 0.869, AUC 0.9185802340507507, avg_entr 0.020164722576737404, f1 0.8690000176429749
ep25_l1_test_time 2.0344927310943604
Test Epoch25 layer2 Acc 0.8686, AUC 0.9216054677963257, avg_entr 0.013849430717527866, f1 0.8686000108718872
ep25_l2_test_time 3.163275718688965
Test Epoch25 layer3 Acc 0.8682, AUC 0.9338945746421814, avg_entr 0.013199880719184875, f1 0.8682000041007996
ep25_l3_test_time 4.530535936355591
Test Epoch25 layer4 Acc 0.868, AUC 0.9336177110671997, avg_entr 0.012366130948066711, f1 0.8679999709129333
ep25_l4_test_time 5.8712451457977295
gc 0
Train Epoch26 Acc 0.982025 (39281/40000), AUC 0.9959359765052795
ep26_train_time 183.40104866027832
Test Epoch26 layer0 Acc 0.8794, AUC 0.9448909759521484, avg_entr 0.11608309298753738, f1 0.8794000148773193
ep26_l0_test_time 0.7699577808380127
Test Epoch26 layer1 Acc 0.8688, AUC 0.918494701385498, avg_entr 0.02003292739391327, f1 0.8687999844551086
ep26_l1_test_time 2.0357086658477783
Test Epoch26 layer2 Acc 0.868, AUC 0.9209954738616943, avg_entr 0.01342216320335865, f1 0.8679999709129333
ep26_l2_test_time 3.2053356170654297
Test Epoch26 layer3 Acc 0.8672, AUC 0.9336115717887878, avg_entr 0.012829823419451714, f1 0.8672000169754028
ep26_l3_test_time 4.564924716949463
Test Epoch26 layer4 Acc 0.8674, AUC 0.9335845708847046, avg_entr 0.012023290619254112, f1 0.8673999905586243
ep26_l4_test_time 5.868412733078003
gc 0
Train Epoch27 Acc 0.982275 (39291/40000), AUC 0.9959571957588196
ep27_train_time 183.2214915752411
Test Epoch27 layer0 Acc 0.8788, AUC 0.9447919130325317, avg_entr 0.114886574447155, f1 0.8787999749183655
ep27_l0_test_time 0.7522873878479004
Test Epoch27 layer1 Acc 0.8688, AUC 0.9183928370475769, avg_entr 0.020117202773690224, f1 0.8687999844551086
ep27_l1_test_time 2.0290021896362305
Test Epoch27 layer2 Acc 0.8682, AUC 0.920576810836792, avg_entr 0.013898327946662903, f1 0.8682000041007996
ep27_l2_test_time 3.235743999481201
Test Epoch27 layer3 Acc 0.868, AUC 0.9333935976028442, avg_entr 0.013281932100653648, f1 0.8679999709129333
ep27_l3_test_time 4.562072038650513
Test Epoch27 layer4 Acc 0.8684, AUC 0.9331627488136292, avg_entr 0.012567618861794472, f1 0.868399977684021
ep27_l4_test_time 5.863292455673218
gc 0
Train Epoch28 Acc 0.9823 (39292/40000), AUC 0.9959378838539124
ep28_train_time 183.45111846923828
Test Epoch28 layer0 Acc 0.88, AUC 0.9447682499885559, avg_entr 0.11538413166999817, f1 0.8799999952316284
ep28_l0_test_time 0.753645658493042
Test Epoch28 layer1 Acc 0.8682, AUC 0.918062686920166, avg_entr 0.020266281440854073, f1 0.8682000041007996
ep28_l1_test_time 2.020400285720825
Test Epoch28 layer2 Acc 0.8682, AUC 0.9202103614807129, avg_entr 0.014114131219685078, f1 0.8682000041007996
ep28_l2_test_time 3.2700045108795166
Test Epoch28 layer3 Acc 0.8682, AUC 0.9331207275390625, avg_entr 0.013555195182561874, f1 0.8682000041007996
ep28_l3_test_time 4.444093465805054
Test Epoch28 layer4 Acc 0.868, AUC 0.9329547882080078, avg_entr 0.012830935418605804, f1 0.8679999709129333
ep28_l4_test_time 5.860543966293335
gc 0
Train Epoch29 Acc 0.982025 (39281/40000), AUC 0.9962846636772156
ep29_train_time 183.45223760604858
Test Epoch29 layer0 Acc 0.88, AUC 0.9447362422943115, avg_entr 0.11475416272878647, f1 0.8799999952316284
ep29_l0_test_time 0.741703987121582
Test Epoch29 layer1 Acc 0.8676, AUC 0.9180475473403931, avg_entr 0.020304137840867043, f1 0.8675999641418457
ep29_l1_test_time 2.025038003921509
Test Epoch29 layer2 Acc 0.8674, AUC 0.9197978377342224, avg_entr 0.01427286397665739, f1 0.8673999905586243
ep29_l2_test_time 3.2722437381744385
Test Epoch29 layer3 Acc 0.8678, AUC 0.9329131841659546, avg_entr 0.013698019087314606, f1 0.8677999973297119
ep29_l3_test_time 4.5047783851623535
Test Epoch29 layer4 Acc 0.8672, AUC 0.932706356048584, avg_entr 0.012969188392162323, f1 0.8672000169754028
ep29_l4_test_time 5.869902849197388
Best AUC tensor(0.8968) 6 0
train_loss (2, 5, 30)
valid_acc (5, 30)
valid_AUC (5, 30)
train_acc (30,)
total_train+valid_time 5638.03514456749
Start Testing
Load ckpt at ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt
Test layer0 Acc 0.8884, AUC 0.952794075012207, avg_entr 0.15693528950214386, f1 0.8884000182151794
l0_test_time 0.7463943958282471
Test layer1 Acc 0.8748, AUC 0.939384937286377, avg_entr 0.03717756271362305, f1 0.8748000264167786
l1_test_time 2.0249149799346924
Test layer2 Acc 0.8764, AUC 0.9456244707107544, avg_entr 0.02892722748219967, f1 0.8763999938964844
l2_test_time 3.288530111312866
Test layer3 Acc 0.8764, AUC 0.9472824335098267, avg_entr 0.027241088449954987, f1 0.8763999938964844
l3_test_time 4.590750694274902
Test layer4 Acc 0.8764, AUC 0.9470741748809814, avg_entr 0.025203144177794456, f1 0.8763999938964844
l4_test_time 5.848956108093262
