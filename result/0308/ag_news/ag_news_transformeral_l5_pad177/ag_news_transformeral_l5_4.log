total count words 102019
vocab size 30000
train size 120000, valid size 3800, test size 3800
found 26754 words in glove
model: TransformerModelML(
  (layers): ModuleList(
    (0): EMBLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Embedding(30000, 300)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=4, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=4, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.weight 9000000
layers.0.ae.g.0.weight 512
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 512
layers.0.ae.h.0.bias 4
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
Total Trainable Params: 13672292
init_time 18.568824291229248
Start Training
gc 0
Train Epoch0 Acc 0.62515 (75018/120000), AUC 0.8570976257324219
ep0_train_time 81.35489892959595
Test Epoch0 layer0 Acc 0.9028947368421053, AUC 0.9771363139152527, avg_entr 0.25035950541496277, f1 0.9028947353363037
ep0_l0_test_time 0.16114282608032227
Save ckpt to ckpt/ag_news_transformeral_l5_pad177//ag_news_transformeral_l5.pt  ,ep 0
Test Epoch0 layer1 Acc 0.9073684210526316, AUC 0.9799206256866455, avg_entr 0.16511240601539612, f1 0.9073684215545654
ep0_l1_test_time 0.292557954788208
Save ckpt to ckpt/ag_news_transformeral_l5_pad177//ag_news_transformeral_l5.pt  ,ep 0
Test Epoch0 layer2 Acc 0.9092105263157895, AUC 0.9804050922393799, avg_entr 0.15598753094673157, f1 0.9092105031013489
ep0_l2_test_time 0.4231224060058594
Save ckpt to ckpt/ag_news_transformeral_l5_pad177//ag_news_transformeral_l5.pt  ,ep 0
Test Epoch0 layer3 Acc 0.9102631578947369, AUC 0.9802814722061157, avg_entr 0.16081953048706055, f1 0.910263180732727
ep0_l3_test_time 0.5504884719848633
Save ckpt to ckpt/ag_news_transformeral_l5_pad177//ag_news_transformeral_l5.pt  ,ep 0
Test Epoch0 layer4 Acc 0.9107894736842105, AUC 0.9805951118469238, avg_entr 0.1611277461051941, f1 0.9107894897460938
ep0_l4_test_time 0.6738536357879639
Save ckpt to ckpt/ag_news_transformeral_l5_pad177//ag_news_transformeral_l5.pt  ,ep 0
gc 0
Train Epoch1 Acc 0.9193916666666667 (110327/120000), AUC 0.9812462329864502
ep1_train_time 81.0722827911377
Test Epoch1 layer0 Acc 0.9144736842105263, AUC 0.980589747428894, avg_entr 0.14513501524925232, f1 0.9144737124443054
ep1_l0_test_time 0.15937185287475586
Save ckpt to ckpt/ag_news_transformeral_l5_pad177//ag_news_transformeral_l5.pt  ,ep 1
Test Epoch1 layer1 Acc 0.9189473684210526, AUC 0.9830331206321716, avg_entr 0.08008275181055069, f1 0.9189472794532776
ep1_l1_test_time 0.2936441898345947
Save ckpt to ckpt/ag_news_transformeral_l5_pad177//ag_news_transformeral_l5.pt  ,ep 1
Test Epoch1 layer2 Acc 0.9184210526315789, AUC 0.9829275608062744, avg_entr 0.07037393748760223, f1 0.9184210896492004
ep1_l2_test_time 0.42365193367004395
Test Epoch1 layer3 Acc 0.9184210526315789, AUC 0.9828656911849976, avg_entr 0.065604068338871, f1 0.9184210896492004
ep1_l3_test_time 0.5490319728851318
Test Epoch1 layer4 Acc 0.9194736842105263, AUC 0.9828776121139526, avg_entr 0.06087462604045868, f1 0.9194737076759338
ep1_l4_test_time 0.67301344871521
Save ckpt to ckpt/ag_news_transformeral_l5_pad177//ag_news_transformeral_l5.pt  ,ep 1
gc 0
Train Epoch2 Acc 0.9343833333333333 (112126/120000), AUC 0.9866200089454651
ep2_train_time 81.06750631332397
Test Epoch2 layer0 Acc 0.9184210526315789, AUC 0.9818817377090454, avg_entr 0.1084870845079422, f1 0.9184210896492004
ep2_l0_test_time 0.15851831436157227
Test Epoch2 layer1 Acc 0.9236842105263158, AUC 0.9828313589096069, avg_entr 0.04123558849096298, f1 0.9236842393875122
ep2_l1_test_time 0.2908000946044922
Save ckpt to ckpt/ag_news_transformeral_l5_pad177//ag_news_transformeral_l5.pt  ,ep 2
Test Epoch2 layer2 Acc 0.9247368421052632, AUC 0.9836195111274719, avg_entr 0.03590356186032295, f1 0.9247368574142456
ep2_l2_test_time 0.42333197593688965
Save ckpt to ckpt/ag_news_transformeral_l5_pad177//ag_news_transformeral_l5.pt  ,ep 2
Test Epoch2 layer3 Acc 0.925, AUC 0.9839454889297485, avg_entr 0.03416956961154938, f1 0.925000011920929
ep2_l3_test_time 0.5518326759338379
Save ckpt to ckpt/ag_news_transformeral_l5_pad177//ag_news_transformeral_l5.pt  ,ep 2
Test Epoch2 layer4 Acc 0.9247368421052632, AUC 0.9837824106216431, avg_entr 0.03200968727469444, f1 0.9247368574142456
ep2_l4_test_time 0.6762378215789795
gc 0
Train Epoch3 Acc 0.9424833333333333 (113098/120000), AUC 0.9890146255493164
ep3_train_time 81.06432604789734
Test Epoch3 layer0 Acc 0.9218421052631579, AUC 0.9823974370956421, avg_entr 0.09019801020622253, f1 0.921842098236084
ep3_l0_test_time 0.15875959396362305
Test Epoch3 layer1 Acc 0.921578947368421, AUC 0.9817377328872681, avg_entr 0.031783327460289, f1 0.9215789437294006
ep3_l1_test_time 0.290118932723999
Test Epoch3 layer2 Acc 0.9223684210526316, AUC 0.9830368757247925, avg_entr 0.026394248008728027, f1 0.9223684072494507
ep3_l2_test_time 0.42070770263671875
Test Epoch3 layer3 Acc 0.9228947368421052, AUC 0.9834454655647278, avg_entr 0.025120165199041367, f1 0.9228947162628174
ep3_l3_test_time 0.5477397441864014
Test Epoch3 layer4 Acc 0.9228947368421052, AUC 0.9830295443534851, avg_entr 0.022798193618655205, f1 0.9228947162628174
ep3_l4_test_time 0.6735687255859375
gc 0
Train Epoch4 Acc 0.948175 (113781/120000), AUC 0.9901682138442993
ep4_train_time 81.15398097038269
Test Epoch4 layer0 Acc 0.9202631578947369, AUC 0.9827550053596497, avg_entr 0.07989155501127243, f1 0.9202631711959839
ep4_l0_test_time 0.16035199165344238
Test Epoch4 layer1 Acc 0.9234210526315789, AUC 0.9822020530700684, avg_entr 0.028375735506415367, f1 0.9234210252761841
ep4_l1_test_time 0.2919347286224365
Test Epoch4 layer2 Acc 0.9223684210526316, AUC 0.983270525932312, avg_entr 0.02335720881819725, f1 0.9223684072494507
ep4_l2_test_time 0.4226398468017578
Test Epoch4 layer3 Acc 0.9228947368421052, AUC 0.9838236570358276, avg_entr 0.021520785987377167, f1 0.9228947162628174
ep4_l3_test_time 0.5500805377960205
Test Epoch4 layer4 Acc 0.9231578947368421, AUC 0.9831953644752502, avg_entr 0.02011205442249775, f1 0.9231578707695007
ep4_l4_test_time 0.6758778095245361
gc 0
Train Epoch5 Acc 0.9520833333333333 (114250/120000), AUC 0.9913489818572998
ep5_train_time 81.00235533714294
Test Epoch5 layer0 Acc 0.9192105263157895, AUC 0.982879102230072, avg_entr 0.07379744946956635, f1 0.9192105531692505
ep5_l0_test_time 0.16002941131591797
Test Epoch5 layer1 Acc 0.9226315789473685, AUC 0.9818605184555054, avg_entr 0.025220345705747604, f1 0.922631561756134
ep5_l1_test_time 0.29123497009277344
Test Epoch5 layer2 Acc 0.9218421052631579, AUC 0.9822912812232971, avg_entr 0.021410474553704262, f1 0.921842098236084
ep5_l2_test_time 0.42230939865112305
Test Epoch5 layer3 Acc 0.9223684210526316, AUC 0.9828024506568909, avg_entr 0.020694220438599586, f1 0.9223684072494507
ep5_l3_test_time 0.5496914386749268
Test Epoch5 layer4 Acc 0.9221052631578948, AUC 0.9826520681381226, avg_entr 0.019111862406134605, f1 0.9221052527427673
ep5_l4_test_time 0.6743481159210205
gc 0
Train Epoch6 Acc 0.9552 (114624/120000), AUC 0.9923266768455505
ep6_train_time 81.01998805999756
Test Epoch6 layer0 Acc 0.9207894736842105, AUC 0.9827284216880798, avg_entr 0.06417208909988403, f1 0.9207894802093506
ep6_l0_test_time 0.15903353691101074
Test Epoch6 layer1 Acc 0.9242105263157895, AUC 0.98017418384552, avg_entr 0.022201715037226677, f1 0.9242105484008789
ep6_l1_test_time 0.2904231548309326
Test Epoch6 layer2 Acc 0.9226315789473685, AUC 0.9801982641220093, avg_entr 0.017526786774396896, f1 0.922631561756134
ep6_l2_test_time 0.4215552806854248
Test Epoch6 layer3 Acc 0.9226315789473685, AUC 0.9810730814933777, avg_entr 0.01596633717417717, f1 0.922631561756134
ep6_l3_test_time 0.5504615306854248
Test Epoch6 layer4 Acc 0.9221052631578948, AUC 0.979651153087616, avg_entr 0.014638788066804409, f1 0.9221052527427673
ep6_l4_test_time 0.6763601303100586
gc 0
Train Epoch7 Acc 0.95725 (114870/120000), AUC 0.9932428598403931
ep7_train_time 81.14984679222107
Test Epoch7 layer0 Acc 0.9226315789473685, AUC 0.9826332926750183, avg_entr 0.060415010899305344, f1 0.922631561756134
ep7_l0_test_time 0.15907049179077148
Test Epoch7 layer1 Acc 0.9223684210526316, AUC 0.9800145030021667, avg_entr 0.020989228039979935, f1 0.9223684072494507
ep7_l1_test_time 0.2903623580932617
Test Epoch7 layer2 Acc 0.9205263157894736, AUC 0.9813873767852783, avg_entr 0.016750307753682137, f1 0.9205263257026672
ep7_l2_test_time 0.4209587574005127
Test Epoch7 layer3 Acc 0.9202631578947369, AUC 0.9813756942749023, avg_entr 0.015611174516379833, f1 0.9202631711959839
ep7_l3_test_time 0.5611603260040283
Test Epoch7 layer4 Acc 0.9207894736842105, AUC 0.9797612428665161, avg_entr 0.014467541128396988, f1 0.9207894802093506
ep7_l4_test_time 0.67340087890625
gc 0
Train Epoch8 Acc 0.95945 (115134/120000), AUC 0.9935539364814758
ep8_train_time 81.17150783538818
Test Epoch8 layer0 Acc 0.9221052631578948, AUC 0.9825128316879272, avg_entr 0.05696156248450279, f1 0.9221052527427673
ep8_l0_test_time 0.15949010848999023
Test Epoch8 layer1 Acc 0.9194736842105263, AUC 0.9788047075271606, avg_entr 0.019896356388926506, f1 0.9194737076759338
ep8_l1_test_time 0.2905592918395996
Test Epoch8 layer2 Acc 0.9197368421052632, AUC 0.9804919958114624, avg_entr 0.015036452561616898, f1 0.9197368621826172
ep8_l2_test_time 0.42116522789001465
Test Epoch8 layer3 Acc 0.9194736842105263, AUC 0.9808527231216431, avg_entr 0.013901601545512676, f1 0.9194737076759338
ep8_l3_test_time 0.5489904880523682
Test Epoch8 layer4 Acc 0.9197368421052632, AUC 0.9807999134063721, avg_entr 0.012846776284277439, f1 0.9197368621826172
ep8_l4_test_time 0.6738395690917969
gc 0
Train Epoch9 Acc 0.9614416666666666 (115373/120000), AUC 0.9939185380935669
ep9_train_time 81.0710654258728
Test Epoch9 layer0 Acc 0.9213157894736842, AUC 0.9824144244194031, avg_entr 0.053629640489816666, f1 0.9213157892227173
ep9_l0_test_time 0.15959405899047852
Test Epoch9 layer1 Acc 0.9205263157894736, AUC 0.9787775874137878, avg_entr 0.017484739422798157, f1 0.9205263257026672
ep9_l1_test_time 0.2903764247894287
Test Epoch9 layer2 Acc 0.9197368421052632, AUC 0.9793784618377686, avg_entr 0.012732856906950474, f1 0.9197368621826172
ep9_l2_test_time 0.4209921360015869
Test Epoch9 layer3 Acc 0.9202631578947369, AUC 0.9795699715614319, avg_entr 0.011382143013179302, f1 0.9202631711959839
ep9_l3_test_time 0.5473330020904541
Test Epoch9 layer4 Acc 0.92, AUC 0.9791866540908813, avg_entr 0.010316905565559864, f1 0.9200000166893005
ep9_l4_test_time 0.6750242710113525
gc 0
Train Epoch10 Acc 0.9641166666666666 (115694/120000), AUC 0.9947627782821655
ep10_train_time 81.17469120025635
Test Epoch10 layer0 Acc 0.9228947368421052, AUC 0.9823321104049683, avg_entr 0.05126815289258957, f1 0.9228947162628174
ep10_l0_test_time 0.15858674049377441
Test Epoch10 layer1 Acc 0.9186842105263158, AUC 0.9781162738800049, avg_entr 0.01753917522728443, f1 0.918684184551239
ep10_l1_test_time 0.2899751663208008
Test Epoch10 layer2 Acc 0.9173684210526316, AUC 0.9794427156448364, avg_entr 0.013262585736811161, f1 0.9173683524131775
ep10_l2_test_time 0.42056894302368164
Test Epoch10 layer3 Acc 0.9178947368421052, AUC 0.9799646139144897, avg_entr 0.012158727273344994, f1 0.917894721031189
ep10_l3_test_time 0.548159122467041
Test Epoch10 layer4 Acc 0.9178947368421052, AUC 0.9791721105575562, avg_entr 0.011226347647607327, f1 0.917894721031189
ep10_l4_test_time 0.6729941368103027
gc 0
Train Epoch11 Acc 0.9649416666666667 (115793/120000), AUC 0.9949666261672974
ep11_train_time 81.02855730056763
Test Epoch11 layer0 Acc 0.9205263157894736, AUC 0.9823915362358093, avg_entr 0.04991026967763901, f1 0.9205263257026672
ep11_l0_test_time 0.1587226390838623
Test Epoch11 layer1 Acc 0.9197368421052632, AUC 0.9773613810539246, avg_entr 0.01662725768983364, f1 0.9197368621826172
ep11_l1_test_time 0.2896883487701416
Test Epoch11 layer2 Acc 0.9186842105263158, AUC 0.977824330329895, avg_entr 0.012062380090355873, f1 0.918684184551239
ep11_l2_test_time 0.4217417240142822
Test Epoch11 layer3 Acc 0.9189473684210526, AUC 0.9782600998878479, avg_entr 0.010773949325084686, f1 0.9189472794532776
ep11_l3_test_time 0.5485682487487793
Test Epoch11 layer4 Acc 0.9186842105263158, AUC 0.9774008989334106, avg_entr 0.00975986197590828, f1 0.918684184551239
ep11_l4_test_time 0.6732926368713379
gc 0
Train Epoch12 Acc 0.9658083333333334 (115897/120000), AUC 0.9950705170631409
ep12_train_time 81.08764839172363
Test Epoch12 layer0 Acc 0.9223684210526316, AUC 0.9822201132774353, avg_entr 0.04728454723954201, f1 0.9223684072494507
ep12_l0_test_time 0.15800714492797852
Test Epoch12 layer1 Acc 0.9197368421052632, AUC 0.9758971929550171, avg_entr 0.016448363661766052, f1 0.9197368621826172
ep12_l1_test_time 0.2897453308105469
Test Epoch12 layer2 Acc 0.9176315789473685, AUC 0.9768275618553162, avg_entr 0.012359201908111572, f1 0.9176315665245056
ep12_l2_test_time 0.42067956924438477
Test Epoch12 layer3 Acc 0.9181578947368421, AUC 0.9767459034919739, avg_entr 0.011033056303858757, f1 0.9181578755378723
ep12_l3_test_time 0.5479178428649902
Test Epoch12 layer4 Acc 0.9181578947368421, AUC 0.9765180349349976, avg_entr 0.010464456863701344, f1 0.9181578755378723
ep12_l4_test_time 0.6733934879302979
gc 0
Train Epoch13 Acc 0.966275 (115953/120000), AUC 0.9952669143676758
ep13_train_time 81.18437957763672
Test Epoch13 layer0 Acc 0.9207894736842105, AUC 0.9821175336837769, avg_entr 0.045200541615486145, f1 0.9207894802093506
ep13_l0_test_time 0.16202211380004883
Test Epoch13 layer1 Acc 0.9181578947368421, AUC 0.9756584763526917, avg_entr 0.015599342063069344, f1 0.9181578755378723
ep13_l1_test_time 0.29050731658935547
Test Epoch13 layer2 Acc 0.9176315789473685, AUC 0.9777905941009521, avg_entr 0.01128162257373333, f1 0.9176315665245056
ep13_l2_test_time 0.4209127426147461
Test Epoch13 layer3 Acc 0.9173684210526316, AUC 0.9778448939323425, avg_entr 0.009776542894542217, f1 0.9173683524131775
ep13_l3_test_time 0.5500583648681641
Test Epoch13 layer4 Acc 0.9173684210526316, AUC 0.9780505895614624, avg_entr 0.009342983365058899, f1 0.9173683524131775
ep13_l4_test_time 0.672006368637085
gc 0
Train Epoch14 Acc 0.9679833333333333 (116158/120000), AUC 0.9955391883850098
ep14_train_time 81.029709815979
Test Epoch14 layer0 Acc 0.9221052631578948, AUC 0.9820919632911682, avg_entr 0.04454199597239494, f1 0.9221052527427673
ep14_l0_test_time 0.15894865989685059
Test Epoch14 layer1 Acc 0.9163157894736842, AUC 0.975342333316803, avg_entr 0.01585587114095688, f1 0.9163157939910889
ep14_l1_test_time 0.2908029556274414
Test Epoch14 layer2 Acc 0.9152631578947369, AUC 0.9761946201324463, avg_entr 0.011653573252260685, f1 0.9152631759643555
ep14_l2_test_time 0.4212632179260254
Test Epoch14 layer3 Acc 0.9152631578947369, AUC 0.9763317108154297, avg_entr 0.010132324881851673, f1 0.9152631759643555
ep14_l3_test_time 0.5502235889434814
Test Epoch14 layer4 Acc 0.9144736842105263, AUC 0.9770056009292603, avg_entr 0.009085344150662422, f1 0.9144737124443054
ep14_l4_test_time 0.6733419895172119
gc 0
Train Epoch15 Acc 0.9683 (116196/120000), AUC 0.9958252906799316
ep15_train_time 81.13623642921448
Test Epoch15 layer0 Acc 0.921578947368421, AUC 0.9820640087127686, avg_entr 0.043504804372787476, f1 0.9215789437294006
ep15_l0_test_time 0.15915489196777344
Test Epoch15 layer1 Acc 0.9178947368421052, AUC 0.9747461676597595, avg_entr 0.014840444549918175, f1 0.917894721031189
ep15_l1_test_time 0.2906370162963867
Test Epoch15 layer2 Acc 0.9163157894736842, AUC 0.9759564399719238, avg_entr 0.010912048630416393, f1 0.9163157939910889
ep15_l2_test_time 0.42144775390625
Test Epoch15 layer3 Acc 0.9163157894736842, AUC 0.9755332469940186, avg_entr 0.009616266936063766, f1 0.9163157939910889
ep15_l3_test_time 0.5489950180053711
Test Epoch15 layer4 Acc 0.9168421052631579, AUC 0.975016176700592, avg_entr 0.00895586609840393, f1 0.9168421030044556
ep15_l4_test_time 0.6731879711151123
gc 0
Train Epoch16 Acc 0.9688416666666667 (116261/120000), AUC 0.9958032369613647
ep16_train_time 81.0864827632904
Test Epoch16 layer0 Acc 0.9213157894736842, AUC 0.9820497632026672, avg_entr 0.041626594960689545, f1 0.9213157892227173
ep16_l0_test_time 0.15854811668395996
Test Epoch16 layer1 Acc 0.9178947368421052, AUC 0.9750053882598877, avg_entr 0.014850229024887085, f1 0.917894721031189
ep16_l1_test_time 0.2899284362792969
Test Epoch16 layer2 Acc 0.9163157894736842, AUC 0.9747272729873657, avg_entr 0.010473267175257206, f1 0.9163157939910889
ep16_l2_test_time 0.420931339263916
Test Epoch16 layer3 Acc 0.9160526315789473, AUC 0.9746652841567993, avg_entr 0.008880254812538624, f1 0.9160526394844055
ep16_l3_test_time 0.5490522384643555
Test Epoch16 layer4 Acc 0.9163157894736842, AUC 0.9738266468048096, avg_entr 0.008214838802814484, f1 0.9163157939910889
ep16_l4_test_time 0.6719272136688232
gc 0
Train Epoch17 Acc 0.9691583333333333 (116299/120000), AUC 0.9958418607711792
ep17_train_time 81.13884472846985
Test Epoch17 layer0 Acc 0.921578947368421, AUC 0.9819941520690918, avg_entr 0.04091731458902359, f1 0.9215789437294006
ep17_l0_test_time 0.15931057929992676
Test Epoch17 layer1 Acc 0.9178947368421052, AUC 0.9754831790924072, avg_entr 0.013983529061079025, f1 0.917894721031189
ep17_l1_test_time 0.29087018966674805
Test Epoch17 layer2 Acc 0.9171052631578948, AUC 0.9754850268363953, avg_entr 0.010098731145262718, f1 0.9171052575111389
ep17_l2_test_time 0.42110490798950195
Test Epoch17 layer3 Acc 0.916578947368421, AUC 0.975909948348999, avg_entr 0.008917934261262417, f1 0.9165789484977722
ep17_l3_test_time 0.5495004653930664
Test Epoch17 layer4 Acc 0.9163157894736842, AUC 0.9748505353927612, avg_entr 0.008334308862686157, f1 0.9163157939910889
ep17_l4_test_time 0.6720638275146484
gc 0
Train Epoch18 Acc 0.9697166666666667 (116366/120000), AUC 0.9960881471633911
ep18_train_time 81.05073714256287
Test Epoch18 layer0 Acc 0.9213157894736842, AUC 0.9819731116294861, avg_entr 0.03998010605573654, f1 0.9213157892227173
ep18_l0_test_time 0.15883135795593262
Test Epoch18 layer1 Acc 0.9181578947368421, AUC 0.9745414853096008, avg_entr 0.013769078068435192, f1 0.9181578755378723
ep18_l1_test_time 0.29035091400146484
Test Epoch18 layer2 Acc 0.9157894736842105, AUC 0.976022481918335, avg_entr 0.009887687861919403, f1 0.9157894849777222
ep18_l2_test_time 0.4204225540161133
Test Epoch18 layer3 Acc 0.9160526315789473, AUC 0.9753246307373047, avg_entr 0.008468124084174633, f1 0.9160526394844055
ep18_l3_test_time 0.5502889156341553
Test Epoch18 layer4 Acc 0.9155263157894736, AUC 0.9747372269630432, avg_entr 0.007589152082800865, f1 0.9155263304710388
ep18_l4_test_time 0.6720070838928223
gc 0
Train Epoch19 Acc 0.9700083333333334 (116401/120000), AUC 0.9961346983909607
ep19_train_time 81.07519769668579
Test Epoch19 layer0 Acc 0.921578947368421, AUC 0.9820020794868469, avg_entr 0.03855463117361069, f1 0.9215789437294006
ep19_l0_test_time 0.15902352333068848
Test Epoch19 layer1 Acc 0.9171052631578948, AUC 0.9747492074966431, avg_entr 0.01388324424624443, f1 0.9171052575111389
ep19_l1_test_time 0.290299654006958
Test Epoch19 layer2 Acc 0.9160526315789473, AUC 0.9743776321411133, avg_entr 0.010205259546637535, f1 0.9160526394844055
ep19_l2_test_time 0.420224666595459
Test Epoch19 layer3 Acc 0.916578947368421, AUC 0.9732771515846252, avg_entr 0.008918901905417442, f1 0.9165789484977722
ep19_l3_test_time 0.5498085021972656
Test Epoch19 layer4 Acc 0.9168421052631579, AUC 0.9728600978851318, avg_entr 0.008351236581802368, f1 0.9168421030044556
ep19_l4_test_time 0.6720798015594482
gc 0
Train Epoch20 Acc 0.9701166666666666 (116414/120000), AUC 0.9961326122283936
ep20_train_time 81.08652019500732
Test Epoch20 layer0 Acc 0.921578947368421, AUC 0.9820135235786438, avg_entr 0.038055822253227234, f1 0.9215789437294006
ep20_l0_test_time 0.15925836563110352
Test Epoch20 layer1 Acc 0.9171052631578948, AUC 0.9744977355003357, avg_entr 0.013629091903567314, f1 0.9171052575111389
ep20_l1_test_time 0.2906687259674072
Test Epoch20 layer2 Acc 0.9157894736842105, AUC 0.9745991230010986, avg_entr 0.01017811056226492, f1 0.9157894849777222
ep20_l2_test_time 0.42050957679748535
Test Epoch20 layer3 Acc 0.9160526315789473, AUC 0.973737359046936, avg_entr 0.009012660011649132, f1 0.9160526394844055
ep20_l3_test_time 0.5495595932006836
Test Epoch20 layer4 Acc 0.9163157894736842, AUC 0.9729270935058594, avg_entr 0.008351864293217659, f1 0.9163157939910889
ep20_l4_test_time 0.6725428104400635
gc 0
Train Epoch21 Acc 0.9701916666666667 (116423/120000), AUC 0.996113657951355
ep21_train_time 81.01573824882507
Test Epoch21 layer0 Acc 0.9194736842105263, AUC 0.9819364547729492, avg_entr 0.037288181483745575, f1 0.9194737076759338
ep21_l0_test_time 0.15865612030029297
Test Epoch21 layer1 Acc 0.916578947368421, AUC 0.9745786190032959, avg_entr 0.01318885013461113, f1 0.9165789484977722
ep21_l1_test_time 0.28988194465637207
Test Epoch21 layer2 Acc 0.9157894736842105, AUC 0.9747706651687622, avg_entr 0.009619987569749355, f1 0.9157894849777222
ep21_l2_test_time 0.42073965072631836
Test Epoch21 layer3 Acc 0.9160526315789473, AUC 0.9740369915962219, avg_entr 0.008336015045642853, f1 0.9160526394844055
ep21_l3_test_time 0.5486459732055664
Test Epoch21 layer4 Acc 0.9163157894736842, AUC 0.9736236333847046, avg_entr 0.007736709900200367, f1 0.9163157939910889
ep21_l4_test_time 0.67201828956604
gc 0
Train Epoch22 Acc 0.9704416666666666 (116453/120000), AUC 0.9961973428726196
ep22_train_time 81.03622603416443
Test Epoch22 layer0 Acc 0.9218421052631579, AUC 0.9819414615631104, avg_entr 0.03645315021276474, f1 0.921842098236084
ep22_l0_test_time 0.15870428085327148
Test Epoch22 layer1 Acc 0.9178947368421052, AUC 0.9743258357048035, avg_entr 0.012812979519367218, f1 0.917894721031189
ep22_l1_test_time 0.29027795791625977
Test Epoch22 layer2 Acc 0.9155263157894736, AUC 0.9745541214942932, avg_entr 0.009697578847408295, f1 0.9155263304710388
ep22_l2_test_time 0.4210171699523926
Test Epoch22 layer3 Acc 0.9152631578947369, AUC 0.973651647567749, avg_entr 0.008383176289498806, f1 0.9152631759643555
ep22_l3_test_time 0.547980546951294
Test Epoch22 layer4 Acc 0.9152631578947369, AUC 0.9734841585159302, avg_entr 0.0075715938583016396, f1 0.9152631759643555
ep22_l4_test_time 0.6722686290740967
gc 0
Train Epoch23 Acc 0.9706916666666666 (116483/120000), AUC 0.996300220489502
ep23_train_time 81.0908854007721
Test Epoch23 layer0 Acc 0.9213157894736842, AUC 0.9819276928901672, avg_entr 0.036491043865680695, f1 0.9213157892227173
ep23_l0_test_time 0.1589524745941162
Test Epoch23 layer1 Acc 0.9171052631578948, AUC 0.974380373954773, avg_entr 0.01297729928046465, f1 0.9171052575111389
ep23_l1_test_time 0.2908151149749756
Test Epoch23 layer2 Acc 0.9163157894736842, AUC 0.9740322828292847, avg_entr 0.00976394210010767, f1 0.9163157939910889
ep23_l2_test_time 0.42104625701904297
Test Epoch23 layer3 Acc 0.9163157894736842, AUC 0.9730541110038757, avg_entr 0.008573168888688087, f1 0.9163157939910889
ep23_l3_test_time 0.5498065948486328
Test Epoch23 layer4 Acc 0.9152631578947369, AUC 0.9722157716751099, avg_entr 0.007927305065095425, f1 0.9152631759643555
ep23_l4_test_time 0.6725890636444092
gc 0
Train Epoch24 Acc 0.9707333333333333 (116488/120000), AUC 0.996246874332428
ep24_train_time 81.0583004951477
Test Epoch24 layer0 Acc 0.9213157894736842, AUC 0.9819145798683167, avg_entr 0.0353870689868927, f1 0.9213157892227173
ep24_l0_test_time 0.1584768295288086
Test Epoch24 layer1 Acc 0.916578947368421, AUC 0.9741089344024658, avg_entr 0.012594959698617458, f1 0.9165789484977722
ep24_l1_test_time 0.2897067070007324
Test Epoch24 layer2 Acc 0.9142105263157895, AUC 0.9747780561447144, avg_entr 0.009911363944411278, f1 0.9142104983329773
ep24_l2_test_time 0.41983914375305176
Test Epoch24 layer3 Acc 0.9147368421052632, AUC 0.9738593697547913, avg_entr 0.008689195849001408, f1 0.9147368669509888
ep24_l3_test_time 0.5490720272064209
Test Epoch24 layer4 Acc 0.915, AUC 0.9728101491928101, avg_entr 0.007986769080162048, f1 0.9150000214576721
ep24_l4_test_time 0.6732215881347656
gc 0
Train Epoch25 Acc 0.9709083333333334 (116509/120000), AUC 0.9963569641113281
ep25_train_time 81.17367458343506
Test Epoch25 layer0 Acc 0.921578947368421, AUC 0.981913685798645, avg_entr 0.03545660898089409, f1 0.9215789437294006
ep25_l0_test_time 0.1590111255645752
Test Epoch25 layer1 Acc 0.9163157894736842, AUC 0.9740957617759705, avg_entr 0.01246141642332077, f1 0.9163157939910889
ep25_l1_test_time 0.2903003692626953
Test Epoch25 layer2 Acc 0.9157894736842105, AUC 0.9750535488128662, avg_entr 0.00998327974230051, f1 0.9157894849777222
ep25_l2_test_time 0.4211618900299072
Test Epoch25 layer3 Acc 0.915, AUC 0.9740837216377258, avg_entr 0.008843841962516308, f1 0.9150000214576721
ep25_l3_test_time 0.5496008396148682
Test Epoch25 layer4 Acc 0.9144736842105263, AUC 0.9733215570449829, avg_entr 0.00809173658490181, f1 0.9144737124443054
ep25_l4_test_time 0.6728434562683105
gc 0
Train Epoch26 Acc 0.9713166666666667 (116558/120000), AUC 0.9963488578796387
ep26_train_time 81.20992231369019
Test Epoch26 layer0 Acc 0.921578947368421, AUC 0.9819141030311584, avg_entr 0.035171009600162506, f1 0.9215789437294006
ep26_l0_test_time 0.158555269241333
Test Epoch26 layer1 Acc 0.916578947368421, AUC 0.9740610718727112, avg_entr 0.012254559434950352, f1 0.9165789484977722
ep26_l1_test_time 0.2903122901916504
Test Epoch26 layer2 Acc 0.915, AUC 0.9751946926116943, avg_entr 0.009519606828689575, f1 0.9150000214576721
ep26_l2_test_time 0.42079877853393555
Test Epoch26 layer3 Acc 0.9155263157894736, AUC 0.9737037420272827, avg_entr 0.008425012230873108, f1 0.9155263304710388
ep26_l3_test_time 0.5488371849060059
Test Epoch26 layer4 Acc 0.9155263157894736, AUC 0.9730508327484131, avg_entr 0.007881996221840382, f1 0.9155263304710388
ep26_l4_test_time 0.6717250347137451
gc 0
Train Epoch27 Acc 0.9712083333333333 (116545/120000), AUC 0.9964026808738708
ep27_train_time 81.1336681842804
Test Epoch27 layer0 Acc 0.9207894736842105, AUC 0.9819310903549194, avg_entr 0.03516372665762901, f1 0.9207894802093506
ep27_l0_test_time 0.16106486320495605
Test Epoch27 layer1 Acc 0.9163157894736842, AUC 0.9741517305374146, avg_entr 0.012244783341884613, f1 0.9163157939910889
ep27_l1_test_time 0.2902493476867676
Test Epoch27 layer2 Acc 0.9152631578947369, AUC 0.9746704697608948, avg_entr 0.009724135510623455, f1 0.9152631759643555
ep27_l2_test_time 0.4203970432281494
Test Epoch27 layer3 Acc 0.9152631578947369, AUC 0.9735521078109741, avg_entr 0.008693879470229149, f1 0.9152631759643555
ep27_l3_test_time 0.5496513843536377
Test Epoch27 layer4 Acc 0.915, AUC 0.972952127456665, avg_entr 0.007996693253517151, f1 0.9150000214576721
ep27_l4_test_time 0.6713771820068359
gc 0
Train Epoch28 Acc 0.9711666666666666 (116540/120000), AUC 0.9963947534561157
ep28_train_time 81.20831680297852
Test Epoch28 layer0 Acc 0.9210526315789473, AUC 0.9819256067276001, avg_entr 0.03520673140883446, f1 0.9210526347160339
ep28_l0_test_time 0.15897488594055176
Test Epoch28 layer1 Acc 0.9168421052631579, AUC 0.9741787910461426, avg_entr 0.01205893512815237, f1 0.9168421030044556
ep28_l1_test_time 0.29008936882019043
Test Epoch28 layer2 Acc 0.9152631578947369, AUC 0.9751743078231812, avg_entr 0.009478387422859669, f1 0.9152631759643555
ep28_l2_test_time 0.4215710163116455
Test Epoch28 layer3 Acc 0.915, AUC 0.9743167757987976, avg_entr 0.008361440151929855, f1 0.9150000214576721
ep28_l3_test_time 0.5481586456298828
Test Epoch28 layer4 Acc 0.915, AUC 0.9735647439956665, avg_entr 0.007821299135684967, f1 0.9150000214576721
ep28_l4_test_time 0.6720659732818604
gc 0
Train Epoch29 Acc 0.9714083333333333 (116569/120000), AUC 0.9964295625686646
ep29_train_time 81.14340686798096
Test Epoch29 layer0 Acc 0.9210526315789473, AUC 0.9818944931030273, avg_entr 0.03484677895903587, f1 0.9210526347160339
ep29_l0_test_time 0.15878510475158691
Test Epoch29 layer1 Acc 0.9168421052631579, AUC 0.9740544557571411, avg_entr 0.012103420682251453, f1 0.9168421030044556
ep29_l1_test_time 0.290449857711792
Test Epoch29 layer2 Acc 0.9147368421052632, AUC 0.9748214483261108, avg_entr 0.009735998697578907, f1 0.9147368669509888
ep29_l2_test_time 0.42152976989746094
Test Epoch29 layer3 Acc 0.9147368421052632, AUC 0.9740520119667053, avg_entr 0.008679122664034367, f1 0.9147368669509888
ep29_l3_test_time 0.5504095554351807
Test Epoch29 layer4 Acc 0.915, AUC 0.9734781980514526, avg_entr 0.008010581135749817, f1 0.9150000214576721
ep29_l4_test_time 0.6728212833404541
Best AUC tensor(0.9250) 2 3
train_loss (2, 5, 30)
valid_acc (5, 30)
valid_AUC (5, 30)
train_acc (30,)
total_train+valid_time 2498.357221841812
Start Testing
Load ckpt at ckpt/ag_news_transformeral_l5_pad177//ag_news_transformeral_l5.pt
Test layer0 Acc 0.9105263157894737, AUC 0.9797086715698242, avg_entr 0.11296193301677704, f1 0.9105263352394104
l0_test_time 0.15928959846496582
Test layer1 Acc 0.9144736842105263, AUC 0.9811307191848755, avg_entr 0.04534366354346275, f1 0.9144737124443054
l1_test_time 0.29023027420043945
Test layer2 Acc 0.9136842105263158, AUC 0.9817100167274475, avg_entr 0.03918604180216789, f1 0.9136841893196106
l2_test_time 0.42183399200439453
Test layer3 Acc 0.9131578947368421, AUC 0.9819148778915405, avg_entr 0.0366983525454998, f1 0.9131578803062439
l3_test_time 0.5508520603179932
Test layer4 Acc 0.9128947368421053, AUC 0.9821695685386658, avg_entr 0.034412093460559845, f1 0.9128947257995605
l4_test_time 0.6741495132446289
