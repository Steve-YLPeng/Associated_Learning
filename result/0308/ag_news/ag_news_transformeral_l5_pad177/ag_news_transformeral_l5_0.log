total count words 102019
vocab size 30000
train size 120000, valid size 3800, test size 3800
found 26754 words in glove
model: TransformerModelML(
  (layers): ModuleList(
    (0): EMBLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Embedding(30000, 300)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=4, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=4, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.weight 9000000
layers.0.ae.g.0.weight 512
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 512
layers.0.ae.h.0.bias 4
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
Total Trainable Params: 13672292
init_time 31.088748455047607
Start Training
gc 0
Train Epoch0 Acc 0.6303833333333333 (75646/120000), AUC 0.8563517332077026
ep0_train_time 167.96052312850952
Test Epoch0 layer0 Acc 0.9057894736842105, AUC 0.9771010279655457, avg_entr 0.251302033662796, f1 0.9057894945144653
ep0_l0_test_time 0.32497143745422363
Save ckpt to ckpt/ag_news_transformeral_l5_pad177//ag_news_transformeral_l5.pt  ,ep 0
Test Epoch0 layer1 Acc 0.9076315789473685, AUC 0.9796547889709473, avg_entr 0.1629045307636261, f1 0.9076315760612488
ep0_l1_test_time 0.5994424819946289
Save ckpt to ckpt/ag_news_transformeral_l5_pad177//ag_news_transformeral_l5.pt  ,ep 0
Test Epoch0 layer2 Acc 0.9089473684210526, AUC 0.979829728603363, avg_entr 0.15451395511627197, f1 0.9089473485946655
ep0_l2_test_time 0.8638970851898193
Save ckpt to ckpt/ag_news_transformeral_l5_pad177//ag_news_transformeral_l5.pt  ,ep 0
Test Epoch0 layer3 Acc 0.9073684210526316, AUC 0.9798644185066223, avg_entr 0.15711122751235962, f1 0.9073684215545654
ep0_l3_test_time 1.1235806941986084
Test Epoch0 layer4 Acc 0.9068421052631579, AUC 0.9797399044036865, avg_entr 0.15728430449962616, f1 0.9068421125411987
ep0_l4_test_time 1.381577730178833
gc 0
Train Epoch1 Acc 0.9199416666666667 (110393/120000), AUC 0.9813709259033203
ep1_train_time 166.64508366584778
Test Epoch1 layer0 Acc 0.9168421052631579, AUC 0.9805442690849304, avg_entr 0.14471305906772614, f1 0.9168421030044556
ep1_l0_test_time 0.31890320777893066
Save ckpt to ckpt/ag_news_transformeral_l5_pad177//ag_news_transformeral_l5.pt  ,ep 1
Test Epoch1 layer1 Acc 0.9202631578947369, AUC 0.9831142425537109, avg_entr 0.08170874416828156, f1 0.9202631711959839
ep1_l1_test_time 0.5989389419555664
Save ckpt to ckpt/ag_news_transformeral_l5_pad177//ag_news_transformeral_l5.pt  ,ep 1
Test Epoch1 layer2 Acc 0.9223684210526316, AUC 0.9829923510551453, avg_entr 0.06660274416208267, f1 0.9223684072494507
ep1_l2_test_time 0.8656647205352783
Save ckpt to ckpt/ag_news_transformeral_l5_pad177//ag_news_transformeral_l5.pt  ,ep 1
Test Epoch1 layer3 Acc 0.9223684210526316, AUC 0.9830622673034668, avg_entr 0.06502648442983627, f1 0.9223684072494507
ep1_l3_test_time 1.1209321022033691
Save ckpt to ckpt/ag_news_transformeral_l5_pad177//ag_news_transformeral_l5.pt  ,ep 1
Test Epoch1 layer4 Acc 0.9218421052631579, AUC 0.9827760457992554, avg_entr 0.059580668807029724, f1 0.921842098236084
ep1_l4_test_time 1.391617774963379
gc 0
Train Epoch2 Acc 0.9346166666666667 (112154/120000), AUC 0.986563503742218
ep2_train_time 166.67520785331726
Test Epoch2 layer0 Acc 0.9192105263157895, AUC 0.9817469120025635, avg_entr 0.1093444898724556, f1 0.9192105531692505
ep2_l0_test_time 0.3241844177246094
Test Epoch2 layer1 Acc 0.92, AUC 0.9838131666183472, avg_entr 0.04177393391728401, f1 0.9200000166893005
ep2_l1_test_time 0.5845353603363037
Test Epoch2 layer2 Acc 0.921578947368421, AUC 0.9838892221450806, avg_entr 0.03491252660751343, f1 0.9215789437294006
ep2_l2_test_time 0.8551959991455078
Test Epoch2 layer3 Acc 0.9210526315789473, AUC 0.9837563633918762, avg_entr 0.03417341411113739, f1 0.9210526347160339
ep2_l3_test_time 1.1208233833312988
Test Epoch2 layer4 Acc 0.921578947368421, AUC 0.9836916327476501, avg_entr 0.030293596908450127, f1 0.9215789437294006
ep2_l4_test_time 1.390383243560791
gc 0
Train Epoch3 Acc 0.94275 (113130/120000), AUC 0.988933801651001
ep3_train_time 166.5861132144928
Test Epoch3 layer0 Acc 0.9194736842105263, AUC 0.9824337363243103, avg_entr 0.09304642677307129, f1 0.9194737076759338
ep3_l0_test_time 0.3306741714477539
Test Epoch3 layer1 Acc 0.9218421052631579, AUC 0.9834825992584229, avg_entr 0.03135082125663757, f1 0.921842098236084
ep3_l1_test_time 0.58847975730896
Test Epoch3 layer2 Acc 0.9223684210526316, AUC 0.9834508895874023, avg_entr 0.02652512677013874, f1 0.9223684072494507
ep3_l2_test_time 0.8594644069671631
Save ckpt to ckpt/ag_news_transformeral_l5_pad177//ag_news_transformeral_l5.pt  ,ep 3
Test Epoch3 layer3 Acc 0.9228947368421052, AUC 0.9835195541381836, avg_entr 0.025555860251188278, f1 0.9228947162628174
ep3_l3_test_time 1.1216323375701904
Save ckpt to ckpt/ag_news_transformeral_l5_pad177//ag_news_transformeral_l5.pt  ,ep 3
Test Epoch3 layer4 Acc 0.9231578947368421, AUC 0.9823986291885376, avg_entr 0.023428654298186302, f1 0.9231578707695007
ep3_l4_test_time 1.3912203311920166
Save ckpt to ckpt/ag_news_transformeral_l5_pad177//ag_news_transformeral_l5.pt  ,ep 3
gc 0
Train Epoch4 Acc 0.9479083333333334 (113749/120000), AUC 0.9902806282043457
ep4_train_time 166.68176078796387
Test Epoch4 layer0 Acc 0.9231578947368421, AUC 0.982661247253418, avg_entr 0.07942727208137512, f1 0.9231578707695007
ep4_l0_test_time 0.32735228538513184
Save ckpt to ckpt/ag_news_transformeral_l5_pad177//ag_news_transformeral_l5.pt  ,ep 4
Test Epoch4 layer1 Acc 0.9221052631578948, AUC 0.9804296493530273, avg_entr 0.02664707973599434, f1 0.9221052527427673
ep4_l1_test_time 0.5973279476165771
Test Epoch4 layer2 Acc 0.9218421052631579, AUC 0.9810739755630493, avg_entr 0.021917060017585754, f1 0.921842098236084
ep4_l2_test_time 0.8594546318054199
Test Epoch4 layer3 Acc 0.9223684210526316, AUC 0.9831664562225342, avg_entr 0.020237237215042114, f1 0.9223684072494507
ep4_l3_test_time 1.1209971904754639
Test Epoch4 layer4 Acc 0.921578947368421, AUC 0.9816938042640686, avg_entr 0.018696976825594902, f1 0.9215789437294006
ep4_l4_test_time 1.386047124862671
gc 0
Train Epoch5 Acc 0.9523666666666667 (114284/120000), AUC 0.9911954402923584
ep5_train_time 166.6329882144928
Test Epoch5 layer0 Acc 0.921578947368421, AUC 0.9826185703277588, avg_entr 0.0708390474319458, f1 0.9215789437294006
ep5_l0_test_time 0.3174278736114502
Test Epoch5 layer1 Acc 0.921578947368421, AUC 0.9815421104431152, avg_entr 0.02374790422618389, f1 0.9215789437294006
ep5_l1_test_time 0.5929129123687744
Test Epoch5 layer2 Acc 0.921578947368421, AUC 0.9834634065628052, avg_entr 0.019362367689609528, f1 0.9215789437294006
ep5_l2_test_time 0.8645861148834229
Test Epoch5 layer3 Acc 0.9213157894736842, AUC 0.983631432056427, avg_entr 0.017198210582137108, f1 0.9213157892227173
ep5_l3_test_time 1.1221652030944824
Test Epoch5 layer4 Acc 0.9210526315789473, AUC 0.9840046167373657, avg_entr 0.015669984742999077, f1 0.9210526347160339
ep5_l4_test_time 1.3943569660186768
gc 0
Train Epoch6 Acc 0.9559 (114708/120000), AUC 0.9922559261322021
ep6_train_time 166.72964525222778
Test Epoch6 layer0 Acc 0.9234210526315789, AUC 0.982719898223877, avg_entr 0.06423540413379669, f1 0.9234210252761841
ep6_l0_test_time 0.33260512351989746
Save ckpt to ckpt/ag_news_transformeral_l5_pad177//ag_news_transformeral_l5.pt  ,ep 6
Test Epoch6 layer1 Acc 0.9231578947368421, AUC 0.9810349941253662, avg_entr 0.021658848971128464, f1 0.9231578707695007
ep6_l1_test_time 0.5950615406036377
Test Epoch6 layer2 Acc 0.9231578947368421, AUC 0.9826918840408325, avg_entr 0.01759425923228264, f1 0.9231578707695007
ep6_l2_test_time 0.8602638244628906
Test Epoch6 layer3 Acc 0.9223684210526316, AUC 0.9830894470214844, avg_entr 0.015966907143592834, f1 0.9223684072494507
ep6_l3_test_time 1.1204469203948975
Test Epoch6 layer4 Acc 0.9221052631578948, AUC 0.9824564456939697, avg_entr 0.014301522634923458, f1 0.9221052527427673
ep6_l4_test_time 1.3935422897338867
gc 0
Train Epoch7 Acc 0.9586166666666667 (115034/120000), AUC 0.9933233261108398
ep7_train_time 166.67863059043884
Test Epoch7 layer0 Acc 0.9218421052631579, AUC 0.9826245307922363, avg_entr 0.06106221303343773, f1 0.921842098236084
ep7_l0_test_time 0.31716442108154297
Test Epoch7 layer1 Acc 0.9207894736842105, AUC 0.978540301322937, avg_entr 0.020493222400546074, f1 0.9207894802093506
ep7_l1_test_time 0.5877788066864014
Test Epoch7 layer2 Acc 0.9194736842105263, AUC 0.9795556664466858, avg_entr 0.015619919635355473, f1 0.9194737076759338
ep7_l2_test_time 0.8566958904266357
Test Epoch7 layer3 Acc 0.9189473684210526, AUC 0.9815186262130737, avg_entr 0.013660610653460026, f1 0.9189472794532776
ep7_l3_test_time 1.1219661235809326
Test Epoch7 layer4 Acc 0.9189473684210526, AUC 0.9784979820251465, avg_entr 0.01248827576637268, f1 0.9189472794532776
ep7_l4_test_time 1.3931217193603516
gc 0
Train Epoch8 Acc 0.96035 (115242/120000), AUC 0.9936956763267517
ep8_train_time 166.6940200328827
Test Epoch8 layer0 Acc 0.9231578947368421, AUC 0.9824157357215881, avg_entr 0.05705389380455017, f1 0.9231578707695007
ep8_l0_test_time 0.32578301429748535
Test Epoch8 layer1 Acc 0.9202631578947369, AUC 0.978552520275116, avg_entr 0.019832218065857887, f1 0.9202631711959839
ep8_l1_test_time 0.5937120914459229
Test Epoch8 layer2 Acc 0.9176315789473685, AUC 0.9803562164306641, avg_entr 0.01598781906068325, f1 0.9176315665245056
ep8_l2_test_time 0.8601517677307129
Test Epoch8 layer3 Acc 0.9171052631578948, AUC 0.9809706211090088, avg_entr 0.013976339250802994, f1 0.9171052575111389
ep8_l3_test_time 1.132951021194458
Test Epoch8 layer4 Acc 0.9178947368421052, AUC 0.9789412617683411, avg_entr 0.012738729827105999, f1 0.917894721031189
ep8_l4_test_time 1.3933327198028564
gc 0
Train Epoch9 Acc 0.9635 (115620/120000), AUC 0.9943827390670776
ep9_train_time 166.62031269073486
Test Epoch9 layer0 Acc 0.921578947368421, AUC 0.9825230836868286, avg_entr 0.05554523319005966, f1 0.9215789437294006
ep9_l0_test_time 0.3186490535736084
Test Epoch9 layer1 Acc 0.9176315789473685, AUC 0.9776096343994141, avg_entr 0.01872771792113781, f1 0.9176315665245056
ep9_l1_test_time 0.59429931640625
Test Epoch9 layer2 Acc 0.9152631578947369, AUC 0.9794641137123108, avg_entr 0.013646727427840233, f1 0.9152631759643555
ep9_l2_test_time 0.8689229488372803
Test Epoch9 layer3 Acc 0.9160526315789473, AUC 0.9806228876113892, avg_entr 0.011589573696255684, f1 0.9160526394844055
ep9_l3_test_time 1.110487699508667
Test Epoch9 layer4 Acc 0.9155263157894736, AUC 0.9809958338737488, avg_entr 0.01061209850013256, f1 0.9155263304710388
ep9_l4_test_time 1.3832552433013916
gc 0
Train Epoch10 Acc 0.9643666666666667 (115724/120000), AUC 0.9948706030845642
ep10_train_time 148.2203013896942
Test Epoch10 layer0 Acc 0.9184210526315789, AUC 0.982418417930603, avg_entr 0.05455904081463814, f1 0.9184210896492004
ep10_l0_test_time 0.15689849853515625
Test Epoch10 layer1 Acc 0.9184210526315789, AUC 0.9778200387954712, avg_entr 0.017893251031637192, f1 0.9184210896492004
ep10_l1_test_time 0.288435697555542
Test Epoch10 layer2 Acc 0.9160526315789473, AUC 0.9804617166519165, avg_entr 0.013376451097428799, f1 0.9160526394844055
ep10_l2_test_time 0.41854238510131836
Test Epoch10 layer3 Acc 0.9163157894736842, AUC 0.9813405871391296, avg_entr 0.011745183728635311, f1 0.9163157939910889
ep10_l3_test_time 0.5455396175384521
Test Epoch10 layer4 Acc 0.916578947368421, AUC 0.9810854196548462, avg_entr 0.010716956108808517, f1 0.9165789484977722
ep10_l4_test_time 0.6714146137237549
gc 0
Train Epoch11 Acc 0.965875 (115905/120000), AUC 0.9952049255371094
ep11_train_time 81.00164985656738
Test Epoch11 layer0 Acc 0.9213157894736842, AUC 0.9822883605957031, avg_entr 0.050949420779943466, f1 0.9213157892227173
ep11_l0_test_time 0.15673017501831055
Test Epoch11 layer1 Acc 0.9171052631578948, AUC 0.9767475128173828, avg_entr 0.016434136778116226, f1 0.9171052575111389
ep11_l1_test_time 0.28844666481018066
Test Epoch11 layer2 Acc 0.915, AUC 0.9784191846847534, avg_entr 0.012057219631969929, f1 0.9150000214576721
ep11_l2_test_time 0.4190354347229004
Test Epoch11 layer3 Acc 0.9152631578947369, AUC 0.9792510271072388, avg_entr 0.01053556241095066, f1 0.9152631759643555
ep11_l3_test_time 0.5500402450561523
Test Epoch11 layer4 Acc 0.915, AUC 0.9781757593154907, avg_entr 0.009525255300104618, f1 0.9150000214576721
ep11_l4_test_time 0.6706972122192383
gc 0
Train Epoch12 Acc 0.9664916666666666 (115979/120000), AUC 0.9952177405357361
ep12_train_time 81.0546293258667
Test Epoch12 layer0 Acc 0.9221052631578948, AUC 0.9822640419006348, avg_entr 0.04811731353402138, f1 0.9221052527427673
ep12_l0_test_time 0.15673589706420898
Test Epoch12 layer1 Acc 0.915, AUC 0.9764672517776489, avg_entr 0.015613890253007412, f1 0.9150000214576721
ep12_l1_test_time 0.2881481647491455
Test Epoch12 layer2 Acc 0.9144736842105263, AUC 0.9792124032974243, avg_entr 0.01083974726498127, f1 0.9144737124443054
ep12_l2_test_time 0.4186980724334717
Test Epoch12 layer3 Acc 0.9142105263157895, AUC 0.9798553586006165, avg_entr 0.008963747881352901, f1 0.9142104983329773
ep12_l3_test_time 0.5471043586730957
Test Epoch12 layer4 Acc 0.9144736842105263, AUC 0.9795228242874146, avg_entr 0.007819566875696182, f1 0.9144737124443054
ep12_l4_test_time 0.6716270446777344
gc 0
Train Epoch13 Acc 0.9679666666666666 (116156/120000), AUC 0.9953662157058716
ep13_train_time 81.01787900924683
Test Epoch13 layer0 Acc 0.9221052631578948, AUC 0.9822084903717041, avg_entr 0.04698103666305542, f1 0.9221052527427673
ep13_l0_test_time 0.1571657657623291
Test Epoch13 layer1 Acc 0.9147368421052632, AUC 0.9762980937957764, avg_entr 0.01612156257033348, f1 0.9147368669509888
ep13_l1_test_time 0.28850436210632324
Test Epoch13 layer2 Acc 0.9118421052631579, AUC 0.9771779179573059, avg_entr 0.0114020686596632, f1 0.9118421077728271
ep13_l2_test_time 0.419295072555542
Test Epoch13 layer3 Acc 0.9121052631578948, AUC 0.9772006273269653, avg_entr 0.009749186225235462, f1 0.9121052622795105
ep13_l3_test_time 0.5481443405151367
Test Epoch13 layer4 Acc 0.9123684210526316, AUC 0.9771953821182251, avg_entr 0.008651918731629848, f1 0.9123684167861938
ep13_l4_test_time 0.6726775169372559
gc 0
Train Epoch14 Acc 0.9686916666666666 (116243/120000), AUC 0.9958546161651611
ep14_train_time 81.00537300109863
Test Epoch14 layer0 Acc 0.9223684210526316, AUC 0.9821447134017944, avg_entr 0.046073637902736664, f1 0.9223684072494507
ep14_l0_test_time 0.1570582389831543
Test Epoch14 layer1 Acc 0.9152631578947369, AUC 0.9757382869720459, avg_entr 0.014790619723498821, f1 0.9152631759643555
ep14_l1_test_time 0.2880709171295166
Test Epoch14 layer2 Acc 0.9144736842105263, AUC 0.9778411984443665, avg_entr 0.010467862710356712, f1 0.9144737124443054
ep14_l2_test_time 0.41902780532836914
Test Epoch14 layer3 Acc 0.9139473684210526, AUC 0.9779077172279358, avg_entr 0.00868107657879591, f1 0.913947343826294
ep14_l3_test_time 0.5474205017089844
Test Epoch14 layer4 Acc 0.9139473684210526, AUC 0.9787288904190063, avg_entr 0.007666870020329952, f1 0.913947343826294
ep14_l4_test_time 0.6705942153930664
gc 0
Train Epoch15 Acc 0.969125 (116295/120000), AUC 0.9959026575088501
ep15_train_time 81.1320173740387
Test Epoch15 layer0 Acc 0.9221052631578948, AUC 0.9820953607559204, avg_entr 0.043840114027261734, f1 0.9221052527427673
ep15_l0_test_time 0.15722250938415527
Test Epoch15 layer1 Acc 0.9147368421052632, AUC 0.975448489189148, avg_entr 0.014082019217312336, f1 0.9147368669509888
ep15_l1_test_time 0.288895845413208
Test Epoch15 layer2 Acc 0.9142105263157895, AUC 0.9770691990852356, avg_entr 0.009629488922655582, f1 0.9142104983329773
ep15_l2_test_time 0.4189341068267822
Test Epoch15 layer3 Acc 0.9144736842105263, AUC 0.9771853685379028, avg_entr 0.00823014322668314, f1 0.9144737124443054
ep15_l3_test_time 0.545759916305542
Test Epoch15 layer4 Acc 0.9144736842105263, AUC 0.9783002138137817, avg_entr 0.007559140678495169, f1 0.9144737124443054
ep15_l4_test_time 0.6717314720153809
gc 0
Train Epoch16 Acc 0.9694083333333333 (116329/120000), AUC 0.9958494305610657
ep16_train_time 81.0779037475586
Test Epoch16 layer0 Acc 0.9221052631578948, AUC 0.9820646643638611, avg_entr 0.042976900935173035, f1 0.9221052527427673
ep16_l0_test_time 0.15717673301696777
Test Epoch16 layer1 Acc 0.9144736842105263, AUC 0.9752985239028931, avg_entr 0.013186465948820114, f1 0.9144737124443054
ep16_l1_test_time 0.28796863555908203
Test Epoch16 layer2 Acc 0.9131578947368421, AUC 0.9762402772903442, avg_entr 0.008598578162491322, f1 0.9131578803062439
ep16_l2_test_time 0.41884422302246094
Test Epoch16 layer3 Acc 0.9134210526315789, AUC 0.9767535328865051, avg_entr 0.007124553434550762, f1 0.9134210348129272
ep16_l3_test_time 0.5456912517547607
Test Epoch16 layer4 Acc 0.9136842105263158, AUC 0.9768806099891663, avg_entr 0.0060903276316821575, f1 0.9136841893196106
ep16_l4_test_time 0.6714632511138916
gc 0
Train Epoch17 Acc 0.9701666666666666 (116420/120000), AUC 0.996042788028717
ep17_train_time 81.09047842025757
Test Epoch17 layer0 Acc 0.9223684210526316, AUC 0.9821057915687561, avg_entr 0.0417172834277153, f1 0.9223684072494507
ep17_l0_test_time 0.15683197975158691
Test Epoch17 layer1 Acc 0.9142105263157895, AUC 0.9755738973617554, avg_entr 0.013110212050378323, f1 0.9142104983329773
ep17_l1_test_time 0.2887547016143799
Test Epoch17 layer2 Acc 0.9134210526315789, AUC 0.9764875769615173, avg_entr 0.008869011886417866, f1 0.9134210348129272
ep17_l2_test_time 0.41904568672180176
Test Epoch17 layer3 Acc 0.9131578947368421, AUC 0.9768169522285461, avg_entr 0.0073914420790970325, f1 0.9131578803062439
ep17_l3_test_time 0.5454432964324951
Test Epoch17 layer4 Acc 0.9134210526315789, AUC 0.9761544466018677, avg_entr 0.006649348419159651, f1 0.9134210348129272
ep17_l4_test_time 0.6734199523925781
gc 0
Train Epoch18 Acc 0.9705166666666667 (116462/120000), AUC 0.996135950088501
ep18_train_time 81.09324479103088
Test Epoch18 layer0 Acc 0.921578947368421, AUC 0.9820687770843506, avg_entr 0.04127508029341698, f1 0.9215789437294006
ep18_l0_test_time 0.1564338207244873
Test Epoch18 layer1 Acc 0.9142105263157895, AUC 0.9751397371292114, avg_entr 0.012976277619600296, f1 0.9142104983329773
ep18_l1_test_time 0.28843188285827637
Test Epoch18 layer2 Acc 0.9134210526315789, AUC 0.9759619832038879, avg_entr 0.009614000096917152, f1 0.9134210348129272
ep18_l2_test_time 0.41838669776916504
Test Epoch18 layer3 Acc 0.9126315789473685, AUC 0.9760980010032654, avg_entr 0.00804712064564228, f1 0.9126315712928772
ep18_l3_test_time 0.5452120304107666
Test Epoch18 layer4 Acc 0.9128947368421053, AUC 0.9756558537483215, avg_entr 0.0071408418007195, f1 0.9128947257995605
ep18_l4_test_time 0.6710059642791748
gc 0
Train Epoch19 Acc 0.9708416666666667 (116501/120000), AUC 0.9961373209953308
ep19_train_time 81.08245730400085
Test Epoch19 layer0 Acc 0.9207894736842105, AUC 0.9820824861526489, avg_entr 0.040550269186496735, f1 0.9207894802093506
ep19_l0_test_time 0.1575155258178711
Test Epoch19 layer1 Acc 0.9142105263157895, AUC 0.9747687578201294, avg_entr 0.012913776561617851, f1 0.9142104983329773
ep19_l1_test_time 0.2885165214538574
Test Epoch19 layer2 Acc 0.9134210526315789, AUC 0.9760832786560059, avg_entr 0.008696327917277813, f1 0.9134210348129272
ep19_l2_test_time 0.4181795120239258
Test Epoch19 layer3 Acc 0.9136842105263158, AUC 0.9757126569747925, avg_entr 0.00729979295283556, f1 0.9136841893196106
ep19_l3_test_time 0.5446231365203857
Test Epoch19 layer4 Acc 0.9131578947368421, AUC 0.9740782976150513, avg_entr 0.006396646145731211, f1 0.9131578803062439
ep19_l4_test_time 0.6721155643463135
gc 0
Train Epoch20 Acc 0.9708166666666667 (116498/120000), AUC 0.9960506558418274
ep20_train_time 81.05485486984253
Test Epoch20 layer0 Acc 0.9218421052631579, AUC 0.9821070432662964, avg_entr 0.038733307272195816, f1 0.921842098236084
ep20_l0_test_time 0.1569366455078125
Test Epoch20 layer1 Acc 0.9139473684210526, AUC 0.9755672216415405, avg_entr 0.011861479841172695, f1 0.913947343826294
ep20_l1_test_time 0.28888797760009766
Test Epoch20 layer2 Acc 0.9134210526315789, AUC 0.9765329360961914, avg_entr 0.008274066261947155, f1 0.9134210348129272
ep20_l2_test_time 0.4199824333190918
Test Epoch20 layer3 Acc 0.9134210526315789, AUC 0.9758588671684265, avg_entr 0.00697768060490489, f1 0.9134210348129272
ep20_l3_test_time 0.5459840297698975
Test Epoch20 layer4 Acc 0.9131578947368421, AUC 0.975739598274231, avg_entr 0.006106732413172722, f1 0.9131578803062439
ep20_l4_test_time 0.6718466281890869
gc 0
Train Epoch21 Acc 0.9710583333333334 (116527/120000), AUC 0.9962615966796875
ep21_train_time 81.0987319946289
Test Epoch21 layer0 Acc 0.921578947368421, AUC 0.9821019172668457, avg_entr 0.038183994591236115, f1 0.9215789437294006
ep21_l0_test_time 0.1571028232574463
Test Epoch21 layer1 Acc 0.9144736842105263, AUC 0.9745786786079407, avg_entr 0.011890767142176628, f1 0.9144737124443054
ep21_l1_test_time 0.2881348133087158
Test Epoch21 layer2 Acc 0.9131578947368421, AUC 0.9754085540771484, avg_entr 0.008127608336508274, f1 0.9131578803062439
ep21_l2_test_time 0.41835665702819824
Test Epoch21 layer3 Acc 0.9134210526315789, AUC 0.9743621349334717, avg_entr 0.006681082304567099, f1 0.9134210348129272
ep21_l3_test_time 0.5461428165435791
Test Epoch21 layer4 Acc 0.9136842105263158, AUC 0.9736557006835938, avg_entr 0.006120264995843172, f1 0.9136841893196106
ep21_l4_test_time 0.6704812049865723
gc 0
Train Epoch22 Acc 0.971425 (116571/120000), AUC 0.996279239654541
ep22_train_time 81.15502262115479
Test Epoch22 layer0 Acc 0.921578947368421, AUC 0.9820632338523865, avg_entr 0.037923526018857956, f1 0.9215789437294006
ep22_l0_test_time 0.15669870376586914
Test Epoch22 layer1 Acc 0.9136842105263158, AUC 0.9744313955307007, avg_entr 0.011890972964465618, f1 0.9136841893196106
ep22_l1_test_time 0.2884178161621094
Test Epoch22 layer2 Acc 0.9142105263157895, AUC 0.9744567275047302, avg_entr 0.00828766729682684, f1 0.9142104983329773
ep22_l2_test_time 0.4189646244049072
Test Epoch22 layer3 Acc 0.9134210526315789, AUC 0.9726324081420898, avg_entr 0.006962369661778212, f1 0.9134210348129272
ep22_l3_test_time 0.5467100143432617
Test Epoch22 layer4 Acc 0.9134210526315789, AUC 0.9723138809204102, avg_entr 0.006040867418050766, f1 0.9134210348129272
ep22_l4_test_time 0.6718487739562988
gc 0
Train Epoch23 Acc 0.971625 (116595/120000), AUC 0.9963011741638184
ep23_train_time 81.17909121513367
Test Epoch23 layer0 Acc 0.9221052631578948, AUC 0.9820652604103088, avg_entr 0.03738563135266304, f1 0.9221052527427673
ep23_l0_test_time 0.1578531265258789
Test Epoch23 layer1 Acc 0.9139473684210526, AUC 0.974686324596405, avg_entr 0.011460738256573677, f1 0.913947343826294
ep23_l1_test_time 0.28873157501220703
Test Epoch23 layer2 Acc 0.9142105263157895, AUC 0.974855363368988, avg_entr 0.008116716518998146, f1 0.9142104983329773
ep23_l2_test_time 0.418471097946167
Test Epoch23 layer3 Acc 0.9136842105263158, AUC 0.9741004109382629, avg_entr 0.006800036411732435, f1 0.9136841893196106
ep23_l3_test_time 0.5479872226715088
Test Epoch23 layer4 Acc 0.9134210526315789, AUC 0.9734964370727539, avg_entr 0.005820541176944971, f1 0.9134210348129272
ep23_l4_test_time 0.6713967323303223
gc 0
Train Epoch24 Acc 0.9720083333333334 (116641/120000), AUC 0.9963716268539429
ep24_train_time 81.15233063697815
Test Epoch24 layer0 Acc 0.9213157894736842, AUC 0.9820345044136047, avg_entr 0.036549683660268784, f1 0.9213157892227173
ep24_l0_test_time 0.15694046020507812
Test Epoch24 layer1 Acc 0.9136842105263158, AUC 0.9744887948036194, avg_entr 0.011573066003620625, f1 0.9136841893196106
ep24_l1_test_time 0.2881772518157959
Test Epoch24 layer2 Acc 0.9139473684210526, AUC 0.9750107526779175, avg_entr 0.008078161627054214, f1 0.913947343826294
ep24_l2_test_time 0.41813039779663086
Test Epoch24 layer3 Acc 0.9136842105263158, AUC 0.9736630320549011, avg_entr 0.0067294687032699585, f1 0.9136841893196106
ep24_l3_test_time 0.5465550422668457
Test Epoch24 layer4 Acc 0.9134210526315789, AUC 0.9728468656539917, avg_entr 0.005863488651812077, f1 0.9134210348129272
ep24_l4_test_time 0.6704277992248535
gc 0
Train Epoch25 Acc 0.9720916666666667 (116651/120000), AUC 0.9963037967681885
ep25_train_time 81.08479309082031
Test Epoch25 layer0 Acc 0.921578947368421, AUC 0.982025146484375, avg_entr 0.03647671267390251, f1 0.9215789437294006
ep25_l0_test_time 0.15662288665771484
Test Epoch25 layer1 Acc 0.9134210526315789, AUC 0.9741959571838379, avg_entr 0.011283672414720058, f1 0.9134210348129272
ep25_l1_test_time 0.2882366180419922
Test Epoch25 layer2 Acc 0.9134210526315789, AUC 0.974612295627594, avg_entr 0.007907290942966938, f1 0.9134210348129272
ep25_l2_test_time 0.41843247413635254
Test Epoch25 layer3 Acc 0.9131578947368421, AUC 0.9733426570892334, avg_entr 0.006633144803345203, f1 0.9131578803062439
ep25_l3_test_time 0.5460720062255859
Test Epoch25 layer4 Acc 0.9134210526315789, AUC 0.9731918573379517, avg_entr 0.005837397649884224, f1 0.9134210348129272
ep25_l4_test_time 0.6695854663848877
gc 0
Train Epoch26 Acc 0.9718333333333333 (116620/120000), AUC 0.9963780641555786
ep26_train_time 81.08796334266663
Test Epoch26 layer0 Acc 0.9223684210526316, AUC 0.9820461869239807, avg_entr 0.03634655103087425, f1 0.9223684072494507
ep26_l0_test_time 0.15627384185791016
Test Epoch26 layer1 Acc 0.9136842105263158, AUC 0.9741588234901428, avg_entr 0.011009247042238712, f1 0.9136841893196106
ep26_l1_test_time 0.28784751892089844
Test Epoch26 layer2 Acc 0.9134210526315789, AUC 0.9749482870101929, avg_entr 0.00742538645863533, f1 0.9134210348129272
ep26_l2_test_time 0.4194362163543701
Test Epoch26 layer3 Acc 0.9128947368421053, AUC 0.9732451438903809, avg_entr 0.005982923321425915, f1 0.9128947257995605
ep26_l3_test_time 0.5465266704559326
Test Epoch26 layer4 Acc 0.9128947368421053, AUC 0.972960889339447, avg_entr 0.005147957243025303, f1 0.9128947257995605
ep26_l4_test_time 0.6704344749450684
gc 0
Train Epoch27 Acc 0.9720333333333333 (116644/120000), AUC 0.9963294267654419
ep27_train_time 81.1788432598114
Test Epoch27 layer0 Acc 0.921578947368421, AUC 0.9820183515548706, avg_entr 0.03612729161977768, f1 0.9215789437294006
ep27_l0_test_time 0.15678787231445312
Test Epoch27 layer1 Acc 0.9139473684210526, AUC 0.9741787910461426, avg_entr 0.011259321123361588, f1 0.913947343826294
ep27_l1_test_time 0.2880678176879883
Test Epoch27 layer2 Acc 0.9139473684210526, AUC 0.9745915532112122, avg_entr 0.007938098162412643, f1 0.913947343826294
ep27_l2_test_time 0.41819000244140625
Test Epoch27 layer3 Acc 0.9142105263157895, AUC 0.9731192588806152, avg_entr 0.0065632592886686325, f1 0.9142104983329773
ep27_l3_test_time 0.546839714050293
Test Epoch27 layer4 Acc 0.9139473684210526, AUC 0.9721606969833374, avg_entr 0.005697382148355246, f1 0.913947343826294
ep27_l4_test_time 0.6711673736572266
gc 0
Train Epoch28 Acc 0.9719083333333334 (116629/120000), AUC 0.9963904619216919
ep28_train_time 81.11301970481873
Test Epoch28 layer0 Acc 0.9221052631578948, AUC 0.9820352792739868, avg_entr 0.036240871995687485, f1 0.9221052527427673
ep28_l0_test_time 0.1563248634338379
Test Epoch28 layer1 Acc 0.9134210526315789, AUC 0.9742695093154907, avg_entr 0.010999674908816814, f1 0.9134210348129272
ep28_l1_test_time 0.2878396511077881
Test Epoch28 layer2 Acc 0.9136842105263158, AUC 0.9748959541320801, avg_entr 0.007735463324934244, f1 0.9136841893196106
ep28_l2_test_time 0.41877245903015137
Test Epoch28 layer3 Acc 0.9131578947368421, AUC 0.9735580682754517, avg_entr 0.006472425535321236, f1 0.9131578803062439
ep28_l3_test_time 0.5480315685272217
Test Epoch28 layer4 Acc 0.9134210526315789, AUC 0.9729572534561157, avg_entr 0.005533578805625439, f1 0.9134210348129272
ep28_l4_test_time 0.6707909107208252
gc 0
Train Epoch29 Acc 0.9723166666666667 (116678/120000), AUC 0.9964074492454529
ep29_train_time 81.08823275566101
Test Epoch29 layer0 Acc 0.9221052631578948, AUC 0.9820117354393005, avg_entr 0.035630494356155396, f1 0.9221052527427673
ep29_l0_test_time 0.1575767993927002
Test Epoch29 layer1 Acc 0.9134210526315789, AUC 0.9742282032966614, avg_entr 0.010930044576525688, f1 0.9134210348129272
ep29_l1_test_time 0.2885701656341553
Test Epoch29 layer2 Acc 0.9134210526315789, AUC 0.9746525287628174, avg_entr 0.007672675419598818, f1 0.9134210348129272
ep29_l2_test_time 0.4186971187591553
Test Epoch29 layer3 Acc 0.9134210526315789, AUC 0.9727362990379333, avg_entr 0.006391205824911594, f1 0.9134210348129272
ep29_l3_test_time 0.5460696220397949
Test Epoch29 layer4 Acc 0.9136842105263158, AUC 0.972002387046814, avg_entr 0.005458006635308266, f1 0.9136841893196106
ep29_l4_test_time 0.6700494289398193
Best AUC tensor(0.9234) 6 0
train_loss (2, 5, 30)
valid_acc (5, 30)
valid_AUC (5, 30)
train_acc (30,)
total_train+valid_time 3443.8771274089813
Start Testing
Load ckpt at ckpt/ag_news_transformeral_l5_pad177//ag_news_transformeral_l5.pt
Test layer0 Acc 0.9136842105263158, AUC 0.9809153079986572, avg_entr 0.07037902623414993, f1 0.9136841893196106
l0_test_time 0.15584850311279297
Test layer1 Acc 0.9131578947368421, AUC 0.9780659675598145, avg_entr 0.026088297367095947, f1 0.9131578803062439
l1_test_time 0.2882883548736572
Test layer2 Acc 0.9142105263157895, AUC 0.9794623851776123, avg_entr 0.021159376949071884, f1 0.9142104983329773
l2_test_time 0.4226675033569336
Test layer3 Acc 0.9152631578947369, AUC 0.9804180264472961, avg_entr 0.019121116027235985, f1 0.9152631759643555
l3_test_time 0.5492911338806152
Test layer4 Acc 0.9147368421052632, AUC 0.9804638624191284, avg_entr 0.01751311495900154, f1 0.9147368669509888
l4_test_time 0.6699755191802979
