total count words 102019
vocab size 30000
train size 120000, valid size 3800, test size 3800
found 26754 words in glove
model: TransformerModelML(
  (layers): ModuleList(
    (0): EMBLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Embedding(30000, 300)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=4, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=4, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.weight 9000000
layers.0.ae.g.0.weight 512
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 512
layers.0.ae.h.0.bias 4
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
Total Trainable Params: 13672292
init_time 18.788313150405884
Start Training
gc 0
Train Epoch0 Acc 0.6294666666666666 (75536/120000), AUC 0.8573323488235474
ep0_train_time 81.29602003097534
Test Epoch0 layer0 Acc 0.9028947368421053, AUC 0.9771960377693176, avg_entr 0.24524737894535065, f1 0.9028947353363037
ep0_l0_test_time 0.15915894508361816
Save ckpt to ckpt/ag_news_transformeral_l5_pad177//ag_news_transformeral_l5.pt  ,ep 0
Test Epoch0 layer1 Acc 0.9097368421052632, AUC 0.9799621105194092, avg_entr 0.1637686938047409, f1 0.9097368717193604
ep0_l1_test_time 0.2912259101867676
Save ckpt to ckpt/ag_news_transformeral_l5_pad177//ag_news_transformeral_l5.pt  ,ep 0
Test Epoch0 layer2 Acc 0.906578947368421, AUC 0.979906439781189, avg_entr 0.1592273712158203, f1 0.9065789580345154
ep0_l2_test_time 0.4206247329711914
Test Epoch0 layer3 Acc 0.9047368421052632, AUC 0.9802421927452087, avg_entr 0.15388253331184387, f1 0.9047367572784424
ep0_l3_test_time 0.5444521903991699
Test Epoch0 layer4 Acc 0.9078947368421053, AUC 0.9801900386810303, avg_entr 0.15203841030597687, f1 0.9078947901725769
ep0_l4_test_time 0.6706950664520264
gc 0
Train Epoch1 Acc 0.9204583333333334 (110455/120000), AUC 0.9816313982009888
ep1_train_time 81.27322673797607
Test Epoch1 layer0 Acc 0.9152631578947369, AUC 0.9806035161018372, avg_entr 0.14331206679344177, f1 0.9152631759643555
ep1_l0_test_time 0.15677380561828613
Save ckpt to ckpt/ag_news_transformeral_l5_pad177//ag_news_transformeral_l5.pt  ,ep 1
Test Epoch1 layer1 Acc 0.9207894736842105, AUC 0.9830992221832275, avg_entr 0.07784595340490341, f1 0.9207894802093506
ep1_l1_test_time 0.29042649269104004
Save ckpt to ckpt/ag_news_transformeral_l5_pad177//ag_news_transformeral_l5.pt  ,ep 1
Test Epoch1 layer2 Acc 0.92, AUC 0.982490062713623, avg_entr 0.06630193442106247, f1 0.9200000166893005
ep1_l2_test_time 0.4221537113189697
Test Epoch1 layer3 Acc 0.9181578947368421, AUC 0.9827069640159607, avg_entr 0.06264635920524597, f1 0.9181578755378723
ep1_l3_test_time 0.5470819473266602
Test Epoch1 layer4 Acc 0.9189473684210526, AUC 0.9826621413230896, avg_entr 0.0553043931722641, f1 0.9189472794532776
ep1_l4_test_time 0.671302080154419
gc 0
Train Epoch2 Acc 0.9348583333333333 (112183/120000), AUC 0.9868663549423218
ep2_train_time 81.07941389083862
Test Epoch2 layer0 Acc 0.9194736842105263, AUC 0.9817211627960205, avg_entr 0.10780869424343109, f1 0.9194737076759338
ep2_l0_test_time 0.15789580345153809
Test Epoch2 layer1 Acc 0.9236842105263158, AUC 0.983022153377533, avg_entr 0.04083426669239998, f1 0.9236842393875122
ep2_l1_test_time 0.289522647857666
Save ckpt to ckpt/ag_news_transformeral_l5_pad177//ag_news_transformeral_l5.pt  ,ep 2
Test Epoch2 layer2 Acc 0.9231578947368421, AUC 0.9826149344444275, avg_entr 0.03515293821692467, f1 0.9231578707695007
ep2_l2_test_time 0.42030930519104004
Test Epoch2 layer3 Acc 0.9231578947368421, AUC 0.9827772974967957, avg_entr 0.033606287091970444, f1 0.9231578707695007
ep2_l3_test_time 0.5485639572143555
Test Epoch2 layer4 Acc 0.9231578947368421, AUC 0.9829529523849487, avg_entr 0.030232857912778854, f1 0.9231578707695007
ep2_l4_test_time 0.6751997470855713
gc 0
Train Epoch3 Acc 0.9429 (113148/120000), AUC 0.9890329241752625
ep3_train_time 80.97067761421204
Test Epoch3 layer0 Acc 0.9226315789473685, AUC 0.9823905229568481, avg_entr 0.08861100673675537, f1 0.922631561756134
ep3_l0_test_time 0.15744996070861816
Test Epoch3 layer1 Acc 0.9221052631578948, AUC 0.9834786057472229, avg_entr 0.03162085637450218, f1 0.9221052527427673
ep3_l1_test_time 0.2885274887084961
Test Epoch3 layer2 Acc 0.9234210526315789, AUC 0.9838464856147766, avg_entr 0.027334125712513924, f1 0.9234210252761841
ep3_l2_test_time 0.4190852642059326
Test Epoch3 layer3 Acc 0.9239473684210526, AUC 0.9841285347938538, avg_entr 0.025890758261084557, f1 0.9239473938941956
ep3_l3_test_time 0.5448997020721436
Save ckpt to ckpt/ag_news_transformeral_l5_pad177//ag_news_transformeral_l5.pt  ,ep 3
Test Epoch3 layer4 Acc 0.9239473684210526, AUC 0.9841665029525757, avg_entr 0.023918285965919495, f1 0.9239473938941956
ep3_l4_test_time 0.6785402297973633
Save ckpt to ckpt/ag_news_transformeral_l5_pad177//ag_news_transformeral_l5.pt  ,ep 3
gc 0
Train Epoch4 Acc 0.9480833333333333 (113770/120000), AUC 0.9904028177261353
ep4_train_time 81.0643162727356
Test Epoch4 layer0 Acc 0.9239473684210526, AUC 0.9825611710548401, avg_entr 0.07820115238428116, f1 0.9239473938941956
ep4_l0_test_time 0.15914535522460938
Save ckpt to ckpt/ag_news_transformeral_l5_pad177//ag_news_transformeral_l5.pt  ,ep 4
Test Epoch4 layer1 Acc 0.9236842105263158, AUC 0.9811413288116455, avg_entr 0.026272861286997795, f1 0.9236842393875122
ep4_l1_test_time 0.29679250717163086
Test Epoch4 layer2 Acc 0.9234210526315789, AUC 0.9800138473510742, avg_entr 0.022070009261369705, f1 0.9234210252761841
ep4_l2_test_time 0.42025089263916016
Test Epoch4 layer3 Acc 0.9239473684210526, AUC 0.9816421866416931, avg_entr 0.020812664180994034, f1 0.9239473938941956
ep4_l3_test_time 0.547459602355957
Save ckpt to ckpt/ag_news_transformeral_l5_pad177//ag_news_transformeral_l5.pt  ,ep 4
Test Epoch4 layer4 Acc 0.9234210526315789, AUC 0.9807260036468506, avg_entr 0.019046982750296593, f1 0.9234210252761841
ep4_l4_test_time 0.6748201847076416
gc 0
Train Epoch5 Acc 0.9524583333333333 (114295/120000), AUC 0.9911332130432129
ep5_train_time 80.98789191246033
Test Epoch5 layer0 Acc 0.9239473684210526, AUC 0.9828094840049744, avg_entr 0.07047738879919052, f1 0.9239473938941956
ep5_l0_test_time 0.1569690704345703
Save ckpt to ckpt/ag_news_transformeral_l5_pad177//ag_news_transformeral_l5.pt  ,ep 5
Test Epoch5 layer1 Acc 0.9228947368421052, AUC 0.9817163348197937, avg_entr 0.022115765139460564, f1 0.9228947162628174
ep5_l1_test_time 0.28992414474487305
Test Epoch5 layer2 Acc 0.9234210526315789, AUC 0.9829844236373901, avg_entr 0.017654143273830414, f1 0.9234210252761841
ep5_l2_test_time 0.41872429847717285
Test Epoch5 layer3 Acc 0.9236842105263158, AUC 0.9836655259132385, avg_entr 0.016237029805779457, f1 0.9236842393875122
ep5_l3_test_time 0.5462651252746582
Test Epoch5 layer4 Acc 0.9234210526315789, AUC 0.9833185076713562, avg_entr 0.01454103272408247, f1 0.9234210252761841
ep5_l4_test_time 0.6713457107543945
gc 0
Train Epoch6 Acc 0.9562083333333333 (114745/120000), AUC 0.9920457601547241
ep6_train_time 80.98481917381287
Test Epoch6 layer0 Acc 0.9213157894736842, AUC 0.9825992584228516, avg_entr 0.06776747107505798, f1 0.9213157892227173
ep6_l0_test_time 0.1578528881072998
Test Epoch6 layer1 Acc 0.9221052631578948, AUC 0.9795628190040588, avg_entr 0.02302306890487671, f1 0.9221052527427673
ep6_l1_test_time 0.28894543647766113
Test Epoch6 layer2 Acc 0.9207894736842105, AUC 0.9809770584106445, avg_entr 0.019467880949378014, f1 0.9207894802093506
ep6_l2_test_time 0.4194464683532715
Test Epoch6 layer3 Acc 0.921578947368421, AUC 0.9832080006599426, avg_entr 0.018009644001722336, f1 0.9215789437294006
ep6_l3_test_time 0.5478832721710205
Test Epoch6 layer4 Acc 0.9213157894736842, AUC 0.9830909967422485, avg_entr 0.016675002872943878, f1 0.9213157892227173
ep6_l4_test_time 0.6720950603485107
gc 0
Train Epoch7 Acc 0.958825 (115059/120000), AUC 0.9930027723312378
ep7_train_time 80.99361872673035
Test Epoch7 layer0 Acc 0.92, AUC 0.9823527336120605, avg_entr 0.05867474153637886, f1 0.9200000166893005
ep7_l0_test_time 0.15799212455749512
Test Epoch7 layer1 Acc 0.9186842105263158, AUC 0.9794251918792725, avg_entr 0.020528970286250114, f1 0.918684184551239
ep7_l1_test_time 0.2892639636993408
Test Epoch7 layer2 Acc 0.9176315789473685, AUC 0.9812850952148438, avg_entr 0.015707001090049744, f1 0.9176315665245056
ep7_l2_test_time 0.41924476623535156
Test Epoch7 layer3 Acc 0.9176315789473685, AUC 0.982048511505127, avg_entr 0.014557908289134502, f1 0.9176315665245056
ep7_l3_test_time 0.5466547012329102
Test Epoch7 layer4 Acc 0.9168421052631579, AUC 0.9815875291824341, avg_entr 0.013219699263572693, f1 0.9168421030044556
ep7_l4_test_time 0.6720025539398193
gc 0
Train Epoch8 Acc 0.9614333333333334 (115372/120000), AUC 0.9938019514083862
ep8_train_time 80.98792719841003
Test Epoch8 layer0 Acc 0.921578947368421, AUC 0.9822116494178772, avg_entr 0.05666009336709976, f1 0.9215789437294006
ep8_l0_test_time 0.15683960914611816
Test Epoch8 layer1 Acc 0.9202631578947369, AUC 0.9781823754310608, avg_entr 0.020333295688033104, f1 0.9202631711959839
ep8_l1_test_time 0.28883934020996094
Test Epoch8 layer2 Acc 0.9192105263157895, AUC 0.9782889485359192, avg_entr 0.01637299358844757, f1 0.9192105531692505
ep8_l2_test_time 0.418590784072876
Test Epoch8 layer3 Acc 0.9184210526315789, AUC 0.9790674448013306, avg_entr 0.01506247278302908, f1 0.9184210896492004
ep8_l3_test_time 0.5467426776885986
Test Epoch8 layer4 Acc 0.9186842105263158, AUC 0.9796426296234131, avg_entr 0.01427860651165247, f1 0.918684184551239
ep8_l4_test_time 0.6720571517944336
gc 0
Train Epoch9 Acc 0.9634083333333333 (115609/120000), AUC 0.9942695498466492
ep9_train_time 81.01171588897705
Test Epoch9 layer0 Acc 0.9218421052631579, AUC 0.9822788238525391, avg_entr 0.05412227287888527, f1 0.921842098236084
ep9_l0_test_time 0.1575300693511963
Test Epoch9 layer1 Acc 0.9176315789473685, AUC 0.977726936340332, avg_entr 0.01879025436937809, f1 0.9176315665245056
ep9_l1_test_time 0.28878116607666016
Test Epoch9 layer2 Acc 0.916578947368421, AUC 0.9786792397499084, avg_entr 0.014404283836483955, f1 0.9165789484977722
ep9_l2_test_time 0.41945481300354004
Test Epoch9 layer3 Acc 0.9173684210526316, AUC 0.9800823926925659, avg_entr 0.012817527167499065, f1 0.9173683524131775
ep9_l3_test_time 0.5478072166442871
Test Epoch9 layer4 Acc 0.9173684210526316, AUC 0.9810725450515747, avg_entr 0.011600218713283539, f1 0.9173683524131775
ep9_l4_test_time 0.6717097759246826
gc 0
Train Epoch10 Acc 0.9659833333333333 (115918/120000), AUC 0.9949221611022949
ep10_train_time 80.98880863189697
Test Epoch10 layer0 Acc 0.9207894736842105, AUC 0.9821763038635254, avg_entr 0.05182621628046036, f1 0.9207894802093506
ep10_l0_test_time 0.15741753578186035
Test Epoch10 layer1 Acc 0.9181578947368421, AUC 0.9756476283073425, avg_entr 0.01835794188082218, f1 0.9181578755378723
ep10_l1_test_time 0.28882408142089844
Test Epoch10 layer2 Acc 0.9160526315789473, AUC 0.9763760566711426, avg_entr 0.01392326783388853, f1 0.9160526394844055
ep10_l2_test_time 0.4190642833709717
Test Epoch10 layer3 Acc 0.9160526315789473, AUC 0.9782814383506775, avg_entr 0.012339831329882145, f1 0.9160526394844055
ep10_l3_test_time 0.5479905605316162
Test Epoch10 layer4 Acc 0.9155263157894736, AUC 0.9786713719367981, avg_entr 0.01123845111578703, f1 0.9155263304710388
ep10_l4_test_time 0.6711270809173584
gc 0
Train Epoch11 Acc 0.9671166666666666 (116054/120000), AUC 0.9951142072677612
ep11_train_time 81.07228779792786
Test Epoch11 layer0 Acc 0.9221052631578948, AUC 0.9821149110794067, avg_entr 0.04989684745669365, f1 0.9221052527427673
ep11_l0_test_time 0.15754938125610352
Test Epoch11 layer1 Acc 0.9157894736842105, AUC 0.9753391742706299, avg_entr 0.018132157623767853, f1 0.9157894849777222
ep11_l1_test_time 0.28847813606262207
Test Epoch11 layer2 Acc 0.916578947368421, AUC 0.9755493402481079, avg_entr 0.013331237249076366, f1 0.9165789484977722
ep11_l2_test_time 0.41957736015319824
Test Epoch11 layer3 Acc 0.9176315789473685, AUC 0.9780416488647461, avg_entr 0.011561121791601181, f1 0.9176315665245056
ep11_l3_test_time 0.5468881130218506
Test Epoch11 layer4 Acc 0.9176315789473685, AUC 0.977674126625061, avg_entr 0.01035817340016365, f1 0.9176315665245056
ep11_l4_test_time 0.6712877750396729
gc 0
Train Epoch12 Acc 0.9684333333333334 (116212/120000), AUC 0.995625913143158
ep12_train_time 81.1015248298645
Test Epoch12 layer0 Acc 0.9197368421052632, AUC 0.9819045662879944, avg_entr 0.04697448015213013, f1 0.9197368621826172
ep12_l0_test_time 0.15692925453186035
Test Epoch12 layer1 Acc 0.9192105263157895, AUC 0.9737265110015869, avg_entr 0.017090702429413795, f1 0.9192105531692505
ep12_l1_test_time 0.2888607978820801
Test Epoch12 layer2 Acc 0.9173684210526316, AUC 0.973445475101471, avg_entr 0.012556642293930054, f1 0.9173683524131775
ep12_l2_test_time 0.4196741580963135
Test Epoch12 layer3 Acc 0.9176315789473685, AUC 0.9764285683631897, avg_entr 0.010830707848072052, f1 0.9176315665245056
ep12_l3_test_time 0.5466568470001221
Test Epoch12 layer4 Acc 0.9173684210526316, AUC 0.9776716232299805, avg_entr 0.009810775518417358, f1 0.9173683524131775
ep12_l4_test_time 0.6720185279846191
gc 0
Train Epoch13 Acc 0.9692 (116304/120000), AUC 0.9956400990486145
ep13_train_time 80.99362754821777
Test Epoch13 layer0 Acc 0.9210526315789473, AUC 0.9818844795227051, avg_entr 0.04595254361629486, f1 0.9210526347160339
ep13_l0_test_time 0.15790343284606934
Test Epoch13 layer1 Acc 0.9176315789473685, AUC 0.9743449091911316, avg_entr 0.01678902842104435, f1 0.9176315665245056
ep13_l1_test_time 0.28881359100341797
Test Epoch13 layer2 Acc 0.9147368421052632, AUC 0.9738515615463257, avg_entr 0.011977492831647396, f1 0.9147368669509888
ep13_l2_test_time 0.4191560745239258
Test Epoch13 layer3 Acc 0.9155263157894736, AUC 0.9756536483764648, avg_entr 0.010111006908118725, f1 0.9155263304710388
ep13_l3_test_time 0.5482661724090576
Test Epoch13 layer4 Acc 0.9152631578947369, AUC 0.9774274230003357, avg_entr 0.009036481380462646, f1 0.9152631759643555
ep13_l4_test_time 0.670062780380249
gc 0
Train Epoch14 Acc 0.9701833333333333 (116422/120000), AUC 0.9958926439285278
ep14_train_time 81.09576439857483
Test Epoch14 layer0 Acc 0.9210526315789473, AUC 0.9818992018699646, avg_entr 0.04485953226685524, f1 0.9210526347160339
ep14_l0_test_time 0.15734124183654785
Test Epoch14 layer1 Acc 0.9163157894736842, AUC 0.9740809798240662, avg_entr 0.016194531694054604, f1 0.9163157939910889
ep14_l1_test_time 0.2889575958251953
Test Epoch14 layer2 Acc 0.9155263157894736, AUC 0.9740729928016663, avg_entr 0.011934103444218636, f1 0.9155263304710388
ep14_l2_test_time 0.418503999710083
Test Epoch14 layer3 Acc 0.9155263157894736, AUC 0.9754090309143066, avg_entr 0.009913238696753979, f1 0.9155263304710388
ep14_l3_test_time 0.5473651885986328
Test Epoch14 layer4 Acc 0.915, AUC 0.9761800169944763, avg_entr 0.009071651846170425, f1 0.9150000214576721
ep14_l4_test_time 0.6702561378479004
gc 0
Train Epoch15 Acc 0.9710416666666667 (116525/120000), AUC 0.995941162109375
ep15_train_time 80.99862742424011
Test Epoch15 layer0 Acc 0.9210526315789473, AUC 0.9818376302719116, avg_entr 0.0436926893889904, f1 0.9210526347160339
ep15_l0_test_time 0.1616809368133545
Test Epoch15 layer1 Acc 0.9160526315789473, AUC 0.973820149898529, avg_entr 0.0157762560993433, f1 0.9160526394844055
ep15_l1_test_time 0.289107084274292
Test Epoch15 layer2 Acc 0.915, AUC 0.9738297462463379, avg_entr 0.0112019507214427, f1 0.9150000214576721
ep15_l2_test_time 0.4196195602416992
Test Epoch15 layer3 Acc 0.9144736842105263, AUC 0.9742534160614014, avg_entr 0.00922657735645771, f1 0.9144737124443054
ep15_l3_test_time 0.5480165481567383
Test Epoch15 layer4 Acc 0.9142105263157895, AUC 0.9759767651557922, avg_entr 0.008456586860120296, f1 0.9142104983329773
ep15_l4_test_time 0.6717281341552734
gc 0
Train Epoch16 Acc 0.971375 (116565/120000), AUC 0.9962206482887268
ep16_train_time 80.97091150283813
Test Epoch16 layer0 Acc 0.9210526315789473, AUC 0.9818223118782043, avg_entr 0.041666872799396515, f1 0.9210526347160339
ep16_l0_test_time 0.15696287155151367
Test Epoch16 layer1 Acc 0.915, AUC 0.9743398427963257, avg_entr 0.015287213027477264, f1 0.9150000214576721
ep16_l1_test_time 0.2895991802215576
Test Epoch16 layer2 Acc 0.9142105263157895, AUC 0.9749088883399963, avg_entr 0.011552315205335617, f1 0.9142104983329773
ep16_l2_test_time 0.4176208972930908
Test Epoch16 layer3 Acc 0.9134210526315789, AUC 0.9751701354980469, avg_entr 0.009566840715706348, f1 0.9134210348129272
ep16_l3_test_time 0.5460829734802246
Test Epoch16 layer4 Acc 0.9136842105263158, AUC 0.9772313833236694, avg_entr 0.008723699487745762, f1 0.9136841893196106
ep16_l4_test_time 0.6690211296081543
gc 0
Train Epoch17 Acc 0.9718666666666667 (116624/120000), AUC 0.9962453246116638
ep17_train_time 81.07957673072815
Test Epoch17 layer0 Acc 0.92, AUC 0.9818189144134521, avg_entr 0.04067869111895561, f1 0.9200000166893005
ep17_l0_test_time 0.15769362449645996
Test Epoch17 layer1 Acc 0.9144736842105263, AUC 0.9735681414604187, avg_entr 0.014774130657315254, f1 0.9144737124443054
ep17_l1_test_time 0.289459228515625
Test Epoch17 layer2 Acc 0.9128947368421053, AUC 0.9736857414245605, avg_entr 0.011090300045907497, f1 0.9128947257995605
ep17_l2_test_time 0.41924285888671875
Test Epoch17 layer3 Acc 0.9131578947368421, AUC 0.974181056022644, avg_entr 0.009517822414636612, f1 0.9131578803062439
ep17_l3_test_time 0.5475945472717285
Test Epoch17 layer4 Acc 0.9136842105263158, AUC 0.97608482837677, avg_entr 0.008524460718035698, f1 0.9136841893196106
ep17_l4_test_time 0.6698191165924072
gc 0
Train Epoch18 Acc 0.9724166666666667 (116690/120000), AUC 0.9963350296020508
ep18_train_time 81.13061761856079
Test Epoch18 layer0 Acc 0.9197368421052632, AUC 0.9817763566970825, avg_entr 0.039903853088617325, f1 0.9197368621826172
ep18_l0_test_time 0.1574561595916748
Test Epoch18 layer1 Acc 0.9136842105263158, AUC 0.9730350971221924, avg_entr 0.014811052940785885, f1 0.9136841893196106
ep18_l1_test_time 0.2888364791870117
Test Epoch18 layer2 Acc 0.9136842105263158, AUC 0.974310040473938, avg_entr 0.010380126535892487, f1 0.9136841893196106
ep18_l2_test_time 0.41854023933410645
Test Epoch18 layer3 Acc 0.9134210526315789, AUC 0.9746478796005249, avg_entr 0.008226425386965275, f1 0.9134210348129272
ep18_l3_test_time 0.5477721691131592
Test Epoch18 layer4 Acc 0.9136842105263158, AUC 0.9754376411437988, avg_entr 0.007485867012292147, f1 0.9136841893196106
ep18_l4_test_time 0.6705625057220459
gc 0
Train Epoch19 Acc 0.972775 (116733/120000), AUC 0.9964381456375122
ep19_train_time 81.0409140586853
Test Epoch19 layer0 Acc 0.9205263157894736, AUC 0.9817776083946228, avg_entr 0.03846929967403412, f1 0.9205263257026672
ep19_l0_test_time 0.1571180820465088
Test Epoch19 layer1 Acc 0.9144736842105263, AUC 0.9731217622756958, avg_entr 0.013974911533296108, f1 0.9144737124443054
ep19_l1_test_time 0.28995752334594727
Test Epoch19 layer2 Acc 0.9131578947368421, AUC 0.9728058576583862, avg_entr 0.00960512738674879, f1 0.9131578803062439
ep19_l2_test_time 0.4193298816680908
Test Epoch19 layer3 Acc 0.9131578947368421, AUC 0.9721680879592896, avg_entr 0.007955910637974739, f1 0.9131578803062439
ep19_l3_test_time 0.5465912818908691
Test Epoch19 layer4 Acc 0.9134210526315789, AUC 0.9729657769203186, avg_entr 0.007391692139208317, f1 0.9134210348129272
ep19_l4_test_time 0.6707594394683838
gc 0
Train Epoch20 Acc 0.9728833333333333 (116746/120000), AUC 0.9962714910507202
ep20_train_time 81.10741758346558
Test Epoch20 layer0 Acc 0.9205263157894736, AUC 0.9817848205566406, avg_entr 0.03779296204447746, f1 0.9205263257026672
ep20_l0_test_time 0.15742754936218262
Test Epoch20 layer1 Acc 0.9142105263157895, AUC 0.9726260304450989, avg_entr 0.014365660026669502, f1 0.9142104983329773
ep20_l1_test_time 0.2900846004486084
Test Epoch20 layer2 Acc 0.9126315789473685, AUC 0.9728536605834961, avg_entr 0.0107134273275733, f1 0.9126315712928772
ep20_l2_test_time 0.4196338653564453
Test Epoch20 layer3 Acc 0.9123684210526316, AUC 0.9726899862289429, avg_entr 0.008914252743124962, f1 0.9123684167861938
ep20_l3_test_time 0.5492067337036133
Test Epoch20 layer4 Acc 0.9131578947368421, AUC 0.9730888605117798, avg_entr 0.008136640302836895, f1 0.9131578803062439
ep20_l4_test_time 0.6713502407073975
gc 0
Train Epoch21 Acc 0.9732166666666666 (116786/120000), AUC 0.9964733123779297
ep21_train_time 122.12922620773315
Test Epoch21 layer0 Acc 0.9186842105263158, AUC 0.9817363619804382, avg_entr 0.03667859360575676, f1 0.918684184551239
ep21_l0_test_time 0.3217604160308838
Test Epoch21 layer1 Acc 0.9144736842105263, AUC 0.9726208448410034, avg_entr 0.01396708469837904, f1 0.9144737124443054
ep21_l1_test_time 0.5930490493774414
Test Epoch21 layer2 Acc 0.9126315789473685, AUC 0.9734102487564087, avg_entr 0.010285183787345886, f1 0.9126315712928772
ep21_l2_test_time 0.8637604713439941
Test Epoch21 layer3 Acc 0.9121052631578948, AUC 0.9729878902435303, avg_entr 0.008485576137900352, f1 0.9121052622795105
ep21_l3_test_time 1.1211378574371338
Test Epoch21 layer4 Acc 0.9121052631578948, AUC 0.9733266234397888, avg_entr 0.0076646325178444386, f1 0.9121052622795105
ep21_l4_test_time 1.3897039890289307
gc 0
Train Epoch22 Acc 0.9738416666666667 (116861/120000), AUC 0.9965555667877197
ep22_train_time 92.94331884384155
Test Epoch22 layer0 Acc 0.9194736842105263, AUC 0.9817327857017517, avg_entr 0.037118200212717056, f1 0.9194737076759338
ep22_l0_test_time 0.1599409580230713
Test Epoch22 layer1 Acc 0.915, AUC 0.972040057182312, avg_entr 0.014086265116930008, f1 0.9150000214576721
ep22_l1_test_time 0.29201316833496094
Test Epoch22 layer2 Acc 0.9142105263157895, AUC 0.9733226895332336, avg_entr 0.010158699005842209, f1 0.9142104983329773
ep22_l2_test_time 0.4212605953216553
Test Epoch22 layer3 Acc 0.9144736842105263, AUC 0.9732194542884827, avg_entr 0.008045051246881485, f1 0.9144737124443054
ep22_l3_test_time 0.5505189895629883
Test Epoch22 layer4 Acc 0.9144736842105263, AUC 0.9736915826797485, avg_entr 0.007102344185113907, f1 0.9144737124443054
ep22_l4_test_time 0.674048900604248
gc 0
Train Epoch23 Acc 0.9740916666666667 (116891/120000), AUC 0.9966034293174744
ep23_train_time 125.86857628822327
Test Epoch23 layer0 Acc 0.92, AUC 0.9817203283309937, avg_entr 0.03727944567799568, f1 0.9200000166893005
ep23_l0_test_time 0.15756869316101074
Test Epoch23 layer1 Acc 0.9155263157894736, AUC 0.9721953868865967, avg_entr 0.014306476339697838, f1 0.9155263304710388
ep23_l1_test_time 0.2889394760131836
Test Epoch23 layer2 Acc 0.9142105263157895, AUC 0.9742125868797302, avg_entr 0.010319501161575317, f1 0.9142104983329773
ep23_l2_test_time 0.4188966751098633
Test Epoch23 layer3 Acc 0.9134210526315789, AUC 0.9740331172943115, avg_entr 0.00818933267146349, f1 0.9134210348129272
ep23_l3_test_time 0.5460145473480225
Test Epoch23 layer4 Acc 0.9134210526315789, AUC 0.9747746586799622, avg_entr 0.007148361764848232, f1 0.9134210348129272
ep23_l4_test_time 0.6704080104827881
gc 0
Train Epoch24 Acc 0.9743333333333334 (116920/120000), AUC 0.9966422319412231
ep24_train_time 136.57231545448303
Test Epoch24 layer0 Acc 0.9205263157894736, AUC 0.9817148447036743, avg_entr 0.03619742393493652, f1 0.9205263257026672
ep24_l0_test_time 0.3114802837371826
Test Epoch24 layer1 Acc 0.9142105263157895, AUC 0.9724783897399902, avg_entr 0.013866016641259193, f1 0.9142104983329773
ep24_l1_test_time 0.5899326801300049
Test Epoch24 layer2 Acc 0.9126315789473685, AUC 0.9737805724143982, avg_entr 0.010649981908500195, f1 0.9126315712928772
ep24_l2_test_time 0.8575890064239502
Test Epoch24 layer3 Acc 0.9118421052631579, AUC 0.9734789729118347, avg_entr 0.008776760660111904, f1 0.9118421077728271
ep24_l3_test_time 1.1175310611724854
Test Epoch24 layer4 Acc 0.911578947368421, AUC 0.9743350744247437, avg_entr 0.008007432334125042, f1 0.9115789532661438
ep24_l4_test_time 1.3900833129882812
gc 0
Train Epoch25 Acc 0.9739416666666667 (116873/120000), AUC 0.9965920448303223
ep25_train_time 166.78059911727905
Test Epoch25 layer0 Acc 0.9205263157894736, AUC 0.981698751449585, avg_entr 0.035769276320934296, f1 0.9205263257026672
ep25_l0_test_time 0.33001208305358887
Test Epoch25 layer1 Acc 0.9142105263157895, AUC 0.9721706509590149, avg_entr 0.013729286380112171, f1 0.9142104983329773
ep25_l1_test_time 0.598081111907959
Test Epoch25 layer2 Acc 0.9118421052631579, AUC 0.9730790853500366, avg_entr 0.01045761164277792, f1 0.9118421077728271
ep25_l2_test_time 0.8618125915527344
Test Epoch25 layer3 Acc 0.9118421052631579, AUC 0.9728410840034485, avg_entr 0.008536020293831825, f1 0.9118421077728271
ep25_l3_test_time 1.1258480548858643
Test Epoch25 layer4 Acc 0.9118421052631579, AUC 0.9733493328094482, avg_entr 0.007826577872037888, f1 0.9118421077728271
ep25_l4_test_time 1.3921406269073486
gc 0
Train Epoch26 Acc 0.974425 (116931/120000), AUC 0.9967223405838013
ep26_train_time 166.67279195785522
Test Epoch26 layer0 Acc 0.9202631578947369, AUC 0.9816864132881165, avg_entr 0.035822801291942596, f1 0.9202631711959839
ep26_l0_test_time 0.33116674423217773
Test Epoch26 layer1 Acc 0.9144736842105263, AUC 0.972066342830658, avg_entr 0.01371993962675333, f1 0.9144737124443054
ep26_l1_test_time 0.5927257537841797
Test Epoch26 layer2 Acc 0.9113157894736842, AUC 0.9734807014465332, avg_entr 0.010183417238295078, f1 0.9113157987594604
ep26_l2_test_time 0.864084005355835
Test Epoch26 layer3 Acc 0.9113157894736842, AUC 0.9731656908988953, avg_entr 0.008341834880411625, f1 0.9113157987594604
ep26_l3_test_time 1.1240272521972656
Test Epoch26 layer4 Acc 0.9121052631578948, AUC 0.9734892845153809, avg_entr 0.007611646316945553, f1 0.9121052622795105
ep26_l4_test_time 1.39150071144104
gc 0
Train Epoch27 Acc 0.974525 (116943/120000), AUC 0.9966957569122314
ep27_train_time 166.74970626831055
Test Epoch27 layer0 Acc 0.9205263157894736, AUC 0.9816890358924866, avg_entr 0.03578063100576401, f1 0.9205263257026672
ep27_l0_test_time 0.33191633224487305
Test Epoch27 layer1 Acc 0.9142105263157895, AUC 0.9721160531044006, avg_entr 0.013851800002157688, f1 0.9142104983329773
ep27_l1_test_time 0.5976693630218506
Test Epoch27 layer2 Acc 0.9136842105263158, AUC 0.9729887247085571, avg_entr 0.010740559548139572, f1 0.9136841893196106
ep27_l2_test_time 0.8597092628479004
Test Epoch27 layer3 Acc 0.9136842105263158, AUC 0.9723568558692932, avg_entr 0.008745143190026283, f1 0.9136841893196106
ep27_l3_test_time 1.123715877532959
Test Epoch27 layer4 Acc 0.9139473684210526, AUC 0.9728282690048218, avg_entr 0.007937935180962086, f1 0.913947343826294
ep27_l4_test_time 1.3858907222747803
gc 0
Train Epoch28 Acc 0.974525 (116943/120000), AUC 0.9967028498649597
ep28_train_time 166.75397372245789
Test Epoch28 layer0 Acc 0.92, AUC 0.9816799163818359, avg_entr 0.03561000898480415, f1 0.9200000166893005
ep28_l0_test_time 0.3281364440917969
Test Epoch28 layer1 Acc 0.9142105263157895, AUC 0.972130298614502, avg_entr 0.013778402470052242, f1 0.9142104983329773
ep28_l1_test_time 0.596808671951294
Test Epoch28 layer2 Acc 0.9134210526315789, AUC 0.973253071308136, avg_entr 0.01069978903979063, f1 0.9134210348129272
ep28_l2_test_time 0.8568928241729736
Test Epoch28 layer3 Acc 0.9136842105263158, AUC 0.9728823304176331, avg_entr 0.00874620582908392, f1 0.9136841893196106
ep28_l3_test_time 1.11946439743042
Test Epoch28 layer4 Acc 0.9139473684210526, AUC 0.9733098745346069, avg_entr 0.007925908081233501, f1 0.913947343826294
ep28_l4_test_time 1.3913071155548096
gc 0
Train Epoch29 Acc 0.9744416666666667 (116933/120000), AUC 0.9966842532157898
ep29_train_time 166.62034368515015
Test Epoch29 layer0 Acc 0.92, AUC 0.981670618057251, avg_entr 0.03548075631260872, f1 0.9200000166893005
ep29_l0_test_time 0.3226897716522217
Test Epoch29 layer1 Acc 0.9142105263157895, AUC 0.9720940589904785, avg_entr 0.01369722280651331, f1 0.9142104983329773
ep29_l1_test_time 0.5919554233551025
Test Epoch29 layer2 Acc 0.9131578947368421, AUC 0.9734214544296265, avg_entr 0.010627616196870804, f1 0.9131578803062439
ep29_l2_test_time 0.8655612468719482
Test Epoch29 layer3 Acc 0.9134210526315789, AUC 0.9729682803153992, avg_entr 0.00865770410746336, f1 0.9134210348129272
ep29_l3_test_time 1.1336703300476074
Test Epoch29 layer4 Acc 0.9136842105263158, AUC 0.9735124707221985, avg_entr 0.007833591662347317, f1 0.9136841893196106
ep29_l4_test_time 1.3874542713165283
Best AUC tensor(0.9239) 5 0
train_loss (2, 5, 30)
valid_acc (5, 30)
valid_AUC (5, 30)
train_acc (30,)
total_train+valid_time 3093.669200658798
Start Testing
Load ckpt at ckpt/ag_news_transformeral_l5_pad177//ag_news_transformeral_l5.pt
Test layer0 Acc 0.915, AUC 0.9810593128204346, avg_entr 0.07661862671375275, f1 0.9150000214576721
l0_test_time 0.3299984931945801
Test layer1 Acc 0.9157894736842105, AUC 0.9798399806022644, avg_entr 0.02703343704342842, f1 0.9157894849777222
l1_test_time 0.5914134979248047
Test layer2 Acc 0.9163157894736842, AUC 0.9821534752845764, avg_entr 0.02180432341992855, f1 0.9163157939910889
l2_test_time 0.863292932510376
Test layer3 Acc 0.9163157894736842, AUC 0.9827579259872437, avg_entr 0.02000577375292778, f1 0.9163157939910889
l3_test_time 1.1197834014892578
Test layer4 Acc 0.9163157894736842, AUC 0.9827452301979065, avg_entr 0.018227996304631233, f1 0.9163157939910889
l4_test_time 1.3906924724578857
