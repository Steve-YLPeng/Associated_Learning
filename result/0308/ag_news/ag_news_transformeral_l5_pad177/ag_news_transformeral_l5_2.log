total count words 102019
vocab size 30000
train size 120000, valid size 3800, test size 3800
found 26754 words in glove
model: TransformerModelML(
  (layers): ModuleList(
    (0): EMBLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Embedding(30000, 300)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=4, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=4, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.weight 9000000
layers.0.ae.g.0.weight 512
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 512
layers.0.ae.h.0.bias 4
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
Total Trainable Params: 13672292
init_time 19.627556562423706
Start Training
gc 0
Train Epoch0 Acc 0.6448166666666667 (77378/120000), AUC 0.8679335117340088
ep0_train_time 167.99567699432373
Test Epoch0 layer0 Acc 0.9026315789473685, AUC 0.9771491885185242, avg_entr 0.25732335448265076, f1 0.9026315808296204
ep0_l0_test_time 0.33106517791748047
Save ckpt to ckpt/ag_news_transformeral_l5_pad177//ag_news_transformeral_l5.pt  ,ep 0
Test Epoch0 layer1 Acc 0.911578947368421, AUC 0.9796895384788513, avg_entr 0.1654479205608368, f1 0.9115789532661438
ep0_l1_test_time 0.6000514030456543
Save ckpt to ckpt/ag_news_transformeral_l5_pad177//ag_news_transformeral_l5.pt  ,ep 0
Test Epoch0 layer2 Acc 0.9092105263157895, AUC 0.9801472425460815, avg_entr 0.15749341249465942, f1 0.9092105031013489
ep0_l2_test_time 0.8601579666137695
Test Epoch0 layer3 Acc 0.9076315789473685, AUC 0.9799321889877319, avg_entr 0.15196993947029114, f1 0.9076315760612488
ep0_l3_test_time 1.1314537525177002
Test Epoch0 layer4 Acc 0.9073684210526316, AUC 0.980190634727478, avg_entr 0.15217174589633942, f1 0.9073684215545654
ep0_l4_test_time 1.3961231708526611
gc 0
Train Epoch1 Acc 0.919825 (110379/120000), AUC 0.9816332459449768
ep1_train_time 166.673570394516
Test Epoch1 layer0 Acc 0.915, AUC 0.9806690812110901, avg_entr 0.1506900042295456, f1 0.9150000214576721
ep1_l0_test_time 0.324052095413208
Save ckpt to ckpt/ag_news_transformeral_l5_pad177//ag_news_transformeral_l5.pt  ,ep 1
Test Epoch1 layer1 Acc 0.9181578947368421, AUC 0.9827805757522583, avg_entr 0.0835665911436081, f1 0.9181578755378723
ep1_l1_test_time 0.5919272899627686
Save ckpt to ckpt/ag_news_transformeral_l5_pad177//ag_news_transformeral_l5.pt  ,ep 1
Test Epoch1 layer2 Acc 0.92, AUC 0.9826242327690125, avg_entr 0.07119026780128479, f1 0.9200000166893005
ep1_l2_test_time 0.8593978881835938
Save ckpt to ckpt/ag_news_transformeral_l5_pad177//ag_news_transformeral_l5.pt  ,ep 1
Test Epoch1 layer3 Acc 0.9210526315789473, AUC 0.9824233651161194, avg_entr 0.06854759156703949, f1 0.9210526347160339
ep1_l3_test_time 1.1306068897247314
Save ckpt to ckpt/ag_news_transformeral_l5_pad177//ag_news_transformeral_l5.pt  ,ep 1
Test Epoch1 layer4 Acc 0.9207894736842105, AUC 0.9827693104743958, avg_entr 0.06407640874385834, f1 0.9207894802093506
ep1_l4_test_time 1.3859672546386719
gc 0
Train Epoch2 Acc 0.9345 (112140/120000), AUC 0.9865344166755676
ep2_train_time 166.66597414016724
Test Epoch2 layer0 Acc 0.9192105263157895, AUC 0.9817367792129517, avg_entr 0.11029475182294846, f1 0.9192105531692505
ep2_l0_test_time 0.3275275230407715
Test Epoch2 layer1 Acc 0.9239473684210526, AUC 0.9836357831954956, avg_entr 0.04431857913732529, f1 0.9239473938941956
ep2_l1_test_time 0.5932269096374512
Save ckpt to ckpt/ag_news_transformeral_l5_pad177//ag_news_transformeral_l5.pt  ,ep 2
Test Epoch2 layer2 Acc 0.9221052631578948, AUC 0.9834418296813965, avg_entr 0.038271475583314896, f1 0.9221052527427673
ep2_l2_test_time 0.8590648174285889
Test Epoch2 layer3 Acc 0.9221052631578948, AUC 0.9828845858573914, avg_entr 0.035544201731681824, f1 0.9221052527427673
ep2_l3_test_time 1.1145188808441162
Test Epoch2 layer4 Acc 0.9218421052631579, AUC 0.9830917119979858, avg_entr 0.03200970217585564, f1 0.921842098236084
ep2_l4_test_time 1.3892838954925537
gc 0
Train Epoch3 Acc 0.9426333333333333 (113116/120000), AUC 0.9887771010398865
ep3_train_time 166.67480540275574
Test Epoch3 layer0 Acc 0.9226315789473685, AUC 0.9823541045188904, avg_entr 0.09142442047595978, f1 0.922631561756134
ep3_l0_test_time 0.32296156883239746
Test Epoch3 layer1 Acc 0.9244736842105263, AUC 0.9830111265182495, avg_entr 0.03204318881034851, f1 0.9244737029075623
ep3_l1_test_time 0.5968353748321533
Save ckpt to ckpt/ag_news_transformeral_l5_pad177//ag_news_transformeral_l5.pt  ,ep 3
Test Epoch3 layer2 Acc 0.9234210526315789, AUC 0.9842694401741028, avg_entr 0.027634883299469948, f1 0.9234210252761841
ep3_l2_test_time 0.8575022220611572
Test Epoch3 layer3 Acc 0.9236842105263158, AUC 0.9843686819076538, avg_entr 0.025977257639169693, f1 0.9236842393875122
ep3_l3_test_time 1.121567964553833
Test Epoch3 layer4 Acc 0.9239473684210526, AUC 0.983330488204956, avg_entr 0.023392386734485626, f1 0.9239473938941956
ep3_l4_test_time 1.3876802921295166
gc 0
Train Epoch4 Acc 0.9481166666666667 (113774/120000), AUC 0.9902584552764893
ep4_train_time 166.73197555541992
Test Epoch4 layer0 Acc 0.9223684210526316, AUC 0.9827503561973572, avg_entr 0.08099807053804398, f1 0.9223684072494507
ep4_l0_test_time 0.33060598373413086
Test Epoch4 layer1 Acc 0.9228947368421052, AUC 0.9818241596221924, avg_entr 0.029450807720422745, f1 0.9228947162628174
ep4_l1_test_time 0.590923547744751
Test Epoch4 layer2 Acc 0.9221052631578948, AUC 0.9820084571838379, avg_entr 0.02589487098157406, f1 0.9221052527427673
ep4_l2_test_time 0.8509786128997803
Test Epoch4 layer3 Acc 0.921578947368421, AUC 0.9826329946517944, avg_entr 0.02438712865114212, f1 0.9215789437294006
ep4_l3_test_time 1.1233773231506348
Test Epoch4 layer4 Acc 0.9210526315789473, AUC 0.9815443158149719, avg_entr 0.022808533161878586, f1 0.9210526347160339
ep4_l4_test_time 1.3941919803619385
gc 0
Train Epoch5 Acc 0.9527416666666667 (114329/120000), AUC 0.9913685321807861
ep5_train_time 166.69249749183655
Test Epoch5 layer0 Acc 0.9221052631578948, AUC 0.9827641248703003, avg_entr 0.0715896338224411, f1 0.9221052527427673
ep5_l0_test_time 0.31752467155456543
Test Epoch5 layer1 Acc 0.9239473684210526, AUC 0.9808437824249268, avg_entr 0.023875564336776733, f1 0.9239473938941956
ep5_l1_test_time 0.5938444137573242
Test Epoch5 layer2 Acc 0.9236842105263158, AUC 0.9824596643447876, avg_entr 0.019248386844992638, f1 0.9236842393875122
ep5_l2_test_time 0.8550159931182861
Test Epoch5 layer3 Acc 0.9234210526315789, AUC 0.9833369255065918, avg_entr 0.017870290204882622, f1 0.9234210252761841
ep5_l3_test_time 1.1338021755218506
Test Epoch5 layer4 Acc 0.9231578947368421, AUC 0.9830617308616638, avg_entr 0.0166959036141634, f1 0.9231578707695007
ep5_l4_test_time 1.387547492980957
gc 0
Train Epoch6 Acc 0.9560583333333333 (114727/120000), AUC 0.9919619560241699
ep6_train_time 166.75520944595337
Test Epoch6 layer0 Acc 0.9221052631578948, AUC 0.9826617240905762, avg_entr 0.0665750801563263, f1 0.9221052527427673
ep6_l0_test_time 0.3293342590332031
Test Epoch6 layer1 Acc 0.9213157894736842, AUC 0.9798761606216431, avg_entr 0.022621607407927513, f1 0.9213157892227173
ep6_l1_test_time 0.5910987854003906
Test Epoch6 layer2 Acc 0.9213157894736842, AUC 0.9812334179878235, avg_entr 0.01784755475819111, f1 0.9213157892227173
ep6_l2_test_time 0.864445686340332
Test Epoch6 layer3 Acc 0.921578947368421, AUC 0.9820581078529358, avg_entr 0.016332942992448807, f1 0.9215789437294006
ep6_l3_test_time 1.11928129196167
Test Epoch6 layer4 Acc 0.9213157894736842, AUC 0.9799822568893433, avg_entr 0.015190837904810905, f1 0.9213157892227173
ep6_l4_test_time 1.3980059623718262
gc 0
Train Epoch7 Acc 0.9588833333333333 (115066/120000), AUC 0.9929941892623901
ep7_train_time 166.66217398643494
Test Epoch7 layer0 Acc 0.9205263157894736, AUC 0.9825664162635803, avg_entr 0.05934703350067139, f1 0.9205263257026672
ep7_l0_test_time 0.32778024673461914
Test Epoch7 layer1 Acc 0.9223684210526316, AUC 0.9788851737976074, avg_entr 0.020434696227312088, f1 0.9223684072494507
ep7_l1_test_time 0.5914480686187744
Test Epoch7 layer2 Acc 0.9218421052631579, AUC 0.9796491861343384, avg_entr 0.016546491533517838, f1 0.921842098236084
ep7_l2_test_time 0.8606374263763428
Test Epoch7 layer3 Acc 0.9223684210526316, AUC 0.9813562631607056, avg_entr 0.015351274982094765, f1 0.9223684072494507
ep7_l3_test_time 1.1268155574798584
Test Epoch7 layer4 Acc 0.9221052631578948, AUC 0.9789803624153137, avg_entr 0.014175317250192165, f1 0.9221052527427673
ep7_l4_test_time 1.3919544219970703
gc 0
Train Epoch8 Acc 0.9604833333333334 (115258/120000), AUC 0.9937279224395752
ep8_train_time 166.7470555305481
Test Epoch8 layer0 Acc 0.9228947368421052, AUC 0.9823082685470581, avg_entr 0.055561941117048264, f1 0.9228947162628174
ep8_l0_test_time 0.32460665702819824
Test Epoch8 layer1 Acc 0.9231578947368421, AUC 0.9785250425338745, avg_entr 0.019041085615754128, f1 0.9231578707695007
ep8_l1_test_time 0.5987644195556641
Test Epoch8 layer2 Acc 0.9218421052631579, AUC 0.9794166088104248, avg_entr 0.013921053148806095, f1 0.921842098236084
ep8_l2_test_time 0.864814043045044
Test Epoch8 layer3 Acc 0.9213157894736842, AUC 0.9816999435424805, avg_entr 0.012870019301772118, f1 0.9213157892227173
ep8_l3_test_time 1.126025915145874
Test Epoch8 layer4 Acc 0.9210526315789473, AUC 0.9792460203170776, avg_entr 0.012068234384059906, f1 0.9210526347160339
ep8_l4_test_time 1.3913817405700684
gc 0
Train Epoch9 Acc 0.963225 (115587/120000), AUC 0.9944351315498352
ep9_train_time 166.76170873641968
Test Epoch9 layer0 Acc 0.9226315789473685, AUC 0.9822899699211121, avg_entr 0.05411204323172569, f1 0.922631561756134
ep9_l0_test_time 0.32932138442993164
Test Epoch9 layer1 Acc 0.9207894736842105, AUC 0.9771265983581543, avg_entr 0.017819924280047417, f1 0.9207894802093506
ep9_l1_test_time 0.5981137752532959
Test Epoch9 layer2 Acc 0.9189473684210526, AUC 0.9781701564788818, avg_entr 0.013013826683163643, f1 0.9189472794532776
ep9_l2_test_time 0.8547201156616211
Test Epoch9 layer3 Acc 0.9192105263157895, AUC 0.9796106815338135, avg_entr 0.011264107190072536, f1 0.9192105531692505
ep9_l3_test_time 1.1204121112823486
Test Epoch9 layer4 Acc 0.9189473684210526, AUC 0.9789038896560669, avg_entr 0.010398906655609608, f1 0.9189472794532776
ep9_l4_test_time 1.3890256881713867
gc 0
Train Epoch10 Acc 0.9647 (115764/120000), AUC 0.9945923686027527
ep10_train_time 166.75086212158203
Test Epoch10 layer0 Acc 0.9226315789473685, AUC 0.982248842716217, avg_entr 0.052305810153484344, f1 0.922631561756134
ep10_l0_test_time 0.3179442882537842
Test Epoch10 layer1 Acc 0.921578947368421, AUC 0.9770728945732117, avg_entr 0.017591988667845726, f1 0.9215789437294006
ep10_l1_test_time 0.5945661067962646
Test Epoch10 layer2 Acc 0.9213157894736842, AUC 0.9783675670623779, avg_entr 0.013473302125930786, f1 0.9213157892227173
ep10_l2_test_time 0.8660492897033691
Test Epoch10 layer3 Acc 0.9210526315789473, AUC 0.9806084632873535, avg_entr 0.012061867862939835, f1 0.9210526347160339
ep10_l3_test_time 1.126366138458252
Test Epoch10 layer4 Acc 0.9207894736842105, AUC 0.9785904884338379, avg_entr 0.011318829841911793, f1 0.9207894802093506
ep10_l4_test_time 1.3943369388580322
gc 0
Train Epoch11 Acc 0.9658833333333333 (115906/120000), AUC 0.9949135780334473
ep11_train_time 166.62901067733765
Test Epoch11 layer0 Acc 0.9221052631578948, AUC 0.9821351170539856, avg_entr 0.05000093951821327, f1 0.9221052527427673
ep11_l0_test_time 0.330949068069458
Test Epoch11 layer1 Acc 0.92, AUC 0.977584719657898, avg_entr 0.017073610797524452, f1 0.9200000166893005
ep11_l1_test_time 0.5917894840240479
Test Epoch11 layer2 Acc 0.9192105263157895, AUC 0.9797207117080688, avg_entr 0.013536970131099224, f1 0.9192105531692505
ep11_l2_test_time 0.8573389053344727
Test Epoch11 layer3 Acc 0.9192105263157895, AUC 0.9807926416397095, avg_entr 0.012063857167959213, f1 0.9192105531692505
ep11_l3_test_time 1.1259794235229492
Test Epoch11 layer4 Acc 0.9189473684210526, AUC 0.9802297353744507, avg_entr 0.01131095178425312, f1 0.9189472794532776
ep11_l4_test_time 1.3810920715332031
gc 0
Train Epoch12 Acc 0.9667416666666667 (116009/120000), AUC 0.995335042476654
ep12_train_time 166.75660753250122
Test Epoch12 layer0 Acc 0.9221052631578948, AUC 0.9820575714111328, avg_entr 0.048065561801195145, f1 0.9221052527427673
ep12_l0_test_time 0.32903242111206055
Test Epoch12 layer1 Acc 0.9207894736842105, AUC 0.9762680530548096, avg_entr 0.01639762707054615, f1 0.9207894802093506
ep12_l1_test_time 0.5886282920837402
Test Epoch12 layer2 Acc 0.92, AUC 0.9767664074897766, avg_entr 0.012002837844192982, f1 0.9200000166893005
ep12_l2_test_time 0.8520505428314209
Test Epoch12 layer3 Acc 0.92, AUC 0.9790791273117065, avg_entr 0.010470768436789513, f1 0.9200000166893005
ep12_l3_test_time 1.1251881122589111
Test Epoch12 layer4 Acc 0.9197368421052632, AUC 0.9761208891868591, avg_entr 0.009643088094890118, f1 0.9197368621826172
ep12_l4_test_time 1.397033452987671
gc 0
Train Epoch13 Acc 0.968375 (116205/120000), AUC 0.995618462562561
ep13_train_time 166.68833899497986
Test Epoch13 layer0 Acc 0.9221052631578948, AUC 0.9820738434791565, avg_entr 0.04659367352724075, f1 0.9221052527427673
ep13_l0_test_time 0.32733702659606934
Test Epoch13 layer1 Acc 0.92, AUC 0.9756146669387817, avg_entr 0.01610320620238781, f1 0.9200000166893005
ep13_l1_test_time 0.5946304798126221
Test Epoch13 layer2 Acc 0.9194736842105263, AUC 0.9756020307540894, avg_entr 0.011628134176135063, f1 0.9194737076759338
ep13_l2_test_time 0.8633801937103271
Test Epoch13 layer3 Acc 0.9197368421052632, AUC 0.9775299429893494, avg_entr 0.009982148185372353, f1 0.9197368621826172
ep13_l3_test_time 1.1220505237579346
Test Epoch13 layer4 Acc 0.9189473684210526, AUC 0.9724423885345459, avg_entr 0.009255236946046352, f1 0.9189472794532776
ep13_l4_test_time 1.393953800201416
gc 0
Train Epoch14 Acc 0.9687833333333333 (116254/120000), AUC 0.9954688549041748
ep14_train_time 166.82418656349182
Test Epoch14 layer0 Acc 0.9194736842105263, AUC 0.9820618629455566, avg_entr 0.04644806683063507, f1 0.9194737076759338
ep14_l0_test_time 0.3247103691101074
Test Epoch14 layer1 Acc 0.9181578947368421, AUC 0.9760067462921143, avg_entr 0.015937965363264084, f1 0.9181578755378723
ep14_l1_test_time 0.5975501537322998
Test Epoch14 layer2 Acc 0.9171052631578948, AUC 0.9765515327453613, avg_entr 0.012331335805356503, f1 0.9171052575111389
ep14_l2_test_time 0.8585634231567383
Test Epoch14 layer3 Acc 0.9168421052631579, AUC 0.97613525390625, avg_entr 0.010650207288563251, f1 0.9168421030044556
ep14_l3_test_time 1.1219253540039062
Test Epoch14 layer4 Acc 0.9178947368421052, AUC 0.9745579361915588, avg_entr 0.009919282980263233, f1 0.917894721031189
ep14_l4_test_time 1.3908271789550781
gc 0
Train Epoch15 Acc 0.9696083333333333 (116353/120000), AUC 0.9957227110862732
ep15_train_time 166.60546827316284
Test Epoch15 layer0 Acc 0.9197368421052632, AUC 0.9820419549942017, avg_entr 0.04460183158516884, f1 0.9197368621826172
ep15_l0_test_time 0.3212704658508301
Test Epoch15 layer1 Acc 0.9186842105263158, AUC 0.9749565720558167, avg_entr 0.01549368817359209, f1 0.918684184551239
ep15_l1_test_time 0.5908467769622803
Test Epoch15 layer2 Acc 0.9186842105263158, AUC 0.9729885458946228, avg_entr 0.011403672397136688, f1 0.918684184551239
ep15_l2_test_time 0.8601922988891602
Test Epoch15 layer3 Acc 0.9192105263157895, AUC 0.9728143215179443, avg_entr 0.009836058132350445, f1 0.9192105531692505
ep15_l3_test_time 1.1138417720794678
Test Epoch15 layer4 Acc 0.9192105263157895, AUC 0.9682828187942505, avg_entr 0.00897156447172165, f1 0.9192105531692505
ep15_l4_test_time 1.3890299797058105
gc 0
Train Epoch16 Acc 0.9699833333333333 (116398/120000), AUC 0.9958417415618896
ep16_train_time 166.72337818145752
Test Epoch16 layer0 Acc 0.9202631578947369, AUC 0.981967031955719, avg_entr 0.04235628619790077, f1 0.9202631711959839
ep16_l0_test_time 0.3274388313293457
Test Epoch16 layer1 Acc 0.9192105263157895, AUC 0.9748470187187195, avg_entr 0.015227735973894596, f1 0.9192105531692505
ep16_l1_test_time 0.602104663848877
Test Epoch16 layer2 Acc 0.9184210526315789, AUC 0.9751521348953247, avg_entr 0.010929136537015438, f1 0.9184210896492004
ep16_l2_test_time 0.8604640960693359
Test Epoch16 layer3 Acc 0.9189473684210526, AUC 0.9756759405136108, avg_entr 0.009448300115764141, f1 0.9189472794532776
ep16_l3_test_time 1.123521327972412
Test Epoch16 layer4 Acc 0.9181578947368421, AUC 0.9727457761764526, avg_entr 0.008758194744586945, f1 0.9181578755378723
ep16_l4_test_time 1.3992388248443604
gc 0
Train Epoch17 Acc 0.9707083333333333 (116485/120000), AUC 0.9959311485290527
ep17_train_time 166.76716661453247
Test Epoch17 layer0 Acc 0.9213157894736842, AUC 0.9819504618644714, avg_entr 0.04180622845888138, f1 0.9213157892227173
ep17_l0_test_time 0.32573366165161133
Test Epoch17 layer1 Acc 0.9189473684210526, AUC 0.9748618602752686, avg_entr 0.015191374346613884, f1 0.9189472794532776
ep17_l1_test_time 0.5960075855255127
Test Epoch17 layer2 Acc 0.9184210526315789, AUC 0.9746801853179932, avg_entr 0.01112418994307518, f1 0.9184210896492004
ep17_l2_test_time 0.8614866733551025
Test Epoch17 layer3 Acc 0.9181578947368421, AUC 0.9756208658218384, avg_entr 0.009252720512449741, f1 0.9181578755378723
ep17_l3_test_time 1.1212701797485352
Test Epoch17 layer4 Acc 0.9184210526315789, AUC 0.9718335270881653, avg_entr 0.00861944817006588, f1 0.9184210896492004
ep17_l4_test_time 1.3925833702087402
gc 0
Train Epoch18 Acc 0.97105 (116526/120000), AUC 0.9959626197814941
ep18_train_time 166.72073435783386
Test Epoch18 layer0 Acc 0.9226315789473685, AUC 0.9819233417510986, avg_entr 0.04108263552188873, f1 0.922631561756134
ep18_l0_test_time 0.3264024257659912
Test Epoch18 layer1 Acc 0.9192105263157895, AUC 0.9740642309188843, avg_entr 0.014759656973183155, f1 0.9192105531692505
ep18_l1_test_time 0.5938777923583984
Test Epoch18 layer2 Acc 0.9181578947368421, AUC 0.973699152469635, avg_entr 0.010554829612374306, f1 0.9181578755378723
ep18_l2_test_time 0.8532037734985352
Test Epoch18 layer3 Acc 0.9189473684210526, AUC 0.9742259383201599, avg_entr 0.008678431622684002, f1 0.9189472794532776
ep18_l3_test_time 1.122797966003418
Test Epoch18 layer4 Acc 0.9192105263157895, AUC 0.9692171812057495, avg_entr 0.008003040216863155, f1 0.9192105531692505
ep18_l4_test_time 1.391571044921875
gc 0
Train Epoch19 Acc 0.9713666666666667 (116564/120000), AUC 0.9960799813270569
ep19_train_time 166.76481556892395
Test Epoch19 layer0 Acc 0.9213157894736842, AUC 0.9819219708442688, avg_entr 0.040083374828100204, f1 0.9213157892227173
ep19_l0_test_time 0.32625246047973633
Test Epoch19 layer1 Acc 0.9207894736842105, AUC 0.9738305807113647, avg_entr 0.014222770929336548, f1 0.9207894802093506
ep19_l1_test_time 0.5974359512329102
Test Epoch19 layer2 Acc 0.9178947368421052, AUC 0.9724898338317871, avg_entr 0.00947218481451273, f1 0.917894721031189
ep19_l2_test_time 0.8472771644592285
Test Epoch19 layer3 Acc 0.9184210526315789, AUC 0.9732764959335327, avg_entr 0.00783447828143835, f1 0.9184210896492004
ep19_l3_test_time 1.1262915134429932
Test Epoch19 layer4 Acc 0.9181578947368421, AUC 0.9683165550231934, avg_entr 0.007288128137588501, f1 0.9181578755378723
ep19_l4_test_time 1.3940021991729736
gc 0
Train Epoch20 Acc 0.971475 (116577/120000), AUC 0.9960482120513916
ep20_train_time 166.7755069732666
Test Epoch20 layer0 Acc 0.9207894736842105, AUC 0.9819458723068237, avg_entr 0.03884413093328476, f1 0.9207894802093506
ep20_l0_test_time 0.32938504219055176
Test Epoch20 layer1 Acc 0.9194736842105263, AUC 0.974235475063324, avg_entr 0.014056551270186901, f1 0.9194737076759338
ep20_l1_test_time 0.593010425567627
Test Epoch20 layer2 Acc 0.9189473684210526, AUC 0.9740803837776184, avg_entr 0.009667638689279556, f1 0.9189472794532776
ep20_l2_test_time 0.8687424659729004
Test Epoch20 layer3 Acc 0.9189473684210526, AUC 0.9739105105400085, avg_entr 0.008067776449024677, f1 0.9189472794532776
ep20_l3_test_time 1.1255319118499756
Test Epoch20 layer4 Acc 0.9189473684210526, AUC 0.9682148694992065, avg_entr 0.00736987916752696, f1 0.9189472794532776
ep20_l4_test_time 1.3955211639404297
gc 0
Train Epoch21 Acc 0.9720333333333333 (116644/120000), AUC 0.9961758255958557
ep21_train_time 166.67863988876343
Test Epoch21 layer0 Acc 0.9218421052631579, AUC 0.9819183349609375, avg_entr 0.03787442669272423, f1 0.921842098236084
ep21_l0_test_time 0.31426572799682617
Test Epoch21 layer1 Acc 0.92, AUC 0.9739160537719727, avg_entr 0.013697706162929535, f1 0.9200000166893005
ep21_l1_test_time 0.5908432006835938
Test Epoch21 layer2 Acc 0.9178947368421052, AUC 0.9737346172332764, avg_entr 0.009066596627235413, f1 0.917894721031189
ep21_l2_test_time 0.866023063659668
Test Epoch21 layer3 Acc 0.9181578947368421, AUC 0.9739769697189331, avg_entr 0.007649422623217106, f1 0.9181578755378723
ep21_l3_test_time 1.1318542957305908
Test Epoch21 layer4 Acc 0.9176315789473685, AUC 0.9686815142631531, avg_entr 0.007256162818521261, f1 0.9176315665245056
ep21_l4_test_time 1.3903584480285645
gc 0
Train Epoch22 Acc 0.9720333333333333 (116644/120000), AUC 0.9961609840393066
ep22_train_time 166.7615146636963
Test Epoch22 layer0 Acc 0.9205263157894736, AUC 0.9819178581237793, avg_entr 0.03768528997898102, f1 0.9205263257026672
ep22_l0_test_time 0.3170816898345947
Test Epoch22 layer1 Acc 0.9205263157894736, AUC 0.9734368324279785, avg_entr 0.013521868735551834, f1 0.9205263257026672
ep22_l1_test_time 0.5953102111816406
Test Epoch22 layer2 Acc 0.9186842105263158, AUC 0.9725217819213867, avg_entr 0.00933466013520956, f1 0.918684184551239
ep22_l2_test_time 0.8644943237304688
Test Epoch22 layer3 Acc 0.9192105263157895, AUC 0.9722128510475159, avg_entr 0.007759804837405682, f1 0.9192105531692505
ep22_l3_test_time 1.119377613067627
Test Epoch22 layer4 Acc 0.9186842105263158, AUC 0.9675713181495667, avg_entr 0.007120518945157528, f1 0.918684184551239
ep22_l4_test_time 1.3856382369995117
gc 0
Train Epoch23 Acc 0.9721916666666667 (116663/120000), AUC 0.9961184859275818
ep23_train_time 166.7166817188263
Test Epoch23 layer0 Acc 0.9210526315789473, AUC 0.9818525314331055, avg_entr 0.03693696856498718, f1 0.9210526347160339
ep23_l0_test_time 0.3292057514190674
Test Epoch23 layer1 Acc 0.9202631578947369, AUC 0.9736262559890747, avg_entr 0.013404015451669693, f1 0.9202631711959839
ep23_l1_test_time 0.5957939624786377
Test Epoch23 layer2 Acc 0.9186842105263158, AUC 0.9726989269256592, avg_entr 0.009230501018464565, f1 0.918684184551239
ep23_l2_test_time 0.858304500579834
Test Epoch23 layer3 Acc 0.9186842105263158, AUC 0.9717132449150085, avg_entr 0.0076866853050887585, f1 0.918684184551239
ep23_l3_test_time 1.122058629989624
Test Epoch23 layer4 Acc 0.9176315789473685, AUC 0.9670454859733582, avg_entr 0.007008952088654041, f1 0.9176315665245056
ep23_l4_test_time 1.3798766136169434
gc 0
Train Epoch24 Acc 0.9722083333333333 (116665/120000), AUC 0.9961327910423279
ep24_train_time 166.65641450881958
Test Epoch24 layer0 Acc 0.9228947368421052, AUC 0.9818867444992065, avg_entr 0.03682641312479973, f1 0.9228947162628174
ep24_l0_test_time 0.32994818687438965
Test Epoch24 layer1 Acc 0.9202631578947369, AUC 0.9733650088310242, avg_entr 0.013199458830058575, f1 0.9202631711959839
ep24_l1_test_time 0.5871429443359375
Test Epoch24 layer2 Acc 0.9192105263157895, AUC 0.9726030826568604, avg_entr 0.009347781538963318, f1 0.9192105531692505
ep24_l2_test_time 0.8584907054901123
Test Epoch24 layer3 Acc 0.9189473684210526, AUC 0.9720844030380249, avg_entr 0.008065866306424141, f1 0.9189472794532776
ep24_l3_test_time 1.1267030239105225
Test Epoch24 layer4 Acc 0.9189473684210526, AUC 0.9677466750144958, avg_entr 0.007443900685757399, f1 0.9189472794532776
ep24_l4_test_time 1.386793851852417
gc 0
Train Epoch25 Acc 0.97245 (116694/120000), AUC 0.9962912797927856
ep25_train_time 166.6810839176178
Test Epoch25 layer0 Acc 0.9223684210526316, AUC 0.9818810224533081, avg_entr 0.0364968404173851, f1 0.9223684072494507
ep25_l0_test_time 0.3140876293182373
Test Epoch25 layer1 Acc 0.9202631578947369, AUC 0.9729377031326294, avg_entr 0.012989800423383713, f1 0.9202631711959839
ep25_l1_test_time 0.5892333984375
Test Epoch25 layer2 Acc 0.9186842105263158, AUC 0.9722203016281128, avg_entr 0.0086904875934124, f1 0.918684184551239
ep25_l2_test_time 0.856187105178833
Test Epoch25 layer3 Acc 0.9186842105263158, AUC 0.9724633693695068, avg_entr 0.007357171270996332, f1 0.918684184551239
ep25_l3_test_time 1.1223201751708984
Test Epoch25 layer4 Acc 0.9184210526315789, AUC 0.9676225185394287, avg_entr 0.0067388447932899, f1 0.9184210896492004
ep25_l4_test_time 1.3924989700317383
gc 0
Train Epoch26 Acc 0.97245 (116694/120000), AUC 0.9961791038513184
ep26_train_time 166.7750701904297
Test Epoch26 layer0 Acc 0.921578947368421, AUC 0.981862485408783, avg_entr 0.03617528825998306, f1 0.9215789437294006
ep26_l0_test_time 0.320446252822876
Test Epoch26 layer1 Acc 0.9197368421052632, AUC 0.973223090171814, avg_entr 0.01291214395314455, f1 0.9197368621826172
ep26_l1_test_time 0.5938832759857178
Test Epoch26 layer2 Acc 0.9197368421052632, AUC 0.9724293947219849, avg_entr 0.009100593626499176, f1 0.9197368621826172
ep26_l2_test_time 0.8640708923339844
Test Epoch26 layer3 Acc 0.9192105263157895, AUC 0.9717185497283936, avg_entr 0.007851864211261272, f1 0.9192105531692505
ep26_l3_test_time 1.1180636882781982
Test Epoch26 layer4 Acc 0.9189473684210526, AUC 0.9678685665130615, avg_entr 0.007248459849506617, f1 0.9189472794532776
ep26_l4_test_time 1.3923165798187256
gc 0
Train Epoch27 Acc 0.9725333333333334 (116704/120000), AUC 0.996174156665802
ep27_train_time 166.82921195030212
Test Epoch27 layer0 Acc 0.921578947368421, AUC 0.9818553924560547, avg_entr 0.03565709665417671, f1 0.9215789437294006
ep27_l0_test_time 0.322690486907959
Test Epoch27 layer1 Acc 0.9197368421052632, AUC 0.9733977913856506, avg_entr 0.012841065414249897, f1 0.9197368621826172
ep27_l1_test_time 0.5987422466278076
Test Epoch27 layer2 Acc 0.9186842105263158, AUC 0.9727431535720825, avg_entr 0.009131401777267456, f1 0.918684184551239
ep27_l2_test_time 0.856590986251831
Test Epoch27 layer3 Acc 0.9184210526315789, AUC 0.9721962213516235, avg_entr 0.007829423993825912, f1 0.9184210896492004
ep27_l3_test_time 1.1233634948730469
Test Epoch27 layer4 Acc 0.9181578947368421, AUC 0.9676889181137085, avg_entr 0.007240280508995056, f1 0.9181578755378723
ep27_l4_test_time 1.383793830871582
gc 0
Train Epoch28 Acc 0.9727333333333333 (116728/120000), AUC 0.9962880611419678
ep28_train_time 166.81532073020935
Test Epoch28 layer0 Acc 0.92, AUC 0.9818471670150757, avg_entr 0.03613217547535896, f1 0.9200000166893005
ep28_l0_test_time 0.3256087303161621
Test Epoch28 layer1 Acc 0.9194736842105263, AUC 0.9734377861022949, avg_entr 0.012802169658243656, f1 0.9194737076759338
ep28_l1_test_time 0.5942027568817139
Test Epoch28 layer2 Acc 0.9178947368421052, AUC 0.9727234840393066, avg_entr 0.00907361414283514, f1 0.917894721031189
ep28_l2_test_time 0.856447696685791
Test Epoch28 layer3 Acc 0.9178947368421052, AUC 0.9724358916282654, avg_entr 0.007625615689903498, f1 0.917894721031189
ep28_l3_test_time 1.1295158863067627
Test Epoch28 layer4 Acc 0.9181578947368421, AUC 0.9677062034606934, avg_entr 0.007033186964690685, f1 0.9181578755378723
ep28_l4_test_time 1.3842113018035889
gc 0
Train Epoch29 Acc 0.9726833333333333 (116722/120000), AUC 0.9963554739952087
ep29_train_time 166.55790758132935
Test Epoch29 layer0 Acc 0.921578947368421, AUC 0.9818478226661682, avg_entr 0.0353945754468441, f1 0.9215789437294006
ep29_l0_test_time 0.3306429386138916
Test Epoch29 layer1 Acc 0.9192105263157895, AUC 0.9731888771057129, avg_entr 0.012808121740818024, f1 0.9192105531692505
ep29_l1_test_time 0.5919616222381592
Test Epoch29 layer2 Acc 0.9173684210526316, AUC 0.9725976586341858, avg_entr 0.009328192099928856, f1 0.9173683524131775
ep29_l2_test_time 0.8559589385986328
Test Epoch29 layer3 Acc 0.9181578947368421, AUC 0.9714066982269287, avg_entr 0.008018271997570992, f1 0.9181578755378723
ep29_l3_test_time 1.12601637840271
Test Epoch29 layer4 Acc 0.9181578947368421, AUC 0.9678096175193787, avg_entr 0.0073758200742304325, f1 0.9181578755378723
ep29_l4_test_time 1.3966894149780273
Best AUC tensor(0.9245) 3 1
train_loss (2, 5, 30)
valid_acc (5, 30)
valid_AUC (5, 30)
train_acc (30,)
total_train+valid_time 5133.932114362717
Start Testing
Load ckpt at ckpt/ag_news_transformeral_l5_pad177//ag_news_transformeral_l5.pt
Test layer0 Acc 0.9136842105263158, AUC 0.9802935123443604, avg_entr 0.09801151603460312, f1 0.9136841893196106
l0_test_time 0.3263583183288574
Test layer1 Acc 0.9139473684210526, AUC 0.9806079864501953, avg_entr 0.04001472890377045, f1 0.913947343826294
l1_test_time 0.5914926528930664
Test layer2 Acc 0.9144736842105263, AUC 0.9822449684143066, avg_entr 0.03467477858066559, f1 0.9144737124443054
l2_test_time 0.8599131107330322
Test layer3 Acc 0.915, AUC 0.9829621315002441, avg_entr 0.032291021198034286, f1 0.9150000214576721
l3_test_time 1.1224358081817627
Test layer4 Acc 0.9152631578947369, AUC 0.9815942049026489, avg_entr 0.029620204120874405, f1 0.9152631759643555
l4_test_time 1.397709608078003
