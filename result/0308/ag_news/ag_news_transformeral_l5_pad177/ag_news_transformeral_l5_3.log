total count words 102019
vocab size 30000
train size 120000, valid size 3800, test size 3800
found 26754 words in glove
model: TransformerModelML(
  (layers): ModuleList(
    (0): EMBLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Embedding(30000, 300)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=4, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=4, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.weight 9000000
layers.0.ae.g.0.weight 512
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 512
layers.0.ae.h.0.bias 4
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
Total Trainable Params: 13672292
init_time 19.25885009765625
Start Training
gc 0
Train Epoch0 Acc 0.637475 (76497/120000), AUC 0.8643804788589478
ep0_train_time 168.49053406715393
Test Epoch0 layer0 Acc 0.9052631578947369, AUC 0.9773173332214355, avg_entr 0.2560620605945587, f1 0.9052631855010986
ep0_l0_test_time 0.308349609375
Save ckpt to ckpt/ag_news_transformeral_l5_pad177//ag_news_transformeral_l5.pt  ,ep 0
Test Epoch0 layer1 Acc 0.9076315789473685, AUC 0.9797412157058716, avg_entr 0.16016973555088043, f1 0.9076315760612488
ep0_l1_test_time 0.5936355590820312
Save ckpt to ckpt/ag_news_transformeral_l5_pad177//ag_news_transformeral_l5.pt  ,ep 0
Test Epoch0 layer2 Acc 0.9084210526315789, AUC 0.9798399209976196, avg_entr 0.15036553144454956, f1 0.9084210395812988
ep0_l2_test_time 0.8605732917785645
Save ckpt to ckpt/ag_news_transformeral_l5_pad177//ag_news_transformeral_l5.pt  ,ep 0
Test Epoch0 layer3 Acc 0.9073684210526316, AUC 0.9798258543014526, avg_entr 0.14759545028209686, f1 0.9073684215545654
ep0_l3_test_time 1.1263413429260254
Test Epoch0 layer4 Acc 0.9076315789473685, AUC 0.9799424409866333, avg_entr 0.14265668392181396, f1 0.9076315760612488
ep0_l4_test_time 1.3944032192230225
gc 0
Train Epoch1 Acc 0.9188833333333334 (110266/120000), AUC 0.9814324378967285
ep1_train_time 167.31594610214233
Test Epoch1 layer0 Acc 0.9178947368421052, AUC 0.9807052612304688, avg_entr 0.14730940759181976, f1 0.917894721031189
ep1_l0_test_time 0.3203396797180176
Save ckpt to ckpt/ag_news_transformeral_l5_pad177//ag_news_transformeral_l5.pt  ,ep 1
Test Epoch1 layer1 Acc 0.9197368421052632, AUC 0.9828381538391113, avg_entr 0.08278428763151169, f1 0.9197368621826172
ep1_l1_test_time 0.5977504253387451
Save ckpt to ckpt/ag_news_transformeral_l5_pad177//ag_news_transformeral_l5.pt  ,ep 1
Test Epoch1 layer2 Acc 0.921578947368421, AUC 0.9830260276794434, avg_entr 0.07114273309707642, f1 0.9215789437294006
ep1_l2_test_time 0.8632657527923584
Save ckpt to ckpt/ag_news_transformeral_l5_pad177//ag_news_transformeral_l5.pt  ,ep 1
Test Epoch1 layer3 Acc 0.9210526315789473, AUC 0.9830834865570068, avg_entr 0.0659814402461052, f1 0.9210526347160339
ep1_l3_test_time 1.1249959468841553
Test Epoch1 layer4 Acc 0.9186842105263158, AUC 0.9832121729850769, avg_entr 0.062305811792612076, f1 0.918684184551239
ep1_l4_test_time 1.3965747356414795
gc 0
Train Epoch2 Acc 0.9340666666666667 (112088/120000), AUC 0.9866878390312195
ep2_train_time 167.17708659172058
Test Epoch2 layer0 Acc 0.9197368421052632, AUC 0.9819933176040649, avg_entr 0.10967162251472473, f1 0.9197368621826172
ep2_l0_test_time 0.32503318786621094
Test Epoch2 layer1 Acc 0.9221052631578948, AUC 0.9822947382926941, avg_entr 0.04538644105195999, f1 0.9221052527427673
ep2_l1_test_time 0.595318078994751
Save ckpt to ckpt/ag_news_transformeral_l5_pad177//ag_news_transformeral_l5.pt  ,ep 2
Test Epoch2 layer2 Acc 0.921578947368421, AUC 0.9826404452323914, avg_entr 0.038998816162347794, f1 0.9215789437294006
ep2_l2_test_time 0.8630468845367432
Test Epoch2 layer3 Acc 0.9226315789473685, AUC 0.982524037361145, avg_entr 0.03425641730427742, f1 0.922631561756134
ep2_l3_test_time 1.1245484352111816
Save ckpt to ckpt/ag_news_transformeral_l5_pad177//ag_news_transformeral_l5.pt  ,ep 2
Test Epoch2 layer4 Acc 0.921578947368421, AUC 0.9822848439216614, avg_entr 0.03305422142148018, f1 0.9215789437294006
ep2_l4_test_time 1.3933286666870117
gc 0
Train Epoch3 Acc 0.9422916666666666 (113075/120000), AUC 0.9889718294143677
ep3_train_time 167.19756817817688
Test Epoch3 layer0 Acc 0.92, AUC 0.9824482202529907, avg_entr 0.09135005623102188, f1 0.9200000166893005
ep3_l0_test_time 0.33405208587646484
Test Epoch3 layer1 Acc 0.9236842105263158, AUC 0.9810881614685059, avg_entr 0.032979898154735565, f1 0.9236842393875122
ep3_l1_test_time 0.5998094081878662
Save ckpt to ckpt/ag_news_transformeral_l5_pad177//ag_news_transformeral_l5.pt  ,ep 3
Test Epoch3 layer2 Acc 0.9252631578947368, AUC 0.9814684391021729, avg_entr 0.02843843214213848, f1 0.9252631664276123
ep3_l2_test_time 0.8641808032989502
Save ckpt to ckpt/ag_news_transformeral_l5_pad177//ag_news_transformeral_l5.pt  ,ep 3
Test Epoch3 layer3 Acc 0.9252631578947368, AUC 0.9824729561805725, avg_entr 0.026237132027745247, f1 0.9252631664276123
ep3_l3_test_time 1.1223249435424805
Save ckpt to ckpt/ag_news_transformeral_l5_pad177//ag_news_transformeral_l5.pt  ,ep 3
Test Epoch3 layer4 Acc 0.9247368421052632, AUC 0.9826936721801758, avg_entr 0.02504006028175354, f1 0.9247368574142456
ep3_l4_test_time 1.3882403373718262
gc 0
Train Epoch4 Acc 0.94735 (113682/120000), AUC 0.9905476570129395
ep4_train_time 167.16313004493713
Test Epoch4 layer0 Acc 0.9226315789473685, AUC 0.9825747013092041, avg_entr 0.07909493148326874, f1 0.922631561756134
ep4_l0_test_time 0.3325986862182617
Test Epoch4 layer1 Acc 0.9223684210526316, AUC 0.9814421534538269, avg_entr 0.027480900287628174, f1 0.9223684072494507
ep4_l1_test_time 0.5980122089385986
Test Epoch4 layer2 Acc 0.9239473684210526, AUC 0.9819385409355164, avg_entr 0.02307332493364811, f1 0.9239473938941956
ep4_l2_test_time 0.8653032779693604
Test Epoch4 layer3 Acc 0.9242105263157895, AUC 0.9814136624336243, avg_entr 0.021312730386853218, f1 0.9242105484008789
ep4_l3_test_time 1.1279642581939697
Test Epoch4 layer4 Acc 0.9234210526315789, AUC 0.9813861846923828, avg_entr 0.01990816555917263, f1 0.9234210252761841
ep4_l4_test_time 1.3971786499023438
gc 0
Train Epoch5 Acc 0.9517416666666667 (114209/120000), AUC 0.991370439529419
ep5_train_time 167.20862436294556
Test Epoch5 layer0 Acc 0.9207894736842105, AUC 0.9827805161476135, avg_entr 0.07172022014856339, f1 0.9207894802093506
ep5_l0_test_time 0.3279712200164795
Test Epoch5 layer1 Acc 0.9210526315789473, AUC 0.9809942841529846, avg_entr 0.025070006027817726, f1 0.9210526347160339
ep5_l1_test_time 0.6041946411132812
Test Epoch5 layer2 Acc 0.9223684210526316, AUC 0.9816926717758179, avg_entr 0.020069412887096405, f1 0.9223684072494507
ep5_l2_test_time 0.8616721630096436
Test Epoch5 layer3 Acc 0.9223684210526316, AUC 0.9822906255722046, avg_entr 0.017929522320628166, f1 0.9223684072494507
ep5_l3_test_time 1.126844882965088
Test Epoch5 layer4 Acc 0.9218421052631579, AUC 0.9823384284973145, avg_entr 0.016076819971203804, f1 0.921842098236084
ep5_l4_test_time 1.399904727935791
gc 0
Train Epoch6 Acc 0.9547333333333333 (114568/120000), AUC 0.992792010307312
ep6_train_time 167.15684914588928
Test Epoch6 layer0 Acc 0.9213157894736842, AUC 0.9826629757881165, avg_entr 0.06475675851106644, f1 0.9213157892227173
ep6_l0_test_time 0.3287680149078369
Test Epoch6 layer1 Acc 0.9228947368421052, AUC 0.9788322448730469, avg_entr 0.022478675469756126, f1 0.9228947162628174
ep6_l1_test_time 0.5971760749816895
Test Epoch6 layer2 Acc 0.9223684210526316, AUC 0.9805843234062195, avg_entr 0.018284054473042488, f1 0.9223684072494507
ep6_l2_test_time 0.8621463775634766
Test Epoch6 layer3 Acc 0.921578947368421, AUC 0.9809200167655945, avg_entr 0.01700008474290371, f1 0.9215789437294006
ep6_l3_test_time 1.131352186203003
Test Epoch6 layer4 Acc 0.9218421052631579, AUC 0.9810394644737244, avg_entr 0.015793822705745697, f1 0.921842098236084
ep6_l4_test_time 1.3840742111206055
gc 0
Train Epoch7 Acc 0.9572916666666667 (114875/120000), AUC 0.9931501150131226
ep7_train_time 167.2534990310669
Test Epoch7 layer0 Acc 0.921578947368421, AUC 0.9825844168663025, avg_entr 0.06005550175905228, f1 0.9215789437294006
ep7_l0_test_time 0.3230280876159668
Test Epoch7 layer1 Acc 0.9189473684210526, AUC 0.9783157110214233, avg_entr 0.021978124976158142, f1 0.9189472794532776
ep7_l1_test_time 0.5958805084228516
Test Epoch7 layer2 Acc 0.9194736842105263, AUC 0.9792115688323975, avg_entr 0.016793828457593918, f1 0.9194737076759338
ep7_l2_test_time 0.8670001029968262
Test Epoch7 layer3 Acc 0.9194736842105263, AUC 0.9796571731567383, avg_entr 0.015464471653103828, f1 0.9194737076759338
ep7_l3_test_time 1.1291661262512207
Test Epoch7 layer4 Acc 0.9192105263157895, AUC 0.9800676107406616, avg_entr 0.014089030213654041, f1 0.9192105531692505
ep7_l4_test_time 1.389965534210205
gc 0
Train Epoch8 Acc 0.9590583333333333 (115087/120000), AUC 0.9937775135040283
ep8_train_time 167.21937489509583
Test Epoch8 layer0 Acc 0.9207894736842105, AUC 0.9824604392051697, avg_entr 0.05852900445461273, f1 0.9207894802093506
ep8_l0_test_time 0.3321046829223633
Test Epoch8 layer1 Acc 0.9192105263157895, AUC 0.975996732711792, avg_entr 0.0212370827794075, f1 0.9192105531692505
ep8_l1_test_time 0.6022446155548096
Test Epoch8 layer2 Acc 0.9205263157894736, AUC 0.9766104221343994, avg_entr 0.016105566173791885, f1 0.9205263257026672
ep8_l2_test_time 0.8654370307922363
Test Epoch8 layer3 Acc 0.9192105263157895, AUC 0.9783283472061157, avg_entr 0.014560695737600327, f1 0.9192105531692505
ep8_l3_test_time 1.1257941722869873
Test Epoch8 layer4 Acc 0.9194736842105263, AUC 0.9780598878860474, avg_entr 0.013373364694416523, f1 0.9194737076759338
ep8_l4_test_time 1.3975510597229004
gc 0
Train Epoch9 Acc 0.9607 (115284/120000), AUC 0.9941005706787109
ep9_train_time 167.1853141784668
Test Epoch9 layer0 Acc 0.9226315789473685, AUC 0.9823276996612549, avg_entr 0.05450132489204407, f1 0.922631561756134
ep9_l0_test_time 0.32829976081848145
Test Epoch9 layer1 Acc 0.92, AUC 0.9765275716781616, avg_entr 0.020715124905109406, f1 0.9200000166893005
ep9_l1_test_time 0.5959110260009766
Test Epoch9 layer2 Acc 0.9173684210526316, AUC 0.9789245128631592, avg_entr 0.015998683869838715, f1 0.9173683524131775
ep9_l2_test_time 0.8595337867736816
Test Epoch9 layer3 Acc 0.9173684210526316, AUC 0.9790962934494019, avg_entr 0.014069218188524246, f1 0.9173683524131775
ep9_l3_test_time 1.1246669292449951
Test Epoch9 layer4 Acc 0.9168421052631579, AUC 0.9790540933609009, avg_entr 0.013080352917313576, f1 0.9168421030044556
ep9_l4_test_time 1.3893194198608398
gc 0
Train Epoch10 Acc 0.9636083333333333 (115633/120000), AUC 0.9949551820755005
ep10_train_time 167.15927815437317
Test Epoch10 layer0 Acc 0.9210526315789473, AUC 0.982255220413208, avg_entr 0.05114348232746124, f1 0.9210526347160339
ep10_l0_test_time 0.32220458984375
Test Epoch10 layer1 Acc 0.9194736842105263, AUC 0.9749806523323059, avg_entr 0.02001281827688217, f1 0.9194737076759338
ep10_l1_test_time 0.5991053581237793
Test Epoch10 layer2 Acc 0.9186842105263158, AUC 0.9766513109207153, avg_entr 0.015177398920059204, f1 0.918684184551239
ep10_l2_test_time 0.8671755790710449
Test Epoch10 layer3 Acc 0.9192105263157895, AUC 0.9787126779556274, avg_entr 0.01326563861221075, f1 0.9192105531692505
ep10_l3_test_time 1.1325058937072754
Test Epoch10 layer4 Acc 0.9192105263157895, AUC 0.9763821363449097, avg_entr 0.012115759775042534, f1 0.9192105531692505
ep10_l4_test_time 1.3928611278533936
gc 0
Train Epoch11 Acc 0.9643833333333334 (115726/120000), AUC 0.9952378869056702
ep11_train_time 167.10951399803162
Test Epoch11 layer0 Acc 0.9210526315789473, AUC 0.9821590781211853, avg_entr 0.04935554787516594, f1 0.9210526347160339
ep11_l0_test_time 0.3264462947845459
Test Epoch11 layer1 Acc 0.9171052631578948, AUC 0.9739503860473633, avg_entr 0.01841617189347744, f1 0.9171052575111389
ep11_l1_test_time 0.6002686023712158
Test Epoch11 layer2 Acc 0.9173684210526316, AUC 0.9753318428993225, avg_entr 0.013868095353245735, f1 0.9173683524131775
ep11_l2_test_time 0.8642241954803467
Test Epoch11 layer3 Acc 0.9168421052631579, AUC 0.9764596819877625, avg_entr 0.012283711694180965, f1 0.9168421030044556
ep11_l3_test_time 1.1207194328308105
Test Epoch11 layer4 Acc 0.916578947368421, AUC 0.9762061834335327, avg_entr 0.011423670686781406, f1 0.9165789484977722
ep11_l4_test_time 1.3949861526489258
gc 0
Train Epoch12 Acc 0.9652833333333334 (115834/120000), AUC 0.9953176379203796
ep12_train_time 167.07436776161194
Test Epoch12 layer0 Acc 0.9205263157894736, AUC 0.9821478128433228, avg_entr 0.047151707112789154, f1 0.9205263257026672
ep12_l0_test_time 0.32964038848876953
Test Epoch12 layer1 Acc 0.9186842105263158, AUC 0.9744790196418762, avg_entr 0.0177881121635437, f1 0.918684184551239
ep12_l1_test_time 0.5989894866943359
Test Epoch12 layer2 Acc 0.9176315789473685, AUC 0.976096510887146, avg_entr 0.013791997916996479, f1 0.9176315665245056
ep12_l2_test_time 0.8577334880828857
Test Epoch12 layer3 Acc 0.9178947368421052, AUC 0.9768911600112915, avg_entr 0.01216124463826418, f1 0.917894721031189
ep12_l3_test_time 1.128488540649414
Test Epoch12 layer4 Acc 0.9173684210526316, AUC 0.9743670225143433, avg_entr 0.011276817880570889, f1 0.9173683524131775
ep12_l4_test_time 1.3999888896942139
gc 0
Train Epoch13 Acc 0.9657 (115884/120000), AUC 0.9953430891036987
ep13_train_time 167.2635338306427
Test Epoch13 layer0 Acc 0.9202631578947369, AUC 0.9821637868881226, avg_entr 0.046409133821725845, f1 0.9202631711959839
ep13_l0_test_time 0.32639002799987793
Test Epoch13 layer1 Acc 0.9171052631578948, AUC 0.9747092723846436, avg_entr 0.01684204861521721, f1 0.9171052575111389
ep13_l1_test_time 0.599841833114624
Test Epoch13 layer2 Acc 0.9168421052631579, AUC 0.9763253927230835, avg_entr 0.012620076537132263, f1 0.9168421030044556
ep13_l2_test_time 0.8616416454315186
Test Epoch13 layer3 Acc 0.9173684210526316, AUC 0.9752932786941528, avg_entr 0.01084041129797697, f1 0.9173683524131775
ep13_l3_test_time 1.1301398277282715
Test Epoch13 layer4 Acc 0.9168421052631579, AUC 0.9764251708984375, avg_entr 0.009637489914894104, f1 0.9168421030044556
ep13_l4_test_time 1.3989558219909668
gc 0
Train Epoch14 Acc 0.9672083333333333 (116065/120000), AUC 0.9959386587142944
ep14_train_time 167.1045024394989
Test Epoch14 layer0 Acc 0.9205263157894736, AUC 0.9820842742919922, avg_entr 0.04487713798880577, f1 0.9205263257026672
ep14_l0_test_time 0.3186924457550049
Test Epoch14 layer1 Acc 0.9171052631578948, AUC 0.973623514175415, avg_entr 0.016442742198705673, f1 0.9171052575111389
ep14_l1_test_time 0.5993912220001221
Test Epoch14 layer2 Acc 0.9173684210526316, AUC 0.9751255512237549, avg_entr 0.012707037851214409, f1 0.9173683524131775
ep14_l2_test_time 0.8559222221374512
Test Epoch14 layer3 Acc 0.9173684210526316, AUC 0.9765855073928833, avg_entr 0.011073052883148193, f1 0.9173683524131775
ep14_l3_test_time 1.127063274383545
Test Epoch14 layer4 Acc 0.9178947368421052, AUC 0.9750144481658936, avg_entr 0.009808002039790154, f1 0.917894721031189
ep14_l4_test_time 1.3892583847045898
gc 0
Train Epoch15 Acc 0.9678166666666667 (116138/120000), AUC 0.9957798719406128
ep15_train_time 167.2277340888977
Test Epoch15 layer0 Acc 0.9205263157894736, AUC 0.9819971323013306, avg_entr 0.04339275881648064, f1 0.9205263257026672
ep15_l0_test_time 0.32677221298217773
Test Epoch15 layer1 Acc 0.9163157894736842, AUC 0.9738067984580994, avg_entr 0.0160291139036417, f1 0.9163157939910889
ep15_l1_test_time 0.596451997756958
Test Epoch15 layer2 Acc 0.9163157894736842, AUC 0.9753934144973755, avg_entr 0.012431588023900986, f1 0.9163157939910889
ep15_l2_test_time 0.8656258583068848
Test Epoch15 layer3 Acc 0.9163157894736842, AUC 0.975678026676178, avg_entr 0.010772953741252422, f1 0.9163157939910889
ep15_l3_test_time 1.1240837574005127
Test Epoch15 layer4 Acc 0.9163157894736842, AUC 0.9755260944366455, avg_entr 0.009832194074988365, f1 0.9163157939910889
ep15_l4_test_time 1.3918991088867188
gc 0
Train Epoch16 Acc 0.9679833333333333 (116158/120000), AUC 0.9958844184875488
ep16_train_time 167.23201441764832
Test Epoch16 layer0 Acc 0.9202631578947369, AUC 0.9820106625556946, avg_entr 0.042092762887477875, f1 0.9202631711959839
ep16_l0_test_time 0.31988024711608887
Test Epoch16 layer1 Acc 0.916578947368421, AUC 0.9727224707603455, avg_entr 0.015736544504761696, f1 0.9165789484977722
ep16_l1_test_time 0.5960450172424316
Test Epoch16 layer2 Acc 0.9155263157894736, AUC 0.9726945161819458, avg_entr 0.01198653969913721, f1 0.9155263304710388
ep16_l2_test_time 0.8562324047088623
Test Epoch16 layer3 Acc 0.9163157894736842, AUC 0.9748274087905884, avg_entr 0.010303197428584099, f1 0.9163157939910889
ep16_l3_test_time 1.1271326541900635
Test Epoch16 layer4 Acc 0.9157894736842105, AUC 0.9722059965133667, avg_entr 0.009427866898477077, f1 0.9157894849777222
ep16_l4_test_time 1.3963384628295898
gc 0
Train Epoch17 Acc 0.9681666666666666 (116180/120000), AUC 0.9959231615066528
ep17_train_time 167.20666313171387
Test Epoch17 layer0 Acc 0.9207894736842105, AUC 0.9819833040237427, avg_entr 0.04070563241839409, f1 0.9207894802093506
ep17_l0_test_time 0.3275885581970215
Test Epoch17 layer1 Acc 0.916578947368421, AUC 0.9722522497177124, avg_entr 0.014802386984229088, f1 0.9165789484977722
ep17_l1_test_time 0.6001782417297363
Test Epoch17 layer2 Acc 0.9163157894736842, AUC 0.9733612537384033, avg_entr 0.011282629333436489, f1 0.9163157939910889
ep17_l2_test_time 0.8632268905639648
Test Epoch17 layer3 Acc 0.9163157894736842, AUC 0.9728085994720459, avg_entr 0.009671492502093315, f1 0.9163157939910889
ep17_l3_test_time 1.1254041194915771
Test Epoch17 layer4 Acc 0.9168421052631579, AUC 0.9733303189277649, avg_entr 0.008945607580244541, f1 0.9168421030044556
ep17_l4_test_time 1.391340732574463
gc 0
Train Epoch18 Acc 0.9693 (116316/120000), AUC 0.9962078928947449
ep18_train_time 167.14648699760437
Test Epoch18 layer0 Acc 0.9202631578947369, AUC 0.9819592833518982, avg_entr 0.040600866079330444, f1 0.9202631711959839
ep18_l0_test_time 0.3296794891357422
Test Epoch18 layer1 Acc 0.9163157894736842, AUC 0.9721864461898804, avg_entr 0.01532724965363741, f1 0.9163157939910889
ep18_l1_test_time 0.5952329635620117
Test Epoch18 layer2 Acc 0.915, AUC 0.972751259803772, avg_entr 0.011738681234419346, f1 0.9150000214576721
ep18_l2_test_time 0.8613810539245605
Test Epoch18 layer3 Acc 0.9147368421052632, AUC 0.9738749265670776, avg_entr 0.010006513446569443, f1 0.9147368669509888
ep18_l3_test_time 1.1281118392944336
Test Epoch18 layer4 Acc 0.9147368421052632, AUC 0.9718744158744812, avg_entr 0.009020580910146236, f1 0.9147368669509888
ep18_l4_test_time 1.3927412033081055
gc 0
Train Epoch19 Acc 0.9693916666666667 (116327/120000), AUC 0.9961934089660645
ep19_train_time 167.3465793132782
Test Epoch19 layer0 Acc 0.9202631578947369, AUC 0.9819908142089844, avg_entr 0.03932610899209976, f1 0.9202631711959839
ep19_l0_test_time 0.3267490863800049
Test Epoch19 layer1 Acc 0.9152631578947369, AUC 0.9727720022201538, avg_entr 0.01457535196095705, f1 0.9152631759643555
ep19_l1_test_time 0.5907313823699951
Test Epoch19 layer2 Acc 0.9157894736842105, AUC 0.9733887910842896, avg_entr 0.010840688832104206, f1 0.9157894849777222
ep19_l2_test_time 0.8601434230804443
Test Epoch19 layer3 Acc 0.9155263157894736, AUC 0.9740223288536072, avg_entr 0.009041053242981434, f1 0.9155263304710388
ep19_l3_test_time 1.1277906894683838
Test Epoch19 layer4 Acc 0.9155263157894736, AUC 0.9722956418991089, avg_entr 0.008244206197559834, f1 0.9155263304710388
ep19_l4_test_time 1.396129846572876
gc 0
Train Epoch20 Acc 0.9692833333333334 (116314/120000), AUC 0.9961962699890137
ep20_train_time 167.24000024795532
Test Epoch20 layer0 Acc 0.9192105263157895, AUC 0.9819375276565552, avg_entr 0.038249775767326355, f1 0.9192105531692505
ep20_l0_test_time 0.31985950469970703
Test Epoch20 layer1 Acc 0.9152631578947369, AUC 0.9721909761428833, avg_entr 0.014340151101350784, f1 0.9152631759643555
ep20_l1_test_time 0.5931870937347412
Test Epoch20 layer2 Acc 0.9147368421052632, AUC 0.971660315990448, avg_entr 0.011294171214103699, f1 0.9147368669509888
ep20_l2_test_time 0.8631057739257812
Test Epoch20 layer3 Acc 0.9152631578947369, AUC 0.9721941947937012, avg_entr 0.009943275712430477, f1 0.9152631759643555
ep20_l3_test_time 1.1292140483856201
Test Epoch20 layer4 Acc 0.915, AUC 0.9706993699073792, avg_entr 0.009200715459883213, f1 0.9150000214576721
ep20_l4_test_time 1.395331621170044
gc 0
Train Epoch21 Acc 0.96975 (116370/120000), AUC 0.996171772480011
ep21_train_time 167.2353639602661
Test Epoch21 layer0 Acc 0.9205263157894736, AUC 0.981914758682251, avg_entr 0.03797772154211998, f1 0.9205263257026672
ep21_l0_test_time 0.3308079242706299
Test Epoch21 layer1 Acc 0.9157894736842105, AUC 0.9720986485481262, avg_entr 0.014065801165997982, f1 0.9157894849777222
ep21_l1_test_time 0.6019136905670166
Test Epoch21 layer2 Acc 0.9163157894736842, AUC 0.9719626903533936, avg_entr 0.010447184555232525, f1 0.9163157939910889
ep21_l2_test_time 0.8615705966949463
Test Epoch21 layer3 Acc 0.9157894736842105, AUC 0.9725559949874878, avg_entr 0.008464929647743702, f1 0.9157894849777222
ep21_l3_test_time 1.128345012664795
Test Epoch21 layer4 Acc 0.9157894736842105, AUC 0.9705719947814941, avg_entr 0.007884765975177288, f1 0.9157894849777222
ep21_l4_test_time 1.3920505046844482
gc 0
Train Epoch22 Acc 0.9701 (116412/120000), AUC 0.99635910987854
ep22_train_time 167.20681738853455
Test Epoch22 layer0 Acc 0.9189473684210526, AUC 0.9819028973579407, avg_entr 0.03706779330968857, f1 0.9189472794532776
ep22_l0_test_time 0.32146739959716797
Test Epoch22 layer1 Acc 0.9155263157894736, AUC 0.9721977114677429, avg_entr 0.013931004330515862, f1 0.9155263304710388
ep22_l1_test_time 0.5942115783691406
Test Epoch22 layer2 Acc 0.9155263157894736, AUC 0.9721919894218445, avg_entr 0.010974051430821419, f1 0.9155263304710388
ep22_l2_test_time 0.8651275634765625
Test Epoch22 layer3 Acc 0.9152631578947369, AUC 0.9722113609313965, avg_entr 0.009333610534667969, f1 0.9152631759643555
ep22_l3_test_time 1.122654676437378
Test Epoch22 layer4 Acc 0.9157894736842105, AUC 0.9712298512458801, avg_entr 0.0086119519546628, f1 0.9157894849777222
ep22_l4_test_time 1.3934831619262695
gc 0
Train Epoch23 Acc 0.970025 (116403/120000), AUC 0.9963447451591492
ep23_train_time 167.17894887924194
Test Epoch23 layer0 Acc 0.92, AUC 0.9819227457046509, avg_entr 0.03711356967687607, f1 0.9200000166893005
ep23_l0_test_time 0.3164684772491455
Test Epoch23 layer1 Acc 0.9155263157894736, AUC 0.9723522663116455, avg_entr 0.01381258200854063, f1 0.9155263304710388
ep23_l1_test_time 0.5959703922271729
Test Epoch23 layer2 Acc 0.9155263157894736, AUC 0.9726297855377197, avg_entr 0.010786059312522411, f1 0.9155263304710388
ep23_l2_test_time 0.8659195899963379
Test Epoch23 layer3 Acc 0.9157894736842105, AUC 0.9726824760437012, avg_entr 0.009153828024864197, f1 0.9157894849777222
ep23_l3_test_time 1.1252291202545166
Test Epoch23 layer4 Acc 0.9152631578947369, AUC 0.9718395471572876, avg_entr 0.00840037688612938, f1 0.9152631759643555
ep23_l4_test_time 1.3956243991851807
gc 0
Train Epoch24 Acc 0.9702833333333334 (116434/120000), AUC 0.996289074420929
ep24_train_time 167.1508333683014
Test Epoch24 layer0 Acc 0.92, AUC 0.9818928241729736, avg_entr 0.03659707307815552, f1 0.9200000166893005
ep24_l0_test_time 0.32712388038635254
Test Epoch24 layer1 Acc 0.915, AUC 0.9721813797950745, avg_entr 0.014010297134518623, f1 0.9150000214576721
ep24_l1_test_time 0.5946516990661621
Test Epoch24 layer2 Acc 0.9160526315789473, AUC 0.9724572896957397, avg_entr 0.010990585200488567, f1 0.9160526394844055
ep24_l2_test_time 0.8622071743011475
Test Epoch24 layer3 Acc 0.9152631578947369, AUC 0.9720066785812378, avg_entr 0.009465061128139496, f1 0.9152631759643555
ep24_l3_test_time 1.1252384185791016
Test Epoch24 layer4 Acc 0.915, AUC 0.9712263941764832, avg_entr 0.008436071686446667, f1 0.9150000214576721
ep24_l4_test_time 1.398674488067627
gc 0
Train Epoch25 Acc 0.97035 (116442/120000), AUC 0.9963796138763428
ep25_train_time 167.13139390945435
Test Epoch25 layer0 Acc 0.9194736842105263, AUC 0.9818916320800781, avg_entr 0.03621239960193634, f1 0.9194737076759338
ep25_l0_test_time 0.3363759517669678
Test Epoch25 layer1 Acc 0.9152631578947369, AUC 0.9716318249702454, avg_entr 0.013360017910599709, f1 0.9152631759643555
ep25_l1_test_time 0.5991456508636475
Test Epoch25 layer2 Acc 0.9152631578947369, AUC 0.9708396196365356, avg_entr 0.010330792516469955, f1 0.9152631759643555
ep25_l2_test_time 0.862311601638794
Test Epoch25 layer3 Acc 0.9152631578947369, AUC 0.9717264175415039, avg_entr 0.008725741878151894, f1 0.9152631759643555
ep25_l3_test_time 1.127199649810791
Test Epoch25 layer4 Acc 0.9155263157894736, AUC 0.9698562622070312, avg_entr 0.00802280381321907, f1 0.9155263304710388
ep25_l4_test_time 1.3933923244476318
gc 0
Train Epoch26 Acc 0.9707083333333333 (116485/120000), AUC 0.9965010285377502
ep26_train_time 167.15270137786865
Test Epoch26 layer0 Acc 0.9205263157894736, AUC 0.9819076657295227, avg_entr 0.036174628883600235, f1 0.9205263257026672
ep26_l0_test_time 0.321713924407959
Test Epoch26 layer1 Acc 0.9152631578947369, AUC 0.9720576405525208, avg_entr 0.01345115341246128, f1 0.9152631759643555
ep26_l1_test_time 0.5902018547058105
Test Epoch26 layer2 Acc 0.9157894736842105, AUC 0.9722195267677307, avg_entr 0.010458803735673428, f1 0.9157894849777222
ep26_l2_test_time 0.8624708652496338
Test Epoch26 layer3 Acc 0.9157894736842105, AUC 0.9721469879150391, avg_entr 0.008795302361249924, f1 0.9157894849777222
ep26_l3_test_time 1.1227681636810303
Test Epoch26 layer4 Acc 0.9152631578947369, AUC 0.9715604782104492, avg_entr 0.008003190159797668, f1 0.9152631759643555
ep26_l4_test_time 1.393913984298706
gc 0
Train Epoch27 Acc 0.9704916666666666 (116459/120000), AUC 0.9964659214019775
ep27_train_time 167.07929921150208
Test Epoch27 layer0 Acc 0.9205263157894736, AUC 0.9819050431251526, avg_entr 0.036100294440984726, f1 0.9205263257026672
ep27_l0_test_time 0.331127405166626
Test Epoch27 layer1 Acc 0.9155263157894736, AUC 0.9721959829330444, avg_entr 0.013093597255647182, f1 0.9155263304710388
ep27_l1_test_time 0.5991494655609131
Test Epoch27 layer2 Acc 0.915, AUC 0.9722409248352051, avg_entr 0.010313273407518864, f1 0.9150000214576721
ep27_l2_test_time 0.8576273918151855
Test Epoch27 layer3 Acc 0.9155263157894736, AUC 0.9721786975860596, avg_entr 0.008695180527865887, f1 0.9155263304710388
ep27_l3_test_time 1.1260862350463867
Test Epoch27 layer4 Acc 0.9152631578947369, AUC 0.9716421961784363, avg_entr 0.008085176348686218, f1 0.9152631759643555
ep27_l4_test_time 1.395991325378418
gc 0
Train Epoch28 Acc 0.9706 (116472/120000), AUC 0.9964349865913391
ep28_train_time 167.24179673194885
Test Epoch28 layer0 Acc 0.921578947368421, AUC 0.9818875789642334, avg_entr 0.03574356809258461, f1 0.9215789437294006
ep28_l0_test_time 0.32827067375183105
Test Epoch28 layer1 Acc 0.915, AUC 0.9719513058662415, avg_entr 0.013247977942228317, f1 0.9150000214576721
ep28_l1_test_time 0.6026160717010498
Test Epoch28 layer2 Acc 0.9155263157894736, AUC 0.9718204736709595, avg_entr 0.009918594732880592, f1 0.9155263304710388
ep28_l2_test_time 0.8587534427642822
Test Epoch28 layer3 Acc 0.915, AUC 0.9719034433364868, avg_entr 0.00804878305643797, f1 0.9150000214576721
ep28_l3_test_time 1.1254615783691406
Test Epoch28 layer4 Acc 0.915, AUC 0.9708431363105774, avg_entr 0.007305164821445942, f1 0.9150000214576721
ep28_l4_test_time 1.3944590091705322
gc 0
Train Epoch29 Acc 0.9707083333333333 (116485/120000), AUC 0.9963767528533936
ep29_train_time 167.12221240997314
Test Epoch29 layer0 Acc 0.9210526315789473, AUC 0.9818910956382751, avg_entr 0.035845253616571426, f1 0.9210526347160339
ep29_l0_test_time 0.3291482925415039
Test Epoch29 layer1 Acc 0.9152631578947369, AUC 0.9722020030021667, avg_entr 0.013139219023287296, f1 0.9152631759643555
ep29_l1_test_time 0.5928497314453125
Test Epoch29 layer2 Acc 0.9155263157894736, AUC 0.9722064733505249, avg_entr 0.009658372960984707, f1 0.9155263304710388
ep29_l2_test_time 0.8616073131561279
Test Epoch29 layer3 Acc 0.9152631578947369, AUC 0.9722204804420471, avg_entr 0.0077878981828689575, f1 0.9152631759643555
ep29_l3_test_time 1.1282405853271484
Test Epoch29 layer4 Acc 0.9152631578947369, AUC 0.971788763999939, avg_entr 0.007271735463291407, f1 0.9152631759643555
ep29_l4_test_time 1.3944792747497559
Best AUC tensor(0.9253) 3 3
train_loss (2, 5, 30)
valid_acc (5, 30)
valid_AUC (5, 30)
train_acc (30,)
total_train+valid_time 5148.718346357346
Start Testing
Load ckpt at ckpt/ag_news_transformeral_l5_pad177//ag_news_transformeral_l5.pt
Test layer0 Acc 0.9123684210526316, AUC 0.9804333448410034, avg_entr 0.09714457392692566, f1 0.9123684167861938
l0_test_time 0.3334174156188965
Test layer1 Acc 0.9142105263157895, AUC 0.9790675640106201, avg_entr 0.037144724279642105, f1 0.9142104983329773
l1_test_time 0.5992050170898438
Test layer2 Acc 0.9136842105263158, AUC 0.9799990057945251, avg_entr 0.031731124967336655, f1 0.9136841893196106
l2_test_time 0.8587591648101807
Test layer3 Acc 0.9139473684210526, AUC 0.9811186194419861, avg_entr 0.029077250510454178, f1 0.913947343826294
l3_test_time 1.1311907768249512
Test layer4 Acc 0.9134210526315789, AUC 0.9811414480209351, avg_entr 0.027752377092838287, f1 0.9134210348129272
l4_test_time 1.4001426696777344
