total count words 222751
vocab size 30000
train size 40000, valid size 5000, test size 5000
found 27937 words in glove
model: TransformerALsideText(
  (layers): ModuleList(
    (0): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=2, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=2, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
  (emb_layers): ModuleList(
    (0): Embedding(30000, 300)
    (1): Embedding(30000, 300)
    (2): Embedding(30000, 300)
    (3): Embedding(30000, 300)
    (4): Embedding(30000, 300)
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.multi_headed_attention.linears.0.weight 90000
layers.0.enc.f.multi_headed_attention.linears.0.bias 300
layers.0.enc.f.multi_headed_attention.linears.1.weight 90000
layers.0.enc.f.multi_headed_attention.linears.1.bias 300
layers.0.enc.f.multi_headed_attention.linears.2.weight 90000
layers.0.enc.f.multi_headed_attention.linears.2.bias 300
layers.0.enc.f.multi_headed_attention.linears.3.weight 90000
layers.0.enc.f.multi_headed_attention.linears.3.bias 300
layers.0.enc.f.feed_forward.w_1.weight 90000
layers.0.enc.f.feed_forward.w_1.bias 300
layers.0.enc.f.feed_forward.w_2.weight 90000
layers.0.enc.f.feed_forward.w_2.bias 300
layers.0.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.0.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.0.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.0.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.0.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.0.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.0.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.0.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.0.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.0.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.0.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.0.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.0.enc.f.encoder.norm.a 300
layers.0.enc.f.encoder.norm.b 300
layers.0.ae.g.0.weight 256
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 256
layers.0.ae.h.0.bias 2
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
emb_layers.0.weight 9000000
emb_layers.1.weight 9000000
emb_layers.2.weight 9000000
emb_layers.3.weight 9000000
emb_layers.4.weight 9000000
Total Trainable Params: 50758378
init_time 21.055000505
Start Training
gc 0
Train Epoch0 Acc 0.507175 (20287/40000), AUC 0.5048027634620667
ep0_train_time 65.84341741300001
Test Epoch0 layer0 Acc 0.5088, AUC 0.629130482673645, avg_entr 0.6736551523208618, f1 0.5088000297546387
ep0_l0_test_time 0.6234439930000093
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 0
Test Epoch0 layer1 Acc 0.5, AUC 0.5869500637054443, avg_entr 0.6789618134498596, f1 0.5
ep0_l1_test_time 0.8309549420000053
Test Epoch0 layer2 Acc 0.5, AUC 0.5586717128753662, avg_entr 0.6783735156059265, f1 0.5
ep0_l2_test_time 1.1420512899999977
Test Epoch0 layer3 Acc 0.5, AUC 0.4677066504955292, avg_entr 0.6812210083007812, f1 0.5
ep0_l3_test_time 1.6201589470000073
Test Epoch0 layer4 Acc 0.5, AUC 0.5265830159187317, avg_entr 0.6821075081825256, f1 0.5
ep0_l4_test_time 2.2479747849999967
gc 0
Train Epoch1 Acc 0.5118 (20472/40000), AUC 0.5190379619598389
ep1_train_time 65.539922498
Test Epoch1 layer0 Acc 0.6374, AUC 0.7182592749595642, avg_entr 0.5909204483032227, f1 0.6373999714851379
ep1_l0_test_time 0.6177052559999936
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 1
Test Epoch1 layer1 Acc 0.5736, AUC 0.7205138206481934, avg_entr 0.5955811142921448, f1 0.5735999941825867
ep1_l1_test_time 0.8260794029999943
Test Epoch1 layer2 Acc 0.5066, AUC 0.7187047004699707, avg_entr 0.6491032242774963, f1 0.506600022315979
ep1_l2_test_time 1.1404745650000052
Test Epoch1 layer3 Acc 0.5, AUC 0.6383397579193115, avg_entr 0.6880059838294983, f1 0.5
ep1_l3_test_time 1.6136243740000111
Test Epoch1 layer4 Acc 0.5, AUC 0.6719253659248352, avg_entr 0.6872130036354065, f1 0.5
ep1_l4_test_time 2.2505026380000004
gc 0
Train Epoch2 Acc 0.5621 (22484/40000), AUC 0.5890456438064575
ep2_train_time 65.516018067
Test Epoch2 layer0 Acc 0.71, AUC 0.7970960140228271, avg_entr 0.4592542052268982, f1 0.7099999785423279
ep2_l0_test_time 0.6198689899999863
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 2
Test Epoch2 layer1 Acc 0.7, AUC 0.8035433292388916, avg_entr 0.43136483430862427, f1 0.699999988079071
ep2_l1_test_time 0.8263409759999831
Test Epoch2 layer2 Acc 0.6982, AUC 0.8041905164718628, avg_entr 0.4438275992870331, f1 0.698199987411499
ep2_l2_test_time 1.1411316399999976
Test Epoch2 layer3 Acc 0.6798, AUC 0.8041313886642456, avg_entr 0.5242863893508911, f1 0.6797999739646912
ep2_l3_test_time 1.6216802579999978
Test Epoch2 layer4 Acc 0.7042, AUC 0.8032876253128052, avg_entr 0.6661884188652039, f1 0.704200029373169
ep2_l4_test_time 2.2565411719999986
gc 0
Train Epoch3 Acc 0.6668 (26672/40000), AUC 0.7350010871887207
ep3_train_time 65.52498075799997
Test Epoch3 layer0 Acc 0.7378, AUC 0.8351157903671265, avg_entr 0.3703776001930237, f1 0.7378000020980835
ep3_l0_test_time 0.617328767999993
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 3
Test Epoch3 layer1 Acc 0.7536, AUC 0.8456346988677979, avg_entr 0.35255128145217896, f1 0.7535999417304993
ep3_l1_test_time 0.8238395930000024
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 3
Test Epoch3 layer2 Acc 0.7576, AUC 0.8476484417915344, avg_entr 0.3565424680709839, f1 0.7576000690460205
ep3_l2_test_time 1.1432017700000188
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 3
Test Epoch3 layer3 Acc 0.7632, AUC 0.8476085662841797, avg_entr 0.37683290243148804, f1 0.7631999850273132
ep3_l3_test_time 1.6207975260000467
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 3
Test Epoch3 layer4 Acc 0.723, AUC 0.8483490943908691, avg_entr 0.399006724357605, f1 0.7229999899864197
ep3_l4_test_time 2.2549737530000016
gc 0
Train Epoch4 Acc 0.72575 (29030/40000), AUC 0.8001501560211182
ep4_train_time 65.46434010800004
Test Epoch4 layer0 Acc 0.7512, AUC 0.8455140590667725, avg_entr 0.3419443964958191, f1 0.7512000203132629
ep4_l0_test_time 0.6220102890000021
Test Epoch4 layer1 Acc 0.7678, AUC 0.8601081371307373, avg_entr 0.3342614471912384, f1 0.767799973487854
ep4_l1_test_time 0.8272233810000102
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 4
Test Epoch4 layer2 Acc 0.7694, AUC 0.8610689640045166, avg_entr 0.35384324193000793, f1 0.7694000005722046
ep4_l2_test_time 1.1446033330000205
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 4
Test Epoch4 layer3 Acc 0.7738, AUC 0.8610970377922058, avg_entr 0.3857094347476959, f1 0.7738000154495239
ep4_l3_test_time 1.6258914919999938
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 4
Test Epoch4 layer4 Acc 0.7744, AUC 0.8614094853401184, avg_entr 0.42194581031799316, f1 0.7743999361991882
ep4_l4_test_time 2.255675752000002
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 4
gc 0
Train Epoch5 Acc 0.762225 (30489/40000), AUC 0.8427642583847046
ep5_train_time 65.47300663700003
Test Epoch5 layer0 Acc 0.7714, AUC 0.8578310608863831, avg_entr 0.3186703324317932, f1 0.771399974822998
ep5_l0_test_time 0.619846482000014
Test Epoch5 layer1 Acc 0.7888, AUC 0.8714696764945984, avg_entr 0.29778632521629333, f1 0.7888000011444092
ep5_l1_test_time 0.8204426099999864
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 5
Test Epoch5 layer2 Acc 0.7878, AUC 0.8725651502609253, avg_entr 0.30969536304473877, f1 0.7877999544143677
ep5_l2_test_time 1.1552875600000334
Test Epoch5 layer3 Acc 0.7882, AUC 0.872577428817749, avg_entr 0.32653382420539856, f1 0.7882000207901001
ep5_l3_test_time 1.6155372180000427
Test Epoch5 layer4 Acc 0.7894, AUC 0.8727251291275024, avg_entr 0.34638574719429016, f1 0.7893999814987183
ep5_l4_test_time 2.249175593000018
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 5
gc 0
Train Epoch6 Acc 0.79195 (31678/40000), AUC 0.8745590448379517
ep6_train_time 65.46970448900004
Test Epoch6 layer0 Acc 0.7722, AUC 0.863378643989563, avg_entr 0.2994968891143799, f1 0.7721999287605286
ep6_l0_test_time 0.6171469409999872
Test Epoch6 layer1 Acc 0.791, AUC 0.8790707588195801, avg_entr 0.24903076887130737, f1 0.7910000681877136
ep6_l1_test_time 0.812383197000031
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 6
Test Epoch6 layer2 Acc 0.7952, AUC 0.8809256553649902, avg_entr 0.250846266746521, f1 0.7952000498771667
ep6_l2_test_time 1.1409078629999385
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 6
Test Epoch6 layer3 Acc 0.791, AUC 0.8810791969299316, avg_entr 0.2649616599082947, f1 0.7910000681877136
ep6_l3_test_time 1.6274414370000159
Test Epoch6 layer4 Acc 0.795, AUC 0.881335973739624, avg_entr 0.2763917148113251, f1 0.7950000166893005
ep6_l4_test_time 2.2580697660000624
gc 0
Train Epoch7 Acc 0.814425 (32577/40000), AUC 0.8947077393531799
ep7_train_time 65.46709687400005
Test Epoch7 layer0 Acc 0.7716, AUC 0.8659124374389648, avg_entr 0.25012147426605225, f1 0.771600067615509
ep7_l0_test_time 0.6171630930000447
Test Epoch7 layer1 Acc 0.7832, AUC 0.8785467743873596, avg_entr 0.1907843053340912, f1 0.7832000851631165
ep7_l1_test_time 0.8123763440000857
Test Epoch7 layer2 Acc 0.7812, AUC 0.8814287185668945, avg_entr 0.175099715590477, f1 0.7811999917030334
ep7_l2_test_time 1.129819346999966
Test Epoch7 layer3 Acc 0.781, AUC 0.8818174600601196, avg_entr 0.17584624886512756, f1 0.781000018119812
ep7_l3_test_time 1.6162744949999706
Test Epoch7 layer4 Acc 0.7746, AUC 0.8822316527366638, avg_entr 0.17692182958126068, f1 0.7746000289916992
ep7_l4_test_time 2.2483794020000687
gc 0
Train Epoch8 Acc 0.824525 (32981/40000), AUC 0.9018988013267517
ep8_train_time 65.48904220200006
Test Epoch8 layer0 Acc 0.7612, AUC 0.869843602180481, avg_entr 0.2514954209327698, f1 0.7612000107765198
ep8_l0_test_time 0.6144451899999694
Test Epoch8 layer1 Acc 0.7738, AUC 0.8873001933097839, avg_entr 0.17503765225410461, f1 0.7738000154495239
ep8_l1_test_time 0.831373264999911
Test Epoch8 layer2 Acc 0.7692, AUC 0.8897534012794495, avg_entr 0.1688942313194275, f1 0.7692000269889832
ep8_l2_test_time 1.142113641999913
Test Epoch8 layer3 Acc 0.7668, AUC 0.889978289604187, avg_entr 0.17217811942100525, f1 0.7667999863624573
ep8_l3_test_time 1.6182111289999739
Test Epoch8 layer4 Acc 0.7662, AUC 0.8901721835136414, avg_entr 0.17147764563560486, f1 0.766200065612793
ep8_l4_test_time 2.248505592000015
gc 0
Train Epoch9 Acc 0.832375 (33295/40000), AUC 0.9067788124084473
ep9_train_time 65.47753692899994
Test Epoch9 layer0 Acc 0.7916, AUC 0.8740888833999634, avg_entr 0.23243549466133118, f1 0.7915999889373779
ep9_l0_test_time 0.6170253319999119
Test Epoch9 layer1 Acc 0.8112, AUC 0.8904681205749512, avg_entr 0.1409018635749817, f1 0.8112000226974487
ep9_l1_test_time 0.8117477949999738
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 9
Test Epoch9 layer2 Acc 0.8118, AUC 0.8937143087387085, avg_entr 0.13513211905956268, f1 0.8118000030517578
ep9_l2_test_time 1.1453878620000069
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 9
Test Epoch9 layer3 Acc 0.81, AUC 0.8936516642570496, avg_entr 0.13313362002372742, f1 0.809999942779541
ep9_l3_test_time 1.6282088829999566
Test Epoch9 layer4 Acc 0.81, AUC 0.8937380313873291, avg_entr 0.12989093363285065, f1 0.809999942779541
ep9_l4_test_time 2.253567731999965
gc 0
Train Epoch10 Acc 0.848075 (33923/40000), AUC 0.9228893518447876
ep10_train_time 65.46569256999999
Test Epoch10 layer0 Acc 0.7944, AUC 0.8770046830177307, avg_entr 0.22141194343566895, f1 0.7943999767303467
ep10_l0_test_time 0.6147261559999606
Test Epoch10 layer1 Acc 0.807, AUC 0.8929409980773926, avg_entr 0.1343652606010437, f1 0.8069999814033508
ep10_l1_test_time 0.812631840999984
Test Epoch10 layer2 Acc 0.8026, AUC 0.8921591639518738, avg_entr 0.12171639502048492, f1 0.802600085735321
ep10_l2_test_time 1.1363114850000784
Test Epoch10 layer3 Acc 0.8008, AUC 0.8915190696716309, avg_entr 0.12005926668643951, f1 0.8007999658584595
ep10_l3_test_time 1.614499480999939
Test Epoch10 layer4 Acc 0.8012, AUC 0.8917186260223389, avg_entr 0.11715295165777206, f1 0.8011999726295471
ep10_l4_test_time 2.2484636100000444
gc 0
Train Epoch11 Acc 0.851225 (34049/40000), AUC 0.9230177402496338
ep11_train_time 65.51462494200007
Test Epoch11 layer0 Acc 0.795, AUC 0.8789377212524414, avg_entr 0.20204350352287292, f1 0.7950000166893005
ep11_l0_test_time 0.6174556539999685
Test Epoch11 layer1 Acc 0.8064, AUC 0.8970405459403992, avg_entr 0.12129563093185425, f1 0.8064000010490417
ep11_l1_test_time 0.8095983719999822
Test Epoch11 layer2 Acc 0.8064, AUC 0.8993643522262573, avg_entr 0.10857966542243958, f1 0.8064000010490417
ep11_l2_test_time 1.1337785679999115
Test Epoch11 layer3 Acc 0.806, AUC 0.8997979164123535, avg_entr 0.10927239805459976, f1 0.8059999942779541
ep11_l3_test_time 1.6151502550000032
Test Epoch11 layer4 Acc 0.8072, AUC 0.9000242948532104, avg_entr 0.10964017361402512, f1 0.807200014591217
ep11_l4_test_time 2.2485335669999813
gc 0
Train Epoch12 Acc 0.865975 (34639/40000), AUC 0.934648871421814
ep12_train_time 65.51215007300004
Test Epoch12 layer0 Acc 0.7948, AUC 0.8808777928352356, avg_entr 0.19756342470645905, f1 0.7947999835014343
ep12_l0_test_time 0.6168065780000234
Test Epoch12 layer1 Acc 0.7916, AUC 0.8971304297447205, avg_entr 0.10775970667600632, f1 0.7915999889373779
ep12_l1_test_time 0.8112924839999778
Test Epoch12 layer2 Acc 0.7868, AUC 0.8986421823501587, avg_entr 0.09417731314897537, f1 0.786799967288971
ep12_l2_test_time 1.1323820019999857
Test Epoch12 layer3 Acc 0.7842, AUC 0.8986826539039612, avg_entr 0.09057919681072235, f1 0.7842000126838684
ep12_l3_test_time 1.6158098910000263
Test Epoch12 layer4 Acc 0.7812, AUC 0.8990269899368286, avg_entr 0.08665470033884048, f1 0.7811999917030334
ep12_l4_test_time 2.2523565470000904
gc 0
Train Epoch13 Acc 0.866675 (34667/40000), AUC 0.9351450204849243
ep13_train_time 65.48584063200008
Test Epoch13 layer0 Acc 0.7946, AUC 0.8783391714096069, avg_entr 0.193792924284935, f1 0.7946000695228577
ep13_l0_test_time 0.6170324229999551
Test Epoch13 layer1 Acc 0.8098, AUC 0.895838737487793, avg_entr 0.11073599755764008, f1 0.8098000288009644
ep13_l1_test_time 0.8091585509998822
Test Epoch13 layer2 Acc 0.8096, AUC 0.8988186717033386, avg_entr 0.10005194693803787, f1 0.8095999956130981
ep13_l2_test_time 1.1344641989999218
Test Epoch13 layer3 Acc 0.8096, AUC 0.8993299007415771, avg_entr 0.09811645746231079, f1 0.8095999956130981
ep13_l3_test_time 1.6164841039999374
Test Epoch13 layer4 Acc 0.8098, AUC 0.899528443813324, avg_entr 0.09810835123062134, f1 0.8098000288009644
ep13_l4_test_time 2.2516945170000326
gc 0
Train Epoch14 Acc 0.87925 (35170/40000), AUC 0.9450880289077759
ep14_train_time 65.49299108200012
Test Epoch14 layer0 Acc 0.7992, AUC 0.8786721229553223, avg_entr 0.1810307651758194, f1 0.7991999983787537
ep14_l0_test_time 0.6196261249999679
Test Epoch14 layer1 Acc 0.8176, AUC 0.8969686627388, avg_entr 0.10528042912483215, f1 0.8176000118255615
ep14_l1_test_time 0.8105207139999493
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 14
Test Epoch14 layer2 Acc 0.817, AUC 0.9000015258789062, avg_entr 0.09914422780275345, f1 0.8169999718666077
ep14_l2_test_time 1.1437857660000645
Test Epoch14 layer3 Acc 0.8166, AUC 0.9006397724151611, avg_entr 0.09795959293842316, f1 0.8166000247001648
ep14_l3_test_time 1.6135182619998432
Test Epoch14 layer4 Acc 0.8176, AUC 0.9008674621582031, avg_entr 0.09601461887359619, f1 0.8176000118255615
ep14_l4_test_time 2.2497409049999533
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 14
gc 0
Train Epoch15 Acc 0.883275 (35331/40000), AUC 0.9471738934516907
ep15_train_time 65.477344313
Test Epoch15 layer0 Acc 0.7968, AUC 0.8788024187088013, avg_entr 0.17841792106628418, f1 0.7968000173568726
ep15_l0_test_time 0.6166939780000575
Test Epoch15 layer1 Acc 0.8166, AUC 0.8962847590446472, avg_entr 0.09945029020309448, f1 0.8166000247001648
ep15_l1_test_time 0.8161495429999377
Test Epoch15 layer2 Acc 0.8144, AUC 0.8988087773323059, avg_entr 0.0856640562415123, f1 0.8144000172615051
ep15_l2_test_time 1.1333090020000327
Test Epoch15 layer3 Acc 0.815, AUC 0.8995628356933594, avg_entr 0.08408839255571365, f1 0.8149999976158142
ep15_l3_test_time 1.6151867029998357
Test Epoch15 layer4 Acc 0.8132, AUC 0.8996284008026123, avg_entr 0.08351939171552658, f1 0.8131999969482422
ep15_l4_test_time 2.2502298350000274
gc 0
Train Epoch16 Acc 0.897175 (35887/40000), AUC 0.9573136568069458
ep16_train_time 65.49808202100007
Test Epoch16 layer0 Acc 0.7872, AUC 0.8754177093505859, avg_entr 0.17451372742652893, f1 0.7871999740600586
ep16_l0_test_time 0.6181242190000376
Test Epoch16 layer1 Acc 0.7962, AUC 0.8880922794342041, avg_entr 0.08776630461215973, f1 0.7961999177932739
ep16_l1_test_time 0.811990804000061
Test Epoch16 layer2 Acc 0.7974, AUC 0.8918403387069702, avg_entr 0.08014620095491409, f1 0.7973999977111816
ep16_l2_test_time 1.1345449220000319
Test Epoch16 layer3 Acc 0.798, AUC 0.8927487134933472, avg_entr 0.07952970266342163, f1 0.7979999780654907
ep16_l3_test_time 1.618437096999969
Test Epoch16 layer4 Acc 0.7988, AUC 0.8929259181022644, avg_entr 0.07673542201519012, f1 0.798799991607666
ep16_l4_test_time 2.252297768000062
gc 0
Train Epoch17 Acc 0.909 (36360/40000), AUC 0.9645248651504517
ep17_train_time 65.46923261400002
Test Epoch17 layer0 Acc 0.7956, AUC 0.8734012246131897, avg_entr 0.16435721516609192, f1 0.7955999970436096
ep17_l0_test_time 0.6176177160000407
Test Epoch17 layer1 Acc 0.8106, AUC 0.8897207975387573, avg_entr 0.08376307785511017, f1 0.8105999231338501
ep17_l1_test_time 0.8103954189998603
Test Epoch17 layer2 Acc 0.81, AUC 0.892642617225647, avg_entr 0.07086507230997086, f1 0.809999942779541
ep17_l2_test_time 1.1313674109999283
Test Epoch17 layer3 Acc 0.81, AUC 0.8936764001846313, avg_entr 0.06979227066040039, f1 0.809999942779541
ep17_l3_test_time 1.6142172090001168
Test Epoch17 layer4 Acc 0.8104, AUC 0.893932044506073, avg_entr 0.06985185295343399, f1 0.8104000091552734
ep17_l4_test_time 2.250982221999948
gc 0
Train Epoch18 Acc 0.919775 (36791/40000), AUC 0.9693363904953003
ep18_train_time 65.59261275799986
Test Epoch18 layer0 Acc 0.797, AUC 0.873675525188446, avg_entr 0.15619198977947235, f1 0.796999990940094
ep18_l0_test_time 0.6171372050000627
Test Epoch18 layer1 Acc 0.8108, AUC 0.8883419632911682, avg_entr 0.07194985449314117, f1 0.8108000159263611
ep18_l1_test_time 0.8110802989999684
Test Epoch18 layer2 Acc 0.8084, AUC 0.8912786245346069, avg_entr 0.06308508664369583, f1 0.8083999752998352
ep18_l2_test_time 1.1332089600000472
Test Epoch18 layer3 Acc 0.808, AUC 0.8926540613174438, avg_entr 0.06135362386703491, f1 0.8080000281333923
ep18_l3_test_time 1.6155690849998336
Test Epoch18 layer4 Acc 0.8074, AUC 0.8929972648620605, avg_entr 0.059331782162189484, f1 0.8073999881744385
ep18_l4_test_time 2.2517427360000966
gc 0
Train Epoch19 Acc 0.92565 (37026/40000), AUC 0.974861741065979
ep19_train_time 65.55376361599997
Test Epoch19 layer0 Acc 0.7834, AUC 0.8708157539367676, avg_entr 0.15873323380947113, f1 0.7833999395370483
ep19_l0_test_time 0.6197612700000263
Test Epoch19 layer1 Acc 0.8054, AUC 0.8861695528030396, avg_entr 0.07433067262172699, f1 0.805400013923645
ep19_l1_test_time 0.817096465999839
Test Epoch19 layer2 Acc 0.8056, AUC 0.8906601667404175, avg_entr 0.0648108720779419, f1 0.8055999875068665
ep19_l2_test_time 1.1441407510001227
Test Epoch19 layer3 Acc 0.806, AUC 0.8926162719726562, avg_entr 0.06596069037914276, f1 0.8059999942779541
ep19_l3_test_time 1.6301105569998526
Test Epoch19 layer4 Acc 0.8062, AUC 0.8929791450500488, avg_entr 0.06544630229473114, f1 0.8062000274658203
ep19_l4_test_time 2.256867138000189
gc 0
Train Epoch20 Acc 0.9308 (37232/40000), AUC 0.9773150682449341
ep20_train_time 65.51718319800011
Test Epoch20 layer0 Acc 0.7844, AUC 0.8697565197944641, avg_entr 0.15398140251636505, f1 0.7843999266624451
ep20_l0_test_time 0.6165781649999644
Test Epoch20 layer1 Acc 0.7916, AUC 0.8812845945358276, avg_entr 0.06375916302204132, f1 0.7915999889373779
ep20_l1_test_time 0.8132774670000344
Test Epoch20 layer2 Acc 0.7938, AUC 0.8855859637260437, avg_entr 0.05331496149301529, f1 0.7937999963760376
ep20_l2_test_time 1.1312388860001192
Test Epoch20 layer3 Acc 0.7932, AUC 0.8872693181037903, avg_entr 0.050485700368881226, f1 0.7932000160217285
ep20_l3_test_time 1.616645508999909
Test Epoch20 layer4 Acc 0.793, AUC 0.8878254890441895, avg_entr 0.05029246583580971, f1 0.7929999828338623
ep20_l4_test_time 2.2497361300002012
gc 0
Train Epoch21 Acc 0.938425 (37537/40000), AUC 0.9811162948608398
ep21_train_time 65.51458384800003
Test Epoch21 layer0 Acc 0.7904, AUC 0.8686221837997437, avg_entr 0.15136662125587463, f1 0.7904000282287598
ep21_l0_test_time 0.6176966809998703
Test Epoch21 layer1 Acc 0.8008, AUC 0.8763114213943481, avg_entr 0.058152154088020325, f1 0.8007999658584595
ep21_l1_test_time 0.8107557990001624
Test Epoch21 layer2 Acc 0.8026, AUC 0.8846924901008606, avg_entr 0.0489983893930912, f1 0.802600085735321
ep21_l2_test_time 1.1327990380000301
Test Epoch21 layer3 Acc 0.8042, AUC 0.8870420455932617, avg_entr 0.04721873253583908, f1 0.8041999936103821
ep21_l3_test_time 1.6143577359998744
Test Epoch21 layer4 Acc 0.803, AUC 0.8869213461875916, avg_entr 0.04587008059024811, f1 0.8029999732971191
ep21_l4_test_time 2.2513597729998764
gc 0
Train Epoch22 Acc 0.94065 (37626/40000), AUC 0.9820414781570435
ep22_train_time 65.87831033300017
Test Epoch22 layer0 Acc 0.7908, AUC 0.8658096790313721, avg_entr 0.14982149004936218, f1 0.7907999753952026
ep22_l0_test_time 0.6183163939999758
Test Epoch22 layer1 Acc 0.8018, AUC 0.8713067770004272, avg_entr 0.05923532694578171, f1 0.801800012588501
ep22_l1_test_time 0.8110936460000175
Test Epoch22 layer2 Acc 0.8008, AUC 0.8800517320632935, avg_entr 0.050861749798059464, f1 0.8007999658584595
ep22_l2_test_time 1.13160446400002
Test Epoch22 layer3 Acc 0.8028, AUC 0.8831278085708618, avg_entr 0.04815337806940079, f1 0.8027999997138977
ep22_l3_test_time 1.6152114429999074
Test Epoch22 layer4 Acc 0.803, AUC 0.88294917345047, avg_entr 0.048239707946777344, f1 0.8029999732971191
ep22_l4_test_time 2.251051784000083
gc 0
Train Epoch23 Acc 0.945125 (37805/40000), AUC 0.9857785701751709
ep23_train_time 65.52743885300015
Test Epoch23 layer0 Acc 0.786, AUC 0.864972710609436, avg_entr 0.1497078388929367, f1 0.7860000133514404
ep23_l0_test_time 0.6194480550000208
Test Epoch23 layer1 Acc 0.7968, AUC 0.8703005313873291, avg_entr 0.05688691511750221, f1 0.7968000173568726
ep23_l1_test_time 0.812717935000137
Test Epoch23 layer2 Acc 0.8004, AUC 0.8800686597824097, avg_entr 0.04739357903599739, f1 0.8004000186920166
ep23_l2_test_time 1.1347483289998763
Test Epoch23 layer3 Acc 0.8014, AUC 0.8832134008407593, avg_entr 0.0470273420214653, f1 0.8014000058174133
ep23_l3_test_time 1.6191865959999632
Test Epoch23 layer4 Acc 0.8016, AUC 0.8832188844680786, avg_entr 0.04687785729765892, f1 0.8015999794006348
ep23_l4_test_time 2.2493572250000398
gc 0
Train Epoch24 Acc 0.946275 (37851/40000), AUC 0.9865334630012512
ep24_train_time 65.50448812600007
Test Epoch24 layer0 Acc 0.7866, AUC 0.8636431097984314, avg_entr 0.14900805056095123, f1 0.7865999937057495
ep24_l0_test_time 0.6201295410000967
Test Epoch24 layer1 Acc 0.7974, AUC 0.8719578981399536, avg_entr 0.054142799228429794, f1 0.7973999977111816
ep24_l1_test_time 0.8193287970000256
Test Epoch24 layer2 Acc 0.7994, AUC 0.8792442679405212, avg_entr 0.04804335534572601, f1 0.7993999123573303
ep24_l2_test_time 1.135733658999925
Test Epoch24 layer3 Acc 0.8006, AUC 0.8816249370574951, avg_entr 0.04630197584629059, f1 0.800599992275238
ep24_l3_test_time 1.6135764239998025
Test Epoch24 layer4 Acc 0.7992, AUC 0.8817315101623535, avg_entr 0.045440495014190674, f1 0.7991999983787537
ep24_l4_test_time 2.2516333739999936
gc 0
Train Epoch25 Acc 0.95115 (38046/40000), AUC 0.9875480532646179
ep25_train_time 65.47097134900014
Test Epoch25 layer0 Acc 0.7824, AUC 0.8620187640190125, avg_entr 0.1484963446855545, f1 0.7824000120162964
ep25_l0_test_time 0.6206084020000162
Test Epoch25 layer1 Acc 0.794, AUC 0.8633244037628174, avg_entr 0.0565992072224617, f1 0.7940000295639038
ep25_l1_test_time 0.8213362340000003
Test Epoch25 layer2 Acc 0.7964, AUC 0.873317539691925, avg_entr 0.0478285513818264, f1 0.7964000105857849
ep25_l2_test_time 1.1344078330000684
Test Epoch25 layer3 Acc 0.7976, AUC 0.8769288063049316, avg_entr 0.04567911475896835, f1 0.7976000308990479
ep25_l3_test_time 1.6169233660000373
Test Epoch25 layer4 Acc 0.7972, AUC 0.8768661618232727, avg_entr 0.04579543694853783, f1 0.7971999645233154
ep25_l4_test_time 2.254490338000096
gc 0
Train Epoch26 Acc 0.95155 (38062/40000), AUC 0.9875310659408569
ep26_train_time 65.49724165299995
Test Epoch26 layer0 Acc 0.7864, AUC 0.862851083278656, avg_entr 0.14612077176570892, f1 0.7864000201225281
ep26_l0_test_time 0.6228950559998339
Test Epoch26 layer1 Acc 0.7964, AUC 0.8661260604858398, avg_entr 0.054124388843774796, f1 0.7964000105857849
ep26_l1_test_time 0.8137037300000429
Test Epoch26 layer2 Acc 0.7962, AUC 0.8773167133331299, avg_entr 0.047800689935684204, f1 0.7961999177932739
ep26_l2_test_time 1.132470375999901
Test Epoch26 layer3 Acc 0.7982, AUC 0.8801379799842834, avg_entr 0.0482913963496685, f1 0.7982000708580017
ep26_l3_test_time 1.612919336999994
Test Epoch26 layer4 Acc 0.7978, AUC 0.880240797996521, avg_entr 0.04888506978750229, f1 0.7978000044822693
ep26_l4_test_time 2.251160716000186
gc 0
Train Epoch27 Acc 0.953925 (38157/40000), AUC 0.989664614200592
ep27_train_time 65.49298705299998
Test Epoch27 layer0 Acc 0.7852, AUC 0.8614227771759033, avg_entr 0.14739307761192322, f1 0.7851999998092651
ep27_l0_test_time 0.6156221999999616
Test Epoch27 layer1 Acc 0.7952, AUC 0.8638144731521606, avg_entr 0.05365173891186714, f1 0.7952000498771667
ep27_l1_test_time 0.8131703050000851
Test Epoch27 layer2 Acc 0.7972, AUC 0.874231219291687, avg_entr 0.045649297535419464, f1 0.7971999645233154
ep27_l2_test_time 1.131172020999884
Test Epoch27 layer3 Acc 0.798, AUC 0.878260612487793, avg_entr 0.04367560148239136, f1 0.7979999780654907
ep27_l3_test_time 1.6150156490000427
Test Epoch27 layer4 Acc 0.7972, AUC 0.8783577680587769, avg_entr 0.043757181614637375, f1 0.7971999645233154
ep27_l4_test_time 2.249635835999925
gc 0
Train Epoch28 Acc 0.9546 (38184/40000), AUC 0.9904413223266602
ep28_train_time 65.64714784999978
Test Epoch28 layer0 Acc 0.786, AUC 0.8617898225784302, avg_entr 0.143569678068161, f1 0.7860000133514404
ep28_l0_test_time 0.6184505500000341
Test Epoch28 layer1 Acc 0.7956, AUC 0.8659236431121826, avg_entr 0.049611467868089676, f1 0.7955999970436096
ep28_l1_test_time 0.8141257219999716
Test Epoch28 layer2 Acc 0.8, AUC 0.8761297464370728, avg_entr 0.04451201856136322, f1 0.8000000715255737
ep28_l2_test_time 1.135641437999766
Test Epoch28 layer3 Acc 0.7988, AUC 0.8801005482673645, avg_entr 0.0425865612924099, f1 0.798799991607666
ep28_l3_test_time 1.6179946460001702
Test Epoch28 layer4 Acc 0.7982, AUC 0.8800724148750305, avg_entr 0.04249156638979912, f1 0.7982000708580017
ep28_l4_test_time 2.2533350450003127
gc 0
Train Epoch29 Acc 0.957075 (38283/40000), AUC 0.9906097650527954
ep29_train_time 65.47026358799985
Test Epoch29 layer0 Acc 0.7836, AUC 0.8602226376533508, avg_entr 0.14303776621818542, f1 0.7835999727249146
ep29_l0_test_time 0.6171770910000305
Test Epoch29 layer1 Acc 0.7912, AUC 0.8576934337615967, avg_entr 0.050213947892189026, f1 0.7911999821662903
ep29_l1_test_time 0.8142217109998455
Test Epoch29 layer2 Acc 0.7918, AUC 0.8702599406242371, avg_entr 0.041686683893203735, f1 0.7918000221252441
ep29_l2_test_time 1.1321302209998976
Test Epoch29 layer3 Acc 0.7946, AUC 0.8761862516403198, avg_entr 0.039772193878889084, f1 0.7946000695228577
ep29_l3_test_time 1.617262503999882
Test Epoch29 layer4 Acc 0.7954, AUC 0.8756425380706787, avg_entr 0.039758000522851944, f1 0.795400083065033
ep29_l4_test_time 2.2497568859998864
gc 0
Train Epoch30 Acc 0.95815 (38326/40000), AUC 0.9907920360565186
ep30_train_time 65.49146586000006
Test Epoch30 layer0 Acc 0.7848, AUC 0.8602147102355957, avg_entr 0.14256435632705688, f1 0.7847999930381775
ep30_l0_test_time 0.6213188739998259
Test Epoch30 layer1 Acc 0.7906, AUC 0.8597011566162109, avg_entr 0.04948903247714043, f1 0.7906000018119812
ep30_l1_test_time 0.8113331489998927
Test Epoch30 layer2 Acc 0.7936, AUC 0.8711076974868774, avg_entr 0.042341448366642, f1 0.7936000227928162
ep30_l2_test_time 1.1336132430001271
Test Epoch30 layer3 Acc 0.7954, AUC 0.8754937648773193, avg_entr 0.04086890444159508, f1 0.795400083065033
ep30_l3_test_time 1.6160925669996686
Test Epoch30 layer4 Acc 0.7962, AUC 0.8754127025604248, avg_entr 0.04063154384493828, f1 0.7961999177932739
ep30_l4_test_time 2.250165133000337
gc 0
Train Epoch31 Acc 0.95925 (38370/40000), AUC 0.9914151430130005
ep31_train_time 65.48404084899994
Test Epoch31 layer0 Acc 0.7884, AUC 0.8593644499778748, avg_entr 0.14211176335811615, f1 0.7884000539779663
ep31_l0_test_time 0.6472504420003133
Test Epoch31 layer1 Acc 0.79, AUC 0.8587822318077087, avg_entr 0.04756437987089157, f1 0.7900000214576721
ep31_l1_test_time 0.8123150690003058
Test Epoch31 layer2 Acc 0.795, AUC 0.8691861629486084, avg_entr 0.041431233286857605, f1 0.7950000166893005
ep31_l2_test_time 1.1304859880001459
Test Epoch31 layer3 Acc 0.7966, AUC 0.8741539120674133, avg_entr 0.03903643414378166, f1 0.7965999841690063
ep31_l3_test_time 1.6151863400000366
Test Epoch31 layer4 Acc 0.7962, AUC 0.8743853569030762, avg_entr 0.038637880235910416, f1 0.7961999177932739
ep31_l4_test_time 2.247426983999958
gc 0
Train Epoch32 Acc 0.959625 (38385/40000), AUC 0.9914898872375488
ep32_train_time 65.57068410300008
Test Epoch32 layer0 Acc 0.7864, AUC 0.8587982654571533, avg_entr 0.14170300960540771, f1 0.7864000201225281
ep32_l0_test_time 0.6167996519998269
Test Epoch32 layer1 Acc 0.7914, AUC 0.858320951461792, avg_entr 0.04662013426423073, f1 0.7914000153541565
ep32_l1_test_time 0.8096435749998818
Test Epoch32 layer2 Acc 0.7932, AUC 0.8688194155693054, avg_entr 0.04086978733539581, f1 0.7932000160217285
ep32_l2_test_time 1.1340647780002655
Test Epoch32 layer3 Acc 0.795, AUC 0.8746387362480164, avg_entr 0.03888604789972305, f1 0.7950000166893005
ep32_l3_test_time 1.616063521000342
Test Epoch32 layer4 Acc 0.7946, AUC 0.8747670650482178, avg_entr 0.03868386149406433, f1 0.7946000695228577
ep32_l4_test_time 2.248859204999917
gc 0
Train Epoch33 Acc 0.959125 (38365/40000), AUC 0.9917309284210205
ep33_train_time 65.65506099000004
Test Epoch33 layer0 Acc 0.7836, AUC 0.8589097857475281, avg_entr 0.14100387692451477, f1 0.7835999727249146
ep33_l0_test_time 0.6174762569999075
Test Epoch33 layer1 Acc 0.791, AUC 0.85687255859375, avg_entr 0.04784218594431877, f1 0.7910000681877136
ep33_l1_test_time 0.8091161829997873
Test Epoch33 layer2 Acc 0.7944, AUC 0.867483913898468, avg_entr 0.04143290966749191, f1 0.7943999767303467
ep33_l2_test_time 1.1456995630001074
Test Epoch33 layer3 Acc 0.7966, AUC 0.8743218183517456, avg_entr 0.0397053137421608, f1 0.7965999841690063
ep33_l3_test_time 1.6226635319999332
Test Epoch33 layer4 Acc 0.7968, AUC 0.8741474151611328, avg_entr 0.03985963389277458, f1 0.7968000173568726
ep33_l4_test_time 2.259989527000016
gc 0
Train Epoch34 Acc 0.9613 (38452/40000), AUC 0.9918969869613647
ep34_train_time 65.51114391500005
Test Epoch34 layer0 Acc 0.7864, AUC 0.8587985038757324, avg_entr 0.14162901043891907, f1 0.7864000201225281
ep34_l0_test_time 0.6165886870003305
Test Epoch34 layer1 Acc 0.7902, AUC 0.8588908314704895, avg_entr 0.04614346846938133, f1 0.7901999950408936
ep34_l1_test_time 0.8129185769998912
Test Epoch34 layer2 Acc 0.7946, AUC 0.8692001700401306, avg_entr 0.040507830679416656, f1 0.7946000695228577
ep34_l2_test_time 1.1403015790001518
Test Epoch34 layer3 Acc 0.7958, AUC 0.8746306896209717, avg_entr 0.038746364414691925, f1 0.7958000302314758
ep34_l3_test_time 1.6209835470003782
Test Epoch34 layer4 Acc 0.7968, AUC 0.8750182390213013, avg_entr 0.03819289430975914, f1 0.7968000173568726
ep34_l4_test_time 2.251213952999933
gc 0
Train Epoch35 Acc 0.9604 (38416/40000), AUC 0.9923721551895142
ep35_train_time 65.50519814099971
Test Epoch35 layer0 Acc 0.7836, AUC 0.8582034111022949, avg_entr 0.14253708720207214, f1 0.7835999727249146
ep35_l0_test_time 0.6164936670002135
Test Epoch35 layer1 Acc 0.7884, AUC 0.8557859659194946, avg_entr 0.04752150550484657, f1 0.7884000539779663
ep35_l1_test_time 0.8099915329999021
Test Epoch35 layer2 Acc 0.7924, AUC 0.8668171167373657, avg_entr 0.04108657315373421, f1 0.7923999428749084
ep35_l2_test_time 1.144594331999997
Test Epoch35 layer3 Acc 0.7938, AUC 0.873341977596283, avg_entr 0.039620354771614075, f1 0.7937999963760376
ep35_l3_test_time 1.6153765789999852
Test Epoch35 layer4 Acc 0.7938, AUC 0.8733594417572021, avg_entr 0.039349060505628586, f1 0.7937999963760376
ep35_l4_test_time 2.251526389999981
gc 0
Train Epoch36 Acc 0.96255 (38502/40000), AUC 0.992691159248352
ep36_train_time 65.68214137199993
Test Epoch36 layer0 Acc 0.7846, AUC 0.8585680723190308, avg_entr 0.14093786478042603, f1 0.784600019454956
ep36_l0_test_time 0.6183974300001864
Test Epoch36 layer1 Acc 0.7894, AUC 0.8569640517234802, avg_entr 0.04560984671115875, f1 0.7893999814987183
ep36_l1_test_time 0.8145576599999913
Test Epoch36 layer2 Acc 0.7928, AUC 0.8677456378936768, avg_entr 0.03893554210662842, f1 0.7928000092506409
ep36_l2_test_time 1.1331923139996434
Test Epoch36 layer3 Acc 0.7936, AUC 0.873853325843811, avg_entr 0.03666609898209572, f1 0.7936000227928162
ep36_l3_test_time 1.6251631620002627
Test Epoch36 layer4 Acc 0.7944, AUC 0.8740543127059937, avg_entr 0.0365091897547245, f1 0.7943999767303467
ep36_l4_test_time 2.2531122619998314
gc 0
Train Epoch37 Acc 0.962275 (38491/40000), AUC 0.9927150011062622
ep37_train_time 65.51034592299993
Test Epoch37 layer0 Acc 0.7814, AUC 0.8580531477928162, avg_entr 0.14114682376384735, f1 0.7814000248908997
ep37_l0_test_time 0.6183626010001717
Test Epoch37 layer1 Acc 0.7888, AUC 0.8556866645812988, avg_entr 0.04690077528357506, f1 0.7888000011444092
ep37_l1_test_time 0.8109861459997774
Test Epoch37 layer2 Acc 0.7944, AUC 0.8669468760490417, avg_entr 0.04055180773139, f1 0.7943999767303467
ep37_l2_test_time 1.1344226480000543
Test Epoch37 layer3 Acc 0.7954, AUC 0.8735637664794922, avg_entr 0.039383675903081894, f1 0.795400083065033
ep37_l3_test_time 1.6165492720001566
Test Epoch37 layer4 Acc 0.7948, AUC 0.8737088441848755, avg_entr 0.03919735550880432, f1 0.7947999835014343
ep37_l4_test_time 2.250799527000254
gc 0
Train Epoch38 Acc 0.96225 (38490/40000), AUC 0.992655873298645
ep38_train_time 65.56703888799984
Test Epoch38 layer0 Acc 0.785, AUC 0.8585267066955566, avg_entr 0.13972768187522888, f1 0.7850000262260437
ep38_l0_test_time 0.6194155359999058
Test Epoch38 layer1 Acc 0.7908, AUC 0.8562940359115601, avg_entr 0.045479968190193176, f1 0.7907999753952026
ep38_l1_test_time 0.8130093429999761
Test Epoch38 layer2 Acc 0.794, AUC 0.8669432997703552, avg_entr 0.03890817239880562, f1 0.7940000295639038
ep38_l2_test_time 1.135111993999999
Test Epoch38 layer3 Acc 0.7952, AUC 0.8735635280609131, avg_entr 0.03671792149543762, f1 0.7952000498771667
ep38_l3_test_time 1.615829316000145
Test Epoch38 layer4 Acc 0.7948, AUC 0.8739370107650757, avg_entr 0.03661280870437622, f1 0.7947999835014343
ep38_l4_test_time 2.2589565839998613
gc 0
Train Epoch39 Acc 0.96305 (38522/40000), AUC 0.9928592443466187
ep39_train_time 65.6317827959997
Test Epoch39 layer0 Acc 0.7836, AUC 0.8583770394325256, avg_entr 0.14048977196216583, f1 0.7835999727249146
ep39_l0_test_time 0.617280987000413
Test Epoch39 layer1 Acc 0.7898, AUC 0.8556109070777893, avg_entr 0.04599704593420029, f1 0.7897999882698059
ep39_l1_test_time 0.8121468089998416
Test Epoch39 layer2 Acc 0.7944, AUC 0.8669159412384033, avg_entr 0.03986367583274841, f1 0.7943999767303467
ep39_l2_test_time 1.1334133479999764
Test Epoch39 layer3 Acc 0.7948, AUC 0.8734321594238281, avg_entr 0.03776361420750618, f1 0.7947999835014343
ep39_l3_test_time 1.6162156279997362
Test Epoch39 layer4 Acc 0.7952, AUC 0.8737410306930542, avg_entr 0.03759616240859032, f1 0.7952000498771667
ep39_l4_test_time 2.24993359400014
gc 0
Train Epoch40 Acc 0.9631 (38524/40000), AUC 0.9931333065032959
ep40_train_time 65.47365507099994
Test Epoch40 layer0 Acc 0.7866, AUC 0.858489453792572, avg_entr 0.1401200294494629, f1 0.7865999937057495
ep40_l0_test_time 0.6200353059998633
Test Epoch40 layer1 Acc 0.7904, AUC 0.8565533757209778, avg_entr 0.04542331397533417, f1 0.7904000282287598
ep40_l1_test_time 0.811000624999906
Test Epoch40 layer2 Acc 0.7944, AUC 0.8671854734420776, avg_entr 0.039419256150722504, f1 0.7943999767303467
ep40_l2_test_time 1.1340910149997399
Test Epoch40 layer3 Acc 0.7944, AUC 0.8737634420394897, avg_entr 0.03714723512530327, f1 0.7943999767303467
ep40_l3_test_time 1.6126316109998697
Test Epoch40 layer4 Acc 0.7936, AUC 0.8740752339363098, avg_entr 0.03700229153037071, f1 0.7936000227928162
ep40_l4_test_time 2.2494201029999203
gc 0
Train Epoch41 Acc 0.963475 (38539/40000), AUC 0.992912232875824
ep41_train_time 65.48062667099975
Test Epoch41 layer0 Acc 0.7856, AUC 0.8582726120948792, avg_entr 0.13972631096839905, f1 0.7856000065803528
ep41_l0_test_time 0.6168836609999744
Test Epoch41 layer1 Acc 0.7898, AUC 0.8555207848548889, avg_entr 0.04545261338353157, f1 0.7897999882698059
ep41_l1_test_time 0.8130664740001521
Test Epoch41 layer2 Acc 0.7944, AUC 0.8664051294326782, avg_entr 0.03911617025732994, f1 0.7943999767303467
ep41_l2_test_time 1.1322368480000478
Test Epoch41 layer3 Acc 0.795, AUC 0.8732075691223145, avg_entr 0.03694769740104675, f1 0.7950000166893005
ep41_l3_test_time 1.6164224340000146
Test Epoch41 layer4 Acc 0.7954, AUC 0.8734392523765564, avg_entr 0.03671625256538391, f1 0.795400083065033
ep41_l4_test_time 2.248580870999831
gc 0
Train Epoch42 Acc 0.963375 (38535/40000), AUC 0.9927964210510254
ep42_train_time 65.61169491300006
Test Epoch42 layer0 Acc 0.7836, AUC 0.8583616018295288, avg_entr 0.13940568268299103, f1 0.7835999727249146
ep42_l0_test_time 0.6173120549997293
Test Epoch42 layer1 Acc 0.7892, AUC 0.8551698923110962, avg_entr 0.04607943817973137, f1 0.7892000079154968
ep42_l1_test_time 0.8104167450001114
Test Epoch42 layer2 Acc 0.7936, AUC 0.8659706115722656, avg_entr 0.0395551435649395, f1 0.7936000227928162
ep42_l2_test_time 1.1347406029999547
Test Epoch42 layer3 Acc 0.7954, AUC 0.8730565905570984, avg_entr 0.03802761062979698, f1 0.795400083065033
ep42_l3_test_time 1.6153081929996915
Test Epoch42 layer4 Acc 0.7944, AUC 0.8732564449310303, avg_entr 0.037978265434503555, f1 0.7943999767303467
ep42_l4_test_time 2.2511946150002586
gc 0
Train Epoch43 Acc 0.963175 (38527/40000), AUC 0.9931689500808716
ep43_train_time 65.48072307300026
Test Epoch43 layer0 Acc 0.7834, AUC 0.8583459854125977, avg_entr 0.13918036222457886, f1 0.7833999395370483
ep43_l0_test_time 0.6260722480001277
Test Epoch43 layer1 Acc 0.7904, AUC 0.8560549020767212, avg_entr 0.0451546348631382, f1 0.7904000282287598
ep43_l1_test_time 0.8157038080003076
Test Epoch43 layer2 Acc 0.7946, AUC 0.8665602207183838, avg_entr 0.039198823273181915, f1 0.7946000695228577
ep43_l2_test_time 1.1401145529998757
Test Epoch43 layer3 Acc 0.7946, AUC 0.8731841444969177, avg_entr 0.03700954467058182, f1 0.7946000695228577
ep43_l3_test_time 1.6310268259999248
Test Epoch43 layer4 Acc 0.7936, AUC 0.8735399842262268, avg_entr 0.03682785481214523, f1 0.7936000227928162
ep43_l4_test_time 2.2504605610001818
gc 0
Train Epoch44 Acc 0.963875 (38555/40000), AUC 0.9932570457458496
ep44_train_time 65.49600373500016
Test Epoch44 layer0 Acc 0.7842, AUC 0.8582515120506287, avg_entr 0.1388736516237259, f1 0.7842000126838684
ep44_l0_test_time 0.617833754000003
Test Epoch44 layer1 Acc 0.7896, AUC 0.8558857440948486, avg_entr 0.04534706845879555, f1 0.7896000146865845
ep44_l1_test_time 0.8118148399998972
Test Epoch44 layer2 Acc 0.7946, AUC 0.8665379285812378, avg_entr 0.03936510160565376, f1 0.7946000695228577
ep44_l2_test_time 1.1346082580002985
Test Epoch44 layer3 Acc 0.7948, AUC 0.8729531764984131, avg_entr 0.037081554532051086, f1 0.7947999835014343
ep44_l3_test_time 1.616986777999955
Test Epoch44 layer4 Acc 0.7942, AUC 0.8733634352684021, avg_entr 0.0368775799870491, f1 0.7942000031471252
ep44_l4_test_time 2.2500673099998494
gc 0
Train Epoch45 Acc 0.9633 (38532/40000), AUC 0.9929637908935547
ep45_train_time 65.47878977499977
Test Epoch45 layer0 Acc 0.7836, AUC 0.8581265211105347, avg_entr 0.13905060291290283, f1 0.7835999727249146
ep45_l0_test_time 0.6157081659998767
Test Epoch45 layer1 Acc 0.7896, AUC 0.8555846214294434, avg_entr 0.04510924592614174, f1 0.7896000146865845
ep45_l1_test_time 0.8116100760003064
Test Epoch45 layer2 Acc 0.7956, AUC 0.8658616542816162, avg_entr 0.03866422548890114, f1 0.7955999970436096
ep45_l2_test_time 1.1299163860003318
Test Epoch45 layer3 Acc 0.7946, AUC 0.8726291060447693, avg_entr 0.03600864112377167, f1 0.7946000695228577
ep45_l3_test_time 1.6146260569998958
Test Epoch45 layer4 Acc 0.7948, AUC 0.8729974031448364, avg_entr 0.03586467355489731, f1 0.7947999835014343
ep45_l4_test_time 2.2483964689999993
gc 0
Train Epoch46 Acc 0.9648 (38592/40000), AUC 0.9932738542556763
ep46_train_time 65.48328169699971
Test Epoch46 layer0 Acc 0.7848, AUC 0.8579673767089844, avg_entr 0.13905486464500427, f1 0.7847999930381775
ep46_l0_test_time 0.6156191410000247
Test Epoch46 layer1 Acc 0.7892, AUC 0.8553919792175293, avg_entr 0.04519762471318245, f1 0.7892000079154968
ep46_l1_test_time 0.8092035300001044
Test Epoch46 layer2 Acc 0.7948, AUC 0.8657779097557068, avg_entr 0.03876674920320511, f1 0.7947999835014343
ep46_l2_test_time 1.1326557359998333
Test Epoch46 layer3 Acc 0.795, AUC 0.8726404309272766, avg_entr 0.036398448050022125, f1 0.7950000166893005
ep46_l3_test_time 1.613882029999786
Test Epoch46 layer4 Acc 0.7952, AUC 0.8730238676071167, avg_entr 0.03619260713458061, f1 0.7952000498771667
ep46_l4_test_time 2.2484714809997968
gc 0
Train Epoch47 Acc 0.96315 (38526/40000), AUC 0.9930086731910706
ep47_train_time 65.49522602900015
Test Epoch47 layer0 Acc 0.785, AUC 0.8579714894294739, avg_entr 0.1389840543270111, f1 0.7850000262260437
ep47_l0_test_time 0.6173585599999569
Test Epoch47 layer1 Acc 0.7894, AUC 0.8552455902099609, avg_entr 0.04508155956864357, f1 0.7893999814987183
ep47_l1_test_time 0.8101441019998674
Test Epoch47 layer2 Acc 0.7952, AUC 0.8656808137893677, avg_entr 0.03861301392316818, f1 0.7952000498771667
ep47_l2_test_time 1.1345569060003982
Test Epoch47 layer3 Acc 0.7954, AUC 0.8726066946983337, avg_entr 0.03621979057788849, f1 0.795400083065033
ep47_l3_test_time 1.6139056370002436
Test Epoch47 layer4 Acc 0.7944, AUC 0.8729592561721802, avg_entr 0.036011435091495514, f1 0.7943999767303467
ep47_l4_test_time 2.249612624999827
gc 0
Train Epoch48 Acc 0.964075 (38563/40000), AUC 0.993381679058075
ep48_train_time 65.51234524100028
Test Epoch48 layer0 Acc 0.7832, AUC 0.8579643964767456, avg_entr 0.13905715942382812, f1 0.7832000851631165
ep48_l0_test_time 0.6168788350000796
Test Epoch48 layer1 Acc 0.789, AUC 0.8555188775062561, avg_entr 0.04486376792192459, f1 0.7889999747276306
ep48_l1_test_time 0.8197412560002704
Test Epoch48 layer2 Acc 0.7938, AUC 0.8660120964050293, avg_entr 0.0385361984372139, f1 0.7937999963760376
ep48_l2_test_time 1.1325669749999179
Test Epoch48 layer3 Acc 0.794, AUC 0.8728018999099731, avg_entr 0.03611500933766365, f1 0.7940000295639038
ep48_l3_test_time 1.623074799999813
Test Epoch48 layer4 Acc 0.7944, AUC 0.8731992244720459, avg_entr 0.0359354093670845, f1 0.7943999767303467
ep48_l4_test_time 2.252725091999764
gc 0
Train Epoch49 Acc 0.963025 (38521/40000), AUC 0.9931817650794983
ep49_train_time 65.47642982900015
Test Epoch49 layer0 Acc 0.7844, AUC 0.8578931093215942, avg_entr 0.1391356885433197, f1 0.7843999266624451
ep49_l0_test_time 0.619887184000163
Test Epoch49 layer1 Acc 0.789, AUC 0.8551449179649353, avg_entr 0.04514262452721596, f1 0.7889999747276306
ep49_l1_test_time 0.8111672609998095
Test Epoch49 layer2 Acc 0.7948, AUC 0.8656346797943115, avg_entr 0.03876437991857529, f1 0.7947999835014343
ep49_l2_test_time 1.1347052379996967
Test Epoch49 layer3 Acc 0.7942, AUC 0.8725664019584656, avg_entr 0.036488454788923264, f1 0.7942000031471252
ep49_l3_test_time 1.6145104699999138
Test Epoch49 layer4 Acc 0.7946, AUC 0.8729281425476074, avg_entr 0.03636461868882179, f1 0.7946000695228577
ep49_l4_test_time 2.249997336999968
Best AUC tensor(0.8176) 14 4
train_as_loss [[8.40969575e+01 5.77537042e+01 5.17770023e+01 5.03548494e+01
  4.98400174e+01 4.95999136e+01 4.94692333e+01 4.93904702e+01
  4.93394243e+01 4.93044956e+01 4.92795785e+01 4.92612050e+01
  4.92472802e+01 4.92364946e+01 4.92279811e+01 4.92211520e+01
  4.92155995e+01 4.92120492e+01 4.92099025e+01 4.92078747e+01
  4.92059657e+01 4.92046003e+01 4.92036996e+01 4.92027985e+01
  4.92018982e+01 4.92012204e+01 4.92007568e+01 4.92002774e+01
  4.91997845e+01 4.91994043e+01 4.91991390e+01 4.91988595e+01
  4.91985666e+01 4.91983380e+01 4.91981749e+01 4.91980032e+01
  4.91978218e+01 4.91976769e+01 4.91975741e+01 4.91974645e+01
  4.91973476e+01 4.91972575e+01 4.91971881e+01 4.91971188e+01
  4.91970404e+01 4.91969823e+01 4.91969387e+01 4.91968900e+01
  4.91968403e+01 4.91968034e+01]
 [2.10006850e+00 2.12121614e-04 1.40284341e-05 3.93683497e-06
  1.60957009e-06 7.99358830e-07 4.37280146e-07 2.55863970e-07
  1.61532216e-07 1.03011970e-07 6.75191035e-08 4.68257968e-08
  3.64809883e-07 1.11329410e-06 2.74354943e-07 2.81620099e-06
  1.03597223e-07 1.15916298e-07 6.57637948e-09 5.70593661e-09
  5.06991812e-09 4.30321230e-09 3.94841221e-09 3.64042212e-09
  3.46340003e-09 3.14388510e-09 2.94459686e-09 2.79320803e-09
  2.62981632e-09 2.54090137e-09 2.43184005e-09 2.36700078e-09
  2.28781560e-09 2.17411958e-09 2.15228638e-09 2.08172173e-09
  2.01279247e-09 1.99138002e-09 1.93885625e-09 1.88118532e-09
  1.88144771e-09 1.86788330e-09 1.83274490e-09 1.76597729e-09
  1.74826295e-09 1.79158670e-09 1.79063843e-09 1.71440248e-09
  1.61940771e-09 1.86242068e-09]
 [2.22158814e+00 1.98880164e-04 1.13613265e-05 3.29104675e-06
  1.32249900e-06 6.66080834e-07 3.74663985e-07 2.13671107e-07
  1.40692178e-07 9.06713751e-08 6.00494677e-08 3.97794184e-08
  5.77528839e-07 1.81077285e-06 4.45694383e-07 4.52018493e-06
  1.67685518e-07 1.00201621e-06 1.83339304e-09 5.41953107e-09
  4.49367165e-09 1.22239205e-09 9.62844888e-10 3.08529953e-09
  2.82285781e-09 8.26078273e-10 6.13553651e-10 2.25287319e-09
  2.03075773e-09 5.60374035e-10 4.74816723e-10 1.88718585e-09
  1.69980859e-09 4.36190917e-10 4.15574473e-10 1.59350611e-09
  1.48325581e-09 3.75632043e-10 3.62195134e-10 1.41615444e-09
  1.38672755e-09 3.54470685e-10 3.38765614e-10 1.29270424e-09
  1.25754786e-09 3.48618482e-10 3.19453712e-10 1.19045639e-09
  1.16186735e-09 3.71144743e-10]
 [2.21074447e+00 4.70482926e-04 1.95645501e-05 5.70963669e-06
  2.21976852e-06 1.11780012e-06 6.48002359e-07 3.55234189e-07
  2.41856191e-07 1.61026370e-07 1.10895571e-07 6.95500953e-08
  6.37482734e-07 1.89298399e-06 4.63987062e-07 4.53096575e-06
  1.89988394e-07 1.83890873e-06 4.54112360e-09 1.02423833e-08
  8.35688153e-09 2.90909359e-09 2.01444255e-09 5.32863891e-09
  4.79478933e-09 1.75649469e-09 1.13667404e-09 3.74707261e-09
  3.37909307e-09 1.05123944e-09 8.04211980e-10 3.06494689e-09
  2.78031731e-09 7.64876331e-10 6.91887812e-10 2.55825493e-09
  2.42868625e-09 6.52959544e-10 5.90961653e-10 2.28303028e-09
  2.23899756e-09 6.17457491e-10 5.52486347e-10 2.04896875e-09
  2.03177040e-09 6.03660802e-10 5.12943624e-10 1.83212795e-09
  1.90681738e-09 6.55457291e-10]
 [2.48809460e+00 1.80354183e-03 1.70740758e-05 5.23742084e-06
  2.02451358e-06 1.05947842e-06 6.69243497e-07 3.71621085e-07
  3.06581682e-07 1.98078005e-07 1.52836665e-07 8.82317934e-08
  5.06495913e-07 1.58009250e-06 4.28274514e-07 3.74800367e-06
  1.80529951e-07 1.71482176e-06 1.26252120e-08 1.55539571e-08
  1.26098818e-08 7.68354855e-09 3.89682034e-09 6.20272180e-09
  5.68430533e-09 3.37236862e-09 1.56957267e-09 3.70462984e-09
  3.34342044e-09 1.40623445e-09 8.66185965e-10 2.93702048e-09
  2.58148133e-09 8.60726866e-10 6.83982359e-10 2.30838354e-09
  2.18981330e-09 6.74696905e-10 5.45338702e-10 2.03602401e-09
  2.04649291e-09 6.44451408e-10 5.27667417e-10 1.86975459e-09
  1.90701520e-09 6.57297039e-10 5.31204235e-10 1.80075102e-09
  1.91332755e-09 7.92125057e-10]]
train_ae_loss [[4.88758182 3.384764   4.29920192 4.46571003 4.74251758 4.86195692
  4.93868712 4.98689412 5.07178159 5.08655541 5.06552972 5.07474732
  4.96687687 4.99105413 4.83761128 4.8041525  4.60439257 4.26051806
  4.12554706 4.02968724 3.89210436 3.65551232 3.57693219 3.51407323
  3.46291153 3.34147202 3.31838838 3.27506639 3.24339188 3.18459197
  3.15736295 3.16526962 3.15428014 3.13539541 3.11394412 3.09940835
  3.10089971 3.07668668 3.07695575 3.09727724 3.10064251 3.06868948
  3.07310096 3.08175297 3.05589411 3.05314085 3.06045855 3.05320306
  3.05723938 3.06363428]
 [4.66585023 3.89681355 4.8881385  4.84883077 5.12169352 5.11822442
  5.04384295 4.96415235 4.99301446 4.9106652  4.76301496 4.76206141
  4.526571   4.56616653 4.31998718 4.25434769 3.92187501 3.39956198
  3.19000437 3.09231502 2.87356995 2.51829882 2.4234906  2.32036052
  2.26803274 2.08047307 2.0472914  1.99375769 1.94660039 1.86323009
  1.81878442 1.81948899 1.80611537 1.76823596 1.75973914 1.72293423
  1.70498585 1.70950207 1.69202548 1.71549471 1.71495277 1.68982131
  1.68520616 1.6972195  1.67424761 1.64893762 1.68407206 1.64772258
  1.64745479 1.66837828]
 [4.60339637 3.2686578  4.28828467 4.09525347 4.30304696 4.27532431
  4.14869627 4.0485996  4.06194039 4.00201286 3.83941614 3.85016992
  3.6291621  3.68825844 3.45998303 3.41623171 3.12017971 2.74438798
  2.55515089 2.42631442 2.24379029 1.97494908 1.90063259 1.7802247
  1.73889635 1.60046946 1.59072485 1.51052594 1.47491846 1.41247443
  1.37816799 1.36280538 1.35031538 1.3248459  1.32723109 1.28286165
  1.26457301 1.27275553 1.25508719 1.27241713 1.26844184 1.25126514
  1.24773584 1.2544299  1.24008224 1.2082159  1.24592834 1.2135178
  1.21577859 1.22983207]
 [5.02220348 2.81132895 3.81914476 3.66041503 3.72342708 3.64660873
  3.47576307 3.35722464 3.3525197  3.30953263 3.13656484 3.15499978
  2.95133257 3.0182946  2.8076658  2.77925992 2.52328364 2.22998363
  2.0710835  1.94299406 1.79500443 1.58143468 1.51851883 1.41132536
  1.37774434 1.26979543 1.26613781 1.19098912 1.16423237 1.11173844
  1.0860717  1.07059044 1.06125288 1.04131976 1.04554444 1.00650231
  0.99144523 0.9964786  0.98365466 0.99639511 0.99467877 0.97848494
  0.97815729 0.97926907 0.97359381 0.94614651 0.9752284  0.94767301
  0.95089864 0.9632563 ]
 [4.95083866 2.41460558 3.37399756 3.54867557 3.38140464 3.24153737
  3.0397488  2.91185342 2.90342774 2.8762618  2.69901814 2.73195551
  2.54005734 2.62335209 2.41867773 2.40011135 2.17555121 1.93545406
  1.8000044  1.66665212 1.5377552  1.35835769 1.30706247 1.20443266
  1.17731653 1.08766084 1.08869304 1.01779056 0.99431077 0.95040824
  0.92839899 0.91384597 0.90541318 0.88919874 0.89334656 0.85858279
  0.8448762  0.84996941 0.8391838  0.84954929 0.84784556 0.83456504
  0.83362116 0.8349942  0.82939452 0.8064917  0.83141522 0.80761252
  0.81026043 0.82097316]]
valid_acc (5, 50)
valid_AUC (5, 50)
train_acc (50,)
total_train+valid_time 3605.3742744419997
Start Testing
Load ckpt at ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt
gc 9
Test layer0 Acc 0.797, AUC 0.8810774087905884, avg_entr 0.18532992899417877, f1 0.796999990940094
l0_test_time 0.6179653469998811
gc 0
Test layer1 Acc 0.8204, AUC 0.9013059139251709, avg_entr 0.10434680432081223, f1 0.820400059223175
l1_test_time 0.8143655490002857
gc 0
Test layer2 Acc 0.8224, AUC 0.9040051698684692, avg_entr 0.09586021304130554, f1 0.8223999738693237
l2_test_time 1.1371368529999017
gc 0
Test layer3 Acc 0.8222, AUC 0.9043945074081421, avg_entr 0.09459380060434341, f1 0.8221999406814575
l3_test_time 1.618895846000214
gc 0
Test layer4 Acc 0.8234, AUC 0.9044231176376343, avg_entr 0.09253351390361786, f1 0.8234000205993652
l4_test_time 2.2504476089998207
gc 0
Test threshold 0.1 Acc 0.8234, AUC 0.8968169689178467, avg_entr 0.13492532074451447, f1 0.8234000205993652
t0.1_test_time 1.1298186899998655
gc 0
Test threshold 0.2 Acc 0.8228, AUC 0.893160343170166, avg_entr 0.14341239631175995, f1 0.8227999806404114
t0.2_test_time 1.0203186950002419
gc 0
Test threshold 0.3 Acc 0.8218, AUC 0.8930823802947998, avg_entr 0.1528209000825882, f1 0.8218000531196594
t0.3_test_time 0.9515021709999019
gc 0
Test threshold 0.4 Acc 0.8214, AUC 0.8917906284332275, avg_entr 0.16214101016521454, f1 0.821399986743927
t0.4_test_time 0.9053605059998517
gc 0
Test threshold 0.5 Acc 0.8196, AUC 0.8899590373039246, avg_entr 0.1728917509317398, f1 0.8195999264717102
t0.5_test_time 0.8601149139999507
gc 0
Test threshold 0.6 Acc 0.8188, AUC 0.8885313868522644, avg_entr 0.18355292081832886, f1 0.8187999725341797
t0.6_test_time 0.8204294180000034
gc 0
Test threshold 0.7 Acc 0.8164, AUC 0.8862347602844238, avg_entr 0.19422024488449097, f1 0.8164000511169434
t0.7_test_time 0.7799113979999674
gc 0
Test threshold 0.8 Acc 0.814, AUC 0.8863701820373535, avg_entr 0.20736415684223175, f1 0.8139999508857727
t0.8_test_time 0.7506903030002832
gc 0
Test threshold 0.9 Acc 0.8102, AUC 0.8847396373748779, avg_entr 0.2253631055355072, f1 0.8101999759674072
t0.9_test_time 0.7089383429997724
