total count words 222751
vocab size 30000
train size 40000, valid size 5000, test size 5000
found 27937 words in glove
model: TransformerALsideText(
  (layers): ModuleList(
    (0): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=2, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=2, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
  (emb_layers): ModuleList(
    (0): Embedding(30000, 300)
    (1): Embedding(30000, 300)
    (2): Embedding(30000, 300)
    (3): Embedding(30000, 300)
    (4): Embedding(30000, 300)
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.multi_headed_attention.linears.0.weight 90000
layers.0.enc.f.multi_headed_attention.linears.0.bias 300
layers.0.enc.f.multi_headed_attention.linears.1.weight 90000
layers.0.enc.f.multi_headed_attention.linears.1.bias 300
layers.0.enc.f.multi_headed_attention.linears.2.weight 90000
layers.0.enc.f.multi_headed_attention.linears.2.bias 300
layers.0.enc.f.multi_headed_attention.linears.3.weight 90000
layers.0.enc.f.multi_headed_attention.linears.3.bias 300
layers.0.enc.f.feed_forward.w_1.weight 90000
layers.0.enc.f.feed_forward.w_1.bias 300
layers.0.enc.f.feed_forward.w_2.weight 90000
layers.0.enc.f.feed_forward.w_2.bias 300
layers.0.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.0.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.0.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.0.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.0.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.0.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.0.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.0.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.0.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.0.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.0.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.0.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.0.enc.f.encoder.norm.a 300
layers.0.enc.f.encoder.norm.b 300
layers.0.ae.g.0.weight 256
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 256
layers.0.ae.h.0.bias 2
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
emb_layers.0.weight 9000000
emb_layers.1.weight 9000000
emb_layers.2.weight 9000000
emb_layers.3.weight 9000000
emb_layers.4.weight 9000000
Total Trainable Params: 50758378
init_time 21.031484093
Start Training
gc 0
Train Epoch0 Acc 0.5045 (20180/40000), AUC 0.5038232803344727
ep0_train_time 65.69818796599999
Test Epoch0 layer0 Acc 0.5254, AUC 0.5959459543228149, avg_entr 0.6891508102416992, f1 0.5253999829292297
ep0_l0_test_time 0.6277654780000006
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 0
Test Epoch0 layer1 Acc 0.5, AUC 0.5386017560958862, avg_entr 0.6854989528656006, f1 0.5
ep0_l1_test_time 0.8136762230000016
Test Epoch0 layer2 Acc 0.5002, AUC 0.5137972831726074, avg_entr 0.6886835694313049, f1 0.5001999735832214
ep0_l2_test_time 1.1296125570000015
Test Epoch0 layer3 Acc 0.4998, AUC 0.5190279483795166, avg_entr 0.6898748278617859, f1 0.4997999966144562
ep0_l3_test_time 1.609749635
Test Epoch0 layer4 Acc 0.5, AUC 0.5026839375495911, avg_entr 0.6933009028434753, f1 0.5
ep0_l4_test_time 2.246974272999992
gc 0
Train Epoch1 Acc 0.51005 (20402/40000), AUC 0.5105472207069397
ep1_train_time 65.589659468
Test Epoch1 layer0 Acc 0.6208, AUC 0.6840682029724121, avg_entr 0.6298021078109741, f1 0.6208000183105469
ep1_l0_test_time 0.6147288309999794
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 1
Test Epoch1 layer1 Acc 0.6144, AUC 0.6885769367218018, avg_entr 0.6802064776420593, f1 0.6144000291824341
ep1_l1_test_time 0.8188673629999812
Test Epoch1 layer2 Acc 0.5612, AUC 0.5989536046981812, avg_entr 0.6910581588745117, f1 0.5612000226974487
ep1_l2_test_time 1.1346304340000017
Test Epoch1 layer3 Acc 0.5134, AUC 0.5769510269165039, avg_entr 0.6906093955039978, f1 0.5134000182151794
ep1_l3_test_time 1.6145492629999865
Test Epoch1 layer4 Acc 0.5, AUC 0.48560935258865356, avg_entr 0.6915569305419922, f1 0.5
ep1_l4_test_time 2.252870207000001
gc 0
Train Epoch2 Acc 0.510375 (20415/40000), AUC 0.5129273533821106
ep2_train_time 65.66001693999999
Test Epoch2 layer0 Acc 0.691, AUC 0.763379693031311, avg_entr 0.5177330374717712, f1 0.6909999847412109
ep2_l0_test_time 0.6229682930000138
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 2
Test Epoch2 layer1 Acc 0.7088, AUC 0.7744387984275818, avg_entr 0.5317064523696899, f1 0.7088000178337097
ep2_l1_test_time 0.8203334220000045
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 2
Test Epoch2 layer2 Acc 0.6056, AUC 0.7693358063697815, avg_entr 0.5908364653587341, f1 0.6055999994277954
ep2_l2_test_time 1.1382884770000032
Test Epoch2 layer3 Acc 0.5, AUC 0.7629648447036743, avg_entr 0.552463710308075, f1 0.5
ep2_l3_test_time 1.6100438720000056
Test Epoch2 layer4 Acc 0.5124, AUC 0.6720548272132874, avg_entr 0.6772578954696655, f1 0.5123999714851379
ep2_l4_test_time 2.248056472000002
gc 0
Train Epoch3 Acc 0.57935 (23174/40000), AUC 0.6109037399291992
ep3_train_time 65.54059945299997
Test Epoch3 layer0 Acc 0.6404, AUC 0.8216427564620972, avg_entr 0.35943642258644104, f1 0.6403999924659729
ep3_l0_test_time 0.6156311930000129
Test Epoch3 layer1 Acc 0.5952, AUC 0.8272800445556641, avg_entr 0.34717291593551636, f1 0.5952000021934509
ep3_l1_test_time 0.8113588090000121
Test Epoch3 layer2 Acc 0.5548, AUC 0.8304275870323181, avg_entr 0.3405038118362427, f1 0.5547999739646912
ep3_l2_test_time 1.130904074
Test Epoch3 layer3 Acc 0.507, AUC 0.8314796090126038, avg_entr 0.2823708951473236, f1 0.5070000290870667
ep3_l3_test_time 1.616735188000007
Test Epoch3 layer4 Acc 0.5, AUC 0.8272261619567871, avg_entr 0.3610711991786957, f1 0.5
ep3_l4_test_time 2.252794421000033
gc 0
Train Epoch4 Acc 0.6671 (26684/40000), AUC 0.7331666946411133
ep4_train_time 65.58332706599998
Test Epoch4 layer0 Acc 0.7592, AUC 0.8418399095535278, avg_entr 0.3670380711555481, f1 0.7591999769210815
ep4_l0_test_time 0.6266225599999871
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 4
Test Epoch4 layer1 Acc 0.7628, AUC 0.846877932548523, avg_entr 0.36819496750831604, f1 0.7627999782562256
ep4_l1_test_time 0.8330023180000126
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 4
Test Epoch4 layer2 Acc 0.7644, AUC 0.8489826321601868, avg_entr 0.3958725929260254, f1 0.7644000053405762
ep4_l2_test_time 1.1446460170000137
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 4
Test Epoch4 layer3 Acc 0.7642, AUC 0.8489509224891663, avg_entr 0.4378548860549927, f1 0.76419997215271
ep4_l3_test_time 1.6243797449999988
Test Epoch4 layer4 Acc 0.7622, AUC 0.8489009141921997, avg_entr 0.4764820337295532, f1 0.7621999979019165
ep4_l4_test_time 2.24560779899997
gc 0
Train Epoch5 Acc 0.710175 (28407/40000), AUC 0.7827621698379517
ep5_train_time 65.490315721
Test Epoch5 layer0 Acc 0.7236, AUC 0.8515436053276062, avg_entr 0.31789901852607727, f1 0.7236000299453735
ep5_l0_test_time 0.6165517590000036
Test Epoch5 layer1 Acc 0.6958, AUC 0.8573787212371826, avg_entr 0.2823003828525543, f1 0.6958000063896179
ep5_l1_test_time 0.8089161290000106
Test Epoch5 layer2 Acc 0.6626, AUC 0.860787034034729, avg_entr 0.266584187746048, f1 0.6625999808311462
ep5_l2_test_time 1.1344147730000032
Test Epoch5 layer3 Acc 0.6096, AUC 0.8615447282791138, avg_entr 0.23449143767356873, f1 0.6096000075340271
ep5_l3_test_time 1.6166037819999701
Test Epoch5 layer4 Acc 0.5614, AUC 0.8616193532943726, avg_entr 0.19033941626548767, f1 0.5613999962806702
ep5_l4_test_time 2.2545543509999675
gc 0
Train Epoch6 Acc 0.76025 (30410/40000), AUC 0.8437833786010742
ep6_train_time 65.577504337
Test Epoch6 layer0 Acc 0.7612, AUC 0.8574352264404297, avg_entr 0.2874871492385864, f1 0.7612000107765198
ep6_l0_test_time 0.6164092239999945
Test Epoch6 layer1 Acc 0.7638, AUC 0.8642525672912598, avg_entr 0.25641295313835144, f1 0.7638000249862671
ep6_l1_test_time 0.8099436219999916
Test Epoch6 layer2 Acc 0.7648, AUC 0.8672526478767395, avg_entr 0.2410343736410141, f1 0.764799952507019
ep6_l2_test_time 1.130330263000019
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 6
Test Epoch6 layer3 Acc 0.763, AUC 0.8675713539123535, avg_entr 0.2462993711233139, f1 0.7630000114440918
ep6_l3_test_time 1.6144049989999303
Test Epoch6 layer4 Acc 0.7634, AUC 0.8676819801330566, avg_entr 0.24635615944862366, f1 0.7634000182151794
ep6_l4_test_time 2.245215667000025
gc 0
Train Epoch7 Acc 0.7778 (31112/40000), AUC 0.8599404096603394
ep7_train_time 65.5712341599999
Test Epoch7 layer0 Acc 0.7738, AUC 0.8631513118743896, avg_entr 0.2949712574481964, f1 0.7738000154495239
ep7_l0_test_time 0.6144083350000074
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 7
Test Epoch7 layer1 Acc 0.778, AUC 0.8701068758964539, avg_entr 0.2629927694797516, f1 0.777999997138977
ep7_l1_test_time 0.8224573549999832
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 7
Test Epoch7 layer2 Acc 0.7848, AUC 0.8730926513671875, avg_entr 0.24097803235054016, f1 0.7847999930381775
ep7_l2_test_time 1.1377080900000465
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 7
Test Epoch7 layer3 Acc 0.7846, AUC 0.8734400272369385, avg_entr 0.24267421662807465, f1 0.784600019454956
ep7_l3_test_time 1.616316136000023
Test Epoch7 layer4 Acc 0.7846, AUC 0.8734899759292603, avg_entr 0.24428115785121918, f1 0.784600019454956
ep7_l4_test_time 2.2481719679999514
gc 0
Train Epoch8 Acc 0.8056 (32224/40000), AUC 0.8866382837295532
ep8_train_time 65.57048665800005
Test Epoch8 layer0 Acc 0.7772, AUC 0.8620648384094238, avg_entr 0.2629680335521698, f1 0.777199923992157
ep8_l0_test_time 0.6145287239999107
Test Epoch8 layer1 Acc 0.7804, AUC 0.8709248304367065, avg_entr 0.23306460678577423, f1 0.7803999781608582
ep8_l1_test_time 0.8162742409999737
Test Epoch8 layer2 Acc 0.7882, AUC 0.8741569519042969, avg_entr 0.2014322578907013, f1 0.7882000207901001
ep8_l2_test_time 1.1373713000000407
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 8
Test Epoch8 layer3 Acc 0.7892, AUC 0.8740485906600952, avg_entr 0.2093753218650818, f1 0.7892000079154968
ep8_l3_test_time 1.6209506580000834
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 8
Test Epoch8 layer4 Acc 0.7894, AUC 0.8742129802703857, avg_entr 0.21517133712768555, f1 0.7893999814987183
ep8_l4_test_time 2.256581202999996
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 8
gc 0
Train Epoch9 Acc 0.812925 (32517/40000), AUC 0.8907996416091919
ep9_train_time 65.46944846299994
Test Epoch9 layer0 Acc 0.7836, AUC 0.8696765303611755, avg_entr 0.23600466549396515, f1 0.7835999727249146
ep9_l0_test_time 0.6280171509999946
Test Epoch9 layer1 Acc 0.7928, AUC 0.8791037201881409, avg_entr 0.192367285490036, f1 0.7928000092506409
ep9_l1_test_time 0.8146852280000303
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 9
Test Epoch9 layer2 Acc 0.7944, AUC 0.8834315538406372, avg_entr 0.16720667481422424, f1 0.7943999767303467
ep9_l2_test_time 1.1387173320000556
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 9
Test Epoch9 layer3 Acc 0.795, AUC 0.8832460641860962, avg_entr 0.17016898095607758, f1 0.7950000166893005
ep9_l3_test_time 1.6187415529999498
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 9
Test Epoch9 layer4 Acc 0.7944, AUC 0.8835068941116333, avg_entr 0.17274576425552368, f1 0.7943999767303467
ep9_l4_test_time 2.247486260999949
gc 0
Train Epoch10 Acc 0.823825 (32953/40000), AUC 0.9011127352714539
ep10_train_time 65.48313508700005
Test Epoch10 layer0 Acc 0.7624, AUC 0.8722226023674011, avg_entr 0.24528047442436218, f1 0.7623999714851379
ep10_l0_test_time 0.6148760020000736
Test Epoch10 layer1 Acc 0.7656, AUC 0.8781239986419678, avg_entr 0.18131741881370544, f1 0.7655999660491943
ep10_l1_test_time 0.8097365080000145
Test Epoch10 layer2 Acc 0.7696, AUC 0.8815930485725403, avg_entr 0.1609352082014084, f1 0.769599974155426
ep10_l2_test_time 1.1313676880000685
Test Epoch10 layer3 Acc 0.7614, AUC 0.8813318014144897, avg_entr 0.16093091666698456, f1 0.7613999843597412
ep10_l3_test_time 1.614464507999969
Test Epoch10 layer4 Acc 0.7578, AUC 0.882032573223114, avg_entr 0.16103185713291168, f1 0.7577999830245972
ep10_l4_test_time 2.2588055920000443
gc 0
Train Epoch11 Acc 0.831775 (33271/40000), AUC 0.9067599177360535
ep11_train_time 65.58649466200006
Test Epoch11 layer0 Acc 0.7808, AUC 0.871902346611023, avg_entr 0.21318501234054565, f1 0.7808000445365906
ep11_l0_test_time 0.6185589469999968
Test Epoch11 layer1 Acc 0.7926, AUC 0.8795114755630493, avg_entr 0.13742361962795258, f1 0.7925999760627747
ep11_l1_test_time 0.8090930700000172
Test Epoch11 layer2 Acc 0.7924, AUC 0.883049726486206, avg_entr 0.1221839115023613, f1 0.7923999428749084
ep11_l2_test_time 1.1326419040000246
Test Epoch11 layer3 Acc 0.7936, AUC 0.8831400871276855, avg_entr 0.11986071616411209, f1 0.7936000227928162
ep11_l3_test_time 1.6142995379999547
Test Epoch11 layer4 Acc 0.792, AUC 0.8833362460136414, avg_entr 0.11747284978628159, f1 0.7920000553131104
ep11_l4_test_time 2.252562117000025
gc 0
Train Epoch12 Acc 0.8503 (34012/40000), AUC 0.922782301902771
ep12_train_time 65.60530891199994
Test Epoch12 layer0 Acc 0.789, AUC 0.8738448619842529, avg_entr 0.22313393652439117, f1 0.7889999747276306
ep12_l0_test_time 0.6154044359999489
Test Epoch12 layer1 Acc 0.8002, AUC 0.8840293884277344, avg_entr 0.14926649630069733, f1 0.8001999855041504
ep12_l1_test_time 0.8102619530000084
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 12
Test Epoch12 layer2 Acc 0.8066, AUC 0.8872309327125549, avg_entr 0.13596846163272858, f1 0.8065999746322632
ep12_l2_test_time 1.1468683550000378
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 12
Test Epoch12 layer3 Acc 0.805, AUC 0.8869909048080444, avg_entr 0.13443808257579803, f1 0.8050000667572021
ep12_l3_test_time 1.6242327209999985
Test Epoch12 layer4 Acc 0.8056, AUC 0.8870447874069214, avg_entr 0.12899042665958405, f1 0.8055999875068665
ep12_l4_test_time 2.254978541000014
gc 0
Train Epoch13 Acc 0.847675 (33907/40000), AUC 0.9161608815193176
ep13_train_time 65.57022157600011
Test Epoch13 layer0 Acc 0.7876, AUC 0.8705664873123169, avg_entr 0.21083061397075653, f1 0.7875999212265015
ep13_l0_test_time 0.6143690989999868
Test Epoch13 layer1 Acc 0.7966, AUC 0.8820055723190308, avg_entr 0.13173724710941315, f1 0.7965999841690063
ep13_l1_test_time 0.8097973760000059
Test Epoch13 layer2 Acc 0.799, AUC 0.8859025239944458, avg_entr 0.11437983065843582, f1 0.7990000247955322
ep13_l2_test_time 1.131332139000051
Test Epoch13 layer3 Acc 0.7972, AUC 0.8859167695045471, avg_entr 0.10992299765348434, f1 0.7971999645233154
ep13_l3_test_time 1.6157857700000022
Test Epoch13 layer4 Acc 0.797, AUC 0.8859594464302063, avg_entr 0.10383841395378113, f1 0.796999990940094
ep13_l4_test_time 2.25107884199997
gc 0
Train Epoch14 Acc 0.87165 (34866/40000), AUC 0.9392409920692444
ep14_train_time 65.54818605599985
Test Epoch14 layer0 Acc 0.7608, AUC 0.8688799142837524, avg_entr 0.18489573895931244, f1 0.7608000040054321
ep14_l0_test_time 0.6155796880000253
Test Epoch14 layer1 Acc 0.7614, AUC 0.8784124851226807, avg_entr 0.1198798194527626, f1 0.7613999843597412
ep14_l1_test_time 0.8075380479999694
Test Epoch14 layer2 Acc 0.7618, AUC 0.8842609524726868, avg_entr 0.10869841277599335, f1 0.7618000507354736
ep14_l2_test_time 1.1331323710001016
Test Epoch14 layer3 Acc 0.7636, AUC 0.8839914202690125, avg_entr 0.11211186647415161, f1 0.7635999917984009
ep14_l3_test_time 1.6155704669999977
Test Epoch14 layer4 Acc 0.7642, AUC 0.884284257888794, avg_entr 0.11602868884801865, f1 0.76419997215271
ep14_l4_test_time 2.249568228999806
gc 0
Train Epoch15 Acc 0.873575 (34943/40000), AUC 0.9412934184074402
ep15_train_time 65.58760758300014
Test Epoch15 layer0 Acc 0.7806, AUC 0.8654213547706604, avg_entr 0.1867322325706482, f1 0.7806000113487244
ep15_l0_test_time 0.6146800340000027
Test Epoch15 layer1 Acc 0.799, AUC 0.8798518776893616, avg_entr 0.11757336556911469, f1 0.7990000247955322
ep15_l1_test_time 0.8116908290000993
Test Epoch15 layer2 Acc 0.8036, AUC 0.8842387199401855, avg_entr 0.10751918703317642, f1 0.803600013256073
ep15_l2_test_time 1.135141305000161
Test Epoch15 layer3 Acc 0.8028, AUC 0.8847447633743286, avg_entr 0.1096917986869812, f1 0.8027999997138977
ep15_l3_test_time 1.616514941000105
Test Epoch15 layer4 Acc 0.8022, AUC 0.8848551511764526, avg_entr 0.11150660365819931, f1 0.8022000193595886
ep15_l4_test_time 2.2502692579998893
gc 0
Train Epoch16 Acc 0.88695 (35478/40000), AUC 0.9508737325668335
ep16_train_time 65.72415254299995
Test Epoch16 layer0 Acc 0.7896, AUC 0.8673447370529175, avg_entr 0.18036925792694092, f1 0.7896000146865845
ep16_l0_test_time 0.6137259930001164
Test Epoch16 layer1 Acc 0.7986, AUC 0.8779650926589966, avg_entr 0.09990747272968292, f1 0.7986000180244446
ep16_l1_test_time 0.8158166390001043
Test Epoch16 layer2 Acc 0.8024, AUC 0.8831084966659546, avg_entr 0.08957963436841965, f1 0.8023999929428101
ep16_l2_test_time 1.1513187650000418
Test Epoch16 layer3 Acc 0.8016, AUC 0.8835124373435974, avg_entr 0.08798203617334366, f1 0.8015999794006348
ep16_l3_test_time 1.6260602699999254
Test Epoch16 layer4 Acc 0.8024, AUC 0.8835545182228088, avg_entr 0.08761316537857056, f1 0.8023999929428101
ep16_l4_test_time 2.256648233000078
gc 0
Train Epoch17 Acc 0.9015 (36060/40000), AUC 0.9620203375816345
ep17_train_time 65.680203076
Test Epoch17 layer0 Acc 0.7818, AUC 0.8644633889198303, avg_entr 0.18024204671382904, f1 0.7817999720573425
ep17_l0_test_time 0.6166159179999795
Test Epoch17 layer1 Acc 0.7978, AUC 0.8752868175506592, avg_entr 0.09022992849349976, f1 0.7978000044822693
ep17_l1_test_time 0.8335526939999909
Test Epoch17 layer2 Acc 0.8048, AUC 0.882798433303833, avg_entr 0.079544797539711, f1 0.8047999739646912
ep17_l2_test_time 1.1353147990000707
Test Epoch17 layer3 Acc 0.804, AUC 0.8829617500305176, avg_entr 0.07642447203397751, f1 0.8040000200271606
ep17_l3_test_time 1.6149180849999993
Test Epoch17 layer4 Acc 0.8042, AUC 0.8829785585403442, avg_entr 0.07415499538183212, f1 0.8041999936103821
ep17_l4_test_time 2.2522792380000283
gc 0
Train Epoch18 Acc 0.90985 (36394/40000), AUC 0.9672600030899048
ep18_train_time 65.62305074599999
Test Epoch18 layer0 Acc 0.7704, AUC 0.861985981464386, avg_entr 0.17690543830394745, f1 0.7703999280929565
ep18_l0_test_time 0.6425121130000662
Test Epoch18 layer1 Acc 0.7724, AUC 0.8722386956214905, avg_entr 0.0847456082701683, f1 0.7724000215530396
ep18_l1_test_time 0.8181909019999694
Test Epoch18 layer2 Acc 0.777, AUC 0.8809629678726196, avg_entr 0.07524589449167252, f1 0.7770000100135803
ep18_l2_test_time 1.1388140369999746
Test Epoch18 layer3 Acc 0.7724, AUC 0.8810772895812988, avg_entr 0.07384222000837326, f1 0.7724000215530396
ep18_l3_test_time 1.6226295510000455
Test Epoch18 layer4 Acc 0.7696, AUC 0.8809842467308044, avg_entr 0.07274334132671356, f1 0.769599974155426
ep18_l4_test_time 2.2568365439999525
gc 0
Train Epoch19 Acc 0.9163 (36652/40000), AUC 0.9706636071205139
ep19_train_time 65.67418925799984
Test Epoch19 layer0 Acc 0.7812, AUC 0.8600659966468811, avg_entr 0.16904835402965546, f1 0.7811999917030334
ep19_l0_test_time 0.6154589939999369
Test Epoch19 layer1 Acc 0.7948, AUC 0.8708566427230835, avg_entr 0.08365365862846375, f1 0.7947999835014343
ep19_l1_test_time 0.8103767729999163
Test Epoch19 layer2 Acc 0.7984, AUC 0.8792800903320312, avg_entr 0.07573971897363663, f1 0.7983999848365784
ep19_l2_test_time 1.134708393999972
Test Epoch19 layer3 Acc 0.7994, AUC 0.8792736530303955, avg_entr 0.07457690685987473, f1 0.7993999123573303
ep19_l3_test_time 1.6152994819999549
Test Epoch19 layer4 Acc 0.7994, AUC 0.879399299621582, avg_entr 0.07273933291435242, f1 0.7993999123573303
ep19_l4_test_time 2.2548659499998394
gc 0
Train Epoch20 Acc 0.91695 (36678/40000), AUC 0.969711422920227
ep20_train_time 65.71635771499996
Test Epoch20 layer0 Acc 0.7804, AUC 0.858149528503418, avg_entr 0.16353631019592285, f1 0.7803999781608582
ep20_l0_test_time 0.6165093849999721
Test Epoch20 layer1 Acc 0.7934, AUC 0.8685908317565918, avg_entr 0.07773862034082413, f1 0.79339998960495
ep20_l1_test_time 0.8084629219999897
Test Epoch20 layer2 Acc 0.7946, AUC 0.8775863647460938, avg_entr 0.06664776802062988, f1 0.7946000695228577
ep20_l2_test_time 1.133052773999907
Test Epoch20 layer3 Acc 0.7952, AUC 0.8776133060455322, avg_entr 0.06489535421133041, f1 0.7952000498771667
ep20_l3_test_time 1.615265964000173
Test Epoch20 layer4 Acc 0.7962, AUC 0.8776645660400391, avg_entr 0.06248436123132706, f1 0.7961999177932739
ep20_l4_test_time 2.2518308969999907
gc 0
Train Epoch21 Acc 0.928925 (37157/40000), AUC 0.977252185344696
ep21_train_time 65.60876612999982
Test Epoch21 layer0 Acc 0.7782, AUC 0.8555065393447876, avg_entr 0.16480903327465057, f1 0.7781999707221985
ep21_l0_test_time 0.615807951999841
Test Epoch21 layer1 Acc 0.7898, AUC 0.863390326499939, avg_entr 0.07178176194429398, f1 0.7897999882698059
ep21_l1_test_time 0.8148580990000482
Test Epoch21 layer2 Acc 0.796, AUC 0.8740068674087524, avg_entr 0.0654093325138092, f1 0.796000063419342
ep21_l2_test_time 1.1364877389999037
Test Epoch21 layer3 Acc 0.794, AUC 0.874083399772644, avg_entr 0.06325100362300873, f1 0.7940000295639038
ep21_l3_test_time 1.621258639999951
Test Epoch21 layer4 Acc 0.7946, AUC 0.874148964881897, avg_entr 0.06118902564048767, f1 0.7946000695228577
ep21_l4_test_time 2.2545387370000753
gc 0
Train Epoch22 Acc 0.933925 (37357/40000), AUC 0.9804428815841675
ep22_train_time 65.60208525300004
Test Epoch22 layer0 Acc 0.7776, AUC 0.855193018913269, avg_entr 0.16551755368709564, f1 0.7775999903678894
ep22_l0_test_time 0.6267631789999086
Test Epoch22 layer1 Acc 0.7928, AUC 0.8643772602081299, avg_entr 0.07194697111845016, f1 0.7928000092506409
ep22_l1_test_time 0.8076195749999897
Test Epoch22 layer2 Acc 0.7966, AUC 0.8756881952285767, avg_entr 0.06157667189836502, f1 0.7965999841690063
ep22_l2_test_time 1.1321201050000127
Test Epoch22 layer3 Acc 0.7966, AUC 0.8753671646118164, avg_entr 0.05925426632165909, f1 0.7965999841690063
ep22_l3_test_time 1.6146111680000104
Test Epoch22 layer4 Acc 0.7968, AUC 0.8754844665527344, avg_entr 0.058053284883499146, f1 0.7968000173568726
ep22_l4_test_time 2.2515363489999345
gc 0
Train Epoch23 Acc 0.93635 (37454/40000), AUC 0.9809832572937012
ep23_train_time 65.56924921599989
Test Epoch23 layer0 Acc 0.7712, AUC 0.8538321256637573, avg_entr 0.1640194207429886, f1 0.7712000012397766
ep23_l0_test_time 0.6153860340000392
Test Epoch23 layer1 Acc 0.7786, AUC 0.8552742600440979, avg_entr 0.0683082714676857, f1 0.7785999774932861
ep23_l1_test_time 0.8100908870001149
Test Epoch23 layer2 Acc 0.7804, AUC 0.8699862957000732, avg_entr 0.05972748249769211, f1 0.7803999781608582
ep23_l2_test_time 1.129523502999973
Test Epoch23 layer3 Acc 0.7794, AUC 0.8699100017547607, avg_entr 0.05735371261835098, f1 0.7794000506401062
ep23_l3_test_time 1.6157657679998465
Test Epoch23 layer4 Acc 0.7786, AUC 0.8697124719619751, avg_entr 0.05543402209877968, f1 0.7785999774932861
ep23_l4_test_time 2.251574996000045
gc 0
Train Epoch24 Acc 0.938175 (37527/40000), AUC 0.9822341203689575
ep24_train_time 65.62422908400004
Test Epoch24 layer0 Acc 0.7794, AUC 0.8522508144378662, avg_entr 0.15929125249385834, f1 0.7794000506401062
ep24_l0_test_time 0.6140759359998356
Test Epoch24 layer1 Acc 0.7866, AUC 0.8558816909790039, avg_entr 0.0634554773569107, f1 0.7865999937057495
ep24_l1_test_time 0.8083560479999505
Test Epoch24 layer2 Acc 0.7904, AUC 0.8702827095985413, avg_entr 0.05618039518594742, f1 0.7904000282287598
ep24_l2_test_time 1.1478071409999302
Test Epoch24 layer3 Acc 0.7898, AUC 0.8698476552963257, avg_entr 0.053086720407009125, f1 0.7897999882698059
ep24_l3_test_time 1.623045972
Test Epoch24 layer4 Acc 0.7904, AUC 0.8700088858604431, avg_entr 0.05147377401590347, f1 0.7904000282287598
ep24_l4_test_time 2.2487450460000673
gc 0
Train Epoch25 Acc 0.9434 (37736/40000), AUC 0.984829306602478
ep25_train_time 65.5441563259999
Test Epoch25 layer0 Acc 0.7754, AUC 0.8514882922172546, avg_entr 0.15946905314922333, f1 0.775399923324585
ep25_l0_test_time 0.6162288269999863
Test Epoch25 layer1 Acc 0.7818, AUC 0.8520158529281616, avg_entr 0.06283220648765564, f1 0.7817999720573425
ep25_l1_test_time 0.8077284179998969
Test Epoch25 layer2 Acc 0.7886, AUC 0.8675172924995422, avg_entr 0.055057279765605927, f1 0.7886000275611877
ep25_l2_test_time 1.130871653999975
Test Epoch25 layer3 Acc 0.7886, AUC 0.867558479309082, avg_entr 0.052682794630527496, f1 0.7886000275611877
ep25_l3_test_time 1.6136630900000455
Test Epoch25 layer4 Acc 0.7886, AUC 0.8676373362541199, avg_entr 0.05142080783843994, f1 0.7886000275611877
ep25_l4_test_time 2.2518288280000434
gc 0
Train Epoch26 Acc 0.9457 (37828/40000), AUC 0.9855415225028992
ep26_train_time 65.58355834300005
Test Epoch26 layer0 Acc 0.7744, AUC 0.8511238098144531, avg_entr 0.15900512039661407, f1 0.7743999361991882
ep26_l0_test_time 0.6158637969999745
Test Epoch26 layer1 Acc 0.7856, AUC 0.8564268350601196, avg_entr 0.05892020836472511, f1 0.7856000065803528
ep26_l1_test_time 0.8129007939999155
Test Epoch26 layer2 Acc 0.7922, AUC 0.8693891167640686, avg_entr 0.052461888641119, f1 0.7922000288963318
ep26_l2_test_time 1.1323897050001506
Test Epoch26 layer3 Acc 0.7928, AUC 0.8695602416992188, avg_entr 0.05032102018594742, f1 0.7928000092506409
ep26_l3_test_time 1.6158977529998992
Test Epoch26 layer4 Acc 0.7932, AUC 0.8695236444473267, avg_entr 0.04885568469762802, f1 0.7932000160217285
ep26_l4_test_time 2.261170616000072
gc 0
Train Epoch27 Acc 0.94705 (37882/40000), AUC 0.9860227108001709
ep27_train_time 65.60005569100008
Test Epoch27 layer0 Acc 0.7744, AUC 0.8505537509918213, avg_entr 0.15807312726974487, f1 0.7743999361991882
ep27_l0_test_time 0.6180159719999665
Test Epoch27 layer1 Acc 0.7844, AUC 0.8544374704360962, avg_entr 0.06136174872517586, f1 0.7843999266624451
ep27_l1_test_time 0.8077966599998945
Test Epoch27 layer2 Acc 0.7894, AUC 0.8684765100479126, avg_entr 0.05659233406186104, f1 0.7893999814987183
ep27_l2_test_time 1.1325502640002014
Test Epoch27 layer3 Acc 0.7886, AUC 0.868505597114563, avg_entr 0.05541042611002922, f1 0.7886000275611877
ep27_l3_test_time 1.6142710169999646
Test Epoch27 layer4 Acc 0.789, AUC 0.8686403036117554, avg_entr 0.054339341819286346, f1 0.7889999747276306
ep27_l4_test_time 2.2522217060000003
gc 0
Train Epoch28 Acc 0.948125 (37925/40000), AUC 0.9860939979553223
ep28_train_time 65.55221932900008
Test Epoch28 layer0 Acc 0.7706, AUC 0.8493872880935669, avg_entr 0.15696607530117035, f1 0.7706000208854675
ep28_l0_test_time 0.6135238100000606
Test Epoch28 layer1 Acc 0.7786, AUC 0.847206711769104, avg_entr 0.057779304683208466, f1 0.7785999774932861
ep28_l1_test_time 0.8089756809999926
Test Epoch28 layer2 Acc 0.7816, AUC 0.8644864559173584, avg_entr 0.048730455338954926, f1 0.7815999984741211
ep28_l2_test_time 1.1292783809999491
Test Epoch28 layer3 Acc 0.7808, AUC 0.8627811074256897, avg_entr 0.04563136771321297, f1 0.7808000445365906
ep28_l3_test_time 1.616041111999948
Test Epoch28 layer4 Acc 0.7812, AUC 0.8631540536880493, avg_entr 0.0443740040063858, f1 0.7811999917030334
ep28_l4_test_time 2.24930855599996
gc 0
Train Epoch29 Acc 0.950125 (38005/40000), AUC 0.9875538349151611
ep29_train_time 65.71100734499987
Test Epoch29 layer0 Acc 0.7714, AUC 0.848586916923523, avg_entr 0.15184539556503296, f1 0.771399974822998
ep29_l0_test_time 0.6145370959998218
Test Epoch29 layer1 Acc 0.7826, AUC 0.8475494384765625, avg_entr 0.05385390669107437, f1 0.7825999855995178
ep29_l1_test_time 0.8076008040002307
Test Epoch29 layer2 Acc 0.7852, AUC 0.8653345108032227, avg_entr 0.04712574556469917, f1 0.7851999998092651
ep29_l2_test_time 1.1382535990001088
Test Epoch29 layer3 Acc 0.7864, AUC 0.8651067018508911, avg_entr 0.044987376779317856, f1 0.7864000201225281
ep29_l3_test_time 1.6148659800001042
Test Epoch29 layer4 Acc 0.7848, AUC 0.8653251528739929, avg_entr 0.04397765174508095, f1 0.7847999930381775
ep29_l4_test_time 2.25105699300002
gc 0
Train Epoch30 Acc 0.949575 (37983/40000), AUC 0.9877904653549194
ep30_train_time 65.59047499100006
Test Epoch30 layer0 Acc 0.7706, AUC 0.8482611179351807, avg_entr 0.15548419952392578, f1 0.7706000208854675
ep30_l0_test_time 0.6161404100002983
Test Epoch30 layer1 Acc 0.779, AUC 0.848131000995636, avg_entr 0.057891327887773514, f1 0.7789999842643738
ep30_l1_test_time 0.8082970520003983
Test Epoch30 layer2 Acc 0.7864, AUC 0.8658788800239563, avg_entr 0.04998715966939926, f1 0.7864000201225281
ep30_l2_test_time 1.1317255320000186
Test Epoch30 layer3 Acc 0.7858, AUC 0.8661797046661377, avg_entr 0.04802483692765236, f1 0.7857999801635742
ep30_l3_test_time 1.6145711019998998
Test Epoch30 layer4 Acc 0.7866, AUC 0.8662325143814087, avg_entr 0.04709640145301819, f1 0.7865999937057495
ep30_l4_test_time 2.24864701100023
gc 0
Train Epoch31 Acc 0.952 (38080/40000), AUC 0.9880827069282532
ep31_train_time 65.55992626199986
Test Epoch31 layer0 Acc 0.7752, AUC 0.8486579060554504, avg_entr 0.15379464626312256, f1 0.7752000093460083
ep31_l0_test_time 0.6143997549997948
Test Epoch31 layer1 Acc 0.7808, AUC 0.8495763540267944, avg_entr 0.05798603966832161, f1 0.7808000445365906
ep31_l1_test_time 0.8088681879999058
Test Epoch31 layer2 Acc 0.7872, AUC 0.865964412689209, avg_entr 0.05124779790639877, f1 0.7871999740600586
ep31_l2_test_time 1.130878149000182
Test Epoch31 layer3 Acc 0.7874, AUC 0.8660287857055664, avg_entr 0.04961660876870155, f1 0.7874000072479248
ep31_l3_test_time 1.6157638760000737
Test Epoch31 layer4 Acc 0.7876, AUC 0.8661782741546631, avg_entr 0.04852300137281418, f1 0.7875999212265015
ep31_l4_test_time 2.25170711499959
gc 0
Train Epoch32 Acc 0.9524 (38096/40000), AUC 0.9888622164726257
ep32_train_time 65.53344312499985
Test Epoch32 layer0 Acc 0.7752, AUC 0.847937822341919, avg_entr 0.15406765043735504, f1 0.7752000093460083
ep32_l0_test_time 0.616788765000365
Test Epoch32 layer1 Acc 0.78, AUC 0.8455918431282043, avg_entr 0.05614641681313515, f1 0.7799999117851257
ep32_l1_test_time 0.8074568240003828
Test Epoch32 layer2 Acc 0.7844, AUC 0.8635151386260986, avg_entr 0.04879763722419739, f1 0.7843999266624451
ep32_l2_test_time 1.1325183150001976
Test Epoch32 layer3 Acc 0.7856, AUC 0.8635824918746948, avg_entr 0.0466642789542675, f1 0.7856000065803528
ep32_l3_test_time 1.6125225119999413
Test Epoch32 layer4 Acc 0.786, AUC 0.8637487888336182, avg_entr 0.04541260004043579, f1 0.7860000133514404
ep32_l4_test_time 2.2521717770000578
gc 0
Train Epoch33 Acc 0.953425 (38137/40000), AUC 0.9890202283859253
ep33_train_time 65.55267750599978
Test Epoch33 layer0 Acc 0.7746, AUC 0.8477605581283569, avg_entr 0.15350939333438873, f1 0.7746000289916992
ep33_l0_test_time 0.6152180730000509
Test Epoch33 layer1 Acc 0.7792, AUC 0.8464770317077637, avg_entr 0.05483898147940636, f1 0.7791999578475952
ep33_l1_test_time 0.8106104399998912
Test Epoch33 layer2 Acc 0.7868, AUC 0.8642386198043823, avg_entr 0.04785683751106262, f1 0.786799967288971
ep33_l2_test_time 1.1429150080002728
Test Epoch33 layer3 Acc 0.787, AUC 0.8643990755081177, avg_entr 0.04565397650003433, f1 0.7870000600814819
ep33_l3_test_time 1.6183874899998045
Test Epoch33 layer4 Acc 0.7872, AUC 0.8644967079162598, avg_entr 0.04443889483809471, f1 0.7871999740600586
ep33_l4_test_time 2.2530187600000318
gc 0
Train Epoch34 Acc 0.95275 (38110/40000), AUC 0.9892985820770264
ep34_train_time 65.60357917600004
Test Epoch34 layer0 Acc 0.7756, AUC 0.8479022979736328, avg_entr 0.15109898149967194, f1 0.775600016117096
ep34_l0_test_time 0.6151461139997991
Test Epoch34 layer1 Acc 0.7806, AUC 0.8457096815109253, avg_entr 0.052979499101638794, f1 0.7806000113487244
ep34_l1_test_time 0.8077424559996871
Test Epoch34 layer2 Acc 0.7856, AUC 0.8636802434921265, avg_entr 0.04678936302661896, f1 0.7856000065803528
ep34_l2_test_time 1.13652167500004
Test Epoch34 layer3 Acc 0.7868, AUC 0.8639816045761108, avg_entr 0.04499512165784836, f1 0.786799967288971
ep34_l3_test_time 1.615200778000144
Test Epoch34 layer4 Acc 0.7874, AUC 0.8641219735145569, avg_entr 0.04403265565633774, f1 0.7874000072479248
ep34_l4_test_time 2.2518424009999762
gc 0
Train Epoch35 Acc 0.95385 (38154/40000), AUC 0.9892514944076538
ep35_train_time 65.66833898300001
Test Epoch35 layer0 Acc 0.7694, AUC 0.847296953201294, avg_entr 0.1530645191669464, f1 0.7694000005722046
ep35_l0_test_time 0.6155890170002749
Test Epoch35 layer1 Acc 0.7776, AUC 0.8452988862991333, avg_entr 0.05388402193784714, f1 0.7775999903678894
ep35_l1_test_time 0.8127982649998557
Test Epoch35 layer2 Acc 0.7842, AUC 0.8633027672767639, avg_entr 0.0465676449239254, f1 0.7842000126838684
ep35_l2_test_time 1.1298084439999911
Test Epoch35 layer3 Acc 0.7852, AUC 0.8636879920959473, avg_entr 0.04469941556453705, f1 0.7851999998092651
ep35_l3_test_time 1.6186394330002258
Test Epoch35 layer4 Acc 0.7854, AUC 0.8637566566467285, avg_entr 0.0435979887843132, f1 0.7853999733924866
ep35_l4_test_time 2.251503273999788
gc 0
Train Epoch36 Acc 0.955825 (38233/40000), AUC 0.989805281162262
ep36_train_time 65.62074644599988
Test Epoch36 layer0 Acc 0.7734, AUC 0.8470418453216553, avg_entr 0.15225686132907867, f1 0.7734000086784363
ep36_l0_test_time 0.6143931030001113
Test Epoch36 layer1 Acc 0.78, AUC 0.8457293510437012, avg_entr 0.053232427686452866, f1 0.7799999117851257
ep36_l1_test_time 0.8075531519998549
Test Epoch36 layer2 Acc 0.7854, AUC 0.8634006381034851, avg_entr 0.04640873894095421, f1 0.7853999733924866
ep36_l2_test_time 1.134981788999994
Test Epoch36 layer3 Acc 0.7866, AUC 0.863807201385498, avg_entr 0.04441919922828674, f1 0.7865999937057495
ep36_l3_test_time 1.6152557850000449
Test Epoch36 layer4 Acc 0.786, AUC 0.8638678789138794, avg_entr 0.04345289245247841, f1 0.7860000133514404
ep36_l4_test_time 2.2512053469999955
gc 0
Train Epoch37 Acc 0.953525 (38141/40000), AUC 0.9897213578224182
ep37_train_time 65.6180245810001
Test Epoch37 layer0 Acc 0.7694, AUC 0.8469276428222656, avg_entr 0.15202251076698303, f1 0.7694000005722046
ep37_l0_test_time 0.6182090620000054
Test Epoch37 layer1 Acc 0.7782, AUC 0.8443769812583923, avg_entr 0.0541376955807209, f1 0.7781999707221985
ep37_l1_test_time 0.809097376000409
Test Epoch37 layer2 Acc 0.784, AUC 0.8627945184707642, avg_entr 0.04577746242284775, f1 0.7839999794960022
ep37_l2_test_time 1.1328977790003592
Test Epoch37 layer3 Acc 0.785, AUC 0.8631362915039062, avg_entr 0.04379494860768318, f1 0.7850000262260437
ep37_l3_test_time 1.61733785899969
Test Epoch37 layer4 Acc 0.7852, AUC 0.863182783126831, avg_entr 0.04269220679998398, f1 0.7851999998092651
ep37_l4_test_time 2.2511029170000256
gc 0
Train Epoch38 Acc 0.953975 (38159/40000), AUC 0.9898371696472168
ep38_train_time 65.62963896200017
Test Epoch38 layer0 Acc 0.7706, AUC 0.846777617931366, avg_entr 0.15210038423538208, f1 0.7706000208854675
ep38_l0_test_time 0.6176398470001914
Test Epoch38 layer1 Acc 0.7764, AUC 0.844066321849823, avg_entr 0.05394251272082329, f1 0.776400089263916
ep38_l1_test_time 0.8126856539997789
Test Epoch38 layer2 Acc 0.7832, AUC 0.8624812960624695, avg_entr 0.045197393745183945, f1 0.7832000851631165
ep38_l2_test_time 1.1347102480003741
Test Epoch38 layer3 Acc 0.784, AUC 0.8628848791122437, avg_entr 0.042909715324640274, f1 0.7839999794960022
ep38_l3_test_time 1.6169613079996452
Test Epoch38 layer4 Acc 0.7846, AUC 0.8628745079040527, avg_entr 0.041668158024549484, f1 0.784600019454956
ep38_l4_test_time 2.258820700999877
gc 0
Train Epoch39 Acc 0.9546 (38184/40000), AUC 0.9900641441345215
ep39_train_time 65.69506727199996
Test Epoch39 layer0 Acc 0.7692, AUC 0.8466037511825562, avg_entr 0.15233057737350464, f1 0.7692000269889832
ep39_l0_test_time 0.6165325679999114
Test Epoch39 layer1 Acc 0.777, AUC 0.8429421186447144, avg_entr 0.05427452549338341, f1 0.7770000100135803
ep39_l1_test_time 0.8071897059999174
Test Epoch39 layer2 Acc 0.7838, AUC 0.8623638153076172, avg_entr 0.044567808508872986, f1 0.7838000059127808
ep39_l2_test_time 1.1324022300000252
Test Epoch39 layer3 Acc 0.784, AUC 0.8626855611801147, avg_entr 0.04224039241671562, f1 0.7839999794960022
ep39_l3_test_time 1.6166619059999903
Test Epoch39 layer4 Acc 0.7838, AUC 0.8626967072486877, avg_entr 0.04110517352819443, f1 0.7838000059127808
ep39_l4_test_time 2.2493495629996687
gc 0
Train Epoch40 Acc 0.955225 (38209/40000), AUC 0.9903188347816467
ep40_train_time 65.54247194599975
Test Epoch40 layer0 Acc 0.7718, AUC 0.846530020236969, avg_entr 0.1512695848941803, f1 0.7717999815940857
ep40_l0_test_time 0.6171771869999247
Test Epoch40 layer1 Acc 0.777, AUC 0.8441410064697266, avg_entr 0.05227942019701004, f1 0.7770000100135803
ep40_l1_test_time 0.8097033560002274
Test Epoch40 layer2 Acc 0.7834, AUC 0.8622675538063049, avg_entr 0.04465017467737198, f1 0.7833999395370483
ep40_l2_test_time 1.1305064650000531
Test Epoch40 layer3 Acc 0.7838, AUC 0.8626111745834351, avg_entr 0.042767204344272614, f1 0.7838000059127808
ep40_l3_test_time 1.6153805899998588
Test Epoch40 layer4 Acc 0.7842, AUC 0.8626855611801147, avg_entr 0.04179352894425392, f1 0.7842000126838684
ep40_l4_test_time 2.2511527370002113
gc 0
Train Epoch41 Acc 0.9562 (38248/40000), AUC 0.9905157089233398
ep41_train_time 65.680664815
Test Epoch41 layer0 Acc 0.7722, AUC 0.8464800119400024, avg_entr 0.15126189589500427, f1 0.7721999287605286
ep41_l0_test_time 0.6157586070003163
Test Epoch41 layer1 Acc 0.7784, AUC 0.8441331386566162, avg_entr 0.051736682653427124, f1 0.7784000039100647
ep41_l1_test_time 0.8090256020000197
Test Epoch41 layer2 Acc 0.7844, AUC 0.8624387979507446, avg_entr 0.04492195323109627, f1 0.7843999266624451
ep41_l2_test_time 1.1305728329998601
Test Epoch41 layer3 Acc 0.7848, AUC 0.8628008961677551, avg_entr 0.04303031787276268, f1 0.7847999930381775
ep41_l3_test_time 1.6157301099997312
Test Epoch41 layer4 Acc 0.7852, AUC 0.8629156947135925, avg_entr 0.04205002635717392, f1 0.7851999998092651
ep41_l4_test_time 2.250684266999997
gc 0
Train Epoch42 Acc 0.956 (38240/40000), AUC 0.9904553890228271
ep42_train_time 65.56056157700004
Test Epoch42 layer0 Acc 0.7716, AUC 0.846348762512207, avg_entr 0.15147875249385834, f1 0.771600067615509
ep42_l0_test_time 0.6156652429999667
Test Epoch42 layer1 Acc 0.7768, AUC 0.8434321880340576, avg_entr 0.052681561559438705, f1 0.7768000364303589
ep42_l1_test_time 0.8049672359998112
Test Epoch42 layer2 Acc 0.7836, AUC 0.8622416853904724, avg_entr 0.04492141678929329, f1 0.7835999727249146
ep42_l2_test_time 1.1327762050000274
Test Epoch42 layer3 Acc 0.7848, AUC 0.8624966144561768, avg_entr 0.043044108897447586, f1 0.7847999930381775
ep42_l3_test_time 1.6132944559999487
Test Epoch42 layer4 Acc 0.7842, AUC 0.8625757694244385, avg_entr 0.04213586822152138, f1 0.7842000126838684
ep42_l4_test_time 2.251922574999753
gc 0
Train Epoch43 Acc 0.954475 (38179/40000), AUC 0.9903507232666016
ep43_train_time 65.52879854799994
Test Epoch43 layer0 Acc 0.771, AUC 0.8463960289955139, avg_entr 0.15153461694717407, f1 0.7710000276565552
ep43_l0_test_time 0.6136559060000764
Test Epoch43 layer1 Acc 0.778, AUC 0.8441672325134277, avg_entr 0.051706165075302124, f1 0.777999997138977
ep43_l1_test_time 0.8114409400000113
Test Epoch43 layer2 Acc 0.7848, AUC 0.8626023530960083, avg_entr 0.04457796365022659, f1 0.7847999930381775
ep43_l2_test_time 1.1294512560002659
Test Epoch43 layer3 Acc 0.7844, AUC 0.8628275394439697, avg_entr 0.04262380301952362, f1 0.7843999266624451
ep43_l3_test_time 1.6205488440000408
Test Epoch43 layer4 Acc 0.784, AUC 0.8629610538482666, avg_entr 0.04154302552342415, f1 0.7839999794960022
ep43_l4_test_time 2.249824821999937
gc 0
Train Epoch44 Acc 0.95535 (38214/40000), AUC 0.990386962890625
ep44_train_time 65.58958272600012
Test Epoch44 layer0 Acc 0.7716, AUC 0.8464747667312622, avg_entr 0.15090861916542053, f1 0.771600067615509
ep44_l0_test_time 0.6168707150000046
Test Epoch44 layer1 Acc 0.7786, AUC 0.8440945744514465, avg_entr 0.05125239118933678, f1 0.7785999774932861
ep44_l1_test_time 0.8155165789999046
Test Epoch44 layer2 Acc 0.7844, AUC 0.8627064228057861, avg_entr 0.04439437389373779, f1 0.7843999266624451
ep44_l2_test_time 1.1333417319997352
Test Epoch44 layer3 Acc 0.7842, AUC 0.8629370927810669, avg_entr 0.04262048378586769, f1 0.7842000126838684
ep44_l3_test_time 1.6168352529998629
Test Epoch44 layer4 Acc 0.7846, AUC 0.8630610704421997, avg_entr 0.04170772805809975, f1 0.784600019454956
ep44_l4_test_time 2.2493754560000525
gc 0
Train Epoch45 Acc 0.9553 (38212/40000), AUC 0.9905627965927124
ep45_train_time 65.73516830200015
Test Epoch45 layer0 Acc 0.7692, AUC 0.8464398980140686, avg_entr 0.15125197172164917, f1 0.7692000269889832
ep45_l0_test_time 0.6172266989997297
Test Epoch45 layer1 Acc 0.7778, AUC 0.8438805937767029, avg_entr 0.05200444161891937, f1 0.7778000235557556
ep45_l1_test_time 0.8089733659999183
Test Epoch45 layer2 Acc 0.7832, AUC 0.862510085105896, avg_entr 0.04453304782509804, f1 0.7832000851631165
ep45_l2_test_time 1.1298660360002941
Test Epoch45 layer3 Acc 0.7828, AUC 0.862783670425415, avg_entr 0.04250713810324669, f1 0.782800018787384
ep45_l3_test_time 1.6133730930000638
Test Epoch45 layer4 Acc 0.7834, AUC 0.862881064414978, avg_entr 0.04151903837919235, f1 0.7833999395370483
ep45_l4_test_time 2.250317497999731
gc 0
Train Epoch46 Acc 0.955775 (38231/40000), AUC 0.9903293251991272
ep46_train_time 65.57043171900023
Test Epoch46 layer0 Acc 0.7718, AUC 0.8464168310165405, avg_entr 0.15073220431804657, f1 0.7717999815940857
ep46_l0_test_time 0.6258596929997111
Test Epoch46 layer1 Acc 0.7782, AUC 0.8436963558197021, avg_entr 0.052246980369091034, f1 0.7781999707221985
ep46_l1_test_time 0.8174476039998808
Test Epoch46 layer2 Acc 0.7838, AUC 0.8624351620674133, avg_entr 0.044701799750328064, f1 0.7838000059127808
ep46_l2_test_time 1.140698790999977
Test Epoch46 layer3 Acc 0.7836, AUC 0.8627250790596008, avg_entr 0.042846132069826126, f1 0.7835999727249146
ep46_l3_test_time 1.6233972709997033
Test Epoch46 layer4 Acc 0.7836, AUC 0.862836480140686, avg_entr 0.041905295103788376, f1 0.7835999727249146
ep46_l4_test_time 2.251296141000239
gc 0
Train Epoch47 Acc 0.955775 (38231/40000), AUC 0.9900941848754883
ep47_train_time 65.5402330869997
Test Epoch47 layer0 Acc 0.7714, AUC 0.8463959693908691, avg_entr 0.1506834179162979, f1 0.771399974822998
ep47_l0_test_time 0.6167804619999515
Test Epoch47 layer1 Acc 0.778, AUC 0.8437932729721069, avg_entr 0.051803167909383774, f1 0.777999997138977
ep47_l1_test_time 0.8079837670002235
Test Epoch47 layer2 Acc 0.7838, AUC 0.8623616695404053, avg_entr 0.04437340795993805, f1 0.7838000059127808
ep47_l2_test_time 1.131785395999941
Test Epoch47 layer3 Acc 0.784, AUC 0.8626857995986938, avg_entr 0.042424581944942474, f1 0.7839999794960022
ep47_l3_test_time 1.6122976699998617
Test Epoch47 layer4 Acc 0.7842, AUC 0.8627761602401733, avg_entr 0.04136556386947632, f1 0.7842000126838684
ep47_l4_test_time 2.252073725000173
gc 0
Train Epoch48 Acc 0.95575 (38230/40000), AUC 0.9904651641845703
ep48_train_time 65.5980234499998
Test Epoch48 layer0 Acc 0.7726, AUC 0.8463611602783203, avg_entr 0.15073125064373016, f1 0.772599995136261
ep48_l0_test_time 0.614103593999971
Test Epoch48 layer1 Acc 0.7772, AUC 0.843585193157196, avg_entr 0.05148164555430412, f1 0.777199923992157
ep48_l1_test_time 0.8097943800003122
Test Epoch48 layer2 Acc 0.7838, AUC 0.8621416091918945, avg_entr 0.044446177780628204, f1 0.7838000059127808
ep48_l2_test_time 1.134902835000048
Test Epoch48 layer3 Acc 0.7834, AUC 0.862485408782959, avg_entr 0.042509932070970535, f1 0.7833999395370483
ep48_l3_test_time 1.6163930469997467
Test Epoch48 layer4 Acc 0.7842, AUC 0.8625708222389221, avg_entr 0.041549574583768845, f1 0.7842000126838684
ep48_l4_test_time 2.2574356869999974
gc 0
Train Epoch49 Acc 0.95595 (38238/40000), AUC 0.9906045794487
ep49_train_time 65.55524128599973
Test Epoch49 layer0 Acc 0.7694, AUC 0.8463221788406372, avg_entr 0.15092431008815765, f1 0.7694000005722046
ep49_l0_test_time 0.6144708179999725
Test Epoch49 layer1 Acc 0.778, AUC 0.8435087203979492, avg_entr 0.051766570657491684, f1 0.777999997138977
ep49_l1_test_time 0.811556837000353
Test Epoch49 layer2 Acc 0.7828, AUC 0.8620878458023071, avg_entr 0.04422568902373314, f1 0.782800018787384
ep49_l2_test_time 1.1382561219998024
Test Epoch49 layer3 Acc 0.7836, AUC 0.8624981641769409, avg_entr 0.04217008501291275, f1 0.7835999727249146
ep49_l3_test_time 1.6225261009999485
Test Epoch49 layer4 Acc 0.7834, AUC 0.8625388145446777, avg_entr 0.04111625999212265, f1 0.7833999395370483
ep49_l4_test_time 2.25797752499966
Best AUC tensor(0.8066) 12 2
train_as_loss [[8.89898587e+01 6.00140838e+01 5.23386941e+01 5.05595037e+01
  4.99392058e+01 4.96567412e+01 4.95054379e+01 4.94152128e+01
  4.93572054e+01 4.93177708e+01 4.92897759e+01 4.92692170e+01
  4.92536958e+01 4.92417084e+01 4.92322704e+01 4.92247195e+01
  4.92185932e+01 4.92146841e+01 4.92123245e+01 4.92100959e+01
  4.92079991e+01 4.92065028e+01 4.92055163e+01 4.92045291e+01
  4.92035422e+01 4.92028006e+01 4.92022925e+01 4.92017714e+01
  4.92012310e+01 4.92008153e+01 4.92005248e+01 4.92002187e+01
  4.91999008e+01 4.91996501e+01 4.91994707e+01 4.91992834e+01
  4.91990878e+01 4.91989249e+01 4.91988160e+01 4.91987002e+01
  4.91985698e+01 4.91984634e+01 4.91983977e+01 4.91983150e+01
  4.91982378e+01 4.91981621e+01 4.91981278e+01 4.91980720e+01
  4.91980127e+01 4.91979805e+01]
 [1.90114256e+00 4.88532066e-04 2.34752378e-05 6.05172888e-06
  2.27028537e-06 1.02453763e-06 5.62279492e-07 3.15252691e-07
  1.85737749e-07 1.23209977e-07 8.02899330e-08 6.17875956e-07
  2.42506511e-07 8.52143351e-07 7.40493301e-08 7.45665453e-07
  6.60859375e-07 2.59197966e-08 8.36769481e-09 6.84309970e-09
  6.10633965e-09 4.86527280e-09 4.72273063e-09 4.29837763e-09
  3.99193699e-09 3.41975705e-09 3.33890683e-09 3.08793173e-09
  3.08837259e-09 2.85027489e-09 2.75393193e-09 2.62961251e-09
  2.53422940e-09 2.44500348e-09 2.39671719e-09 2.26853876e-09
  2.27446615e-09 2.19455930e-09 2.14449472e-09 2.07911265e-09
  2.03009045e-09 2.03319869e-09 1.98279812e-09 1.94139838e-09
  1.89386165e-09 1.99033192e-09 1.90745547e-09 1.85230117e-09
  1.78094759e-09 2.03921719e-09]
 [2.01363622e+00 6.01185887e-04 2.02036174e-05 5.16265144e-06
  2.01856886e-06 9.51498979e-07 5.49576979e-07 3.21008476e-07
  1.89588759e-07 1.28508904e-07 8.38966429e-08 7.46099013e-08
  5.07920592e-08 7.52197207e-08 3.10358045e-08 2.53378919e-07
  3.09233305e-07 2.89538737e-08 9.77692900e-09 7.55358850e-09
  7.20718167e-09 4.77507676e-09 4.67684833e-09 4.40762322e-09
  4.13370486e-09 3.18391967e-09 3.13450136e-09 2.87307057e-09
  2.96519492e-09 2.68109470e-09 2.56867121e-09 2.41339465e-09
  2.30758053e-09 2.22909445e-09 2.16354648e-09 2.04349737e-09
  2.03744836e-09 1.97963195e-09 1.90239707e-09 1.85471689e-09
  1.79150431e-09 1.81396151e-09 1.74290251e-09 1.71695268e-09
  1.66122567e-09 1.75270774e-09 1.69567424e-09 1.63661668e-09
  1.56570868e-09 1.83307107e-09]
 [2.37477389e+00 2.78759220e-03 2.23589501e-05 5.92872626e-06
  2.48515102e-06 1.25718324e-06 7.84054316e-07 4.72163481e-07
  2.80206851e-07 2.03787935e-07 1.34547216e-07 1.29949554e-07
  9.25559573e-08 8.50148061e-08 5.05420036e-08 6.87680440e-08
  8.24898588e-08 1.87025499e-08 1.77645030e-08 1.35146224e-08
  1.38847557e-08 7.11713162e-09 6.91313110e-09 6.80431929e-09
  6.58268737e-09 4.39084283e-09 4.38417911e-09 3.96511280e-09
  4.22350303e-09 3.60801310e-09 3.47667900e-09 3.21455016e-09
  3.11528009e-09 2.92536208e-09 2.85645885e-09 2.67889512e-09
  2.72865058e-09 2.58265964e-09 2.50894336e-09 2.40826611e-09
  2.33896372e-09 2.33432711e-09 2.27510932e-09 2.22244478e-09
  2.16296870e-09 2.29537884e-09 2.19003360e-09 2.10834232e-09
  2.00677231e-09 2.43301939e-09]
 [2.45002639e+00 7.93865132e-03 2.59352163e-05 6.94045946e-06
  2.99004508e-06 1.64075183e-06 1.07634597e-06 7.00999178e-07
  4.10052258e-07 3.26314640e-07 2.26472320e-07 2.64133675e-07
  1.95500077e-07 1.67499548e-07 1.24625309e-07 1.32270180e-07
  1.43119139e-07 3.31849979e-08 3.44339690e-08 2.64288898e-08
  3.00041086e-08 1.02648540e-08 9.37300456e-09 1.02088206e-08
  1.04193022e-08 5.14929520e-09 5.13292821e-09 4.52732839e-09
  5.36780991e-09 4.04769888e-09 3.98007059e-09 3.50567465e-09
  3.35956643e-09 3.14353066e-09 3.01847035e-09 2.75118049e-09
  2.91296356e-09 2.69339974e-09 2.59346271e-09 2.48159384e-09
  2.40832903e-09 2.44567955e-09 2.33756609e-09 2.30104641e-09
  2.24595748e-09 2.41370303e-09 2.33982402e-09 2.27723444e-09
  2.19142397e-09 2.50207184e-09]]
train_ae_loss [[4.07348069 2.8263334  3.91031191 4.19265771 4.42201106 4.66958015
  4.74170778 4.84947558 4.83183793 4.90512845 4.86909434 4.9101499
  4.7533383  4.84471651 4.61074938 4.63432137 4.43810597 4.12816494
  3.97781403 3.88703343 3.87076039 3.58736768 3.52704944 3.45528767
  3.39107388 3.30732619 3.26588048 3.24119227 3.21331526 3.15299729
  3.1449136  3.14269274 3.12460992 3.07603929 3.07243126 3.07499676
  3.07006109 3.05406392 3.0581137  3.06636259 3.0542427  3.0310341
  3.05118906 3.05446939 3.05316378 3.04713199 3.05677915 3.03961634
  3.05113953 3.03955562]
 [3.47534958 2.86875369 4.09501156 4.15098025 4.24902317 4.49692629
  4.47369412 4.53272229 4.41974632 4.4556262  4.32436162 4.3023913
  4.03272522 4.18093237 3.74280304 3.73458563 3.48389944 3.02741033
  2.8685383  2.71948374 2.71101545 2.35039458 2.24928166 2.14529153
  2.08528764 1.9413358  1.90070158 1.86291148 1.83679164 1.73761862
  1.7333829  1.73622263 1.71828413 1.66265216 1.64581119 1.64956341
  1.62391228 1.61848595 1.62180439 1.61650145 1.62313129 1.58424699
  1.61095855 1.6110849  1.60271205 1.59625261 1.61612346 1.58534866
  1.60436654 1.58646286]
 [4.09920996 2.55442469 3.78184867 3.93468215 3.8362666  4.03552156
  3.91703017 3.92033416 3.74194369 3.77257741 3.66892293 3.66279946
  3.40649986 3.56389443 3.13635312 3.1473182  2.91768002 2.50228456
  2.37462394 2.24405287 2.24236102 1.91814145 1.82276429 1.73432868
  1.68421673 1.5519789  1.51683943 1.49210209 1.46386797 1.3806817
  1.37472091 1.37419803 1.35669577 1.31012068 1.29499936 1.30023234
  1.274707   1.27058035 1.26917675 1.26929038 1.27245749 1.23609868
  1.26321691 1.26072532 1.25680039 1.25159608 1.26696395 1.243866
  1.2501795  1.23857565]
 [4.55181069 2.25576315 3.35130456 3.88345908 3.57392694 3.68284488
  3.47620127 3.45409247 3.25598982 3.28372071 3.18753041 3.19400353
  2.94645317 3.11703041 2.70278343 2.71805056 2.5128945  2.1388346
  2.02887859 1.9162857  1.92265599 1.63368105 1.55016708 1.47554031
  1.43403069 1.31587712 1.28770169 1.26545829 1.24225447 1.1721403
  1.16545622 1.16520039 1.14857107 1.10939545 1.09648051 1.10165973
  1.07743885 1.07610032 1.07363326 1.07311492 1.07587289 1.04537292
  1.06660629 1.06595748 1.06276748 1.0573469  1.07123688 1.05317207
  1.05622439 1.04619938]
 [4.62612803 2.45842858 3.65377865 4.44236325 4.30839558 4.20752519
  3.82293189 3.76023533 3.50348997 3.52602391 3.41865937 3.42988889
  3.14592804 3.35372823 2.87655061 2.8905553  2.66894348 2.25888916
  2.14095294 2.02271114 2.03227565 1.71970049 1.6302479  1.55355049
  1.50765537 1.38037757 1.35095808 1.3272208  1.30423767 1.22862817
  1.22245569 1.22166193 1.20419687 1.16215541 1.14788063 1.15359904
  1.12819564 1.12725326 1.12355842 1.12380423 1.12577504 1.09320762
  1.11594615 1.11550999 1.11279388 1.10636172 1.12062379 1.10139442
  1.10529307 1.09451015]]
valid_acc (5, 50)
valid_AUC (5, 50)
train_acc (50,)
total_train+valid_time 3608.483171352
Start Testing
Load ckpt at ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt
gc 9
Test layer0 Acc 0.7882, AUC 0.871691107749939, avg_entr 0.22618046402931213, f1 0.7882000207901001
l0_test_time 0.62564036699996
gc 0
Test layer1 Acc 0.7986, AUC 0.8844085931777954, avg_entr 0.14929085969924927, f1 0.7986000180244446
l1_test_time 0.818050342999868
gc 0
Test layer2 Acc 0.8018, AUC 0.8884553909301758, avg_entr 0.13781484961509705, f1 0.801800012588501
l2_test_time 1.1363902819998657
gc 0
Test layer3 Acc 0.8014, AUC 0.8880686163902283, avg_entr 0.13664324581623077, f1 0.8014000058174133
l3_test_time 1.6208622640001522
gc 0
Test layer4 Acc 0.8004, AUC 0.8880664706230164, avg_entr 0.13104771077632904, f1 0.8004000186920166
l4_test_time 2.261578052999994
gc 0
Test threshold 0.1 Acc 0.8004, AUC 0.8813799619674683, avg_entr 0.19415283203125, f1 0.8004000186920166
t0.1_test_time 1.3113378889997875
gc 0
Test threshold 0.2 Acc 0.8, AUC 0.8785697221755981, avg_entr 0.20347636938095093, f1 0.8000000715255737
t0.2_test_time 1.1700784280001244
gc 0
Test threshold 0.3 Acc 0.7994, AUC 0.8776801824569702, avg_entr 0.2121010571718216, f1 0.7993999123573303
t0.3_test_time 1.0879426319997947
gc 0
Test threshold 0.4 Acc 0.7984, AUC 0.875382125377655, avg_entr 0.2215651571750641, f1 0.7983999848365784
t0.4_test_time 1.010661694999726
gc 0
Test threshold 0.5 Acc 0.798, AUC 0.8752275705337524, avg_entr 0.23120258748531342, f1 0.7979999780654907
t0.5_test_time 0.9428519309999501
gc 0
Test threshold 0.6 Acc 0.7982, AUC 0.8744326233863831, avg_entr 0.241227924823761, f1 0.7982000708580017
t0.6_test_time 0.8853273170002467
gc 0
Test threshold 0.7 Acc 0.7978, AUC 0.8748859167098999, avg_entr 0.2549374997615814, f1 0.7978000044822693
t0.7_test_time 0.8261773659996834
gc 0
Test threshold 0.8 Acc 0.7988, AUC 0.8747481107711792, avg_entr 0.2681457996368408, f1 0.798799991607666
t0.8_test_time 0.7662309839997761
gc 0
Test threshold 0.9 Acc 0.7972, AUC 0.8742717504501343, avg_entr 0.287546843290329, f1 0.7971999645233154
t0.9_test_time 0.7508119509998323
