total count words 222751
vocab size 30000
train size 40000, valid size 5000, test size 5000
found 27937 words in glove
model: TransformerALsideText(
  (layers): ModuleList(
    (0): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=2, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=2, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
  (emb_layers): ModuleList(
    (0): Embedding(30000, 300)
    (1): Embedding(30000, 300)
    (2): Embedding(30000, 300)
    (3): Embedding(30000, 300)
    (4): Embedding(30000, 300)
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.multi_headed_attention.linears.0.weight 90000
layers.0.enc.f.multi_headed_attention.linears.0.bias 300
layers.0.enc.f.multi_headed_attention.linears.1.weight 90000
layers.0.enc.f.multi_headed_attention.linears.1.bias 300
layers.0.enc.f.multi_headed_attention.linears.2.weight 90000
layers.0.enc.f.multi_headed_attention.linears.2.bias 300
layers.0.enc.f.multi_headed_attention.linears.3.weight 90000
layers.0.enc.f.multi_headed_attention.linears.3.bias 300
layers.0.enc.f.feed_forward.w_1.weight 90000
layers.0.enc.f.feed_forward.w_1.bias 300
layers.0.enc.f.feed_forward.w_2.weight 90000
layers.0.enc.f.feed_forward.w_2.bias 300
layers.0.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.0.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.0.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.0.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.0.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.0.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.0.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.0.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.0.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.0.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.0.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.0.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.0.enc.f.encoder.norm.a 300
layers.0.enc.f.encoder.norm.b 300
layers.0.ae.g.0.weight 256
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 256
layers.0.ae.h.0.bias 2
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
emb_layers.0.weight 9000000
emb_layers.1.weight 9000000
emb_layers.2.weight 9000000
emb_layers.3.weight 9000000
emb_layers.4.weight 9000000
Total Trainable Params: 50758378
init_time 20.951041595
Start Training
gc 0
Train Epoch0 Acc 0.5026 (20104/40000), AUC 0.5013222694396973
ep0_train_time 65.924130204
Test Epoch0 layer0 Acc 0.5376, AUC 0.6271171569824219, avg_entr 0.6922793388366699, f1 0.5375999808311462
ep0_l0_test_time 0.6305682880000063
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 0
Test Epoch0 layer1 Acc 0.5006, AUC 0.5531049966812134, avg_entr 0.6937263011932373, f1 0.5005999803543091
ep0_l1_test_time 0.8292587189999949
Test Epoch0 layer2 Acc 0.5026, AUC 0.5101233720779419, avg_entr 0.6925762295722961, f1 0.5026000142097473
ep0_l2_test_time 1.1426971170000115
Test Epoch0 layer3 Acc 0.523, AUC 0.5340611338615417, avg_entr 0.6942732930183411, f1 0.5230000019073486
ep0_l3_test_time 1.6244767470000028
Test Epoch0 layer4 Acc 0.5002, AUC 0.5031908750534058, avg_entr 0.693466067314148, f1 0.5001999735832214
ep0_l4_test_time 2.261546468000006
gc 0
Train Epoch1 Acc 0.510575 (20423/40000), AUC 0.5118916034698486
ep1_train_time 65.654205394
Test Epoch1 layer0 Acc 0.6788, AUC 0.7518573999404907, avg_entr 0.5999554395675659, f1 0.6787999868392944
ep1_l0_test_time 0.6327290339999934
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 1
Test Epoch1 layer1 Acc 0.6576, AUC 0.7433386445045471, avg_entr 0.6526261568069458, f1 0.6575999855995178
ep1_l1_test_time 0.8376888970000209
Test Epoch1 layer2 Acc 0.6336, AUC 0.7315183281898499, avg_entr 0.6865999698638916, f1 0.6335999965667725
ep1_l2_test_time 1.153342978000012
Test Epoch1 layer3 Acc 0.6158, AUC 0.6735043525695801, avg_entr 0.6968814730644226, f1 0.6158000230789185
ep1_l3_test_time 1.631620306000002
Test Epoch1 layer4 Acc 0.512, AUC 0.529548168182373, avg_entr 0.6961973905563354, f1 0.5120000243186951
ep1_l4_test_time 2.2671766919999925
gc 0
Train Epoch2 Acc 0.54405 (21762/40000), AUC 0.5635865926742554
ep2_train_time 65.702946269
Test Epoch2 layer0 Acc 0.7258, AUC 0.8063164949417114, avg_entr 0.49828842282295227, f1 0.7257999777793884
ep2_l0_test_time 0.6277589020000107
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 2
Test Epoch2 layer1 Acc 0.7142, AUC 0.809420645236969, avg_entr 0.5382360219955444, f1 0.7142000198364258
ep2_l1_test_time 0.8324272339999936
Test Epoch2 layer2 Acc 0.6762, AUC 0.8099795579910278, avg_entr 0.5700085759162903, f1 0.6761999726295471
ep2_l2_test_time 1.1502276409999865
Test Epoch2 layer3 Acc 0.6642, AUC 0.8083224892616272, avg_entr 0.6345455646514893, f1 0.6642000079154968
ep2_l3_test_time 1.630424879000003
Test Epoch2 layer4 Acc 0.7178, AUC 0.8029381036758423, avg_entr 0.6875548362731934, f1 0.7178000211715698
ep2_l4_test_time 2.2618090849999817
gc 0
Train Epoch3 Acc 0.6273 (25092/40000), AUC 0.6744903326034546
ep3_train_time 65.68733707299998
Test Epoch3 layer0 Acc 0.7538, AUC 0.8322076797485352, avg_entr 0.3992234468460083, f1 0.7537999749183655
ep3_l0_test_time 0.627318100000025
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 3
Test Epoch3 layer1 Acc 0.758, AUC 0.8369594812393188, avg_entr 0.4099418520927429, f1 0.7580000162124634
ep3_l1_test_time 0.8296469530000081
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 3
Test Epoch3 layer2 Acc 0.7562, AUC 0.8378906846046448, avg_entr 0.44438594579696655, f1 0.7561999559402466
ep3_l2_test_time 1.1490259459999947
Test Epoch3 layer3 Acc 0.7536, AUC 0.8382282257080078, avg_entr 0.4981391727924347, f1 0.7535999417304993
ep3_l3_test_time 1.6237896139999748
Test Epoch3 layer4 Acc 0.76, AUC 0.8385080099105835, avg_entr 0.5923752188682556, f1 0.7599999904632568
ep3_l4_test_time 2.2581266479999726
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 3
gc 0
Train Epoch4 Acc 0.7204 (28816/40000), AUC 0.7929331064224243
ep4_train_time 65.53346730600003
Test Epoch4 layer0 Acc 0.7584, AUC 0.8472638726234436, avg_entr 0.35406479239463806, f1 0.758400022983551
ep4_l0_test_time 0.6271270780000009
Test Epoch4 layer1 Acc 0.7742, AUC 0.8560980558395386, avg_entr 0.3489063084125519, f1 0.7742000222206116
ep4_l1_test_time 0.8226598880000324
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 4
Test Epoch4 layer2 Acc 0.7728, AUC 0.8577138185501099, avg_entr 0.3685908615589142, f1 0.7728000283241272
ep4_l2_test_time 1.150338440999974
Test Epoch4 layer3 Acc 0.7686, AUC 0.8574818968772888, avg_entr 0.3781931400299072, f1 0.7685999870300293
ep4_l3_test_time 1.6278693269999849
Test Epoch4 layer4 Acc 0.7686, AUC 0.8577824831008911, avg_entr 0.39772966504096985, f1 0.7685999870300293
ep4_l4_test_time 2.2625259420000248
gc 0
Train Epoch5 Acc 0.7704 (30816/40000), AUC 0.8551168441772461
ep5_train_time 65.65289598200002
Test Epoch5 layer0 Acc 0.7734, AUC 0.8574714064598083, avg_entr 0.32008981704711914, f1 0.7734000086784363
ep5_l0_test_time 0.6321340920000011
Test Epoch5 layer1 Acc 0.7828, AUC 0.8694444894790649, avg_entr 0.31674692034721375, f1 0.782800018787384
ep5_l1_test_time 0.8190071139999873
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 5
Test Epoch5 layer2 Acc 0.7878, AUC 0.8706377744674683, avg_entr 0.33294790983200073, f1 0.7877999544143677
ep5_l2_test_time 1.14833063399999
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 5
Test Epoch5 layer3 Acc 0.784, AUC 0.870279848575592, avg_entr 0.33489739894866943, f1 0.7839999794960022
ep5_l3_test_time 1.631100360000005
Test Epoch5 layer4 Acc 0.7852, AUC 0.8706228733062744, avg_entr 0.35303592681884766, f1 0.7851999998092651
ep5_l4_test_time 2.258302442999991
gc 0
Train Epoch6 Acc 0.780175 (31207/40000), AUC 0.8614144325256348
ep6_train_time 65.55467288
Test Epoch6 layer0 Acc 0.7572, AUC 0.8654763698577881, avg_entr 0.25767794251441956, f1 0.7572000026702881
ep6_l0_test_time 0.6286494189999985
Test Epoch6 layer1 Acc 0.7286, AUC 0.8779638409614563, avg_entr 0.21348223090171814, f1 0.728600025177002
ep6_l1_test_time 0.84000440300008
Test Epoch6 layer2 Acc 0.7044, AUC 0.8789764642715454, avg_entr 0.20178918540477753, f1 0.7044000029563904
ep6_l2_test_time 1.1458597910000208
Test Epoch6 layer3 Acc 0.6956, AUC 0.8788064122200012, avg_entr 0.19537968933582306, f1 0.6955999732017517
ep6_l3_test_time 1.6297833539999829
Test Epoch6 layer4 Acc 0.6828, AUC 0.8786289691925049, avg_entr 0.19767993688583374, f1 0.6827999949455261
ep6_l4_test_time 2.2647501489999513
gc 0
Train Epoch7 Acc 0.80995 (32398/40000), AUC 0.8896456956863403
ep7_train_time 65.6733921880001
Test Epoch7 layer0 Acc 0.785, AUC 0.8693444728851318, avg_entr 0.2731079161167145, f1 0.7850000262260437
ep7_l0_test_time 0.6316626190000534
Test Epoch7 layer1 Acc 0.7998, AUC 0.8836808204650879, avg_entr 0.23491667211055756, f1 0.7997999787330627
ep7_l1_test_time 0.8272304750000785
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 7
Test Epoch7 layer2 Acc 0.7996, AUC 0.8851566910743713, avg_entr 0.2036488950252533, f1 0.7996000051498413
ep7_l2_test_time 1.1547690749999902
Test Epoch7 layer3 Acc 0.7996, AUC 0.8852914571762085, avg_entr 0.19775234162807465, f1 0.7996000051498413
ep7_l3_test_time 1.6258859380000104
Test Epoch7 layer4 Acc 0.8002, AUC 0.8852850198745728, avg_entr 0.19934645295143127, f1 0.8001999855041504
ep7_l4_test_time 2.26260938300004
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 7
gc 0
Train Epoch8 Acc 0.82665 (33066/40000), AUC 0.9047839641571045
ep8_train_time 65.67463872199994
Test Epoch8 layer0 Acc 0.7736, AUC 0.8740048408508301, avg_entr 0.22147902846336365, f1 0.7735999822616577
ep8_l0_test_time 0.628977395999982
Test Epoch8 layer1 Acc 0.7926, AUC 0.889200747013092, avg_entr 0.1618080884218216, f1 0.7925999760627747
ep8_l1_test_time 0.8197687249999035
Test Epoch8 layer2 Acc 0.7922, AUC 0.8906792402267456, avg_entr 0.13719117641448975, f1 0.7922000288963318
ep8_l2_test_time 1.143429642000001
Test Epoch8 layer3 Acc 0.7914, AUC 0.8901481032371521, avg_entr 0.13104145228862762, f1 0.7914000153541565
ep8_l3_test_time 1.6277990359999421
Test Epoch8 layer4 Acc 0.7934, AUC 0.8900868892669678, avg_entr 0.1295756995677948, f1 0.79339998960495
ep8_l4_test_time 2.2630408070000385
gc 0
Train Epoch9 Acc 0.832775 (33311/40000), AUC 0.9098443984985352
ep9_train_time 65.859543486
Test Epoch9 layer0 Acc 0.7838, AUC 0.8733633756637573, avg_entr 0.20880180597305298, f1 0.7838000059127808
ep9_l0_test_time 0.6257089709999946
Test Epoch9 layer1 Acc 0.7938, AUC 0.8891056776046753, avg_entr 0.14804251492023468, f1 0.7937999963760376
ep9_l1_test_time 0.8214377499999728
Test Epoch9 layer2 Acc 0.7952, AUC 0.8904244303703308, avg_entr 0.13732992112636566, f1 0.7952000498771667
ep9_l2_test_time 1.1420670510000264
Test Epoch9 layer3 Acc 0.794, AUC 0.8899475336074829, avg_entr 0.13249041140079498, f1 0.7940000295639038
ep9_l3_test_time 1.6260844320000842
Test Epoch9 layer4 Acc 0.7974, AUC 0.8897647261619568, avg_entr 0.13697105646133423, f1 0.7973999977111816
ep9_l4_test_time 2.263538245999939
gc 0
Train Epoch10 Acc 0.844075 (33763/40000), AUC 0.9186356663703918
ep10_train_time 65.66737396200006
Test Epoch10 layer0 Acc 0.7948, AUC 0.878643274307251, avg_entr 0.21617534756660461, f1 0.7947999835014343
ep10_l0_test_time 0.6285786710000139
Test Epoch10 layer1 Acc 0.8144, AUC 0.8937517404556274, avg_entr 0.15486958622932434, f1 0.8144000172615051
ep10_l1_test_time 0.819632373000104
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 10
Test Epoch10 layer2 Acc 0.8126, AUC 0.8964580297470093, avg_entr 0.14722704887390137, f1 0.8126000165939331
ep10_l2_test_time 1.14946430100008
Test Epoch10 layer3 Acc 0.8128, AUC 0.8964231014251709, avg_entr 0.14630216360092163, f1 0.8127999901771545
ep10_l3_test_time 1.6316642530000536
Test Epoch10 layer4 Acc 0.8136, AUC 0.8964250683784485, avg_entr 0.148945152759552, f1 0.8136000037193298
ep10_l4_test_time 2.264194442999951
gc 0
Train Epoch11 Acc 0.858875 (34355/40000), AUC 0.929132342338562
ep11_train_time 65.71386120700004
Test Epoch11 layer0 Acc 0.7934, AUC 0.8773418664932251, avg_entr 0.20565129816532135, f1 0.79339998960495
ep11_l0_test_time 0.654193505999956
Test Epoch11 layer1 Acc 0.81, AUC 0.8910852670669556, avg_entr 0.12930680811405182, f1 0.809999942779541
ep11_l1_test_time 0.8193898019999324
Test Epoch11 layer2 Acc 0.8128, AUC 0.8942655324935913, avg_entr 0.12459638714790344, f1 0.8127999901771545
ep11_l2_test_time 1.1467701579999812
Test Epoch11 layer3 Acc 0.8128, AUC 0.8942180871963501, avg_entr 0.12427757680416107, f1 0.8127999901771545
ep11_l3_test_time 1.6274561989999938
Test Epoch11 layer4 Acc 0.8136, AUC 0.8941394090652466, avg_entr 0.1268310546875, f1 0.8136000037193298
ep11_l4_test_time 2.2644013069999573
gc 0
Train Epoch12 Acc 0.869275 (34771/40000), AUC 0.9380202293395996
ep12_train_time 65.79912929099999
Test Epoch12 layer0 Acc 0.7956, AUC 0.8782294988632202, avg_entr 0.19729569554328918, f1 0.7955999970436096
ep12_l0_test_time 0.6269457350000494
Test Epoch12 layer1 Acc 0.8132, AUC 0.8924587965011597, avg_entr 0.12569104135036469, f1 0.8131999969482422
ep12_l1_test_time 0.8213874829999668
Test Epoch12 layer2 Acc 0.8128, AUC 0.895849883556366, avg_entr 0.11702150851488113, f1 0.8127999901771545
ep12_l2_test_time 1.14241898399996
Test Epoch12 layer3 Acc 0.8112, AUC 0.8956384062767029, avg_entr 0.11411669850349426, f1 0.8112000226974487
ep12_l3_test_time 1.6271994369999447
Test Epoch12 layer4 Acc 0.8104, AUC 0.8956916332244873, avg_entr 0.11420540511608124, f1 0.8104000091552734
ep12_l4_test_time 2.2630169520000436
gc 0
Train Epoch13 Acc 0.877925 (35117/40000), AUC 0.9450055360794067
ep13_train_time 65.80397536299995
Test Epoch13 layer0 Acc 0.8016, AUC 0.8808087706565857, avg_entr 0.17858466506004333, f1 0.8015999794006348
ep13_l0_test_time 0.6263593720000245
Test Epoch13 layer1 Acc 0.8134, AUC 0.8946799039840698, avg_entr 0.10748984664678574, f1 0.8133999109268188
ep13_l1_test_time 0.8264463829998476
Test Epoch13 layer2 Acc 0.817, AUC 0.898118793964386, avg_entr 0.0973069965839386, f1 0.8169999718666077
ep13_l2_test_time 1.1680502110000361
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 13
Test Epoch13 layer3 Acc 0.8164, AUC 0.8979508876800537, avg_entr 0.09502590447664261, f1 0.8164000511169434
ep13_l3_test_time 1.647850968000057
Test Epoch13 layer4 Acc 0.8172, AUC 0.8978565335273743, avg_entr 0.09463273733854294, f1 0.8172000050544739
ep13_l4_test_time 2.2731914870000764
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 13
gc 0
Train Epoch14 Acc 0.887525 (35501/40000), AUC 0.9512273073196411
ep14_train_time 65.81072190999998
Test Epoch14 layer0 Acc 0.7904, AUC 0.8778645992279053, avg_entr 0.17655245959758759, f1 0.7904000282287598
ep14_l0_test_time 0.6357811479999782
Test Epoch14 layer1 Acc 0.799, AUC 0.8913683295249939, avg_entr 0.10269694775342941, f1 0.7990000247955322
ep14_l1_test_time 0.8246890249999979
Test Epoch14 layer2 Acc 0.7986, AUC 0.895007312297821, avg_entr 0.09336032718420029, f1 0.7986000180244446
ep14_l2_test_time 1.1519301079999877
Test Epoch14 layer3 Acc 0.7992, AUC 0.8948233127593994, avg_entr 0.09187018126249313, f1 0.7991999983787537
ep14_l3_test_time 1.633617662000006
Test Epoch14 layer4 Acc 0.7976, AUC 0.8946903944015503, avg_entr 0.09193305671215057, f1 0.7976000308990479
ep14_l4_test_time 2.266804959000183
gc 0
Train Epoch15 Acc 0.894675 (35787/40000), AUC 0.9564852118492126
ep15_train_time 65.64830882000001
Test Epoch15 layer0 Acc 0.777, AUC 0.8765961527824402, avg_entr 0.16333343088626862, f1 0.7770000100135803
ep15_l0_test_time 0.6271273260001635
Test Epoch15 layer1 Acc 0.791, AUC 0.889305055141449, avg_entr 0.08878377825021744, f1 0.7910000681877136
ep15_l1_test_time 0.8205974909999441
Test Epoch15 layer2 Acc 0.7942, AUC 0.8943971395492554, avg_entr 0.08339204639196396, f1 0.7942000031471252
ep15_l2_test_time 1.1446890170000188
Test Epoch15 layer3 Acc 0.7918, AUC 0.8942722082138062, avg_entr 0.0808543711900711, f1 0.7918000221252441
ep15_l3_test_time 1.6281332629998815
Test Epoch15 layer4 Acc 0.7902, AUC 0.8937059640884399, avg_entr 0.07955873012542725, f1 0.7901999950408936
ep15_l4_test_time 2.2634272710001824
gc 0
Train Epoch16 Acc 0.903425 (36137/40000), AUC 0.9609740972518921
ep16_train_time 65.72318368599986
Test Epoch16 layer0 Acc 0.7896, AUC 0.8743530511856079, avg_entr 0.16147826611995697, f1 0.7896000146865845
ep16_l0_test_time 0.6291363499999534
Test Epoch16 layer1 Acc 0.807, AUC 0.8892862796783447, avg_entr 0.08421169221401215, f1 0.8069999814033508
ep16_l1_test_time 0.8217434969999431
Test Epoch16 layer2 Acc 0.8072, AUC 0.8930450677871704, avg_entr 0.07809873670339584, f1 0.807200014591217
ep16_l2_test_time 1.1471619349999855
Test Epoch16 layer3 Acc 0.8078, AUC 0.8929738998413086, avg_entr 0.07531428337097168, f1 0.8077999353408813
ep16_l3_test_time 1.6266450910000003
Test Epoch16 layer4 Acc 0.8064, AUC 0.8931136131286621, avg_entr 0.07430373877286911, f1 0.8064000010490417
ep16_l4_test_time 2.2730463269999746
gc 0
Train Epoch17 Acc 0.911525 (36461/40000), AUC 0.9656145572662354
ep17_train_time 65.66041789199994
Test Epoch17 layer0 Acc 0.7892, AUC 0.8722617626190186, avg_entr 0.16290773451328278, f1 0.7892000079154968
ep17_l0_test_time 0.6274351810000098
Test Epoch17 layer1 Acc 0.8018, AUC 0.886168360710144, avg_entr 0.0831889882683754, f1 0.801800012588501
ep17_l1_test_time 0.8218689699999686
Test Epoch17 layer2 Acc 0.8018, AUC 0.8899686336517334, avg_entr 0.07433780282735825, f1 0.801800012588501
ep17_l2_test_time 1.1434182509999573
Test Epoch17 layer3 Acc 0.8024, AUC 0.890063464641571, avg_entr 0.07322806119918823, f1 0.8023999929428101
ep17_l3_test_time 1.6281417400000464
Test Epoch17 layer4 Acc 0.802, AUC 0.8892934322357178, avg_entr 0.07352324575185776, f1 0.8019999861717224
ep17_l4_test_time 2.263199597000039
gc 0
Train Epoch18 Acc 0.930675 (37227/40000), AUC 0.9781422019004822
ep18_train_time 65.6601830489999
Test Epoch18 layer0 Acc 0.792, AUC 0.868455708026886, avg_entr 0.15443339943885803, f1 0.7920000553131104
ep18_l0_test_time 0.6275165489998926
Test Epoch18 layer1 Acc 0.7976, AUC 0.8741867542266846, avg_entr 0.06963150948286057, f1 0.7976000308990479
ep18_l1_test_time 0.819732563000116
Test Epoch18 layer2 Acc 0.799, AUC 0.8799201250076294, avg_entr 0.06175466999411583, f1 0.7990000247955322
ep18_l2_test_time 1.14563117900002
Test Epoch18 layer3 Acc 0.799, AUC 0.8805751204490662, avg_entr 0.05936722457408905, f1 0.7990000247955322
ep18_l3_test_time 1.6381176100001085
Test Epoch18 layer4 Acc 0.7984, AUC 0.8798989057540894, avg_entr 0.05883295461535454, f1 0.7983999848365784
ep18_l4_test_time 2.2639121679999334
gc 0
Train Epoch19 Acc 0.93935 (37574/40000), AUC 0.9826086759567261
ep19_train_time 65.71031526299998
Test Epoch19 layer0 Acc 0.7918, AUC 0.8687341213226318, avg_entr 0.15400288999080658, f1 0.7918000221252441
ep19_l0_test_time 0.6290718380000726
Test Epoch19 layer1 Acc 0.8054, AUC 0.8793426752090454, avg_entr 0.06228135526180267, f1 0.805400013923645
ep19_l1_test_time 0.8201561930000025
Test Epoch19 layer2 Acc 0.8082, AUC 0.885901689529419, avg_entr 0.05543552339076996, f1 0.808199942111969
ep19_l2_test_time 1.1436058780000167
Test Epoch19 layer3 Acc 0.8082, AUC 0.8874921798706055, avg_entr 0.05333207547664642, f1 0.808199942111969
ep19_l3_test_time 1.6273896950001472
Test Epoch19 layer4 Acc 0.8082, AUC 0.8872488737106323, avg_entr 0.05253155529499054, f1 0.808199942111969
ep19_l4_test_time 2.2700595740000153
gc 0
Train Epoch20 Acc 0.943975 (37759/40000), AUC 0.9840891361236572
ep20_train_time 65.69251289399995
Test Epoch20 layer0 Acc 0.7836, AUC 0.8671084642410278, avg_entr 0.14280807971954346, f1 0.7835999727249146
ep20_l0_test_time 0.626438883999981
Test Epoch20 layer1 Acc 0.791, AUC 0.8733349442481995, avg_entr 0.05683756619691849, f1 0.7910000681877136
ep20_l1_test_time 0.821605707000117
Test Epoch20 layer2 Acc 0.7918, AUC 0.8824207782745361, avg_entr 0.04890136793255806, f1 0.7918000221252441
ep20_l2_test_time 1.1432997790000172
Test Epoch20 layer3 Acc 0.7914, AUC 0.8839945197105408, avg_entr 0.0456671342253685, f1 0.7914000153541565
ep20_l3_test_time 1.626629825000009
Test Epoch20 layer4 Acc 0.791, AUC 0.8832640647888184, avg_entr 0.04443565383553505, f1 0.7910000681877136
ep20_l4_test_time 2.263127802999861
gc 0
Train Epoch21 Acc 0.949625 (37985/40000), AUC 0.9871603846549988
ep21_train_time 65.74744329100008
Test Epoch21 layer0 Acc 0.7784, AUC 0.8635299205780029, avg_entr 0.14790014922618866, f1 0.7784000039100647
ep21_l0_test_time 0.6269752489999973
Test Epoch21 layer1 Acc 0.785, AUC 0.8617323040962219, avg_entr 0.056000083684921265, f1 0.7850000262260437
ep21_l1_test_time 0.8210323180001069
Test Epoch21 layer2 Acc 0.7884, AUC 0.8719003796577454, avg_entr 0.04988083243370056, f1 0.7884000539779663
ep21_l2_test_time 1.1454406450000079
Test Epoch21 layer3 Acc 0.7898, AUC 0.8734016418457031, avg_entr 0.047453392297029495, f1 0.7897999882698059
ep21_l3_test_time 1.6328627520001646
Test Epoch21 layer4 Acc 0.7892, AUC 0.8722126483917236, avg_entr 0.04751019552350044, f1 0.7892000079154968
ep21_l4_test_time 2.2611047410000538
gc 0
Train Epoch22 Acc 0.9556 (38224/40000), AUC 0.9895240068435669
ep22_train_time 65.70447796599979
Test Epoch22 layer0 Acc 0.7852, AUC 0.8614275455474854, avg_entr 0.14713440835475922, f1 0.7851999998092651
ep22_l0_test_time 0.6282007229999635
Test Epoch22 layer1 Acc 0.795, AUC 0.8605707883834839, avg_entr 0.05458613857626915, f1 0.7950000166893005
ep22_l1_test_time 0.8195496589999038
Test Epoch22 layer2 Acc 0.7966, AUC 0.8705662488937378, avg_entr 0.04595846310257912, f1 0.7965999841690063
ep22_l2_test_time 1.1440185649998966
Test Epoch22 layer3 Acc 0.7964, AUC 0.8734887838363647, avg_entr 0.042735859751701355, f1 0.7964000105857849
ep22_l3_test_time 1.6259787999999844
Test Epoch22 layer4 Acc 0.7964, AUC 0.8731392025947571, avg_entr 0.04232020303606987, f1 0.7964000105857849
ep22_l4_test_time 2.263228465999873
gc 0
Train Epoch23 Acc 0.95915 (38366/40000), AUC 0.9905539751052856
ep23_train_time 65.658134883
Test Epoch23 layer0 Acc 0.784, AUC 0.8601489067077637, avg_entr 0.14368772506713867, f1 0.7839999794960022
ep23_l0_test_time 0.6265842170000724
Test Epoch23 layer1 Acc 0.7964, AUC 0.8563404083251953, avg_entr 0.050469402223825455, f1 0.7964000105857849
ep23_l1_test_time 0.8216742849999719
Test Epoch23 layer2 Acc 0.7976, AUC 0.8683193922042847, avg_entr 0.04285556077957153, f1 0.7976000308990479
ep23_l2_test_time 1.145027841000001
Test Epoch23 layer3 Acc 0.7978, AUC 0.8718382120132446, avg_entr 0.040414147078990936, f1 0.7978000044822693
ep23_l3_test_time 1.6309062739999263
Test Epoch23 layer4 Acc 0.798, AUC 0.8711395859718323, avg_entr 0.040041904896497726, f1 0.7979999780654907
ep23_l4_test_time 2.2647405390000586
gc 0
Train Epoch24 Acc 0.96185 (38474/40000), AUC 0.9917200803756714
ep24_train_time 65.7524814809999
Test Epoch24 layer0 Acc 0.7826, AUC 0.858196496963501, avg_entr 0.14531174302101135, f1 0.7825999855995178
ep24_l0_test_time 0.6284302250001019
Test Epoch24 layer1 Acc 0.7916, AUC 0.852651834487915, avg_entr 0.049245841801166534, f1 0.7915999889373779
ep24_l1_test_time 0.8210921770000823
Test Epoch24 layer2 Acc 0.7924, AUC 0.8647937774658203, avg_entr 0.042521845549345016, f1 0.7923999428749084
ep24_l2_test_time 1.1461869600000227
Test Epoch24 layer3 Acc 0.7928, AUC 0.8699101209640503, avg_entr 0.0398678183555603, f1 0.7928000092506409
ep24_l3_test_time 1.6297419259999515
Test Epoch24 layer4 Acc 0.7922, AUC 0.8690686225891113, avg_entr 0.03948090970516205, f1 0.7922000288963318
ep24_l4_test_time 2.26368861800006
gc 0
Train Epoch25 Acc 0.961425 (38457/40000), AUC 0.9917525053024292
ep25_train_time 65.70884628700014
Test Epoch25 layer0 Acc 0.7796, AUC 0.8581954836845398, avg_entr 0.14305563271045685, f1 0.7796000838279724
ep25_l0_test_time 0.6286062060000859
Test Epoch25 layer1 Acc 0.7898, AUC 0.8511554598808289, avg_entr 0.04650961235165596, f1 0.7897999882698059
ep25_l1_test_time 0.8227969420001955
Test Epoch25 layer2 Acc 0.7898, AUC 0.8663326501846313, avg_entr 0.04153510928153992, f1 0.7897999882698059
ep25_l2_test_time 1.1440184839998437
Test Epoch25 layer3 Acc 0.7898, AUC 0.8703190088272095, avg_entr 0.039475880563259125, f1 0.7897999882698059
ep25_l3_test_time 1.6258501289999003
Test Epoch25 layer4 Acc 0.7886, AUC 0.8699483871459961, avg_entr 0.03959067538380623, f1 0.7886000275611877
ep25_l4_test_time 2.2659068509999543
gc 0
Train Epoch26 Acc 0.9672 (38688/40000), AUC 0.993411660194397
ep26_train_time 65.76934724000012
Test Epoch26 layer0 Acc 0.7842, AUC 0.8572194576263428, avg_entr 0.13913419842720032, f1 0.7842000126838684
ep26_l0_test_time 0.6287377109999852
Test Epoch26 layer1 Acc 0.7874, AUC 0.8475556373596191, avg_entr 0.046053532510995865, f1 0.7874000072479248
ep26_l1_test_time 0.8223736669999653
Test Epoch26 layer2 Acc 0.79, AUC 0.8637012243270874, avg_entr 0.03951789066195488, f1 0.7900000214576721
ep26_l2_test_time 1.1520896869999433
Test Epoch26 layer3 Acc 0.7898, AUC 0.8668968677520752, avg_entr 0.037143103778362274, f1 0.7897999882698059
ep26_l3_test_time 1.628612152000187
Test Epoch26 layer4 Acc 0.79, AUC 0.8664711713790894, avg_entr 0.03677354007959366, f1 0.7900000214576721
ep26_l4_test_time 2.2642493470000318
gc 0
Train Epoch27 Acc 0.9675 (38700/40000), AUC 0.9938278198242188
ep27_train_time 65.67178288399987
Test Epoch27 layer0 Acc 0.78, AUC 0.8564432263374329, avg_entr 0.13956677913665771, f1 0.7799999117851257
ep27_l0_test_time 0.6276994479999303
Test Epoch27 layer1 Acc 0.7868, AUC 0.8501265645027161, avg_entr 0.046647265553474426, f1 0.786799967288971
ep27_l1_test_time 0.8206690399999843
Test Epoch27 layer2 Acc 0.7892, AUC 0.8630280494689941, avg_entr 0.03951290249824524, f1 0.7892000079154968
ep27_l2_test_time 1.1455430700000306
Test Epoch27 layer3 Acc 0.7892, AUC 0.8661476373672485, avg_entr 0.036097005009651184, f1 0.7892000079154968
ep27_l3_test_time 1.6282652439999765
Test Epoch27 layer4 Acc 0.789, AUC 0.865820050239563, avg_entr 0.03569592535495758, f1 0.7889999747276306
ep27_l4_test_time 2.2636276749999524
gc 0
Train Epoch28 Acc 0.968625 (38745/40000), AUC 0.9940673112869263
ep28_train_time 65.7168988530002
Test Epoch28 layer0 Acc 0.7828, AUC 0.8561775088310242, avg_entr 0.13869789242744446, f1 0.782800018787384
ep28_l0_test_time 0.6296897790002731
Test Epoch28 layer1 Acc 0.7838, AUC 0.8470379114151001, avg_entr 0.04425528645515442, f1 0.7838000059127808
ep28_l1_test_time 0.8239331100003255
Test Epoch28 layer2 Acc 0.7874, AUC 0.8606534600257874, avg_entr 0.037463653832674026, f1 0.7874000072479248
ep28_l2_test_time 1.1567053570001917
Test Epoch28 layer3 Acc 0.7874, AUC 0.8643683195114136, avg_entr 0.034899238497018814, f1 0.7874000072479248
ep28_l3_test_time 1.6342043039999226
Test Epoch28 layer4 Acc 0.7876, AUC 0.8638908863067627, avg_entr 0.034627556800842285, f1 0.7875999212265015
ep28_l4_test_time 2.263848065000275
gc 0
Train Epoch29 Acc 0.968325 (38733/40000), AUC 0.9939955472946167
ep29_train_time 65.68131228500033
Test Epoch29 layer0 Acc 0.78, AUC 0.8546534180641174, avg_entr 0.13933776319026947, f1 0.7799999117851257
ep29_l0_test_time 0.6291311739996672
Test Epoch29 layer1 Acc 0.7814, AUC 0.8462731838226318, avg_entr 0.04384450241923332, f1 0.7814000248908997
ep29_l1_test_time 0.8272109909999017
Test Epoch29 layer2 Acc 0.7828, AUC 0.8587284088134766, avg_entr 0.03782224282622337, f1 0.782800018787384
ep29_l2_test_time 1.1503093960000115
Test Epoch29 layer3 Acc 0.784, AUC 0.8630464673042297, avg_entr 0.03554858639836311, f1 0.7839999794960022
ep29_l3_test_time 1.6344042670002636
Test Epoch29 layer4 Acc 0.7842, AUC 0.8625861406326294, avg_entr 0.03552640601992607, f1 0.7842000126838684
ep29_l4_test_time 2.26432534200012
gc 0
Train Epoch30 Acc 0.9716 (38864/40000), AUC 0.9950128197669983
ep30_train_time 65.702295048
Test Epoch30 layer0 Acc 0.7786, AUC 0.8541655540466309, avg_entr 0.14224815368652344, f1 0.7785999774932861
ep30_l0_test_time 0.626438397000129
Test Epoch30 layer1 Acc 0.7822, AUC 0.8438800573348999, avg_entr 0.044192105531692505, f1 0.7821999788284302
ep30_l1_test_time 0.8194823239996367
Test Epoch30 layer2 Acc 0.7844, AUC 0.8573246598243713, avg_entr 0.03761647641658783, f1 0.7843999266624451
ep30_l2_test_time 1.1461402559998533
Test Epoch30 layer3 Acc 0.7842, AUC 0.8617278337478638, avg_entr 0.03505437821149826, f1 0.7842000126838684
ep30_l3_test_time 1.6280332960000123
Test Epoch30 layer4 Acc 0.7844, AUC 0.8613138198852539, avg_entr 0.034949228167533875, f1 0.7843999266624451
ep30_l4_test_time 2.2629343879998487
gc 0
Train Epoch31 Acc 0.97165 (38866/40000), AUC 0.9948995113372803
ep31_train_time 65.68574282600002
Test Epoch31 layer0 Acc 0.7794, AUC 0.8540389537811279, avg_entr 0.13985194265842438, f1 0.7794000506401062
ep31_l0_test_time 0.6378065520002565
Test Epoch31 layer1 Acc 0.7828, AUC 0.8417244553565979, avg_entr 0.044995177537202835, f1 0.782800018787384
ep31_l1_test_time 0.8186075730000084
Test Epoch31 layer2 Acc 0.7836, AUC 0.8570067882537842, avg_entr 0.03868937864899635, f1 0.7835999727249146
ep31_l2_test_time 1.1434461899998496
Test Epoch31 layer3 Acc 0.7834, AUC 0.8618324995040894, avg_entr 0.035741399973630905, f1 0.7833999395370483
ep31_l3_test_time 1.6264067829997657
Test Epoch31 layer4 Acc 0.784, AUC 0.8615291714668274, avg_entr 0.035503387451171875, f1 0.7839999794960022
ep31_l4_test_time 2.2743934439999975
gc 0
Train Epoch32 Acc 0.971825 (38873/40000), AUC 0.9950234293937683
ep32_train_time 65.70184754699994
Test Epoch32 layer0 Acc 0.7792, AUC 0.8539872169494629, avg_entr 0.13850714266300201, f1 0.7791999578475952
ep32_l0_test_time 0.6279189880001468
Test Epoch32 layer1 Acc 0.7816, AUC 0.8427531719207764, avg_entr 0.04321588948369026, f1 0.7815999984741211
ep32_l1_test_time 0.8240019400000165
Test Epoch32 layer2 Acc 0.7852, AUC 0.858533501625061, avg_entr 0.03628972917795181, f1 0.7851999998092651
ep32_l2_test_time 1.1445114470002409
Test Epoch32 layer3 Acc 0.7842, AUC 0.8625484108924866, avg_entr 0.03272479772567749, f1 0.7842000126838684
ep32_l3_test_time 1.62840988500011
Test Epoch32 layer4 Acc 0.7846, AUC 0.8624920845031738, avg_entr 0.03248981758952141, f1 0.784600019454956
ep32_l4_test_time 2.2648835909999434
gc 0
Train Epoch33 Acc 0.972075 (38883/40000), AUC 0.9950746297836304
ep33_train_time 65.69030200899988
Test Epoch33 layer0 Acc 0.7788, AUC 0.8531417846679688, avg_entr 0.13964179158210754, f1 0.7788000106811523
ep33_l0_test_time 0.6264141959995868
Test Epoch33 layer1 Acc 0.7816, AUC 0.8372950553894043, avg_entr 0.04205459728837013, f1 0.7815999984741211
ep33_l1_test_time 0.8197555159999865
Test Epoch33 layer2 Acc 0.7824, AUC 0.853728175163269, avg_entr 0.036059025675058365, f1 0.7824000120162964
ep33_l2_test_time 1.145632005999687
Test Epoch33 layer3 Acc 0.7852, AUC 0.8597560524940491, avg_entr 0.033198364078998566, f1 0.7851999998092651
ep33_l3_test_time 1.6338700070000414
Test Epoch33 layer4 Acc 0.7852, AUC 0.8590865135192871, avg_entr 0.03283950686454773, f1 0.7851999998092651
ep33_l4_test_time 2.268811517999893
gc 0
Train Epoch34 Acc 0.97285 (38914/40000), AUC 0.9954014420509338
ep34_train_time 65.7453213849999
Test Epoch34 layer0 Acc 0.7808, AUC 0.8533077239990234, avg_entr 0.138474702835083, f1 0.7808000445365906
ep34_l0_test_time 0.6310198670003047
Test Epoch34 layer1 Acc 0.7794, AUC 0.8386819362640381, avg_entr 0.041120804846286774, f1 0.7794000506401062
ep34_l1_test_time 0.8303351590002421
Test Epoch34 layer2 Acc 0.7816, AUC 0.8549913167953491, avg_entr 0.03509807959198952, f1 0.7815999984741211
ep34_l2_test_time 1.1541488030002256
Test Epoch34 layer3 Acc 0.7816, AUC 0.8602644205093384, avg_entr 0.032145820558071136, f1 0.7815999984741211
ep34_l3_test_time 1.634916355000314
Test Epoch34 layer4 Acc 0.7826, AUC 0.8597306609153748, avg_entr 0.03187946602702141, f1 0.7825999855995178
ep34_l4_test_time 2.267241097000351
gc 0
Train Epoch35 Acc 0.9735 (38940/40000), AUC 0.9957479238510132
ep35_train_time 65.73914279500013
Test Epoch35 layer0 Acc 0.7796, AUC 0.8539217710494995, avg_entr 0.13730944693088531, f1 0.7796000838279724
ep35_l0_test_time 0.6362306320002062
Test Epoch35 layer1 Acc 0.784, AUC 0.8409099578857422, avg_entr 0.041880641132593155, f1 0.7839999794960022
ep35_l1_test_time 0.8231537560000106
Test Epoch35 layer2 Acc 0.7842, AUC 0.8576536178588867, avg_entr 0.03501592576503754, f1 0.7842000126838684
ep35_l2_test_time 1.1436332499997661
Test Epoch35 layer3 Acc 0.7854, AUC 0.8626081943511963, avg_entr 0.031907930970191956, f1 0.7853999733924866
ep35_l3_test_time 1.6297253920001822
Test Epoch35 layer4 Acc 0.7854, AUC 0.8621824383735657, avg_entr 0.03155973553657532, f1 0.7853999733924866
ep35_l4_test_time 2.2628420979999646
gc 0
Train Epoch36 Acc 0.973925 (38957/40000), AUC 0.9954224824905396
ep36_train_time 65.71353966600009
Test Epoch36 layer0 Acc 0.7786, AUC 0.8535558581352234, avg_entr 0.13786616921424866, f1 0.7785999774932861
ep36_l0_test_time 0.6366361369996412
Test Epoch36 layer1 Acc 0.7816, AUC 0.8401427268981934, avg_entr 0.042013052850961685, f1 0.7815999984741211
ep36_l1_test_time 0.8245590759997867
Test Epoch36 layer2 Acc 0.783, AUC 0.8555988073348999, avg_entr 0.03512122109532356, f1 0.7829999923706055
ep36_l2_test_time 1.1440137790000335
Test Epoch36 layer3 Acc 0.7826, AUC 0.8610942363739014, avg_entr 0.031982626765966415, f1 0.7825999855995178
ep36_l3_test_time 1.6289554859999953
Test Epoch36 layer4 Acc 0.782, AUC 0.8608177900314331, avg_entr 0.03178513050079346, f1 0.7820000052452087
ep36_l4_test_time 2.2716558849997455
gc 0
Train Epoch37 Acc 0.973425 (38937/40000), AUC 0.9957213401794434
ep37_train_time 65.80332642400026
Test Epoch37 layer0 Acc 0.7782, AUC 0.8528250455856323, avg_entr 0.13843269646167755, f1 0.7781999707221985
ep37_l0_test_time 0.6267536039999868
Test Epoch37 layer1 Acc 0.781, AUC 0.8393503427505493, avg_entr 0.04129670560359955, f1 0.781000018119812
ep37_l1_test_time 0.8216792240000359
Test Epoch37 layer2 Acc 0.7824, AUC 0.8542962670326233, avg_entr 0.03381125256419182, f1 0.7824000120162964
ep37_l2_test_time 1.1448890259998734
Test Epoch37 layer3 Acc 0.7824, AUC 0.8601644039154053, avg_entr 0.03041820414364338, f1 0.7824000120162964
ep37_l3_test_time 1.6270408119999047
Test Epoch37 layer4 Acc 0.7826, AUC 0.8596495985984802, avg_entr 0.03005455620586872, f1 0.7825999855995178
ep37_l4_test_time 2.2632269910000105
gc 0
Train Epoch38 Acc 0.974375 (38975/40000), AUC 0.9958677291870117
ep38_train_time 65.79132413200023
Test Epoch38 layer0 Acc 0.7782, AUC 0.8530372381210327, avg_entr 0.13730038702487946, f1 0.7781999707221985
ep38_l0_test_time 0.6280994129997453
Test Epoch38 layer1 Acc 0.7816, AUC 0.8403294086456299, avg_entr 0.04092160984873772, f1 0.7815999984741211
ep38_l1_test_time 0.8216089659999852
Test Epoch38 layer2 Acc 0.7834, AUC 0.8550034165382385, avg_entr 0.034874189645051956, f1 0.7833999395370483
ep38_l2_test_time 1.1487516040001537
Test Epoch38 layer3 Acc 0.7842, AUC 0.8613268136978149, avg_entr 0.03191614896059036, f1 0.7842000126838684
ep38_l3_test_time 1.62809216200003
Test Epoch38 layer4 Acc 0.7844, AUC 0.8610134124755859, avg_entr 0.03168318048119545, f1 0.7843999266624451
ep38_l4_test_time 2.262488937999933
gc 0
Train Epoch39 Acc 0.9736 (38944/40000), AUC 0.9956281185150146
ep39_train_time 65.73588989600012
Test Epoch39 layer0 Acc 0.7794, AUC 0.852858304977417, avg_entr 0.13734634220600128, f1 0.7794000506401062
ep39_l0_test_time 0.6264771250002923
Test Epoch39 layer1 Acc 0.7818, AUC 0.8399418592453003, avg_entr 0.04020684212446213, f1 0.7817999720573425
ep39_l1_test_time 0.8241527749996749
Test Epoch39 layer2 Acc 0.7848, AUC 0.8544988632202148, avg_entr 0.03452783077955246, f1 0.7847999930381775
ep39_l2_test_time 1.1504059889998643
Test Epoch39 layer3 Acc 0.7852, AUC 0.861026406288147, avg_entr 0.0313507504761219, f1 0.7851999998092651
ep39_l3_test_time 1.6281280530001823
Test Epoch39 layer4 Acc 0.7854, AUC 0.8605943918228149, avg_entr 0.03106594830751419, f1 0.7853999733924866
ep39_l4_test_time 2.2628096579996964
gc 0
Train Epoch40 Acc 0.9733 (38932/40000), AUC 0.9957394599914551
ep40_train_time 65.73858445300039
Test Epoch40 layer0 Acc 0.778, AUC 0.8527623414993286, avg_entr 0.1372711956501007, f1 0.777999997138977
ep40_l0_test_time 0.6407905969999774
Test Epoch40 layer1 Acc 0.7816, AUC 0.839225172996521, avg_entr 0.04136143997311592, f1 0.7815999984741211
ep40_l1_test_time 0.8212348870001733
Test Epoch40 layer2 Acc 0.7832, AUC 0.8547090888023376, avg_entr 0.03519700467586517, f1 0.7832000851631165
ep40_l2_test_time 1.1450979409996762
Test Epoch40 layer3 Acc 0.7854, AUC 0.8605934381484985, avg_entr 0.03224063664674759, f1 0.7853999733924866
ep40_l3_test_time 1.6271112110002832
Test Epoch40 layer4 Acc 0.785, AUC 0.8603959083557129, avg_entr 0.03204774111509323, f1 0.7850000262260437
ep40_l4_test_time 2.2646898809998675
gc 0
Train Epoch41 Acc 0.974575 (38983/40000), AUC 0.9961564540863037
ep41_train_time 65.69142452300002
Test Epoch41 layer0 Acc 0.779, AUC 0.8527262210845947, avg_entr 0.1367957442998886, f1 0.7789999842643738
ep41_l0_test_time 0.6292173349997938
Test Epoch41 layer1 Acc 0.7798, AUC 0.8385077714920044, avg_entr 0.040725428611040115, f1 0.7797999978065491
ep41_l1_test_time 0.824710191000122
Test Epoch41 layer2 Acc 0.7818, AUC 0.8545317649841309, avg_entr 0.03421630337834358, f1 0.7817999720573425
ep41_l2_test_time 1.1461325649997889
Test Epoch41 layer3 Acc 0.783, AUC 0.8601344227790833, avg_entr 0.03123599849641323, f1 0.7829999923706055
ep41_l3_test_time 1.6259979510000448
Test Epoch41 layer4 Acc 0.783, AUC 0.8597476482391357, avg_entr 0.03092314675450325, f1 0.7829999923706055
ep41_l4_test_time 2.2653005699999085
gc 0
Train Epoch42 Acc 0.9731 (38924/40000), AUC 0.9958982467651367
ep42_train_time 65.67510201100004
Test Epoch42 layer0 Acc 0.778, AUC 0.8527539372444153, avg_entr 0.136297345161438, f1 0.777999997138977
ep42_l0_test_time 0.6268294090000381
Test Epoch42 layer1 Acc 0.7806, AUC 0.8382364511489868, avg_entr 0.04052821546792984, f1 0.7806000113487244
ep42_l1_test_time 0.8215675319997899
Test Epoch42 layer2 Acc 0.782, AUC 0.8544133305549622, avg_entr 0.034022506326436996, f1 0.7820000052452087
ep42_l2_test_time 1.1431344830002672
Test Epoch42 layer3 Acc 0.7818, AUC 0.8604412078857422, avg_entr 0.03096354380249977, f1 0.7817999720573425
ep42_l3_test_time 1.635686674000226
Test Epoch42 layer4 Acc 0.7818, AUC 0.8601164221763611, avg_entr 0.030682068318128586, f1 0.7817999720573425
ep42_l4_test_time 2.272105202000148
gc 0
Train Epoch43 Acc 0.974375 (38975/40000), AUC 0.9961022138595581
ep43_train_time 65.75775528800023
Test Epoch43 layer0 Acc 0.7772, AUC 0.8524928092956543, avg_entr 0.13715378940105438, f1 0.777199923992157
ep43_l0_test_time 0.6269624379997367
Test Epoch43 layer1 Acc 0.7814, AUC 0.8382143378257751, avg_entr 0.04037783294916153, f1 0.7814000248908997
ep43_l1_test_time 0.8312471869999172
Test Epoch43 layer2 Acc 0.7834, AUC 0.8534746766090393, avg_entr 0.03433513268828392, f1 0.7833999395370483
ep43_l2_test_time 1.1465908929999387
Test Epoch43 layer3 Acc 0.7834, AUC 0.8602625727653503, avg_entr 0.031550414860248566, f1 0.7833999395370483
ep43_l3_test_time 1.6294928530001016
Test Epoch43 layer4 Acc 0.7836, AUC 0.8599174618721008, avg_entr 0.03134807571768761, f1 0.7835999727249146
ep43_l4_test_time 2.26379099199994
gc 0
Train Epoch44 Acc 0.9752 (39008/40000), AUC 0.9961367845535278
ep44_train_time 65.77236162400004
Test Epoch44 layer0 Acc 0.7774, AUC 0.8523775339126587, avg_entr 0.1364612877368927, f1 0.777400016784668
ep44_l0_test_time 0.6383967930000836
Test Epoch44 layer1 Acc 0.7804, AUC 0.8389209508895874, avg_entr 0.040315814316272736, f1 0.7803999781608582
ep44_l1_test_time 0.8208640509997167
Test Epoch44 layer2 Acc 0.7842, AUC 0.8535724878311157, avg_entr 0.03440920636057854, f1 0.7842000126838684
ep44_l2_test_time 1.1453082080001877
Test Epoch44 layer3 Acc 0.785, AUC 0.8603338599205017, avg_entr 0.03152346983551979, f1 0.7850000262260437
ep44_l3_test_time 1.6260385310001766
Test Epoch44 layer4 Acc 0.7852, AUC 0.8600572943687439, avg_entr 0.03129379078745842, f1 0.7851999998092651
ep44_l4_test_time 2.263528201999634
gc 0
Train Epoch45 Acc 0.97395 (38958/40000), AUC 0.9960792660713196
ep45_train_time 65.75115836699979
Test Epoch45 layer0 Acc 0.7794, AUC 0.8526892066001892, avg_entr 0.13605643808841705, f1 0.7794000506401062
ep45_l0_test_time 0.6267990540000028
Test Epoch45 layer1 Acc 0.781, AUC 0.83866286277771, avg_entr 0.040752820670604706, f1 0.781000018119812
ep45_l1_test_time 0.8217914949996157
Test Epoch45 layer2 Acc 0.783, AUC 0.8536676168441772, avg_entr 0.03464336693286896, f1 0.7829999923706055
ep45_l2_test_time 1.1419959729996663
Test Epoch45 layer3 Acc 0.7828, AUC 0.8603267073631287, avg_entr 0.03175736963748932, f1 0.782800018787384
ep45_l3_test_time 1.6289116699999795
Test Epoch45 layer4 Acc 0.7828, AUC 0.860171914100647, avg_entr 0.03155054897069931, f1 0.782800018787384
ep45_l4_test_time 2.2651868699999795
gc 0
Train Epoch46 Acc 0.97545 (39018/40000), AUC 0.9963420629501343
ep46_train_time 65.72546516400007
Test Epoch46 layer0 Acc 0.7794, AUC 0.8526442050933838, avg_entr 0.13614895939826965, f1 0.7794000506401062
ep46_l0_test_time 0.6270519940003396
Test Epoch46 layer1 Acc 0.7814, AUC 0.8388400077819824, avg_entr 0.04093929007649422, f1 0.7814000248908997
ep46_l1_test_time 0.8203333430001294
Test Epoch46 layer2 Acc 0.7822, AUC 0.8537493944168091, avg_entr 0.03460073098540306, f1 0.7821999788284302
ep46_l2_test_time 1.143047827999908
Test Epoch46 layer3 Acc 0.7828, AUC 0.8602167963981628, avg_entr 0.0317004956305027, f1 0.782800018787384
ep46_l3_test_time 1.6275640230001045
Test Epoch46 layer4 Acc 0.7832, AUC 0.8600984811782837, avg_entr 0.03148917108774185, f1 0.7832000851631165
ep46_l4_test_time 2.2628053130001717
gc 0
Train Epoch47 Acc 0.9757 (39028/40000), AUC 0.996148407459259
ep47_train_time 65.76176865599973
Test Epoch47 layer0 Acc 0.7784, AUC 0.852638840675354, avg_entr 0.13600634038448334, f1 0.7784000039100647
ep47_l0_test_time 0.6356548530002328
Test Epoch47 layer1 Acc 0.7818, AUC 0.8388106822967529, avg_entr 0.040572866797447205, f1 0.7817999720573425
ep47_l1_test_time 0.8215841720002572
Test Epoch47 layer2 Acc 0.783, AUC 0.853736162185669, avg_entr 0.03419589623808861, f1 0.7829999923706055
ep47_l2_test_time 1.1462392859998545
Test Epoch47 layer3 Acc 0.7834, AUC 0.8602834343910217, avg_entr 0.031283777207136154, f1 0.7833999395370483
ep47_l3_test_time 1.628310959999908
Test Epoch47 layer4 Acc 0.783, AUC 0.8601508140563965, avg_entr 0.031006867066025734, f1 0.7829999923706055
ep47_l4_test_time 2.2622769120002886
gc 0
Train Epoch48 Acc 0.976225 (39049/40000), AUC 0.9963136911392212
ep48_train_time 65.69541225000012
Test Epoch48 layer0 Acc 0.7788, AUC 0.8524233102798462, avg_entr 0.13623543083667755, f1 0.7788000106811523
ep48_l0_test_time 0.62771656599989
Test Epoch48 layer1 Acc 0.781, AUC 0.8380368947982788, avg_entr 0.0406784750521183, f1 0.781000018119812
ep48_l1_test_time 0.8214299490000485
Test Epoch48 layer2 Acc 0.7814, AUC 0.8530151844024658, avg_entr 0.03414158150553703, f1 0.7814000248908997
ep48_l2_test_time 1.1451458069996079
Test Epoch48 layer3 Acc 0.7822, AUC 0.8597714900970459, avg_entr 0.031211309134960175, f1 0.7821999788284302
ep48_l3_test_time 1.627121845000147
Test Epoch48 layer4 Acc 0.7824, AUC 0.8595799207687378, avg_entr 0.03094489686191082, f1 0.7824000120162964
ep48_l4_test_time 2.2636129349998555
gc 0
Train Epoch49 Acc 0.9752 (39008/40000), AUC 0.9961937665939331
ep49_train_time 65.91758107199985
Test Epoch49 layer0 Acc 0.7784, AUC 0.8524249792098999, avg_entr 0.13607257604599, f1 0.7784000039100647
ep49_l0_test_time 0.6310546939998858
Test Epoch49 layer1 Acc 0.781, AUC 0.8381948471069336, avg_entr 0.040390029549598694, f1 0.781000018119812
ep49_l1_test_time 0.8290975399995659
Test Epoch49 layer2 Acc 0.7818, AUC 0.853263258934021, avg_entr 0.03368964418768883, f1 0.7817999720573425
ep49_l2_test_time 1.1442549710000094
Test Epoch49 layer3 Acc 0.7824, AUC 0.859889030456543, avg_entr 0.030645519495010376, f1 0.7824000120162964
ep49_l3_test_time 1.627551480999955
Test Epoch49 layer4 Acc 0.7824, AUC 0.8596682548522949, avg_entr 0.030345169827342033, f1 0.7824000120162964
ep49_l4_test_time 2.26366228400002
Best AUC tensor(0.8172) 13 4
train_as_loss [[9.12121373e+01 6.04357246e+01 5.24943210e+01 5.06320062e+01
  4.99794995e+01 4.96818180e+01 4.95223250e+01 4.94272304e+01
  4.93661259e+01 4.93246008e+01 4.92951427e+01 4.92735211e+01
  4.92572076e+01 4.92446109e+01 4.92347002e+01 4.92267730e+01
  4.92203442e+01 4.92150678e+01 4.92116707e+01 4.92096058e+01
  4.92076475e+01 4.92058007e+01 4.92044748e+01 4.92035998e+01
  4.92027202e+01 4.92018412e+01 4.92011793e+01 4.92007253e+01
  4.92002574e+01 4.91997739e+01 4.91994013e+01 4.91991387e+01
  4.91988656e+01 4.91985781e+01 4.91983525e+01 4.91981939e+01
  4.91980249e+01 4.91978458e+01 4.91977041e+01 4.91976010e+01
  4.91974954e+01 4.91973791e+01 4.91972865e+01 4.91972245e+01
  4.91971495e+01 4.91970794e+01 4.91970155e+01 4.91969740e+01
  4.91969333e+01 4.91968806e+01]
 [2.18416974e+00 4.98459150e-04 2.11196343e-05 5.01795765e-06
  1.87865968e-06 8.82276909e-07 4.92309979e-07 2.79596505e-07
  1.75669854e-07 1.13545713e-07 7.63410767e-08 5.29222201e-08
  3.93143565e-08 1.70826092e-07 2.76935625e-07 1.57963771e-07
  5.33497369e-07 3.26468483e-07 8.54049449e-09 6.70086927e-09
  5.78762284e-09 4.93497761e-09 4.49556871e-09 4.12953766e-09
  3.81228325e-09 3.49657640e-09 3.25995829e-09 3.09857516e-09
  2.96315250e-09 2.80857580e-09 2.67367732e-09 2.58226756e-09
  2.51966493e-09 2.43242211e-09 2.34386062e-09 2.29017247e-09
  2.25100682e-09 2.18338979e-09 2.14961988e-09 2.09546211e-09
  2.05073293e-09 2.01871966e-09 2.00743630e-09 1.97017603e-09
  1.92502062e-09 1.86807794e-09 1.94965449e-09 1.89304743e-09
  1.81669304e-09 1.76164709e-09]
 [1.77036176e+00 6.45092630e-04 1.69828582e-05 3.84993146e-06
  1.36342942e-06 6.17456884e-07 3.56869584e-07 1.85862747e-07
  1.16992863e-07 7.66578780e-08 5.01817796e-08 3.41165200e-08
  2.94003715e-08 2.42495685e-08 2.46677633e-08 1.71297230e-08
  6.97041547e-08 1.26426918e-07 5.20994440e-09 4.73471891e-09
  3.98836854e-09 3.28282309e-09 2.83666336e-09 2.57442159e-09
  2.35495941e-09 2.18905081e-09 1.89539173e-09 1.77975107e-09
  1.72848060e-09 1.61656395e-09 1.50675366e-09 1.45958961e-09
  1.43863515e-09 1.37735434e-09 1.30160584e-09 1.26587521e-09
  1.24838868e-09 1.21154591e-09 1.20093899e-09 1.16085537e-09
  1.13143132e-09 1.12450652e-09 1.13448981e-09 1.11913633e-09
  1.08673952e-09 1.05201614e-09 1.10966906e-09 1.08261431e-09
  1.03513339e-09 1.01504667e-09]
 [2.09379327e+00 7.02828916e-04 1.67664193e-05 4.89120801e-06
  2.00021551e-06 9.32457125e-07 5.89404612e-07 2.95088369e-07
  1.91735565e-07 1.39279262e-07 9.13551682e-08 6.16148131e-08
  6.07994033e-08 4.67139069e-08 4.13454174e-08 3.08390223e-08
  8.69664801e-08 7.31471257e-08 1.00791031e-08 9.31100864e-09
  7.94254791e-09 6.81929984e-09 5.14329140e-09 4.61608829e-09
  4.29968110e-09 4.07569598e-09 3.27818234e-09 3.10002673e-09
  3.01431198e-09 2.84601331e-09 2.60941802e-09 2.54674281e-09
  2.51748768e-09 2.41214380e-09 2.26734255e-09 2.20425818e-09
  2.17487952e-09 2.11536566e-09 2.11869139e-09 2.05615417e-09
  2.01235807e-09 1.96715745e-09 2.00295974e-09 1.97046564e-09
  1.90828701e-09 1.84813826e-09 1.98506744e-09 1.95439063e-09
  1.85984035e-09 1.84711214e-09]
 [2.07641088e+00 1.06026555e-03 1.57675860e-05 5.74719569e-06
  2.58247495e-06 1.20158032e-06 8.72278076e-07 3.61064975e-07
  2.31312496e-07 2.19977264e-07 1.37817224e-07 9.16043237e-08
  1.24111063e-07 9.03570339e-08 7.34788335e-08 6.22081327e-08
  1.36080788e-07 1.14278319e-07 1.73412429e-08 1.57985207e-08
  1.39890259e-08 1.32973424e-08 6.40886034e-09 5.72938978e-09
  5.45574733e-09 5.77162281e-09 3.25869552e-09 3.02837787e-09
  3.07969444e-09 2.88629433e-09 2.49238771e-09 2.46392833e-09
  2.47125228e-09 2.31132274e-09 2.12622348e-09 2.03961507e-09
  2.01649712e-09 1.99301153e-09 1.94366842e-09 1.87328311e-09
  1.84518494e-09 1.80811417e-09 1.82371409e-09 1.78813858e-09
  1.74306457e-09 1.69161619e-09 1.84192367e-09 1.82202264e-09
  1.74047540e-09 1.71255660e-09]]
train_ae_loss [[4.61233441 3.02656204 3.91043368 4.31375317 4.57439972 4.6664272
  4.83651142 4.84729689 4.87493359 4.90290944 4.9170658  4.81683557
  4.74040003 4.61146984 4.51464371 4.40117756 4.2960746  4.13419378
  3.68329681 3.48105903 3.38122957 3.27854841 3.04559608 2.95831196
  2.9177693  2.84395813 2.74278881 2.70322416 2.68816828 2.65164237
  2.60417304 2.59317245 2.56577037 2.54918663 2.53760716 2.53847765
  2.52334057 2.513273   2.50363406 2.50418411 2.49114516 2.49134018
  2.47548048 2.49888125 2.4890569  2.48096229 2.4780672  2.46235808
  2.47024496 2.47527675]
 [3.5420541  2.68992828 3.52218533 3.76070726 3.95884249 3.93440605
  4.08049825 4.00424873 3.91764723 3.90407451 3.83837009 3.66730368
  3.51749698 3.37216645 3.24058401 3.09949442 2.95704456 2.78543652
  2.21777758 1.99751972 1.90892685 1.7782362  1.51598877 1.43244251
  1.35824718 1.31421532 1.19028517 1.1580997  1.13175727 1.12182194
  1.0570632  1.0448309  1.01719537 1.00170066 0.99743579 0.98936957
  0.98611493 0.94492101 0.95971396 0.95481065 0.94337829 0.92762684
  0.93101899 0.95420573 0.93580722 0.93365761 0.9372206  0.91306503
  0.923697   0.92713518]
 [3.85339439 2.74608005 3.83006522 3.96150988 4.08980365 3.99019624
  4.13762548 3.97837858 3.83719352 3.84085565 3.78252893 3.59610274
  3.44434732 3.29830513 3.16375575 3.02416146 2.87748316 2.70369427
  2.11917626 1.9013448  1.8150135  1.68351564 1.42198773 1.3376449
  1.25846968 1.22657209 1.10344527 1.06796964 1.03930521 1.03688717
  0.96544906 0.95821023 0.92639673 0.91452687 0.9069841  0.89957686
  0.89920245 0.8552322  0.87935321 0.86677856 0.85966839 0.83882239
  0.84751371 0.86988175 0.84932379 0.8534736  0.85112029 0.82523042
  0.83663334 0.83518805]
 [4.18039625 2.83133418 4.14210322 4.3226073  4.19019796 3.97519911
  4.10862528 3.89630344 3.72734127 3.72267013 3.65312247 3.45762335
  3.30008292 3.1562387  3.02095469 2.8811191  2.73324865 2.56454479
  1.99116866 1.78430216 1.7026778  1.57672785 1.32684938 1.24603986
  1.17252989 1.14555494 1.02724399 0.99236084 0.9654366  0.96352231
  0.89498324 0.88926504 0.85627215 0.84810166 0.83920118 0.8323848
  0.83357989 0.79096839 0.81482011 0.80169423 0.79680942 0.77747325
  0.78474096 0.80506742 0.78542837 0.79059918 0.78740647 0.76276047
  0.77400574 0.77031784]
 [4.44745034 2.64130079 3.93552483 4.63894747 4.1761378  3.80337433
  3.92951475 3.67354421 3.49122272 3.49213787 3.42725799 3.23722977
  3.08578664 2.95028328 2.82746628 2.69519813 2.55545038 2.40103048
  1.84893355 1.65442675 1.58418723 1.46273051 1.2300772  1.15577225
  1.08639517 1.06294265 0.95326678 0.91961483 0.89518306 0.8933105
  0.8301117  0.82431226 0.79377832 0.78615881 0.77794309 0.77142815
  0.77270337 0.73365314 0.75522646 0.74327272 0.73889969 0.72042695
  0.72787229 0.74600943 0.72783751 0.73355092 0.73038532 0.70736076
  0.71728709 0.71396184]]
valid_acc (5, 50)
valid_AUC (5, 50)
train_acc (50,)
total_train+valid_time 3616.417238548
Start Testing
Load ckpt at ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt
gc 9
Test layer0 Acc 0.802, AUC 0.8837549090385437, avg_entr 0.1854996234178543, f1 0.8019999861717224
l0_test_time 0.6344918740001049
gc 0
Test layer1 Acc 0.8188, AUC 0.8975228071212769, avg_entr 0.11128286272287369, f1 0.8187999725341797
l1_test_time 0.8224225029998706
gc 0
Test layer2 Acc 0.8208, AUC 0.9003827571868896, avg_entr 0.10017543286085129, f1 0.8208000063896179
l2_test_time 1.1466053959998135
gc 0
Test layer3 Acc 0.8194, AUC 0.9001588821411133, avg_entr 0.09804810583591461, f1 0.8194000124931335
l3_test_time 1.6274921180001911
gc 0
Test layer4 Acc 0.818, AUC 0.8998557925224304, avg_entr 0.09759625047445297, f1 0.8180000185966492
l4_test_time 2.266633959000046
gc 0
Test threshold 0.1 Acc 0.8176, AUC 0.8925991058349609, avg_entr 0.14438767731189728, f1 0.8176000118255615
t0.1_test_time 1.1657642130003296
gc 0
Test threshold 0.2 Acc 0.8174, AUC 0.8889074921607971, avg_entr 0.15184009075164795, f1 0.8173999786376953
t0.2_test_time 1.0402229199999056
gc 0
Test threshold 0.3 Acc 0.8166, AUC 0.888690173625946, avg_entr 0.16040121018886566, f1 0.8166000247001648
t0.3_test_time 0.9729182200003379
gc 0
Test threshold 0.4 Acc 0.816, AUC 0.8869423866271973, avg_entr 0.17108847200870514, f1 0.8160000443458557
t0.4_test_time 0.9342898800000512
gc 0
Test threshold 0.5 Acc 0.8152, AUC 0.8869093656539917, avg_entr 0.18071143329143524, f1 0.8152000308036804
t0.5_test_time 0.8810365289996298
gc 0
Test threshold 0.6 Acc 0.8144, AUC 0.886120617389679, avg_entr 0.1918358951807022, f1 0.8144000172615051
t0.6_test_time 0.8451920950001295
gc 0
Test threshold 0.7 Acc 0.8122, AUC 0.8855462074279785, avg_entr 0.20301228761672974, f1 0.8122000098228455
t0.7_test_time 0.7997891330001039
gc 0
Test threshold 0.8 Acc 0.8106, AUC 0.8873263597488403, avg_entr 0.21724465489387512, f1 0.8105999231338501
t0.8_test_time 0.7626554429998578
gc 0
Test threshold 0.9 Acc 0.8068, AUC 0.8858305215835571, avg_entr 0.23271377384662628, f1 0.8068000078201294
t0.9_test_time 0.7266590230001384
