total count words 222751
vocab size 30000
train size 40000, valid size 5000, test size 5000
found 27937 words in glove
model: TransformerALsideText(
  (layers): ModuleList(
    (0): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=2, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=2, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
  (emb_layers): ModuleList(
    (0): Embedding(30000, 300)
    (1): Embedding(30000, 300)
    (2): Embedding(30000, 300)
    (3): Embedding(30000, 300)
    (4): Embedding(30000, 300)
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.multi_headed_attention.linears.0.weight 90000
layers.0.enc.f.multi_headed_attention.linears.0.bias 300
layers.0.enc.f.multi_headed_attention.linears.1.weight 90000
layers.0.enc.f.multi_headed_attention.linears.1.bias 300
layers.0.enc.f.multi_headed_attention.linears.2.weight 90000
layers.0.enc.f.multi_headed_attention.linears.2.bias 300
layers.0.enc.f.multi_headed_attention.linears.3.weight 90000
layers.0.enc.f.multi_headed_attention.linears.3.bias 300
layers.0.enc.f.feed_forward.w_1.weight 90000
layers.0.enc.f.feed_forward.w_1.bias 300
layers.0.enc.f.feed_forward.w_2.weight 90000
layers.0.enc.f.feed_forward.w_2.bias 300
layers.0.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.0.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.0.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.0.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.0.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.0.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.0.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.0.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.0.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.0.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.0.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.0.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.0.enc.f.encoder.norm.a 300
layers.0.enc.f.encoder.norm.b 300
layers.0.ae.g.0.weight 256
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 256
layers.0.ae.h.0.bias 2
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
emb_layers.0.weight 9000000
emb_layers.1.weight 9000000
emb_layers.2.weight 9000000
emb_layers.3.weight 9000000
emb_layers.4.weight 9000000
Total Trainable Params: 50758378
init_time 20.995300227
Start Training
gc 0
Train Epoch0 Acc 0.500525 (20021/40000), AUC 0.4998822510242462
ep0_train_time 65.78197786700001
Test Epoch0 layer0 Acc 0.522, AUC 0.6086925864219666, avg_entr 0.6865736842155457, f1 0.5220000147819519
ep0_l0_test_time 0.6197350620000037
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 0
Test Epoch0 layer1 Acc 0.5462, AUC 0.5650953650474548, avg_entr 0.6929417848587036, f1 0.5461999773979187
ep0_l1_test_time 0.8182832360000134
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 0
Test Epoch0 layer2 Acc 0.4978, AUC 0.5113376975059509, avg_entr 0.6906139850616455, f1 0.49779999256134033
ep0_l2_test_time 1.138033969999995
Test Epoch0 layer3 Acc 0.512, AUC 0.5289571285247803, avg_entr 0.6871638894081116, f1 0.5120000243186951
ep0_l3_test_time 1.6195165559999936
Test Epoch0 layer4 Acc 0.5, AUC 0.5232137441635132, avg_entr 0.692046046257019, f1 0.5
ep0_l4_test_time 2.2478462599999887
gc 0
Train Epoch1 Acc 0.51125 (20450/40000), AUC 0.5155472755432129
ep1_train_time 65.52996928099999
Test Epoch1 layer0 Acc 0.6532, AUC 0.7227883338928223, avg_entr 0.623927891254425, f1 0.6531999707221985
ep1_l0_test_time 0.6384989049999774
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 1
Test Epoch1 layer1 Acc 0.659, AUC 0.7236001491546631, avg_entr 0.6853872537612915, f1 0.6589999794960022
ep1_l1_test_time 0.8210789540000007
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 1
Test Epoch1 layer2 Acc 0.6242, AUC 0.6814591884613037, avg_entr 0.6904906034469604, f1 0.6241999864578247
ep1_l2_test_time 1.1427848300000107
Test Epoch1 layer3 Acc 0.5, AUC 0.5169395804405212, avg_entr 0.6879395842552185, f1 0.5
ep1_l3_test_time 1.619199253000005
Test Epoch1 layer4 Acc 0.5, AUC 0.564397931098938, avg_entr 0.6914249658584595, f1 0.5
ep1_l4_test_time 2.2499232940000127
gc 0
Train Epoch2 Acc 0.52955 (21182/40000), AUC 0.5440460443496704
ep2_train_time 65.451920129
Test Epoch2 layer0 Acc 0.646, AUC 0.7965452671051025, avg_entr 0.36871856451034546, f1 0.6460000276565552
ep2_l0_test_time 0.6205218030000026
Test Epoch2 layer1 Acc 0.599, AUC 0.8011109232902527, avg_entr 0.33412835001945496, f1 0.5989999771118164
ep2_l1_test_time 0.8097044019999942
Test Epoch2 layer2 Acc 0.5384, AUC 0.7959049940109253, avg_entr 0.3079846501350403, f1 0.5383999943733215
ep2_l2_test_time 1.134947742999998
Test Epoch2 layer3 Acc 0.5036, AUC 0.796207070350647, avg_entr 0.38205474615097046, f1 0.503600001335144
ep2_l3_test_time 1.6150081129999876
Test Epoch2 layer4 Acc 0.5, AUC 0.7660866975784302, avg_entr 0.6310157179832458, f1 0.5
ep2_l4_test_time 2.253162789000015
gc 0
Train Epoch3 Acc 0.60315 (24126/40000), AUC 0.6478064060211182
ep3_train_time 65.63047047200001
Test Epoch3 layer0 Acc 0.7366, AUC 0.8159794807434082, avg_entr 0.43228739500045776, f1 0.7365999817848206
ep3_l0_test_time 0.6176554300000134
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 3
Test Epoch3 layer1 Acc 0.7348, AUC 0.8264484405517578, avg_entr 0.4440276324748993, f1 0.7347999811172485
ep3_l1_test_time 0.8211669990000132
Test Epoch3 layer2 Acc 0.7234, AUC 0.826158881187439, avg_entr 0.4529988169670105, f1 0.7233999967575073
ep3_l2_test_time 1.1318405869999992
Test Epoch3 layer3 Acc 0.7054, AUC 0.8253480195999146, avg_entr 0.47368359565734863, f1 0.7053999900817871
ep3_l3_test_time 1.6205886099999702
Test Epoch3 layer4 Acc 0.7166, AUC 0.8267326354980469, avg_entr 0.5327076315879822, f1 0.7165999412536621
ep3_l4_test_time 2.254027095999959
gc 0
Train Epoch4 Acc 0.700925 (28037/40000), AUC 0.777348518371582
ep4_train_time 65.49590102299999
Test Epoch4 layer0 Acc 0.7458, AUC 0.8352407217025757, avg_entr 0.352512389421463, f1 0.7458000183105469
ep4_l0_test_time 0.6181009700000004
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 4
Test Epoch4 layer1 Acc 0.7588, AUC 0.8448562622070312, avg_entr 0.349567711353302, f1 0.7588000297546387
ep4_l1_test_time 0.8230792370000017
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 4
Test Epoch4 layer2 Acc 0.761, AUC 0.8450556397438049, avg_entr 0.35524189472198486, f1 0.7610000371932983
ep4_l2_test_time 1.141775397999993
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 4
Test Epoch4 layer3 Acc 0.7598, AUC 0.844616711139679, avg_entr 0.3616153597831726, f1 0.7598000168800354
ep4_l3_test_time 1.6187705290000167
Test Epoch4 layer4 Acc 0.7628, AUC 0.8449698686599731, avg_entr 0.3797188103199005, f1 0.7627999782562256
ep4_l4_test_time 2.245558630000005
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 4
gc 0
Train Epoch5 Acc 0.7533 (30132/40000), AUC 0.8342244029045105
ep5_train_time 65.46459394800002
Test Epoch5 layer0 Acc 0.7584, AUC 0.8495975732803345, avg_entr 0.3056519627571106, f1 0.758400022983551
ep5_l0_test_time 0.6256226159999869
Test Epoch5 layer1 Acc 0.7606, AUC 0.8584951162338257, avg_entr 0.27376508712768555, f1 0.7605999708175659
ep5_l1_test_time 0.8276658809999731
Test Epoch5 layer2 Acc 0.758, AUC 0.8582066297531128, avg_entr 0.25617942214012146, f1 0.7580000162124634
ep5_l2_test_time 1.1351911999999516
Test Epoch5 layer3 Acc 0.754, AUC 0.8580878973007202, avg_entr 0.24584606289863586, f1 0.7540000081062317
ep5_l3_test_time 1.6187028839999584
Test Epoch5 layer4 Acc 0.7522, AUC 0.8581095933914185, avg_entr 0.24410918354988098, f1 0.7522000074386597
ep5_l4_test_time 2.2547049039999933
gc 0
Train Epoch6 Acc 0.77235 (30894/40000), AUC 0.8561692237854004
ep6_train_time 65.75450988199998
Test Epoch6 layer0 Acc 0.7488, AUC 0.8568339347839355, avg_entr 0.30405575037002563, f1 0.7487999796867371
ep6_l0_test_time 0.6486420610000323
Test Epoch6 layer1 Acc 0.7552, AUC 0.8667221665382385, avg_entr 0.3070234954357147, f1 0.7552000284194946
ep6_l1_test_time 0.8282153440000002
Test Epoch6 layer2 Acc 0.7466, AUC 0.8680458068847656, avg_entr 0.2716274559497833, f1 0.7465999126434326
ep6_l2_test_time 1.1342234060000465
Test Epoch6 layer3 Acc 0.7494, AUC 0.8685668706893921, avg_entr 0.25769680738449097, f1 0.7494000196456909
ep6_l3_test_time 1.6147666069999786
Test Epoch6 layer4 Acc 0.7478, AUC 0.8684626221656799, avg_entr 0.258038192987442, f1 0.7477999925613403
ep6_l4_test_time 2.2491672310000013
gc 0
Train Epoch7 Acc 0.7946 (31784/40000), AUC 0.8732470273971558
ep7_train_time 65.54266885999994
Test Epoch7 layer0 Acc 0.7786, AUC 0.8639301061630249, avg_entr 0.2777428925037384, f1 0.7785999774932861
ep7_l0_test_time 0.6349815680000575
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 7
Test Epoch7 layer1 Acc 0.7858, AUC 0.8732951879501343, avg_entr 0.2536454200744629, f1 0.7857999801635742
ep7_l1_test_time 0.813875932999963
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 7
Test Epoch7 layer2 Acc 0.7834, AUC 0.8737064599990845, avg_entr 0.20413276553153992, f1 0.7833999395370483
ep7_l2_test_time 1.1442600560000074
Test Epoch7 layer3 Acc 0.784, AUC 0.8736649751663208, avg_entr 0.1961800754070282, f1 0.7839999794960022
ep7_l3_test_time 1.6178726489999917
Test Epoch7 layer4 Acc 0.785, AUC 0.8739761710166931, avg_entr 0.19624246656894684, f1 0.7850000262260437
ep7_l4_test_time 2.2616687000000866
gc 0
Train Epoch8 Acc 0.804325 (32173/40000), AUC 0.8843578100204468
ep8_train_time 65.49642897399997
Test Epoch8 layer0 Acc 0.7862, AUC 0.8704833388328552, avg_entr 0.2494535595178604, f1 0.7861999869346619
ep8_l0_test_time 0.6189594429999943
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 8
Test Epoch8 layer1 Acc 0.7958, AUC 0.8799688816070557, avg_entr 0.2117929756641388, f1 0.7958000302314758
ep8_l1_test_time 0.8215966390000631
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 8
Test Epoch8 layer2 Acc 0.7958, AUC 0.8820956945419312, avg_entr 0.16699934005737305, f1 0.7958000302314758
ep8_l2_test_time 1.1396514120000347
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 8
Test Epoch8 layer3 Acc 0.797, AUC 0.8820250034332275, avg_entr 0.1603555977344513, f1 0.796999990940094
ep8_l3_test_time 1.6237624200000482
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 8
Test Epoch8 layer4 Acc 0.7952, AUC 0.8818408250808716, avg_entr 0.15637926757335663, f1 0.7952000498771667
ep8_l4_test_time 2.258228425000084
gc 0
Train Epoch9 Acc 0.822525 (32901/40000), AUC 0.9019002914428711
ep9_train_time 65.56932248700002
Test Epoch9 layer0 Acc 0.7884, AUC 0.872650682926178, avg_entr 0.2492610067129135, f1 0.7884000539779663
ep9_l0_test_time 0.61665787000004
Test Epoch9 layer1 Acc 0.8014, AUC 0.8849432468414307, avg_entr 0.19117173552513123, f1 0.8014000058174133
ep9_l1_test_time 0.8108568420000211
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 9
Test Epoch9 layer2 Acc 0.7982, AUC 0.8858681917190552, avg_entr 0.1642397940158844, f1 0.7982000708580017
ep9_l2_test_time 1.1447460320000573
Test Epoch9 layer3 Acc 0.7946, AUC 0.8858915567398071, avg_entr 0.16025802493095398, f1 0.7946000695228577
ep9_l3_test_time 1.6186515099999497
Test Epoch9 layer4 Acc 0.7924, AUC 0.8858762383460999, avg_entr 0.15633095800876617, f1 0.7923999428749084
ep9_l4_test_time 2.2535685660000127
gc 0
Train Epoch10 Acc 0.830625 (33225/40000), AUC 0.9063204526901245
ep10_train_time 65.49246422399995
Test Epoch10 layer0 Acc 0.7914, AUC 0.876096248626709, avg_entr 0.229202538728714, f1 0.7914000153541565
ep10_l0_test_time 0.6174210729999459
Test Epoch10 layer1 Acc 0.8012, AUC 0.8851795196533203, avg_entr 0.17163105309009552, f1 0.8011999726295471
ep10_l1_test_time 0.8187062709999964
Test Epoch10 layer2 Acc 0.7988, AUC 0.8862922191619873, avg_entr 0.14326773583889008, f1 0.798799991607666
ep10_l2_test_time 1.1332474909999064
Test Epoch10 layer3 Acc 0.7984, AUC 0.8864316940307617, avg_entr 0.140364408493042, f1 0.7983999848365784
ep10_l3_test_time 1.6178321200000028
Test Epoch10 layer4 Acc 0.7976, AUC 0.8863720893859863, avg_entr 0.13879336416721344, f1 0.7976000308990479
ep10_l4_test_time 2.2521033179999677
gc 0
Train Epoch11 Acc 0.8457 (33828/40000), AUC 0.918371856212616
ep11_train_time 65.73195468400002
Test Epoch11 layer0 Acc 0.7944, AUC 0.8765239715576172, avg_entr 0.21730244159698486, f1 0.7943999767303467
ep11_l0_test_time 0.617918828000029
Test Epoch11 layer1 Acc 0.8084, AUC 0.8926868438720703, avg_entr 0.15863491594791412, f1 0.8083999752998352
ep11_l1_test_time 0.8096850809999978
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 11
Test Epoch11 layer2 Acc 0.8096, AUC 0.8947761654853821, avg_entr 0.13704948127269745, f1 0.8095999956130981
ep11_l2_test_time 1.1503304220000246
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 11
Test Epoch11 layer3 Acc 0.8096, AUC 0.8951910734176636, avg_entr 0.13839569687843323, f1 0.8095999956130981
ep11_l3_test_time 1.6264012580001008
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 11
Test Epoch11 layer4 Acc 0.809, AUC 0.8952127695083618, avg_entr 0.14173804223537445, f1 0.8090000748634338
ep11_l4_test_time 2.2575869970000895
gc 0
Train Epoch12 Acc 0.861025 (34441/40000), AUC 0.9313247203826904
ep12_train_time 65.57995761000006
Test Epoch12 layer0 Acc 0.7912, AUC 0.8782435655593872, avg_entr 0.19877438247203827, f1 0.7911999821662903
ep12_l0_test_time 0.618863594000004
Test Epoch12 layer1 Acc 0.802, AUC 0.8924890756607056, avg_entr 0.12840503454208374, f1 0.8019999861717224
ep12_l1_test_time 0.8107030799999393
Test Epoch12 layer2 Acc 0.8036, AUC 0.893304705619812, avg_entr 0.12271623313426971, f1 0.803600013256073
ep12_l2_test_time 1.134929020999948
Test Epoch12 layer3 Acc 0.8036, AUC 0.8932862281799316, avg_entr 0.12435474991798401, f1 0.803600013256073
ep12_l3_test_time 1.6159134170000016
Test Epoch12 layer4 Acc 0.804, AUC 0.8931713104248047, avg_entr 0.12632620334625244, f1 0.8040000200271606
ep12_l4_test_time 2.258386115999997
gc 0
Train Epoch13 Acc 0.867525 (34701/40000), AUC 0.937423050403595
ep13_train_time 65.59133116999999
Test Epoch13 layer0 Acc 0.7688, AUC 0.8743628263473511, avg_entr 0.20342393219470978, f1 0.7688000202178955
ep13_l0_test_time 0.6268522740001572
Test Epoch13 layer1 Acc 0.7586, AUC 0.8846964836120605, avg_entr 0.11262381821870804, f1 0.7585999965667725
ep13_l1_test_time 0.8462159110001721
Test Epoch13 layer2 Acc 0.7484, AUC 0.8899502754211426, avg_entr 0.10650143027305603, f1 0.7483999729156494
ep13_l2_test_time 1.132223576999877
Test Epoch13 layer3 Acc 0.7414, AUC 0.8904868364334106, avg_entr 0.10493568331003189, f1 0.7414000034332275
ep13_l3_test_time 1.6199004929999319
Test Epoch13 layer4 Acc 0.7352, AUC 0.8898690342903137, avg_entr 0.10270354896783829, f1 0.7351999282836914
ep13_l4_test_time 2.251374670999894
gc 0
Train Epoch14 Acc 0.8752 (35008/40000), AUC 0.9396921396255493
ep14_train_time 65.70827159500004
Test Epoch14 layer0 Acc 0.7882, AUC 0.8748733997344971, avg_entr 0.18752650916576385, f1 0.7882000207901001
ep14_l0_test_time 0.6174094850000529
Test Epoch14 layer1 Acc 0.8002, AUC 0.8884134888648987, avg_entr 0.09249052405357361, f1 0.8001999855041504
ep14_l1_test_time 0.8222106390001045
Test Epoch14 layer2 Acc 0.7984, AUC 0.8903151154518127, avg_entr 0.0867532417178154, f1 0.7983999848365784
ep14_l2_test_time 1.1389934100000119
Test Epoch14 layer3 Acc 0.7984, AUC 0.8899763822555542, avg_entr 0.08619675040245056, f1 0.7983999848365784
ep14_l3_test_time 1.6192814270000326
Test Epoch14 layer4 Acc 0.798, AUC 0.8897649645805359, avg_entr 0.08451477438211441, f1 0.7979999780654907
ep14_l4_test_time 2.25462788599998
gc 0
Train Epoch15 Acc 0.886775 (35471/40000), AUC 0.9505014419555664
ep15_train_time 65.57670359000008
Test Epoch15 layer0 Acc 0.7874, AUC 0.8738706707954407, avg_entr 0.1890147626399994, f1 0.7874000072479248
ep15_l0_test_time 0.6195212329998867
Test Epoch15 layer1 Acc 0.7934, AUC 0.8898137211799622, avg_entr 0.1013074740767479, f1 0.79339998960495
ep15_l1_test_time 0.8104960680000204
Test Epoch15 layer2 Acc 0.7958, AUC 0.8907427787780762, avg_entr 0.0966859832406044, f1 0.7958000302314758
ep15_l2_test_time 1.134850151999899
Test Epoch15 layer3 Acc 0.793, AUC 0.8911949396133423, avg_entr 0.09429760277271271, f1 0.7929999828338623
ep15_l3_test_time 1.6160780949999207
Test Epoch15 layer4 Acc 0.7888, AUC 0.8913998603820801, avg_entr 0.09053082019090652, f1 0.7888000011444092
ep15_l4_test_time 2.2544623190001403
gc 0
Train Epoch16 Acc 0.90025 (36010/40000), AUC 0.9621633291244507
ep16_train_time 65.55177271100001
Test Epoch16 layer0 Acc 0.7836, AUC 0.8719877004623413, avg_entr 0.17310404777526855, f1 0.7835999727249146
ep16_l0_test_time 0.6174185600000328
Test Epoch16 layer1 Acc 0.7984, AUC 0.8856159448623657, avg_entr 0.08905760198831558, f1 0.7983999848365784
ep16_l1_test_time 0.8120078669999202
Test Epoch16 layer2 Acc 0.7984, AUC 0.8879618644714355, avg_entr 0.08431170880794525, f1 0.7983999848365784
ep16_l2_test_time 1.1337391779998143
Test Epoch16 layer3 Acc 0.8004, AUC 0.8880968689918518, avg_entr 0.08325918018817902, f1 0.8004000186920166
ep16_l3_test_time 1.616826021999941
Test Epoch16 layer4 Acc 0.8006, AUC 0.8871760964393616, avg_entr 0.08340383321046829, f1 0.800599992275238
ep16_l4_test_time 2.253630433999888
gc 0
Train Epoch17 Acc 0.915375 (36615/40000), AUC 0.9697409272193909
ep17_train_time 65.56570609100004
Test Epoch17 layer0 Acc 0.7846, AUC 0.870701789855957, avg_entr 0.16590093076229095, f1 0.784600019454956
ep17_l0_test_time 0.617027182999891
Test Epoch17 layer1 Acc 0.8032, AUC 0.8855097889900208, avg_entr 0.08196291327476501, f1 0.8032000660896301
ep17_l1_test_time 0.8107775569999376
Test Epoch17 layer2 Acc 0.8046, AUC 0.8898429870605469, avg_entr 0.07346442341804504, f1 0.8046000599861145
ep17_l2_test_time 1.1354670139999143
Test Epoch17 layer3 Acc 0.8072, AUC 0.8900583982467651, avg_entr 0.07227440178394318, f1 0.807200014591217
ep17_l3_test_time 1.6177244559999053
Test Epoch17 layer4 Acc 0.807, AUC 0.8895155191421509, avg_entr 0.0720004141330719, f1 0.8069999814033508
ep17_l4_test_time 2.254688444999829
gc 0
Train Epoch18 Acc 0.9229 (36916/40000), AUC 0.9741659164428711
ep18_train_time 65.55604007500006
Test Epoch18 layer0 Acc 0.7862, AUC 0.8686991930007935, avg_entr 0.15697436034679413, f1 0.7861999869346619
ep18_l0_test_time 0.6163588599999912
Test Epoch18 layer1 Acc 0.798, AUC 0.8770037293434143, avg_entr 0.07050781697034836, f1 0.7979999780654907
ep18_l1_test_time 0.8128422120000778
Test Epoch18 layer2 Acc 0.7976, AUC 0.8834941387176514, avg_entr 0.06513799726963043, f1 0.7976000308990479
ep18_l2_test_time 1.1324038459999883
Test Epoch18 layer3 Acc 0.7976, AUC 0.8830859661102295, avg_entr 0.06324758380651474, f1 0.7976000308990479
ep18_l3_test_time 1.6181717829999798
Test Epoch18 layer4 Acc 0.7978, AUC 0.8827619552612305, avg_entr 0.0614144541323185, f1 0.7978000044822693
ep18_l4_test_time 2.2540648089998285
gc 0
Train Epoch19 Acc 0.9257 (37028/40000), AUC 0.9751366972923279
ep19_train_time 65.57883236099997
Test Epoch19 layer0 Acc 0.786, AUC 0.8670306205749512, avg_entr 0.1630626618862152, f1 0.7860000133514404
ep19_l0_test_time 0.6187177040001188
Test Epoch19 layer1 Acc 0.8, AUC 0.8766458034515381, avg_entr 0.06853198260068893, f1 0.8000000715255737
ep19_l1_test_time 0.8109710389999236
Test Epoch19 layer2 Acc 0.8018, AUC 0.8822997212409973, avg_entr 0.06362457573413849, f1 0.801800012588501
ep19_l2_test_time 1.1378766899999846
Test Epoch19 layer3 Acc 0.8002, AUC 0.8818939328193665, avg_entr 0.06159452348947525, f1 0.8001999855041504
ep19_l3_test_time 1.6223147960001825
Test Epoch19 layer4 Acc 0.8002, AUC 0.8814429044723511, avg_entr 0.06135126203298569, f1 0.8001999855041504
ep19_l4_test_time 2.2660213220001424
gc 0
Train Epoch20 Acc 0.932 (37280/40000), AUC 0.9799947738647461
ep20_train_time 65.62531974300009
Test Epoch20 layer0 Acc 0.7652, AUC 0.8645936250686646, avg_entr 0.16637268662452698, f1 0.7652000188827515
ep20_l0_test_time 0.6206834919998983
Test Epoch20 layer1 Acc 0.7848, AUC 0.8723586797714233, avg_entr 0.06790462136268616, f1 0.7847999930381775
ep20_l1_test_time 0.8120857829999295
Test Epoch20 layer2 Acc 0.7856, AUC 0.8814360499382019, avg_entr 0.06363849341869354, f1 0.7856000065803528
ep20_l2_test_time 1.137168906999932
Test Epoch20 layer3 Acc 0.7868, AUC 0.8801231980323792, avg_entr 0.061746057122945786, f1 0.786799967288971
ep20_l3_test_time 1.6159801189999143
Test Epoch20 layer4 Acc 0.7894, AUC 0.8785505890846252, avg_entr 0.06074366346001625, f1 0.7893999814987183
ep20_l4_test_time 2.2559165359998588
gc 0
Train Epoch21 Acc 0.9398 (37592/40000), AUC 0.9839874505996704
ep21_train_time 65.59255208400009
Test Epoch21 layer0 Acc 0.7804, AUC 0.8623313307762146, avg_entr 0.15582704544067383, f1 0.7803999781608582
ep21_l0_test_time 0.6210165499999221
Test Epoch21 layer1 Acc 0.7918, AUC 0.865483283996582, avg_entr 0.05969135835766792, f1 0.7918000221252441
ep21_l1_test_time 0.812363459999915
Test Epoch21 layer2 Acc 0.7922, AUC 0.8767500519752502, avg_entr 0.05574902892112732, f1 0.7922000288963318
ep21_l2_test_time 1.1332400149999557
Test Epoch21 layer3 Acc 0.7926, AUC 0.8758176565170288, avg_entr 0.053043171763420105, f1 0.7925999760627747
ep21_l3_test_time 1.6213052670000252
Test Epoch21 layer4 Acc 0.7934, AUC 0.8746286630630493, avg_entr 0.0515974797308445, f1 0.79339998960495
ep21_l4_test_time 2.260911694000015
gc 0
Train Epoch22 Acc 0.945125 (37805/40000), AUC 0.9849166870117188
ep22_train_time 65.70562599899995
Test Epoch22 layer0 Acc 0.779, AUC 0.8622900247573853, avg_entr 0.1512247920036316, f1 0.7789999842643738
ep22_l0_test_time 0.6213133619999098
Test Epoch22 layer1 Acc 0.792, AUC 0.8698654174804688, avg_entr 0.05754116177558899, f1 0.7920000553131104
ep22_l1_test_time 0.8227628669999376
Test Epoch22 layer2 Acc 0.792, AUC 0.8777746558189392, avg_entr 0.051987916231155396, f1 0.7920000553131104
ep22_l2_test_time 1.13668428699998
Test Epoch22 layer3 Acc 0.794, AUC 0.877164363861084, avg_entr 0.05132381618022919, f1 0.7940000295639038
ep22_l3_test_time 1.6258162690000972
Test Epoch22 layer4 Acc 0.7944, AUC 0.8769463300704956, avg_entr 0.05082567036151886, f1 0.7943999767303467
ep22_l4_test_time 2.2633897999999135
gc 0
Train Epoch23 Acc 0.946775 (37871/40000), AUC 0.9858705997467041
ep23_train_time 65.64513204400009
Test Epoch23 layer0 Acc 0.775, AUC 0.8598141074180603, avg_entr 0.15395000576972961, f1 0.7749999761581421
ep23_l0_test_time 0.6216637229999833
Test Epoch23 layer1 Acc 0.7896, AUC 0.8637953996658325, avg_entr 0.05811389163136482, f1 0.7896000146865845
ep23_l1_test_time 0.820350241999904
Test Epoch23 layer2 Acc 0.79, AUC 0.8753663301467896, avg_entr 0.05331779271364212, f1 0.7900000214576721
ep23_l2_test_time 1.1369432849999157
Test Epoch23 layer3 Acc 0.7906, AUC 0.8744778633117676, avg_entr 0.05178184062242508, f1 0.7906000018119812
ep23_l3_test_time 1.6231412239999372
Test Epoch23 layer4 Acc 0.7908, AUC 0.8740661144256592, avg_entr 0.0500679649412632, f1 0.7907999753952026
ep23_l4_test_time 2.2594231239997953
gc 0
Train Epoch24 Acc 0.949 (37960/40000), AUC 0.9882354736328125
ep24_train_time 65.72408519200008
Test Epoch24 layer0 Acc 0.7732, AUC 0.8575870990753174, avg_entr 0.1519070863723755, f1 0.7731999754905701
ep24_l0_test_time 0.6313811909999458
Test Epoch24 layer1 Acc 0.7864, AUC 0.8564558029174805, avg_entr 0.054601676762104034, f1 0.7864000201225281
ep24_l1_test_time 0.811245025000062
Test Epoch24 layer2 Acc 0.789, AUC 0.8719837665557861, avg_entr 0.04915319010615349, f1 0.7889999747276306
ep24_l2_test_time 1.1345400870000049
Test Epoch24 layer3 Acc 0.7888, AUC 0.8699617981910706, avg_entr 0.04686889424920082, f1 0.7888000011444092
ep24_l3_test_time 1.6201420869999765
Test Epoch24 layer4 Acc 0.7882, AUC 0.8683872818946838, avg_entr 0.04509961977601051, f1 0.7882000207901001
ep24_l4_test_time 2.2533117379998657
gc 0
Train Epoch25 Acc 0.95215 (38086/40000), AUC 0.9891567230224609
ep25_train_time 65.59590624099997
Test Epoch25 layer0 Acc 0.7756, AUC 0.8580992221832275, avg_entr 0.15061217546463013, f1 0.775600016117096
ep25_l0_test_time 0.6181041509998977
Test Epoch25 layer1 Acc 0.791, AUC 0.8582764863967896, avg_entr 0.05427660420536995, f1 0.7910000681877136
ep25_l1_test_time 0.8104829129999871
Test Epoch25 layer2 Acc 0.7904, AUC 0.8716583251953125, avg_entr 0.049312952905893326, f1 0.7904000282287598
ep25_l2_test_time 1.1346395069999744
Test Epoch25 layer3 Acc 0.7904, AUC 0.8700522780418396, avg_entr 0.04766464978456497, f1 0.7904000282287598
ep25_l3_test_time 1.6170874780000304
Test Epoch25 layer4 Acc 0.7912, AUC 0.869629979133606, avg_entr 0.046278215944767, f1 0.7911999821662903
ep25_l4_test_time 2.252129613000079
gc 0
Train Epoch26 Acc 0.9539 (38156/40000), AUC 0.9895625114440918
ep26_train_time 65.57942809099995
Test Epoch26 layer0 Acc 0.7752, AUC 0.8568630814552307, avg_entr 0.14983105659484863, f1 0.7752000093460083
ep26_l0_test_time 0.6177230789999157
Test Epoch26 layer1 Acc 0.788, AUC 0.8555455207824707, avg_entr 0.05173858627676964, f1 0.7879999876022339
ep26_l1_test_time 0.8352820390000488
Test Epoch26 layer2 Acc 0.786, AUC 0.8691555261611938, avg_entr 0.048256099224090576, f1 0.7860000133514404
ep26_l2_test_time 1.159711335999873
Test Epoch26 layer3 Acc 0.7876, AUC 0.8682307004928589, avg_entr 0.045674677938222885, f1 0.7875999212265015
ep26_l3_test_time 1.634175083000173
Test Epoch26 layer4 Acc 0.7884, AUC 0.8675341606140137, avg_entr 0.04438409581780434, f1 0.7884000539779663
ep26_l4_test_time 2.275621602000001
gc 0
Train Epoch27 Acc 0.956225 (38249/40000), AUC 0.990058422088623
ep27_train_time 65.95086552499993
Test Epoch27 layer0 Acc 0.774, AUC 0.8570152521133423, avg_entr 0.14656813442707062, f1 0.7739999890327454
ep27_l0_test_time 0.617727332999948
Test Epoch27 layer1 Acc 0.7848, AUC 0.8532991409301758, avg_entr 0.05059143528342247, f1 0.7847999930381775
ep27_l1_test_time 0.8138081679999232
Test Epoch27 layer2 Acc 0.7868, AUC 0.8683992624282837, avg_entr 0.04250115156173706, f1 0.786799967288971
ep27_l2_test_time 1.134873607000145
Test Epoch27 layer3 Acc 0.788, AUC 0.8672064542770386, avg_entr 0.040000755339860916, f1 0.7879999876022339
ep27_l3_test_time 1.6251401590000114
Test Epoch27 layer4 Acc 0.7878, AUC 0.865935742855072, avg_entr 0.03813738375902176, f1 0.7877999544143677
ep27_l4_test_time 2.2583750609999242
gc 0
Train Epoch28 Acc 0.957825 (38313/40000), AUC 0.9910374879837036
ep28_train_time 65.65868842699979
Test Epoch28 layer0 Acc 0.777, AUC 0.8565396070480347, avg_entr 0.14764076471328735, f1 0.7770000100135803
ep28_l0_test_time 0.6173777159997371
Test Epoch28 layer1 Acc 0.7878, AUC 0.8507123589515686, avg_entr 0.048915423452854156, f1 0.7877999544143677
ep28_l1_test_time 0.8099914889999127
Test Epoch28 layer2 Acc 0.789, AUC 0.8664870262145996, avg_entr 0.04201327636837959, f1 0.7889999747276306
ep28_l2_test_time 1.1404440260002957
Test Epoch28 layer3 Acc 0.7902, AUC 0.8659998774528503, avg_entr 0.03972164914011955, f1 0.7901999950408936
ep28_l3_test_time 1.6304660810001224
Test Epoch28 layer4 Acc 0.7908, AUC 0.8654729127883911, avg_entr 0.037597741931676865, f1 0.7907999753952026
ep28_l4_test_time 2.2594789789995957
gc 0
Train Epoch29 Acc 0.9593 (38372/40000), AUC 0.9915040731430054
ep29_train_time 65.55398978699986
Test Epoch29 layer0 Acc 0.775, AUC 0.8550722002983093, avg_entr 0.14747880399227142, f1 0.7749999761581421
ep29_l0_test_time 0.6173573979999674
Test Epoch29 layer1 Acc 0.7836, AUC 0.8513957262039185, avg_entr 0.05038658529520035, f1 0.7835999727249146
ep29_l1_test_time 0.8147084229999564
Test Epoch29 layer2 Acc 0.785, AUC 0.8658124208450317, avg_entr 0.0452420637011528, f1 0.7850000262260437
ep29_l2_test_time 1.133608729000116
Test Epoch29 layer3 Acc 0.7856, AUC 0.8651516437530518, avg_entr 0.04342281445860863, f1 0.7856000065803528
ep29_l3_test_time 1.6179172520000975
Test Epoch29 layer4 Acc 0.7878, AUC 0.8647414445877075, avg_entr 0.041788335889577866, f1 0.7877999544143677
ep29_l4_test_time 2.253299904999949
gc 0
Train Epoch30 Acc 0.959275 (38371/40000), AUC 0.9917184114456177
ep30_train_time 65.56835655700024
Test Epoch30 layer0 Acc 0.7724, AUC 0.855466365814209, avg_entr 0.14566543698310852, f1 0.7724000215530396
ep30_l0_test_time 0.616806835999796
Test Epoch30 layer1 Acc 0.7834, AUC 0.8515551090240479, avg_entr 0.04799777269363403, f1 0.7833999395370483
ep30_l1_test_time 0.8164216980003403
Test Epoch30 layer2 Acc 0.7858, AUC 0.8666737079620361, avg_entr 0.04286208003759384, f1 0.7857999801635742
ep30_l2_test_time 1.137046752999595
Test Epoch30 layer3 Acc 0.7858, AUC 0.8665454983711243, avg_entr 0.04045876860618591, f1 0.7857999801635742
ep30_l3_test_time 1.6175146420000601
Test Epoch30 layer4 Acc 0.7878, AUC 0.8660473823547363, avg_entr 0.03845520317554474, f1 0.7877999544143677
ep30_l4_test_time 2.2562847430003785
gc 0
Train Epoch31 Acc 0.9598 (38392/40000), AUC 0.99146968126297
ep31_train_time 65.61554659800004
Test Epoch31 layer0 Acc 0.7752, AUC 0.855491042137146, avg_entr 0.14548015594482422, f1 0.7752000093460083
ep31_l0_test_time 0.652567308000016
Test Epoch31 layer1 Acc 0.7858, AUC 0.8515644669532776, avg_entr 0.04947364330291748, f1 0.7857999801635742
ep31_l1_test_time 0.811933905999922
Test Epoch31 layer2 Acc 0.7856, AUC 0.8658042550086975, avg_entr 0.04464885964989662, f1 0.7856000065803528
ep31_l2_test_time 1.1361325240000042
Test Epoch31 layer3 Acc 0.786, AUC 0.8654636144638062, avg_entr 0.0425858311355114, f1 0.7860000133514404
ep31_l3_test_time 1.6221241130001545
Test Epoch31 layer4 Acc 0.7846, AUC 0.8647225499153137, avg_entr 0.041609518229961395, f1 0.784600019454956
ep31_l4_test_time 2.2631898069998897
gc 0
Train Epoch32 Acc 0.9613 (38452/40000), AUC 0.991894543170929
ep32_train_time 65.57095088400001
Test Epoch32 layer0 Acc 0.773, AUC 0.8543643951416016, avg_entr 0.14543978869915009, f1 0.7730000019073486
ep32_l0_test_time 0.6227015939998637
Test Epoch32 layer1 Acc 0.7846, AUC 0.8495836853981018, avg_entr 0.0473373681306839, f1 0.784600019454956
ep32_l1_test_time 0.8306021230000624
Test Epoch32 layer2 Acc 0.7854, AUC 0.8647100925445557, avg_entr 0.042857103049755096, f1 0.7853999733924866
ep32_l2_test_time 1.1441958979999072
Test Epoch32 layer3 Acc 0.7862, AUC 0.8640425205230713, avg_entr 0.039980415254831314, f1 0.7861999869346619
ep32_l3_test_time 1.6266339469998456
Test Epoch32 layer4 Acc 0.7854, AUC 0.8631848692893982, avg_entr 0.038234781473875046, f1 0.7853999733924866
ep32_l4_test_time 2.2552776370002903
gc 0
Train Epoch33 Acc 0.9629 (38516/40000), AUC 0.9926382303237915
ep33_train_time 65.5956903690003
Test Epoch33 layer0 Acc 0.7736, AUC 0.8546614646911621, avg_entr 0.14511583745479584, f1 0.7735999822616577
ep33_l0_test_time 0.6177724770000168
Test Epoch33 layer1 Acc 0.7834, AUC 0.849468469619751, avg_entr 0.04719403013586998, f1 0.7833999395370483
ep33_l1_test_time 0.8106922299998587
Test Epoch33 layer2 Acc 0.7832, AUC 0.864504337310791, avg_entr 0.04354248195886612, f1 0.7832000851631165
ep33_l2_test_time 1.1351855969996905
Test Epoch33 layer3 Acc 0.7834, AUC 0.8640599846839905, avg_entr 0.040954820811748505, f1 0.7833999395370483
ep33_l3_test_time 1.6228313950000484
Test Epoch33 layer4 Acc 0.7838, AUC 0.8640366792678833, avg_entr 0.03888678923249245, f1 0.7838000059127808
ep33_l4_test_time 2.252350897999804
gc 0
Train Epoch34 Acc 0.960875 (38435/40000), AUC 0.9922267198562622
ep34_train_time 65.59885080100003
Test Epoch34 layer0 Acc 0.7728, AUC 0.8542317152023315, avg_entr 0.14502902328968048, f1 0.7728000283241272
ep34_l0_test_time 0.6180558029996064
Test Epoch34 layer1 Acc 0.7858, AUC 0.8497331142425537, avg_entr 0.04682186618447304, f1 0.7857999801635742
ep34_l1_test_time 0.8162341279999055
Test Epoch34 layer2 Acc 0.785, AUC 0.864856481552124, avg_entr 0.0428665392100811, f1 0.7850000262260437
ep34_l2_test_time 1.1332920119998562
Test Epoch34 layer3 Acc 0.7866, AUC 0.8645310401916504, avg_entr 0.040462806820869446, f1 0.7865999937057495
ep34_l3_test_time 1.6164441589999115
Test Epoch34 layer4 Acc 0.788, AUC 0.8633856773376465, avg_entr 0.03832325339317322, f1 0.7879999876022339
ep34_l4_test_time 2.253166109000176
gc 0
Train Epoch35 Acc 0.9627 (38508/40000), AUC 0.9925602674484253
ep35_train_time 65.56051045799995
Test Epoch35 layer0 Acc 0.7734, AUC 0.8538670539855957, avg_entr 0.14484401047229767, f1 0.7734000086784363
ep35_l0_test_time 0.6185206739996829
Test Epoch35 layer1 Acc 0.7846, AUC 0.848885178565979, avg_entr 0.047598715871572495, f1 0.784600019454956
ep35_l1_test_time 0.810430505000113
Test Epoch35 layer2 Acc 0.7858, AUC 0.8638774156570435, avg_entr 0.04313134029507637, f1 0.7857999801635742
ep35_l2_test_time 1.1353623320001134
Test Epoch35 layer3 Acc 0.7866, AUC 0.8636828064918518, avg_entr 0.040631361305713654, f1 0.7865999937057495
ep35_l3_test_time 1.6245212690000699
Test Epoch35 layer4 Acc 0.7868, AUC 0.863169252872467, avg_entr 0.03891969472169876, f1 0.786799967288971
ep35_l4_test_time 2.25771998800019
gc 0
Train Epoch36 Acc 0.962925 (38517/40000), AUC 0.9926633834838867
ep36_train_time 65.66965374399979
Test Epoch36 layer0 Acc 0.7706, AUC 0.8535044193267822, avg_entr 0.14492902159690857, f1 0.7706000208854675
ep36_l0_test_time 0.6299058310000873
Test Epoch36 layer1 Acc 0.7818, AUC 0.8477051854133606, avg_entr 0.046881504356861115, f1 0.7817999720573425
ep36_l1_test_time 0.8228105359999063
Test Epoch36 layer2 Acc 0.7822, AUC 0.8632779121398926, avg_entr 0.043413762003183365, f1 0.7821999788284302
ep36_l2_test_time 1.1439172109999163
Test Epoch36 layer3 Acc 0.7832, AUC 0.8629443645477295, avg_entr 0.04074353724718094, f1 0.7832000851631165
ep36_l3_test_time 1.6170086699999047
Test Epoch36 layer4 Acc 0.7826, AUC 0.8622058629989624, avg_entr 0.038625895977020264, f1 0.7825999855995178
ep36_l4_test_time 2.25516805999996
gc 0
Train Epoch37 Acc 0.963725 (38549/40000), AUC 0.992977499961853
ep37_train_time 65.57626749099973
Test Epoch37 layer0 Acc 0.7714, AUC 0.8533151745796204, avg_entr 0.14560142159461975, f1 0.771399974822998
ep37_l0_test_time 0.6182065930001954
Test Epoch37 layer1 Acc 0.7838, AUC 0.8471550941467285, avg_entr 0.04796223342418671, f1 0.7838000059127808
ep37_l1_test_time 0.8129641680002351
Test Epoch37 layer2 Acc 0.7824, AUC 0.8628145456314087, avg_entr 0.04281710833311081, f1 0.7824000120162964
ep37_l2_test_time 1.1400048119999155
Test Epoch37 layer3 Acc 0.7836, AUC 0.862553060054779, avg_entr 0.041189853101968765, f1 0.7835999727249146
ep37_l3_test_time 1.6295779709998897
Test Epoch37 layer4 Acc 0.7842, AUC 0.8618569374084473, avg_entr 0.03959543630480766, f1 0.7842000126838684
ep37_l4_test_time 2.2649433869996756
gc 0
Train Epoch38 Acc 0.96345 (38538/40000), AUC 0.9929512739181519
ep38_train_time 65.6581488060001
Test Epoch38 layer0 Acc 0.7702, AUC 0.8531330227851868, avg_entr 0.14526957273483276, f1 0.7702000141143799
ep38_l0_test_time 0.6263802129997202
Test Epoch38 layer1 Acc 0.7812, AUC 0.8466029167175293, avg_entr 0.04716791212558746, f1 0.7811999917030334
ep38_l1_test_time 0.8164169430001493
Test Epoch38 layer2 Acc 0.783, AUC 0.8624386787414551, avg_entr 0.043002624064683914, f1 0.7829999923706055
ep38_l2_test_time 1.1411965539996345
Test Epoch38 layer3 Acc 0.7836, AUC 0.8621731400489807, avg_entr 0.040483955293893814, f1 0.7835999727249146
ep38_l3_test_time 1.6255131599996275
Test Epoch38 layer4 Acc 0.7826, AUC 0.8620682954788208, avg_entr 0.03888622671365738, f1 0.7825999855995178
ep38_l4_test_time 2.256008309999743
gc 0
Train Epoch39 Acc 0.962925 (38517/40000), AUC 0.9927544593811035
ep39_train_time 65.5747316930001
Test Epoch39 layer0 Acc 0.7746, AUC 0.8531419038772583, avg_entr 0.14454370737075806, f1 0.7746000289916992
ep39_l0_test_time 0.6203623870001138
Test Epoch39 layer1 Acc 0.7836, AUC 0.8472425937652588, avg_entr 0.046164680272340775, f1 0.7835999727249146
ep39_l1_test_time 0.8159882380000454
Test Epoch39 layer2 Acc 0.7838, AUC 0.8630777597427368, avg_entr 0.042260996997356415, f1 0.7838000059127808
ep39_l2_test_time 1.133135990000028
Test Epoch39 layer3 Acc 0.7836, AUC 0.8629559278488159, avg_entr 0.03955747187137604, f1 0.7835999727249146
ep39_l3_test_time 1.6197773580001922
Test Epoch39 layer4 Acc 0.7842, AUC 0.8624433279037476, avg_entr 0.03769272565841675, f1 0.7842000126838684
ep39_l4_test_time 2.2547733639999024
gc 0
Train Epoch40 Acc 0.965125 (38605/40000), AUC 0.9931610226631165
ep40_train_time 65.59218849600029
Test Epoch40 layer0 Acc 0.7724, AUC 0.8531162738800049, avg_entr 0.14432667195796967, f1 0.7724000215530396
ep40_l0_test_time 0.618038216000059
Test Epoch40 layer1 Acc 0.7822, AUC 0.8467066287994385, avg_entr 0.0464588962495327, f1 0.7821999788284302
ep40_l1_test_time 0.8121926829999211
Test Epoch40 layer2 Acc 0.7824, AUC 0.862586498260498, avg_entr 0.04238360375165939, f1 0.7824000120162964
ep40_l2_test_time 1.1362880639999275
Test Epoch40 layer3 Acc 0.7814, AUC 0.8624449968338013, avg_entr 0.03978557139635086, f1 0.7814000248908997
ep40_l3_test_time 1.6194071120003173
Test Epoch40 layer4 Acc 0.7814, AUC 0.8622467517852783, avg_entr 0.038046885281801224, f1 0.7814000248908997
ep40_l4_test_time 2.2532590679998066
gc 0
Train Epoch41 Acc 0.964 (38560/40000), AUC 0.9932781457901001
ep41_train_time 65.5735716510003
Test Epoch41 layer0 Acc 0.7718, AUC 0.8528990149497986, avg_entr 0.14467620849609375, f1 0.7717999815940857
ep41_l0_test_time 0.6194801530000404
Test Epoch41 layer1 Acc 0.782, AUC 0.846969723701477, avg_entr 0.04617965221405029, f1 0.7820000052452087
ep41_l1_test_time 0.8152777010000136
Test Epoch41 layer2 Acc 0.7818, AUC 0.8624206781387329, avg_entr 0.04215070977807045, f1 0.7817999720573425
ep41_l2_test_time 1.142107340000166
Test Epoch41 layer3 Acc 0.783, AUC 0.8623331785202026, avg_entr 0.03973088413476944, f1 0.7829999923706055
ep41_l3_test_time 1.6261323830003676
Test Epoch41 layer4 Acc 0.7822, AUC 0.862035870552063, avg_entr 0.03808579966425896, f1 0.7821999788284302
ep41_l4_test_time 2.262853807999818
gc 0
Train Epoch42 Acc 0.96375 (38550/40000), AUC 0.9927864670753479
ep42_train_time 65.58254606499986
Test Epoch42 layer0 Acc 0.773, AUC 0.8531298637390137, avg_entr 0.14386025071144104, f1 0.7730000019073486
ep42_l0_test_time 0.6158668170000965
Test Epoch42 layer1 Acc 0.7814, AUC 0.8470066785812378, avg_entr 0.0457368828356266, f1 0.7814000248908997
ep42_l1_test_time 0.8204099040003712
Test Epoch42 layer2 Acc 0.7826, AUC 0.8628968596458435, avg_entr 0.041926950216293335, f1 0.7825999855995178
ep42_l2_test_time 1.133937256000081
Test Epoch42 layer3 Acc 0.782, AUC 0.8629177808761597, avg_entr 0.03954499959945679, f1 0.7820000052452087
ep42_l3_test_time 1.617540661000021
Test Epoch42 layer4 Acc 0.7816, AUC 0.862632691860199, avg_entr 0.03769342228770256, f1 0.7815999984741211
ep42_l4_test_time 2.2531000349999886
gc 0
Train Epoch43 Acc 0.9648 (38592/40000), AUC 0.9930579662322998
ep43_train_time 65.64266258899988
Test Epoch43 layer0 Acc 0.7726, AUC 0.8529009819030762, avg_entr 0.14432412385940552, f1 0.772599995136261
ep43_l0_test_time 0.6238855189999413
Test Epoch43 layer1 Acc 0.7808, AUC 0.8467592000961304, avg_entr 0.04569348692893982, f1 0.7808000445365906
ep43_l1_test_time 0.8109099099997366
Test Epoch43 layer2 Acc 0.7826, AUC 0.8623365163803101, avg_entr 0.041702479124069214, f1 0.7825999855995178
ep43_l2_test_time 1.1336858130002838
Test Epoch43 layer3 Acc 0.7834, AUC 0.8622564077377319, avg_entr 0.03920059651136398, f1 0.7833999395370483
ep43_l3_test_time 1.6161963399999877
Test Epoch43 layer4 Acc 0.7826, AUC 0.8618612289428711, avg_entr 0.03740561008453369, f1 0.7825999855995178
ep43_l4_test_time 2.255927294999765
gc 0
Train Epoch44 Acc 0.9643 (38572/40000), AUC 0.993111252784729
ep44_train_time 65.62195504200008
Test Epoch44 layer0 Acc 0.7722, AUC 0.852912187576294, avg_entr 0.1438789814710617, f1 0.7721999287605286
ep44_l0_test_time 0.6180873640000755
Test Epoch44 layer1 Acc 0.7818, AUC 0.8463764786720276, avg_entr 0.04570373520255089, f1 0.7817999720573425
ep44_l1_test_time 0.8102964959998644
Test Epoch44 layer2 Acc 0.7826, AUC 0.8621107935905457, avg_entr 0.041540492326021194, f1 0.7825999855995178
ep44_l2_test_time 1.131634628000029
Test Epoch44 layer3 Acc 0.7826, AUC 0.8621103167533875, avg_entr 0.03912105783820152, f1 0.7825999855995178
ep44_l3_test_time 1.6167793229997187
Test Epoch44 layer4 Acc 0.7812, AUC 0.8617323637008667, avg_entr 0.03731375187635422, f1 0.7811999917030334
ep44_l4_test_time 2.254055958000208
gc 0
Train Epoch45 Acc 0.965125 (38605/40000), AUC 0.9934160113334656
ep45_train_time 65.6178392060001
Test Epoch45 layer0 Acc 0.7708, AUC 0.8529072403907776, avg_entr 0.1439855545759201, f1 0.7707999348640442
ep45_l0_test_time 0.6179241919999185
Test Epoch45 layer1 Acc 0.782, AUC 0.8462034463882446, avg_entr 0.04563065618276596, f1 0.7820000052452087
ep45_l1_test_time 0.8125691929999448
Test Epoch45 layer2 Acc 0.7832, AUC 0.8619526624679565, avg_entr 0.041402433067560196, f1 0.7832000851631165
ep45_l2_test_time 1.134991350999826
Test Epoch45 layer3 Acc 0.7826, AUC 0.8620379567146301, avg_entr 0.03901529312133789, f1 0.7825999855995178
ep45_l3_test_time 1.6192021830001977
Test Epoch45 layer4 Acc 0.7812, AUC 0.8616145849227905, avg_entr 0.037247903645038605, f1 0.7811999917030334
ep45_l4_test_time 2.254545513999801
gc 0
Train Epoch46 Acc 0.9643 (38572/40000), AUC 0.9933121204376221
ep46_train_time 65.64330089399982
Test Epoch46 layer0 Acc 0.7712, AUC 0.8529177904129028, avg_entr 0.14399828016757965, f1 0.7712000012397766
ep46_l0_test_time 0.6193606100000579
Test Epoch46 layer1 Acc 0.7806, AUC 0.8463711738586426, avg_entr 0.045651715248823166, f1 0.7806000113487244
ep46_l1_test_time 0.8111809639999592
Test Epoch46 layer2 Acc 0.7824, AUC 0.8620553016662598, avg_entr 0.04141323268413544, f1 0.7824000120162964
ep46_l2_test_time 1.1345472150001115
Test Epoch46 layer3 Acc 0.782, AUC 0.8621501922607422, avg_entr 0.03900208696722984, f1 0.7820000052452087
ep46_l3_test_time 1.6157486130000507
Test Epoch46 layer4 Acc 0.7814, AUC 0.8617326021194458, avg_entr 0.037209346890449524, f1 0.7814000248908997
ep46_l4_test_time 2.2532470010000907
gc 0
Train Epoch47 Acc 0.96525 (38610/40000), AUC 0.9932710528373718
ep47_train_time 65.57965097700026
Test Epoch47 layer0 Acc 0.7728, AUC 0.8529651165008545, avg_entr 0.14351995289325714, f1 0.7728000283241272
ep47_l0_test_time 0.6247238119999565
Test Epoch47 layer1 Acc 0.7806, AUC 0.8465356826782227, avg_entr 0.045565903186798096, f1 0.7806000113487244
ep47_l1_test_time 0.8115926820000823
Test Epoch47 layer2 Acc 0.782, AUC 0.8623337745666504, avg_entr 0.041419003158807755, f1 0.7820000052452087
ep47_l2_test_time 1.1384841559997767
Test Epoch47 layer3 Acc 0.782, AUC 0.8624346256256104, avg_entr 0.03893953934311867, f1 0.7820000052452087
ep47_l3_test_time 1.617907882000054
Test Epoch47 layer4 Acc 0.7822, AUC 0.861916184425354, avg_entr 0.03692314401268959, f1 0.7821999788284302
ep47_l4_test_time 2.2536225369999556
gc 0
Train Epoch48 Acc 0.96485 (38594/40000), AUC 0.9931893348693848
ep48_train_time 65.64481117900004
Test Epoch48 layer0 Acc 0.7708, AUC 0.8528152108192444, avg_entr 0.14359746873378754, f1 0.7707999348640442
ep48_l0_test_time 0.6201871750004102
Test Epoch48 layer1 Acc 0.7814, AUC 0.8462772369384766, avg_entr 0.0453757643699646, f1 0.7814000248908997
ep48_l1_test_time 0.8167857809999077
Test Epoch48 layer2 Acc 0.7824, AUC 0.8619430065155029, avg_entr 0.041186802089214325, f1 0.7824000120162964
ep48_l2_test_time 1.1415846879999663
Test Epoch48 layer3 Acc 0.7818, AUC 0.8620809316635132, avg_entr 0.0387384258210659, f1 0.7817999720573425
ep48_l3_test_time 1.6213323619999755
Test Epoch48 layer4 Acc 0.781, AUC 0.8616603016853333, avg_entr 0.03685610741376877, f1 0.781000018119812
ep48_l4_test_time 2.254099318000044
gc 0
Train Epoch49 Acc 0.965475 (38619/40000), AUC 0.9934367537498474
ep49_train_time 65.56761859599965
Test Epoch49 layer0 Acc 0.7712, AUC 0.8527949452400208, avg_entr 0.14367328584194183, f1 0.7712000012397766
ep49_l0_test_time 0.6211757860000944
Test Epoch49 layer1 Acc 0.7814, AUC 0.8461045622825623, avg_entr 0.04527898132801056, f1 0.7814000248908997
ep49_l1_test_time 0.8102530219998698
Test Epoch49 layer2 Acc 0.7814, AUC 0.8620315790176392, avg_entr 0.041129279881715775, f1 0.7814000248908997
ep49_l2_test_time 1.1348256390001552
Test Epoch49 layer3 Acc 0.7828, AUC 0.8621297478675842, avg_entr 0.0386747308075428, f1 0.782800018787384
ep49_l3_test_time 1.621772564999901
Test Epoch49 layer4 Acc 0.7816, AUC 0.8616078495979309, avg_entr 0.03671243414282799, f1 0.7815999984741211
ep49_l4_test_time 2.25657307199981
Best AUC tensor(0.8096) 11 3
train_as_loss [[8.64554987e+01 5.90195634e+01 5.21565067e+01 5.05105877e+01
  4.99219829e+01 4.96497948e+01 4.95025198e+01 4.94140795e+01
  4.93569057e+01 4.93178753e+01 4.92900810e+01 4.92696118e+01
  4.92541213e+01 4.92421348e+01 4.92326825e+01 4.92251106e+01
  4.92189555e+01 4.92150272e+01 4.92126515e+01 4.92104085e+01
  4.92082989e+01 4.92067897e+01 4.92057955e+01 4.92048002e+01
  4.92038052e+01 4.92030586e+01 4.92025454e+01 4.92020184e+01
  4.92014731e+01 4.92010543e+01 4.92007597e+01 4.92004524e+01
  4.92001292e+01 4.91998769e+01 4.91996989e+01 4.91995087e+01
  4.91993083e+01 4.91991485e+01 4.91990350e+01 4.91989155e+01
  4.91987864e+01 4.91986820e+01 4.91986108e+01 4.91985315e+01
  4.91984479e+01 4.91983824e+01 4.91983317e+01 4.91982820e+01
  4.91982258e+01 4.91981803e+01]
 [2.40669370e+00 2.81794058e-04 1.25092446e-05 2.93555660e-06
  1.14426462e-06 5.49077177e-07 3.03611292e-07 1.84811785e-07
  1.15809310e-07 7.78364046e-08 5.34927758e-08 3.95671385e-08
  2.85004089e-08 2.09439626e-08 3.54563257e-07 1.78895771e-08
  4.22173872e-08 9.38292530e-09 6.50041241e-09 5.66074720e-09
  2.16416228e-08 5.02652557e-09 4.08907692e-09 3.84518557e-09
  1.49240035e-08 3.47434922e-09 3.11882521e-09 2.98248908e-09
  1.17876447e-08 2.74759920e-09 2.59100483e-09 2.51759572e-09
  1.00235903e-08 2.37627254e-09 2.29473693e-09 2.22457147e-09
  8.75330792e-09 2.16619420e-09 2.06019310e-09 2.00325740e-09
  7.72553222e-09 2.04518776e-09 1.86804074e-09 1.79785114e-09
  6.68575372e-09 2.02585899e-09 1.73368528e-09 1.65129598e-09
  5.77083817e-09 2.09955592e-09]
 [2.54912231e+00 5.07875168e-04 1.41974990e-05 3.20063120e-06
  1.21613361e-06 5.65050358e-07 3.08277656e-07 1.89365900e-07
  1.12843873e-07 7.71452079e-08 5.12796925e-08 4.05476279e-08
  2.90210414e-08 1.92423196e-08 1.02743012e-07 1.67665048e-08
  4.46476104e-08 1.31738228e-08 5.80511973e-09 5.15187608e-09
  2.00971967e-08 5.85980412e-09 3.57860391e-09 3.44481048e-09
  1.31514388e-08 3.58455855e-09 2.66398422e-09 2.59327989e-09
  1.01089479e-08 2.62185737e-09 2.18715217e-09 2.15271189e-09
  8.49518664e-09 2.18091565e-09 1.94077577e-09 1.89267409e-09
  7.40312437e-09 1.97762717e-09 1.74763928e-09 1.69770185e-09
  6.50348782e-09 1.91756933e-09 1.62406675e-09 1.57030103e-09
  5.71000822e-09 1.96521092e-09 1.57297599e-09 1.48771850e-09
  5.08835149e-09 2.19742552e-09]
 [2.39364319e+00 4.06032617e-04 1.01997282e-05 2.55949621e-06
  1.08818142e-06 5.36415725e-07 3.16310110e-07 2.06562620e-07
  1.21766429e-07 8.80508172e-08 5.95606309e-08 5.21237263e-08
  4.12777972e-08 2.36512690e-08 5.73737089e-08 2.31172883e-08
  5.98717731e-08 2.68971985e-08 6.46753646e-09 6.07458119e-09
  2.33832751e-08 9.28663631e-09 3.80889306e-09 3.88798213e-09
  1.39138496e-08 4.65904147e-09 2.67985594e-09 2.71486086e-09
  1.01650104e-08 2.99777889e-09 2.15080321e-09 2.17910195e-09
  8.38941093e-09 2.33360235e-09 1.91131368e-09 1.86802644e-09
  7.17958082e-09 2.07233323e-09 1.70038165e-09 1.66350750e-09
  6.24908949e-09 1.99884134e-09 1.59004931e-09 1.54439791e-09
  5.41486438e-09 2.07787139e-09 1.56343874e-09 1.48663607e-09
  4.80670915e-09 2.31298455e-09]
 [2.01943267e+00 7.88180835e-04 1.03901089e-05 2.69106973e-06
  1.24331697e-06 6.37529093e-07 4.05217494e-07 2.74217689e-07
  1.61464632e-07 1.20157436e-07 8.40583044e-08 8.21316987e-08
  7.42468284e-08 4.02794741e-08 7.81497324e-08 4.75711555e-08
  1.01137496e-07 6.28188998e-08 8.32519320e-09 8.58851786e-09
  3.06285126e-08 1.73178694e-08 4.29497998e-09 4.86199034e-09
  1.51731820e-08 6.67075699e-09 2.68144583e-09 2.88748670e-09
  1.00722063e-08 3.59667942e-09 2.03800382e-09 2.15015367e-09
  8.01585789e-09 2.47292722e-09 1.79827417e-09 1.75972013e-09
  6.61423200e-09 2.09044892e-09 1.57202342e-09 1.54185003e-09
  5.68169379e-09 1.97067871e-09 1.46834641e-09 1.42163119e-09
  4.88902186e-09 2.04998942e-09 1.46040578e-09 1.37467116e-09
  4.32860729e-09 2.27957931e-09]]
train_ae_loss [[4.09265712 2.94116532 3.77314272 4.27738097 4.53612485 4.65393021
  4.7527173  4.81808428 4.87726512 4.83091661 4.85430883 4.77967227
  4.6704313  4.63246746 4.59198396 4.42235009 4.35356124 3.90290105
  3.76273387 3.68859148 3.63165271 3.33778355 3.26983164 3.20929252
  3.18607383 3.07090749 3.00536869 3.00882838 2.9688299  2.9141713
  2.88382709 2.88571651 2.87264933 2.8550398  2.81972746 2.81560304
  2.8083148  2.81266917 2.80588964 2.82160214 2.78406103 2.79454179
  2.78169177 2.8123906  2.79193139 2.79167383 2.79298134 2.79844892
  2.78885344 2.80312046]
 [3.4827675  2.99923136 3.90736777 4.20153685 4.3180123  4.34755014
  4.40369761 4.41775169 4.4187285  4.25640922 4.20342891 4.04633035
  3.82553041 3.71395885 3.67560399 3.41471239 3.13798426 2.7008299
  2.51780846 2.44793183 2.27695065 1.98750639 1.9017182  1.86126983
  1.77162408 1.67643804 1.59148158 1.57515646 1.52240251 1.4598326
  1.44412792 1.45377983 1.41077265 1.40319573 1.37302143 1.36751883
  1.33957926 1.35263246 1.3445382  1.35071759 1.33315325 1.3354696
  1.30585624 1.3373592  1.3092259  1.31630453 1.31954767 1.32697461
  1.3167297  1.31367131]
 [4.29204122 2.8489423  3.92457064 4.03192988 4.04625437 4.00960706
  4.02146761 3.96261465 3.93497328 3.7552175  3.74704909 3.61357484
  3.40118456 3.30406276 3.28166381 3.03152726 2.75106949 2.36250132
  2.19368109 2.13595407 1.97850827 1.71352303 1.63792827 1.60835756
  1.52014954 1.44012352 1.36301159 1.34657626 1.29748946 1.23949611
  1.2295709  1.23530854 1.19664712 1.19156125 1.16280321 1.15989939
  1.13594146 1.1450457  1.13723295 1.13995909 1.12349223 1.12807355
  1.1034446  1.12895243 1.1045563  1.11194702 1.11025373 1.11721116
  1.11061381 1.10807994]
 [3.67321138 3.24199034 4.70552938 4.95974795 4.74117428 4.6219047
  4.60039743 4.51299548 4.47826982 4.25491926 4.24468034 4.0890748
  3.83624224 3.72792739 3.71671547 3.41533865 3.07706087 2.63760809
  2.44463763 2.38414269 2.19587011 1.89659371 1.80884808 1.7808815
  1.67372648 1.5830437  1.49861031 1.47967288 1.42163942 1.35517634
  1.34426637 1.35196892 1.30598646 1.30133849 1.26847766 1.26623548
  1.23582888 1.24735512 1.23979276 1.24403711 1.2240483  1.23035701
  1.20411208 1.22750359 1.20089906 1.20938114 1.20988975 1.21475053
  1.20929372 1.20366552]
 [4.14072084 3.0353649  4.40476852 5.11266222 4.54667303 4.31876846
  4.26021311 4.14131249 4.09648136 3.86982672 3.86523972 3.71240552
  3.47126003 3.36913888 3.3715456  3.0750055  2.74993093 2.35277147
  2.18106756 2.12959349 1.95122476 1.68198318 1.6028842  1.5802048
  1.47663737 1.39533057 1.3208922  1.30107804 1.24680805 1.18852791
  1.17736091 1.18506886 1.13892588 1.137307   1.1068275  1.10710873
  1.07616236 1.08822719 1.08010494 1.08792327 1.06772758 1.0728478
  1.05179945 1.06891981 1.04447843 1.0520621  1.05555735 1.05778773
  1.05383269 1.04630683]]
valid_acc (5, 50)
valid_AUC (5, 50)
train_acc (50,)
total_train+valid_time 3609.8637688820004
Start Testing
Load ckpt at ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt
gc 9
Test layer0 Acc 0.7904, AUC 0.874001145362854, avg_entr 0.2181626409292221, f1 0.7904000282287598
l0_test_time 0.6235285660000045
gc 0
Test layer1 Acc 0.8074, AUC 0.8929380774497986, avg_entr 0.1588016301393509, f1 0.8073999881744385
l1_test_time 0.8176541349998843
gc 0
Test layer2 Acc 0.8094, AUC 0.8937027454376221, avg_entr 0.13798406720161438, f1 0.8094000220298767
l2_test_time 1.1414003870004308
gc 0
Test layer3 Acc 0.8102, AUC 0.8937151432037354, avg_entr 0.13962015509605408, f1 0.8101999759674072
l3_test_time 1.628952948999995
gc 0
Test layer4 Acc 0.8108, AUC 0.8934015035629272, avg_entr 0.142970010638237, f1 0.8108000159263611
l4_test_time 2.263075580000077
gc 0
Test threshold 0.1 Acc 0.8108, AUC 0.8894059658050537, avg_entr 0.20402103662490845, f1 0.8108000159263611
t0.1_test_time 1.3317366800001764
gc 0
Test threshold 0.2 Acc 0.8096, AUC 0.8853449821472168, avg_entr 0.20977136492729187, f1 0.8095999956130981
t0.2_test_time 1.1700824430004104
gc 0
Test threshold 0.3 Acc 0.8092, AUC 0.8844833374023438, avg_entr 0.2174396961927414, f1 0.8092000484466553
t0.3_test_time 1.0864351079999324
gc 0
Test threshold 0.4 Acc 0.808, AUC 0.8830126523971558, avg_entr 0.22402282059192657, f1 0.8080000281333923
t0.4_test_time 0.9997003240000595
gc 0
Test threshold 0.5 Acc 0.8066, AUC 0.880899965763092, avg_entr 0.23214462399482727, f1 0.8065999746322632
t0.5_test_time 0.940425686999788
gc 0
Test threshold 0.6 Acc 0.8068, AUC 0.8808546662330627, avg_entr 0.2402530312538147, f1 0.8068000078201294
t0.6_test_time 0.8881175820001772
gc 0
Test threshold 0.7 Acc 0.8054, AUC 0.8791873455047607, avg_entr 0.25079217553138733, f1 0.805400013923645
t0.7_test_time 0.8326156579996677
gc 0
Test threshold 0.8 Acc 0.8032, AUC 0.8777254819869995, avg_entr 0.2647608816623688, f1 0.8032000660896301
t0.8_test_time 0.7866070430000036
gc 0
Test threshold 0.9 Acc 0.7994, AUC 0.8765217065811157, avg_entr 0.2833402454853058, f1 0.7993999123573303
t0.9_test_time 0.7295239360000778
