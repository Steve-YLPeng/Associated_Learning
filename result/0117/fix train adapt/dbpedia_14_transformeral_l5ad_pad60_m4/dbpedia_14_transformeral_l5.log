total count words 887881
vocab size 30000
found 28354 words in glove
Load ckpt from ckpt/dbpedia_14_transformeral_l5_pad60_m3//dbpedia_14_transformeral_l5_prefix.pt
model: TransformerModelML(
  (layers): ModuleList(
    (0): EMBLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Embedding(30000, 300)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=14, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=14, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.weight 9000000
layers.0.ae.g.0.weight 1792
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 1792
layers.0.ae.h.0.bias 14
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
Total Trainable Params: 13674862
Start Training
train_mask {0, 1, 2, 3}
gc 9
Train Epoch0 Acc 0.012514285714285714 (7008/560000), AUC 0.4606587588787079
ep0_train_time 138.2170214653015
Test Epoch0 threshold 0.1 Acc 0.9764142857142857, AUC 0.9982694387435913, avg_entr 0.0040915426798164845
ep0_t0.1_test_time 1.8496553897857666
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m4//dbpedia_14_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.2 Acc 0.9749714285714286, AUC 0.9983679056167603, avg_entr 0.008897211402654648
ep0_t0.2_test_time 1.5415902137756348
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m4//dbpedia_14_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.3 Acc 0.9747571428571429, AUC 0.9983676671981812, avg_entr 0.00916357059031725
ep0_t0.3_test_time 1.458315134048462
Test Epoch0 threshold 0.4 Acc 0.9747571428571429, AUC 0.9983676671981812, avg_entr 0.00916357059031725
ep0_t0.4_test_time 1.475264072418213
Test Epoch0 threshold 0.5 Acc 0.9747571428571429, AUC 0.9983676671981812, avg_entr 0.00916357059031725
ep0_t0.5_test_time 1.4553425312042236
Test Epoch0 threshold 0.6 Acc 0.9747571428571429, AUC 0.9983676671981812, avg_entr 0.00916357059031725
ep0_t0.6_test_time 1.500990629196167
Test Epoch0 threshold 0.7 Acc 0.9747571428571429, AUC 0.9983676671981812, avg_entr 0.00916357059031725
ep0_t0.7_test_time 1.4661173820495605
Test Epoch0 threshold 0.8 Acc 0.9747571428571429, AUC 0.9983676671981812, avg_entr 0.00916357059031725
ep0_t0.8_test_time 1.452946662902832
Test Epoch0 threshold 0.9 Acc 0.9747571428571429, AUC 0.9983676671981812, avg_entr 0.00916357059031725
ep0_t0.9_test_time 1.5258512496948242
gc 0
Train Epoch1 Acc 0.030301785714285715 (16969/560000), AUC 0.44756558537483215
ep1_train_time 136.64454889297485
Test Epoch1 threshold 0.1 Acc 0.9763285714285714, AUC 0.9982242584228516, avg_entr 0.004100581631064415
ep1_t0.1_test_time 1.8452551364898682
Test Epoch1 threshold 0.2 Acc 0.9750714285714286, AUC 0.9983565211296082, avg_entr 0.008803789503872395
ep1_t0.2_test_time 1.5210912227630615
Test Epoch1 threshold 0.3 Acc 0.9748428571428571, AUC 0.9983536601066589, avg_entr 0.00910982582718134
ep1_t0.3_test_time 1.4589028358459473
Test Epoch1 threshold 0.4 Acc 0.9748428571428571, AUC 0.9983536601066589, avg_entr 0.00910982582718134
ep1_t0.4_test_time 1.4652214050292969
Test Epoch1 threshold 0.5 Acc 0.9748428571428571, AUC 0.9983536601066589, avg_entr 0.00910982582718134
ep1_t0.5_test_time 1.460719108581543
Test Epoch1 threshold 0.6 Acc 0.9748428571428571, AUC 0.9983536601066589, avg_entr 0.00910982582718134
ep1_t0.6_test_time 1.4601964950561523
Test Epoch1 threshold 0.7 Acc 0.9748428571428571, AUC 0.9983536601066589, avg_entr 0.00910982582718134
ep1_t0.7_test_time 1.465198278427124
Test Epoch1 threshold 0.8 Acc 0.9748428571428571, AUC 0.9983536601066589, avg_entr 0.00910982582718134
ep1_t0.8_test_time 1.4681529998779297
Test Epoch1 threshold 0.9 Acc 0.9748428571428571, AUC 0.9983536601066589, avg_entr 0.00910982582718134
ep1_t0.9_test_time 1.4712936878204346
gc 0
Train Epoch2 Acc 0.010255357142857142 (5743/560000), AUC 0.445441871881485
ep2_train_time 136.46250534057617
Test Epoch2 threshold 0.1 Acc 0.9766857142857143, AUC 0.9982313513755798, avg_entr 0.004068190231919289
ep2_t0.1_test_time 1.8509275913238525
Test Epoch2 threshold 0.2 Acc 0.9752571428571428, AUC 0.998353898525238, avg_entr 0.00881531834602356
ep2_t0.2_test_time 1.5217616558074951
Test Epoch2 threshold 0.3 Acc 0.9749857142857142, AUC 0.9983510971069336, avg_entr 0.009125697426497936
ep2_t0.3_test_time 1.462785005569458
Test Epoch2 threshold 0.4 Acc 0.9749857142857142, AUC 0.9983510971069336, avg_entr 0.009125697426497936
ep2_t0.4_test_time 1.455322027206421
Test Epoch2 threshold 0.5 Acc 0.9749857142857142, AUC 0.9983510971069336, avg_entr 0.009125697426497936
ep2_t0.5_test_time 1.446821689605713
Test Epoch2 threshold 0.6 Acc 0.9749857142857142, AUC 0.9983510971069336, avg_entr 0.009125697426497936
ep2_t0.6_test_time 1.4653520584106445
Test Epoch2 threshold 0.7 Acc 0.9749857142857142, AUC 0.9983510971069336, avg_entr 0.009125697426497936
ep2_t0.7_test_time 1.4558589458465576
Test Epoch2 threshold 0.8 Acc 0.9749857142857142, AUC 0.9983510971069336, avg_entr 0.009125697426497936
ep2_t0.8_test_time 1.4528610706329346
Test Epoch2 threshold 0.9 Acc 0.9749857142857142, AUC 0.9983510971069336, avg_entr 0.009125697426497936
ep2_t0.9_test_time 1.476926565170288
gc 0
Train Epoch3 Acc 0.0052589285714285715 (2945/560000), AUC 0.4449734389781952
ep3_train_time 137.09948229789734
Test Epoch3 threshold 0.1 Acc 0.9765142857142857, AUC 0.9982389807701111, avg_entr 0.00408307695761323
ep3_t0.1_test_time 1.8454194068908691
Test Epoch3 threshold 0.2 Acc 0.9752571428571428, AUC 0.9983533620834351, avg_entr 0.008836375549435616
ep3_t0.2_test_time 1.518829107284546
Test Epoch3 threshold 0.3 Acc 0.9749714285714286, AUC 0.9983504414558411, avg_entr 0.009137455374002457
ep3_t0.3_test_time 1.4579999446868896
Test Epoch3 threshold 0.4 Acc 0.9749714285714286, AUC 0.9983504414558411, avg_entr 0.009137455374002457
ep3_t0.4_test_time 1.4509930610656738
Test Epoch3 threshold 0.5 Acc 0.9749714285714286, AUC 0.9983504414558411, avg_entr 0.009137455374002457
ep3_t0.5_test_time 1.4687161445617676
Test Epoch3 threshold 0.6 Acc 0.9749714285714286, AUC 0.9983504414558411, avg_entr 0.009137455374002457
ep3_t0.6_test_time 1.4867911338806152
Test Epoch3 threshold 0.7 Acc 0.9749714285714286, AUC 0.9983504414558411, avg_entr 0.009137455374002457
ep3_t0.7_test_time 1.4652354717254639
Test Epoch3 threshold 0.8 Acc 0.9749714285714286, AUC 0.9983504414558411, avg_entr 0.009137455374002457
ep3_t0.8_test_time 1.4553098678588867
Test Epoch3 threshold 0.9 Acc 0.9749714285714286, AUC 0.9983504414558411, avg_entr 0.009137455374002457
ep3_t0.9_test_time 1.4534239768981934
gc 0
Train Epoch4 Acc 0.004125 (2310/560000), AUC 0.4449048638343811
ep4_train_time 136.6413073539734
Test Epoch4 threshold 0.1 Acc 0.9765428571428572, AUC 0.9982226490974426, avg_entr 0.004081541206687689
ep4_t0.1_test_time 1.8327951431274414
Test Epoch4 threshold 0.2 Acc 0.9752, AUC 0.9983511567115784, avg_entr 0.008818930946290493
ep4_t0.2_test_time 1.5456395149230957
Test Epoch4 threshold 0.3 Acc 0.9749285714285715, AUC 0.9983484148979187, avg_entr 0.009124048054218292
ep4_t0.3_test_time 1.4567880630493164
Test Epoch4 threshold 0.4 Acc 0.9749285714285715, AUC 0.9983484148979187, avg_entr 0.009124048054218292
ep4_t0.4_test_time 1.4586365222930908
Test Epoch4 threshold 0.5 Acc 0.9749285714285715, AUC 0.9983484148979187, avg_entr 0.009124048054218292
ep4_t0.5_test_time 1.46065354347229
Test Epoch4 threshold 0.6 Acc 0.9749285714285715, AUC 0.9983484148979187, avg_entr 0.009124048054218292
ep4_t0.6_test_time 1.471132755279541
Test Epoch4 threshold 0.7 Acc 0.9749285714285715, AUC 0.9983484148979187, avg_entr 0.009124048054218292
ep4_t0.7_test_time 1.4710333347320557
Test Epoch4 threshold 0.8 Acc 0.9749285714285715, AUC 0.9983484148979187, avg_entr 0.009124048054218292
ep4_t0.8_test_time 1.4602901935577393
Test Epoch4 threshold 0.9 Acc 0.9749285714285715, AUC 0.9983484148979187, avg_entr 0.009124048054218292
ep4_t0.9_test_time 1.4661593437194824
gc 0
Train Epoch5 Acc 0.003819642857142857 (2139/560000), AUC 0.444821298122406
ep5_train_time 136.62009239196777
Test Epoch5 threshold 0.1 Acc 0.9765571428571429, AUC 0.9982224702835083, avg_entr 0.0040710195899009705
ep5_t0.1_test_time 1.8784804344177246
Test Epoch5 threshold 0.2 Acc 0.9752142857142857, AUC 0.9983507990837097, avg_entr 0.008828474208712578
ep5_t0.2_test_time 1.5100960731506348
Test Epoch5 threshold 0.3 Acc 0.9749428571428571, AUC 0.9983482360839844, avg_entr 0.009129798971116543
ep5_t0.3_test_time 1.4551410675048828
Test Epoch5 threshold 0.4 Acc 0.9749428571428571, AUC 0.9983482360839844, avg_entr 0.009129798971116543
ep5_t0.4_test_time 1.4649872779846191
Test Epoch5 threshold 0.5 Acc 0.9749428571428571, AUC 0.9983482360839844, avg_entr 0.009129798971116543
ep5_t0.5_test_time 1.4498226642608643
Test Epoch5 threshold 0.6 Acc 0.9749428571428571, AUC 0.9983482360839844, avg_entr 0.009129798971116543
ep5_t0.6_test_time 1.4478578567504883
Test Epoch5 threshold 0.7 Acc 0.9749428571428571, AUC 0.9983482360839844, avg_entr 0.009129798971116543
ep5_t0.7_test_time 1.4605681896209717
Test Epoch5 threshold 0.8 Acc 0.9749428571428571, AUC 0.9983482360839844, avg_entr 0.009129798971116543
ep5_t0.8_test_time 1.4672389030456543
Test Epoch5 threshold 0.9 Acc 0.9749428571428571, AUC 0.9983482360839844, avg_entr 0.009129798971116543
ep5_t0.9_test_time 1.4679045677185059
gc 0
Train Epoch6 Acc 0.0037660714285714285 (2109/560000), AUC 0.4447786211967468
ep6_train_time 136.7009711265564
Test Epoch6 threshold 0.1 Acc 0.9765428571428572, AUC 0.9982224702835083, avg_entr 0.004068422131240368
ep6_t0.1_test_time 1.8589816093444824
Test Epoch6 threshold 0.2 Acc 0.9752, AUC 0.9983506798744202, avg_entr 0.008828811347484589
ep6_t0.2_test_time 1.527914047241211
Test Epoch6 threshold 0.3 Acc 0.9749285714285715, AUC 0.9983479380607605, avg_entr 0.00913012120872736
ep6_t0.3_test_time 1.45804762840271
Test Epoch6 threshold 0.4 Acc 0.9749285714285715, AUC 0.9983479380607605, avg_entr 0.00913012120872736
ep6_t0.4_test_time 1.4618124961853027
Test Epoch6 threshold 0.5 Acc 0.9749285714285715, AUC 0.9983479380607605, avg_entr 0.00913012120872736
ep6_t0.5_test_time 1.4768447875976562
Test Epoch6 threshold 0.6 Acc 0.9749285714285715, AUC 0.9983479380607605, avg_entr 0.00913012120872736
ep6_t0.6_test_time 1.4561192989349365
Test Epoch6 threshold 0.7 Acc 0.9749285714285715, AUC 0.9983479380607605, avg_entr 0.00913012120872736
ep6_t0.7_test_time 1.477856159210205
Test Epoch6 threshold 0.8 Acc 0.9749285714285715, AUC 0.9983479380607605, avg_entr 0.00913012120872736
ep6_t0.8_test_time 1.4697513580322266
Test Epoch6 threshold 0.9 Acc 0.9749285714285715, AUC 0.9983479380607605, avg_entr 0.00913012120872736
ep6_t0.9_test_time 1.46405029296875
gc 0
Train Epoch7 Acc 0.0038232142857142857 (2141/560000), AUC 0.4448344111442566
ep7_train_time 136.50267243385315
Test Epoch7 threshold 0.1 Acc 0.9765428571428572, AUC 0.9982224106788635, avg_entr 0.004068450070917606
ep7_t0.1_test_time 1.8574070930480957
Test Epoch7 threshold 0.2 Acc 0.9752, AUC 0.9983506798744202, avg_entr 0.008828867226839066
ep7_t0.2_test_time 1.5627353191375732
Test Epoch7 threshold 0.3 Acc 0.9749285714285715, AUC 0.9983481168746948, avg_entr 0.009130184538662434
ep7_t0.3_test_time 1.465764045715332
Test Epoch7 threshold 0.4 Acc 0.9749285714285715, AUC 0.9983481168746948, avg_entr 0.009130184538662434
ep7_t0.4_test_time 1.460500955581665
Test Epoch7 threshold 0.5 Acc 0.9749285714285715, AUC 0.9983481168746948, avg_entr 0.009130184538662434
ep7_t0.5_test_time 1.4666938781738281
Test Epoch7 threshold 0.6 Acc 0.9749285714285715, AUC 0.9983481168746948, avg_entr 0.009130184538662434
ep7_t0.6_test_time 1.4691669940948486
Test Epoch7 threshold 0.7 Acc 0.9749285714285715, AUC 0.9983481168746948, avg_entr 0.009130184538662434
ep7_t0.7_test_time 1.456040382385254
Test Epoch7 threshold 0.8 Acc 0.9749285714285715, AUC 0.9983481168746948, avg_entr 0.009130184538662434
ep7_t0.8_test_time 1.4556937217712402
Test Epoch7 threshold 0.9 Acc 0.9749285714285715, AUC 0.9983481168746948, avg_entr 0.009130184538662434
ep7_t0.9_test_time 1.4693775177001953
gc 0
Train Epoch8 Acc 0.003914285714285714 (2192/560000), AUC 0.44483086466789246
ep8_train_time 136.82072925567627
Test Epoch8 threshold 0.1 Acc 0.9765428571428572, AUC 0.9982223510742188, avg_entr 0.004068383015692234
ep8_t0.1_test_time 1.841627836227417
Test Epoch8 threshold 0.2 Acc 0.9752, AUC 0.9983506798744202, avg_entr 0.00882888026535511
ep8_t0.2_test_time 1.514392614364624
Test Epoch8 threshold 0.3 Acc 0.9749285714285715, AUC 0.9983479380607605, avg_entr 0.009130200371146202
ep8_t0.3_test_time 1.4532291889190674
Test Epoch8 threshold 0.4 Acc 0.9749285714285715, AUC 0.9983479380607605, avg_entr 0.009130200371146202
ep8_t0.4_test_time 1.4765539169311523
Test Epoch8 threshold 0.5 Acc 0.9749285714285715, AUC 0.9983479380607605, avg_entr 0.009130200371146202
ep8_t0.5_test_time 1.444892168045044
Test Epoch8 threshold 0.6 Acc 0.9749285714285715, AUC 0.9983479380607605, avg_entr 0.009130200371146202
ep8_t0.6_test_time 1.4569778442382812
Test Epoch8 threshold 0.7 Acc 0.9749285714285715, AUC 0.9983479380607605, avg_entr 0.009130200371146202
ep8_t0.7_test_time 1.4590249061584473
Test Epoch8 threshold 0.8 Acc 0.9749285714285715, AUC 0.9983479380607605, avg_entr 0.009130200371146202
ep8_t0.8_test_time 1.4538941383361816
Test Epoch8 threshold 0.9 Acc 0.9749285714285715, AUC 0.9983479380607605, avg_entr 0.009130200371146202
ep8_t0.9_test_time 1.4568114280700684
gc 0
Train Epoch9 Acc 0.003726785714285714 (2087/560000), AUC 0.44482722878456116
ep9_train_time 136.99834537506104
Test Epoch9 threshold 0.1 Acc 0.9765428571428572, AUC 0.9982224106788635, avg_entr 0.004069097805768251
ep9_t0.1_test_time 1.8367693424224854
Test Epoch9 threshold 0.2 Acc 0.9752, AUC 0.9983506798744202, avg_entr 0.008828898891806602
ep9_t0.2_test_time 1.5470335483551025
Test Epoch9 threshold 0.3 Acc 0.9749285714285715, AUC 0.9983479380607605, avg_entr 0.009130226448178291
ep9_t0.3_test_time 1.4763283729553223
Test Epoch9 threshold 0.4 Acc 0.9749285714285715, AUC 0.9983479380607605, avg_entr 0.009130226448178291
ep9_t0.4_test_time 1.4618206024169922
Test Epoch9 threshold 0.5 Acc 0.9749285714285715, AUC 0.9983479380607605, avg_entr 0.009130226448178291
ep9_t0.5_test_time 1.4481606483459473
Test Epoch9 threshold 0.6 Acc 0.9749285714285715, AUC 0.9983479380607605, avg_entr 0.009130226448178291
ep9_t0.6_test_time 1.471160888671875
Test Epoch9 threshold 0.7 Acc 0.9749285714285715, AUC 0.9983479380607605, avg_entr 0.009130226448178291
ep9_t0.7_test_time 1.4609081745147705
Test Epoch9 threshold 0.8 Acc 0.9749285714285715, AUC 0.9983479380607605, avg_entr 0.009130226448178291
ep9_t0.8_test_time 1.4742891788482666
Test Epoch9 threshold 0.9 Acc 0.9749285714285715, AUC 0.9983479380607605, avg_entr 0.009130226448178291
ep9_t0.9_test_time 1.4597787857055664
Best AUC 0.9983679056167603
train_loss (2, 5, 10)
valid_acc (10, 9)
valid_AUC (10, 9)
train_acc (10,)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Start Testing
Load ckpt at ckpt/dbpedia_14_transformeral_l5_pad60_m4//dbpedia_14_transformeral_l5_prefix.pt
[[4705   40   21   14   12   66   39    8    4    6    4   16   19   46]
 [  36 4895    2    1    7    0   33    8    3    2    1    0    4    8]
 [  38   13 4606   17   53    3   10    2    4    1    1   85   26  141]
 [   3    2   21 4959   11    0    1    0    0    0    0    0    0    3]
 [  10   20   65   12 4863    8    7    0    2    1    0    0    3    9]
 [  33    1    2    2    1 4949    5    2    1    2    0    0    1    1]
 [  60   45    6    3   10   16 4799   37    9    5    1    1    6    2]
 [   0    1    0    0    2    0   12 4963   17    3    1    0    0    1]
 [   1    2    2    0    4    0   10   12 4969    0    0    0    0    0]
 [   1    0    0    2    0    0    0    7    0 4961   28    0    0    1]
 [  13    1    0    0    0    1    1    4    0   33 4947    0    0    0]
 [   7    0   37    2    0    0    0    0    0    0    0 4931   12   11]
 [  10    2   24    4    1    2    1    3    1    2    0   21 4888   41]
 [  27    2   76    7   14    4    5    9    1    5    2    6   44 4798]]
Figure(640x480)
tensor([1.8280e-07, 9.7817e-02, 1.1377e-04,  ..., 1.2142e-07, 1.6976e-03,
        9.1186e-07])
[[4765   40   19    5   13   36   45    2    0    3    2   13   11   46]
 [  39 4916    2    0    6    0   28    0    1    1    0    0    1    6]
 [  28   14 4716   12   62    2    9    0    0    1    3   40   24   89]
 [   4    1   11 4966   12    0    0    1    0    1    0    2    1    1]
 [   8   10   65   18 4873    6    8    0    1    3    0    1    3    4]
 [  46    0    0    1    0 4944    3    2    1    0    0    0    2    1]
 [  52   41    3    2    8   15 4839   24    5    4    0    1    4    2]
 [   3    1    0    0    0    0   18 4954   14    6    2    0    1    1]
 [   3    1    2    0    4    0   11    9 4970    0    0    0    0    0]
 [   1    1    1    1    0    0    0    2    0 4976   18    0    0    0]
 [  12    1    0    0    0    2    1    1    0   38 4945    0    0    0]
 [   6    0   39    2    0    0    0    0    0    0    0 4935   11    7]
 [   9    1   14    0    0    1    0    0    0    0    0   20 4923   32]
 [  27    8   66    3    2    3    2    0    0    0    1    6   50 4832]]
Figure(640x480)
tensor([9.2051e-08, 1.8547e-01, 2.1893e-07,  ..., 9.9709e-08, 1.3385e-07,
        1.1526e-07])
[[4774   36   21    5   12   37   43    2    0    4    1   10   10   45]
 [  43 4913    2    0    6    0   28    0    1    1    0    0    0    6]
 [  25   12 4727   15   61    2    8    0    0    1    3   37   20   89]
 [   4    1   11 4966   12    0    0    1    0    1    0    2    1    1]
 [   8   10   65   18 4873    6    6    1    1    3    0    1    3    5]
 [  42    0    0    0    0 4951    2    1    1    0    0    0    2    1]
 [  57   39    3    1    6   17 4834   26    5    5    0    1    3    3]
 [   4    1    0    0    0    0   15 4958   13    6    0    0    2    1]
 [   3    1    1    0    4    0   11    9 4971    0    0    0    0    0]
 [   1    1    1    1    0    0    0    2    0 4979   15    0    0    0]
 [  12    1    0    0    0    4    1    2    0   40 4939    0    0    1]
 [   7    0   41    2    0    0    0    0    0    0    0 4928   14    8]
 [  10    2   15    0    0    3    0    0    0    0    0   18 4918   34]
 [  25    7   66    3    2    3    2    0    0    0    1    5   48 4838]]
Figure(640x480)
tensor([2.6607e-07, 1.0297e-04, 2.6967e-07,  ..., 1.4173e-07, 1.3637e-07,
        1.4498e-07])
[[4773   36   21    5   12   37   42    2    0    3    2   11   10   46]
 [  44 4914    2    0    6    0   26    0    1    1    0    0    0    6]
 [  25   12 4728   14   60    2    8    0    0    1    3   36   21   90]
 [   4    1   12 4966   11    0    0    1    0    1    0    2    1    1]
 [   8   11   67   19 4869    6    6    0    1    3    0    1    3    6]
 [  40    0    0    0    0 4952    2    1    1    1    0    0    2    1]
 [  57   41    4    2    7   17 4826   27    5    5    0    1    4    4]
 [   4    1    0    0    0    0   13 4960   13    6    0    0    2    1]
 [   3    1    1    0    4    0   11    9 4971    0    0    0    0    0]
 [   1    1    1    1    0    0    0    2    0 4980   14    0    0    0]
 [  13    1    0    0    0    4    0    1    0   45 4935    0    0    1]
 [   7    0   43    2    0    0    0    0    0    0    0 4927   13    8]
 [  10    2   14    0    0    2    0    0    0    0    0   18 4919   35]
 [  25    6   65    3    2    3    2    0    0    0    1    5   46 4842]]
Figure(640x480)
tensor([1.9343e-07, 3.1941e-05, 1.8670e-07,  ..., 1.4715e-07, 1.3899e-07,
        1.5061e-07])
[[   0    0   45    0    0    0    0    0 4950    0    0    0    0    5]
 [   0    0 4689    0    0    0    0    0  308    0    0    0    0    3]
 [   0    0 2361    0    0    0    0    0 2612    0    0    0    3   24]
 [   0    0 4976    0    0    0    0    0   16    0    0    0    6    2]
 [   0    0   41    0    0    0    0    0  222    0    5    0   44 4688]
 [   0    0   12    0    0    0    0    0 4988    0    0    0    0    0]
 [   0    0   25    0    0    0    0    0 4973    0    1    0    0    1]
 [   0    0   14    0    0    0    0    0 4985    0    0    0    0    1]
 [   0    0 4974    0    0    0    0    0   22    0    0    0    0    4]
 [   0    0   31    0    0    0    0    0 4954    0    0    0    0   15]
 [   0    0 4931    0    0    0    0    0   50    0    0    0    0   19]
 [   0    0 4812    0    0    0    0    0  188    0    0    0    0    0]
 [   0    0 4982    0    0    0    0    0   18    0    0    0    0    0]
 [   0    0 2572    0    0    0    0    0 2427    0    0    0    0    1]]
Figure(640x480)
tensor([1.1632, 1.0834, 1.1241,  ..., 0.4343, 0.4484, 0.4573])
