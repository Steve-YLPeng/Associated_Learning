total count words 222751
vocab size 30000
found 27937 words in glove
Load ckpt from ckpt/imdb_transformeral_l5_pad500_m4//imdb_transformeral_l5_prefix.pt
model: TransformerModelML(
  (layers): ModuleList(
    (0): EMBLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Embedding(30000, 300)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=2, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=2, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.weight 9000000
layers.0.ae.g.0.weight 256
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 256
layers.0.ae.h.0.bias 2
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
Total Trainable Params: 13671778
Start Training
train_mask {0, 1, 2, 3, 4}
gc 9
Train Epoch0 Acc 0.939025 (37561/40000), AUC 0.9733204245567322
ep0_train_time 92.21010136604309
Test Epoch0 threshold 0.1 Acc 0.9381, AUC 0.9719330668449402, avg_entr 0.01418289914727211
ep0_t0.1_test_time 1.467350721359253
Save ckpt to ckpt/imdb_transformeral_l5_pad500_m5//imdb_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.2 Acc 0.937, AUC 0.9736226797103882, avg_entr 0.017524467781186104
ep0_t0.2_test_time 1.4058895111083984
Save ckpt to ckpt/imdb_transformeral_l5_pad500_m5//imdb_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.3 Acc 0.9363, AUC 0.9735614061355591, avg_entr 0.020740538835525513
ep0_t0.3_test_time 1.364332914352417
Test Epoch0 threshold 0.4 Acc 0.9359, AUC 0.9745631217956543, avg_entr 0.02543911524116993
ep0_t0.4_test_time 1.3382329940795898
Save ckpt to ckpt/imdb_transformeral_l5_pad500_m5//imdb_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.5 Acc 0.9354, AUC 0.9752518534660339, avg_entr 0.02884681150317192
ep0_t0.5_test_time 1.2968971729278564
Save ckpt to ckpt/imdb_transformeral_l5_pad500_m5//imdb_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.6 Acc 0.9349, AUC 0.9761940836906433, avg_entr 0.03372516483068466
ep0_t0.6_test_time 1.2720179557800293
Save ckpt to ckpt/imdb_transformeral_l5_pad500_m5//imdb_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.7 Acc 0.9332, AUC 0.9765849709510803, avg_entr 0.03953593596816063
ep0_t0.7_test_time 1.2511317729949951
Save ckpt to ckpt/imdb_transformeral_l5_pad500_m5//imdb_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.8 Acc 0.9324, AUC 0.9762772917747498, avg_entr 0.04608612135052681
ep0_t0.8_test_time 1.2689242362976074
Test Epoch0 threshold 0.9 Acc 0.9325, AUC 0.9770634174346924, avg_entr 0.05460969731211662
ep0_t0.9_test_time 1.178232192993164
Save ckpt to ckpt/imdb_transformeral_l5_pad500_m5//imdb_transformeral_l5_prefix.pt  ,ep 0
gc 0
Train Epoch1 Acc 0.9552 (38208/40000), AUC 0.9859291315078735
ep1_train_time 91.89947152137756
Test Epoch1 threshold 0.1 Acc 0.9332, AUC 0.9696207046508789, avg_entr 0.01292261853814125
ep1_t0.1_test_time 1.4828670024871826
Test Epoch1 threshold 0.2 Acc 0.9326, AUC 0.9707510471343994, avg_entr 0.015540916472673416
ep1_t0.2_test_time 1.4364027976989746
Test Epoch1 threshold 0.3 Acc 0.9329, AUC 0.9719957709312439, avg_entr 0.019168531522154808
ep1_t0.3_test_time 1.3744471073150635
Test Epoch1 threshold 0.4 Acc 0.9326, AUC 0.9732015132904053, avg_entr 0.02331301011145115
ep1_t0.4_test_time 1.3903164863586426
Test Epoch1 threshold 0.5 Acc 0.932, AUC 0.9736737012863159, avg_entr 0.027630474418401718
ep1_t0.5_test_time 1.2910962104797363
Test Epoch1 threshold 0.6 Acc 0.9321, AUC 0.9741551876068115, avg_entr 0.03268882632255554
ep1_t0.6_test_time 1.2634484767913818
Test Epoch1 threshold 0.7 Acc 0.9323, AUC 0.9749088287353516, avg_entr 0.038134150207042694
ep1_t0.7_test_time 1.260704517364502
Test Epoch1 threshold 0.8 Acc 0.9306, AUC 0.974824070930481, avg_entr 0.04379599913954735
ep1_t0.8_test_time 1.2059369087219238
Test Epoch1 threshold 0.9 Acc 0.9295, AUC 0.9759442806243896, avg_entr 0.05155793949961662
ep1_t0.9_test_time 1.199723243713379
gc 0
Train Epoch2 Acc 0.962675 (38507/40000), AUC 0.9885696768760681
ep2_train_time 91.9361252784729
Test Epoch2 threshold 0.1 Acc 0.9328, AUC 0.9671477675437927, avg_entr 0.010384265333414078
ep2_t0.1_test_time 1.433943748474121
Test Epoch2 threshold 0.2 Acc 0.9321, AUC 0.9687216281890869, avg_entr 0.013206304982304573
ep2_t0.2_test_time 1.3871667385101318
Test Epoch2 threshold 0.3 Acc 0.9317, AUC 0.9698472023010254, avg_entr 0.0171805489808321
ep2_t0.3_test_time 1.3305795192718506
Test Epoch2 threshold 0.4 Acc 0.9315, AUC 0.9705260992050171, avg_entr 0.021731562912464142
ep2_t0.4_test_time 1.2965281009674072
Test Epoch2 threshold 0.5 Acc 0.9311, AUC 0.9719855189323425, avg_entr 0.026399889960885048
ep2_t0.5_test_time 1.2602667808532715
Test Epoch2 threshold 0.6 Acc 0.9307, AUC 0.9726241827011108, avg_entr 0.031146276742219925
ep2_t0.6_test_time 1.2389543056488037
Test Epoch2 threshold 0.7 Acc 0.9305, AUC 0.9732344746589661, avg_entr 0.03632383048534393
ep2_t0.7_test_time 1.2125577926635742
Test Epoch2 threshold 0.8 Acc 0.93, AUC 0.9742550849914551, avg_entr 0.04265192523598671
ep2_t0.8_test_time 1.1953907012939453
Test Epoch2 threshold 0.9 Acc 0.9291, AUC 0.9747022390365601, avg_entr 0.05093992501497269
ep2_t0.9_test_time 1.1814427375793457
gc 0
Train Epoch3 Acc 0.964575 (38583/40000), AUC 0.9888765215873718
ep3_train_time 92.06368732452393
Test Epoch3 threshold 0.1 Acc 0.9318, AUC 0.9662103056907654, avg_entr 0.011928259395062923
ep3_t0.1_test_time 1.4574611186981201
Test Epoch3 threshold 0.2 Acc 0.9316, AUC 0.9681193232536316, avg_entr 0.015126301907002926
ep3_t0.2_test_time 1.3943798542022705
Test Epoch3 threshold 0.3 Acc 0.9315, AUC 0.9696395397186279, avg_entr 0.018889252096414566
ep3_t0.3_test_time 1.3450076580047607
Test Epoch3 threshold 0.4 Acc 0.9311, AUC 0.970899224281311, avg_entr 0.02276988886296749
ep3_t0.4_test_time 1.321887731552124
Test Epoch3 threshold 0.5 Acc 0.9306, AUC 0.9716397523880005, avg_entr 0.02765071764588356
ep3_t0.5_test_time 1.2820847034454346
Test Epoch3 threshold 0.6 Acc 0.9299, AUC 0.972525954246521, avg_entr 0.03193511813879013
ep3_t0.6_test_time 1.257936716079712
Test Epoch3 threshold 0.7 Acc 0.9299, AUC 0.9733568429946899, avg_entr 0.03755635395646095
ep3_t0.7_test_time 1.246492624282837
Test Epoch3 threshold 0.8 Acc 0.9298, AUC 0.9741126894950867, avg_entr 0.04360123351216316
ep3_t0.8_test_time 1.2070021629333496
Test Epoch3 threshold 0.9 Acc 0.9289, AUC 0.9750328063964844, avg_entr 0.05166192725300789
ep3_t0.9_test_time 1.1898491382598877
gc 0
Train Epoch4 Acc 0.965175 (38607/40000), AUC 0.9894107580184937
ep4_train_time 92.02058815956116
Test Epoch4 threshold 0.1 Acc 0.9322, AUC 0.9661078453063965, avg_entr 0.011245670728385448
ep4_t0.1_test_time 1.4453492164611816
Test Epoch4 threshold 0.2 Acc 0.9314, AUC 0.9682729244232178, avg_entr 0.014378359541296959
ep4_t0.2_test_time 1.379941463470459
Test Epoch4 threshold 0.3 Acc 0.931, AUC 0.9695025682449341, avg_entr 0.018223684281110764
ep4_t0.3_test_time 1.3310511112213135
Test Epoch4 threshold 0.4 Acc 0.9314, AUC 0.9705362319946289, avg_entr 0.02230197750031948
ep4_t0.4_test_time 1.3232057094573975
Test Epoch4 threshold 0.5 Acc 0.9313, AUC 0.9717899560928345, avg_entr 0.026362750679254532
ep4_t0.5_test_time 1.3100347518920898
Test Epoch4 threshold 0.6 Acc 0.9302, AUC 0.9722598791122437, avg_entr 0.031744372099637985
ep4_t0.6_test_time 1.2568750381469727
Test Epoch4 threshold 0.7 Acc 0.93, AUC 0.9733381271362305, avg_entr 0.03664461895823479
ep4_t0.7_test_time 1.228806495666504
Test Epoch4 threshold 0.8 Acc 0.9296, AUC 0.9739589691162109, avg_entr 0.04285988211631775
ep4_t0.8_test_time 1.2005298137664795
Test Epoch4 threshold 0.9 Acc 0.9286, AUC 0.9748061895370483, avg_entr 0.05171111598610878
ep4_t0.9_test_time 1.208808422088623
gc 0
Train Epoch5 Acc 0.965175 (38607/40000), AUC 0.9893107414245605
ep5_train_time 91.99221992492676
Test Epoch5 threshold 0.1 Acc 0.9318, AUC 0.9659912586212158, avg_entr 0.011309891939163208
ep5_t0.1_test_time 1.443852424621582
Test Epoch5 threshold 0.2 Acc 0.9312, AUC 0.9683324098587036, avg_entr 0.014568693935871124
ep5_t0.2_test_time 1.3870141506195068
Test Epoch5 threshold 0.3 Acc 0.9308, AUC 0.9696438312530518, avg_entr 0.01828768104314804
ep5_t0.3_test_time 1.3348236083984375
Test Epoch5 threshold 0.4 Acc 0.9312, AUC 0.9702765941619873, avg_entr 0.02236335538327694
ep5_t0.4_test_time 1.3123328685760498
Test Epoch5 threshold 0.5 Acc 0.9306, AUC 0.9716660976409912, avg_entr 0.026621730998158455
ep5_t0.5_test_time 1.2813172340393066
Test Epoch5 threshold 0.6 Acc 0.93, AUC 0.9722938537597656, avg_entr 0.03164999559521675
ep5_t0.6_test_time 1.2477986812591553
Test Epoch5 threshold 0.7 Acc 0.9298, AUC 0.9734739065170288, avg_entr 0.03773299977183342
ep5_t0.7_test_time 1.2318124771118164
Test Epoch5 threshold 0.8 Acc 0.9297, AUC 0.9739930629730225, avg_entr 0.04267193377017975
ep5_t0.8_test_time 1.204164743423462
Test Epoch5 threshold 0.9 Acc 0.9282, AUC 0.9749722480773926, avg_entr 0.05176836624741554
ep5_t0.9_test_time 1.1859233379364014
gc 0
Train Epoch6 Acc 0.965325 (38613/40000), AUC 0.989222526550293
ep6_train_time 91.99031066894531
Test Epoch6 threshold 0.1 Acc 0.932, AUC 0.9659551382064819, avg_entr 0.011271263472735882
ep6_t0.1_test_time 1.491156816482544
Test Epoch6 threshold 0.2 Acc 0.9314, AUC 0.9682646989822388, avg_entr 0.014559597708284855
ep6_t0.2_test_time 1.3837225437164307
Test Epoch6 threshold 0.3 Acc 0.931, AUC 0.9696063995361328, avg_entr 0.01836724951863289
ep6_t0.3_test_time 1.3502800464630127
Test Epoch6 threshold 0.4 Acc 0.9313, AUC 0.9703699350357056, avg_entr 0.0223133135586977
ep6_t0.4_test_time 1.3051238059997559
Test Epoch6 threshold 0.5 Acc 0.9309, AUC 0.9716171026229858, avg_entr 0.026675710454583168
ep6_t0.5_test_time 1.2758896350860596
Test Epoch6 threshold 0.6 Acc 0.9302, AUC 0.9722643494606018, avg_entr 0.0315658375620842
ep6_t0.6_test_time 1.2639782428741455
Test Epoch6 threshold 0.7 Acc 0.9301, AUC 0.9733994007110596, avg_entr 0.037680983543395996
ep6_t0.7_test_time 1.2238788604736328
Test Epoch6 threshold 0.8 Acc 0.9299, AUC 0.9739698171615601, avg_entr 0.04264173284173012
ep6_t0.8_test_time 1.2194547653198242
Test Epoch6 threshold 0.9 Acc 0.9283, AUC 0.9749689102172852, avg_entr 0.051787443459033966
ep6_t0.9_test_time 1.1873643398284912
gc 0
Train Epoch7 Acc 0.965575 (38623/40000), AUC 0.9892971515655518
ep7_train_time 91.99978303909302
Test Epoch7 threshold 0.1 Acc 0.932, AUC 0.9659522771835327, avg_entr 0.011272301897406578
ep7_t0.1_test_time 1.464651107788086
Test Epoch7 threshold 0.2 Acc 0.9314, AUC 0.9682639837265015, avg_entr 0.014551612548530102
ep7_t0.2_test_time 1.384437084197998
Test Epoch7 threshold 0.3 Acc 0.931, AUC 0.969603419303894, avg_entr 0.01836458407342434
ep7_t0.3_test_time 1.3472845554351807
Test Epoch7 threshold 0.4 Acc 0.9312, AUC 0.9703845977783203, avg_entr 0.02225060947239399
ep7_t0.4_test_time 1.3041889667510986
Test Epoch7 threshold 0.5 Acc 0.9309, AUC 0.9716144800186157, avg_entr 0.026673687621951103
ep7_t0.5_test_time 1.2753362655639648
Test Epoch7 threshold 0.6 Acc 0.9302, AUC 0.9722646474838257, avg_entr 0.03153865411877632
ep7_t0.6_test_time 1.267636775970459
Test Epoch7 threshold 0.7 Acc 0.9301, AUC 0.9733973741531372, avg_entr 0.03767884522676468
ep7_t0.7_test_time 1.230860948562622
Test Epoch7 threshold 0.8 Acc 0.9299, AUC 0.9739678502082825, avg_entr 0.04263962060213089
ep7_t0.8_test_time 1.2143518924713135
Test Epoch7 threshold 0.9 Acc 0.9283, AUC 0.9749675393104553, avg_entr 0.051785290241241455
ep7_t0.9_test_time 1.1932106018066406
gc 0
Train Epoch8 Acc 0.9651 (38604/40000), AUC 0.9892947673797607
ep8_train_time 91.97903680801392
Test Epoch8 threshold 0.1 Acc 0.932, AUC 0.9659615159034729, avg_entr 0.01126999594271183
ep8_t0.1_test_time 1.4447722434997559
Test Epoch8 threshold 0.2 Acc 0.9314, AUC 0.9682759046554565, avg_entr 0.014563195407390594
ep8_t0.2_test_time 1.3991105556488037
Test Epoch8 threshold 0.3 Acc 0.931, AUC 0.9696015119552612, avg_entr 0.018359148874878883
ep8_t0.3_test_time 1.361727237701416
Test Epoch8 threshold 0.4 Acc 0.9312, AUC 0.9703830480575562, avg_entr 0.022250035777688026
ep8_t0.4_test_time 1.311469554901123
Test Epoch8 threshold 0.5 Acc 0.9309, AUC 0.9716123342514038, avg_entr 0.026669323444366455
ep8_t0.5_test_time 1.2794499397277832
Test Epoch8 threshold 0.6 Acc 0.9302, AUC 0.9722684025764465, avg_entr 0.03148454427719116
ep8_t0.6_test_time 1.259171724319458
Test Epoch8 threshold 0.7 Acc 0.9301, AUC 0.9733920097351074, avg_entr 0.03772369399666786
ep8_t0.7_test_time 1.235414743423462
Test Epoch8 threshold 0.8 Acc 0.9298, AUC 0.9739642143249512, avg_entr 0.04266704246401787
ep8_t0.8_test_time 1.2159044742584229
Test Epoch8 threshold 0.9 Acc 0.9283, AUC 0.9749670028686523, avg_entr 0.05178190395236015
ep8_t0.9_test_time 1.201578140258789
gc 0
Train Epoch9 Acc 0.965 (38600/40000), AUC 0.9892676472663879
ep9_train_time 91.95255303382874
Test Epoch9 threshold 0.1 Acc 0.932, AUC 0.965961217880249, avg_entr 0.011271028779447079
ep9_t0.1_test_time 1.4462385177612305
Test Epoch9 threshold 0.2 Acc 0.9314, AUC 0.9682756662368774, avg_entr 0.014564280398190022
ep9_t0.2_test_time 1.3832154273986816
Test Epoch9 threshold 0.3 Acc 0.931, AUC 0.9696015119552612, avg_entr 0.0183589905500412
ep9_t0.3_test_time 1.3491027355194092
Test Epoch9 threshold 0.4 Acc 0.9312, AUC 0.9703829288482666, avg_entr 0.022249512374401093
ep9_t0.4_test_time 1.3034121990203857
Test Epoch9 threshold 0.5 Acc 0.9309, AUC 0.9716120958328247, avg_entr 0.026669509708881378
ep9_t0.5_test_time 1.2714948654174805
Test Epoch9 threshold 0.6 Acc 0.9302, AUC 0.9722679257392883, avg_entr 0.031487591564655304
ep9_t0.6_test_time 1.2627956867218018
Test Epoch9 threshold 0.7 Acc 0.9301, AUC 0.9733917713165283, avg_entr 0.037723906338214874
ep9_t0.7_test_time 1.3252313137054443
Test Epoch9 threshold 0.8 Acc 0.9298, AUC 0.9739639759063721, avg_entr 0.04266751557588577
ep9_t0.8_test_time 1.2057271003723145
Test Epoch9 threshold 0.9 Acc 0.9283, AUC 0.9749665856361389, avg_entr 0.05178190767765045
ep9_t0.9_test_time 1.2038321495056152
Best AUC 0.9770634174346924
train_loss (2, 5, 10)
valid_acc (10, 9)
valid_AUC (10, 9)
train_acc (10,)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Start Testing
Load ckpt at ckpt/imdb_transformeral_l5_pad500_m5//imdb_transformeral_l5_prefix.pt
[[4685  334]
 [ 385 4596]]
Figure(640x480)
tensor([6.8494e-01, 3.2863e-01, 4.6886e-11,  ..., 2.3589e-06, 2.2314e-02,
        2.3299e-08])
[[4717  302]
 [ 295 4686]]
Figure(640x480)
tensor([2.7568e-03, 6.1228e-01, 7.2002e-09,  ..., 4.2271e-08, 9.9455e-10,
        6.8429e-08])
[[4713  306]
 [ 291 4690]]
Figure(640x480)
tensor([7.8091e-04, 7.3367e-01, 6.8989e-09,  ..., 7.1383e-08, 3.5995e-09,
        5.4937e-08])
[[4703  316]
 [ 286 4695]]
Figure(640x480)
tensor([4.7436e-04, 5.8386e-01, 1.5813e-08,  ..., 5.2180e-08, 3.3180e-08,
        1.7214e-08])
[[4709  310]
 [ 289 4692]]
Figure(640x480)
tensor([3.4848e-04, 5.1768e-01, 2.2005e-08,  ..., 4.0258e-08, 6.1800e-08,
        3.0583e-08])
