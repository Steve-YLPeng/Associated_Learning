total count words 887881
vocab size 30000
found 28354 words in glove
Load ckpt from ckpt/dbpedia_14_transformeral_l5_pad60_m1//dbpedia_14_transformeral_l5_prefix.pt
model: TransformerModelML(
  (layers): ModuleList(
    (0): EMBLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Embedding(30000, 300)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=14, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=14, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.weight 9000000
layers.0.ae.g.0.weight 1792
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 1792
layers.0.ae.h.0.bias 14
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
Total Trainable Params: 13674862
Start Training
train_mask {0, 1}
gc 9
Train Epoch0 Acc 0.11029285714285714 (61764/560000), AUC 0.5122609734535217
ep0_train_time 102.98664236068726
Test Epoch0 threshold 0.1 Acc 0.9756, AUC 0.9982007145881653, avg_entr 0.004779123235493898
ep0_t0.1_test_time 1.7869982719421387
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m2//dbpedia_14_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.2 Acc 0.9749, AUC 0.9982914924621582, avg_entr 0.009168055839836597
ep0_t0.2_test_time 1.5429303646087646
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m2//dbpedia_14_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.3 Acc 0.9746, AUC 0.9982932209968567, avg_entr 0.009466527961194515
ep0_t0.3_test_time 1.4940571784973145
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m2//dbpedia_14_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.4 Acc 0.9746142857142858, AUC 0.9982932209968567, avg_entr 0.00947005394846201
ep0_t0.4_test_time 1.483222246170044
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m2//dbpedia_14_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.5 Acc 0.9746142857142858, AUC 0.9982932209968567, avg_entr 0.00947005394846201
ep0_t0.5_test_time 1.5003385543823242
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m2//dbpedia_14_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.6 Acc 0.9746142857142858, AUC 0.9982932209968567, avg_entr 0.00947005394846201
ep0_t0.6_test_time 1.4783086776733398
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m2//dbpedia_14_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.7 Acc 0.9746142857142858, AUC 0.9982932209968567, avg_entr 0.00947005394846201
ep0_t0.7_test_time 1.483717441558838
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m2//dbpedia_14_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.8 Acc 0.9746142857142858, AUC 0.9982932209968567, avg_entr 0.00947005394846201
ep0_t0.8_test_time 1.4792397022247314
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m2//dbpedia_14_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.9 Acc 0.9746142857142858, AUC 0.9982932209968567, avg_entr 0.00947005394846201
ep0_t0.9_test_time 1.4877312183380127
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m2//dbpedia_14_transformeral_l5_prefix.pt  ,ep 0
gc 0
Train Epoch1 Acc 0.13773392857142858 (77131/560000), AUC 0.4838132858276367
ep1_train_time 101.45243692398071
Test Epoch1 threshold 0.1 Acc 0.9760714285714286, AUC 0.9982327222824097, avg_entr 0.004651625175029039
ep1_t0.1_test_time 1.8031766414642334
Test Epoch1 threshold 0.2 Acc 0.9750142857142857, AUC 0.9982949495315552, avg_entr 0.009309756569564342
ep1_t0.2_test_time 1.5428698062896729
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m2//dbpedia_14_transformeral_l5_prefix.pt  ,ep 1
Test Epoch1 threshold 0.3 Acc 0.9747, AUC 0.9982981085777283, avg_entr 0.00958577636629343
ep1_t0.3_test_time 1.4873170852661133
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m2//dbpedia_14_transformeral_l5_prefix.pt  ,ep 1
Test Epoch1 threshold 0.4 Acc 0.9747, AUC 0.9982981085777283, avg_entr 0.00958577636629343
ep1_t0.4_test_time 1.4990546703338623
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m2//dbpedia_14_transformeral_l5_prefix.pt  ,ep 1
Test Epoch1 threshold 0.5 Acc 0.9747, AUC 0.9982981085777283, avg_entr 0.00958577636629343
ep1_t0.5_test_time 1.4792792797088623
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m2//dbpedia_14_transformeral_l5_prefix.pt  ,ep 1
Test Epoch1 threshold 0.6 Acc 0.9747, AUC 0.9982981085777283, avg_entr 0.00958577636629343
ep1_t0.6_test_time 1.4820852279663086
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m2//dbpedia_14_transformeral_l5_prefix.pt  ,ep 1
Test Epoch1 threshold 0.7 Acc 0.9747, AUC 0.9982981085777283, avg_entr 0.00958577636629343
ep1_t0.7_test_time 1.474350929260254
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m2//dbpedia_14_transformeral_l5_prefix.pt  ,ep 1
Test Epoch1 threshold 0.8 Acc 0.9747, AUC 0.9982981085777283, avg_entr 0.00958577636629343
ep1_t0.8_test_time 1.4850921630859375
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m2//dbpedia_14_transformeral_l5_prefix.pt  ,ep 1
Test Epoch1 threshold 0.9 Acc 0.9747, AUC 0.9982981085777283, avg_entr 0.00958577636629343
ep1_t0.9_test_time 1.4849638938903809
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m2//dbpedia_14_transformeral_l5_prefix.pt  ,ep 1
gc 0
Train Epoch2 Acc 0.13938392857142856 (78055/560000), AUC 0.47828397154808044
ep2_train_time 101.67151427268982
Test Epoch2 threshold 0.1 Acc 0.9761285714285715, AUC 0.998212456703186, avg_entr 0.004611423704773188
ep2_t0.1_test_time 1.812119960784912
Test Epoch2 threshold 0.2 Acc 0.9750857142857143, AUC 0.9983047246932983, avg_entr 0.009234992787241936
ep2_t0.2_test_time 1.5693931579589844
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m2//dbpedia_14_transformeral_l5_prefix.pt  ,ep 2
Test Epoch2 threshold 0.3 Acc 0.9746571428571429, AUC 0.9983010292053223, avg_entr 0.009532183408737183
ep2_t0.3_test_time 1.4947614669799805
Test Epoch2 threshold 0.4 Acc 0.9746571428571429, AUC 0.9983010292053223, avg_entr 0.009532183408737183
ep2_t0.4_test_time 1.5054504871368408
Test Epoch2 threshold 0.5 Acc 0.9746571428571429, AUC 0.9983010292053223, avg_entr 0.009532183408737183
ep2_t0.5_test_time 1.4882097244262695
Test Epoch2 threshold 0.6 Acc 0.9746571428571429, AUC 0.9983010292053223, avg_entr 0.009532183408737183
ep2_t0.6_test_time 1.4974651336669922
Test Epoch2 threshold 0.7 Acc 0.9746571428571429, AUC 0.9983010292053223, avg_entr 0.009532183408737183
ep2_t0.7_test_time 1.4877092838287354
Test Epoch2 threshold 0.8 Acc 0.9746571428571429, AUC 0.9983010292053223, avg_entr 0.009532183408737183
ep2_t0.8_test_time 1.5026957988739014
Test Epoch2 threshold 0.9 Acc 0.9746571428571429, AUC 0.9983010292053223, avg_entr 0.009532183408737183
ep2_t0.9_test_time 1.5226783752441406
gc 0
Train Epoch3 Acc 0.13982857142857144 (78304/560000), AUC 0.4768189489841461
ep3_train_time 101.49937009811401
Test Epoch3 threshold 0.1 Acc 0.9762571428571428, AUC 0.9982253313064575, avg_entr 0.004543131683021784
ep3_t0.1_test_time 1.808241367340088
Test Epoch3 threshold 0.2 Acc 0.9750714285714286, AUC 0.9983076453208923, avg_entr 0.009208446368575096
ep3_t0.2_test_time 1.5570063591003418
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m2//dbpedia_14_transformeral_l5_prefix.pt  ,ep 3
Test Epoch3 threshold 0.3 Acc 0.9747285714285714, AUC 0.998304009437561, avg_entr 0.009509115479886532
ep3_t0.3_test_time 1.4944746494293213
Test Epoch3 threshold 0.4 Acc 0.9747285714285714, AUC 0.998304009437561, avg_entr 0.009509115479886532
ep3_t0.4_test_time 1.491344928741455
Test Epoch3 threshold 0.5 Acc 0.9747285714285714, AUC 0.998304009437561, avg_entr 0.009509115479886532
ep3_t0.5_test_time 1.4914541244506836
Test Epoch3 threshold 0.6 Acc 0.9747285714285714, AUC 0.998304009437561, avg_entr 0.009509115479886532
ep3_t0.6_test_time 1.5034360885620117
Test Epoch3 threshold 0.7 Acc 0.9747285714285714, AUC 0.998304009437561, avg_entr 0.009509115479886532
ep3_t0.7_test_time 1.5011091232299805
Test Epoch3 threshold 0.8 Acc 0.9747285714285714, AUC 0.998304009437561, avg_entr 0.009509115479886532
ep3_t0.8_test_time 1.498194694519043
Test Epoch3 threshold 0.9 Acc 0.9747285714285714, AUC 0.998304009437561, avg_entr 0.009509115479886532
ep3_t0.9_test_time 1.5027143955230713
gc 0
Train Epoch4 Acc 0.14003392857142857 (78419/560000), AUC 0.47653883695602417
ep4_train_time 101.66873216629028
Test Epoch4 threshold 0.1 Acc 0.9762142857142857, AUC 0.9982251524925232, avg_entr 0.004532677587121725
ep4_t0.1_test_time 1.8124289512634277
Test Epoch4 threshold 0.2 Acc 0.9751428571428571, AUC 0.9983076453208923, avg_entr 0.009191197343170643
ep4_t0.2_test_time 1.5644617080688477
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m2//dbpedia_14_transformeral_l5_prefix.pt  ,ep 4
Test Epoch4 threshold 0.3 Acc 0.9747714285714286, AUC 0.9983038306236267, avg_entr 0.009496728889644146
ep4_t0.3_test_time 1.4905741214752197
Test Epoch4 threshold 0.4 Acc 0.9747714285714286, AUC 0.9983038306236267, avg_entr 0.009496728889644146
ep4_t0.4_test_time 1.4806315898895264
Test Epoch4 threshold 0.5 Acc 0.9747714285714286, AUC 0.9983038306236267, avg_entr 0.009496728889644146
ep4_t0.5_test_time 1.5125083923339844
Test Epoch4 threshold 0.6 Acc 0.9747714285714286, AUC 0.9983038306236267, avg_entr 0.009496728889644146
ep4_t0.6_test_time 1.490891456604004
Test Epoch4 threshold 0.7 Acc 0.9747714285714286, AUC 0.9983038306236267, avg_entr 0.009496728889644146
ep4_t0.7_test_time 1.4846572875976562
Test Epoch4 threshold 0.8 Acc 0.9747714285714286, AUC 0.9983038306236267, avg_entr 0.009496728889644146
ep4_t0.8_test_time 1.4828448295593262
Test Epoch4 threshold 0.9 Acc 0.9747714285714286, AUC 0.9983038306236267, avg_entr 0.009496728889644146
ep4_t0.9_test_time 1.491194248199463
gc 0
Train Epoch5 Acc 0.14000535714285714 (78403/560000), AUC 0.47653648257255554
ep5_train_time 101.5282154083252
Test Epoch5 threshold 0.1 Acc 0.9763, AUC 0.9982214570045471, avg_entr 0.004535115323960781
ep5_t0.1_test_time 1.7983553409576416
Test Epoch5 threshold 0.2 Acc 0.9751714285714286, AUC 0.9983072876930237, avg_entr 0.00919315218925476
ep5_t0.2_test_time 1.5482158660888672
Test Epoch5 threshold 0.3 Acc 0.9748, AUC 0.9983035326004028, avg_entr 0.009495319798588753
ep5_t0.3_test_time 1.4841468334197998
Test Epoch5 threshold 0.4 Acc 0.9748, AUC 0.9983035326004028, avg_entr 0.009495319798588753
ep5_t0.4_test_time 1.499699592590332
Test Epoch5 threshold 0.5 Acc 0.9748, AUC 0.9983035326004028, avg_entr 0.009495319798588753
ep5_t0.5_test_time 1.5067269802093506
Test Epoch5 threshold 0.6 Acc 0.9748, AUC 0.9983035326004028, avg_entr 0.009495319798588753
ep5_t0.6_test_time 1.5016517639160156
Test Epoch5 threshold 0.7 Acc 0.9748, AUC 0.9983035326004028, avg_entr 0.009495319798588753
ep5_t0.7_test_time 1.5038723945617676
Test Epoch5 threshold 0.8 Acc 0.9748, AUC 0.9983035326004028, avg_entr 0.009495319798588753
ep5_t0.8_test_time 1.4817023277282715
Test Epoch5 threshold 0.9 Acc 0.9748, AUC 0.9983035326004028, avg_entr 0.009495319798588753
ep5_t0.9_test_time 1.4994916915893555
gc 0
Train Epoch6 Acc 0.14009464285714285 (78453/560000), AUC 0.4765125811100006
ep6_train_time 101.54112911224365
Test Epoch6 threshold 0.1 Acc 0.9763142857142857, AUC 0.9982224702835083, avg_entr 0.004539485555142164
ep6_t0.1_test_time 1.805422067642212
Test Epoch6 threshold 0.2 Acc 0.9751714285714286, AUC 0.9983072280883789, avg_entr 0.009194374084472656
ep6_t0.2_test_time 1.559143304824829
Test Epoch6 threshold 0.3 Acc 0.9748, AUC 0.9983034729957581, avg_entr 0.009496522136032581
ep6_t0.3_test_time 1.5096468925476074
Test Epoch6 threshold 0.4 Acc 0.9748, AUC 0.9983034729957581, avg_entr 0.009496522136032581
ep6_t0.4_test_time 1.516465187072754
Test Epoch6 threshold 0.5 Acc 0.9748, AUC 0.9983034729957581, avg_entr 0.009496522136032581
ep6_t0.5_test_time 1.511772632598877
Test Epoch6 threshold 0.6 Acc 0.9748, AUC 0.9983034729957581, avg_entr 0.009496522136032581
ep6_t0.6_test_time 1.504847764968872
Test Epoch6 threshold 0.7 Acc 0.9748, AUC 0.9983034729957581, avg_entr 0.009496522136032581
ep6_t0.7_test_time 1.4971153736114502
Test Epoch6 threshold 0.8 Acc 0.9748, AUC 0.9983034729957581, avg_entr 0.009496522136032581
ep6_t0.8_test_time 1.484133005142212
Test Epoch6 threshold 0.9 Acc 0.9748, AUC 0.9983034729957581, avg_entr 0.009496522136032581
ep6_t0.9_test_time 1.522221565246582
gc 0
Train Epoch7 Acc 0.1399607142857143 (78378/560000), AUC 0.47648319602012634
ep7_train_time 101.49393463134766
Test Epoch7 threshold 0.1 Acc 0.9763142857142857, AUC 0.9982224106788635, avg_entr 0.004539436660706997
ep7_t0.1_test_time 1.819265365600586
Test Epoch7 threshold 0.2 Acc 0.9751714285714286, AUC 0.9983072280883789, avg_entr 0.009194341488182545
ep7_t0.2_test_time 1.5623564720153809
Test Epoch7 threshold 0.3 Acc 0.9748, AUC 0.9983033537864685, avg_entr 0.009496472775936127
ep7_t0.3_test_time 1.489664077758789
Test Epoch7 threshold 0.4 Acc 0.9748, AUC 0.9983033537864685, avg_entr 0.009496472775936127
ep7_t0.4_test_time 1.486952543258667
Test Epoch7 threshold 0.5 Acc 0.9748, AUC 0.9983033537864685, avg_entr 0.009496472775936127
ep7_t0.5_test_time 1.502824306488037
Test Epoch7 threshold 0.6 Acc 0.9748, AUC 0.9983033537864685, avg_entr 0.009496472775936127
ep7_t0.6_test_time 1.4931144714355469
Test Epoch7 threshold 0.7 Acc 0.9748, AUC 0.9983033537864685, avg_entr 0.009496472775936127
ep7_t0.7_test_time 1.4985361099243164
Test Epoch7 threshold 0.8 Acc 0.9748, AUC 0.9983033537864685, avg_entr 0.009496472775936127
ep7_t0.8_test_time 1.4875848293304443
Test Epoch7 threshold 0.9 Acc 0.9748, AUC 0.9983033537864685, avg_entr 0.009496472775936127
ep7_t0.9_test_time 1.489898681640625
gc 0
Train Epoch8 Acc 0.14001785714285714 (78410/560000), AUC 0.47644081711769104
ep8_train_time 101.60446310043335
Test Epoch8 threshold 0.1 Acc 0.9763142857142857, AUC 0.9982224702835083, avg_entr 0.004539411049336195
ep8_t0.1_test_time 1.801924228668213
Test Epoch8 threshold 0.2 Acc 0.9751714285714286, AUC 0.9983072280883789, avg_entr 0.009194343350827694
ep8_t0.2_test_time 1.54335355758667
Test Epoch8 threshold 0.3 Acc 0.9748, AUC 0.9983034729957581, avg_entr 0.009496469981968403
ep8_t0.3_test_time 1.5044758319854736
Test Epoch8 threshold 0.4 Acc 0.9748, AUC 0.9983034729957581, avg_entr 0.009496469981968403
ep8_t0.4_test_time 1.4825615882873535
Test Epoch8 threshold 0.5 Acc 0.9748, AUC 0.9983034729957581, avg_entr 0.009496469981968403
ep8_t0.5_test_time 1.4918279647827148
Test Epoch8 threshold 0.6 Acc 0.9748, AUC 0.9983034729957581, avg_entr 0.009496469981968403
ep8_t0.6_test_time 1.49326491355896
Test Epoch8 threshold 0.7 Acc 0.9748, AUC 0.9983034729957581, avg_entr 0.009496469981968403
ep8_t0.7_test_time 1.4901628494262695
Test Epoch8 threshold 0.8 Acc 0.9748, AUC 0.9983034729957581, avg_entr 0.009496469981968403
ep8_t0.8_test_time 1.5043401718139648
Test Epoch8 threshold 0.9 Acc 0.9748, AUC 0.9983034729957581, avg_entr 0.009496469981968403
ep8_t0.9_test_time 1.4890005588531494
gc 0
Train Epoch9 Acc 0.14004464285714285 (78425/560000), AUC 0.47635847330093384
ep9_train_time 101.63723158836365
Test Epoch9 threshold 0.1 Acc 0.9763142857142857, AUC 0.9982224702835083, avg_entr 0.004540761932730675
ep9_t0.1_test_time 1.797541856765747
Test Epoch9 threshold 0.2 Acc 0.9751714285714286, AUC 0.9983072280883789, avg_entr 0.00919433031231165
ep9_t0.2_test_time 1.5597155094146729
Test Epoch9 threshold 0.3 Acc 0.9748, AUC 0.9983034729957581, avg_entr 0.009496469981968403
ep9_t0.3_test_time 1.4837498664855957
Test Epoch9 threshold 0.4 Acc 0.9748, AUC 0.9983034729957581, avg_entr 0.009496469981968403
ep9_t0.4_test_time 1.4862110614776611
Test Epoch9 threshold 0.5 Acc 0.9748, AUC 0.9983034729957581, avg_entr 0.009496469981968403
ep9_t0.5_test_time 1.485689401626587
Test Epoch9 threshold 0.6 Acc 0.9748, AUC 0.9983034729957581, avg_entr 0.009496469981968403
ep9_t0.6_test_time 1.511732578277588
Test Epoch9 threshold 0.7 Acc 0.9748, AUC 0.9983034729957581, avg_entr 0.009496469981968403
ep9_t0.7_test_time 1.5073540210723877
Test Epoch9 threshold 0.8 Acc 0.9748, AUC 0.9983034729957581, avg_entr 0.009496469981968403
ep9_t0.8_test_time 1.4960885047912598
Test Epoch9 threshold 0.9 Acc 0.9748, AUC 0.9983034729957581, avg_entr 0.009496469981968403
ep9_t0.9_test_time 1.4923036098480225
Best AUC 0.9983076453208923
train_loss (2, 5, 10)
valid_acc (10, 9)
valid_AUC (10, 9)
train_acc (10,)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Start Testing
Load ckpt at ckpt/dbpedia_14_transformeral_l5_pad60_m2//dbpedia_14_transformeral_l5_prefix.pt
[[4704   39   23   15   11   66   42    7    4    6    4   15   17   47]
 [  33 4896    4    1    8    1   32    8    3    1    1    0    4    8]
 [  35   14 4634   18   54    2   10    3    3    1    1   75   21  129]
 [   3    2   26 4953   11    0    1    0    0    1    0    0    0    3]
 [   9   19   67   13 4862    7    5    0    2    1    0    0    3   12]
 [  33    1    2    2    1 4944    5    5    1    3    1    0    1    1]
 [  59   48    7    2    9   13 4801   34   14    5    1    1    4    2]
 [   0    1    0    0    2    0   12 4964   16    3    1    0    0    1]
 [   1    3    2    0    3    0    8   13 4970    0    0    0    0    0]
 [   1    0    1    2    0    0    0    7    0 4954   34    0    0    1]
 [  12    1    0    0    0    0    2    4    0   33 4947    0    0    1]
 [   6    0   37    2    0    0    0    0    0    0    1 4927   12   15]
 [   9    1   26    5    1    2    1    3    1    2    0   22 4884   43]
 [  30    6   85    7   12    3    3    8    1    3    2    6   40 4794]]
Figure(640x480)
tensor([6.7984e-07, 1.3542e-01, 2.7532e-04,  ..., 4.0521e-07, 1.4863e-03,
        4.9099e-06])
[[4735   37   19    9   12   51   46    3    0    5    3   21   10   49]
 [  36 4914    6    1    7    0   27    0    0    1    0    0    2    6]
 [  28   13 4713   11   67    1   11    1    0    1    1   49   20   84]
 [   5    1   20 4957   12    0    0    1    0    1    0    1    1    1]
 [   7   10   66   13 4877    5    8    0    1    3    0    0    3    7]
 [  40    1    1    0    1 4947    5    1    1    1    0    0    1    1]
 [  56   33    5    2    9   13 4842   23    5    5    0    1    3    3]
 [   3    1    0    0    1    0   14 4961   13    3    2    0    1    1]
 [   2    2    2    0    5    0   13   11 4964    0    0    0    0    1]
 [   1    1    1    1    0    0    0    5    0 4957   34    0    0    0]
 [  10    1    0    0    0    2    1    3    0   33 4950    0    0    0]
 [   7    1   33    2    0    0    0    0    0    0    0 4935   11   11]
 [  10    1   15    0    1    2    0    1    0    0    0   17 4913   40]
 [  26    5   70    3    5    3    2    1    0    1    1    8   41 4834]]
Figure(640x480)
tensor([3.1231e-07, 3.9551e-01, 3.7083e-07,  ..., 3.6658e-07, 5.6591e-07,
        4.3673e-07])
[[   0    0  367    0    6    0 4453    0  138    0   29    0    7    0]
 [   0    0   71    0    0    0   77    0   73    0 4777    0    2    0]
 [   0    0  208    1    9    0    8    0 4733    0   38    0    3    0]
 [   0    0 4933    0    0    0    5    0   50    0   10    0    2    0]
 [   0    1   22    0    3    0   38    0  192    0 4108    0  636    0]
 [   0    0 4935    0    0    0   13    0   36    0   16    0    0    0]
 [   0    0 3735    0    0    0 1129    0   94    0   16    0   21    5]
 [   0    0    6    0    1    0   58    0 4935    0    0    0    0    0]
 [   0    0  385    0    2    0 4179    0   14    0  420    0    0    0]
 [   0    3  339    0   12    0    2    0   33    0 4357    0  254    0]
 [   0    0    4    0 4194    0   10    0    9    0   24    0  759    0]
 [   0    0 4969    0    0    0    3    0   26    0    2    0    0    0]
 [   0    4   34    0    3    0    1    0 4406  538   14    0    0    0]
 [   0    0 4567    0    0    0    6    0  239    0  184    0    4    0]]
Figure(640x480)
tensor([1.4518, 1.6144, 1.8186,  ..., 1.5193, 1.1952, 1.5950])
[[1104    0    0 2710    0    0 1184    0    0    0    0    2    0    0]
 [   9    0    0 2601    0    0 2320    0    0    0    0   70    0    0]
 [  23    0    0  906    0    0 4070    0    0    0    0    1    0    0]
 [   3    0    0 4980    0    0   16    0    0    0    0    1    0    0]
 [4406    0    0  527    0    0   67    0    0    0    0    0    0    0]
 [  15    0    0 4869    0    0  116    0    0    0    0    0    0    0]
 [1102    0    0 3797    0    0   45    0    0   53    0    3    0    0]
 [   0    0    0 1908    0    0 3092    0    0    0    0    0    0    0]
 [   5    0    0 1312    0    0 3683    0    0    0    0    0    0    0]
 [   0    0    0  145    0    0  494    0    0    0    0 4361    0    0]
 [   5    0    0 4989    0    0    4    0    0    0    0    2    0    0]
 [3080    0    0  507    0    0 1396    0    0    0    0   17    0    0]
 [  13    0    0 4807    0    0   80  100    0    0    0    0    0    0]
 [  19    0    0 4963    0    0    9    9    0    0    0    0    0    0]]
Figure(640x480)
tensor([0.9827, 0.8375, 0.9311,  ..., 1.3718, 1.4027, 1.1217])
[[   0    0    0   92    0    0    0    1    0    0    0    0 2872 2035]
 [   0    0    0   17    0    0    0  129    0    0    0    0 2854 2000]
 [   0    0    0  159    0    0    0    0    0    0    2    0 4836    3]
 [   0  373    0  100    0    0    0    0    0    0    0    0 4524    3]
 [   0    0    0   20    0    0    0   10    0    0    0    0 4760  210]
 [   0    0    0 4366    0    0    0    0    0    0    0    0   33  601]
 [   0    0    0 4016    0    0    0    0    0    0    0    0  982    2]
 [   0   21    0 4948    0    0    0    0    0    0    1    0   29    1]
 [   0    0    0  234    0    0    0    0    0    0    0    0 4766    0]
 [   0    1    0    8    2    0    0    0    0    4   11    0 2120 2854]
 [   0    0    0   78    0    0    0    0    0    5 4782    0   14  121]
 [   0    0    0 4968    0    0    0    0    0    0    0    0   28    4]
 [   0    0    0   70    0    0    0    0    0    0    0    0 4916   14]
 [   0    0    0 4527    0    0    0    0    0    0    0    0  461   12]]
Figure(640x480)
tensor([0.3397, 0.9039, 0.5357,  ..., 1.2853, 1.0369, 1.1631])
