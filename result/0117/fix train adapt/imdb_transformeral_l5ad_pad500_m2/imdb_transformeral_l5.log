total count words 222751
vocab size 30000
found 27937 words in glove
Load ckpt from ckpt/imdb_transformeral_l5_pad500_m1//imdb_transformeral_l5_prefix.pt
model: TransformerModelML(
  (layers): ModuleList(
    (0): EMBLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Embedding(30000, 300)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=2, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=2, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.weight 9000000
layers.0.ae.g.0.weight 256
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 256
layers.0.ae.h.0.bias 2
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
Total Trainable Params: 13671778
Start Training
train_mask {0, 1}
gc 9
Train Epoch0 Acc 0.5022 (20088/40000), AUC 0.6067109704017639
ep0_train_time 57.9625506401062
Test Epoch0 threshold 0.1 Acc 0.8974, AUC 0.9588069319725037, avg_entr 0.14473479986190796
ep0_t0.1_test_time 1.6923120021820068
Save ckpt to ckpt/imdb_transformeral_l5_pad500_m2//imdb_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.2 Acc 0.8973, AUC 0.9576176404953003, avg_entr 0.14955247938632965
ep0_t0.2_test_time 1.5731112957000732
Test Epoch0 threshold 0.3 Acc 0.8971, AUC 0.9568465948104858, avg_entr 0.1566048264503479
ep0_t0.3_test_time 1.5046942234039307
Test Epoch0 threshold 0.4 Acc 0.8971, AUC 0.9566034078598022, avg_entr 0.1640775054693222
ep0_t0.4_test_time 1.4561536312103271
Test Epoch0 threshold 0.5 Acc 0.8974, AUC 0.9561449289321899, avg_entr 0.17293623089790344
ep0_t0.5_test_time 1.4181146621704102
Test Epoch0 threshold 0.6 Acc 0.897, AUC 0.9560567140579224, avg_entr 0.18041013181209564
ep0_t0.6_test_time 1.347449541091919
Test Epoch0 threshold 0.7 Acc 0.8968, AUC 0.9565509557723999, avg_entr 0.18923430144786835
ep0_t0.7_test_time 1.3173518180847168
Test Epoch0 threshold 0.8 Acc 0.8966, AUC 0.956771969795227, avg_entr 0.1995549499988556
ep0_t0.8_test_time 1.2653493881225586
Test Epoch0 threshold 0.9 Acc 0.8963, AUC 0.9566556215286255, avg_entr 0.21068766713142395
ep0_t0.9_test_time 1.2279202938079834
gc 0
Train Epoch1 Acc 0.5034 (20136/40000), AUC 0.8185397386550903
ep1_train_time 57.74342727661133
Test Epoch1 threshold 0.1 Acc 0.9048, AUC 0.9625117778778076, avg_entr 0.13095815479755402
ep1_t0.1_test_time 1.6480340957641602
Save ckpt to ckpt/imdb_transformeral_l5_pad500_m2//imdb_transformeral_l5_prefix.pt  ,ep 1
Test Epoch1 threshold 0.2 Acc 0.9049, AUC 0.9613461494445801, avg_entr 0.13626596331596375
ep1_t0.2_test_time 1.5304107666015625
Test Epoch1 threshold 0.3 Acc 0.9047, AUC 0.9603997468948364, avg_entr 0.14209669828414917
ep1_t0.3_test_time 1.457226037979126
Test Epoch1 threshold 0.4 Acc 0.9045, AUC 0.9602294564247131, avg_entr 0.14815442264080048
ep1_t0.4_test_time 1.4237167835235596
Test Epoch1 threshold 0.5 Acc 0.9043, AUC 0.9594782590866089, avg_entr 0.15578623116016388
ep1_t0.5_test_time 1.362778902053833
Test Epoch1 threshold 0.6 Acc 0.9041, AUC 0.9597911238670349, avg_entr 0.1623869389295578
ep1_t0.6_test_time 1.3381049633026123
Test Epoch1 threshold 0.7 Acc 0.9032, AUC 0.959700882434845, avg_entr 0.17069198191165924
ep1_t0.7_test_time 1.3065366744995117
Test Epoch1 threshold 0.8 Acc 0.9023, AUC 0.9594754576683044, avg_entr 0.18011343479156494
ep1_t0.8_test_time 1.2680294513702393
Test Epoch1 threshold 0.9 Acc 0.9015, AUC 0.9592770338058472, avg_entr 0.18940486013889313
ep1_t0.9_test_time 1.2309937477111816
gc 0
Train Epoch2 Acc 0.504 (20160/40000), AUC 0.8430705070495605
ep2_train_time 57.67128300666809
Test Epoch2 threshold 0.1 Acc 0.9055, AUC 0.9626659154891968, avg_entr 0.12761327624320984
ep2_t0.1_test_time 1.630997896194458
Save ckpt to ckpt/imdb_transformeral_l5_pad500_m2//imdb_transformeral_l5_prefix.pt  ,ep 2
Test Epoch2 threshold 0.2 Acc 0.9053, AUC 0.9618922472000122, avg_entr 0.13190564513206482
ep2_t0.2_test_time 1.5374934673309326
Test Epoch2 threshold 0.3 Acc 0.9053, AUC 0.9610128998756409, avg_entr 0.13818784058094025
ep2_t0.3_test_time 1.4702129364013672
Test Epoch2 threshold 0.4 Acc 0.9053, AUC 0.9602518677711487, avg_entr 0.14437761902809143
ep2_t0.4_test_time 1.4226255416870117
Test Epoch2 threshold 0.5 Acc 0.9047, AUC 0.9599930047988892, avg_entr 0.1509575992822647
ep2_t0.5_test_time 1.3758482933044434
Test Epoch2 threshold 0.6 Acc 0.9045, AUC 0.9603914618492126, avg_entr 0.15942054986953735
ep2_t0.6_test_time 1.3716950416564941
Test Epoch2 threshold 0.7 Acc 0.9044, AUC 0.960227370262146, avg_entr 0.16756625473499298
ep2_t0.7_test_time 1.3068323135375977
Test Epoch2 threshold 0.8 Acc 0.9041, AUC 0.9600553512573242, avg_entr 0.17591583728790283
ep2_t0.8_test_time 1.303269386291504
Test Epoch2 threshold 0.9 Acc 0.9024, AUC 0.9599551558494568, avg_entr 0.1866612732410431
ep2_t0.9_test_time 1.224189281463623
gc 0
Train Epoch3 Acc 0.5041 (20164/40000), AUC 0.8476336598396301
ep3_train_time 57.6533317565918
Test Epoch3 threshold 0.1 Acc 0.9061, AUC 0.9629472494125366, avg_entr 0.12650322914123535
ep3_t0.1_test_time 1.6270182132720947
Save ckpt to ckpt/imdb_transformeral_l5_pad500_m2//imdb_transformeral_l5_prefix.pt  ,ep 3
Test Epoch3 threshold 0.2 Acc 0.9061, AUC 0.9618696570396423, avg_entr 0.13168801367282867
ep3_t0.2_test_time 1.5221600532531738
Test Epoch3 threshold 0.3 Acc 0.9057, AUC 0.9611177444458008, avg_entr 0.13751056790351868
ep3_t0.3_test_time 1.446939468383789
Test Epoch3 threshold 0.4 Acc 0.9054, AUC 0.9602346420288086, avg_entr 0.14374764263629913
ep3_t0.4_test_time 1.4166648387908936
Test Epoch3 threshold 0.5 Acc 0.9053, AUC 0.9601017236709595, avg_entr 0.15094245970249176
ep3_t0.5_test_time 1.3736097812652588
Test Epoch3 threshold 0.6 Acc 0.9049, AUC 0.9605244398117065, avg_entr 0.1578977108001709
ep3_t0.6_test_time 1.3440923690795898
Test Epoch3 threshold 0.7 Acc 0.9044, AUC 0.9605789184570312, avg_entr 0.16663439571857452
ep3_t0.7_test_time 1.2802634239196777
Test Epoch3 threshold 0.8 Acc 0.903, AUC 0.9601860046386719, avg_entr 0.17534153163433075
ep3_t0.8_test_time 1.2478442192077637
Test Epoch3 threshold 0.9 Acc 0.9024, AUC 0.9602257013320923, avg_entr 0.18523749709129333
ep3_t0.9_test_time 1.2149202823638916
gc 0
Train Epoch4 Acc 0.5042 (20168/40000), AUC 0.8502690196037292
ep4_train_time 57.724074363708496
Test Epoch4 threshold 0.1 Acc 0.9059, AUC 0.9630017876625061, avg_entr 0.12605544924736023
ep4_t0.1_test_time 1.628993272781372
Save ckpt to ckpt/imdb_transformeral_l5_pad500_m2//imdb_transformeral_l5_prefix.pt  ,ep 4
Test Epoch4 threshold 0.2 Acc 0.9059, AUC 0.9618786573410034, avg_entr 0.13131079077720642
ep4_t0.2_test_time 1.520921230316162
Test Epoch4 threshold 0.3 Acc 0.9055, AUC 0.9611857533454895, avg_entr 0.13704721629619598
ep4_t0.3_test_time 1.44724440574646
Test Epoch4 threshold 0.4 Acc 0.9054, AUC 0.9602738618850708, avg_entr 0.14343637228012085
ep4_t0.4_test_time 1.403639793395996
Test Epoch4 threshold 0.5 Acc 0.9051, AUC 0.9602407217025757, avg_entr 0.1504940539598465
ep4_t0.5_test_time 1.3637840747833252
Test Epoch4 threshold 0.6 Acc 0.9047, AUC 0.9604386687278748, avg_entr 0.15735012292861938
ep4_t0.6_test_time 1.3359873294830322
Test Epoch4 threshold 0.7 Acc 0.9042, AUC 0.9606292843818665, avg_entr 0.1660747081041336
ep4_t0.7_test_time 1.3017261028289795
Test Epoch4 threshold 0.8 Acc 0.9028, AUC 0.9602203369140625, avg_entr 0.17485520243644714
ep4_t0.8_test_time 1.2483572959899902
Test Epoch4 threshold 0.9 Acc 0.9022, AUC 0.9602632522583008, avg_entr 0.1847756952047348
ep4_t0.9_test_time 1.2178215980529785
gc 0
Train Epoch5 Acc 0.504525 (20181/40000), AUC 0.8484911918640137
ep5_train_time 57.6728458404541
Test Epoch5 threshold 0.1 Acc 0.9059, AUC 0.9630381464958191, avg_entr 0.1258695125579834
ep5_t0.1_test_time 1.6270158290863037
Save ckpt to ckpt/imdb_transformeral_l5_pad500_m2//imdb_transformeral_l5_prefix.pt  ,ep 5
Test Epoch5 threshold 0.2 Acc 0.9059, AUC 0.9618791341781616, avg_entr 0.13112597167491913
ep5_t0.2_test_time 1.5375099182128906
Test Epoch5 threshold 0.3 Acc 0.9055, AUC 0.9611735343933105, avg_entr 0.13681550323963165
ep5_t0.3_test_time 1.4530599117279053
Test Epoch5 threshold 0.4 Acc 0.9053, AUC 0.9602959752082825, avg_entr 0.14294257760047913
ep5_t0.4_test_time 1.4031662940979004
Test Epoch5 threshold 0.5 Acc 0.9051, AUC 0.9602132439613342, avg_entr 0.15030471980571747
ep5_t0.5_test_time 1.3973193168640137
Test Epoch5 threshold 0.6 Acc 0.9049, AUC 0.9604195952415466, avg_entr 0.15694157779216766
ep5_t0.6_test_time 1.3272759914398193
Test Epoch5 threshold 0.7 Acc 0.9044, AUC 0.9606730341911316, avg_entr 0.1654810756444931
ep5_t0.7_test_time 1.2879633903503418
Test Epoch5 threshold 0.8 Acc 0.9026, AUC 0.9601593613624573, avg_entr 0.17448590695858002
ep5_t0.8_test_time 1.247039556503296
Test Epoch5 threshold 0.9 Acc 0.9019, AUC 0.9602308869361877, avg_entr 0.1843487024307251
ep5_t0.9_test_time 1.2171573638916016
gc 0
Train Epoch6 Acc 0.50425 (20170/40000), AUC 0.8496948480606079
ep6_train_time 57.66029238700867
Test Epoch6 threshold 0.1 Acc 0.906, AUC 0.963043212890625, avg_entr 0.12582126259803772
ep6_t0.1_test_time 1.634079933166504
Save ckpt to ckpt/imdb_transformeral_l5_pad500_m2//imdb_transformeral_l5_prefix.pt  ,ep 6
Test Epoch6 threshold 0.2 Acc 0.906, AUC 0.9618788361549377, avg_entr 0.1311005800962448
ep6_t0.2_test_time 1.5239286422729492
Test Epoch6 threshold 0.3 Acc 0.9056, AUC 0.9611833095550537, avg_entr 0.13680996000766754
ep6_t0.3_test_time 1.4561710357666016
Test Epoch6 threshold 0.4 Acc 0.9055, AUC 0.9603097438812256, avg_entr 0.14299678802490234
ep6_t0.4_test_time 1.398653268814087
Test Epoch6 threshold 0.5 Acc 0.9052, AUC 0.9602397680282593, avg_entr 0.150165393948555
ep6_t0.5_test_time 1.3594512939453125
Test Epoch6 threshold 0.6 Acc 0.905, AUC 0.9604213237762451, avg_entr 0.1569252461194992
ep6_t0.6_test_time 1.329904317855835
Test Epoch6 threshold 0.7 Acc 0.9045, AUC 0.9606618881225586, avg_entr 0.16551218926906586
ep6_t0.7_test_time 1.2874829769134521
Test Epoch6 threshold 0.8 Acc 0.9026, AUC 0.9602100849151611, avg_entr 0.17441393435001373
ep6_t0.8_test_time 1.2515661716461182
Test Epoch6 threshold 0.9 Acc 0.902, AUC 0.9602116346359253, avg_entr 0.18422479927539825
ep6_t0.9_test_time 1.2044332027435303
gc 0
Train Epoch7 Acc 0.5043 (20172/40000), AUC 0.8495661020278931
ep7_train_time 57.68695569038391
Test Epoch7 threshold 0.1 Acc 0.9061, AUC 0.9630401134490967, avg_entr 0.12582111358642578
ep7_t0.1_test_time 1.6399314403533936
Test Epoch7 threshold 0.2 Acc 0.9061, AUC 0.961884617805481, avg_entr 0.13107499480247498
ep7_t0.2_test_time 1.5201406478881836
Test Epoch7 threshold 0.3 Acc 0.9057, AUC 0.9611840844154358, avg_entr 0.1367984265089035
ep7_t0.3_test_time 1.4736621379852295
Test Epoch7 threshold 0.4 Acc 0.9056, AUC 0.960304856300354, avg_entr 0.14301614463329315
ep7_t0.4_test_time 1.3986494541168213
Test Epoch7 threshold 0.5 Acc 0.9053, AUC 0.960240364074707, avg_entr 0.15015339851379395
ep7_t0.5_test_time 1.3568956851959229
Test Epoch7 threshold 0.6 Acc 0.9051, AUC 0.9604217410087585, avg_entr 0.15691277384757996
ep7_t0.6_test_time 1.322371244430542
Test Epoch7 threshold 0.7 Acc 0.9046, AUC 0.9606626033782959, avg_entr 0.16549964249134064
ep7_t0.7_test_time 1.2851791381835938
Test Epoch7 threshold 0.8 Acc 0.9027, AUC 0.9602104425430298, avg_entr 0.174402117729187
ep7_t0.8_test_time 1.2448554039001465
Test Epoch7 threshold 0.9 Acc 0.9021, AUC 0.9602117538452148, avg_entr 0.18421365320682526
ep7_t0.9_test_time 1.2067012786865234
gc 0
Train Epoch8 Acc 0.50435 (20174/40000), AUC 0.8508411645889282
ep8_train_time 57.69474697113037
Test Epoch8 threshold 0.1 Acc 0.9061, AUC 0.9630415439605713, avg_entr 0.12581217288970947
ep8_t0.1_test_time 1.6257777214050293
Test Epoch8 threshold 0.2 Acc 0.9061, AUC 0.961885929107666, avg_entr 0.1310652643442154
ep8_t0.2_test_time 1.5224764347076416
Test Epoch8 threshold 0.3 Acc 0.9057, AUC 0.9611952304840088, avg_entr 0.13675858080387115
ep8_t0.3_test_time 1.4525549411773682
Test Epoch8 threshold 0.4 Acc 0.9056, AUC 0.9603058099746704, avg_entr 0.14300547540187836
ep8_t0.4_test_time 1.394355297088623
Test Epoch8 threshold 0.5 Acc 0.9053, AUC 0.9602409601211548, avg_entr 0.15014344453811646
ep8_t0.5_test_time 1.3674612045288086
Test Epoch8 threshold 0.6 Acc 0.9051, AUC 0.9604215621948242, avg_entr 0.15690229833126068
ep8_t0.6_test_time 1.322983980178833
Test Epoch8 threshold 0.7 Acc 0.9046, AUC 0.9606632590293884, avg_entr 0.16548949480056763
ep8_t0.7_test_time 1.2895550727844238
Test Epoch8 threshold 0.8 Acc 0.9027, AUC 0.9602105617523193, avg_entr 0.1743927150964737
ep8_t0.8_test_time 1.2617359161376953
Test Epoch8 threshold 0.9 Acc 0.9021, AUC 0.9602113962173462, avg_entr 0.18420539796352386
ep8_t0.9_test_time 1.2078430652618408
gc 0
Train Epoch9 Acc 0.5045 (20180/40000), AUC 0.8505208492279053
ep9_train_time 57.71912336349487
Test Epoch9 threshold 0.1 Acc 0.9061, AUC 0.9630433320999146, avg_entr 0.12580007314682007
ep9_t0.1_test_time 1.6347033977508545
Save ckpt to ckpt/imdb_transformeral_l5_pad500_m2//imdb_transformeral_l5_prefix.pt  ,ep 9
Test Epoch9 threshold 0.2 Acc 0.9061, AUC 0.9618918895721436, avg_entr 0.13103903830051422
ep9_t0.2_test_time 1.5288207530975342
Test Epoch9 threshold 0.3 Acc 0.9057, AUC 0.9612088203430176, avg_entr 0.1367356926202774
ep9_t0.3_test_time 1.455307960510254
Test Epoch9 threshold 0.4 Acc 0.9056, AUC 0.9603070020675659, avg_entr 0.14298884570598602
ep9_t0.4_test_time 1.4076581001281738
Test Epoch9 threshold 0.5 Acc 0.9053, AUC 0.960241436958313, avg_entr 0.15012793242931366
ep9_t0.5_test_time 1.3635656833648682
Test Epoch9 threshold 0.6 Acc 0.9051, AUC 0.9604213237762451, avg_entr 0.15688537061214447
ep9_t0.6_test_time 1.3317041397094727
Test Epoch9 threshold 0.7 Acc 0.9046, AUC 0.9606636762619019, avg_entr 0.16547276079654694
ep9_t0.7_test_time 1.2985832691192627
Test Epoch9 threshold 0.8 Acc 0.9027, AUC 0.9602103233337402, avg_entr 0.17437705397605896
ep9_t0.8_test_time 1.2985954284667969
Test Epoch9 threshold 0.9 Acc 0.9021, AUC 0.9602108001708984, avg_entr 0.1841912418603897
ep9_t0.9_test_time 1.2711145877838135
Best AUC 0.9630433320999146
train_loss (2, 5, 10)
valid_acc (10, 9)
valid_AUC (10, 9)
train_acc (10,)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Start Testing
Load ckpt at ckpt/imdb_transformeral_l5_pad500_m2//imdb_transformeral_l5_prefix.pt
[[4490  447]
 [ 571 4492]]
Figure(640x480)
tensor([4.3029e-01, 3.9391e-03, 4.8563e-02,  ..., 6.2609e-01, 4.0317e-04,
        4.2310e-04])
[[4484  453]
 [ 485 4578]]
Figure(640x480)
tensor([5.5251e-01, 9.4829e-02, 4.0051e-03,  ..., 6.9851e-01, 5.7270e-03,
        8.4869e-05])
[[   0 4937]
 [   0 5063]]
Figure(640x480)
tensor([0.1638, 0.1553, 0.1969,  ..., 0.0898, 0.1773, 0.2365])
[[4937    0]
 [5062    1]]
Figure(640x480)
tensor([0.5060, 0.5067, 0.5027,  ..., 0.4872, 0.5090, 0.5027])
[[4937    0]
 [5043   20]]
Figure(640x480)
tensor([0.4996, 0.4900, 0.5067,  ..., 0.4199, 0.5465, 0.5978])
