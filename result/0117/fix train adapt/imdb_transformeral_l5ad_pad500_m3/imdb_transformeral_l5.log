total count words 222751
vocab size 30000
found 27937 words in glove
Load ckpt from ckpt/imdb_transformeral_l5_pad500_m2//imdb_transformeral_l5_prefix.pt
model: TransformerModelML(
  (layers): ModuleList(
    (0): EMBLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Embedding(30000, 300)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=2, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=2, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.weight 9000000
layers.0.ae.g.0.weight 256
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 256
layers.0.ae.h.0.bias 2
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
Total Trainable Params: 13671778
Start Training
train_mask {0, 1, 2}
gc 9
Train Epoch0 Acc 0.1857 (7428/40000), AUC 0.06975392252206802
ep0_train_time 69.4368405342102
Test Epoch0 threshold 0.1 Acc 0.9252, AUC 0.9664351940155029, avg_entr 0.0354095920920372
ep0_t0.1_test_time 1.557967185974121
Save ckpt to ckpt/imdb_transformeral_l5_pad500_m3//imdb_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.2 Acc 0.9249, AUC 0.9662604928016663, avg_entr 0.040954314172267914
ep0_t0.2_test_time 1.466822862625122
Test Epoch0 threshold 0.3 Acc 0.9237, AUC 0.9664199352264404, avg_entr 0.0461411289870739
ep0_t0.3_test_time 1.4019546508789062
Test Epoch0 threshold 0.4 Acc 0.9238, AUC 0.9658641219139099, avg_entr 0.051711175590753555
ep0_t0.4_test_time 1.3638784885406494
Test Epoch0 threshold 0.5 Acc 0.9224, AUC 0.9663910865783691, avg_entr 0.0578019879758358
ep0_t0.5_test_time 1.3392932415008545
Test Epoch0 threshold 0.6 Acc 0.9219, AUC 0.9663914442062378, avg_entr 0.06578173488378525
ep0_t0.6_test_time 1.2920491695404053
Test Epoch0 threshold 0.7 Acc 0.9207, AUC 0.966183066368103, avg_entr 0.07319027185440063
ep0_t0.7_test_time 1.2752337455749512
Test Epoch0 threshold 0.8 Acc 0.9193, AUC 0.9671194553375244, avg_entr 0.08255218714475632
ep0_t0.8_test_time 1.2343780994415283
Save ckpt to ckpt/imdb_transformeral_l5_pad500_m3//imdb_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.9 Acc 0.917, AUC 0.967322587966919, avg_entr 0.0948476567864418
ep0_t0.9_test_time 1.2115883827209473
Save ckpt to ckpt/imdb_transformeral_l5_pad500_m3//imdb_transformeral_l5_prefix.pt  ,ep 0
gc 0
Train Epoch1 Acc 0.118325 (4733/40000), AUC 0.03358735144138336
ep1_train_time 69.0545723438263
Test Epoch1 threshold 0.1 Acc 0.9235, AUC 0.9621002674102783, avg_entr 0.025892380625009537
ep1_t0.1_test_time 1.4814190864562988
Test Epoch1 threshold 0.2 Acc 0.9229, AUC 0.96164870262146, avg_entr 0.03097527101635933
ep1_t0.2_test_time 1.3986873626708984
Test Epoch1 threshold 0.3 Acc 0.9221, AUC 0.9624039530754089, avg_entr 0.03628283366560936
ep1_t0.3_test_time 1.3644123077392578
Test Epoch1 threshold 0.4 Acc 0.9219, AUC 0.9629144668579102, avg_entr 0.041259825229644775
ep1_t0.4_test_time 1.326608657836914
Test Epoch1 threshold 0.5 Acc 0.9205, AUC 0.9634556770324707, avg_entr 0.046659573912620544
ep1_t0.5_test_time 1.293165922164917
Test Epoch1 threshold 0.6 Acc 0.9199, AUC 0.9640417695045471, avg_entr 0.05187661200761795
ep1_t0.6_test_time 1.2601492404937744
Test Epoch1 threshold 0.7 Acc 0.919, AUC 0.9649636745452881, avg_entr 0.05993811786174774
ep1_t0.7_test_time 1.2377982139587402
Test Epoch1 threshold 0.8 Acc 0.9184, AUC 0.966329038143158, avg_entr 0.06933264434337616
ep1_t0.8_test_time 1.2107717990875244
Test Epoch1 threshold 0.9 Acc 0.9179, AUC 0.9673826694488525, avg_entr 0.08045782148838043
ep1_t0.9_test_time 1.173403024673462
Save ckpt to ckpt/imdb_transformeral_l5_pad500_m3//imdb_transformeral_l5_prefix.pt  ,ep 1
gc 0
Train Epoch2 Acc 0.16955 (6782/40000), AUC 0.029909461736679077
ep2_train_time 69.02558183670044
Test Epoch2 threshold 0.1 Acc 0.9247, AUC 0.9595770835876465, avg_entr 0.02494794689118862
ep2_t0.1_test_time 1.477661371231079
Test Epoch2 threshold 0.2 Acc 0.924, AUC 0.9600620269775391, avg_entr 0.0299708042293787
ep2_t0.2_test_time 1.3886678218841553
Test Epoch2 threshold 0.3 Acc 0.9242, AUC 0.9605687856674194, avg_entr 0.035102952271699905
ep2_t0.3_test_time 1.3511691093444824
Test Epoch2 threshold 0.4 Acc 0.9239, AUC 0.9619390368461609, avg_entr 0.040553294122219086
ep2_t0.4_test_time 1.3251378536224365
Test Epoch2 threshold 0.5 Acc 0.9234, AUC 0.962864875793457, avg_entr 0.0470772385597229
ep2_t0.5_test_time 1.2891578674316406
Test Epoch2 threshold 0.6 Acc 0.9228, AUC 0.9640008211135864, avg_entr 0.05276601389050484
ep2_t0.6_test_time 1.2548120021820068
Test Epoch2 threshold 0.7 Acc 0.9223, AUC 0.9648217558860779, avg_entr 0.05983305722475052
ep2_t0.7_test_time 1.2386500835418701
Test Epoch2 threshold 0.8 Acc 0.9212, AUC 0.9659383296966553, avg_entr 0.06716133654117584
ep2_t0.8_test_time 1.2019028663635254
Test Epoch2 threshold 0.9 Acc 0.9197, AUC 0.9666534662246704, avg_entr 0.0772751122713089
ep2_t0.9_test_time 1.1822593212127686
gc 0
Train Epoch3 Acc 0.1813 (7252/40000), AUC 0.02896382473409176
ep3_train_time 69.0304753780365
Test Epoch3 threshold 0.1 Acc 0.9237, AUC 0.9595276117324829, avg_entr 0.023645594716072083
ep3_t0.1_test_time 1.4803338050842285
Test Epoch3 threshold 0.2 Acc 0.9237, AUC 0.9598883986473083, avg_entr 0.028913967311382294
ep3_t0.2_test_time 1.3837993144989014
Test Epoch3 threshold 0.3 Acc 0.9229, AUC 0.9605870246887207, avg_entr 0.033860087394714355
ep3_t0.3_test_time 1.3529362678527832
Test Epoch3 threshold 0.4 Acc 0.923, AUC 0.9616119265556335, avg_entr 0.039685823023319244
ep3_t0.4_test_time 1.3327090740203857
Test Epoch3 threshold 0.5 Acc 0.923, AUC 0.9624913930892944, avg_entr 0.04562373459339142
ep3_t0.5_test_time 1.3080923557281494
Test Epoch3 threshold 0.6 Acc 0.9217, AUC 0.9634427428245544, avg_entr 0.05163511261343956
ep3_t0.6_test_time 1.262619972229004
Test Epoch3 threshold 0.7 Acc 0.9213, AUC 0.9645075798034668, avg_entr 0.058605894446372986
ep3_t0.7_test_time 1.236131191253662
Test Epoch3 threshold 0.8 Acc 0.9209, AUC 0.9655563831329346, avg_entr 0.06608309596776962
ep3_t0.8_test_time 1.2392857074737549
Test Epoch3 threshold 0.9 Acc 0.9188, AUC 0.9668488502502441, avg_entr 0.07752879709005356
ep3_t0.9_test_time 1.1848738193511963
gc 0
Train Epoch4 Acc 0.18555 (7422/40000), AUC 0.02877013385295868
ep4_train_time 69.03781080245972
Test Epoch4 threshold 0.1 Acc 0.9243, AUC 0.9596443176269531, avg_entr 0.024629969149827957
ep4_t0.1_test_time 1.4707074165344238
Test Epoch4 threshold 0.2 Acc 0.9239, AUC 0.9597179889678955, avg_entr 0.02953793667256832
ep4_t0.2_test_time 1.3902015686035156
Test Epoch4 threshold 0.3 Acc 0.9235, AUC 0.9604089856147766, avg_entr 0.03469362482428551
ep4_t0.3_test_time 1.3462958335876465
Test Epoch4 threshold 0.4 Acc 0.9238, AUC 0.9616789817810059, avg_entr 0.039778728038072586
ep4_t0.4_test_time 1.3451685905456543
Test Epoch4 threshold 0.5 Acc 0.9232, AUC 0.9624094367027283, avg_entr 0.04599134251475334
ep4_t0.5_test_time 1.285628318786621
Test Epoch4 threshold 0.6 Acc 0.9224, AUC 0.9634815454483032, avg_entr 0.05234205350279808
ep4_t0.6_test_time 1.262098789215088
Test Epoch4 threshold 0.7 Acc 0.9218, AUC 0.9645249843597412, avg_entr 0.05937421694397926
ep4_t0.7_test_time 1.2308826446533203
Test Epoch4 threshold 0.8 Acc 0.9212, AUC 0.9656245708465576, avg_entr 0.06666835397481918
ep4_t0.8_test_time 1.2186250686645508
Test Epoch4 threshold 0.9 Acc 0.9188, AUC 0.9665578603744507, avg_entr 0.07810002565383911
ep4_t0.9_test_time 1.1834964752197266
gc 0
Train Epoch5 Acc 0.1856 (7424/40000), AUC 0.02890433371067047
ep5_train_time 69.0823905467987
Test Epoch5 threshold 0.1 Acc 0.9245, AUC 0.9593793153762817, avg_entr 0.024419646710157394
ep5_t0.1_test_time 1.49937105178833
Test Epoch5 threshold 0.2 Acc 0.9244, AUC 0.9599146842956543, avg_entr 0.029453640803694725
ep5_t0.2_test_time 1.3911337852478027
Test Epoch5 threshold 0.3 Acc 0.9237, AUC 0.9603744149208069, avg_entr 0.034442655742168427
ep5_t0.3_test_time 1.352675199508667
Test Epoch5 threshold 0.4 Acc 0.924, AUC 0.9614696502685547, avg_entr 0.039720311760902405
ep5_t0.4_test_time 1.3151252269744873
Test Epoch5 threshold 0.5 Acc 0.9235, AUC 0.9623551964759827, avg_entr 0.04595954716205597
ep5_t0.5_test_time 1.2928285598754883
Test Epoch5 threshold 0.6 Acc 0.9228, AUC 0.9635655879974365, avg_entr 0.05231379345059395
ep5_t0.6_test_time 1.2759428024291992
Test Epoch5 threshold 0.7 Acc 0.9218, AUC 0.9646549224853516, avg_entr 0.059384144842624664
ep5_t0.7_test_time 1.2472829818725586
Test Epoch5 threshold 0.8 Acc 0.9211, AUC 0.9656692743301392, avg_entr 0.06655897200107574
ep5_t0.8_test_time 1.289743423461914
Test Epoch5 threshold 0.9 Acc 0.9192, AUC 0.9669718742370605, avg_entr 0.07797391712665558
ep5_t0.9_test_time 1.2559409141540527
gc 0
Train Epoch6 Acc 0.185525 (7421/40000), AUC 0.02835335209965706
ep6_train_time 69.26342391967773
Test Epoch6 threshold 0.1 Acc 0.9244, AUC 0.9593767523765564, avg_entr 0.02439063787460327
ep6_t0.1_test_time 1.4711663722991943
Test Epoch6 threshold 0.2 Acc 0.9243, AUC 0.9598961472511292, avg_entr 0.029466543346643448
ep6_t0.2_test_time 1.3860397338867188
Test Epoch6 threshold 0.3 Acc 0.9236, AUC 0.9603797793388367, avg_entr 0.03444669023156166
ep6_t0.3_test_time 1.3513426780700684
Test Epoch6 threshold 0.4 Acc 0.9239, AUC 0.9614649415016174, avg_entr 0.03971485793590546
ep6_t0.4_test_time 1.3147461414337158
Test Epoch6 threshold 0.5 Acc 0.9234, AUC 0.9623526930809021, avg_entr 0.04592449590563774
ep6_t0.5_test_time 1.2921066284179688
Test Epoch6 threshold 0.6 Acc 0.9226, AUC 0.9635640382766724, avg_entr 0.05230598896741867
ep6_t0.6_test_time 1.2815895080566406
Test Epoch6 threshold 0.7 Acc 0.9217, AUC 0.9646501541137695, avg_entr 0.059353552758693695
ep6_t0.7_test_time 1.2349700927734375
Test Epoch6 threshold 0.8 Acc 0.921, AUC 0.9656633138656616, avg_entr 0.06653986126184464
ep6_t0.8_test_time 1.2078263759613037
Test Epoch6 threshold 0.9 Acc 0.9191, AUC 0.9669662714004517, avg_entr 0.07796710729598999
ep6_t0.9_test_time 1.176973581314087
gc 0
Train Epoch7 Acc 0.184625 (7385/40000), AUC 0.02844594046473503
ep7_train_time 69.07393336296082
Test Epoch7 threshold 0.1 Acc 0.9244, AUC 0.9593746066093445, avg_entr 0.02438202127814293
ep7_t0.1_test_time 1.4668970108032227
Test Epoch7 threshold 0.2 Acc 0.9243, AUC 0.9598941206932068, avg_entr 0.02946361154317856
ep7_t0.2_test_time 1.4018075466156006
Test Epoch7 threshold 0.3 Acc 0.9236, AUC 0.9603795409202576, avg_entr 0.03444012627005577
ep7_t0.3_test_time 1.3802378177642822
Test Epoch7 threshold 0.4 Acc 0.9239, AUC 0.9614651203155518, avg_entr 0.039714038372039795
ep7_t0.4_test_time 1.361764669418335
Test Epoch7 threshold 0.5 Acc 0.9234, AUC 0.9623582363128662, avg_entr 0.045940693467855453
ep7_t0.5_test_time 1.2856559753417969
Test Epoch7 threshold 0.6 Acc 0.9226, AUC 0.9635640382766724, avg_entr 0.0523042157292366
ep7_t0.6_test_time 1.2585780620574951
Test Epoch7 threshold 0.7 Acc 0.9217, AUC 0.9646493196487427, avg_entr 0.05935085937380791
ep7_t0.7_test_time 1.2290384769439697
Test Epoch7 threshold 0.8 Acc 0.921, AUC 0.9656468629837036, avg_entr 0.06665338575839996
ep7_t0.8_test_time 1.212517499923706
Test Epoch7 threshold 0.9 Acc 0.9191, AUC 0.9669651389122009, avg_entr 0.07796613872051239
ep7_t0.9_test_time 1.180884838104248
gc 0
Train Epoch8 Acc 0.184725 (7389/40000), AUC 0.028915878385305405
ep8_train_time 69.12562227249146
Test Epoch8 threshold 0.1 Acc 0.9244, AUC 0.9593729972839355, avg_entr 0.02437678910791874
ep8_t0.1_test_time 1.471020221710205
Test Epoch8 threshold 0.2 Acc 0.9243, AUC 0.9598921537399292, avg_entr 0.029458746314048767
ep8_t0.2_test_time 1.396942377090454
Test Epoch8 threshold 0.3 Acc 0.9236, AUC 0.9603781700134277, avg_entr 0.03443581983447075
ep8_t0.3_test_time 1.349501371383667
Test Epoch8 threshold 0.4 Acc 0.9239, AUC 0.96147620677948, avg_entr 0.03973707929253578
ep8_t0.4_test_time 1.3159737586975098
Test Epoch8 threshold 0.5 Acc 0.9233, AUC 0.9623607397079468, avg_entr 0.04594235122203827
ep8_t0.5_test_time 1.2928671836853027
Test Epoch8 threshold 0.6 Acc 0.9225, AUC 0.9635630249977112, avg_entr 0.05230267345905304
ep8_t0.6_test_time 1.2613775730133057
Test Epoch8 threshold 0.7 Acc 0.9216, AUC 0.9646481275558472, avg_entr 0.059348881244659424
ep8_t0.7_test_time 1.2321362495422363
Test Epoch8 threshold 0.8 Acc 0.9209, AUC 0.965645432472229, avg_entr 0.06665033847093582
ep8_t0.8_test_time 1.2196125984191895
Test Epoch8 threshold 0.9 Acc 0.9191, AUC 0.9669642448425293, avg_entr 0.07796529680490494
ep8_t0.9_test_time 1.180938720703125
gc 0
Train Epoch9 Acc 0.184 (7360/40000), AUC 0.028190813958644867
ep9_train_time 69.06181812286377
Test Epoch9 threshold 0.1 Acc 0.9244, AUC 0.9593656659126282, avg_entr 0.02437073364853859
ep9_t0.1_test_time 1.4634368419647217
Test Epoch9 threshold 0.2 Acc 0.9243, AUC 0.9598860144615173, avg_entr 0.029461901634931564
ep9_t0.2_test_time 1.3882677555084229
Test Epoch9 threshold 0.3 Acc 0.9236, AUC 0.9603759050369263, avg_entr 0.03442571312189102
ep9_t0.3_test_time 1.3445563316345215
Test Epoch9 threshold 0.4 Acc 0.9239, AUC 0.9614747762680054, avg_entr 0.039730485528707504
ep9_t0.4_test_time 1.3140201568603516
Test Epoch9 threshold 0.5 Acc 0.9233, AUC 0.9623599648475647, avg_entr 0.04593895748257637
ep9_t0.5_test_time 1.2870042324066162
Test Epoch9 threshold 0.6 Acc 0.9225, AUC 0.9635583162307739, avg_entr 0.05238555744290352
ep9_t0.6_test_time 1.2538857460021973
Test Epoch9 threshold 0.7 Acc 0.9216, AUC 0.9646462798118591, avg_entr 0.05934491753578186
ep9_t0.7_test_time 1.2306387424468994
Test Epoch9 threshold 0.8 Acc 0.9209, AUC 0.9656423330307007, avg_entr 0.06664298474788666
ep9_t0.8_test_time 1.2039952278137207
Test Epoch9 threshold 0.9 Acc 0.9191, AUC 0.966961681842804, avg_entr 0.07795999944210052
ep9_t0.9_test_time 1.172983169555664
Best AUC 0.9673826694488525
train_loss (2, 5, 10)
valid_acc (10, 9)
valid_AUC (10, 9)
train_acc (10,)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Start Testing
Load ckpt at ckpt/imdb_transformeral_l5_pad500_m3//imdb_transformeral_l5_prefix.pt
[[4705  355]
 [ 516 4424]]
Figure(640x480)
tensor([1.1885e-03, 4.3893e-06, 1.3244e-01,  ..., 4.2606e-01, 1.7419e-01,
        4.6027e-02])
[[4756  304]
 [ 460 4480]]
Figure(640x480)
tensor([3.7839e-05, 1.4737e-06, 8.1558e-05,  ..., 1.8556e-01, 5.2308e-04,
        2.0074e-04])
[[4753  307]
 [ 458 4482]]
Figure(640x480)
tensor([3.6673e-05, 4.0481e-06, 7.3150e-05,  ..., 4.8957e-02, 7.5049e-05,
        3.4153e-04])
[[   0 5060]
 [  15 4925]]
Figure(640x480)
tensor([0.1815, 0.2904, 0.0554,  ..., 0.0407, 0.1855, 0.0515])
[[ 133 4927]
 [3478 1462]]
Figure(640x480)
tensor([0.6822, 0.5820, 0.4126,  ..., 0.5976, 0.6821, 0.4445])
