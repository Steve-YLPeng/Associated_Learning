total count words 887881
vocab size 30000
found 28354 words in glove
Load ckpt from ckpt/dbpedia_14_transformeral_l5_pad60_m2//dbpedia_14_transformeral_l5_prefix.pt
model: TransformerModelML(
  (layers): ModuleList(
    (0): EMBLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Embedding(30000, 300)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=14, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=14, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.weight 9000000
layers.0.ae.g.0.weight 1792
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 1792
layers.0.ae.h.0.bias 14
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
Total Trainable Params: 13674862
Start Training
train_mask {0, 1, 2}
gc 9
Train Epoch0 Acc 0.1405482142857143 (78707/560000), AUC 0.5783856511116028
ep0_train_time 120.2060284614563
Test Epoch0 threshold 0.1 Acc 0.9758428571428571, AUC 0.9981710314750671, avg_entr 0.00425855815410614
ep0_t0.1_test_time 1.9718410968780518
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m3//dbpedia_14_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.2 Acc 0.9747428571428571, AUC 0.9983134269714355, avg_entr 0.0089150071144104
ep0_t0.2_test_time 1.5104906558990479
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m3//dbpedia_14_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.3 Acc 0.9744142857142857, AUC 0.9983107447624207, avg_entr 0.009206686168909073
ep0_t0.3_test_time 1.4550516605377197
Test Epoch0 threshold 0.4 Acc 0.9744142857142857, AUC 0.9983107447624207, avg_entr 0.009206686168909073
ep0_t0.4_test_time 1.4677302837371826
Test Epoch0 threshold 0.5 Acc 0.9744142857142857, AUC 0.9983107447624207, avg_entr 0.009206686168909073
ep0_t0.5_test_time 1.4631447792053223
Test Epoch0 threshold 0.6 Acc 0.9744142857142857, AUC 0.9983107447624207, avg_entr 0.009206686168909073
ep0_t0.6_test_time 1.464646339416504
Test Epoch0 threshold 0.7 Acc 0.9744142857142857, AUC 0.9983107447624207, avg_entr 0.009206686168909073
ep0_t0.7_test_time 1.444464921951294
Test Epoch0 threshold 0.8 Acc 0.9744142857142857, AUC 0.9983107447624207, avg_entr 0.009206686168909073
ep0_t0.8_test_time 1.4562385082244873
Test Epoch0 threshold 0.9 Acc 0.9744142857142857, AUC 0.9983107447624207, avg_entr 0.009206686168909073
ep0_t0.9_test_time 1.4642632007598877
gc 0
Train Epoch1 Acc 0.09280178571428571 (51969/560000), AUC 0.5650824904441833
ep1_train_time 118.7056188583374
Test Epoch1 threshold 0.1 Acc 0.9765714285714285, AUC 0.9982231855392456, avg_entr 0.004182471428066492
ep1_t0.1_test_time 1.8999416828155518
Test Epoch1 threshold 0.2 Acc 0.9753, AUC 0.998331606388092, avg_entr 0.00892577227205038
ep1_t0.2_test_time 1.5331008434295654
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m3//dbpedia_14_transformeral_l5_prefix.pt  ,ep 1
Test Epoch1 threshold 0.3 Acc 0.9750142857142857, AUC 0.9983276724815369, avg_entr 0.009228037670254707
ep1_t0.3_test_time 1.4594945907592773
Test Epoch1 threshold 0.4 Acc 0.9750142857142857, AUC 0.9983276724815369, avg_entr 0.009228037670254707
ep1_t0.4_test_time 1.4660868644714355
Test Epoch1 threshold 0.5 Acc 0.9750142857142857, AUC 0.9983276724815369, avg_entr 0.009228037670254707
ep1_t0.5_test_time 1.4735827445983887
Test Epoch1 threshold 0.6 Acc 0.9750142857142857, AUC 0.9983276724815369, avg_entr 0.009228037670254707
ep1_t0.6_test_time 1.467344045639038
Test Epoch1 threshold 0.7 Acc 0.9750142857142857, AUC 0.9983276724815369, avg_entr 0.009228037670254707
ep1_t0.7_test_time 1.4704153537750244
Test Epoch1 threshold 0.8 Acc 0.9750142857142857, AUC 0.9983276724815369, avg_entr 0.009228037670254707
ep1_t0.8_test_time 1.462017297744751
Test Epoch1 threshold 0.9 Acc 0.9750142857142857, AUC 0.9983276724815369, avg_entr 0.009228037670254707
ep1_t0.9_test_time 1.454591989517212
gc 0
Train Epoch2 Acc 0.07183928571428572 (40230/560000), AUC 0.5459322333335876
ep2_train_time 119.01759576797485
Test Epoch2 threshold 0.1 Acc 0.9764142857142857, AUC 0.9982125163078308, avg_entr 0.004138446878641844
ep2_t0.1_test_time 1.80930495262146
Test Epoch2 threshold 0.2 Acc 0.9751, AUC 0.9983340501785278, avg_entr 0.00891072116792202
ep2_t0.2_test_time 1.5179531574249268
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m3//dbpedia_14_transformeral_l5_prefix.pt  ,ep 2
Test Epoch2 threshold 0.3 Acc 0.9747857142857143, AUC 0.9983299970626831, avg_entr 0.0092415576800704
ep2_t0.3_test_time 1.4576702117919922
Test Epoch2 threshold 0.4 Acc 0.9747857142857143, AUC 0.9983299970626831, avg_entr 0.0092415576800704
ep2_t0.4_test_time 1.4732396602630615
Test Epoch2 threshold 0.5 Acc 0.9747857142857143, AUC 0.9983299970626831, avg_entr 0.0092415576800704
ep2_t0.5_test_time 1.4562158584594727
Test Epoch2 threshold 0.6 Acc 0.9747857142857143, AUC 0.9983299970626831, avg_entr 0.0092415576800704
ep2_t0.6_test_time 1.4636814594268799
Test Epoch2 threshold 0.7 Acc 0.9747857142857143, AUC 0.9983299970626831, avg_entr 0.0092415576800704
ep2_t0.7_test_time 1.4635791778564453
Test Epoch2 threshold 0.8 Acc 0.9747857142857143, AUC 0.9983299970626831, avg_entr 0.0092415576800704
ep2_t0.8_test_time 1.4570419788360596
Test Epoch2 threshold 0.9 Acc 0.9747857142857143, AUC 0.9983299970626831, avg_entr 0.0092415576800704
ep2_t0.9_test_time 1.4662706851959229
gc 0
Train Epoch3 Acc 0.07160178571428572 (40097/560000), AUC 0.5406492948532104
ep3_train_time 119.25557923316956
Test Epoch3 threshold 0.1 Acc 0.9763857142857143, AUC 0.9982016682624817, avg_entr 0.004128108266741037
ep3_t0.1_test_time 1.8399605751037598
Test Epoch3 threshold 0.2 Acc 0.9752, AUC 0.9983323216438293, avg_entr 0.008899269625544548
ep3_t0.2_test_time 1.5119178295135498
Test Epoch3 threshold 0.3 Acc 0.9748428571428571, AUC 0.9983288645744324, avg_entr 0.00922592543065548
ep3_t0.3_test_time 1.4555795192718506
Test Epoch3 threshold 0.4 Acc 0.9748428571428571, AUC 0.9983288645744324, avg_entr 0.00922592543065548
ep3_t0.4_test_time 1.4443283081054688
Test Epoch3 threshold 0.5 Acc 0.9748428571428571, AUC 0.9983288645744324, avg_entr 0.00922592543065548
ep3_t0.5_test_time 1.4630794525146484
Test Epoch3 threshold 0.6 Acc 0.9748428571428571, AUC 0.9983288645744324, avg_entr 0.00922592543065548
ep3_t0.6_test_time 1.454970121383667
Test Epoch3 threshold 0.7 Acc 0.9748428571428571, AUC 0.9983288645744324, avg_entr 0.00922592543065548
ep3_t0.7_test_time 1.4545838832855225
Test Epoch3 threshold 0.8 Acc 0.9748428571428571, AUC 0.9983288645744324, avg_entr 0.00922592543065548
ep3_t0.8_test_time 1.4681999683380127
Test Epoch3 threshold 0.9 Acc 0.9748428571428571, AUC 0.9983288645744324, avg_entr 0.00922592543065548
ep3_t0.9_test_time 1.4532127380371094
gc 0
Train Epoch4 Acc 0.07158214285714286 (40086/560000), AUC 0.5395148992538452
ep4_train_time 119.0007712841034
Test Epoch4 threshold 0.1 Acc 0.9763857142857143, AUC 0.9982157349586487, avg_entr 0.004133275244385004
ep4_t0.1_test_time 1.8260841369628906
Test Epoch4 threshold 0.2 Acc 0.9751571428571428, AUC 0.998332142829895, avg_entr 0.00891069881618023
ep4_t0.2_test_time 1.5235793590545654
Test Epoch4 threshold 0.3 Acc 0.9748714285714286, AUC 0.9983286261558533, avg_entr 0.009231430478394032
ep4_t0.3_test_time 1.4671599864959717
Test Epoch4 threshold 0.4 Acc 0.9748714285714286, AUC 0.9983286261558533, avg_entr 0.009231430478394032
ep4_t0.4_test_time 1.4709203243255615
Test Epoch4 threshold 0.5 Acc 0.9748714285714286, AUC 0.9983286261558533, avg_entr 0.009231430478394032
ep4_t0.5_test_time 1.4701826572418213
Test Epoch4 threshold 0.6 Acc 0.9748714285714286, AUC 0.9983286261558533, avg_entr 0.009231430478394032
ep4_t0.6_test_time 1.4676942825317383
Test Epoch4 threshold 0.7 Acc 0.9748714285714286, AUC 0.9983286261558533, avg_entr 0.009231430478394032
ep4_t0.7_test_time 1.445242166519165
Test Epoch4 threshold 0.8 Acc 0.9748714285714286, AUC 0.9983286261558533, avg_entr 0.009231430478394032
ep4_t0.8_test_time 1.4779272079467773
Test Epoch4 threshold 0.9 Acc 0.9748714285714286, AUC 0.9983286261558533, avg_entr 0.009231430478394032
ep4_t0.9_test_time 1.4669811725616455
gc 0
Train Epoch5 Acc 0.07160178571428572 (40097/560000), AUC 0.5391649603843689
ep5_train_time 119.17676115036011
Test Epoch5 threshold 0.1 Acc 0.9763428571428572, AUC 0.9982154965400696, avg_entr 0.004138832446187735
ep5_t0.1_test_time 1.8097076416015625
Test Epoch5 threshold 0.2 Acc 0.9751285714285715, AUC 0.9983320832252502, avg_entr 0.008907428942620754
ep5_t0.2_test_time 1.524928331375122
Test Epoch5 threshold 0.3 Acc 0.9748428571428571, AUC 0.9983283281326294, avg_entr 0.009234009310603142
ep5_t0.3_test_time 1.4612271785736084
Test Epoch5 threshold 0.4 Acc 0.9748428571428571, AUC 0.9983283281326294, avg_entr 0.009234009310603142
ep5_t0.4_test_time 1.487515926361084
Test Epoch5 threshold 0.5 Acc 0.9748428571428571, AUC 0.9983283281326294, avg_entr 0.009234009310603142
ep5_t0.5_test_time 1.4797604084014893
Test Epoch5 threshold 0.6 Acc 0.9748428571428571, AUC 0.9983283281326294, avg_entr 0.009234009310603142
ep5_t0.6_test_time 1.4521489143371582
Test Epoch5 threshold 0.7 Acc 0.9748428571428571, AUC 0.9983283281326294, avg_entr 0.009234009310603142
ep5_t0.7_test_time 1.4571526050567627
Test Epoch5 threshold 0.8 Acc 0.9748428571428571, AUC 0.9983283281326294, avg_entr 0.009234009310603142
ep5_t0.8_test_time 1.4498274326324463
Test Epoch5 threshold 0.9 Acc 0.9748428571428571, AUC 0.9983283281326294, avg_entr 0.009234009310603142
ep5_t0.9_test_time 1.4681363105773926
gc 0
Train Epoch6 Acc 0.07158035714285714 (40085/560000), AUC 0.5392530560493469
ep6_train_time 119.29632091522217
Test Epoch6 threshold 0.1 Acc 0.9763571428571428, AUC 0.9982157349586487, avg_entr 0.004137697629630566
ep6_t0.1_test_time 1.832794189453125
Test Epoch6 threshold 0.2 Acc 0.9751285714285715, AUC 0.9983320236206055, avg_entr 0.008909439668059349
ep6_t0.2_test_time 1.545896053314209
Test Epoch6 threshold 0.3 Acc 0.9748428571428571, AUC 0.9983283281326294, avg_entr 0.009233139455318451
ep6_t0.3_test_time 1.4709930419921875
Test Epoch6 threshold 0.4 Acc 0.9748428571428571, AUC 0.9983283281326294, avg_entr 0.009233139455318451
ep6_t0.4_test_time 1.4701716899871826
Test Epoch6 threshold 0.5 Acc 0.9748428571428571, AUC 0.9983283281326294, avg_entr 0.009233139455318451
ep6_t0.5_test_time 1.462294340133667
Test Epoch6 threshold 0.6 Acc 0.9748428571428571, AUC 0.9983283281326294, avg_entr 0.009233139455318451
ep6_t0.6_test_time 1.451742172241211
Test Epoch6 threshold 0.7 Acc 0.9748428571428571, AUC 0.9983283281326294, avg_entr 0.009233139455318451
ep6_t0.7_test_time 1.4593627452850342
Test Epoch6 threshold 0.8 Acc 0.9748428571428571, AUC 0.9983283281326294, avg_entr 0.009233139455318451
ep6_t0.8_test_time 1.4522933959960938
Test Epoch6 threshold 0.9 Acc 0.9748428571428571, AUC 0.9983283281326294, avg_entr 0.009233139455318451
ep6_t0.9_test_time 1.4651567935943604
gc 0
Train Epoch7 Acc 0.07158035714285714 (40085/560000), AUC 0.5391451120376587
ep7_train_time 118.89885020256042
Test Epoch7 threshold 0.1 Acc 0.9763571428571428, AUC 0.9982157349586487, avg_entr 0.0041389488615095615
ep7_t0.1_test_time 1.8564634323120117
Test Epoch7 threshold 0.2 Acc 0.9751285714285715, AUC 0.9983320832252502, avg_entr 0.008909333497285843
ep7_t0.2_test_time 1.5214555263519287
Test Epoch7 threshold 0.3 Acc 0.9748428571428571, AUC 0.9983283877372742, avg_entr 0.009233023039996624
ep7_t0.3_test_time 1.4652788639068604
Test Epoch7 threshold 0.4 Acc 0.9748428571428571, AUC 0.9983283877372742, avg_entr 0.009233023039996624
ep7_t0.4_test_time 1.4575488567352295
Test Epoch7 threshold 0.5 Acc 0.9748428571428571, AUC 0.9983283877372742, avg_entr 0.009233023039996624
ep7_t0.5_test_time 1.4633772373199463
Test Epoch7 threshold 0.6 Acc 0.9748428571428571, AUC 0.9983283877372742, avg_entr 0.009233023039996624
ep7_t0.6_test_time 1.4572160243988037
Test Epoch7 threshold 0.7 Acc 0.9748428571428571, AUC 0.9983283877372742, avg_entr 0.009233023039996624
ep7_t0.7_test_time 1.463047981262207
Test Epoch7 threshold 0.8 Acc 0.9748428571428571, AUC 0.9983283877372742, avg_entr 0.009233023039996624
ep7_t0.8_test_time 1.4612820148468018
Test Epoch7 threshold 0.9 Acc 0.9748428571428571, AUC 0.9983283877372742, avg_entr 0.009233023039996624
ep7_t0.9_test_time 1.4605450630187988
gc 0
Train Epoch8 Acc 0.0715875 (40089/560000), AUC 0.5391903519630432
ep8_train_time 119.19470596313477
Test Epoch8 threshold 0.1 Acc 0.9763571428571428, AUC 0.9982157945632935, avg_entr 0.004139658994972706
ep8_t0.1_test_time 1.8680553436279297
Test Epoch8 threshold 0.2 Acc 0.9751285714285715, AUC 0.9983320832252502, avg_entr 0.00890928041189909
ep8_t0.2_test_time 1.5140271186828613
Test Epoch8 threshold 0.3 Acc 0.9748428571428571, AUC 0.9983285069465637, avg_entr 0.009232966229319572
ep8_t0.3_test_time 1.464433193206787
Test Epoch8 threshold 0.4 Acc 0.9748428571428571, AUC 0.9983285069465637, avg_entr 0.009232966229319572
ep8_t0.4_test_time 1.4490926265716553
Test Epoch8 threshold 0.5 Acc 0.9748428571428571, AUC 0.9983285069465637, avg_entr 0.009232966229319572
ep8_t0.5_test_time 1.4534640312194824
Test Epoch8 threshold 0.6 Acc 0.9748428571428571, AUC 0.9983285069465637, avg_entr 0.009232966229319572
ep8_t0.6_test_time 1.4452052116394043
Test Epoch8 threshold 0.7 Acc 0.9748428571428571, AUC 0.9983285069465637, avg_entr 0.009232966229319572
ep8_t0.7_test_time 1.4592344760894775
Test Epoch8 threshold 0.8 Acc 0.9748428571428571, AUC 0.9983285069465637, avg_entr 0.009232966229319572
ep8_t0.8_test_time 1.4517896175384521
Test Epoch8 threshold 0.9 Acc 0.9748428571428571, AUC 0.9983285069465637, avg_entr 0.009232966229319572
ep8_t0.9_test_time 1.4541988372802734
gc 0
Train Epoch9 Acc 0.07159464285714286 (40093/560000), AUC 0.5391748547554016
ep9_train_time 119.10504341125488
Test Epoch9 threshold 0.1 Acc 0.9763714285714286, AUC 0.9982157349586487, avg_entr 0.0041410350240767
ep9_t0.1_test_time 1.8133103847503662
Test Epoch9 threshold 0.2 Acc 0.9751428571428571, AUC 0.9983320832252502, avg_entr 0.00890925619751215
ep9_t0.2_test_time 1.5281949043273926
Test Epoch9 threshold 0.3 Acc 0.9748571428571429, AUC 0.9983283877372742, avg_entr 0.009232933633029461
ep9_t0.3_test_time 1.4781701564788818
Test Epoch9 threshold 0.4 Acc 0.9748571428571429, AUC 0.9983283877372742, avg_entr 0.009232933633029461
ep9_t0.4_test_time 1.4609529972076416
Test Epoch9 threshold 0.5 Acc 0.9748571428571429, AUC 0.9983283877372742, avg_entr 0.009232933633029461
ep9_t0.5_test_time 1.458289623260498
Test Epoch9 threshold 0.6 Acc 0.9748571428571429, AUC 0.9983283877372742, avg_entr 0.009232933633029461
ep9_t0.6_test_time 1.456202507019043
Test Epoch9 threshold 0.7 Acc 0.9748571428571429, AUC 0.9983283877372742, avg_entr 0.009232933633029461
ep9_t0.7_test_time 1.47239089012146
Test Epoch9 threshold 0.8 Acc 0.9748571428571429, AUC 0.9983283877372742, avg_entr 0.009232933633029461
ep9_t0.8_test_time 1.4646151065826416
Test Epoch9 threshold 0.9 Acc 0.9748571428571429, AUC 0.9983283877372742, avg_entr 0.009232933633029461
ep9_t0.9_test_time 1.465742588043213
Best AUC 0.9983340501785278
train_loss (2, 5, 10)
valid_acc (10, 9)
valid_AUC (10, 9)
train_acc (10,)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Start Testing
Load ckpt at ckpt/dbpedia_14_transformeral_l5_pad60_m3//dbpedia_14_transformeral_l5_prefix.pt
[[4710   37   22   15   12   63   41    8    4    6    4   17   19   42]
 [  36 4893    3    1    8    0   33    8    3    2    1    0    4    8]
 [  34   13 4644   17   51    2   10    3    4    1    1   76   22  122]
 [   4    1   29 4950   12    0    1    0    0    0    0    0    0    3]
 [   8   19   72   12 4855    7    6    1    2    1    0    0    4   13]
 [  35    1    3    3    1 4944    5    3    1    2    0    0    1    1]
 [  60   46    7    3   10   16 4796   37   12    5    1    1    4    2]
 [   0    1    0    0    2    0   12 4963   17    3    1    0    0    1]
 [   1    3    2    0    3    0    9   11 4971    0    0    0    0    0]
 [   1    0    1    2    0    0    0    7    0 4955   33    0    0    1]
 [  12    1    0    0    0    0    2    4    0   30 4950    0    0    1]
 [   6    0   38    2    0    0    0    0    0    0    0 4929   11   14]
 [   8    2   25    4    1    2    1    3    1    2    0   22 4886   43]
 [  32    3   85    7   12    3    4    9    1    5    4    6   40 4789]]
Figure(640x480)
tensor([2.5282e-07, 8.0670e-02, 4.5256e-04,  ..., 1.6201e-07, 1.6826e-03,
        3.6757e-06])
[[4740   40   20    7   14   48   45    5    0    3    2   17   12   47]
 [  35 4911    5    1    6    0   30    0    3    1    1    0    1    6]
 [  26    8 4752   10   60    1    8    0    0    1    3   38   16   77]
 [   4    1   19 4958   12    0    0    1    0    1    0    2    1    1]
 [   8    8   73   12 4873    5    8    1    1    3    0    0    2    6]
 [  39    0    0    0    1 4950    4    2    1    0    0    0    1    2]
 [  56   38    5    2    8   12 4836   26    6    4    0    1    2    4]
 [   4    1    0    0    0    0   14 4961   13    4    2    0    0    1]
 [   2    1    2    0    4    0   10   11 4970    0    0    0    0    0]
 [   1    1    1    1    0    0    0    4    0 4963   29    0    0    0]
 [  12    1    0    0    0    1    1    2    0   25 4957    0    0    1]
 [   7    0   42    2    0    0    0    0    0    0    0 4930   10    9]
 [  11    2   16    0    0    1    0    1    0    0    0   20 4912   37]
 [  28    6   72    3    2    3    2    0    0    0    2    5   47 4830]]
Figure(640x480)
tensor([1.2385e-07, 3.3094e-01, 1.6359e-06,  ..., 1.3188e-07, 2.9737e-07,
        1.8400e-07])
[[4746   39   22    6   13   47   46    5    0    2    2   12   11   49]
 [  35 4913    5    0    6    0   30    0    3    1    1    0    0    6]
 [  25    7 4761   11   60    2    8    0    0    1    3   37   14   71]
 [   4    1   19 4958   12    0    0    1    0    1    0    2    1    1]
 [   9    8   71   12 4872    5    8    1    1    3    0    0    3    7]
 [  38    0    0    0    1 4952    4    1    1    0    0    0    1    2]
 [  54   36    5    2    8   13 4839   27    7    3    0    1    2    3]
 [   4    1    0    0    0    0   15 4961   12    4    1    0    1    1]
 [   2    0    1    0    4    0   11   10 4971    0    0    0    0    1]
 [   1    1    1    0    0    0    0    5    0 4963   29    0    0    0]
 [  12    1    0    0    0    1    1    2    0   26 4956    0    0    1]
 [   9    0   44    2    0    0    0    0    0    0    0 4923   12   10]
 [  12    2   15    0    0    2    0    0    0    0    0   18 4914   37]
 [  29    5   74    3    3    3    2    0    0    0    2    5   46 4828]]
Figure(640x480)
tensor([1.7666e-07, 1.7253e-03, 3.4227e-07,  ..., 1.5144e-07, 1.4644e-07,
        1.7306e-07])
[[   0    0 4835   71    0    0   94    0    0    0    0    0    0    0]
 [   0    0 4967   10    0    0   23    0    0    0    0    0    0    0]
 [   0    0 4905   92    0    0    3    0    0    0    0    0    0    0]
 [   0    0   56 4943    0    0    1    0    0    0    0    0    0    0]
 [   0    0 4873  121    0    0    6    0    0    0    0    0    0    0]
 [   0    0   45 2725    0    0 2230    0    0    0    0    0    0    0]
 [   0    0   95  101    0    0 4804    0    0    0    0    0    0    0]
 [   0    0   33 4944    0    0   23    0    0    0    0    0    0    0]
 [   0    0   24   11    0    0 4965    0    0    0    0    0    0    0]
 [   0    0 5000    0    0    0    0    0    0    0    0    0    0    0]
 [   0    0 4994    6    0    0    0    0    0    0    0    0    0    0]
 [   0    0   47 4953    0    0    0    0    0    0    0    0    0    0]
 [   0    0   63 4937    0    0    0    0    0    0    0    0    0    0]
 [   0    0 4935   65    0    0    0    0    0    0    0    0    0    0]]
Figure(640x480)
tensor([1.3006, 1.3890, 1.2689,  ..., 1.0835, 1.1482, 1.0433])
[[   0    0    0    0    0    0    0    0    0    0    0    0   56 4944]
 [   0    0    0    0    0    0    0    0    0    0    0    0   11 4989]
 [   0    0    0    0    0    0    0    9    0    0    0    0   13 4978]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1 4999]
 [   0    0    0    0    0    0    0    7    0    0    0    0   15 4978]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2 4998]
 [   0    0    0    0    0    0    0    0    0    0    0    0 4763  237]
 [   0    0    0    0    0    0    0    0    0    0    0    0   14 4986]
 [   0    0    0    0    0    0    0    0    0    0    0    0   10 4990]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0 5000]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0 4999]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0 5000]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0 5000]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1 4999]]
Figure(640x480)
tensor([1.3070, 1.2995, 1.2351,  ..., 0.7001, 0.6819, 0.6987])
