total count words 102019
vocab size 30000
found 26754 words in glove
Load ckpt from ckpt/ag_news_transformeral_l5_pad250_m4//ag_news_transformeral_l5_prefix.pt
model: TransformerModelML(
  (layers): ModuleList(
    (0): EMBLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Embedding(30000, 300)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=4, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=4, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.weight 9000000
layers.0.ae.g.0.weight 512
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 512
layers.0.ae.h.0.bias 4
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
Total Trainable Params: 13672292
Start Training
train_mask {0, 1, 2, 3, 4}
gc 9
Train Epoch0 Acc 0.922825 (110739/120000), AUC 0.9841370582580566
ep0_train_time 116.01613736152649
Test Epoch0 threshold 0.1 Acc 0.9180263157894737, AUC 0.979631781578064, avg_entr 0.0068343402817845345
ep0_t0.1_test_time 0.5421111583709717
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m5//ag_news_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.2 Acc 0.9189473684210526, AUC 0.9810469150543213, avg_entr 0.01239858940243721
ep0_t0.2_test_time 0.4928092956542969
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m5//ag_news_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.3 Acc 0.9190789473684211, AUC 0.9820342063903809, avg_entr 0.02225695550441742
ep0_t0.3_test_time 0.4459869861602783
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m5//ag_news_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.4 Acc 0.92, AUC 0.982136070728302, avg_entr 0.02384517341852188
ep0_t0.4_test_time 0.43254876136779785
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m5//ag_news_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.5 Acc 0.9196052631578947, AUC 0.9820835590362549, avg_entr 0.02484949678182602
ep0_t0.5_test_time 0.4236760139465332
Test Epoch0 threshold 0.6 Acc 0.9193421052631578, AUC 0.9821128845214844, avg_entr 0.025116272270679474
ep0_t0.6_test_time 0.4242863655090332
Test Epoch0 threshold 0.7 Acc 0.9193421052631578, AUC 0.9821128845214844, avg_entr 0.025116272270679474
ep0_t0.7_test_time 0.41860055923461914
Test Epoch0 threshold 0.8 Acc 0.9193421052631578, AUC 0.9821128845214844, avg_entr 0.025116272270679474
ep0_t0.8_test_time 0.41686511039733887
Test Epoch0 threshold 0.9 Acc 0.9193421052631578, AUC 0.9821128845214844, avg_entr 0.025116272270679474
ep0_t0.9_test_time 0.4161062240600586
gc 0
Train Epoch1 Acc 0.958575 (115029/120000), AUC 0.9931541681289673
ep1_train_time 115.46879148483276
Test Epoch1 threshold 0.1 Acc 0.9173684210526316, AUC 0.9786600470542908, avg_entr 0.006890720222145319
ep1_t0.1_test_time 0.5429768562316895
Test Epoch1 threshold 0.2 Acc 0.9175, AUC 0.9801748991012573, avg_entr 0.0127870449796319
ep1_t0.2_test_time 0.4904634952545166
Test Epoch1 threshold 0.3 Acc 0.9186842105263158, AUC 0.9821252822875977, avg_entr 0.023104019463062286
ep1_t0.3_test_time 0.44148802757263184
Test Epoch1 threshold 0.4 Acc 0.9185526315789474, AUC 0.9821056723594666, avg_entr 0.02404511161148548
ep1_t0.4_test_time 0.432192325592041
Test Epoch1 threshold 0.5 Acc 0.9178947368421052, AUC 0.9821092486381531, avg_entr 0.024904999881982803
ep1_t0.5_test_time 0.4205191135406494
Test Epoch1 threshold 0.6 Acc 0.9178947368421052, AUC 0.9820849299430847, avg_entr 0.025300689041614532
ep1_t0.6_test_time 0.4173290729522705
Test Epoch1 threshold 0.7 Acc 0.9178947368421052, AUC 0.9820849299430847, avg_entr 0.025300689041614532
ep1_t0.7_test_time 0.4181361198425293
Test Epoch1 threshold 0.8 Acc 0.9178947368421052, AUC 0.9820849299430847, avg_entr 0.025300689041614532
ep1_t0.8_test_time 0.41788411140441895
Test Epoch1 threshold 0.9 Acc 0.9178947368421052, AUC 0.9820849299430847, avg_entr 0.025300689041614532
ep1_t0.9_test_time 0.41582250595092773
gc 0
Train Epoch2 Acc 0.9612 (115344/120000), AUC 0.9939502477645874
ep2_train_time 115.58908462524414
Test Epoch2 threshold 0.1 Acc 0.9177631578947368, AUC 0.9796741008758545, avg_entr 0.007181380409747362
ep2_t0.1_test_time 0.548043966293335
Test Epoch2 threshold 0.2 Acc 0.9188157894736843, AUC 0.9809471368789673, avg_entr 0.012423471547663212
ep2_t0.2_test_time 0.48800015449523926
Test Epoch2 threshold 0.3 Acc 0.9186842105263158, AUC 0.9820791482925415, avg_entr 0.022378157824277878
ep2_t0.3_test_time 0.45053696632385254
Test Epoch2 threshold 0.4 Acc 0.9190789473684211, AUC 0.9821858406066895, avg_entr 0.02391369454562664
ep2_t0.4_test_time 0.4243340492248535
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m5//ag_news_transformeral_l5_prefix.pt  ,ep 2
Test Epoch2 threshold 0.5 Acc 0.9188157894736843, AUC 0.9821680784225464, avg_entr 0.024617912247776985
ep2_t0.5_test_time 0.42400121688842773
Test Epoch2 threshold 0.6 Acc 0.9185526315789474, AUC 0.982149600982666, avg_entr 0.0248830895870924
ep2_t0.6_test_time 0.4167919158935547
Test Epoch2 threshold 0.7 Acc 0.9185526315789474, AUC 0.982149600982666, avg_entr 0.0248830895870924
ep2_t0.7_test_time 0.41937971115112305
Test Epoch2 threshold 0.8 Acc 0.9185526315789474, AUC 0.982149600982666, avg_entr 0.0248830895870924
ep2_t0.8_test_time 0.4180426597595215
Test Epoch2 threshold 0.9 Acc 0.9185526315789474, AUC 0.982149600982666, avg_entr 0.0248830895870924
ep2_t0.9_test_time 0.4160957336425781
gc 0
Train Epoch3 Acc 0.9616166666666667 (115394/120000), AUC 0.994200587272644
ep3_train_time 115.57793164253235
Test Epoch3 threshold 0.1 Acc 0.9176315789473685, AUC 0.9794162511825562, avg_entr 0.007098725996911526
ep3_t0.1_test_time 0.5361466407775879
Test Epoch3 threshold 0.2 Acc 0.9185526315789474, AUC 0.981074333190918, avg_entr 0.01240786723792553
ep3_t0.2_test_time 0.494473934173584
Test Epoch3 threshold 0.3 Acc 0.9190789473684211, AUC 0.9821882247924805, avg_entr 0.022386305034160614
ep3_t0.3_test_time 0.4498317241668701
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m5//ag_news_transformeral_l5_prefix.pt  ,ep 3
Test Epoch3 threshold 0.4 Acc 0.9193421052631578, AUC 0.9821984767913818, avg_entr 0.02370082587003708
ep3_t0.4_test_time 0.4405992031097412
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m5//ag_news_transformeral_l5_prefix.pt  ,ep 3
Test Epoch3 threshold 0.5 Acc 0.9192105263157895, AUC 0.9821491837501526, avg_entr 0.024752575904130936
ep3_t0.5_test_time 0.42386364936828613
Test Epoch3 threshold 0.6 Acc 0.9189473684210526, AUC 0.9821626543998718, avg_entr 0.024971166625618935
ep3_t0.6_test_time 0.41818976402282715
Test Epoch3 threshold 0.7 Acc 0.9189473684210526, AUC 0.9821626543998718, avg_entr 0.024971166625618935
ep3_t0.7_test_time 0.41870856285095215
Test Epoch3 threshold 0.8 Acc 0.9189473684210526, AUC 0.9821626543998718, avg_entr 0.024971166625618935
ep3_t0.8_test_time 0.41774439811706543
Test Epoch3 threshold 0.9 Acc 0.9189473684210526, AUC 0.9821626543998718, avg_entr 0.024971166625618935
ep3_t0.9_test_time 0.41750121116638184
gc 0
Train Epoch4 Acc 0.96185 (115422/120000), AUC 0.9943045377731323
ep4_train_time 115.5931122303009
Test Epoch4 threshold 0.1 Acc 0.9175, AUC 0.979308545589447, avg_entr 0.007095337845385075
ep4_t0.1_test_time 0.5395615100860596
Test Epoch4 threshold 0.2 Acc 0.9181578947368421, AUC 0.9810333251953125, avg_entr 0.012501450255513191
ep4_t0.2_test_time 0.49545931816101074
Test Epoch4 threshold 0.3 Acc 0.9192105263157895, AUC 0.9821771383285522, avg_entr 0.02237563021481037
ep4_t0.3_test_time 0.44514036178588867
Test Epoch4 threshold 0.4 Acc 0.9194736842105263, AUC 0.9822021126747131, avg_entr 0.02378375269472599
ep4_t0.4_test_time 0.4274125099182129
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m5//ag_news_transformeral_l5_prefix.pt  ,ep 4
Test Epoch4 threshold 0.5 Acc 0.9193421052631578, AUC 0.9821583032608032, avg_entr 0.024654436856508255
ep4_t0.5_test_time 0.4242572784423828
Test Epoch4 threshold 0.6 Acc 0.9189473684210526, AUC 0.9821611046791077, avg_entr 0.024999232962727547
ep4_t0.6_test_time 0.418140172958374
Test Epoch4 threshold 0.7 Acc 0.9189473684210526, AUC 0.9821611046791077, avg_entr 0.024999232962727547
ep4_t0.7_test_time 0.4193711280822754
Test Epoch4 threshold 0.8 Acc 0.9189473684210526, AUC 0.9821611046791077, avg_entr 0.024999232962727547
ep4_t0.8_test_time 0.4176521301269531
Test Epoch4 threshold 0.9 Acc 0.9189473684210526, AUC 0.9821611046791077, avg_entr 0.024999232962727547
ep4_t0.9_test_time 0.4188046455383301
gc 0
Train Epoch5 Acc 0.9618916666666667 (115427/120000), AUC 0.9942901134490967
ep5_train_time 115.48496103286743
Test Epoch5 threshold 0.1 Acc 0.9175, AUC 0.9794169664382935, avg_entr 0.007181006949394941
ep5_t0.1_test_time 0.5411076545715332
Test Epoch5 threshold 0.2 Acc 0.9182894736842105, AUC 0.9810739755630493, avg_entr 0.01247275061905384
ep5_t0.2_test_time 0.49811887741088867
Test Epoch5 threshold 0.3 Acc 0.9189473684210526, AUC 0.9821842908859253, avg_entr 0.022361474111676216
ep5_t0.3_test_time 0.44733762741088867
Test Epoch5 threshold 0.4 Acc 0.9194736842105263, AUC 0.9821996688842773, avg_entr 0.02370544523000717
ep5_t0.4_test_time 0.4289438724517822
Test Epoch5 threshold 0.5 Acc 0.9193421052631578, AUC 0.9821566939353943, avg_entr 0.024687640368938446
ep5_t0.5_test_time 0.4198801517486572
Test Epoch5 threshold 0.6 Acc 0.9190789473684211, AUC 0.9821639060974121, avg_entr 0.02496924437582493
ep5_t0.6_test_time 0.41814398765563965
Test Epoch5 threshold 0.7 Acc 0.9190789473684211, AUC 0.9821639060974121, avg_entr 0.02496924437582493
ep5_t0.7_test_time 0.4195234775543213
Test Epoch5 threshold 0.8 Acc 0.9190789473684211, AUC 0.9821639060974121, avg_entr 0.02496924437582493
ep5_t0.8_test_time 0.41765475273132324
Test Epoch5 threshold 0.9 Acc 0.9190789473684211, AUC 0.9821639060974121, avg_entr 0.02496924437582493
ep5_t0.9_test_time 0.41751837730407715
gc 0
Train Epoch6 Acc 0.9619666666666666 (115436/120000), AUC 0.9942153096199036
ep6_train_time 115.68245601654053
Test Epoch6 threshold 0.1 Acc 0.9175, AUC 0.9794241189956665, avg_entr 0.007173406425863504
ep6_t0.1_test_time 0.5384933948516846
Test Epoch6 threshold 0.2 Acc 0.9182894736842105, AUC 0.9810479283332825, avg_entr 0.01243333425372839
ep6_t0.2_test_time 0.4952871799468994
Test Epoch6 threshold 0.3 Acc 0.9192105263157895, AUC 0.982184648513794, avg_entr 0.022372668609023094
ep6_t0.3_test_time 0.4453279972076416
Test Epoch6 threshold 0.4 Acc 0.9196052631578947, AUC 0.982200026512146, avg_entr 0.02371922880411148
ep6_t0.4_test_time 0.42708897590637207
Test Epoch6 threshold 0.5 Acc 0.9194736842105263, AUC 0.9821571707725525, avg_entr 0.024702636525034904
ep6_t0.5_test_time 0.4185616970062256
Test Epoch6 threshold 0.6 Acc 0.9192105263157895, AUC 0.9821643233299255, avg_entr 0.024984298273921013
ep6_t0.6_test_time 0.4161977767944336
Test Epoch6 threshold 0.7 Acc 0.9192105263157895, AUC 0.9821643233299255, avg_entr 0.024984298273921013
ep6_t0.7_test_time 0.4187161922454834
Test Epoch6 threshold 0.8 Acc 0.9192105263157895, AUC 0.9821643233299255, avg_entr 0.024984298273921013
ep6_t0.8_test_time 0.41681480407714844
Test Epoch6 threshold 0.9 Acc 0.9192105263157895, AUC 0.9821643233299255, avg_entr 0.024984298273921013
ep6_t0.9_test_time 0.4170711040496826
gc 0
Train Epoch7 Acc 0.9619833333333333 (115438/120000), AUC 0.9943045377731323
ep7_train_time 115.82045769691467
Test Epoch7 threshold 0.1 Acc 0.9175, AUC 0.9794236421585083, avg_entr 0.007173170801252127
ep7_t0.1_test_time 0.5403454303741455
Test Epoch7 threshold 0.2 Acc 0.9182894736842105, AUC 0.9810479879379272, avg_entr 0.012433158233761787
ep7_t0.2_test_time 0.4970996379852295
Test Epoch7 threshold 0.3 Acc 0.9192105263157895, AUC 0.9821851849555969, avg_entr 0.022373082116246223
ep7_t0.3_test_time 0.4468531608581543
Test Epoch7 threshold 0.4 Acc 0.9196052631578947, AUC 0.9822005033493042, avg_entr 0.02372012846171856
ep7_t0.4_test_time 0.4285764694213867
Test Epoch7 threshold 0.5 Acc 0.9194736842105263, AUC 0.9821576476097107, avg_entr 0.02470354735851288
ep7_t0.5_test_time 0.4216880798339844
Test Epoch7 threshold 0.6 Acc 0.9192105263157895, AUC 0.9821648597717285, avg_entr 0.024985216557979584
ep7_t0.6_test_time 0.4186437129974365
Test Epoch7 threshold 0.7 Acc 0.9192105263157895, AUC 0.9821648597717285, avg_entr 0.024985216557979584
ep7_t0.7_test_time 0.42059922218322754
Test Epoch7 threshold 0.8 Acc 0.9192105263157895, AUC 0.9821648597717285, avg_entr 0.024985216557979584
ep7_t0.8_test_time 0.4194481372833252
Test Epoch7 threshold 0.9 Acc 0.9192105263157895, AUC 0.9821648597717285, avg_entr 0.024985216557979584
ep7_t0.9_test_time 0.4178459644317627
gc 0
Train Epoch8 Acc 0.9621666666666666 (115460/120000), AUC 0.9944226145744324
ep8_train_time 115.45953226089478
Test Epoch8 threshold 0.1 Acc 0.9175, AUC 0.9794157147407532, avg_entr 0.007173294201493263
ep8_t0.1_test_time 0.5405540466308594
Test Epoch8 threshold 0.2 Acc 0.9182894736842105, AUC 0.981040358543396, avg_entr 0.012433858588337898
ep8_t0.2_test_time 0.49928760528564453
Test Epoch8 threshold 0.3 Acc 0.9192105263157895, AUC 0.9821781516075134, avg_entr 0.022373255342245102
ep8_t0.3_test_time 0.4474506378173828
Test Epoch8 threshold 0.4 Acc 0.9196052631578947, AUC 0.9821934700012207, avg_entr 0.023720301687717438
ep8_t0.4_test_time 0.4294130802154541
Test Epoch8 threshold 0.5 Acc 0.9194736842105263, AUC 0.982150673866272, avg_entr 0.02470381371676922
ep8_t0.5_test_time 0.42193603515625
Test Epoch8 threshold 0.6 Acc 0.9192105263157895, AUC 0.982157826423645, avg_entr 0.024985482916235924
ep8_t0.6_test_time 0.41888880729675293
Test Epoch8 threshold 0.7 Acc 0.9192105263157895, AUC 0.982157826423645, avg_entr 0.024985482916235924
ep8_t0.7_test_time 0.42131781578063965
Test Epoch8 threshold 0.8 Acc 0.9192105263157895, AUC 0.982157826423645, avg_entr 0.024985482916235924
ep8_t0.8_test_time 0.41979408264160156
Test Epoch8 threshold 0.9 Acc 0.9192105263157895, AUC 0.982157826423645, avg_entr 0.024985482916235924
ep8_t0.9_test_time 0.4204840660095215
gc 0
Train Epoch9 Acc 0.961925 (115431/120000), AUC 0.9944003820419312
ep9_train_time 115.62859392166138
Test Epoch9 threshold 0.1 Acc 0.9175, AUC 0.9794154167175293, avg_entr 0.007173571735620499
ep9_t0.1_test_time 0.5414557456970215
Test Epoch9 threshold 0.2 Acc 0.9182894736842105, AUC 0.9810402989387512, avg_entr 0.012434091418981552
ep9_t0.2_test_time 0.49738454818725586
Test Epoch9 threshold 0.3 Acc 0.9192105263157895, AUC 0.9821782112121582, avg_entr 0.02237335778772831
ep9_t0.3_test_time 0.4472935199737549
Test Epoch9 threshold 0.4 Acc 0.9196052631578947, AUC 0.9821935296058655, avg_entr 0.02372038923203945
ep9_t0.4_test_time 0.4291725158691406
Test Epoch9 threshold 0.5 Acc 0.9194736842105263, AUC 0.9821507334709167, avg_entr 0.02470395155251026
ep9_t0.5_test_time 0.4215240478515625
Test Epoch9 threshold 0.6 Acc 0.9192105263157895, AUC 0.9821578860282898, avg_entr 0.024985628202557564
ep9_t0.6_test_time 0.4181556701660156
Test Epoch9 threshold 0.7 Acc 0.9192105263157895, AUC 0.9821578860282898, avg_entr 0.024985628202557564
ep9_t0.7_test_time 0.41991639137268066
Test Epoch9 threshold 0.8 Acc 0.9192105263157895, AUC 0.9821578860282898, avg_entr 0.024985628202557564
ep9_t0.8_test_time 0.418656587600708
Test Epoch9 threshold 0.9 Acc 0.9192105263157895, AUC 0.9821578860282898, avg_entr 0.024985628202557564
ep9_t0.9_test_time 0.418595552444458
Best AUC 0.9822021126747131
train_loss (2, 5, 10)
valid_acc (10, 9)
valid_AUC (10, 9)
train_acc (10,)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Start Testing
Load ckpt at ckpt/ag_news_transformeral_l5_pad250_m5//ag_news_transformeral_l5_prefix.pt
[[1703   61   88   48]
 [  11 1872    7   10]
 [  45   20 1686  149]
 [  39   15  123 1723]]
Figure(640x480)
tensor([7.4776e-04, 1.7898e-08, 2.6613e-03,  ..., 3.5325e-02, 7.8475e-08,
        3.3462e-03])
[[1712   53   74   61]
 [  19 1856   13   12]
 [  55   18 1680  147]
 [  46   13  134 1707]]
Figure(640x480)
tensor([1.4233e-06, 2.7412e-08, 1.7029e-08,  ..., 3.2155e-08, 2.3977e-08,
        2.5267e-08])
[[1712   52   74   62]
 [  22 1852   13   13]
 [  56   16 1682  146]
 [  46   14  137 1703]]
Figure(640x480)
tensor([5.0757e-07, 2.9679e-08, 2.6719e-08,  ..., 1.5841e-07, 2.6665e-08,
        3.0589e-08])
[[1715   51   73   61]
 [  22 1851   14   13]
 [  56   17 1681  146]
 [  47   12  140 1701]]
Figure(640x480)
tensor([2.1323e-07, 2.7710e-08, 2.8015e-08,  ..., 1.6318e-07, 2.6975e-08,
        3.1552e-08])
[[1714   51   76   59]
 [  22 1852   13   13]
 [  56   17 1682  145]
 [  47   14  139 1700]]
Figure(640x480)
tensor([1.9578e-07, 2.4066e-08, 2.7089e-08,  ..., 1.5952e-07, 2.7679e-08,
        3.1277e-08])
