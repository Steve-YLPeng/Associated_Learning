total count words 887881
vocab size 30000
found 28354 words in glove
model: TransformerModelML(
  (layers): ModuleList(
    (0): EMBLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Embedding(30000, 300)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=14, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=14, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.weight 9000000
layers.0.ae.g.0.weight 1792
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 1792
layers.0.ae.h.0.bias 14
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
Total Trainable Params: 13674862
Start Training
train_mask {0}
gc 0
Train Epoch0 Acc 0.13910714285714285 (77900/560000), AUC 0.5586963295936584
ep0_train_time 85.38997769355774
Test Epoch0 threshold 0.1 Acc 0.9736142857142858, AUC 0.9982348084449768, avg_entr 0.03203842043876648
ep0_t0.1_test_time 1.4735941886901855
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m1//dbpedia_14_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.2 Acc 0.9736142857142858, AUC 0.9982348084449768, avg_entr 0.03203842043876648
ep0_t0.2_test_time 1.44919753074646
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m1//dbpedia_14_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.3 Acc 0.9736142857142858, AUC 0.9982348084449768, avg_entr 0.03203842043876648
ep0_t0.3_test_time 1.493194818496704
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m1//dbpedia_14_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.4 Acc 0.9736142857142858, AUC 0.9982348084449768, avg_entr 0.03203842043876648
ep0_t0.4_test_time 1.4473824501037598
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m1//dbpedia_14_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.5 Acc 0.9736142857142858, AUC 0.9982348084449768, avg_entr 0.03203842043876648
ep0_t0.5_test_time 1.450507640838623
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m1//dbpedia_14_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.6 Acc 0.9736142857142858, AUC 0.9982348084449768, avg_entr 0.03203842043876648
ep0_t0.6_test_time 1.4490962028503418
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m1//dbpedia_14_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.7 Acc 0.9736142857142858, AUC 0.9982348084449768, avg_entr 0.03203842043876648
ep0_t0.7_test_time 1.4829213619232178
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m1//dbpedia_14_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.8 Acc 0.9736142857142858, AUC 0.9982348084449768, avg_entr 0.03203842043876648
ep0_t0.8_test_time 1.4452545642852783
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m1//dbpedia_14_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.9 Acc 0.9736142857142858, AUC 0.9982348084449768, avg_entr 0.03203842043876648
ep0_t0.9_test_time 1.4599664211273193
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m1//dbpedia_14_transformeral_l5_prefix.pt  ,ep 0
gc 0
Train Epoch1 Acc 0.1441107142857143 (80702/560000), AUC 0.5607596635818481
ep1_train_time 83.71903109550476
Test Epoch1 threshold 0.1 Acc 0.9739428571428571, AUC 0.998295247554779, avg_entr 0.022801939398050308
ep1_t0.1_test_time 1.4756731986999512
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m1//dbpedia_14_transformeral_l5_prefix.pt  ,ep 1
Test Epoch1 threshold 0.2 Acc 0.9739428571428571, AUC 0.998295247554779, avg_entr 0.022801939398050308
ep1_t0.2_test_time 1.4703984260559082
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m1//dbpedia_14_transformeral_l5_prefix.pt  ,ep 1
Test Epoch1 threshold 0.3 Acc 0.9739428571428571, AUC 0.998295247554779, avg_entr 0.022801939398050308
ep1_t0.3_test_time 1.4579732418060303
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m1//dbpedia_14_transformeral_l5_prefix.pt  ,ep 1
Test Epoch1 threshold 0.4 Acc 0.9739428571428571, AUC 0.998295247554779, avg_entr 0.022801939398050308
ep1_t0.4_test_time 1.4589147567749023
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m1//dbpedia_14_transformeral_l5_prefix.pt  ,ep 1
Test Epoch1 threshold 0.5 Acc 0.9739428571428571, AUC 0.998295247554779, avg_entr 0.022801939398050308
ep1_t0.5_test_time 1.4477338790893555
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m1//dbpedia_14_transformeral_l5_prefix.pt  ,ep 1
Test Epoch1 threshold 0.6 Acc 0.9739428571428571, AUC 0.998295247554779, avg_entr 0.022801939398050308
ep1_t0.6_test_time 1.4506721496582031
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m1//dbpedia_14_transformeral_l5_prefix.pt  ,ep 1
Test Epoch1 threshold 0.7 Acc 0.9739428571428571, AUC 0.998295247554779, avg_entr 0.022801939398050308
ep1_t0.7_test_time 1.4511969089508057
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m1//dbpedia_14_transformeral_l5_prefix.pt  ,ep 1
Test Epoch1 threshold 0.8 Acc 0.9739428571428571, AUC 0.998295247554779, avg_entr 0.022801939398050308
ep1_t0.8_test_time 1.4525320529937744
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m1//dbpedia_14_transformeral_l5_prefix.pt  ,ep 1
Test Epoch1 threshold 0.9 Acc 0.9739428571428571, AUC 0.998295247554779, avg_entr 0.022801939398050308
ep1_t0.9_test_time 1.4602046012878418
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m1//dbpedia_14_transformeral_l5_prefix.pt  ,ep 1
gc 0
Train Epoch2 Acc 0.1439517857142857 (80613/560000), AUC 0.5595128536224365
ep2_train_time 83.70618510246277
Test Epoch2 threshold 0.1 Acc 0.9739571428571429, AUC 0.9983102083206177, avg_entr 0.019230900332331657
ep2_t0.1_test_time 1.450620412826538
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m1//dbpedia_14_transformeral_l5_prefix.pt  ,ep 2
Test Epoch2 threshold 0.2 Acc 0.9739571428571429, AUC 0.9983102083206177, avg_entr 0.019230900332331657
ep2_t0.2_test_time 1.451941728591919
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m1//dbpedia_14_transformeral_l5_prefix.pt  ,ep 2
Test Epoch2 threshold 0.3 Acc 0.9739571428571429, AUC 0.9983102083206177, avg_entr 0.019230900332331657
ep2_t0.3_test_time 1.4609026908874512
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m1//dbpedia_14_transformeral_l5_prefix.pt  ,ep 2
Test Epoch2 threshold 0.4 Acc 0.9739571428571429, AUC 0.9983102083206177, avg_entr 0.019230900332331657
ep2_t0.4_test_time 1.4642078876495361
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m1//dbpedia_14_transformeral_l5_prefix.pt  ,ep 2
Test Epoch2 threshold 0.5 Acc 0.9739571428571429, AUC 0.9983102083206177, avg_entr 0.019230900332331657
ep2_t0.5_test_time 1.4563653469085693
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m1//dbpedia_14_transformeral_l5_prefix.pt  ,ep 2
Test Epoch2 threshold 0.6 Acc 0.9739571428571429, AUC 0.9983102083206177, avg_entr 0.019230900332331657
ep2_t0.6_test_time 1.4555535316467285
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m1//dbpedia_14_transformeral_l5_prefix.pt  ,ep 2
Test Epoch2 threshold 0.7 Acc 0.9739571428571429, AUC 0.9983102083206177, avg_entr 0.019230900332331657
ep2_t0.7_test_time 1.4482109546661377
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m1//dbpedia_14_transformeral_l5_prefix.pt  ,ep 2
Test Epoch2 threshold 0.8 Acc 0.9739571428571429, AUC 0.9983102083206177, avg_entr 0.019230900332331657
ep2_t0.8_test_time 1.4426050186157227
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m1//dbpedia_14_transformeral_l5_prefix.pt  ,ep 2
Test Epoch2 threshold 0.9 Acc 0.9739571428571429, AUC 0.9983102083206177, avg_entr 0.019230900332331657
ep2_t0.9_test_time 1.4648475646972656
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m1//dbpedia_14_transformeral_l5_prefix.pt  ,ep 2
gc 0
Train Epoch3 Acc 0.1435 (80360/560000), AUC 0.5589922070503235
ep3_train_time 83.65101909637451
Test Epoch3 threshold 0.1 Acc 0.9741428571428571, AUC 0.9983177781105042, avg_entr 0.017567912116646767
ep3_t0.1_test_time 1.4687869548797607
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m1//dbpedia_14_transformeral_l5_prefix.pt  ,ep 3
Test Epoch3 threshold 0.2 Acc 0.9741428571428571, AUC 0.9983177781105042, avg_entr 0.017567912116646767
ep3_t0.2_test_time 1.4726970195770264
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m1//dbpedia_14_transformeral_l5_prefix.pt  ,ep 3
Test Epoch3 threshold 0.3 Acc 0.9741428571428571, AUC 0.9983177781105042, avg_entr 0.017567912116646767
ep3_t0.3_test_time 1.4620020389556885
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m1//dbpedia_14_transformeral_l5_prefix.pt  ,ep 3
Test Epoch3 threshold 0.4 Acc 0.9741428571428571, AUC 0.9983177781105042, avg_entr 0.017567912116646767
ep3_t0.4_test_time 1.450577974319458
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m1//dbpedia_14_transformeral_l5_prefix.pt  ,ep 3
Test Epoch3 threshold 0.5 Acc 0.9741428571428571, AUC 0.9983177781105042, avg_entr 0.017567912116646767
ep3_t0.5_test_time 1.4730322360992432
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m1//dbpedia_14_transformeral_l5_prefix.pt  ,ep 3
Test Epoch3 threshold 0.6 Acc 0.9741428571428571, AUC 0.9983177781105042, avg_entr 0.017567912116646767
ep3_t0.6_test_time 1.4639554023742676
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m1//dbpedia_14_transformeral_l5_prefix.pt  ,ep 3
Test Epoch3 threshold 0.7 Acc 0.9741428571428571, AUC 0.9983177781105042, avg_entr 0.017567912116646767
ep3_t0.7_test_time 1.4766931533813477
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m1//dbpedia_14_transformeral_l5_prefix.pt  ,ep 3
Test Epoch3 threshold 0.8 Acc 0.9741428571428571, AUC 0.9983177781105042, avg_entr 0.017567912116646767
ep3_t0.8_test_time 1.453566551208496
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m1//dbpedia_14_transformeral_l5_prefix.pt  ,ep 3
Test Epoch3 threshold 0.9 Acc 0.9741428571428571, AUC 0.9983177781105042, avg_entr 0.017567912116646767
ep3_t0.9_test_time 1.4612796306610107
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m1//dbpedia_14_transformeral_l5_prefix.pt  ,ep 3
gc 0
Train Epoch4 Acc 0.1431892857142857 (80186/560000), AUC 0.5587823987007141
ep4_train_time 83.62592506408691
Test Epoch4 threshold 0.1 Acc 0.9741857142857143, AUC 0.9983198046684265, avg_entr 0.016944654285907745
ep4_t0.1_test_time 1.4584155082702637
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m1//dbpedia_14_transformeral_l5_prefix.pt  ,ep 4
Test Epoch4 threshold 0.2 Acc 0.9741857142857143, AUC 0.9983198046684265, avg_entr 0.016944654285907745
ep4_t0.2_test_time 1.4560561180114746
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m1//dbpedia_14_transformeral_l5_prefix.pt  ,ep 4
Test Epoch4 threshold 0.3 Acc 0.9741857142857143, AUC 0.9983198046684265, avg_entr 0.016944654285907745
ep4_t0.3_test_time 1.456024169921875
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m1//dbpedia_14_transformeral_l5_prefix.pt  ,ep 4
Test Epoch4 threshold 0.4 Acc 0.9741857142857143, AUC 0.9983198046684265, avg_entr 0.016944654285907745
ep4_t0.4_test_time 1.461421012878418
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m1//dbpedia_14_transformeral_l5_prefix.pt  ,ep 4
Test Epoch4 threshold 0.5 Acc 0.9741857142857143, AUC 0.9983198046684265, avg_entr 0.016944654285907745
ep4_t0.5_test_time 1.4840681552886963
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m1//dbpedia_14_transformeral_l5_prefix.pt  ,ep 4
Test Epoch4 threshold 0.6 Acc 0.9741857142857143, AUC 0.9983198046684265, avg_entr 0.016944654285907745
ep4_t0.6_test_time 1.463777780532837
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m1//dbpedia_14_transformeral_l5_prefix.pt  ,ep 4
Test Epoch4 threshold 0.7 Acc 0.9741857142857143, AUC 0.9983198046684265, avg_entr 0.016944654285907745
ep4_t0.7_test_time 1.4529166221618652
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m1//dbpedia_14_transformeral_l5_prefix.pt  ,ep 4
Test Epoch4 threshold 0.8 Acc 0.9741857142857143, AUC 0.9983198046684265, avg_entr 0.016944654285907745
ep4_t0.8_test_time 1.4472341537475586
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m1//dbpedia_14_transformeral_l5_prefix.pt  ,ep 4
Test Epoch4 threshold 0.9 Acc 0.9741857142857143, AUC 0.9983198046684265, avg_entr 0.016944654285907745
ep4_t0.9_test_time 1.4546165466308594
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m1//dbpedia_14_transformeral_l5_prefix.pt  ,ep 4
gc 0
Train Epoch5 Acc 0.14306785714285714 (80118/560000), AUC 0.5586567521095276
ep5_train_time 83.66985988616943
Test Epoch5 threshold 0.1 Acc 0.9741142857142857, AUC 0.9983190894126892, avg_entr 0.016878299415111542
ep5_t0.1_test_time 1.4606211185455322
Test Epoch5 threshold 0.2 Acc 0.9741142857142857, AUC 0.9983190894126892, avg_entr 0.016878299415111542
ep5_t0.2_test_time 1.4715580940246582
Test Epoch5 threshold 0.3 Acc 0.9741142857142857, AUC 0.9983190894126892, avg_entr 0.016878299415111542
ep5_t0.3_test_time 1.457061767578125
Test Epoch5 threshold 0.4 Acc 0.9741142857142857, AUC 0.9983190894126892, avg_entr 0.016878299415111542
ep5_t0.4_test_time 1.4498271942138672
Test Epoch5 threshold 0.5 Acc 0.9741142857142857, AUC 0.9983190894126892, avg_entr 0.016878299415111542
ep5_t0.5_test_time 1.451650857925415
Test Epoch5 threshold 0.6 Acc 0.9741142857142857, AUC 0.9983190894126892, avg_entr 0.016878299415111542
ep5_t0.6_test_time 1.4799137115478516
Test Epoch5 threshold 0.7 Acc 0.9741142857142857, AUC 0.9983190894126892, avg_entr 0.016878299415111542
ep5_t0.7_test_time 1.4511950016021729
Test Epoch5 threshold 0.8 Acc 0.9741142857142857, AUC 0.9983190894126892, avg_entr 0.016878299415111542
ep5_t0.8_test_time 1.460395336151123
Test Epoch5 threshold 0.9 Acc 0.9741142857142857, AUC 0.9983190894126892, avg_entr 0.016878299415111542
ep5_t0.9_test_time 1.454620599746704
gc 0
Train Epoch6 Acc 0.14305535714285714 (80111/560000), AUC 0.5586317777633667
ep6_train_time 83.76350450515747
Test Epoch6 threshold 0.1 Acc 0.9741285714285715, AUC 0.9983192086219788, avg_entr 0.016853105276823044
ep6_t0.1_test_time 1.460343599319458
Test Epoch6 threshold 0.2 Acc 0.9741285714285715, AUC 0.9983192086219788, avg_entr 0.016853105276823044
ep6_t0.2_test_time 1.469477891921997
Test Epoch6 threshold 0.3 Acc 0.9741285714285715, AUC 0.9983192086219788, avg_entr 0.016853105276823044
ep6_t0.3_test_time 1.455611228942871
Test Epoch6 threshold 0.4 Acc 0.9741285714285715, AUC 0.9983192086219788, avg_entr 0.016853105276823044
ep6_t0.4_test_time 1.4492297172546387
Test Epoch6 threshold 0.5 Acc 0.9741285714285715, AUC 0.9983192086219788, avg_entr 0.016853105276823044
ep6_t0.5_test_time 1.4505808353424072
Test Epoch6 threshold 0.6 Acc 0.9741285714285715, AUC 0.9983192086219788, avg_entr 0.016853105276823044
ep6_t0.6_test_time 1.4699692726135254
Test Epoch6 threshold 0.7 Acc 0.9741285714285715, AUC 0.9983192086219788, avg_entr 0.016853105276823044
ep6_t0.7_test_time 1.4732177257537842
Test Epoch6 threshold 0.8 Acc 0.9741285714285715, AUC 0.9983192086219788, avg_entr 0.016853105276823044
ep6_t0.8_test_time 1.4544751644134521
Test Epoch6 threshold 0.9 Acc 0.9741285714285715, AUC 0.9983192086219788, avg_entr 0.016853105276823044
ep6_t0.9_test_time 1.466712474822998
gc 0
Train Epoch7 Acc 0.1430357142857143 (80100/560000), AUC 0.5586269497871399
ep7_train_time 83.91810894012451
Test Epoch7 threshold 0.1 Acc 0.9741285714285715, AUC 0.9983193278312683, avg_entr 0.016851091757416725
ep7_t0.1_test_time 1.4496569633483887
Test Epoch7 threshold 0.2 Acc 0.9741285714285715, AUC 0.9983193278312683, avg_entr 0.016851091757416725
ep7_t0.2_test_time 1.4577298164367676
Test Epoch7 threshold 0.3 Acc 0.9741285714285715, AUC 0.9983193278312683, avg_entr 0.016851091757416725
ep7_t0.3_test_time 1.452768087387085
Test Epoch7 threshold 0.4 Acc 0.9741285714285715, AUC 0.9983193278312683, avg_entr 0.016851091757416725
ep7_t0.4_test_time 1.4601712226867676
Test Epoch7 threshold 0.5 Acc 0.9741285714285715, AUC 0.9983193278312683, avg_entr 0.016851091757416725
ep7_t0.5_test_time 1.4547014236450195
Test Epoch7 threshold 0.6 Acc 0.9741285714285715, AUC 0.9983193278312683, avg_entr 0.016851091757416725
ep7_t0.6_test_time 1.451784610748291
Test Epoch7 threshold 0.7 Acc 0.9741285714285715, AUC 0.9983193278312683, avg_entr 0.016851091757416725
ep7_t0.7_test_time 1.4667577743530273
Test Epoch7 threshold 0.8 Acc 0.9741285714285715, AUC 0.9983193278312683, avg_entr 0.016851091757416725
ep7_t0.8_test_time 1.4522745609283447
Test Epoch7 threshold 0.9 Acc 0.9741285714285715, AUC 0.9983193278312683, avg_entr 0.016851091757416725
ep7_t0.9_test_time 1.4684498310089111
gc 0
Train Epoch8 Acc 0.14303392857142858 (80099/560000), AUC 0.5586267709732056
ep8_train_time 83.73259043693542
Test Epoch8 threshold 0.1 Acc 0.9741285714285715, AUC 0.9983193278312683, avg_entr 0.01685086451470852
ep8_t0.1_test_time 1.4635770320892334
Test Epoch8 threshold 0.2 Acc 0.9741285714285715, AUC 0.9983193278312683, avg_entr 0.01685086451470852
ep8_t0.2_test_time 1.4496612548828125
Test Epoch8 threshold 0.3 Acc 0.9741285714285715, AUC 0.9983193278312683, avg_entr 0.01685086451470852
ep8_t0.3_test_time 1.449718952178955
Test Epoch8 threshold 0.4 Acc 0.9741285714285715, AUC 0.9983193278312683, avg_entr 0.01685086451470852
ep8_t0.4_test_time 1.4611361026763916
Test Epoch8 threshold 0.5 Acc 0.9741285714285715, AUC 0.9983193278312683, avg_entr 0.01685086451470852
ep8_t0.5_test_time 1.4525701999664307
Test Epoch8 threshold 0.6 Acc 0.9741285714285715, AUC 0.9983193278312683, avg_entr 0.01685086451470852
ep8_t0.6_test_time 1.4708693027496338
Test Epoch8 threshold 0.7 Acc 0.9741285714285715, AUC 0.9983193278312683, avg_entr 0.01685086451470852
ep8_t0.7_test_time 1.459402084350586
Test Epoch8 threshold 0.8 Acc 0.9741285714285715, AUC 0.9983193278312683, avg_entr 0.01685086451470852
ep8_t0.8_test_time 1.4536197185516357
Test Epoch8 threshold 0.9 Acc 0.9741285714285715, AUC 0.9983193278312683, avg_entr 0.01685086451470852
ep8_t0.9_test_time 1.4467902183532715
gc 0
Train Epoch9 Acc 0.14303214285714286 (80098/560000), AUC 0.5586267113685608
ep9_train_time 83.84413814544678
Test Epoch9 threshold 0.1 Acc 0.9741285714285715, AUC 0.9983193278312683, avg_entr 0.016850607469677925
ep9_t0.1_test_time 1.4514806270599365
Test Epoch9 threshold 0.2 Acc 0.9741285714285715, AUC 0.9983193278312683, avg_entr 0.016850607469677925
ep9_t0.2_test_time 1.4650592803955078
Test Epoch9 threshold 0.3 Acc 0.9741285714285715, AUC 0.9983193278312683, avg_entr 0.016850607469677925
ep9_t0.3_test_time 1.4539613723754883
Test Epoch9 threshold 0.4 Acc 0.9741285714285715, AUC 0.9983193278312683, avg_entr 0.016850607469677925
ep9_t0.4_test_time 1.4435174465179443
Test Epoch9 threshold 0.5 Acc 0.9741285714285715, AUC 0.9983193278312683, avg_entr 0.016850607469677925
ep9_t0.5_test_time 1.458115577697754
Test Epoch9 threshold 0.6 Acc 0.9741285714285715, AUC 0.9983193278312683, avg_entr 0.016850607469677925
ep9_t0.6_test_time 1.4543733596801758
Test Epoch9 threshold 0.7 Acc 0.9741285714285715, AUC 0.9983193278312683, avg_entr 0.016850607469677925
ep9_t0.7_test_time 1.4779658317565918
Test Epoch9 threshold 0.8 Acc 0.9741285714285715, AUC 0.9983193278312683, avg_entr 0.016850607469677925
ep9_t0.8_test_time 1.4512970447540283
Test Epoch9 threshold 0.9 Acc 0.9741285714285715, AUC 0.9983193278312683, avg_entr 0.016850607469677925
ep9_t0.9_test_time 1.4626033306121826
Best AUC 0.9983198046684265
train_loss (2, 5, 10)
valid_acc (10, 9)
valid_AUC (10, 9)
train_acc (10,)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Start Testing
Load ckpt at ckpt/dbpedia_14_transformeral_l5_pad60_m1//dbpedia_14_transformeral_l5_prefix.pt
[[4705   40   22   13   12   69   37    7    4    5    6   18   15   47]
 [  34 4901    5    1    7    0   32    5    3    1    0    0    3    8]
 [  36   13 4614   16   55    2    9    4    4    2    1   80   22  142]
 [   3    2   29 4951   10    0    0    1    0    1    0    1    0    2]
 [  10   22   74   13 4851    8    6    1    2    0    1    1    2    9]
 [  35    1    0    1    2 4943    8    5    0    3    1    0    0    1]
 [  63   45    4    2   10   16 4798   36   13    5    0    1    3    4]
 [   1    1    0    0    2    0   13 4961   17    3    1    0    0    1]
 [   0    3    2    0    2    0    9   14 4968    1    0    0    0    1]
 [   1    0    1    1    0    0    0    8    0 4953   35    0    0    1]
 [  10    1    0    0    0    2    2    5    1   35 4944    0    0    0]
 [   6    0   39    2    0    0    2    0    0    0    1 4923   15   12]
 [   9    1   24    6    0    2    1    2    2    2    0   23 4881   47]
 [  28    6   79    6   13    3    6    6    2    2    2    7   40 4800]]
Figure(640x480)
tensor([9.8594e-04, 7.1372e-01, 2.6894e-03,  ..., 4.8748e-04, 2.2678e-02,
        2.9877e-03])
[[  77    0    0  132   55    0    0    0  652    2 4072    0   10    0]
 [  21    0    0   75  563    0    1    3  407   26 3846   14   44    0]
 [  20    0    0 2064   83    0    0    0  312    6 2496   10    4    5]
 [ 213    0    0  220    6    1    0    0  737   21 3792    9    1    0]
 [   8    0    0   91   21    0    0    0  235    2 4637    1    4    1]
 [  42    0    0 2436  280    1    0    0   67    0 1956    1  217    0]
 [   2    0    0  255   48    0    0    0  290    0 4404    1    0    0]
 [ 213    0    0   10  191    0    0    0 1619    0 2961    0    6    0]
 [   7    0    0    3    0    0   37    0   14    0 4924    0    2   13]
 [  19    0    0  259   25    0    0    0   65    0 3378    0 1183   71]
 [ 190    7    0  201   68    0   20    0  275    0 4185    3   51    0]
 [ 103    0    0  379  386    0    0    1  750  219 3146    7    5    4]
 [  26    0    0 2380   37    0    0    0  398    1 2132    1   21    4]
 [  66    0    0 2550   31    0    0    0  159   10 2180    0    3    1]]
Figure(640x480)
tensor([2.2755, 2.4547, 1.8528,  ..., 1.8538, 2.5065, 1.9989])
[[  15  390    9   73  269    5    1    0    2  240 3982   11    3    0]
 [   5   16    0   10   26    0    0    0    0    1 4942    0    0    0]
 [   1  265    1   51  133    1    0    3    0   34 4510    0    0    1]
 [   0    4    0   55   60    0    0   15    0    6 4860    0    0    0]
 [   2  623    0  113   13    0    0    0    1   22 4224    1    1    0]
 [   3   51    0 1729   37   19    0    1    0  689 2471    0    0    0]
 [   0  473    4    5   53   16    1    3    0   66 4376    0    2    1]
 [   1  146   10    5    1    0    0    0    0  909 3928    0    0    0]
 [   0 1033  175    0    3    0  114    2    5  227 3331    0   51   59]
 [   1   34   14   58   20    0    0    0    0   47 4826    0    0    0]
 [   0   60    3  478   22    0    0   38    0  419 3980    0    0    0]
 [  12   66    6   36   35    2    0    0    2   73 4763    3    1    1]
 [  48  403    0    9   48    1    0    2    0  596 3884    0    5    4]
 [   4   40   28   21    4    3    0    0    0  209 4688    0    0    3]]
Figure(640x480)
tensor([1.4710, 2.1310, 2.3449,  ..., 1.7382, 1.7594, 2.0622])
[[   0    0    0   13    0    0  257 4155    8    4   51    2   94  416]
 [   0    0    0   11   20    4  351 4078  237  157   89    1    2   50]
 [   0    0    0   14    1    0   16 4895   25    2   20    0   22    5]
 [   0    0    0   12    1    0   38 4771   15   47   29    0   85    2]
 [   0    0    0   17    1    2   91 4701    5  103   26    1   16   37]
 [   0    0    0    1    0    0  555 4287   39    0    9    0   36   73]
 [   0    0    0   12    4    7   64 4055   45   47  703    3   52    8]
 [   0    0    0  141    3   10  488 2819   31  103  547   34   38  786]
 [   0    0    0  229  315    0    8 1900  562   21 1904    0    1   60]
 [   0    0    0   46   13   47  544 2785  172   25   71    0 1177  120]
 [   0    0    0  135    3    8   37 4060  117   16   13    0  534   77]
 [   0    0    0  112    1    2  204 3976  266  121   16    1   73  228]
 [   0    0    1  104    0    0  483 3678  201   14  239    6  186   88]
 [   0    0    0   23    1    1  110 4386  284   13   21    0   38  123]]
Figure(640x480)
tensor([1.9121, 1.7198, 2.3587,  ..., 1.6482, 0.9842, 0.8833])
[[   0  173   16   66    0  312    0  841    0 3190    7   14    0  381]
 [   0  573  564  368    0   78    1  256    0 2648   15   17    0  480]
 [   0   37  584   31    0   99    3   49    0 4104    5    5    0   83]
 [   0  295 1138  719    0   21    3   97    0 2028   38   16    0  645]
 [   2  981  453    6    0 1291    0  410    0 1751   10   20    0   76]
 [   0    7    4   45    1 2813    0 1089    1  818    0    2    0  220]
 [   2  304  141   80    0 2384    0   61    0 1718    4  204    0  102]
 [   0    7    7    2    0   27    0   14    0 4822    0    3    0  118]
 [   7  824   32    3    0  118    0  308    0 3550    0  132    0   26]
 [   1    2   21   14    0   68    0    2    0 4858    0   16    0   18]
 [   0    1  104  527    0  699    0   42    0 3527    2   82    0   16]
 [   4   44  360  115    2  176    8 1019    0 2767    1   31    0  473]
 [   1   15   21   15    1  310    0   61    0 4547    0    1    0   28]
 [   0    2  218  427    0   94    0  106    0 3832    2    4    0  315]]
Figure(640x480)
tensor([2.5098, 2.0816, 1.8930,  ..., 1.1803, 1.0967, 1.6556])
