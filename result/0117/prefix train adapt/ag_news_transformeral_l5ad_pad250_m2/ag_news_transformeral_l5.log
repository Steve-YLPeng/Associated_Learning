total count words 102019
vocab size 30000
found 26754 words in glove
Load ckpt from ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt
model: TransformerModelML(
  (layers): ModuleList(
    (0): EMBLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Embedding(30000, 300)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=4, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=4, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.weight 9000000
layers.0.ae.g.0.weight 512
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 512
layers.0.ae.h.0.bias 4
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
Total Trainable Params: 13672292
Start Training
train_mask {0, 1}
gc 9
Train Epoch0 Acc 0.19633333333333333 (23560/120000), AUC 0.3785572648048401
ep0_train_time 73.61524057388306
Test Epoch0 threshold 0.1 Acc 0.9147368421052632, AUC 0.9790077209472656, avg_entr 0.03069348819553852
ep0_t0.1_test_time 0.5123775005340576
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m2//ag_news_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.2 Acc 0.9147368421052632, AUC 0.9787771105766296, avg_entr 0.03277401253581047
ep0_t0.2_test_time 0.48037290573120117
Test Epoch0 threshold 0.3 Acc 0.9135526315789474, AUC 0.9786414504051208, avg_entr 0.04124496504664421
ep0_t0.3_test_time 0.45039820671081543
Test Epoch0 threshold 0.4 Acc 0.9126315789473685, AUC 0.9786655902862549, avg_entr 0.04380045086145401
ep0_t0.4_test_time 0.44020748138427734
Test Epoch0 threshold 0.5 Acc 0.9119736842105263, AUC 0.9787135720252991, avg_entr 0.04574577510356903
ep0_t0.5_test_time 0.42458534240722656
Test Epoch0 threshold 0.6 Acc 0.9119736842105263, AUC 0.9787464141845703, avg_entr 0.04628389701247215
ep0_t0.6_test_time 0.41641807556152344
Test Epoch0 threshold 0.7 Acc 0.9119736842105263, AUC 0.9787464141845703, avg_entr 0.04628389701247215
ep0_t0.7_test_time 0.41594815254211426
Test Epoch0 threshold 0.8 Acc 0.9119736842105263, AUC 0.9787464141845703, avg_entr 0.04628389701247215
ep0_t0.8_test_time 0.4160606861114502
Test Epoch0 threshold 0.9 Acc 0.9119736842105263, AUC 0.9787464141845703, avg_entr 0.04628389701247215
ep0_t0.9_test_time 0.4159979820251465
gc 0
Train Epoch1 Acc 0.21993333333333334 (26392/120000), AUC 0.3240090608596802
ep1_train_time 73.21472430229187
Test Epoch1 threshold 0.1 Acc 0.9171052631578948, AUC 0.9795258045196533, avg_entr 0.025608599185943604
ep1_t0.1_test_time 0.5061826705932617
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m2//ag_news_transformeral_l5_prefix.pt  ,ep 1
Test Epoch1 threshold 0.2 Acc 0.9169736842105263, AUC 0.9796429872512817, avg_entr 0.028746332973241806
ep1_t0.2_test_time 0.4823801517486572
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m2//ag_news_transformeral_l5_prefix.pt  ,ep 1
Test Epoch1 threshold 0.3 Acc 0.9148684210526316, AUC 0.9793790578842163, avg_entr 0.03876282647252083
ep1_t0.3_test_time 0.4534940719604492
Test Epoch1 threshold 0.4 Acc 0.9135526315789474, AUC 0.979357123374939, avg_entr 0.041598059237003326
ep1_t0.4_test_time 0.44271349906921387
Test Epoch1 threshold 0.5 Acc 0.9136842105263158, AUC 0.979282796382904, avg_entr 0.04370374232530594
ep1_t0.5_test_time 0.4233839511871338
Test Epoch1 threshold 0.6 Acc 0.9135526315789474, AUC 0.979311466217041, avg_entr 0.04410812631249428
ep1_t0.6_test_time 0.4179227352142334
Test Epoch1 threshold 0.7 Acc 0.9135526315789474, AUC 0.979311466217041, avg_entr 0.04410812631249428
ep1_t0.7_test_time 0.4183487892150879
Test Epoch1 threshold 0.8 Acc 0.9135526315789474, AUC 0.979311466217041, avg_entr 0.04410812631249428
ep1_t0.8_test_time 0.4183847904205322
Test Epoch1 threshold 0.9 Acc 0.9135526315789474, AUC 0.979311466217041, avg_entr 0.04410812631249428
ep1_t0.9_test_time 0.4191935062408447
gc 0
Train Epoch2 Acc 0.21405833333333332 (25687/120000), AUC 0.3291006088256836
ep2_train_time 73.24781513214111
Test Epoch2 threshold 0.1 Acc 0.9173684210526316, AUC 0.979469895362854, avg_entr 0.02289092354476452
ep2_t0.1_test_time 0.5037593841552734
Test Epoch2 threshold 0.2 Acc 0.9177631578947368, AUC 0.9797531366348267, avg_entr 0.025941044092178345
ep2_t0.2_test_time 0.4767763614654541
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m2//ag_news_transformeral_l5_prefix.pt  ,ep 2
Test Epoch2 threshold 0.3 Acc 0.9167105263157894, AUC 0.9794612526893616, avg_entr 0.0360385999083519
ep2_t0.3_test_time 0.4599783420562744
Test Epoch2 threshold 0.4 Acc 0.9142105263157895, AUC 0.9795008897781372, avg_entr 0.0392187274992466
ep2_t0.4_test_time 0.44150257110595703
Test Epoch2 threshold 0.5 Acc 0.9142105263157895, AUC 0.9794930219650269, avg_entr 0.04111282527446747
ep2_t0.5_test_time 0.4261324405670166
Test Epoch2 threshold 0.6 Acc 0.9147368421052632, AUC 0.9795044660568237, avg_entr 0.04180663824081421
ep2_t0.6_test_time 0.42175936698913574
Test Epoch2 threshold 0.7 Acc 0.9147368421052632, AUC 0.9795044660568237, avg_entr 0.04180663824081421
ep2_t0.7_test_time 0.4182896614074707
Test Epoch2 threshold 0.8 Acc 0.9147368421052632, AUC 0.9795044660568237, avg_entr 0.04180663824081421
ep2_t0.8_test_time 0.42317962646484375
Test Epoch2 threshold 0.9 Acc 0.9147368421052632, AUC 0.9795044660568237, avg_entr 0.04180663824081421
ep2_t0.9_test_time 0.4210937023162842
gc 0
Train Epoch3 Acc 0.20440833333333333 (24529/120000), AUC 0.3346797227859497
ep3_train_time 73.18762993812561
Test Epoch3 threshold 0.1 Acc 0.9185526315789474, AUC 0.9793938398361206, avg_entr 0.02181718312203884
ep3_t0.1_test_time 0.5025539398193359
Test Epoch3 threshold 0.2 Acc 0.9178947368421052, AUC 0.9793342351913452, avg_entr 0.02545817755162716
ep3_t0.2_test_time 0.47419023513793945
Test Epoch3 threshold 0.3 Acc 0.9161842105263158, AUC 0.9795750975608826, avg_entr 0.03518110141158104
ep3_t0.3_test_time 0.45059728622436523
Test Epoch3 threshold 0.4 Acc 0.9146052631578947, AUC 0.9796053767204285, avg_entr 0.03853583708405495
ep3_t0.4_test_time 0.440673828125
Test Epoch3 threshold 0.5 Acc 0.9142105263157895, AUC 0.9795533418655396, avg_entr 0.04065515473484993
ep3_t0.5_test_time 0.4279348850250244
Test Epoch3 threshold 0.6 Acc 0.9144736842105263, AUC 0.9795820116996765, avg_entr 0.041092969477176666
ep3_t0.6_test_time 0.4170522689819336
Test Epoch3 threshold 0.7 Acc 0.9144736842105263, AUC 0.9795820116996765, avg_entr 0.041092969477176666
ep3_t0.7_test_time 0.417722225189209
Test Epoch3 threshold 0.8 Acc 0.9144736842105263, AUC 0.9795820116996765, avg_entr 0.041092969477176666
ep3_t0.8_test_time 0.41941285133361816
Test Epoch3 threshold 0.9 Acc 0.9144736842105263, AUC 0.9795820116996765, avg_entr 0.041092969477176666
ep3_t0.9_test_time 0.41785764694213867
gc 0
Train Epoch4 Acc 0.20059166666666667 (24071/120000), AUC 0.33673179149627686
ep4_train_time 73.26111102104187
Test Epoch4 threshold 0.1 Acc 0.9188157894736843, AUC 0.979561984539032, avg_entr 0.021852243691682816
ep4_t0.1_test_time 0.5019993782043457
Test Epoch4 threshold 0.2 Acc 0.9188157894736843, AUC 0.9795807003974915, avg_entr 0.02554626390337944
ep4_t0.2_test_time 0.4743993282318115
Test Epoch4 threshold 0.3 Acc 0.9172368421052631, AUC 0.9795691967010498, avg_entr 0.035348862409591675
ep4_t0.3_test_time 0.45213866233825684
Test Epoch4 threshold 0.4 Acc 0.9144736842105263, AUC 0.9795922636985779, avg_entr 0.0386456623673439
ep4_t0.4_test_time 0.4380984306335449
Test Epoch4 threshold 0.5 Acc 0.9146052631578947, AUC 0.9795794486999512, avg_entr 0.040463078767061234
ep4_t0.5_test_time 0.42491960525512695
Test Epoch4 threshold 0.6 Acc 0.9151315789473684, AUC 0.979584813117981, avg_entr 0.04113393649458885
ep4_t0.6_test_time 0.4174215793609619
Test Epoch4 threshold 0.7 Acc 0.9151315789473684, AUC 0.979584813117981, avg_entr 0.04113393649458885
ep4_t0.7_test_time 0.4187326431274414
Test Epoch4 threshold 0.8 Acc 0.9151315789473684, AUC 0.979584813117981, avg_entr 0.04113393649458885
ep4_t0.8_test_time 0.41901659965515137
Test Epoch4 threshold 0.9 Acc 0.9151315789473684, AUC 0.979584813117981, avg_entr 0.04113393649458885
ep4_t0.9_test_time 0.41741275787353516
gc 0
Train Epoch5 Acc 0.19993333333333332 (23992/120000), AUC 0.3368973433971405
ep5_train_time 73.26129651069641
Test Epoch5 threshold 0.1 Acc 0.9186842105263158, AUC 0.9795770049095154, avg_entr 0.02181028388440609
ep5_t0.1_test_time 0.5042698383331299
Test Epoch5 threshold 0.2 Acc 0.9186842105263158, AUC 0.9795705080032349, avg_entr 0.025452550500631332
ep5_t0.2_test_time 0.47676515579223633
Test Epoch5 threshold 0.3 Acc 0.9172368421052631, AUC 0.9795770645141602, avg_entr 0.03530358523130417
ep5_t0.3_test_time 0.4520301818847656
Test Epoch5 threshold 0.4 Acc 0.9143421052631578, AUC 0.9795892238616943, avg_entr 0.038641367107629776
ep5_t0.4_test_time 0.4408435821533203
Test Epoch5 threshold 0.5 Acc 0.9144736842105263, AUC 0.9795823693275452, avg_entr 0.040393583476543427
ep5_t0.5_test_time 0.4270660877227783
Test Epoch5 threshold 0.6 Acc 0.915, AUC 0.9795868396759033, avg_entr 0.041089899837970734
ep5_t0.6_test_time 0.42328786849975586
Test Epoch5 threshold 0.7 Acc 0.915, AUC 0.9795868396759033, avg_entr 0.041089899837970734
ep5_t0.7_test_time 0.42098164558410645
Test Epoch5 threshold 0.8 Acc 0.915, AUC 0.9795868396759033, avg_entr 0.041089899837970734
ep5_t0.8_test_time 0.41953086853027344
Test Epoch5 threshold 0.9 Acc 0.915, AUC 0.9795868396759033, avg_entr 0.041089899837970734
ep5_t0.9_test_time 0.4175729751586914
gc 0
Train Epoch6 Acc 0.199875 (23985/120000), AUC 0.33760616183280945
ep6_train_time 73.32460474967957
Test Epoch6 threshold 0.1 Acc 0.9186842105263158, AUC 0.9794890284538269, avg_entr 0.021751152351498604
ep6_t0.1_test_time 0.5022034645080566
Test Epoch6 threshold 0.2 Acc 0.9186842105263158, AUC 0.9795653820037842, avg_entr 0.0254567489027977
ep6_t0.2_test_time 0.4755594730377197
Test Epoch6 threshold 0.3 Acc 0.9172368421052631, AUC 0.9795776605606079, avg_entr 0.035310011357069016
ep6_t0.3_test_time 0.4494669437408447
Test Epoch6 threshold 0.4 Acc 0.9143421052631578, AUC 0.9795891046524048, avg_entr 0.0386253260076046
ep6_t0.4_test_time 0.4392426013946533
Test Epoch6 threshold 0.5 Acc 0.9146052631578947, AUC 0.9795842170715332, avg_entr 0.040391966700553894
ep6_t0.5_test_time 0.4246244430541992
Test Epoch6 threshold 0.6 Acc 0.915, AUC 0.9795868396759033, avg_entr 0.04107356816530228
ep6_t0.6_test_time 0.41870641708374023
Test Epoch6 threshold 0.7 Acc 0.915, AUC 0.9795868396759033, avg_entr 0.04107356816530228
ep6_t0.7_test_time 0.4189453125
Test Epoch6 threshold 0.8 Acc 0.915, AUC 0.9795868396759033, avg_entr 0.04107356816530228
ep6_t0.8_test_time 0.41881465911865234
Test Epoch6 threshold 0.9 Acc 0.915, AUC 0.9795868396759033, avg_entr 0.04107356816530228
ep6_t0.9_test_time 0.41837334632873535
gc 0
Train Epoch7 Acc 0.20010833333333333 (24013/120000), AUC 0.3370148837566376
ep7_train_time 73.29080247879028
Test Epoch7 threshold 0.1 Acc 0.9186842105263158, AUC 0.9794884920120239, avg_entr 0.02174709178507328
ep7_t0.1_test_time 0.5045323371887207
Test Epoch7 threshold 0.2 Acc 0.9186842105263158, AUC 0.979570746421814, avg_entr 0.02540624886751175
ep7_t0.2_test_time 0.47575950622558594
Test Epoch7 threshold 0.3 Acc 0.9172368421052631, AUC 0.9795774221420288, avg_entr 0.03530560061335564
ep7_t0.3_test_time 0.4497513771057129
Test Epoch7 threshold 0.4 Acc 0.9143421052631578, AUC 0.9795889258384705, avg_entr 0.038621075451374054
ep7_t0.4_test_time 0.4387836456298828
Test Epoch7 threshold 0.5 Acc 0.9146052631578947, AUC 0.9795840382575989, avg_entr 0.04038785398006439
ep7_t0.5_test_time 0.4258148670196533
Test Epoch7 threshold 0.6 Acc 0.915, AUC 0.9795867204666138, avg_entr 0.04106944426894188
ep7_t0.6_test_time 0.42204952239990234
Test Epoch7 threshold 0.7 Acc 0.915, AUC 0.9795867204666138, avg_entr 0.04106944426894188
ep7_t0.7_test_time 0.4269232749938965
Test Epoch7 threshold 0.8 Acc 0.915, AUC 0.9795867204666138, avg_entr 0.04106944426894188
ep7_t0.8_test_time 0.4199502468109131
Test Epoch7 threshold 0.9 Acc 0.915, AUC 0.9795867204666138, avg_entr 0.04106944426894188
ep7_t0.9_test_time 0.41804957389831543
gc 0
Train Epoch8 Acc 0.1992 (23904/120000), AUC 0.3372122645378113
ep8_train_time 73.29085350036621
Test Epoch8 threshold 0.1 Acc 0.9186842105263158, AUC 0.9794878959655762, avg_entr 0.021744485944509506
ep8_t0.1_test_time 0.5037369728088379
Test Epoch8 threshold 0.2 Acc 0.9186842105263158, AUC 0.9795704483985901, avg_entr 0.02540414035320282
ep8_t0.2_test_time 0.4747347831726074
Test Epoch8 threshold 0.3 Acc 0.9172368421052631, AUC 0.9795772433280945, avg_entr 0.035304438322782516
ep8_t0.3_test_time 0.4490697383880615
Test Epoch8 threshold 0.4 Acc 0.9143421052631578, AUC 0.9795888662338257, avg_entr 0.038620300590991974
ep8_t0.4_test_time 0.4385538101196289
Test Epoch8 threshold 0.5 Acc 0.9146052631578947, AUC 0.9795840978622437, avg_entr 0.04038729891180992
ep8_t0.5_test_time 0.4267466068267822
Test Epoch8 threshold 0.6 Acc 0.915, AUC 0.9795867204666138, avg_entr 0.04106888547539711
ep8_t0.6_test_time 0.41932225227355957
Test Epoch8 threshold 0.7 Acc 0.915, AUC 0.9795867204666138, avg_entr 0.04106888547539711
ep8_t0.7_test_time 0.41692471504211426
Test Epoch8 threshold 0.8 Acc 0.915, AUC 0.9795867204666138, avg_entr 0.04106888547539711
ep8_t0.8_test_time 0.4177060127258301
Test Epoch8 threshold 0.9 Acc 0.915, AUC 0.9795867204666138, avg_entr 0.04106888547539711
ep8_t0.9_test_time 0.4172835350036621
gc 0
Train Epoch9 Acc 0.1997 (23964/120000), AUC 0.33691170811653137
ep9_train_time 73.20905494689941
Test Epoch9 threshold 0.1 Acc 0.9185526315789474, AUC 0.9794872999191284, avg_entr 0.021741995587944984
ep9_t0.1_test_time 0.5039145946502686
Test Epoch9 threshold 0.2 Acc 0.9185526315789474, AUC 0.9795702695846558, avg_entr 0.025402069091796875
ep9_t0.2_test_time 0.4747314453125
Test Epoch9 threshold 0.3 Acc 0.9172368421052631, AUC 0.9795773029327393, avg_entr 0.035303112119436264
ep9_t0.3_test_time 0.44931650161743164
Test Epoch9 threshold 0.4 Acc 0.9143421052631578, AUC 0.9795889854431152, avg_entr 0.038619447499513626
ep9_t0.4_test_time 0.4389526844024658
Test Epoch9 threshold 0.5 Acc 0.9146052631578947, AUC 0.9795841574668884, avg_entr 0.04038674756884575
ep9_t0.5_test_time 0.4272637367248535
Test Epoch9 threshold 0.6 Acc 0.915, AUC 0.9795867204666138, avg_entr 0.04106825590133667
ep9_t0.6_test_time 0.4192216396331787
Test Epoch9 threshold 0.7 Acc 0.915, AUC 0.9795867204666138, avg_entr 0.04106825590133667
ep9_t0.7_test_time 0.4180276393890381
Test Epoch9 threshold 0.8 Acc 0.915, AUC 0.9795867204666138, avg_entr 0.04106825590133667
ep9_t0.8_test_time 0.41809821128845215
Test Epoch9 threshold 0.9 Acc 0.915, AUC 0.9795867204666138, avg_entr 0.04106825590133667
ep9_t0.9_test_time 0.41750240325927734
Best AUC 0.9797531366348267
train_loss (2, 5, 10)
valid_acc (10, 9)
valid_AUC (10, 9)
train_acc (10,)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Start Testing
Load ckpt at ckpt/ag_news_transformeral_l5_pad250_m2//ag_news_transformeral_l5_prefix.pt
[[1705   62   90   43]
 [  12 1874    7    7]
 [  54   17 1669  160]
 [  54   19  123 1704]]
Figure(640x480)
tensor([1.2515e-02, 3.6020e-05, 3.4438e-03,  ..., 1.5678e-02, 6.2009e-04,
        5.3952e-01])
[[1706   59   82   53]
 [   6 1875   11    8]
 [  48   22 1673  157]
 [  45   18  119 1718]]
Figure(640x480)
tensor([9.5024e-05, 2.4580e-05, 1.4941e-04,  ..., 1.9629e-04, 1.9397e-05,
        4.9436e-03])
[[1784    0  116    0]
 [  65    0 1835    0]
 [ 180    1 1719    0]
 [1635    0  263    2]]
Figure(640x480)
tensor([1.0357, 0.4122, 0.5533,  ..., 0.4169, 0.9729, 0.9905])
[[1899    0    1    0]
 [1857    0   43    0]
 [1900    0    0    0]
 [1900    0    0    0]]
Figure(640x480)
tensor([0.4852, 0.7569, 0.7875,  ..., 0.7676, 0.4249, 0.6921])
[[   4    3 1893    0]
 [   0    0 1900    0]
 [ 284    3 1613    0]
 [  18    0 1882    0]]
Figure(640x480)
tensor([1.0287, 0.4877, 0.7220,  ..., 0.7170, 0.9125, 0.9051])
