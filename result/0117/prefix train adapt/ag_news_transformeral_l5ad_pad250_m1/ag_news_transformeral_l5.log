total count words 102019
vocab size 30000
found 26754 words in glove
model: TransformerModelML(
  (layers): ModuleList(
    (0): EMBLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Embedding(30000, 300)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=4, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=4, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.weight 9000000
layers.0.ae.g.0.weight 512
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 512
layers.0.ae.h.0.bias 4
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
Total Trainable Params: 13672292
Start Training
train_mask {0}
gc 0
Train Epoch0 Acc 0.2967166666666667 (35606/120000), AUC 0.6137881278991699
ep0_train_time 58.96725535392761
Test Epoch0 threshold 0.1 Acc 0.8935526315789474, AUC 0.973774254322052, avg_entr 0.19790369272232056
ep0_t0.1_test_time 0.43076300621032715
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.2 Acc 0.8935526315789474, AUC 0.973774254322052, avg_entr 0.19790369272232056
ep0_t0.2_test_time 0.43052005767822266
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.3 Acc 0.8935526315789474, AUC 0.973774254322052, avg_entr 0.19790369272232056
ep0_t0.3_test_time 0.4295918941497803
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.4 Acc 0.8935526315789474, AUC 0.973774254322052, avg_entr 0.19790369272232056
ep0_t0.4_test_time 0.4306831359863281
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.5 Acc 0.8935526315789474, AUC 0.973774254322052, avg_entr 0.19790369272232056
ep0_t0.5_test_time 0.42787885665893555
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.6 Acc 0.8935526315789474, AUC 0.973774254322052, avg_entr 0.19790369272232056
ep0_t0.6_test_time 0.4302849769592285
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.7 Acc 0.8935526315789474, AUC 0.973774254322052, avg_entr 0.19790369272232056
ep0_t0.7_test_time 0.4299287796020508
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.8 Acc 0.8935526315789474, AUC 0.973774254322052, avg_entr 0.19790369272232056
ep0_t0.8_test_time 0.42879247665405273
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.9 Acc 0.8935526315789474, AUC 0.973774254322052, avg_entr 0.19790369272232056
ep0_t0.9_test_time 0.4300353527069092
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 0
gc 0
Train Epoch1 Acc 0.3252583333333333 (39031/120000), AUC 0.6430597901344299
ep1_train_time 58.82793879508972
Test Epoch1 threshold 0.1 Acc 0.900921052631579, AUC 0.9752851724624634, avg_entr 0.1543748527765274
ep1_t0.1_test_time 0.4297473430633545
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 1
Test Epoch1 threshold 0.2 Acc 0.900921052631579, AUC 0.9752851724624634, avg_entr 0.1543748527765274
ep1_t0.2_test_time 0.43102526664733887
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 1
Test Epoch1 threshold 0.3 Acc 0.900921052631579, AUC 0.9752851724624634, avg_entr 0.1543748527765274
ep1_t0.3_test_time 0.4306025505065918
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 1
Test Epoch1 threshold 0.4 Acc 0.900921052631579, AUC 0.9752851724624634, avg_entr 0.1543748527765274
ep1_t0.4_test_time 0.42876100540161133
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 1
Test Epoch1 threshold 0.5 Acc 0.900921052631579, AUC 0.9752851724624634, avg_entr 0.1543748527765274
ep1_t0.5_test_time 0.4262402057647705
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 1
Test Epoch1 threshold 0.6 Acc 0.900921052631579, AUC 0.9752851724624634, avg_entr 0.1543748527765274
ep1_t0.6_test_time 0.4299631118774414
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 1
Test Epoch1 threshold 0.7 Acc 0.900921052631579, AUC 0.9752851724624634, avg_entr 0.1543748527765274
ep1_t0.7_test_time 0.4266629219055176
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 1
Test Epoch1 threshold 0.8 Acc 0.900921052631579, AUC 0.9752851724624634, avg_entr 0.1543748527765274
ep1_t0.8_test_time 0.42629528045654297
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 1
Test Epoch1 threshold 0.9 Acc 0.900921052631579, AUC 0.9752851724624634, avg_entr 0.1543748527765274
ep1_t0.9_test_time 0.4244349002838135
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 1
gc 0
Train Epoch2 Acc 0.3248083333333333 (38977/120000), AUC 0.6432943344116211
ep2_train_time 58.872302532196045
Test Epoch2 threshold 0.1 Acc 0.9030263157894737, AUC 0.9756139516830444, avg_entr 0.1444661021232605
ep2_t0.1_test_time 0.42913818359375
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 2
Test Epoch2 threshold 0.2 Acc 0.9030263157894737, AUC 0.9756139516830444, avg_entr 0.1444661021232605
ep2_t0.2_test_time 0.4282865524291992
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 2
Test Epoch2 threshold 0.3 Acc 0.9030263157894737, AUC 0.9756139516830444, avg_entr 0.1444661021232605
ep2_t0.3_test_time 0.4273338317871094
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 2
Test Epoch2 threshold 0.4 Acc 0.9030263157894737, AUC 0.9756139516830444, avg_entr 0.1444661021232605
ep2_t0.4_test_time 0.42888379096984863
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 2
Test Epoch2 threshold 0.5 Acc 0.9030263157894737, AUC 0.9756139516830444, avg_entr 0.1444661021232605
ep2_t0.5_test_time 0.42876696586608887
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 2
Test Epoch2 threshold 0.6 Acc 0.9030263157894737, AUC 0.9756139516830444, avg_entr 0.1444661021232605
ep2_t0.6_test_time 0.42539072036743164
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 2
Test Epoch2 threshold 0.7 Acc 0.9030263157894737, AUC 0.9756139516830444, avg_entr 0.1444661021232605
ep2_t0.7_test_time 0.42644500732421875
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 2
Test Epoch2 threshold 0.8 Acc 0.9030263157894737, AUC 0.9756139516830444, avg_entr 0.1444661021232605
ep2_t0.8_test_time 0.4277920722961426
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 2
Test Epoch2 threshold 0.9 Acc 0.9030263157894737, AUC 0.9756139516830444, avg_entr 0.1444661021232605
ep2_t0.9_test_time 0.42913007736206055
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 2
gc 0
Train Epoch3 Acc 0.32445833333333335 (38935/120000), AUC 0.6432037353515625
ep3_train_time 58.78502678871155
Test Epoch3 threshold 0.1 Acc 0.9025, AUC 0.97568678855896, avg_entr 0.14189136028289795
ep3_t0.1_test_time 0.42890405654907227
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 3
Test Epoch3 threshold 0.2 Acc 0.9025, AUC 0.97568678855896, avg_entr 0.14189136028289795
ep3_t0.2_test_time 0.4293978214263916
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 3
Test Epoch3 threshold 0.3 Acc 0.9025, AUC 0.97568678855896, avg_entr 0.14189136028289795
ep3_t0.3_test_time 0.42731738090515137
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 3
Test Epoch3 threshold 0.4 Acc 0.9025, AUC 0.97568678855896, avg_entr 0.14189136028289795
ep3_t0.4_test_time 0.42769932746887207
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 3
Test Epoch3 threshold 0.5 Acc 0.9025, AUC 0.97568678855896, avg_entr 0.14189136028289795
ep3_t0.5_test_time 0.4289870262145996
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 3
Test Epoch3 threshold 0.6 Acc 0.9025, AUC 0.97568678855896, avg_entr 0.14189136028289795
ep3_t0.6_test_time 0.4262888431549072
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 3
Test Epoch3 threshold 0.7 Acc 0.9025, AUC 0.97568678855896, avg_entr 0.14189136028289795
ep3_t0.7_test_time 0.4275979995727539
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 3
Test Epoch3 threshold 0.8 Acc 0.9025, AUC 0.97568678855896, avg_entr 0.14189136028289795
ep3_t0.8_test_time 0.4275209903717041
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 3
Test Epoch3 threshold 0.9 Acc 0.9025, AUC 0.97568678855896, avg_entr 0.14189136028289795
ep3_t0.9_test_time 0.42762136459350586
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 3
gc 0
Train Epoch4 Acc 0.32443333333333335 (38932/120000), AUC 0.6431736350059509
ep4_train_time 58.92757987976074
Test Epoch4 threshold 0.1 Acc 0.9023684210526316, AUC 0.9757002592086792, avg_entr 0.1408034861087799
ep4_t0.1_test_time 0.4294886589050293
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 4
Test Epoch4 threshold 0.2 Acc 0.9023684210526316, AUC 0.9757002592086792, avg_entr 0.1408034861087799
ep4_t0.2_test_time 0.4307746887207031
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 4
Test Epoch4 threshold 0.3 Acc 0.9023684210526316, AUC 0.9757002592086792, avg_entr 0.1408034861087799
ep4_t0.3_test_time 0.4294002056121826
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 4
Test Epoch4 threshold 0.4 Acc 0.9023684210526316, AUC 0.9757002592086792, avg_entr 0.1408034861087799
ep4_t0.4_test_time 0.42969655990600586
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 4
Test Epoch4 threshold 0.5 Acc 0.9023684210526316, AUC 0.9757002592086792, avg_entr 0.1408034861087799
ep4_t0.5_test_time 0.4309358596801758
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 4
Test Epoch4 threshold 0.6 Acc 0.9023684210526316, AUC 0.9757002592086792, avg_entr 0.1408034861087799
ep4_t0.6_test_time 0.4311823844909668
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 4
Test Epoch4 threshold 0.7 Acc 0.9023684210526316, AUC 0.9757002592086792, avg_entr 0.1408034861087799
ep4_t0.7_test_time 0.42722487449645996
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 4
Test Epoch4 threshold 0.8 Acc 0.9023684210526316, AUC 0.9757002592086792, avg_entr 0.1408034861087799
ep4_t0.8_test_time 0.42703723907470703
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 4
Test Epoch4 threshold 0.9 Acc 0.9023684210526316, AUC 0.9757002592086792, avg_entr 0.1408034861087799
ep4_t0.9_test_time 0.4283931255340576
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 4
gc 0
Train Epoch5 Acc 0.3244 (38928/120000), AUC 0.6431549787521362
ep5_train_time 58.91841697692871
Test Epoch5 threshold 0.1 Acc 0.9025, AUC 0.9757121801376343, avg_entr 0.1405922919511795
ep5_t0.1_test_time 0.43085813522338867
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 5
Test Epoch5 threshold 0.2 Acc 0.9025, AUC 0.9757121801376343, avg_entr 0.1405922919511795
ep5_t0.2_test_time 0.42680811882019043
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 5
Test Epoch5 threshold 0.3 Acc 0.9025, AUC 0.9757121801376343, avg_entr 0.1405922919511795
ep5_t0.3_test_time 0.42750120162963867
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 5
Test Epoch5 threshold 0.4 Acc 0.9025, AUC 0.9757121801376343, avg_entr 0.1405922919511795
ep5_t0.4_test_time 0.4286336898803711
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 5
Test Epoch5 threshold 0.5 Acc 0.9025, AUC 0.9757121801376343, avg_entr 0.1405922919511795
ep5_t0.5_test_time 0.4296555519104004
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 5
Test Epoch5 threshold 0.6 Acc 0.9025, AUC 0.9757121801376343, avg_entr 0.1405922919511795
ep5_t0.6_test_time 0.427731990814209
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 5
Test Epoch5 threshold 0.7 Acc 0.9025, AUC 0.9757121801376343, avg_entr 0.1405922919511795
ep5_t0.7_test_time 0.4257049560546875
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 5
Test Epoch5 threshold 0.8 Acc 0.9025, AUC 0.9757121801376343, avg_entr 0.1405922919511795
ep5_t0.8_test_time 0.42773890495300293
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 5
Test Epoch5 threshold 0.9 Acc 0.9025, AUC 0.9757121801376343, avg_entr 0.1405922919511795
ep5_t0.9_test_time 0.42966127395629883
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 5
gc 0
Train Epoch6 Acc 0.3243916666666667 (38927/120000), AUC 0.6431474089622498
ep6_train_time 58.876559257507324
Test Epoch6 threshold 0.1 Acc 0.9023684210526316, AUC 0.9757121801376343, avg_entr 0.140456423163414
ep6_t0.1_test_time 0.4286658763885498
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 6
Test Epoch6 threshold 0.2 Acc 0.9023684210526316, AUC 0.9757121801376343, avg_entr 0.140456423163414
ep6_t0.2_test_time 0.4254631996154785
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 6
Test Epoch6 threshold 0.3 Acc 0.9023684210526316, AUC 0.9757121801376343, avg_entr 0.140456423163414
ep6_t0.3_test_time 0.42592549324035645
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 6
Test Epoch6 threshold 0.4 Acc 0.9023684210526316, AUC 0.9757121801376343, avg_entr 0.140456423163414
ep6_t0.4_test_time 0.42534494400024414
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 6
Test Epoch6 threshold 0.5 Acc 0.9023684210526316, AUC 0.9757121801376343, avg_entr 0.140456423163414
ep6_t0.5_test_time 0.42546701431274414
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 6
Test Epoch6 threshold 0.6 Acc 0.9023684210526316, AUC 0.9757121801376343, avg_entr 0.140456423163414
ep6_t0.6_test_time 0.4249746799468994
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 6
Test Epoch6 threshold 0.7 Acc 0.9023684210526316, AUC 0.9757121801376343, avg_entr 0.140456423163414
ep6_t0.7_test_time 0.42908191680908203
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 6
Test Epoch6 threshold 0.8 Acc 0.9023684210526316, AUC 0.9757121801376343, avg_entr 0.140456423163414
ep6_t0.8_test_time 0.4246962070465088
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 6
Test Epoch6 threshold 0.9 Acc 0.9023684210526316, AUC 0.9757121801376343, avg_entr 0.140456423163414
ep6_t0.9_test_time 0.4265580177307129
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 6
gc 0
Train Epoch7 Acc 0.3244 (38928/120000), AUC 0.643145740032196
ep7_train_time 58.821709394454956
Test Epoch7 threshold 0.1 Acc 0.9023684210526316, AUC 0.9757121205329895, avg_entr 0.14041711390018463
ep7_t0.1_test_time 0.43004655838012695
Test Epoch7 threshold 0.2 Acc 0.9023684210526316, AUC 0.9757121205329895, avg_entr 0.14041711390018463
ep7_t0.2_test_time 0.42737722396850586
Test Epoch7 threshold 0.3 Acc 0.9023684210526316, AUC 0.9757121205329895, avg_entr 0.14041711390018463
ep7_t0.3_test_time 0.4273092746734619
Test Epoch7 threshold 0.4 Acc 0.9023684210526316, AUC 0.9757121205329895, avg_entr 0.14041711390018463
ep7_t0.4_test_time 0.42606449127197266
Test Epoch7 threshold 0.5 Acc 0.9023684210526316, AUC 0.9757121205329895, avg_entr 0.14041711390018463
ep7_t0.5_test_time 0.4261918067932129
Test Epoch7 threshold 0.6 Acc 0.9023684210526316, AUC 0.9757121205329895, avg_entr 0.14041711390018463
ep7_t0.6_test_time 0.42489027976989746
Test Epoch7 threshold 0.7 Acc 0.9023684210526316, AUC 0.9757121205329895, avg_entr 0.14041711390018463
ep7_t0.7_test_time 0.4273393154144287
Test Epoch7 threshold 0.8 Acc 0.9023684210526316, AUC 0.9757121205329895, avg_entr 0.14041711390018463
ep7_t0.8_test_time 0.42570018768310547
Test Epoch7 threshold 0.9 Acc 0.9023684210526316, AUC 0.9757121205329895, avg_entr 0.14041711390018463
ep7_t0.9_test_time 0.426685094833374
gc 0
Train Epoch8 Acc 0.3244 (38928/120000), AUC 0.6431466341018677
ep8_train_time 58.96995711326599
Test Epoch8 threshold 0.1 Acc 0.9023684210526316, AUC 0.9757121801376343, avg_entr 0.14041252434253693
ep8_t0.1_test_time 0.4298233985900879
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 8
Test Epoch8 threshold 0.2 Acc 0.9023684210526316, AUC 0.9757121801376343, avg_entr 0.14041252434253693
ep8_t0.2_test_time 0.42853331565856934
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 8
Test Epoch8 threshold 0.3 Acc 0.9023684210526316, AUC 0.9757121801376343, avg_entr 0.14041252434253693
ep8_t0.3_test_time 0.4279351234436035
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 8
Test Epoch8 threshold 0.4 Acc 0.9023684210526316, AUC 0.9757121801376343, avg_entr 0.14041252434253693
ep8_t0.4_test_time 0.4271547794342041
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 8
Test Epoch8 threshold 0.5 Acc 0.9023684210526316, AUC 0.9757121801376343, avg_entr 0.14041252434253693
ep8_t0.5_test_time 0.42775797843933105
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 8
Test Epoch8 threshold 0.6 Acc 0.9023684210526316, AUC 0.9757121801376343, avg_entr 0.14041252434253693
ep8_t0.6_test_time 0.4282107353210449
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 8
Test Epoch8 threshold 0.7 Acc 0.9023684210526316, AUC 0.9757121801376343, avg_entr 0.14041252434253693
ep8_t0.7_test_time 0.4277219772338867
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 8
Test Epoch8 threshold 0.8 Acc 0.9023684210526316, AUC 0.9757121801376343, avg_entr 0.14041252434253693
ep8_t0.8_test_time 0.426100492477417
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 8
Test Epoch8 threshold 0.9 Acc 0.9023684210526316, AUC 0.9757121801376343, avg_entr 0.14041252434253693
ep8_t0.9_test_time 0.42676353454589844
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 8
gc 0
Train Epoch9 Acc 0.3244 (38928/120000), AUC 0.6431472897529602
ep9_train_time 58.97920060157776
Test Epoch9 threshold 0.1 Acc 0.9023684210526316, AUC 0.9757121801376343, avg_entr 0.14040757715702057
ep9_t0.1_test_time 0.4306330680847168
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 9
Test Epoch9 threshold 0.2 Acc 0.9023684210526316, AUC 0.9757121801376343, avg_entr 0.14040757715702057
ep9_t0.2_test_time 0.4277212619781494
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 9
Test Epoch9 threshold 0.3 Acc 0.9023684210526316, AUC 0.9757121801376343, avg_entr 0.14040757715702057
ep9_t0.3_test_time 0.4275498390197754
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 9
Test Epoch9 threshold 0.4 Acc 0.9023684210526316, AUC 0.9757121801376343, avg_entr 0.14040757715702057
ep9_t0.4_test_time 0.42684483528137207
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 9
Test Epoch9 threshold 0.5 Acc 0.9023684210526316, AUC 0.9757121801376343, avg_entr 0.14040757715702057
ep9_t0.5_test_time 0.42859840393066406
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 9
Test Epoch9 threshold 0.6 Acc 0.9023684210526316, AUC 0.9757121801376343, avg_entr 0.14040757715702057
ep9_t0.6_test_time 0.4262809753417969
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 9
Test Epoch9 threshold 0.7 Acc 0.9023684210526316, AUC 0.9757121801376343, avg_entr 0.14040757715702057
ep9_t0.7_test_time 0.42694640159606934
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 9
Test Epoch9 threshold 0.8 Acc 0.9023684210526316, AUC 0.9757121801376343, avg_entr 0.14040757715702057
ep9_t0.8_test_time 0.42769932746887207
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 9
Test Epoch9 threshold 0.9 Acc 0.9023684210526316, AUC 0.9757121801376343, avg_entr 0.14040757715702057
ep9_t0.9_test_time 0.42668700218200684
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt  ,ep 9
Best AUC 0.9757121801376343
train_loss (2, 5, 10)
valid_acc (10, 9)
valid_AUC (10, 9)
train_acc (10,)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Start Testing
Load ckpt at ckpt/ag_news_transformeral_l5_pad250_m1//ag_news_transformeral_l5_prefix.pt
[[1690   69   89   52]
 [  14 1873    4    9]
 [  61   26 1612  201]
 [  60   23  134 1683]]
Figure(640x480)
tensor([0.3126, 0.0223, 0.1902,  ..., 0.2316, 0.2036, 0.7392])
[[ 655    0  660  585]
 [ 250    0 1649    1]
 [ 107    0 1786    7]
 [  26    0 1867    7]]
Figure(640x480)
tensor([0.5421, 0.7226, 0.7182,  ..., 0.9243, 0.3161, 0.5570])
[[ 721    8 1072   99]
 [ 601  170  612  517]
 [ 228   17 1528  127]
 [1525   11  201  163]]
Figure(640x480)
tensor([0.5910, 0.7891, 1.0565,  ..., 1.0602, 0.9480, 0.4995])
[[1185    0  633   82]
 [1235    0  548  117]
 [1537    5  164  194]
 [1849    1   30   20]]
Figure(640x480)
tensor([0.8343, 0.5958, 0.9981,  ..., 0.7742, 0.7981, 0.5922])
[[ 928   29  942    1]
 [   4    8 1887    1]
 [ 386    0 1511    3]
 [ 101    1 1798    0]]
Figure(640x480)
tensor([0.9497, 0.8292, 0.8973,  ..., 0.8625, 0.4180, 0.7433])
