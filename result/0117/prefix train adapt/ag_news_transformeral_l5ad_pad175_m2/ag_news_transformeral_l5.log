total count words 102019
vocab size 30000
found 26754 words in glove
Load ckpt from ckpt/ag_news_transformeral_l5_pad175_m1//ag_news_transformeral_l5_prefix.pt
model: TransformerModelML(
  (layers): ModuleList(
    (0): EMBLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Embedding(30000, 300)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=4, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=4, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.weight 9000000
layers.0.ae.g.0.weight 512
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 512
layers.0.ae.h.0.bias 4
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
Total Trainable Params: 13672292
Start Training
train_mask {0, 1}
gc 9
Train Epoch0 Acc 0.2501333333333333 (30016/120000), AUC 0.5458662509918213
ep0_train_time 51.24248957633972
Test Epoch0 threshold 0.1 Acc 0.9161842105263158, AUC 0.9792230129241943, avg_entr 0.030140673741698265
ep0_t0.1_test_time 0.38352370262145996
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m2//ag_news_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.2 Acc 0.9140789473684211, AUC 0.9792975187301636, avg_entr 0.03296702355146408
ep0_t0.2_test_time 0.36406850814819336
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m2//ag_news_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.3 Acc 0.9119736842105263, AUC 0.9794633388519287, avg_entr 0.03961797431111336
ep0_t0.3_test_time 0.35240769386291504
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m2//ag_news_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.4 Acc 0.9121052631578948, AUC 0.9796915054321289, avg_entr 0.04308514669537544
ep0_t0.4_test_time 0.3433966636657715
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m2//ag_news_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.5 Acc 0.9113157894736842, AUC 0.979671061038971, avg_entr 0.0448029525578022
ep0_t0.5_test_time 0.32744407653808594
Test Epoch0 threshold 0.6 Acc 0.9107894736842105, AUC 0.9796683192253113, avg_entr 0.045264530926942825
ep0_t0.6_test_time 0.31911206245422363
Test Epoch0 threshold 0.7 Acc 0.9107894736842105, AUC 0.9796683192253113, avg_entr 0.045264530926942825
ep0_t0.7_test_time 0.3181428909301758
Test Epoch0 threshold 0.8 Acc 0.9107894736842105, AUC 0.9796683192253113, avg_entr 0.045264530926942825
ep0_t0.8_test_time 0.3192927837371826
Test Epoch0 threshold 0.9 Acc 0.9107894736842105, AUC 0.9796683192253113, avg_entr 0.045264530926942825
ep0_t0.9_test_time 0.3172638416290283
gc 0
Train Epoch1 Acc 0.2501 (30012/120000), AUC 0.5926651358604431
ep1_train_time 51.015249490737915
Test Epoch1 threshold 0.1 Acc 0.9202631578947369, AUC 0.9803078174591064, avg_entr 0.021405279636383057
ep1_t0.1_test_time 0.37651729583740234
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m2//ag_news_transformeral_l5_prefix.pt  ,ep 1
Test Epoch1 threshold 0.2 Acc 0.9193421052631578, AUC 0.9801399111747742, avg_entr 0.02452719211578369
ep1_t0.2_test_time 0.36123228073120117
Test Epoch1 threshold 0.3 Acc 0.9189473684210526, AUC 0.9806719422340393, avg_entr 0.03355579823255539
ep1_t0.3_test_time 0.34682226181030273
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m2//ag_news_transformeral_l5_prefix.pt  ,ep 1
Test Epoch1 threshold 0.4 Acc 0.9177631578947368, AUC 0.9805803894996643, avg_entr 0.03677412495017052
ep1_t0.4_test_time 0.338184118270874
Test Epoch1 threshold 0.5 Acc 0.9168421052631579, AUC 0.9805648922920227, avg_entr 0.038796305656433105
ep1_t0.5_test_time 0.3200387954711914
Test Epoch1 threshold 0.6 Acc 0.9168421052631579, AUC 0.9805654287338257, avg_entr 0.03915376216173172
ep1_t0.6_test_time 0.3158278465270996
Test Epoch1 threshold 0.7 Acc 0.9168421052631579, AUC 0.9805654287338257, avg_entr 0.03915376216173172
ep1_t0.7_test_time 0.31576085090637207
Test Epoch1 threshold 0.8 Acc 0.9168421052631579, AUC 0.9805654287338257, avg_entr 0.03915376216173172
ep1_t0.8_test_time 0.3159623146057129
Test Epoch1 threshold 0.9 Acc 0.9168421052631579, AUC 0.9805654287338257, avg_entr 0.03915376216173172
ep1_t0.9_test_time 0.316145658493042
gc 0
Train Epoch2 Acc 0.25565 (30678/120000), AUC 0.606783390045166
ep2_train_time 50.98460364341736
Test Epoch2 threshold 0.1 Acc 0.9192105263157895, AUC 0.9799030423164368, avg_entr 0.01837044022977352
ep2_t0.1_test_time 0.3745119571685791
Test Epoch2 threshold 0.2 Acc 0.9192105263157895, AUC 0.9805712699890137, avg_entr 0.022635972127318382
ep2_t0.2_test_time 0.35640954971313477
Test Epoch2 threshold 0.3 Acc 0.9182894736842105, AUC 0.980683445930481, avg_entr 0.0318378321826458
ep2_t0.3_test_time 0.34490060806274414
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m2//ag_news_transformeral_l5_prefix.pt  ,ep 2
Test Epoch2 threshold 0.4 Acc 0.9176315789473685, AUC 0.9806957840919495, avg_entr 0.035022664815187454
ep2_t0.4_test_time 0.3395380973815918
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m2//ag_news_transformeral_l5_prefix.pt  ,ep 2
Test Epoch2 threshold 0.5 Acc 0.9173684210526316, AUC 0.9808388948440552, avg_entr 0.03681080788373947
ep2_t0.5_test_time 0.32348155975341797
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m2//ag_news_transformeral_l5_prefix.pt  ,ep 2
Test Epoch2 threshold 0.6 Acc 0.9172368421052631, AUC 0.9808042049407959, avg_entr 0.03745484724640846
ep2_t0.6_test_time 0.31683993339538574
Test Epoch2 threshold 0.7 Acc 0.9172368421052631, AUC 0.9808042049407959, avg_entr 0.03745484724640846
ep2_t0.7_test_time 0.31521153450012207
Test Epoch2 threshold 0.8 Acc 0.9172368421052631, AUC 0.9808042049407959, avg_entr 0.03745484724640846
ep2_t0.8_test_time 0.31458473205566406
Test Epoch2 threshold 0.9 Acc 0.9172368421052631, AUC 0.9808042049407959, avg_entr 0.03745484724640846
ep2_t0.9_test_time 0.3146965503692627
gc 0
Train Epoch3 Acc 0.26890833333333336 (32269/120000), AUC 0.6154277324676514
ep3_train_time 51.00228524208069
Test Epoch3 threshold 0.1 Acc 0.9180263157894737, AUC 0.9799637198448181, avg_entr 0.017793145030736923
ep3_t0.1_test_time 0.3725881576538086
Test Epoch3 threshold 0.2 Acc 0.9189473684210526, AUC 0.980165958404541, avg_entr 0.02126464620232582
ep3_t0.2_test_time 0.3554983139038086
Test Epoch3 threshold 0.3 Acc 0.9193421052631578, AUC 0.9807942509651184, avg_entr 0.03070765919983387
ep3_t0.3_test_time 0.34244465827941895
Test Epoch3 threshold 0.4 Acc 0.9173684210526316, AUC 0.9807701706886292, avg_entr 0.03378622606396675
ep3_t0.4_test_time 0.3336827754974365
Test Epoch3 threshold 0.5 Acc 0.9173684210526316, AUC 0.9809045791625977, avg_entr 0.03597763925790787
ep3_t0.5_test_time 0.31833863258361816
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m2//ag_news_transformeral_l5_prefix.pt  ,ep 3
Test Epoch3 threshold 0.6 Acc 0.9173684210526316, AUC 0.9808760285377502, avg_entr 0.036369506269693375
ep3_t0.6_test_time 0.3180227279663086
Test Epoch3 threshold 0.7 Acc 0.9173684210526316, AUC 0.9808760285377502, avg_entr 0.036369506269693375
ep3_t0.7_test_time 0.31644129753112793
Test Epoch3 threshold 0.8 Acc 0.9173684210526316, AUC 0.9808760285377502, avg_entr 0.036369506269693375
ep3_t0.8_test_time 0.31391310691833496
Test Epoch3 threshold 0.9 Acc 0.9173684210526316, AUC 0.9808760285377502, avg_entr 0.036369506269693375
ep3_t0.9_test_time 0.3139059543609619
gc 0
Train Epoch4 Acc 0.27475 (32970/120000), AUC 0.6174403429031372
ep4_train_time 51.146427154541016
Test Epoch4 threshold 0.1 Acc 0.9181578947368421, AUC 0.979902982711792, avg_entr 0.017636971548199654
ep4_t0.1_test_time 0.3713219165802002
Test Epoch4 threshold 0.2 Acc 0.9190789473684211, AUC 0.980102002620697, avg_entr 0.02129296399652958
ep4_t0.2_test_time 0.3565244674682617
Test Epoch4 threshold 0.3 Acc 0.9196052631578947, AUC 0.9808543920516968, avg_entr 0.030857598409056664
ep4_t0.3_test_time 0.34383320808410645
Test Epoch4 threshold 0.4 Acc 0.9178947368421052, AUC 0.9807935357093811, avg_entr 0.03391813486814499
ep4_t0.4_test_time 0.33305788040161133
Test Epoch4 threshold 0.5 Acc 0.9177631578947368, AUC 0.9809157848358154, avg_entr 0.035728443413972855
ep4_t0.5_test_time 0.3202683925628662
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m2//ag_news_transformeral_l5_prefix.pt  ,ep 4
Test Epoch4 threshold 0.6 Acc 0.9176315789473685, AUC 0.9809073209762573, avg_entr 0.0362643226981163
ep4_t0.6_test_time 0.31755638122558594
Test Epoch4 threshold 0.7 Acc 0.9176315789473685, AUC 0.9809073209762573, avg_entr 0.0362643226981163
ep4_t0.7_test_time 0.31529998779296875
Test Epoch4 threshold 0.8 Acc 0.9176315789473685, AUC 0.9809073209762573, avg_entr 0.0362643226981163
ep4_t0.8_test_time 0.3154168128967285
Test Epoch4 threshold 0.9 Acc 0.9176315789473685, AUC 0.9809073209762573, avg_entr 0.0362643226981163
ep4_t0.9_test_time 0.3151993751525879
gc 0
Train Epoch5 Acc 0.2768333333333333 (33220/120000), AUC 0.6181026697158813
ep5_train_time 50.94221878051758
Test Epoch5 threshold 0.1 Acc 0.9182894736842105, AUC 0.9799051880836487, avg_entr 0.017552576959133148
ep5_t0.1_test_time 0.3727147579193115
Test Epoch5 threshold 0.2 Acc 0.9189473684210526, AUC 0.9801326990127563, avg_entr 0.021379318088293076
ep5_t0.2_test_time 0.35657644271850586
Test Epoch5 threshold 0.3 Acc 0.9196052631578947, AUC 0.9808093905448914, avg_entr 0.030827898532152176
ep5_t0.3_test_time 0.3434438705444336
Test Epoch5 threshold 0.4 Acc 0.9180263157894737, AUC 0.9807982444763184, avg_entr 0.033731117844581604
ep5_t0.4_test_time 0.335801362991333
Test Epoch5 threshold 0.5 Acc 0.9173684210526316, AUC 0.9809170365333557, avg_entr 0.03568637743592262
ep5_t0.5_test_time 0.3212451934814453
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m2//ag_news_transformeral_l5_prefix.pt  ,ep 5
Test Epoch5 threshold 0.6 Acc 0.9173684210526316, AUC 0.9809144735336304, avg_entr 0.03615337610244751
ep5_t0.6_test_time 0.31896138191223145
Test Epoch5 threshold 0.7 Acc 0.9173684210526316, AUC 0.9809144735336304, avg_entr 0.03615337610244751
ep5_t0.7_test_time 0.31582212448120117
Test Epoch5 threshold 0.8 Acc 0.9173684210526316, AUC 0.9809144735336304, avg_entr 0.03615337610244751
ep5_t0.8_test_time 0.3142404556274414
Test Epoch5 threshold 0.9 Acc 0.9173684210526316, AUC 0.9809144735336304, avg_entr 0.03615337610244751
ep5_t0.9_test_time 0.3158869743347168
gc 0
Train Epoch6 Acc 0.27679166666666666 (33215/120000), AUC 0.6175534725189209
ep6_train_time 50.98188614845276
Test Epoch6 threshold 0.1 Acc 0.9186842105263158, AUC 0.9799131751060486, avg_entr 0.0175275057554245
ep6_t0.1_test_time 0.37421369552612305
Test Epoch6 threshold 0.2 Acc 0.9194736842105263, AUC 0.9802553653717041, avg_entr 0.021507272496819496
ep6_t0.2_test_time 0.35623955726623535
Test Epoch6 threshold 0.3 Acc 0.9198684210526316, AUC 0.9808160662651062, avg_entr 0.030843723565340042
ep6_t0.3_test_time 0.3448464870452881
Test Epoch6 threshold 0.4 Acc 0.9186842105263158, AUC 0.9808096885681152, avg_entr 0.03373878076672554
ep6_t0.4_test_time 0.33687591552734375
Test Epoch6 threshold 0.5 Acc 0.9177631578947368, AUC 0.9809293746948242, avg_entr 0.035749539732933044
ep6_t0.5_test_time 0.3213663101196289
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m2//ag_news_transformeral_l5_prefix.pt  ,ep 6
Test Epoch6 threshold 0.6 Acc 0.9177631578947368, AUC 0.9809122085571289, avg_entr 0.03611608222126961
ep6_t0.6_test_time 0.3205385208129883
Test Epoch6 threshold 0.7 Acc 0.9177631578947368, AUC 0.9809122085571289, avg_entr 0.03611608222126961
ep6_t0.7_test_time 0.31649017333984375
Test Epoch6 threshold 0.8 Acc 0.9177631578947368, AUC 0.9809122085571289, avg_entr 0.03611608222126961
ep6_t0.8_test_time 0.3166012763977051
Test Epoch6 threshold 0.9 Acc 0.9177631578947368, AUC 0.9809122085571289, avg_entr 0.03611608222126961
ep6_t0.9_test_time 0.31583523750305176
gc 0
Train Epoch7 Acc 0.27668333333333334 (33202/120000), AUC 0.6182569861412048
ep7_train_time 51.02470302581787
Test Epoch7 threshold 0.1 Acc 0.9186842105263158, AUC 0.9799209833145142, avg_entr 0.0174979567527771
ep7_t0.1_test_time 0.3723931312561035
Test Epoch7 threshold 0.2 Acc 0.9194736842105263, AUC 0.9802537560462952, avg_entr 0.02150483801960945
ep7_t0.2_test_time 0.3571772575378418
Test Epoch7 threshold 0.3 Acc 0.92, AUC 0.9808204770088196, avg_entr 0.030797520652413368
ep7_t0.3_test_time 0.34410595893859863
Test Epoch7 threshold 0.4 Acc 0.9186842105263158, AUC 0.9808093309402466, avg_entr 0.03373938798904419
ep7_t0.4_test_time 0.33528757095336914
Test Epoch7 threshold 0.5 Acc 0.9177631578947368, AUC 0.9809291958808899, avg_entr 0.03575160726904869
ep7_t0.5_test_time 0.3203568458557129
Test Epoch7 threshold 0.6 Acc 0.9176315789473685, AUC 0.9809120297431946, avg_entr 0.03611816093325615
ep7_t0.6_test_time 0.31566596031188965
Test Epoch7 threshold 0.7 Acc 0.9176315789473685, AUC 0.9809120297431946, avg_entr 0.03611816093325615
ep7_t0.7_test_time 0.31545448303222656
Test Epoch7 threshold 0.8 Acc 0.9176315789473685, AUC 0.9809120297431946, avg_entr 0.03611816093325615
ep7_t0.8_test_time 0.31555724143981934
Test Epoch7 threshold 0.9 Acc 0.9176315789473685, AUC 0.9809120297431946, avg_entr 0.03611816093325615
ep7_t0.9_test_time 0.3164534568786621
gc 0
Train Epoch8 Acc 0.2768 (33216/120000), AUC 0.618327260017395
ep8_train_time 50.98683166503906
Test Epoch8 threshold 0.1 Acc 0.9188157894736843, AUC 0.9799191355705261, avg_entr 0.017496643587946892
ep8_t0.1_test_time 0.37111926078796387
Test Epoch8 threshold 0.2 Acc 0.9196052631578947, AUC 0.9802519083023071, avg_entr 0.021504098549485207
ep8_t0.2_test_time 0.3550393581390381
Test Epoch8 threshold 0.3 Acc 0.9201315789473684, AUC 0.9808191657066345, avg_entr 0.030797066166996956
ep8_t0.3_test_time 0.3427574634552002
Test Epoch8 threshold 0.4 Acc 0.9186842105263158, AUC 0.980807900428772, avg_entr 0.03373972699046135
ep8_t0.4_test_time 0.33498120307922363
Test Epoch8 threshold 0.5 Acc 0.9177631578947368, AUC 0.9809278249740601, avg_entr 0.0357523187994957
ep8_t0.5_test_time 0.3188462257385254
Test Epoch8 threshold 0.6 Acc 0.9176315789473685, AUC 0.9809105396270752, avg_entr 0.03611879050731659
ep8_t0.6_test_time 0.31411051750183105
Test Epoch8 threshold 0.7 Acc 0.9176315789473685, AUC 0.9809105396270752, avg_entr 0.03611879050731659
ep8_t0.7_test_time 0.3147604465484619
Test Epoch8 threshold 0.8 Acc 0.9176315789473685, AUC 0.9809105396270752, avg_entr 0.03611879050731659
ep8_t0.8_test_time 0.31412506103515625
Test Epoch8 threshold 0.9 Acc 0.9176315789473685, AUC 0.9809105396270752, avg_entr 0.03611879050731659
ep8_t0.9_test_time 0.31421756744384766
gc 0
Train Epoch9 Acc 0.27700833333333336 (33241/120000), AUC 0.6185672283172607
ep9_train_time 51.01861906051636
Test Epoch9 threshold 0.1 Acc 0.9188157894736843, AUC 0.979918360710144, avg_entr 0.01749526523053646
ep9_t0.1_test_time 0.3742852210998535
Test Epoch9 threshold 0.2 Acc 0.9196052631578947, AUC 0.9802515506744385, avg_entr 0.021503064781427383
ep9_t0.2_test_time 0.356128454208374
Test Epoch9 threshold 0.3 Acc 0.9201315789473684, AUC 0.9808189868927002, avg_entr 0.030796276405453682
ep9_t0.3_test_time 0.3442201614379883
Test Epoch9 threshold 0.4 Acc 0.9186842105263158, AUC 0.980807900428772, avg_entr 0.033739641308784485
ep9_t0.4_test_time 0.33477306365966797
Test Epoch9 threshold 0.5 Acc 0.9177631578947368, AUC 0.9809278249740601, avg_entr 0.035752490162849426
ep9_t0.5_test_time 0.31893277168273926
Test Epoch9 threshold 0.6 Acc 0.9176315789473685, AUC 0.9809105396270752, avg_entr 0.036118894815444946
ep9_t0.6_test_time 0.3164968490600586
Test Epoch9 threshold 0.7 Acc 0.9176315789473685, AUC 0.9809105396270752, avg_entr 0.036118894815444946
ep9_t0.7_test_time 0.3170359134674072
Test Epoch9 threshold 0.8 Acc 0.9176315789473685, AUC 0.9809105396270752, avg_entr 0.036118894815444946
ep9_t0.8_test_time 0.3151547908782959
Test Epoch9 threshold 0.9 Acc 0.9176315789473685, AUC 0.9809105396270752, avg_entr 0.036118894815444946
ep9_t0.9_test_time 0.31548118591308594
Best AUC 0.9809293746948242
train_loss (2, 5, 10)
valid_acc (10, 9)
valid_AUC (10, 9)
train_acc (10,)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Start Testing
Load ckpt at ckpt/ag_news_transformeral_l5_pad175_m2//ag_news_transformeral_l5_prefix.pt
[[1703   63   88   46]
 [   9 1877    7    7]
 [  41   19 1675  165]
 [  49   18  113 1720]]
Figure(640x480)
tensor([8.3700e-03, 1.5229e-05, 3.4783e-03,  ..., 6.6237e-03, 1.1095e-04,
        1.9448e-01])
[[1711   57   74   58]
 [   9 1872    8   11]
 [  48   16 1683  153]
 [  47   14  124 1715]]
Figure(640x480)
tensor([5.8398e-05, 1.5337e-05, 2.6110e-05,  ..., 9.4685e-05, 1.3778e-05,
        7.2856e-05])
[[   0  743 1154    3]
 [   0    0 1900    0]
 [   0   22 1878    0]
 [   0  292 1608    0]]
Figure(640x480)
tensor([0.5235, 0.7218, 0.4608,  ..., 0.4095, 0.7424, 0.7106])
[[   0   58 1842    0]
 [   0 1859   41    0]
 [   0   12 1888    0]
 [   0    9 1891    0]]
Figure(640x480)
tensor([0.6098, 0.6515, 0.8808,  ..., 0.7012, 0.6267, 0.7216])
[[   0   12 1888    0]
 [   0  238 1662    0]
 [   0    0 1900    0]
 [   0    2 1898    0]]
Figure(640x480)
tensor([0.2125, 0.8807, 0.6999,  ..., 0.0544, 0.2523, 0.2589])
