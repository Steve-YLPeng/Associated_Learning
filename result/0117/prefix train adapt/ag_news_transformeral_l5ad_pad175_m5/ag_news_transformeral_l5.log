total count words 102019
vocab size 30000
found 26754 words in glove
Load ckpt from ckpt/ag_news_transformeral_l5_pad175_m4//ag_news_transformeral_l5_prefix.pt
model: TransformerModelML(
  (layers): ModuleList(
    (0): EMBLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Embedding(30000, 300)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=4, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=4, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.weight 9000000
layers.0.ae.g.0.weight 512
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 512
layers.0.ae.h.0.bias 4
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
Total Trainable Params: 13672292
Start Training
train_mask {0, 1, 2, 3, 4}
gc 9
Train Epoch0 Acc 0.9108583333333333 (109303/120000), AUC 0.983157753944397
ep0_train_time 80.30445432662964
Test Epoch0 threshold 0.1 Acc 0.9178947368421052, AUC 0.9796797633171082, avg_entr 0.007814723998308182
ep0_t0.1_test_time 0.432758092880249
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m5//ag_news_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.2 Acc 0.9182894736842105, AUC 0.9809765219688416, avg_entr 0.013163093477487564
ep0_t0.2_test_time 0.3883628845214844
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m5//ag_news_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.3 Acc 0.9189473684210526, AUC 0.9821648597717285, avg_entr 0.022873222827911377
ep0_t0.3_test_time 0.34021925926208496
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m5//ag_news_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.4 Acc 0.9189473684210526, AUC 0.9821569919586182, avg_entr 0.02452729642391205
ep0_t0.4_test_time 0.32588696479797363
Test Epoch0 threshold 0.5 Acc 0.9192105263157895, AUC 0.9821710586547852, avg_entr 0.025560269132256508
ep0_t0.5_test_time 0.31412816047668457
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m5//ag_news_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.6 Acc 0.9193421052631578, AUC 0.9821821451187134, avg_entr 0.025652918964624405
ep0_t0.6_test_time 0.31325554847717285
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m5//ag_news_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.7 Acc 0.9193421052631578, AUC 0.9821821451187134, avg_entr 0.025652918964624405
ep0_t0.7_test_time 0.3123610019683838
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m5//ag_news_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.8 Acc 0.9193421052631578, AUC 0.9821821451187134, avg_entr 0.025652918964624405
ep0_t0.8_test_time 0.31258320808410645
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m5//ag_news_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.9 Acc 0.9193421052631578, AUC 0.9821821451187134, avg_entr 0.025652918964624405
ep0_t0.9_test_time 0.31239891052246094
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m5//ag_news_transformeral_l5_prefix.pt  ,ep 0
gc 0
Train Epoch1 Acc 0.9598666666666666 (115184/120000), AUC 0.9936030507087708
ep1_train_time 80.01678991317749
Test Epoch1 threshold 0.1 Acc 0.9156578947368421, AUC 0.9794628024101257, avg_entr 0.007617568131536245
ep1_t0.1_test_time 0.4288928508758545
Test Epoch1 threshold 0.2 Acc 0.9168421052631579, AUC 0.9807363748550415, avg_entr 0.01326750684529543
ep1_t0.2_test_time 0.3887765407562256
Test Epoch1 threshold 0.3 Acc 0.9167105263157894, AUC 0.9818838834762573, avg_entr 0.022400377318263054
ep1_t0.3_test_time 0.3395261764526367
Test Epoch1 threshold 0.4 Acc 0.9169736842105263, AUC 0.9821575880050659, avg_entr 0.023936668410897255
ep1_t0.4_test_time 0.3255307674407959
Test Epoch1 threshold 0.5 Acc 0.9171052631578948, AUC 0.9821348190307617, avg_entr 0.024817558005452156
ep1_t0.5_test_time 0.3164994716644287
Test Epoch1 threshold 0.6 Acc 0.9172368421052631, AUC 0.9821428656578064, avg_entr 0.025065768510103226
ep1_t0.6_test_time 0.31337809562683105
Test Epoch1 threshold 0.7 Acc 0.9172368421052631, AUC 0.9821428656578064, avg_entr 0.025065768510103226
ep1_t0.7_test_time 0.3143601417541504
Test Epoch1 threshold 0.8 Acc 0.9172368421052631, AUC 0.9821428656578064, avg_entr 0.025065768510103226
ep1_t0.8_test_time 0.31433749198913574
Test Epoch1 threshold 0.9 Acc 0.9172368421052631, AUC 0.9821428656578064, avg_entr 0.025065768510103226
ep1_t0.9_test_time 0.31330323219299316
gc 0
Train Epoch2 Acc 0.9623333333333334 (115480/120000), AUC 0.9943084120750427
ep2_train_time 79.97078061103821
Test Epoch2 threshold 0.1 Acc 0.9164473684210527, AUC 0.9788500070571899, avg_entr 0.007728371769189835
ep2_t0.1_test_time 0.4281163215637207
Test Epoch2 threshold 0.2 Acc 0.9160526315789473, AUC 0.9806252717971802, avg_entr 0.01281219907104969
ep2_t0.2_test_time 0.3838341236114502
Test Epoch2 threshold 0.3 Acc 0.9177631578947368, AUC 0.9818950295448303, avg_entr 0.022615298628807068
ep2_t0.3_test_time 0.34011101722717285
Test Epoch2 threshold 0.4 Acc 0.9175, AUC 0.9820849895477295, avg_entr 0.024084730073809624
ep2_t0.4_test_time 0.3254878520965576
Test Epoch2 threshold 0.5 Acc 0.9182894736842105, AUC 0.9821019768714905, avg_entr 0.02492626942694187
ep2_t0.5_test_time 0.31335973739624023
Test Epoch2 threshold 0.6 Acc 0.9184210526315789, AUC 0.9821063280105591, avg_entr 0.025255927816033363
ep2_t0.6_test_time 0.31081271171569824
Test Epoch2 threshold 0.7 Acc 0.9184210526315789, AUC 0.9821063280105591, avg_entr 0.025255927816033363
ep2_t0.7_test_time 0.3111143112182617
Test Epoch2 threshold 0.8 Acc 0.9184210526315789, AUC 0.9821063280105591, avg_entr 0.025255927816033363
ep2_t0.8_test_time 0.31098246574401855
Test Epoch2 threshold 0.9 Acc 0.9184210526315789, AUC 0.9821063280105591, avg_entr 0.025255927816033363
ep2_t0.9_test_time 0.31054186820983887
gc 0
Train Epoch3 Acc 0.9633 (115596/120000), AUC 0.9945390224456787
ep3_train_time 80.0461962223053
Test Epoch3 threshold 0.1 Acc 0.9161842105263158, AUC 0.9787065982818604, avg_entr 0.007433212827891111
ep3_t0.1_test_time 0.42763853073120117
Test Epoch3 threshold 0.2 Acc 0.9167105263157894, AUC 0.9804131984710693, avg_entr 0.01256867591291666
ep3_t0.2_test_time 0.3902456760406494
Test Epoch3 threshold 0.3 Acc 0.9185526315789474, AUC 0.9818928837776184, avg_entr 0.02247537486255169
ep3_t0.3_test_time 0.33927154541015625
Test Epoch3 threshold 0.4 Acc 0.9185526315789474, AUC 0.9820812344551086, avg_entr 0.023797310888767242
ep3_t0.4_test_time 0.32321786880493164
Test Epoch3 threshold 0.5 Acc 0.9186842105263158, AUC 0.9821277260780334, avg_entr 0.024456802755594254
ep3_t0.5_test_time 0.32184362411499023
Test Epoch3 threshold 0.6 Acc 0.9186842105263158, AUC 0.9821174740791321, avg_entr 0.02482002228498459
ep3_t0.6_test_time 0.3129901885986328
Test Epoch3 threshold 0.7 Acc 0.9186842105263158, AUC 0.9821174740791321, avg_entr 0.02482002228498459
ep3_t0.7_test_time 0.311737060546875
Test Epoch3 threshold 0.8 Acc 0.9186842105263158, AUC 0.9821174740791321, avg_entr 0.02482002228498459
ep3_t0.8_test_time 0.31117916107177734
Test Epoch3 threshold 0.9 Acc 0.9186842105263158, AUC 0.9821174740791321, avg_entr 0.02482002228498459
ep3_t0.9_test_time 0.3105454444885254
gc 0
Train Epoch4 Acc 0.9634 (115608/120000), AUC 0.99463951587677
ep4_train_time 80.08044743537903
Test Epoch4 threshold 0.1 Acc 0.9161842105263158, AUC 0.9788250923156738, avg_entr 0.007525559514760971
ep4_t0.1_test_time 0.42292094230651855
Test Epoch4 threshold 0.2 Acc 0.9163157894736842, AUC 0.9803019165992737, avg_entr 0.012829756364226341
ep4_t0.2_test_time 0.3879396915435791
Test Epoch4 threshold 0.3 Acc 0.9182894736842105, AUC 0.9818831086158752, avg_entr 0.022387616336345673
ep4_t0.3_test_time 0.34090375900268555
Test Epoch4 threshold 0.4 Acc 0.9182894736842105, AUC 0.9820781350135803, avg_entr 0.023788925260305405
ep4_t0.4_test_time 0.3248167037963867
Test Epoch4 threshold 0.5 Acc 0.9184210526315789, AUC 0.9821290373802185, avg_entr 0.024412859231233597
ep4_t0.5_test_time 0.314908504486084
Test Epoch4 threshold 0.6 Acc 0.9184210526315789, AUC 0.9821169376373291, avg_entr 0.02480105496942997
ep4_t0.6_test_time 0.31008052825927734
Test Epoch4 threshold 0.7 Acc 0.9184210526315789, AUC 0.9821169376373291, avg_entr 0.02480105496942997
ep4_t0.7_test_time 0.3100557327270508
Test Epoch4 threshold 0.8 Acc 0.9184210526315789, AUC 0.9821169376373291, avg_entr 0.02480105496942997
ep4_t0.8_test_time 0.310197114944458
Test Epoch4 threshold 0.9 Acc 0.9184210526315789, AUC 0.9821169376373291, avg_entr 0.02480105496942997
ep4_t0.9_test_time 0.31134891510009766
gc 0
Train Epoch5 Acc 0.9637083333333333 (115645/120000), AUC 0.9945627450942993
ep5_train_time 80.040780544281
Test Epoch5 threshold 0.1 Acc 0.9160526315789473, AUC 0.9788944721221924, avg_entr 0.0075556137599051
ep5_t0.1_test_time 0.4233570098876953
Test Epoch5 threshold 0.2 Acc 0.9161842105263158, AUC 0.9803050756454468, avg_entr 0.012828179635107517
ep5_t0.2_test_time 0.3903484344482422
Test Epoch5 threshold 0.3 Acc 0.9182894736842105, AUC 0.9818851947784424, avg_entr 0.022370997816324234
ep5_t0.3_test_time 0.3396034240722656
Test Epoch5 threshold 0.4 Acc 0.9182894736842105, AUC 0.9820802211761475, avg_entr 0.023775530979037285
ep5_t0.4_test_time 0.323075532913208
Test Epoch5 threshold 0.5 Acc 0.9184210526315789, AUC 0.98213130235672, avg_entr 0.024399597197771072
ep5_t0.5_test_time 0.3141627311706543
Test Epoch5 threshold 0.6 Acc 0.9184210526315789, AUC 0.982119083404541, avg_entr 0.02478846162557602
ep5_t0.6_test_time 0.3104701042175293
Test Epoch5 threshold 0.7 Acc 0.9184210526315789, AUC 0.982119083404541, avg_entr 0.02478846162557602
ep5_t0.7_test_time 0.3102996349334717
Test Epoch5 threshold 0.8 Acc 0.9184210526315789, AUC 0.982119083404541, avg_entr 0.02478846162557602
ep5_t0.8_test_time 0.3110353946685791
Test Epoch5 threshold 0.9 Acc 0.9184210526315789, AUC 0.982119083404541, avg_entr 0.02478846162557602
ep5_t0.9_test_time 0.3159143924713135
gc 0
Train Epoch6 Acc 0.9635 (115620/120000), AUC 0.9945877194404602
ep6_train_time 80.01423454284668
Test Epoch6 threshold 0.1 Acc 0.9161842105263158, AUC 0.978906512260437, avg_entr 0.007529991678893566
ep6_t0.1_test_time 0.4227757453918457
Test Epoch6 threshold 0.2 Acc 0.9163157894736842, AUC 0.9803041219711304, avg_entr 0.012830241583287716
ep6_t0.2_test_time 0.3906538486480713
Test Epoch6 threshold 0.3 Acc 0.9184210526315789, AUC 0.9818848371505737, avg_entr 0.022373734042048454
ep6_t0.3_test_time 0.34139132499694824
Test Epoch6 threshold 0.4 Acc 0.9184210526315789, AUC 0.9820799231529236, avg_entr 0.02377847395837307
ep6_t0.4_test_time 0.32337498664855957
Test Epoch6 threshold 0.5 Acc 0.9185526315789474, AUC 0.9821308851242065, avg_entr 0.02440243400633335
ep6_t0.5_test_time 0.316117525100708
Test Epoch6 threshold 0.6 Acc 0.9185526315789474, AUC 0.9821187257766724, avg_entr 0.02479131706058979
ep6_t0.6_test_time 0.31104302406311035
Test Epoch6 threshold 0.7 Acc 0.9185526315789474, AUC 0.9821187257766724, avg_entr 0.02479131706058979
ep6_t0.7_test_time 0.31099557876586914
Test Epoch6 threshold 0.8 Acc 0.9185526315789474, AUC 0.9821187257766724, avg_entr 0.02479131706058979
ep6_t0.8_test_time 0.3108489513397217
Test Epoch6 threshold 0.9 Acc 0.9185526315789474, AUC 0.9821187257766724, avg_entr 0.02479131706058979
ep6_t0.9_test_time 0.31139230728149414
gc 0
Train Epoch7 Acc 0.9637833333333333 (115654/120000), AUC 0.9947433471679688
ep7_train_time 79.97048473358154
Test Epoch7 threshold 0.1 Acc 0.9163157894736842, AUC 0.9789069890975952, avg_entr 0.0075301420874893665
ep7_t0.1_test_time 0.42171621322631836
Test Epoch7 threshold 0.2 Acc 0.9164473684210527, AUC 0.9803044199943542, avg_entr 0.01283066626638174
ep7_t0.2_test_time 0.38921284675598145
Test Epoch7 threshold 0.3 Acc 0.9184210526315789, AUC 0.9818849563598633, avg_entr 0.022374404594302177
ep7_t0.3_test_time 0.3400886058807373
Test Epoch7 threshold 0.4 Acc 0.9184210526315789, AUC 0.9820799827575684, avg_entr 0.023778848350048065
ep7_t0.4_test_time 0.3229093551635742
Test Epoch7 threshold 0.5 Acc 0.9185526315789474, AUC 0.9821309447288513, avg_entr 0.024402834475040436
ep7_t0.5_test_time 0.3155477046966553
Test Epoch7 threshold 0.6 Acc 0.9185526315789474, AUC 0.9821187853813171, avg_entr 0.02479170635342598
ep7_t0.6_test_time 0.31120729446411133
Test Epoch7 threshold 0.7 Acc 0.9185526315789474, AUC 0.9821187853813171, avg_entr 0.02479170635342598
ep7_t0.7_test_time 0.3094170093536377
Test Epoch7 threshold 0.8 Acc 0.9185526315789474, AUC 0.9821187853813171, avg_entr 0.02479170635342598
ep7_t0.8_test_time 0.31041669845581055
Test Epoch7 threshold 0.9 Acc 0.9185526315789474, AUC 0.9821187853813171, avg_entr 0.02479170635342598
ep7_t0.9_test_time 0.312777042388916
gc 0
Train Epoch8 Acc 0.9636 (115632/120000), AUC 0.9946343898773193
ep8_train_time 80.04976463317871
Test Epoch8 threshold 0.1 Acc 0.9163157894736842, AUC 0.9789077639579773, avg_entr 0.007530171424150467
ep8_t0.1_test_time 0.42323780059814453
Test Epoch8 threshold 0.2 Acc 0.9164473684210527, AUC 0.9803048372268677, avg_entr 0.012830945663154125
ep8_t0.2_test_time 0.39129066467285156
Test Epoch8 threshold 0.3 Acc 0.9184210526315789, AUC 0.9818849563598633, avg_entr 0.022374847903847694
ep8_t0.3_test_time 0.3411595821380615
Test Epoch8 threshold 0.4 Acc 0.9184210526315789, AUC 0.9820798635482788, avg_entr 0.023779194802045822
ep8_t0.4_test_time 0.32187318801879883
Test Epoch8 threshold 0.5 Acc 0.9185526315789474, AUC 0.9821308851242065, avg_entr 0.024403171613812447
ep8_t0.5_test_time 0.3146369457244873
Test Epoch8 threshold 0.6 Acc 0.9185526315789474, AUC 0.9821187257766724, avg_entr 0.024792036041617393
ep8_t0.6_test_time 0.31038427352905273
Test Epoch8 threshold 0.7 Acc 0.9185526315789474, AUC 0.9821187257766724, avg_entr 0.024792036041617393
ep8_t0.7_test_time 0.3098304271697998
Test Epoch8 threshold 0.8 Acc 0.9185526315789474, AUC 0.9821187257766724, avg_entr 0.024792036041617393
ep8_t0.8_test_time 0.3100278377532959
Test Epoch8 threshold 0.9 Acc 0.9185526315789474, AUC 0.9821187257766724, avg_entr 0.024792036041617393
ep8_t0.9_test_time 0.3094661235809326
gc 0
Train Epoch9 Acc 0.963675 (115641/120000), AUC 0.9946770071983337
ep9_train_time 80.00186014175415
Test Epoch9 threshold 0.1 Acc 0.9163157894736842, AUC 0.9789080023765564, avg_entr 0.00753039913251996
ep9_t0.1_test_time 0.42215561866760254
Test Epoch9 threshold 0.2 Acc 0.9164473684210527, AUC 0.9803051948547363, avg_entr 0.012831193394958973
ep9_t0.2_test_time 0.38929057121276855
Test Epoch9 threshold 0.3 Acc 0.9184210526315789, AUC 0.9818849563598633, avg_entr 0.022375013679265976
ep9_t0.3_test_time 0.33995509147644043
Test Epoch9 threshold 0.4 Acc 0.9184210526315789, AUC 0.9820799231529236, avg_entr 0.02377922460436821
ep9_t0.4_test_time 0.32166528701782227
Test Epoch9 threshold 0.5 Acc 0.9185526315789474, AUC 0.9821309447288513, avg_entr 0.024403229355812073
ep9_t0.5_test_time 0.31438326835632324
Test Epoch9 threshold 0.6 Acc 0.9185526315789474, AUC 0.9821187853813171, avg_entr 0.02479211799800396
ep9_t0.6_test_time 0.30991339683532715
Test Epoch9 threshold 0.7 Acc 0.9185526315789474, AUC 0.9821187853813171, avg_entr 0.02479211799800396
ep9_t0.7_test_time 0.3098714351654053
Test Epoch9 threshold 0.8 Acc 0.9185526315789474, AUC 0.9821187853813171, avg_entr 0.02479211799800396
ep9_t0.8_test_time 0.3095228672027588
Test Epoch9 threshold 0.9 Acc 0.9185526315789474, AUC 0.9821187853813171, avg_entr 0.02479211799800396
ep9_t0.9_test_time 0.30948400497436523
Best AUC 0.9821821451187134
train_loss (2, 5, 10)
valid_acc (10, 9)
valid_AUC (10, 9)
train_acc (10,)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Start Testing
Load ckpt at ckpt/ag_news_transformeral_l5_pad175_m5//ag_news_transformeral_l5_prefix.pt
[[1711   55   89   45]
 [  12 1865   11   12]
 [  43   13 1707  137]
 [  46   11  139 1704]]
Figure(640x480)
tensor([7.1148e-04, 2.6943e-08, 2.1976e-03,  ..., 9.9527e-02, 5.3629e-08,
        7.9091e-04])
[[1731   52   59   58]
 [  24 1849   11   16]
 [  66   15 1667  152]
 [  55   11  127 1707]]
Figure(640x480)
tensor([6.9362e-06, 3.2966e-08, 2.4902e-08,  ..., 1.8576e-07, 3.2436e-08,
        1.6277e-07])
[[1734   49   59   58]
 [  26 1844   12   18]
 [  62   14 1676  148]
 [  54   10  129 1707]]
Figure(640x480)
tensor([1.1444e-06, 5.0328e-08, 4.3403e-08,  ..., 1.8366e-07, 2.8345e-08,
        4.3538e-08])
[[1733   49   58   60]
 [  29 1840   11   20]
 [  63   15 1669  153]
 [  50   10  121 1719]]
Figure(640x480)
tensor([1.2272e-06, 3.4307e-08, 3.7672e-08,  ..., 1.9311e-07, 2.8040e-08,
        5.5348e-08])
[[1733   50   57   60]
 [  27 1842   11   20]
 [  64   15 1665  156]
 [  50   10  120 1720]]
Figure(640x480)
tensor([1.0628e-06, 3.3868e-08, 4.0871e-08,  ..., 1.9552e-07, 3.3273e-08,
        5.8188e-08])
