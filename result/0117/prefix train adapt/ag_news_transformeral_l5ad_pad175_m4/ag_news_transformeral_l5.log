total count words 102019
vocab size 30000
found 26754 words in glove
Load ckpt from ckpt/ag_news_transformeral_l5_pad175_m3//ag_news_transformeral_l5_prefix.pt
model: TransformerModelML(
  (layers): ModuleList(
    (0): EMBLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Embedding(30000, 300)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=4, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=4, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.weight 9000000
layers.0.ae.g.0.weight 512
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 512
layers.0.ae.h.0.bias 4
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
Total Trainable Params: 13672292
Start Training
train_mask {0, 1, 2, 3}
gc 9
Train Epoch0 Acc 0.25215 (30258/120000), AUC 0.5206896066665649
ep0_train_time 70.73314476013184
Test Epoch0 threshold 0.1 Acc 0.9161842105263158, AUC 0.9800477623939514, avg_entr 0.008536064997315407
ep0_t0.1_test_time 0.4055814743041992
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m4//ag_news_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.2 Acc 0.9167105263157894, AUC 0.9811315536499023, avg_entr 0.013059737160801888
ep0_t0.2_test_time 0.38115739822387695
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m4//ag_news_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.3 Acc 0.9173684210526316, AUC 0.9818018674850464, avg_entr 0.021779783070087433
ep0_t0.3_test_time 0.3464474678039551
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m4//ag_news_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.4 Acc 0.9175, AUC 0.9819331765174866, avg_entr 0.023470314219594002
ep0_t0.4_test_time 0.3294198513031006
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m4//ag_news_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.5 Acc 0.9185526315789474, AUC 0.982053279876709, avg_entr 0.024834617972373962
ep0_t0.5_test_time 0.31679439544677734
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m4//ag_news_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.6 Acc 0.9185526315789474, AUC 0.9820435047149658, avg_entr 0.024956224486231804
ep0_t0.6_test_time 0.3143002986907959
Test Epoch0 threshold 0.7 Acc 0.9185526315789474, AUC 0.9820435047149658, avg_entr 0.024956224486231804
ep0_t0.7_test_time 0.31394076347351074
Test Epoch0 threshold 0.8 Acc 0.9185526315789474, AUC 0.9820435047149658, avg_entr 0.024956224486231804
ep0_t0.8_test_time 0.3192470073699951
Test Epoch0 threshold 0.9 Acc 0.9185526315789474, AUC 0.9820435047149658, avg_entr 0.024956224486231804
ep0_t0.9_test_time 0.31200456619262695
gc 0
Train Epoch1 Acc 0.250025 (30003/120000), AUC 0.5805001258850098
ep1_train_time 70.32288098335266
Test Epoch1 threshold 0.1 Acc 0.9169736842105263, AUC 0.9797488451004028, avg_entr 0.007698150351643562
ep1_t0.1_test_time 0.3989396095275879
Test Epoch1 threshold 0.2 Acc 0.9175, AUC 0.9811072945594788, avg_entr 0.012758941389620304
ep1_t0.2_test_time 0.3654606342315674
Test Epoch1 threshold 0.3 Acc 0.9188157894736843, AUC 0.9820711612701416, avg_entr 0.022309325635433197
ep1_t0.3_test_time 0.33971571922302246
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m4//ag_news_transformeral_l5_prefix.pt  ,ep 1
Test Epoch1 threshold 0.4 Acc 0.9186842105263158, AUC 0.9821256995201111, avg_entr 0.02373204007744789
ep1_t0.4_test_time 0.32561707496643066
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m4//ag_news_transformeral_l5_prefix.pt  ,ep 1
Test Epoch1 threshold 0.5 Acc 0.9180263157894737, AUC 0.9821606278419495, avg_entr 0.024530556052923203
ep1_t0.5_test_time 0.317352294921875
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m4//ag_news_transformeral_l5_prefix.pt  ,ep 1
Test Epoch1 threshold 0.6 Acc 0.9184210526315789, AUC 0.9821807146072388, avg_entr 0.024845516309142113
ep1_t0.6_test_time 0.31179308891296387
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m4//ag_news_transformeral_l5_prefix.pt  ,ep 1
Test Epoch1 threshold 0.7 Acc 0.9184210526315789, AUC 0.9821807146072388, avg_entr 0.024845516309142113
ep1_t0.7_test_time 0.311936616897583
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m4//ag_news_transformeral_l5_prefix.pt  ,ep 1
Test Epoch1 threshold 0.8 Acc 0.9184210526315789, AUC 0.9821807146072388, avg_entr 0.024845516309142113
ep1_t0.8_test_time 0.31253600120544434
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m4//ag_news_transformeral_l5_prefix.pt  ,ep 1
Test Epoch1 threshold 0.9 Acc 0.9184210526315789, AUC 0.9821807146072388, avg_entr 0.024845516309142113
ep1_t0.9_test_time 0.311657190322876
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m4//ag_news_transformeral_l5_prefix.pt  ,ep 1
gc 0
Train Epoch2 Acc 0.25001666666666666 (30002/120000), AUC 0.5813598036766052
ep2_train_time 70.54635405540466
Test Epoch2 threshold 0.1 Acc 0.9167105263157894, AUC 0.9794142246246338, avg_entr 0.0078061409294605255
ep2_t0.1_test_time 0.3994932174682617
Test Epoch2 threshold 0.2 Acc 0.9177631578947368, AUC 0.981063723564148, avg_entr 0.012823040597140789
ep2_t0.2_test_time 0.37593913078308105
Test Epoch2 threshold 0.3 Acc 0.9188157894736843, AUC 0.9820055961608887, avg_entr 0.02259129285812378
ep2_t0.3_test_time 0.33588194847106934
Test Epoch2 threshold 0.4 Acc 0.9186842105263158, AUC 0.9820830821990967, avg_entr 0.023791026324033737
ep2_t0.4_test_time 0.3232307434082031
Test Epoch2 threshold 0.5 Acc 0.9188157894736843, AUC 0.98215252161026, avg_entr 0.024606866762042046
ep2_t0.5_test_time 0.312391996383667
Test Epoch2 threshold 0.6 Acc 0.9188157894736843, AUC 0.9821579456329346, avg_entr 0.024924304336309433
ep2_t0.6_test_time 0.30927205085754395
Test Epoch2 threshold 0.7 Acc 0.9188157894736843, AUC 0.9821579456329346, avg_entr 0.024924304336309433
ep2_t0.7_test_time 0.30896568298339844
Test Epoch2 threshold 0.8 Acc 0.9188157894736843, AUC 0.9821579456329346, avg_entr 0.024924304336309433
ep2_t0.8_test_time 0.3089282512664795
Test Epoch2 threshold 0.9 Acc 0.9188157894736843, AUC 0.9821579456329346, avg_entr 0.024924304336309433
ep2_t0.9_test_time 0.3116457462310791
gc 0
Train Epoch3 Acc 0.24999166666666667 (29999/120000), AUC 0.5793477892875671
ep3_train_time 70.356693983078
Test Epoch3 threshold 0.1 Acc 0.9175, AUC 0.9794962406158447, avg_entr 0.007951945066452026
ep3_t0.1_test_time 0.39928483963012695
Test Epoch3 threshold 0.2 Acc 0.9185526315789474, AUC 0.9811065196990967, avg_entr 0.012792191468179226
ep3_t0.2_test_time 0.37419581413269043
Test Epoch3 threshold 0.3 Acc 0.9196052631578947, AUC 0.9820037484169006, avg_entr 0.022431114688515663
ep3_t0.3_test_time 0.33910131454467773
Test Epoch3 threshold 0.4 Acc 0.9192105263157895, AUC 0.9820923805236816, avg_entr 0.023556023836135864
ep3_t0.4_test_time 0.32239341735839844
Test Epoch3 threshold 0.5 Acc 0.9189473684210526, AUC 0.9821421504020691, avg_entr 0.024583863094449043
ep3_t0.5_test_time 0.31153225898742676
Test Epoch3 threshold 0.6 Acc 0.9189473684210526, AUC 0.9821604490280151, avg_entr 0.024749983102083206
ep3_t0.6_test_time 0.309293270111084
Test Epoch3 threshold 0.7 Acc 0.9189473684210526, AUC 0.9821604490280151, avg_entr 0.024749983102083206
ep3_t0.7_test_time 0.3103785514831543
Test Epoch3 threshold 0.8 Acc 0.9189473684210526, AUC 0.9821604490280151, avg_entr 0.024749983102083206
ep3_t0.8_test_time 0.31112003326416016
Test Epoch3 threshold 0.9 Acc 0.9189473684210526, AUC 0.9821604490280151, avg_entr 0.024749983102083206
ep3_t0.9_test_time 0.30905795097351074
gc 0
Train Epoch4 Acc 0.25000833333333333 (30001/120000), AUC 0.578431248664856
ep4_train_time 70.54274678230286
Test Epoch4 threshold 0.1 Acc 0.916578947368421, AUC 0.979354739189148, avg_entr 0.00789481308311224
ep4_t0.1_test_time 0.39851856231689453
Test Epoch4 threshold 0.2 Acc 0.9176315789473685, AUC 0.9809854626655579, avg_entr 0.013028671033680439
ep4_t0.2_test_time 0.3728668689727783
Test Epoch4 threshold 0.3 Acc 0.9181578947368421, AUC 0.981982409954071, avg_entr 0.022304583340883255
ep4_t0.3_test_time 0.33456873893737793
Test Epoch4 threshold 0.4 Acc 0.9184210526315789, AUC 0.9820923805236816, avg_entr 0.023787830024957657
ep4_t0.4_test_time 0.3198559284210205
Test Epoch4 threshold 0.5 Acc 0.9186842105263158, AUC 0.9821581840515137, avg_entr 0.02462751232087612
ep4_t0.5_test_time 0.3119683265686035
Test Epoch4 threshold 0.6 Acc 0.9186842105263158, AUC 0.9821498394012451, avg_entr 0.024878747761249542
ep4_t0.6_test_time 0.3100888729095459
Test Epoch4 threshold 0.7 Acc 0.9186842105263158, AUC 0.9821498394012451, avg_entr 0.024878747761249542
ep4_t0.7_test_time 0.30841517448425293
Test Epoch4 threshold 0.8 Acc 0.9186842105263158, AUC 0.9821498394012451, avg_entr 0.024878747761249542
ep4_t0.8_test_time 0.3083212375640869
Test Epoch4 threshold 0.9 Acc 0.9186842105263158, AUC 0.9821498394012451, avg_entr 0.024878747761249542
ep4_t0.9_test_time 0.307891845703125
gc 0
Train Epoch5 Acc 0.25000833333333333 (30001/120000), AUC 0.5785062909126282
ep5_train_time 70.40448188781738
Test Epoch5 threshold 0.1 Acc 0.9169736842105263, AUC 0.9793857336044312, avg_entr 0.007903086952865124
ep5_t0.1_test_time 0.4000091552734375
Test Epoch5 threshold 0.2 Acc 0.9173684210526316, AUC 0.9809403419494629, avg_entr 0.012749643996357918
ep5_t0.2_test_time 0.3740239143371582
Test Epoch5 threshold 0.3 Acc 0.9182894736842105, AUC 0.9819698929786682, avg_entr 0.02237914688885212
ep5_t0.3_test_time 0.3359050750732422
Test Epoch5 threshold 0.4 Acc 0.9185526315789474, AUC 0.9820876121520996, avg_entr 0.023788031190633774
ep5_t0.4_test_time 0.3218984603881836
Test Epoch5 threshold 0.5 Acc 0.9186842105263158, AUC 0.9821600317955017, avg_entr 0.024577844887971878
ep5_t0.5_test_time 0.31200194358825684
Test Epoch5 threshold 0.6 Acc 0.9185526315789474, AUC 0.982151448726654, avg_entr 0.024830535054206848
ep5_t0.6_test_time 0.30858945846557617
Test Epoch5 threshold 0.7 Acc 0.9185526315789474, AUC 0.982151448726654, avg_entr 0.024830535054206848
ep5_t0.7_test_time 0.309506893157959
Test Epoch5 threshold 0.8 Acc 0.9185526315789474, AUC 0.982151448726654, avg_entr 0.024830535054206848
ep5_t0.8_test_time 0.3095099925994873
Test Epoch5 threshold 0.9 Acc 0.9185526315789474, AUC 0.982151448726654, avg_entr 0.024830535054206848
ep5_t0.9_test_time 0.310028076171875
gc 0
Train Epoch6 Acc 0.25000833333333333 (30001/120000), AUC 0.5784889459609985
ep6_train_time 70.33357882499695
Test Epoch6 threshold 0.1 Acc 0.9167105263157894, AUC 0.9793668389320374, avg_entr 0.007872832007706165
ep6_t0.1_test_time 0.39966845512390137
Test Epoch6 threshold 0.2 Acc 0.9173684210526316, AUC 0.9809362888336182, avg_entr 0.012790808454155922
ep6_t0.2_test_time 0.3738369941711426
Test Epoch6 threshold 0.3 Acc 0.9182894736842105, AUC 0.9819705486297607, avg_entr 0.02238008938729763
ep6_t0.3_test_time 0.3367481231689453
Test Epoch6 threshold 0.4 Acc 0.9185526315789474, AUC 0.9820878505706787, avg_entr 0.023788172751665115
ep6_t0.4_test_time 0.3204538822174072
Test Epoch6 threshold 0.5 Acc 0.9186842105263158, AUC 0.9821600914001465, avg_entr 0.02457822673022747
ep6_t0.5_test_time 0.3116116523742676
Test Epoch6 threshold 0.6 Acc 0.9185526315789474, AUC 0.9821515083312988, avg_entr 0.02483062818646431
ep6_t0.6_test_time 0.3094632625579834
Test Epoch6 threshold 0.7 Acc 0.9185526315789474, AUC 0.9821515083312988, avg_entr 0.02483062818646431
ep6_t0.7_test_time 0.31033802032470703
Test Epoch6 threshold 0.8 Acc 0.9185526315789474, AUC 0.9821515083312988, avg_entr 0.02483062818646431
ep6_t0.8_test_time 0.3138096332550049
Test Epoch6 threshold 0.9 Acc 0.9185526315789474, AUC 0.9821515083312988, avg_entr 0.02483062818646431
ep6_t0.9_test_time 0.3085944652557373
gc 0
Train Epoch7 Acc 0.24999166666666667 (29999/120000), AUC 0.5783477425575256
ep7_train_time 70.30989599227905
Test Epoch7 threshold 0.1 Acc 0.9167105263157894, AUC 0.9793605804443359, avg_entr 0.007885828614234924
ep7_t0.1_test_time 0.40018343925476074
Test Epoch7 threshold 0.2 Acc 0.9173684210526316, AUC 0.9809356927871704, avg_entr 0.012789729051291943
ep7_t0.2_test_time 0.37636613845825195
Test Epoch7 threshold 0.3 Acc 0.9182894736842105, AUC 0.9819755554199219, avg_entr 0.022341031581163406
ep7_t0.3_test_time 0.33588743209838867
Test Epoch7 threshold 0.4 Acc 0.9185526315789474, AUC 0.9820871949195862, avg_entr 0.023788440972566605
ep7_t0.4_test_time 0.32019805908203125
Test Epoch7 threshold 0.5 Acc 0.9186842105263158, AUC 0.9821596145629883, avg_entr 0.02457839995622635
ep7_t0.5_test_time 0.3117837905883789
Test Epoch7 threshold 0.6 Acc 0.9185526315789474, AUC 0.9821510314941406, avg_entr 0.024830656126141548
ep7_t0.6_test_time 0.30854296684265137
Test Epoch7 threshold 0.7 Acc 0.9185526315789474, AUC 0.9821510314941406, avg_entr 0.024830656126141548
ep7_t0.7_test_time 0.30913519859313965
Test Epoch7 threshold 0.8 Acc 0.9185526315789474, AUC 0.9821510314941406, avg_entr 0.024830656126141548
ep7_t0.8_test_time 0.3083338737487793
Test Epoch7 threshold 0.9 Acc 0.9185526315789474, AUC 0.9821510314941406, avg_entr 0.024830656126141548
ep7_t0.9_test_time 0.31018853187561035
gc 0
Train Epoch8 Acc 0.24999166666666667 (29999/120000), AUC 0.5782541632652283
ep8_train_time 70.31876373291016
Test Epoch8 threshold 0.1 Acc 0.9167105263157894, AUC 0.9793612360954285, avg_entr 0.007884877733886242
ep8_t0.1_test_time 0.4013690948486328
Test Epoch8 threshold 0.2 Acc 0.9173684210526316, AUC 0.9809360504150391, avg_entr 0.012789112515747547
ep8_t0.2_test_time 0.3745386600494385
Test Epoch8 threshold 0.3 Acc 0.9182894736842105, AUC 0.981975793838501, avg_entr 0.02234121412038803
ep8_t0.3_test_time 0.33509254455566406
Test Epoch8 threshold 0.4 Acc 0.9185526315789474, AUC 0.9820873141288757, avg_entr 0.023788725957274437
ep8_t0.4_test_time 0.3202652931213379
Test Epoch8 threshold 0.5 Acc 0.9186842105263158, AUC 0.9821598529815674, avg_entr 0.024578535929322243
ep8_t0.5_test_time 0.3114156723022461
Test Epoch8 threshold 0.6 Acc 0.9185526315789474, AUC 0.982151210308075, avg_entr 0.02483062818646431
ep8_t0.6_test_time 0.30875396728515625
Test Epoch8 threshold 0.7 Acc 0.9185526315789474, AUC 0.982151210308075, avg_entr 0.02483062818646431
ep8_t0.7_test_time 0.30928850173950195
Test Epoch8 threshold 0.8 Acc 0.9185526315789474, AUC 0.982151210308075, avg_entr 0.02483062818646431
ep8_t0.8_test_time 0.3105580806732178
Test Epoch8 threshold 0.9 Acc 0.9185526315789474, AUC 0.982151210308075, avg_entr 0.02483062818646431
ep8_t0.9_test_time 0.30953478813171387
gc 0
Train Epoch9 Acc 0.25 (30000/120000), AUC 0.5784984827041626
ep9_train_time 70.37254238128662
Test Epoch9 threshold 0.1 Acc 0.9167105263157894, AUC 0.9793604612350464, avg_entr 0.007884609512984753
ep9_t0.1_test_time 0.39961838722229004
Test Epoch9 threshold 0.2 Acc 0.9172368421052631, AUC 0.9809359908103943, avg_entr 0.012788976542651653
ep9_t0.2_test_time 0.37464046478271484
Test Epoch9 threshold 0.3 Acc 0.9181578947368421, AUC 0.981975793838501, avg_entr 0.022341370582580566
ep9_t0.3_test_time 0.33731865882873535
Test Epoch9 threshold 0.4 Acc 0.9184210526315789, AUC 0.9820873737335205, avg_entr 0.023788930848240852
ep9_t0.4_test_time 0.3293783664703369
Test Epoch9 threshold 0.5 Acc 0.9185526315789474, AUC 0.9821598529815674, avg_entr 0.024578677490353584
ep9_t0.5_test_time 0.31351733207702637
Test Epoch9 threshold 0.6 Acc 0.9184210526315789, AUC 0.9821512699127197, avg_entr 0.024830684065818787
ep9_t0.6_test_time 0.3151726722717285
Test Epoch9 threshold 0.7 Acc 0.9184210526315789, AUC 0.9821512699127197, avg_entr 0.024830684065818787
ep9_t0.7_test_time 0.3141801357269287
Test Epoch9 threshold 0.8 Acc 0.9184210526315789, AUC 0.9821512699127197, avg_entr 0.024830684065818787
ep9_t0.8_test_time 0.3142578601837158
Test Epoch9 threshold 0.9 Acc 0.9184210526315789, AUC 0.9821512699127197, avg_entr 0.024830684065818787
ep9_t0.9_test_time 0.310560941696167
Best AUC 0.9821807146072388
train_loss (2, 5, 10)
valid_acc (10, 9)
valid_AUC (10, 9)
train_acc (10,)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Start Testing
Load ckpt at ckpt/ag_news_transformeral_l5_pad175_m4//ag_news_transformeral_l5_prefix.pt
[[1701   59   86   54]
 [  11 1869    8   12]
 [  42   21 1679  158]
 [  38   16  115 1731]]
Figure(640x480)
tensor([1.3472e-03, 5.0707e-08, 8.2744e-04,  ..., 2.9406e-02, 1.4937e-07,
        7.5967e-03])
[[1716   52   68   64]
 [  21 1856    8   15]
 [  53   16 1658  173]
 [  46   11  111 1732]]
Figure(640x480)
tensor([1.8668e-06, 3.6437e-08, 3.1738e-08,  ..., 2.1936e-07, 5.2344e-08,
        5.9004e-08])
[[1715   50   70   65]
 [  20 1853    9   18]
 [  50   16 1661  173]
 [  45   11  112 1732]]
Figure(640x480)
tensor([6.2864e-07, 3.8171e-08, 4.2726e-08,  ..., 3.4654e-07, 5.1346e-08,
        6.7732e-08])
[[1714   51   68   67]
 [  21 1851    9   19]
 [  48   16 1656  180]
 [  44   10  110 1736]]
Figure(640x480)
tensor([6.0840e-07, 4.5212e-08, 5.9635e-08,  ..., 3.3064e-07, 4.5367e-08,
        7.8371e-08])
[[   0    0    0 1900]
 [   0    0    0 1900]
 [   0    0    0 1900]
 [   0    0    0 1900]]
Figure(640x480)
tensor([0.1999, 0.3268, 0.3140,  ..., 0.2257, 0.1203, 0.1570])
