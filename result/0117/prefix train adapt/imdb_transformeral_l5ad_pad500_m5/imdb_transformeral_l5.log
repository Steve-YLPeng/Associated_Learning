total count words 222751
vocab size 30000
found 27937 words in glove
Load ckpt from ckpt/imdb_transformeral_l5_pad500_m4//imdb_transformeral_l5_prefix.pt
model: TransformerModelML(
  (layers): ModuleList(
    (0): EMBLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Embedding(30000, 300)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=2, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=2, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.weight 9000000
layers.0.ae.g.0.weight 256
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 256
layers.0.ae.h.0.bias 2
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
Total Trainable Params: 13671778
Start Training
train_mask {0, 1, 2, 3, 4}
gc 9
Train Epoch0 Acc 0.93065 (37226/40000), AUC 0.9713425636291504
ep0_train_time 92.33950090408325
Test Epoch0 threshold 0.1 Acc 0.9448, AUC 0.9756084084510803, avg_entr 0.010036580264568329
ep0_t0.1_test_time 1.4234306812286377
Save ckpt to ckpt/imdb_transformeral_l5_pad500_m5//imdb_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.2 Acc 0.9445, AUC 0.976314127445221, avg_entr 0.01285391952842474
ep0_t0.2_test_time 1.374321699142456
Save ckpt to ckpt/imdb_transformeral_l5_pad500_m5//imdb_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.3 Acc 0.944, AUC 0.9770455360412598, avg_entr 0.016439011320471764
ep0_t0.3_test_time 1.332568883895874
Save ckpt to ckpt/imdb_transformeral_l5_pad500_m5//imdb_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.4 Acc 0.9436, AUC 0.9777002334594727, avg_entr 0.019871357828378677
ep0_t0.4_test_time 1.2798659801483154
Save ckpt to ckpt/imdb_transformeral_l5_pad500_m5//imdb_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.5 Acc 0.9438, AUC 0.9783175587654114, avg_entr 0.024671513587236404
ep0_t0.5_test_time 1.248840570449829
Save ckpt to ckpt/imdb_transformeral_l5_pad500_m5//imdb_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.6 Acc 0.9435, AUC 0.9787343740463257, avg_entr 0.029024096205830574
ep0_t0.6_test_time 1.229706048965454
Save ckpt to ckpt/imdb_transformeral_l5_pad500_m5//imdb_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.7 Acc 0.9432, AUC 0.9791262149810791, avg_entr 0.03416925296187401
ep0_t0.7_test_time 1.2338104248046875
Save ckpt to ckpt/imdb_transformeral_l5_pad500_m5//imdb_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.8 Acc 0.9417, AUC 0.9796706438064575, avg_entr 0.03965437784790993
ep0_t0.8_test_time 1.2268989086151123
Save ckpt to ckpt/imdb_transformeral_l5_pad500_m5//imdb_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.9 Acc 0.9411, AUC 0.9801024794578552, avg_entr 0.04625282809138298
ep0_t0.9_test_time 1.2558424472808838
Save ckpt to ckpt/imdb_transformeral_l5_pad500_m5//imdb_transformeral_l5_prefix.pt  ,ep 0
gc 0
Train Epoch1 Acc 0.95345 (38138/40000), AUC 0.984099268913269
ep1_train_time 92.0274863243103
Test Epoch1 threshold 0.1 Acc 0.9403, AUC 0.973077654838562, avg_entr 0.010386746376752853
ep1_t0.1_test_time 1.3989553451538086
Test Epoch1 threshold 0.2 Acc 0.9387, AUC 0.973544716835022, avg_entr 0.013471679762005806
ep1_t0.2_test_time 1.3423666954040527
Test Epoch1 threshold 0.3 Acc 0.9369, AUC 0.9750738143920898, avg_entr 0.0169345922768116
ep1_t0.3_test_time 1.3033638000488281
Test Epoch1 threshold 0.4 Acc 0.9358, AUC 0.9763777256011963, avg_entr 0.020125584676861763
ep1_t0.4_test_time 1.268204927444458
Test Epoch1 threshold 0.5 Acc 0.9344, AUC 0.9769951701164246, avg_entr 0.0239158533513546
ep1_t0.5_test_time 1.2385590076446533
Test Epoch1 threshold 0.6 Acc 0.933, AUC 0.9779284000396729, avg_entr 0.027856357395648956
ep1_t0.6_test_time 1.2131662368774414
Test Epoch1 threshold 0.7 Acc 0.9317, AUC 0.9783844947814941, avg_entr 0.03252841904759407
ep1_t0.7_test_time 1.196364402770996
Test Epoch1 threshold 0.8 Acc 0.9301, AUC 0.9792322516441345, avg_entr 0.03781733289361
ep1_t0.8_test_time 1.1779286861419678
Test Epoch1 threshold 0.9 Acc 0.9278, AUC 0.979720413684845, avg_entr 0.0454874150454998
ep1_t0.9_test_time 1.1576917171478271
gc 0
Train Epoch2 Acc 0.96275 (38510/40000), AUC 0.9882743954658508
ep2_train_time 92.12684035301208
Test Epoch2 threshold 0.1 Acc 0.9405, AUC 0.9698443412780762, avg_entr 0.011012792587280273
ep2_t0.1_test_time 1.4031178951263428
Test Epoch2 threshold 0.2 Acc 0.9401, AUC 0.9712378978729248, avg_entr 0.013443771749734879
ep2_t0.2_test_time 1.341444730758667
Test Epoch2 threshold 0.3 Acc 0.9401, AUC 0.9723128080368042, avg_entr 0.01607525907456875
ep2_t0.3_test_time 1.3291397094726562
Test Epoch2 threshold 0.4 Acc 0.9388, AUC 0.9729905128479004, avg_entr 0.019461655989289284
ep2_t0.4_test_time 1.2719309329986572
Test Epoch2 threshold 0.5 Acc 0.939, AUC 0.9736389517784119, avg_entr 0.023528767749667168
ep2_t0.5_test_time 1.2535300254821777
Test Epoch2 threshold 0.6 Acc 0.9384, AUC 0.9744572043418884, avg_entr 0.027984045445919037
ep2_t0.6_test_time 1.2404704093933105
Test Epoch2 threshold 0.7 Acc 0.939, AUC 0.9751909971237183, avg_entr 0.03274206072092056
ep2_t0.7_test_time 1.2253782749176025
Test Epoch2 threshold 0.8 Acc 0.9383, AUC 0.9764630794525146, avg_entr 0.03824618458747864
ep2_t0.8_test_time 1.1870832443237305
Test Epoch2 threshold 0.9 Acc 0.9376, AUC 0.97681725025177, avg_entr 0.043491169810295105
ep2_t0.9_test_time 1.1686718463897705
gc 0
Train Epoch3 Acc 0.96445 (38578/40000), AUC 0.9887274503707886
ep3_train_time 92.17614197731018
Test Epoch3 threshold 0.1 Acc 0.9402, AUC 0.9699856638908386, avg_entr 0.00938008539378643
ep3_t0.1_test_time 1.4217684268951416
Test Epoch3 threshold 0.2 Acc 0.9395, AUC 0.9717422723770142, avg_entr 0.012515364214777946
ep3_t0.2_test_time 1.337188720703125
Test Epoch3 threshold 0.3 Acc 0.939, AUC 0.973060667514801, avg_entr 0.016019731760025024
ep3_t0.3_test_time 1.3018395900726318
Test Epoch3 threshold 0.4 Acc 0.9394, AUC 0.9740894436836243, avg_entr 0.01891830377280712
ep3_t0.4_test_time 1.29392671585083
Test Epoch3 threshold 0.5 Acc 0.9382, AUC 0.974907398223877, avg_entr 0.02236878126859665
ep3_t0.5_test_time 1.2561771869659424
Test Epoch3 threshold 0.6 Acc 0.9377, AUC 0.9755035042762756, avg_entr 0.025896307080984116
ep3_t0.6_test_time 1.2343263626098633
Test Epoch3 threshold 0.7 Acc 0.9372, AUC 0.9761677384376526, avg_entr 0.030811486765742302
ep3_t0.7_test_time 1.2123243808746338
Test Epoch3 threshold 0.8 Acc 0.9364, AUC 0.9770208597183228, avg_entr 0.037325695157051086
ep3_t0.8_test_time 1.1882681846618652
Test Epoch3 threshold 0.9 Acc 0.9357, AUC 0.9776817560195923, avg_entr 0.043972574174404144
ep3_t0.9_test_time 1.166562557220459
gc 0
Train Epoch4 Acc 0.964825 (38593/40000), AUC 0.9889105558395386
ep4_train_time 92.03726959228516
Test Epoch4 threshold 0.1 Acc 0.9403, AUC 0.9695185422897339, avg_entr 0.009629711508750916
ep4_t0.1_test_time 1.4016034603118896
Test Epoch4 threshold 0.2 Acc 0.9401, AUC 0.97080397605896, avg_entr 0.01287731435149908
ep4_t0.2_test_time 1.3456075191497803
Test Epoch4 threshold 0.3 Acc 0.9398, AUC 0.9718547463417053, avg_entr 0.015609100461006165
ep4_t0.3_test_time 1.3104965686798096
Test Epoch4 threshold 0.4 Acc 0.9387, AUC 0.9727358818054199, avg_entr 0.019600413739681244
ep4_t0.4_test_time 1.2984657287597656
Test Epoch4 threshold 0.5 Acc 0.9388, AUC 0.9733623266220093, avg_entr 0.023994306102395058
ep4_t0.5_test_time 1.2647831439971924
Test Epoch4 threshold 0.6 Acc 0.9385, AUC 0.974391758441925, avg_entr 0.027559448033571243
ep4_t0.6_test_time 1.2270658016204834
Test Epoch4 threshold 0.7 Acc 0.9391, AUC 0.975656270980835, avg_entr 0.031890105456113815
ep4_t0.7_test_time 1.2176494598388672
Test Epoch4 threshold 0.8 Acc 0.9384, AUC 0.9761503338813782, avg_entr 0.037055570632219315
ep4_t0.8_test_time 1.206336498260498
Test Epoch4 threshold 0.9 Acc 0.9376, AUC 0.9767431020736694, avg_entr 0.04237842932343483
ep4_t0.9_test_time 1.1708660125732422
gc 0
Train Epoch5 Acc 0.9647 (38588/40000), AUC 0.9890632629394531
ep5_train_time 92.08617043495178
Test Epoch5 threshold 0.1 Acc 0.9406, AUC 0.9697092771530151, avg_entr 0.009622001089155674
ep5_t0.1_test_time 1.4384479522705078
Test Epoch5 threshold 0.2 Acc 0.9406, AUC 0.9708552956581116, avg_entr 0.012753801420331001
ep5_t0.2_test_time 1.33341646194458
Test Epoch5 threshold 0.3 Acc 0.94, AUC 0.9718911647796631, avg_entr 0.01580333709716797
ep5_t0.3_test_time 1.3334016799926758
Test Epoch5 threshold 0.4 Acc 0.9389, AUC 0.9728606939315796, avg_entr 0.019308509305119514
ep5_t0.4_test_time 1.2804913520812988
Test Epoch5 threshold 0.5 Acc 0.9386, AUC 0.9738878607749939, avg_entr 0.023371921852231026
ep5_t0.5_test_time 1.2574450969696045
Test Epoch5 threshold 0.6 Acc 0.9386, AUC 0.9748412370681763, avg_entr 0.027796905487775803
ep5_t0.6_test_time 1.2256419658660889
Test Epoch5 threshold 0.7 Acc 0.9386, AUC 0.9752794504165649, avg_entr 0.03210519999265671
ep5_t0.7_test_time 1.205031394958496
Test Epoch5 threshold 0.8 Acc 0.9381, AUC 0.9759255647659302, avg_entr 0.037085603922605515
ep5_t0.8_test_time 1.1872036457061768
Test Epoch5 threshold 0.9 Acc 0.9374, AUC 0.9767061471939087, avg_entr 0.04265256226062775
ep5_t0.9_test_time 1.1700849533081055
gc 0
Train Epoch6 Acc 0.9652 (38608/40000), AUC 0.9889614582061768
ep6_train_time 92.4338858127594
Test Epoch6 threshold 0.1 Acc 0.9406, AUC 0.9697141051292419, avg_entr 0.00959948729723692
ep6_t0.1_test_time 1.4035577774047852
Test Epoch6 threshold 0.2 Acc 0.9406, AUC 0.9708786010742188, avg_entr 0.012700854800641537
ep6_t0.2_test_time 1.3339710235595703
Test Epoch6 threshold 0.3 Acc 0.9401, AUC 0.9719144105911255, avg_entr 0.01565762609243393
ep6_t0.3_test_time 1.2998509407043457
Test Epoch6 threshold 0.4 Acc 0.9389, AUC 0.9728795289993286, avg_entr 0.019299417734146118
ep6_t0.4_test_time 1.2758171558380127
Test Epoch6 threshold 0.5 Acc 0.9385, AUC 0.9737871885299683, avg_entr 0.023458924144506454
ep6_t0.5_test_time 1.2447988986968994
Test Epoch6 threshold 0.6 Acc 0.9386, AUC 0.9748446941375732, avg_entr 0.027783969417214394
ep6_t0.6_test_time 1.224137544631958
Test Epoch6 threshold 0.7 Acc 0.9386, AUC 0.9754936695098877, avg_entr 0.03225436806678772
ep6_t0.7_test_time 1.2055790424346924
Test Epoch6 threshold 0.8 Acc 0.9381, AUC 0.9759267568588257, avg_entr 0.03701644763350487
ep6_t0.8_test_time 1.1832246780395508
Test Epoch6 threshold 0.9 Acc 0.9373, AUC 0.97669517993927, avg_entr 0.04245581105351448
ep6_t0.9_test_time 1.1667144298553467
gc 0
Train Epoch7 Acc 0.96465 (38586/40000), AUC 0.989063024520874
ep7_train_time 92.03662443161011
Test Epoch7 threshold 0.1 Acc 0.9407, AUC 0.9697195291519165, avg_entr 0.00959757436066866
ep7_t0.1_test_time 1.3954215049743652
Test Epoch7 threshold 0.2 Acc 0.9407, AUC 0.9708839058876038, avg_entr 0.012688309885561466
ep7_t0.2_test_time 1.3389294147491455
Test Epoch7 threshold 0.3 Acc 0.9402, AUC 0.9719118475914001, avg_entr 0.015647152438759804
ep7_t0.3_test_time 1.2991538047790527
Test Epoch7 threshold 0.4 Acc 0.939, AUC 0.9728825092315674, avg_entr 0.01930762641131878
ep7_t0.4_test_time 1.278855562210083
Test Epoch7 threshold 0.5 Acc 0.9386, AUC 0.9737905263900757, avg_entr 0.023399392142891884
ep7_t0.5_test_time 1.2441658973693848
Test Epoch7 threshold 0.6 Acc 0.9387, AUC 0.9748466610908508, avg_entr 0.027781710028648376
ep7_t0.6_test_time 1.2268579006195068
Test Epoch7 threshold 0.7 Acc 0.9387, AUC 0.9754953384399414, avg_entr 0.03222131356596947
ep7_t0.7_test_time 1.211193561553955
Test Epoch7 threshold 0.8 Acc 0.9382, AUC 0.9759275913238525, avg_entr 0.03701063618063927
ep7_t0.8_test_time 1.2698030471801758
Test Epoch7 threshold 0.9 Acc 0.9374, AUC 0.9766966104507446, avg_entr 0.04246191680431366
ep7_t0.9_test_time 1.2450437545776367
gc 0
Train Epoch8 Acc 0.965225 (38609/40000), AUC 0.9889825582504272
ep8_train_time 92.13891363143921
Test Epoch8 threshold 0.1 Acc 0.9408, AUC 0.9697253704071045, avg_entr 0.009590428322553635
ep8_t0.1_test_time 1.4020190238952637
Test Epoch8 threshold 0.2 Acc 0.9408, AUC 0.9708856344223022, avg_entr 0.01268422044813633
ep8_t0.2_test_time 1.336130142211914
Test Epoch8 threshold 0.3 Acc 0.9402, AUC 0.9719154834747314, avg_entr 0.01569693721830845
ep8_t0.3_test_time 1.2965784072875977
Test Epoch8 threshold 0.4 Acc 0.9391, AUC 0.9728772640228271, avg_entr 0.019278110936284065
ep8_t0.4_test_time 1.2736048698425293
Test Epoch8 threshold 0.5 Acc 0.9387, AUC 0.9737919569015503, avg_entr 0.023421933874487877
ep8_t0.5_test_time 1.24599027633667
Test Epoch8 threshold 0.6 Acc 0.9388, AUC 0.974847674369812, avg_entr 0.02778330072760582
ep8_t0.6_test_time 1.2560718059539795
Test Epoch8 threshold 0.7 Acc 0.9388, AUC 0.9754968881607056, avg_entr 0.032215412706136703
ep8_t0.7_test_time 1.2788012027740479
Test Epoch8 threshold 0.8 Acc 0.9383, AUC 0.9759286046028137, avg_entr 0.03700580447912216
ep8_t0.8_test_time 1.2074041366577148
Test Epoch8 threshold 0.9 Acc 0.9374, AUC 0.9766978025436401, avg_entr 0.042459096759557724
ep8_t0.9_test_time 1.1999425888061523
gc 0
Train Epoch9 Acc 0.965025 (38601/40000), AUC 0.9889353513717651
ep9_train_time 92.0332407951355
Test Epoch9 threshold 0.1 Acc 0.9408, AUC 0.9697306752204895, avg_entr 0.009586630389094353
ep9_t0.1_test_time 1.405031681060791
Test Epoch9 threshold 0.2 Acc 0.9408, AUC 0.9708901643753052, avg_entr 0.012686516158282757
ep9_t0.2_test_time 1.3609883785247803
Test Epoch9 threshold 0.3 Acc 0.9402, AUC 0.9719210863113403, avg_entr 0.015692921355366707
ep9_t0.3_test_time 1.314995527267456
Test Epoch9 threshold 0.4 Acc 0.9391, AUC 0.9728792905807495, avg_entr 0.019256174564361572
ep9_t0.4_test_time 1.2994282245635986
Test Epoch9 threshold 0.5 Acc 0.9387, AUC 0.9737957119941711, avg_entr 0.023418070748448372
ep9_t0.5_test_time 1.2527575492858887
Test Epoch9 threshold 0.6 Acc 0.9388, AUC 0.9748426675796509, avg_entr 0.027832938358187675
ep9_t0.6_test_time 1.2320327758789062
Test Epoch9 threshold 0.7 Acc 0.9388, AUC 0.9754987955093384, avg_entr 0.03220897540450096
ep9_t0.7_test_time 1.1997272968292236
Test Epoch9 threshold 0.8 Acc 0.9384, AUC 0.9759301543235779, avg_entr 0.03697466105222702
ep9_t0.8_test_time 1.1854863166809082
Test Epoch9 threshold 0.9 Acc 0.9374, AUC 0.976699709892273, avg_entr 0.04245625436306
ep9_t0.9_test_time 1.1719772815704346
Best AUC 0.9801024794578552
train_loss (2, 5, 10)
valid_acc (10, 9)
valid_AUC (10, 9)
train_acc (10,)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Start Testing
Load ckpt at ckpt/imdb_transformeral_l5_pad500_m5//imdb_transformeral_l5_prefix.pt
[[4721  353]
 [ 272 4654]]
Figure(640x480)
tensor([8.1817e-03, 2.9575e-08, 6.6055e-06,  ..., 4.9171e-07, 1.8028e-07,
        5.9526e-04])
[[4764  310]
 [ 222 4704]]
Figure(640x480)
tensor([2.6525e-07, 1.0410e-08, 3.3997e-08,  ..., 1.8034e-08, 1.2438e-08,
        5.1478e-09])
[[4726  348]
 [ 196 4730]]
Figure(640x480)
tensor([3.0523e-07, 2.5359e-08, 4.9588e-08,  ..., 2.7027e-08, 2.8367e-08,
        3.2123e-08])
[[4724  350]
 [ 196 4730]]
Figure(640x480)
tensor([1.6504e-07, 2.1666e-08, 5.6484e-08,  ..., 2.7177e-08, 2.7072e-08,
        3.0408e-08])
[[4704  370]
 [ 183 4743]]
Figure(640x480)
tensor([4.9981e-07, 1.2320e-08, 4.1084e-08,  ..., 3.0693e-08, 1.2808e-08,
        1.7041e-08])
