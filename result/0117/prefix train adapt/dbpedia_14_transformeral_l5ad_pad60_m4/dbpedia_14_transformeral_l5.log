total count words 887881
vocab size 30000
found 28354 words in glove
Load ckpt from ckpt/dbpedia_14_transformeral_l5_pad60_m3//dbpedia_14_transformeral_l5_prefix.pt
model: TransformerModelML(
  (layers): ModuleList(
    (0): EMBLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Embedding(30000, 300)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=14, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=14, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.weight 9000000
layers.0.ae.g.0.weight 1792
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 1792
layers.0.ae.h.0.bias 14
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
Total Trainable Params: 13674862
Start Training
train_mask {0, 1, 2, 3}
gc 9
Train Epoch0 Acc 0.036701785714285715 (20553/560000), AUC 0.5391730070114136
ep0_train_time 137.4942286014557
Test Epoch0 threshold 0.1 Acc 0.9758, AUC 0.9982427954673767, avg_entr 0.004139999859035015
ep0_t0.1_test_time 1.8641786575317383
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m4//dbpedia_14_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.2 Acc 0.9744428571428572, AUC 0.9984225034713745, avg_entr 0.008824475109577179
ep0_t0.2_test_time 1.5328114032745361
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m4//dbpedia_14_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.3 Acc 0.9741857142857143, AUC 0.9984198808670044, avg_entr 0.009103337302803993
ep0_t0.3_test_time 1.455904483795166
Test Epoch0 threshold 0.4 Acc 0.9741857142857143, AUC 0.9984198808670044, avg_entr 0.009103337302803993
ep0_t0.4_test_time 1.4610729217529297
Test Epoch0 threshold 0.5 Acc 0.9741857142857143, AUC 0.9984198808670044, avg_entr 0.009103337302803993
ep0_t0.5_test_time 1.462146520614624
Test Epoch0 threshold 0.6 Acc 0.9741857142857143, AUC 0.9984198808670044, avg_entr 0.009103337302803993
ep0_t0.6_test_time 1.4922761917114258
Test Epoch0 threshold 0.7 Acc 0.9741857142857143, AUC 0.9984198808670044, avg_entr 0.009103337302803993
ep0_t0.7_test_time 1.4821045398712158
Test Epoch0 threshold 0.8 Acc 0.9741857142857143, AUC 0.9984198808670044, avg_entr 0.009103337302803993
ep0_t0.8_test_time 1.4849543571472168
Test Epoch0 threshold 0.9 Acc 0.9741857142857143, AUC 0.9984198808670044, avg_entr 0.009103337302803993
ep0_t0.9_test_time 1.4632081985473633
gc 0
Train Epoch1 Acc 0.07163571428571429 (40116/560000), AUC 0.5216864943504333
ep1_train_time 135.5319013595581
Test Epoch1 threshold 0.1 Acc 0.9761142857142857, AUC 0.9982879757881165, avg_entr 0.004176775924861431
ep1_t0.1_test_time 1.874281883239746
Test Epoch1 threshold 0.2 Acc 0.9750142857142857, AUC 0.998450756072998, avg_entr 0.008880391716957092
ep1_t0.2_test_time 1.5305397510528564
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m4//dbpedia_14_transformeral_l5_prefix.pt  ,ep 1
Test Epoch1 threshold 0.3 Acc 0.9747142857142858, AUC 0.9984513521194458, avg_entr 0.009155446663498878
ep1_t0.3_test_time 1.466766595840454
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m4//dbpedia_14_transformeral_l5_prefix.pt  ,ep 1
Test Epoch1 threshold 0.4 Acc 0.9747142857142858, AUC 0.9984513521194458, avg_entr 0.009155446663498878
ep1_t0.4_test_time 1.48630690574646
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m4//dbpedia_14_transformeral_l5_prefix.pt  ,ep 1
Test Epoch1 threshold 0.5 Acc 0.9747142857142858, AUC 0.9984513521194458, avg_entr 0.009155446663498878
ep1_t0.5_test_time 1.4555892944335938
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m4//dbpedia_14_transformeral_l5_prefix.pt  ,ep 1
Test Epoch1 threshold 0.6 Acc 0.9747142857142858, AUC 0.9984513521194458, avg_entr 0.009155446663498878
ep1_t0.6_test_time 1.4580652713775635
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m4//dbpedia_14_transformeral_l5_prefix.pt  ,ep 1
Test Epoch1 threshold 0.7 Acc 0.9747142857142858, AUC 0.9984513521194458, avg_entr 0.009155446663498878
ep1_t0.7_test_time 1.5290207862854004
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m4//dbpedia_14_transformeral_l5_prefix.pt  ,ep 1
Test Epoch1 threshold 0.8 Acc 0.9747142857142858, AUC 0.9984513521194458, avg_entr 0.009155446663498878
ep1_t0.8_test_time 1.4756431579589844
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m4//dbpedia_14_transformeral_l5_prefix.pt  ,ep 1
Test Epoch1 threshold 0.9 Acc 0.9747142857142858, AUC 0.9984513521194458, avg_entr 0.009155446663498878
ep1_t0.9_test_time 1.4713211059570312
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m4//dbpedia_14_transformeral_l5_prefix.pt  ,ep 1
gc 0
Train Epoch2 Acc 0.07148035714285714 (40029/560000), AUC 0.524495005607605
ep2_train_time 135.78168511390686
Test Epoch2 threshold 0.1 Acc 0.9764571428571429, AUC 0.9982629418373108, avg_entr 0.004102849867194891
ep2_t0.1_test_time 1.8976960182189941
Test Epoch2 threshold 0.2 Acc 0.9752142857142857, AUC 0.9984560012817383, avg_entr 0.008817159570753574
ep2_t0.2_test_time 1.5323717594146729
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m4//dbpedia_14_transformeral_l5_prefix.pt  ,ep 2
Test Epoch2 threshold 0.3 Acc 0.9749285714285715, AUC 0.9984549283981323, avg_entr 0.009116067551076412
ep2_t0.3_test_time 1.4857077598571777
Test Epoch2 threshold 0.4 Acc 0.9749285714285715, AUC 0.9984549283981323, avg_entr 0.009116067551076412
ep2_t0.4_test_time 1.4693238735198975
Test Epoch2 threshold 0.5 Acc 0.9749285714285715, AUC 0.9984549283981323, avg_entr 0.009116067551076412
ep2_t0.5_test_time 1.4632205963134766
Test Epoch2 threshold 0.6 Acc 0.9749285714285715, AUC 0.9984549283981323, avg_entr 0.009116067551076412
ep2_t0.6_test_time 1.4831132888793945
Test Epoch2 threshold 0.7 Acc 0.9749285714285715, AUC 0.9984549283981323, avg_entr 0.009116067551076412
ep2_t0.7_test_time 1.4630928039550781
Test Epoch2 threshold 0.8 Acc 0.9749285714285715, AUC 0.9984549283981323, avg_entr 0.009116067551076412
ep2_t0.8_test_time 1.4707860946655273
Test Epoch2 threshold 0.9 Acc 0.9749285714285715, AUC 0.9984549283981323, avg_entr 0.009116067551076412
ep2_t0.9_test_time 1.4624197483062744
gc 0
Train Epoch3 Acc 0.07148392857142857 (40031/560000), AUC 0.5238715410232544
ep3_train_time 136.26071500778198
Test Epoch3 threshold 0.1 Acc 0.9763, AUC 0.998277485370636, avg_entr 0.004141473211348057
ep3_t0.1_test_time 1.865412712097168
Test Epoch3 threshold 0.2 Acc 0.9750428571428571, AUC 0.9984537959098816, avg_entr 0.008871547877788544
ep3_t0.2_test_time 1.532585859298706
Test Epoch3 threshold 0.3 Acc 0.9747428571428571, AUC 0.9984517693519592, avg_entr 0.009162742644548416
ep3_t0.3_test_time 1.4721062183380127
Test Epoch3 threshold 0.4 Acc 0.9747428571428571, AUC 0.9984517693519592, avg_entr 0.009162742644548416
ep3_t0.4_test_time 1.4711551666259766
Test Epoch3 threshold 0.5 Acc 0.9747428571428571, AUC 0.9984517693519592, avg_entr 0.009162742644548416
ep3_t0.5_test_time 1.450355052947998
Test Epoch3 threshold 0.6 Acc 0.9747428571428571, AUC 0.9984517693519592, avg_entr 0.009162742644548416
ep3_t0.6_test_time 1.4707319736480713
Test Epoch3 threshold 0.7 Acc 0.9747428571428571, AUC 0.9984517693519592, avg_entr 0.009162742644548416
ep3_t0.7_test_time 1.4653651714324951
Test Epoch3 threshold 0.8 Acc 0.9747428571428571, AUC 0.9984517693519592, avg_entr 0.009162742644548416
ep3_t0.8_test_time 1.462829828262329
Test Epoch3 threshold 0.9 Acc 0.9747428571428571, AUC 0.9984517693519592, avg_entr 0.009162742644548416
ep3_t0.9_test_time 1.4784142971038818
gc 0
Train Epoch4 Acc 0.0714875 (40033/560000), AUC 0.5235403180122375
ep4_train_time 135.8662030696869
Test Epoch4 threshold 0.1 Acc 0.9763142857142857, AUC 0.9982896447181702, avg_entr 0.004142962861806154
ep4_t0.1_test_time 1.8562753200531006
Test Epoch4 threshold 0.2 Acc 0.9751142857142857, AUC 0.998452365398407, avg_entr 0.00885620154440403
ep4_t0.2_test_time 1.5216248035430908
Test Epoch4 threshold 0.3 Acc 0.9748285714285714, AUC 0.9984502792358398, avg_entr 0.009142058901488781
ep4_t0.3_test_time 1.4705238342285156
Test Epoch4 threshold 0.4 Acc 0.9748285714285714, AUC 0.9984502792358398, avg_entr 0.009142058901488781
ep4_t0.4_test_time 1.4739770889282227
Test Epoch4 threshold 0.5 Acc 0.9748285714285714, AUC 0.9984502792358398, avg_entr 0.009142058901488781
ep4_t0.5_test_time 1.4853253364562988
Test Epoch4 threshold 0.6 Acc 0.9748285714285714, AUC 0.9984502792358398, avg_entr 0.009142058901488781
ep4_t0.6_test_time 1.4768383502960205
Test Epoch4 threshold 0.7 Acc 0.9748285714285714, AUC 0.9984502792358398, avg_entr 0.009142058901488781
ep4_t0.7_test_time 1.471581220626831
Test Epoch4 threshold 0.8 Acc 0.9748285714285714, AUC 0.9984502792358398, avg_entr 0.009142058901488781
ep4_t0.8_test_time 1.4627017974853516
Test Epoch4 threshold 0.9 Acc 0.9748285714285714, AUC 0.9984502792358398, avg_entr 0.009142058901488781
ep4_t0.9_test_time 1.4692299365997314
gc 0
Train Epoch5 Acc 0.07147678571428571 (40027/560000), AUC 0.523688018321991
ep5_train_time 135.95169854164124
Test Epoch5 threshold 0.1 Acc 0.9763428571428572, AUC 0.998288631439209, avg_entr 0.004153508692979813
ep5_t0.1_test_time 1.8567776679992676
Test Epoch5 threshold 0.2 Acc 0.9751, AUC 0.9984521865844727, avg_entr 0.008857779204845428
ep5_t0.2_test_time 1.5151307582855225
Test Epoch5 threshold 0.3 Acc 0.9748142857142857, AUC 0.9984501600265503, avg_entr 0.009143656119704247
ep5_t0.3_test_time 1.4903919696807861
Test Epoch5 threshold 0.4 Acc 0.9748142857142857, AUC 0.9984501600265503, avg_entr 0.009143656119704247
ep5_t0.4_test_time 1.4598491191864014
Test Epoch5 threshold 0.5 Acc 0.9748142857142857, AUC 0.9984501600265503, avg_entr 0.009143656119704247
ep5_t0.5_test_time 1.4647326469421387
Test Epoch5 threshold 0.6 Acc 0.9748142857142857, AUC 0.9984501600265503, avg_entr 0.009143656119704247
ep5_t0.6_test_time 1.462113857269287
Test Epoch5 threshold 0.7 Acc 0.9748142857142857, AUC 0.9984501600265503, avg_entr 0.009143656119704247
ep5_t0.7_test_time 1.464406967163086
Test Epoch5 threshold 0.8 Acc 0.9748142857142857, AUC 0.9984501600265503, avg_entr 0.009143656119704247
ep5_t0.8_test_time 1.459029197692871
Test Epoch5 threshold 0.9 Acc 0.9748142857142857, AUC 0.9984501600265503, avg_entr 0.009143656119704247
ep5_t0.9_test_time 1.4756100177764893
gc 0
Train Epoch6 Acc 0.07149821428571429 (40039/560000), AUC 0.5235225558280945
ep6_train_time 136.1063051223755
Test Epoch6 threshold 0.1 Acc 0.9763428571428572, AUC 0.9982888102531433, avg_entr 0.004150304943323135
ep6_t0.1_test_time 1.8745548725128174
Test Epoch6 threshold 0.2 Acc 0.9751, AUC 0.9984521865844727, avg_entr 0.008858749642968178
ep6_t0.2_test_time 1.5313568115234375
Test Epoch6 threshold 0.3 Acc 0.9748285714285714, AUC 0.9984502792358398, avg_entr 0.009141817688941956
ep6_t0.3_test_time 1.464919090270996
Test Epoch6 threshold 0.4 Acc 0.9748285714285714, AUC 0.9984502792358398, avg_entr 0.009141817688941956
ep6_t0.4_test_time 1.460143804550171
Test Epoch6 threshold 0.5 Acc 0.9748285714285714, AUC 0.9984502792358398, avg_entr 0.009141817688941956
ep6_t0.5_test_time 1.4673750400543213
Test Epoch6 threshold 0.6 Acc 0.9748285714285714, AUC 0.9984502792358398, avg_entr 0.009141817688941956
ep6_t0.6_test_time 1.4738657474517822
Test Epoch6 threshold 0.7 Acc 0.9748285714285714, AUC 0.9984502792358398, avg_entr 0.009141817688941956
ep6_t0.7_test_time 1.4739880561828613
Test Epoch6 threshold 0.8 Acc 0.9748285714285714, AUC 0.9984502792358398, avg_entr 0.009141817688941956
ep6_t0.8_test_time 1.4757652282714844
Test Epoch6 threshold 0.9 Acc 0.9748285714285714, AUC 0.9984502792358398, avg_entr 0.009141817688941956
ep6_t0.9_test_time 1.5082459449768066
gc 0
Train Epoch7 Acc 0.07148035714285714 (40029/560000), AUC 0.523378312587738
ep7_train_time 136.12682819366455
Test Epoch7 threshold 0.1 Acc 0.9763571428571428, AUC 0.9982887506484985, avg_entr 0.004149341490119696
ep7_t0.1_test_time 1.880641222000122
Test Epoch7 threshold 0.2 Acc 0.9751142857142857, AUC 0.9984521865844727, avg_entr 0.008858554065227509
ep7_t0.2_test_time 1.52724289894104
Test Epoch7 threshold 0.3 Acc 0.9748428571428571, AUC 0.9984502792358398, avg_entr 0.009141629561781883
ep7_t0.3_test_time 1.4818713665008545
Test Epoch7 threshold 0.4 Acc 0.9748428571428571, AUC 0.9984502792358398, avg_entr 0.009141629561781883
ep7_t0.4_test_time 1.4775750637054443
Test Epoch7 threshold 0.5 Acc 0.9748428571428571, AUC 0.9984502792358398, avg_entr 0.009141629561781883
ep7_t0.5_test_time 1.4677231311798096
Test Epoch7 threshold 0.6 Acc 0.9748428571428571, AUC 0.9984502792358398, avg_entr 0.009141629561781883
ep7_t0.6_test_time 1.4582388401031494
Test Epoch7 threshold 0.7 Acc 0.9748428571428571, AUC 0.9984502792358398, avg_entr 0.009141629561781883
ep7_t0.7_test_time 1.4654955863952637
Test Epoch7 threshold 0.8 Acc 0.9748428571428571, AUC 0.9984502792358398, avg_entr 0.009141629561781883
ep7_t0.8_test_time 1.4649386405944824
Test Epoch7 threshold 0.9 Acc 0.9748428571428571, AUC 0.9984502792358398, avg_entr 0.009141629561781883
ep7_t0.9_test_time 1.5157504081726074
gc 0
Train Epoch8 Acc 0.0715125 (40047/560000), AUC 0.5235317945480347
ep8_train_time 136.02523970603943
Test Epoch8 threshold 0.1 Acc 0.9763571428571428, AUC 0.9982888102531433, avg_entr 0.004147910512983799
ep8_t0.1_test_time 1.8362390995025635
Test Epoch8 threshold 0.2 Acc 0.9751142857142857, AUC 0.9984522461891174, avg_entr 0.008855634368956089
ep8_t0.2_test_time 1.5239944458007812
Test Epoch8 threshold 0.3 Acc 0.9748428571428571, AUC 0.9984502792358398, avg_entr 0.009141557849943638
ep8_t0.3_test_time 1.4561865329742432
Test Epoch8 threshold 0.4 Acc 0.9748428571428571, AUC 0.9984502792358398, avg_entr 0.009141557849943638
ep8_t0.4_test_time 1.4708530902862549
Test Epoch8 threshold 0.5 Acc 0.9748428571428571, AUC 0.9984502792358398, avg_entr 0.009141557849943638
ep8_t0.5_test_time 1.4626891613006592
Test Epoch8 threshold 0.6 Acc 0.9748428571428571, AUC 0.9984502792358398, avg_entr 0.009141557849943638
ep8_t0.6_test_time 1.4581363201141357
Test Epoch8 threshold 0.7 Acc 0.9748428571428571, AUC 0.9984502792358398, avg_entr 0.009141557849943638
ep8_t0.7_test_time 1.4585785865783691
Test Epoch8 threshold 0.8 Acc 0.9748428571428571, AUC 0.9984502792358398, avg_entr 0.009141557849943638
ep8_t0.8_test_time 1.466170310974121
Test Epoch8 threshold 0.9 Acc 0.9748428571428571, AUC 0.9984502792358398, avg_entr 0.009141557849943638
ep8_t0.9_test_time 1.4545891284942627
gc 0
Train Epoch9 Acc 0.07148571428571429 (40032/560000), AUC 0.5236364006996155
ep9_train_time 136.3100516796112
Test Epoch9 threshold 0.1 Acc 0.9763571428571428, AUC 0.9982887506484985, avg_entr 0.004147965461015701
ep9_t0.1_test_time 1.8603501319885254
Test Epoch9 threshold 0.2 Acc 0.9751142857142857, AUC 0.9984522461891174, avg_entr 0.008855589665472507
ep9_t0.2_test_time 1.5221130847930908
Test Epoch9 threshold 0.3 Acc 0.9748428571428571, AUC 0.9984502792358398, avg_entr 0.009141515009105206
ep9_t0.3_test_time 1.45513916015625
Test Epoch9 threshold 0.4 Acc 0.9748428571428571, AUC 0.9984502792358398, avg_entr 0.009141515009105206
ep9_t0.4_test_time 1.459702730178833
Test Epoch9 threshold 0.5 Acc 0.9748428571428571, AUC 0.9984502792358398, avg_entr 0.009141515009105206
ep9_t0.5_test_time 1.4604449272155762
Test Epoch9 threshold 0.6 Acc 0.9748428571428571, AUC 0.9984502792358398, avg_entr 0.009141515009105206
ep9_t0.6_test_time 1.4706673622131348
Test Epoch9 threshold 0.7 Acc 0.9748428571428571, AUC 0.9984502792358398, avg_entr 0.009141515009105206
ep9_t0.7_test_time 1.4766521453857422
Test Epoch9 threshold 0.8 Acc 0.9748428571428571, AUC 0.9984502792358398, avg_entr 0.009141515009105206
ep9_t0.8_test_time 1.4609708786010742
Test Epoch9 threshold 0.9 Acc 0.9748428571428571, AUC 0.9984502792358398, avg_entr 0.009141515009105206
ep9_t0.9_test_time 1.4516525268554688
Best AUC 0.9984560012817383
train_loss (2, 5, 10)
valid_acc (10, 9)
valid_AUC (10, 9)
train_acc (10,)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Start Testing
Load ckpt at ckpt/dbpedia_14_transformeral_l5_pad60_m4//dbpedia_14_transformeral_l5_prefix.pt
[[4720   40   22   13   12   61   39    7    3    5    4   16   16   42]
 [  34 4902    1    1    7    0   31    6    3    2    1    0    4    8]
 [  36   12 4634   17   52    2   10    3    4    1    1   74   22  132]
 [   2    2   26 4955   11    0    1    0    0    0    0    0    0    3]
 [  10   18   70   12 4859    8    6    1    2    0    0    0    1   13]
 [  38    1    3    3    1 4942    5    2    1    2    0    0    1    1]
 [  64   43    7    3   10   16 4798   37    9    5    1    1    5    1]
 [   0    1    0    0    2    0   12 4962   18    3    1    0    0    1]
 [   1    2    2    0    4    0   10   12 4969    0    0    0    0    0]
 [   1    0    1    2    0    0    0    7    0 4954   34    0    0    1]
 [  12    1    0    0    0    0    2    3    0   32 4949    1    0    0]
 [   6    0   39    2    0    0    0    0    0    1    2 4925   11   14]
 [   8    2   23    4    1    3    1    3    1    2    0   20 4888   44]
 [  32    3   82    7   13    3    3    9    1    5    3    7   44 4788]]
Figure(640x480)
tensor([1.2407e-07, 1.6060e-02, 2.0984e-04,  ..., 9.9260e-08, 1.8048e-03,
        8.5199e-06])
[[4757   42   21    4   12   42   42    3    0    4    2   17   14   40]
 [  35 4914    1    0   10    1   30    0    1    1    1    0    0    6]
 [  29   11 4740   11   69    1    6    0    0    1    2   41   16   73]
 [   4    1   18 4960   11    2    0    0    0    0    0    1    1    2]
 [   6    7   63    8 4891    7    6    1    2    2    0    0    2    5]
 [  43    0    0    0    1 4945    4    3    1    0    0    0    1    2]
 [  53   35    6    1    7   11 4848   24    4    4    0    1    2    4]
 [   2    1    0    0    1    0   17 4962   11    4    0    0    1    1]
 [   2    2    1    0    5    0   12   12 4966    0    0    0    0    0]
 [   1    0    2    1    0    0    0    4    0 4965   26    0    0    1]
 [  12    1    0    0    0    1    1    2    0   34 4949    0    0    0]
 [   6    0   36    2    0    0    0    0    0    0    0 4939   12    5]
 [  10    1   16    0    0    0    0    1    0    0    0   22 4906   44]
 [  32    3   80    3    6    3    4    1    0    0    3    7   49 4809]]
Figure(640x480)
tensor([6.9136e-08, 3.6398e-01, 9.2603e-08,  ..., 7.0454e-08, 1.0620e-07,
        9.3994e-08])
[[4763   39   21    4   12   43   44    5    0    2    2   15   12   38]
 [  34 4912    1    0   10    1   32    0    2    1    1    0    0    6]
 [  29    9 4747   11   69    1    6    0    1    1    1   41   16   68]
 [   4    1   16 4960   12    2    0    1    0    0    0    1    1    2]
 [   6    7   63    9 4889    6    6    1    2    4    0    0    2    5]
 [  42    0    1    0    1 4945    4    3    1    0    0    0    1    2]
 [  53   36    4    1    5   12 4852   23    4    3    0    1    3    3]
 [   2    1    0    0    1    0   17 4962   11    4    0    0    1    1]
 [   3    2    1    0    3    0   10   12 4969    0    0    0    0    0]
 [   1    0    2    1    0    0    0    4    0 4966   25    0    0    1]
 [  12    1    1    0    0    1    1    1    0   31 4952    0    0    0]
 [   6    0   35    2    0    0    0    0    0    0    0 4940   12    5]
 [  11    1   15    0    0    0    0    1    0    0    0   22 4909   41]
 [  36    3   80    3    5    3    4    2    0    0    2    7   46 4809]]
Figure(640x480)
tensor([1.0904e-07, 6.5786e-04, 1.1420e-07,  ..., 1.0925e-07, 1.0454e-07,
        1.0653e-07])
[[4770   38   21    4   11   42   43    4    0    2    2   14   12   37]
 [  34 4912    1    0   10    1   32    0    2    1    1    0    0    6]
 [  29    9 4748   11   69    1    6    0    1    1    1   41   16   67]
 [   4    1   16 4960   12    2    0    1    0    0    0    1    1    2]
 [   6    7   63    9 4889    5    6    1    2    4    0    0    3    5]
 [  42    0    1    0    1 4945    4    3    1    0    0    0    1    2]
 [  54   36    4    1    6   12 4850   23    4    3    0    1    3    3]
 [   2    1    0    0    1    0   18 4961   11    4    0    0    1    1]
 [   3    2    1    0    3    0   11   12 4968    0    0    0    0    0]
 [   1    0    2    1    0    0    0    4    0 4966   25    0    0    1]
 [  12    1    1    0    0    1    1    1    0   31 4952    0    0    0]
 [   7    0   35    2    0    0    0    0    0    0    0 4939   12    5]
 [  10    1   15    0    0    0    0    1    0    0    0   22 4910   41]
 [  36    2   80    3    6    3    4    2    0    0    2    9   47 4806]]
Figure(640x480)
tensor([1.3773e-07, 1.3796e-04, 1.4064e-07,  ..., 1.5297e-07, 1.5197e-07,
        1.6313e-07])
[[   0    0    1    3    1    0    0    0    0 4966    0    0    0   29]
 [   0    0    0    0    0    0    0   12    0  117    0    0    0 4871]
 [   0    0    0 4226    4    0    0    4    0  762    0    0    0    4]
 [   0    0    0   12    0    0    0 4570    0  418    0    0    0    0]
 [   0    0    0   22    0    0    0    3    0 4969    0    0    0    6]
 [   0    0    0    0    0    0    0    0    0 5000    0    0    0    0]
 [   0    0   11    0    1    0    0    0    0 4963    0    0    0   25]
 [   0    0 4893    0    0    0    0    0    0  106    0    0    0    1]
 [   0    0    8    1    0    0    0    1    0 4990    0    0    0    0]
 [   0    0    1    0    0    0    0    0    0 4999    0    0    0    0]
 [   0    0    0    0    0    0    0    0    0 4999    0    0    0    1]
 [   0    0    0   10    5    0    0    1    0 4984    0    0    0    0]
 [   0    0    0  238 4611    0    0    0    0  151    0    0    0    0]
 [   0    0    0   29   17    0    0    0    0 4952    0    0    0    2]]
Figure(640x480)
tensor([1.0221, 1.3533, 1.0017,  ..., 1.2580, 1.2370, 1.2649])
