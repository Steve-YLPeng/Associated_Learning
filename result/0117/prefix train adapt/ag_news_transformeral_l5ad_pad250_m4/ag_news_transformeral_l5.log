total count words 102019
vocab size 30000
found 26754 words in glove
Load ckpt from ckpt/ag_news_transformeral_l5_pad250_m3//ag_news_transformeral_l5_prefix.pt
model: TransformerModelML(
  (layers): ModuleList(
    (0): EMBLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Embedding(30000, 300)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=4, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=4, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.weight 9000000
layers.0.ae.g.0.weight 512
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 512
layers.0.ae.h.0.bias 4
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
Total Trainable Params: 13672292
Start Training
train_mask {0, 1, 2, 3}
gc 9
Train Epoch0 Acc 0.24590833333333334 (29509/120000), AUC 0.47050368785858154
ep0_train_time 101.97783374786377
Test Epoch0 threshold 0.1 Acc 0.9185526315789474, AUC 0.9799220561981201, avg_entr 0.008063922636210918
ep0_t0.1_test_time 0.5445756912231445
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m4//ag_news_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.2 Acc 0.9189473684210526, AUC 0.9802337288856506, avg_entr 0.013551752083003521
ep0_t0.2_test_time 0.506725549697876
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m4//ag_news_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.3 Acc 0.9185526315789474, AUC 0.9818699359893799, avg_entr 0.02388877421617508
ep0_t0.3_test_time 0.4654958248138428
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m4//ag_news_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.4 Acc 0.9180263157894737, AUC 0.9818544983863831, avg_entr 0.025684529915452003
ep0_t0.4_test_time 0.44567346572875977
Test Epoch0 threshold 0.5 Acc 0.9185526315789474, AUC 0.9819320440292358, avg_entr 0.026838338002562523
ep0_t0.5_test_time 0.4351370334625244
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m4//ag_news_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.6 Acc 0.9184210526315789, AUC 0.9819328784942627, avg_entr 0.02711522951722145
ep0_t0.6_test_time 0.43758392333984375
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m4//ag_news_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.7 Acc 0.9184210526315789, AUC 0.9819328784942627, avg_entr 0.02711522951722145
ep0_t0.7_test_time 0.43914055824279785
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m4//ag_news_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.8 Acc 0.9184210526315789, AUC 0.9819328784942627, avg_entr 0.02711522951722145
ep0_t0.8_test_time 0.43338918685913086
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m4//ag_news_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.9 Acc 0.9184210526315789, AUC 0.9819328784942627, avg_entr 0.02711522951722145
ep0_t0.9_test_time 0.43334531784057617
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m4//ag_news_transformeral_l5_prefix.pt  ,ep 0
gc 0
Train Epoch1 Acc 0.24624166666666666 (29549/120000), AUC 0.4915465712547302
ep1_train_time 101.81690287590027
Test Epoch1 threshold 0.1 Acc 0.9196052631578947, AUC 0.9795668125152588, avg_entr 0.007780492305755615
ep1_t0.1_test_time 0.539863109588623
Test Epoch1 threshold 0.2 Acc 0.9196052631578947, AUC 0.9807213544845581, avg_entr 0.013402157463133335
ep1_t0.2_test_time 0.5063285827636719
Test Epoch1 threshold 0.3 Acc 0.9201315789473684, AUC 0.9820939898490906, avg_entr 0.023824462667107582
ep1_t0.3_test_time 0.46942877769470215
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m4//ag_news_transformeral_l5_prefix.pt  ,ep 1
Test Epoch1 threshold 0.4 Acc 0.9198684210526316, AUC 0.9820886850357056, avg_entr 0.025384120643138885
ep1_t0.4_test_time 0.4499201774597168
Test Epoch1 threshold 0.5 Acc 0.9193421052631578, AUC 0.9820461273193359, avg_entr 0.026319120079278946
ep1_t0.5_test_time 0.4373738765716553
Test Epoch1 threshold 0.6 Acc 0.9188157894736843, AUC 0.982032835483551, avg_entr 0.02649173140525818
ep1_t0.6_test_time 0.4335789680480957
Test Epoch1 threshold 0.7 Acc 0.9188157894736843, AUC 0.982032835483551, avg_entr 0.02649173140525818
ep1_t0.7_test_time 0.4378819465637207
Test Epoch1 threshold 0.8 Acc 0.9188157894736843, AUC 0.982032835483551, avg_entr 0.02649173140525818
ep1_t0.8_test_time 0.4358537197113037
Test Epoch1 threshold 0.9 Acc 0.9188157894736843, AUC 0.982032835483551, avg_entr 0.02649173140525818
ep1_t0.9_test_time 0.4344499111175537
gc 0
Train Epoch2 Acc 0.2464 (29568/120000), AUC 0.5216699838638306
ep2_train_time 101.80764102935791
Test Epoch2 threshold 0.1 Acc 0.9186842105263158, AUC 0.9795462489128113, avg_entr 0.007835183292627335
ep2_t0.1_test_time 0.5406069755554199
Test Epoch2 threshold 0.2 Acc 0.9192105263157895, AUC 0.9809956550598145, avg_entr 0.013041858561336994
ep2_t0.2_test_time 0.5076255798339844
Test Epoch2 threshold 0.3 Acc 0.9192105263157895, AUC 0.9820044040679932, avg_entr 0.02346448227763176
ep2_t0.3_test_time 0.4634099006652832
Test Epoch2 threshold 0.4 Acc 0.9193421052631578, AUC 0.9820930361747742, avg_entr 0.0251853559166193
ep2_t0.4_test_time 0.44390320777893066
Test Epoch2 threshold 0.5 Acc 0.9188157894736843, AUC 0.9820957183837891, avg_entr 0.02596588246524334
ep2_t0.5_test_time 0.43467235565185547
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m4//ag_news_transformeral_l5_prefix.pt  ,ep 2
Test Epoch2 threshold 0.6 Acc 0.9188157894736843, AUC 0.9820743799209595, avg_entr 0.026245545595884323
ep2_t0.6_test_time 0.43506836891174316
Test Epoch2 threshold 0.7 Acc 0.9188157894736843, AUC 0.9820743799209595, avg_entr 0.026245545595884323
ep2_t0.7_test_time 0.43220043182373047
Test Epoch2 threshold 0.8 Acc 0.9188157894736843, AUC 0.9820743799209595, avg_entr 0.026245545595884323
ep2_t0.8_test_time 0.4318413734436035
Test Epoch2 threshold 0.9 Acc 0.9188157894736843, AUC 0.9820743799209595, avg_entr 0.026245545595884323
ep2_t0.9_test_time 0.4313924312591553
gc 0
Train Epoch3 Acc 0.246475 (29577/120000), AUC 0.5285298824310303
ep3_train_time 101.86532711982727
Test Epoch3 threshold 0.1 Acc 0.9184210526315789, AUC 0.9795088768005371, avg_entr 0.008193330839276314
ep3_t0.1_test_time 0.540125846862793
Test Epoch3 threshold 0.2 Acc 0.9188157894736843, AUC 0.9809224605560303, avg_entr 0.012696126475930214
ep3_t0.2_test_time 0.5000948905944824
Test Epoch3 threshold 0.3 Acc 0.9197368421052632, AUC 0.9820417165756226, avg_entr 0.023313848301768303
ep3_t0.3_test_time 0.4648895263671875
Test Epoch3 threshold 0.4 Acc 0.9194736842105263, AUC 0.9820727109909058, avg_entr 0.024895846843719482
ep3_t0.4_test_time 0.4411032199859619
Test Epoch3 threshold 0.5 Acc 0.9189473684210526, AUC 0.9821022152900696, avg_entr 0.02574571780860424
ep3_t0.5_test_time 0.4324512481689453
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m4//ag_news_transformeral_l5_prefix.pt  ,ep 3
Test Epoch3 threshold 0.6 Acc 0.9189473684210526, AUC 0.9820892810821533, avg_entr 0.02592514455318451
ep3_t0.6_test_time 0.43335771560668945
Test Epoch3 threshold 0.7 Acc 0.9189473684210526, AUC 0.9820892810821533, avg_entr 0.02592514455318451
ep3_t0.7_test_time 0.4304163455963135
Test Epoch3 threshold 0.8 Acc 0.9189473684210526, AUC 0.9820892810821533, avg_entr 0.02592514455318451
ep3_t0.8_test_time 0.4296457767486572
Test Epoch3 threshold 0.9 Acc 0.9189473684210526, AUC 0.9820892810821533, avg_entr 0.02592514455318451
ep3_t0.9_test_time 0.43078112602233887
gc 0
Train Epoch4 Acc 0.24656666666666666 (29588/120000), AUC 0.5305417776107788
ep4_train_time 101.97166585922241
Test Epoch4 threshold 0.1 Acc 0.9186842105263158, AUC 0.9793374538421631, avg_entr 0.008003258146345615
ep4_t0.1_test_time 0.5381171703338623
Test Epoch4 threshold 0.2 Acc 0.9190789473684211, AUC 0.9807747602462769, avg_entr 0.013042760081589222
ep4_t0.2_test_time 0.505556583404541
Test Epoch4 threshold 0.3 Acc 0.9196052631578947, AUC 0.9820426106452942, avg_entr 0.023345964029431343
ep4_t0.3_test_time 0.46722912788391113
Test Epoch4 threshold 0.4 Acc 0.9194736842105263, AUC 0.982062578201294, avg_entr 0.024908635765314102
ep4_t0.4_test_time 0.4484133720397949
Test Epoch4 threshold 0.5 Acc 0.9190789473684211, AUC 0.9820972681045532, avg_entr 0.025886695832014084
ep4_t0.5_test_time 0.43396639823913574
Test Epoch4 threshold 0.6 Acc 0.9190789473684211, AUC 0.982088565826416, avg_entr 0.026009228080511093
ep4_t0.6_test_time 0.4325687885284424
Test Epoch4 threshold 0.7 Acc 0.9190789473684211, AUC 0.982088565826416, avg_entr 0.026009228080511093
ep4_t0.7_test_time 0.43151068687438965
Test Epoch4 threshold 0.8 Acc 0.9190789473684211, AUC 0.982088565826416, avg_entr 0.026009228080511093
ep4_t0.8_test_time 0.43436360359191895
Test Epoch4 threshold 0.9 Acc 0.9190789473684211, AUC 0.982088565826416, avg_entr 0.026009228080511093
ep4_t0.9_test_time 0.43132472038269043
gc 0
Train Epoch5 Acc 0.24649166666666666 (29579/120000), AUC 0.5313464999198914
ep5_train_time 101.93495631217957
Test Epoch5 threshold 0.1 Acc 0.9185526315789474, AUC 0.9793633222579956, avg_entr 0.007953956723213196
ep5_t0.1_test_time 0.5377295017242432
Test Epoch5 threshold 0.2 Acc 0.9192105263157895, AUC 0.980820894241333, avg_entr 0.012939192354679108
ep5_t0.2_test_time 0.5060679912567139
Test Epoch5 threshold 0.3 Acc 0.9198684210526316, AUC 0.9820457100868225, avg_entr 0.02332429401576519
ep5_t0.3_test_time 0.46248340606689453
Test Epoch5 threshold 0.4 Acc 0.9197368421052632, AUC 0.9820727705955505, avg_entr 0.024835992604494095
ep5_t0.4_test_time 0.4427785873413086
Test Epoch5 threshold 0.5 Acc 0.9193421052631578, AUC 0.9821047782897949, avg_entr 0.025805549696087837
ep5_t0.5_test_time 0.43171238899230957
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m4//ag_news_transformeral_l5_prefix.pt  ,ep 5
Test Epoch5 threshold 0.6 Acc 0.9193421052631578, AUC 0.9820918440818787, avg_entr 0.025989040732383728
ep5_t0.6_test_time 0.4373636245727539
Test Epoch5 threshold 0.7 Acc 0.9193421052631578, AUC 0.9820918440818787, avg_entr 0.025989040732383728
ep5_t0.7_test_time 0.44074487686157227
Test Epoch5 threshold 0.8 Acc 0.9193421052631578, AUC 0.9820918440818787, avg_entr 0.025989040732383728
ep5_t0.8_test_time 0.44020509719848633
Test Epoch5 threshold 0.9 Acc 0.9193421052631578, AUC 0.9820918440818787, avg_entr 0.025989040732383728
ep5_t0.9_test_time 0.4396383762359619
gc 0
Train Epoch6 Acc 0.24655 (29586/120000), AUC 0.531596302986145
ep6_train_time 101.83639097213745
Test Epoch6 threshold 0.1 Acc 0.9186842105263158, AUC 0.9793636202812195, avg_entr 0.007956495508551598
ep6_t0.1_test_time 0.5435969829559326
Test Epoch6 threshold 0.2 Acc 0.9194736842105263, AUC 0.9808231592178345, avg_entr 0.012932902202010155
ep6_t0.2_test_time 0.5075516700744629
Test Epoch6 threshold 0.3 Acc 0.9198684210526316, AUC 0.9820463061332703, avg_entr 0.023330720141530037
ep6_t0.3_test_time 0.46859073638916016
Test Epoch6 threshold 0.4 Acc 0.9197368421052632, AUC 0.9820731282234192, avg_entr 0.024841144680976868
ep6_t0.4_test_time 0.4510064125061035
Test Epoch6 threshold 0.5 Acc 0.9193421052631578, AUC 0.9821053743362427, avg_entr 0.02581038884818554
ep6_t0.5_test_time 0.43764591217041016
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m4//ag_news_transformeral_l5_prefix.pt  ,ep 6
Test Epoch6 threshold 0.6 Acc 0.9193421052631578, AUC 0.9820923805236816, avg_entr 0.025993868708610535
ep6_t0.6_test_time 0.4340951442718506
Test Epoch6 threshold 0.7 Acc 0.9193421052631578, AUC 0.9820923805236816, avg_entr 0.025993868708610535
ep6_t0.7_test_time 0.43297672271728516
Test Epoch6 threshold 0.8 Acc 0.9193421052631578, AUC 0.9820923805236816, avg_entr 0.025993868708610535
ep6_t0.8_test_time 0.43228769302368164
Test Epoch6 threshold 0.9 Acc 0.9193421052631578, AUC 0.9820923805236816, avg_entr 0.025993868708610535
ep6_t0.9_test_time 0.4336686134338379
gc 0
Train Epoch7 Acc 0.24645833333333333 (29575/120000), AUC 0.5317291617393494
ep7_train_time 101.90676856040955
Test Epoch7 threshold 0.1 Acc 0.9185526315789474, AUC 0.9793635010719299, avg_entr 0.007956732995808125
ep7_t0.1_test_time 0.5371122360229492
Test Epoch7 threshold 0.2 Acc 0.9193421052631578, AUC 0.9808233380317688, avg_entr 0.012932687066495419
ep7_t0.2_test_time 0.5040669441223145
Test Epoch7 threshold 0.3 Acc 0.9198684210526316, AUC 0.9820461869239807, avg_entr 0.02333035133779049
ep7_t0.3_test_time 0.46645283699035645
Test Epoch7 threshold 0.4 Acc 0.9197368421052632, AUC 0.9820730686187744, avg_entr 0.0248411875218153
ep7_t0.4_test_time 0.4455118179321289
Test Epoch7 threshold 0.5 Acc 0.9193421052631578, AUC 0.9821052551269531, avg_entr 0.025810327380895615
ep7_t0.5_test_time 0.4319596290588379
Test Epoch7 threshold 0.6 Acc 0.9193421052631578, AUC 0.9820923209190369, avg_entr 0.025993837043642998
ep7_t0.6_test_time 0.43104982376098633
Test Epoch7 threshold 0.7 Acc 0.9193421052631578, AUC 0.9820923209190369, avg_entr 0.025993837043642998
ep7_t0.7_test_time 0.4313685894012451
Test Epoch7 threshold 0.8 Acc 0.9193421052631578, AUC 0.9820923209190369, avg_entr 0.025993837043642998
ep7_t0.8_test_time 0.4322628974914551
Test Epoch7 threshold 0.9 Acc 0.9193421052631578, AUC 0.9820923209190369, avg_entr 0.025993837043642998
ep7_t0.9_test_time 0.43188905715942383
gc 0
Train Epoch8 Acc 0.24645833333333333 (29575/120000), AUC 0.5318180322647095
ep8_train_time 101.78850364685059
Test Epoch8 threshold 0.1 Acc 0.9186842105263158, AUC 0.9793630838394165, avg_entr 0.007957326248288155
ep8_t0.1_test_time 0.5369248390197754
Test Epoch8 threshold 0.2 Acc 0.9194736842105263, AUC 0.9808233380317688, avg_entr 0.012933081947267056
ep8_t0.2_test_time 0.5055210590362549
Test Epoch8 threshold 0.3 Acc 0.92, AUC 0.9820462465286255, avg_entr 0.0233305711299181
ep8_t0.3_test_time 0.4649207592010498
Test Epoch8 threshold 0.4 Acc 0.9197368421052632, AUC 0.9820730686187744, avg_entr 0.024841487407684326
ep8_t0.4_test_time 0.44477343559265137
Test Epoch8 threshold 0.5 Acc 0.9193421052631578, AUC 0.9821052551269531, avg_entr 0.02581038512289524
ep8_t0.5_test_time 0.433884859085083
Test Epoch8 threshold 0.6 Acc 0.9193421052631578, AUC 0.9820922613143921, avg_entr 0.02599390409886837
ep8_t0.6_test_time 0.43197202682495117
Test Epoch8 threshold 0.7 Acc 0.9193421052631578, AUC 0.9820922613143921, avg_entr 0.02599390409886837
ep8_t0.7_test_time 0.43379902839660645
Test Epoch8 threshold 0.8 Acc 0.9193421052631578, AUC 0.9820922613143921, avg_entr 0.02599390409886837
ep8_t0.8_test_time 0.4331059455871582
Test Epoch8 threshold 0.9 Acc 0.9193421052631578, AUC 0.9820922613143921, avg_entr 0.02599390409886837
ep8_t0.9_test_time 0.4336123466491699
gc 0
Train Epoch9 Acc 0.24651666666666666 (29582/120000), AUC 0.5317067503929138
ep9_train_time 101.79726028442383
Test Epoch9 threshold 0.1 Acc 0.9186842105263158, AUC 0.9793632626533508, avg_entr 0.007957828231155872
ep9_t0.1_test_time 0.5409245491027832
Test Epoch9 threshold 0.2 Acc 0.9194736842105263, AUC 0.9808236360549927, avg_entr 0.01293319370597601
ep9_t0.2_test_time 0.5072526931762695
Test Epoch9 threshold 0.3 Acc 0.92, AUC 0.9820463061332703, avg_entr 0.023330524563789368
ep9_t0.3_test_time 0.4716532230377197
Test Epoch9 threshold 0.4 Acc 0.9197368421052632, AUC 0.9820731282234192, avg_entr 0.024841642007231712
ep9_t0.4_test_time 0.44584131240844727
Test Epoch9 threshold 0.5 Acc 0.9193421052631578, AUC 0.9821053743362427, avg_entr 0.025810351595282555
ep9_t0.5_test_time 0.4349861145019531
Save ckpt to ckpt/ag_news_transformeral_l5_pad250_m4//ag_news_transformeral_l5_prefix.pt  ,ep 9
Test Epoch9 threshold 0.6 Acc 0.9193421052631578, AUC 0.9820923805236816, avg_entr 0.02599388360977173
ep9_t0.6_test_time 0.43619871139526367
Test Epoch9 threshold 0.7 Acc 0.9193421052631578, AUC 0.9820923805236816, avg_entr 0.02599388360977173
ep9_t0.7_test_time 0.433305025100708
Test Epoch9 threshold 0.8 Acc 0.9193421052631578, AUC 0.9820923805236816, avg_entr 0.02599388360977173
ep9_t0.8_test_time 0.4333019256591797
Test Epoch9 threshold 0.9 Acc 0.9193421052631578, AUC 0.9820923805236816, avg_entr 0.02599388360977173
ep9_t0.9_test_time 0.43311643600463867
Best AUC 0.9821053743362427
train_loss (2, 5, 10)
valid_acc (10, 9)
valid_AUC (10, 9)
train_acc (10,)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Start Testing
Load ckpt at ckpt/ag_news_transformeral_l5_pad250_m4//ag_news_transformeral_l5_prefix.pt
[[1705   61   85   49]
 [   8 1875    8    9]
 [  44   20 1687  149]
 [  47   15  118 1720]]
Figure(640x480)
tensor([3.0161e-04, 4.6005e-08, 3.5540e-04,  ..., 1.5985e-02, 1.9356e-07,
        3.5035e-02])
[[1722   54   67   57]
 [  16 1863   10   11]
 [  52   18 1679  151]
 [  49   10  130 1711]]
Figure(640x480)
tensor([1.1335e-06, 4.8581e-08, 4.0043e-08,  ..., 2.1329e-07, 4.4695e-08,
        3.7402e-08])
[[1725   52   66   57]
 [  20 1858   10   12]
 [  52   18 1680  150]
 [  48   10  132 1710]]
Figure(640x480)
tensor([2.3455e-07, 4.6931e-08, 5.3204e-08,  ..., 2.1051e-07, 4.0512e-08,
        4.3053e-08])
[[1726   51   66   57]
 [  17 1861   10   12]
 [  52   18 1678  152]
 [  48    9  132 1711]]
Figure(640x480)
tensor([2.6182e-07, 4.9002e-08, 4.9377e-08,  ..., 2.1564e-07, 4.2255e-08,
        4.5630e-08])
[[1855    0   45    0]
 [  78    0 1822    0]
 [1890    0   10    0]
 [1893    0    7    0]]
Figure(640x480)
tensor([0.3991, 0.0937, 0.1004,  ..., 0.6778, 0.4046, 0.4133])
