total count words 887881
vocab size 30000
found 28354 words in glove
Load ckpt from ckpt/dbpedia_14_transformeral_l5_pad60_m4//dbpedia_14_transformeral_l5_prefix.pt
model: TransformerModelML(
  (layers): ModuleList(
    (0): EMBLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Embedding(30000, 300)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=14, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=14, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.weight 9000000
layers.0.ae.g.0.weight 1792
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 1792
layers.0.ae.h.0.bias 14
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
Total Trainable Params: 13674862
Start Training
train_mask {0, 1, 2, 3, 4}
gc 9
Train Epoch0 Acc 0.9613875 (538377/560000), AUC 0.9969727396965027
ep0_train_time 155.36075139045715
Test Epoch0 threshold 0.1 Acc 0.9762, AUC 0.998293399810791, avg_entr 0.004093735478818417
ep0_t0.1_test_time 1.8814139366149902
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m5//dbpedia_14_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.2 Acc 0.9749285714285715, AUC 0.9984530210494995, avg_entr 0.008812475949525833
ep0_t0.2_test_time 1.5101802349090576
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60_m5//dbpedia_14_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.3 Acc 0.9746857142857143, AUC 0.9984521269798279, avg_entr 0.009085959754884243
ep0_t0.3_test_time 1.4549760818481445
Test Epoch0 threshold 0.4 Acc 0.9746857142857143, AUC 0.9984521269798279, avg_entr 0.009085959754884243
ep0_t0.4_test_time 1.4663114547729492
Test Epoch0 threshold 0.5 Acc 0.9746857142857143, AUC 0.9984521269798279, avg_entr 0.009085959754884243
ep0_t0.5_test_time 1.5222983360290527
Test Epoch0 threshold 0.6 Acc 0.9746857142857143, AUC 0.9984521269798279, avg_entr 0.009085959754884243
ep0_t0.6_test_time 1.481196403503418
Test Epoch0 threshold 0.7 Acc 0.9746857142857143, AUC 0.9984521269798279, avg_entr 0.009085959754884243
ep0_t0.7_test_time 1.5142700672149658
Test Epoch0 threshold 0.8 Acc 0.9746857142857143, AUC 0.9984521269798279, avg_entr 0.009085959754884243
ep0_t0.8_test_time 1.4632892608642578
Test Epoch0 threshold 0.9 Acc 0.9746857142857143, AUC 0.9984521269798279, avg_entr 0.009085959754884243
ep0_t0.9_test_time 1.475440502166748
gc 0
Train Epoch1 Acc 0.9892357142857143 (553972/560000), AUC 0.9988296627998352
ep1_train_time 153.42573070526123
Test Epoch1 threshold 0.1 Acc 0.9763857142857143, AUC 0.998265266418457, avg_entr 0.0040895151905715466
ep1_t0.1_test_time 1.897646427154541
Test Epoch1 threshold 0.2 Acc 0.9750142857142857, AUC 0.9984421730041504, avg_entr 0.008760825730860233
ep1_t0.2_test_time 1.5157771110534668
Test Epoch1 threshold 0.3 Acc 0.9746714285714285, AUC 0.9984413385391235, avg_entr 0.009021209552884102
ep1_t0.3_test_time 1.456378698348999
Test Epoch1 threshold 0.4 Acc 0.9746714285714285, AUC 0.9984413385391235, avg_entr 0.009021209552884102
ep1_t0.4_test_time 1.4550938606262207
Test Epoch1 threshold 0.5 Acc 0.9746714285714285, AUC 0.9984413385391235, avg_entr 0.009021209552884102
ep1_t0.5_test_time 1.510145664215088
Test Epoch1 threshold 0.6 Acc 0.9746714285714285, AUC 0.9984413385391235, avg_entr 0.009021209552884102
ep1_t0.6_test_time 1.4715259075164795
Test Epoch1 threshold 0.7 Acc 0.9746714285714285, AUC 0.9984413385391235, avg_entr 0.009021209552884102
ep1_t0.7_test_time 1.4804463386535645
Test Epoch1 threshold 0.8 Acc 0.9746714285714285, AUC 0.9984413385391235, avg_entr 0.009021209552884102
ep1_t0.8_test_time 1.4585163593292236
Test Epoch1 threshold 0.9 Acc 0.9746714285714285, AUC 0.9984413385391235, avg_entr 0.009021209552884102
ep1_t0.9_test_time 1.4863064289093018
gc 0
Train Epoch2 Acc 0.9904589285714286 (554657/560000), AUC 0.998930811882019
ep2_train_time 153.34694695472717
Test Epoch2 threshold 0.1 Acc 0.9763857142857143, AUC 0.9982832670211792, avg_entr 0.004051376134157181
ep2_t0.1_test_time 1.8772311210632324
Test Epoch2 threshold 0.2 Acc 0.9751714285714286, AUC 0.9984503984451294, avg_entr 0.008785023353993893
ep2_t0.2_test_time 1.5294063091278076
Test Epoch2 threshold 0.3 Acc 0.9748142857142857, AUC 0.9984480738639832, avg_entr 0.009061453863978386
ep2_t0.3_test_time 1.4510915279388428
Test Epoch2 threshold 0.4 Acc 0.9748142857142857, AUC 0.9984480738639832, avg_entr 0.009061453863978386
ep2_t0.4_test_time 1.4623427391052246
Test Epoch2 threshold 0.5 Acc 0.9748142857142857, AUC 0.9984480738639832, avg_entr 0.009061453863978386
ep2_t0.5_test_time 1.4437799453735352
Test Epoch2 threshold 0.6 Acc 0.9748142857142857, AUC 0.9984480738639832, avg_entr 0.009061453863978386
ep2_t0.6_test_time 1.4603264331817627
Test Epoch2 threshold 0.7 Acc 0.9748142857142857, AUC 0.9984480738639832, avg_entr 0.009061453863978386
ep2_t0.7_test_time 1.4678528308868408
Test Epoch2 threshold 0.8 Acc 0.9748142857142857, AUC 0.9984480738639832, avg_entr 0.009061453863978386
ep2_t0.8_test_time 1.452510118484497
Test Epoch2 threshold 0.9 Acc 0.9748142857142857, AUC 0.9984480738639832, avg_entr 0.009061453863978386
ep2_t0.9_test_time 1.4674475193023682
gc 0
Train Epoch3 Acc 0.9907053571428571 (554795/560000), AUC 0.9989897012710571
ep3_train_time 154.23288440704346
Test Epoch3 threshold 0.1 Acc 0.9763285714285714, AUC 0.9982711672782898, avg_entr 0.004062752239406109
ep3_t0.1_test_time 1.9072933197021484
Test Epoch3 threshold 0.2 Acc 0.9750571428571428, AUC 0.9984480142593384, avg_entr 0.008777189068496227
ep3_t0.2_test_time 1.522615909576416
Test Epoch3 threshold 0.3 Acc 0.9746857142857143, AUC 0.9984459280967712, avg_entr 0.009049909189343452
ep3_t0.3_test_time 1.4646151065826416
Test Epoch3 threshold 0.4 Acc 0.9746857142857143, AUC 0.9984459280967712, avg_entr 0.009049909189343452
ep3_t0.4_test_time 1.4527482986450195
Test Epoch3 threshold 0.5 Acc 0.9746857142857143, AUC 0.9984459280967712, avg_entr 0.009049909189343452
ep3_t0.5_test_time 1.461580753326416
Test Epoch3 threshold 0.6 Acc 0.9746857142857143, AUC 0.9984459280967712, avg_entr 0.009049909189343452
ep3_t0.6_test_time 1.4542722702026367
Test Epoch3 threshold 0.7 Acc 0.9746857142857143, AUC 0.9984459280967712, avg_entr 0.009049909189343452
ep3_t0.7_test_time 1.460451364517212
Test Epoch3 threshold 0.8 Acc 0.9746857142857143, AUC 0.9984459280967712, avg_entr 0.009049909189343452
ep3_t0.8_test_time 1.4685683250427246
Test Epoch3 threshold 0.9 Acc 0.9746857142857143, AUC 0.9984459280967712, avg_entr 0.009049909189343452
ep3_t0.9_test_time 1.4566996097564697
gc 0
Train Epoch4 Acc 0.9907535714285715 (554822/560000), AUC 0.9989808201789856
ep4_train_time 153.60385537147522
Test Epoch4 threshold 0.1 Acc 0.9763142857142857, AUC 0.9982704520225525, avg_entr 0.0040487791411578655
ep4_t0.1_test_time 1.8776679039001465
Test Epoch4 threshold 0.2 Acc 0.9751142857142857, AUC 0.9984509348869324, avg_entr 0.008744709193706512
ep4_t0.2_test_time 1.511603832244873
Test Epoch4 threshold 0.3 Acc 0.9747, AUC 0.9984480738639832, avg_entr 0.009025775827467442
ep4_t0.3_test_time 1.4518051147460938
Test Epoch4 threshold 0.4 Acc 0.9747, AUC 0.9984480738639832, avg_entr 0.009025775827467442
ep4_t0.4_test_time 1.4607200622558594
Test Epoch4 threshold 0.5 Acc 0.9747, AUC 0.9984480738639832, avg_entr 0.009025775827467442
ep4_t0.5_test_time 1.46397066116333
Test Epoch4 threshold 0.6 Acc 0.9747, AUC 0.9984480738639832, avg_entr 0.009025775827467442
ep4_t0.6_test_time 1.4660556316375732
Test Epoch4 threshold 0.7 Acc 0.9747, AUC 0.9984480738639832, avg_entr 0.009025775827467442
ep4_t0.7_test_time 1.4630706310272217
Test Epoch4 threshold 0.8 Acc 0.9747, AUC 0.9984480738639832, avg_entr 0.009025775827467442
ep4_t0.8_test_time 1.4568791389465332
Test Epoch4 threshold 0.9 Acc 0.9747, AUC 0.9984480738639832, avg_entr 0.009025775827467442
ep4_t0.9_test_time 1.5072379112243652
gc 0
Train Epoch5 Acc 0.9907678571428571 (554830/560000), AUC 0.9989907145500183
ep5_train_time 154.680983543396
Test Epoch5 threshold 0.1 Acc 0.9763142857142857, AUC 0.9982795715332031, avg_entr 0.0040582045912742615
ep5_t0.1_test_time 1.8497068881988525
Test Epoch5 threshold 0.2 Acc 0.9750857142857143, AUC 0.9984506964683533, avg_entr 0.008751749992370605
ep5_t0.2_test_time 1.4998178482055664
Test Epoch5 threshold 0.3 Acc 0.9746714285714285, AUC 0.9984479546546936, avg_entr 0.009031123481690884
ep5_t0.3_test_time 1.4598793983459473
Test Epoch5 threshold 0.4 Acc 0.9746714285714285, AUC 0.9984479546546936, avg_entr 0.009031123481690884
ep5_t0.4_test_time 1.4766638278961182
Test Epoch5 threshold 0.5 Acc 0.9746714285714285, AUC 0.9984479546546936, avg_entr 0.009031123481690884
ep5_t0.5_test_time 1.4517803192138672
Test Epoch5 threshold 0.6 Acc 0.9746714285714285, AUC 0.9984479546546936, avg_entr 0.009031123481690884
ep5_t0.6_test_time 1.4629199504852295
Test Epoch5 threshold 0.7 Acc 0.9746714285714285, AUC 0.9984479546546936, avg_entr 0.009031123481690884
ep5_t0.7_test_time 1.4537410736083984
Test Epoch5 threshold 0.8 Acc 0.9746714285714285, AUC 0.9984479546546936, avg_entr 0.009031123481690884
ep5_t0.8_test_time 1.4563164710998535
Test Epoch5 threshold 0.9 Acc 0.9746714285714285, AUC 0.9984479546546936, avg_entr 0.009031123481690884
ep5_t0.9_test_time 1.4580857753753662
gc 0
Train Epoch6 Acc 0.9908232142857143 (554861/560000), AUC 0.998948872089386
ep6_train_time 154.65111136436462
Test Epoch6 threshold 0.1 Acc 0.9763142857142857, AUC 0.9982786178588867, avg_entr 0.004059233702719212
ep6_t0.1_test_time 1.8560409545898438
Test Epoch6 threshold 0.2 Acc 0.9750857142857143, AUC 0.9984506368637085, avg_entr 0.008751586079597473
ep6_t0.2_test_time 1.518061876296997
Test Epoch6 threshold 0.3 Acc 0.9746714285714285, AUC 0.9984479546546936, avg_entr 0.009030926041305065
ep6_t0.3_test_time 1.4753916263580322
Test Epoch6 threshold 0.4 Acc 0.9746714285714285, AUC 0.9984479546546936, avg_entr 0.009030926041305065
ep6_t0.4_test_time 1.4611337184906006
Test Epoch6 threshold 0.5 Acc 0.9746714285714285, AUC 0.9984479546546936, avg_entr 0.009030926041305065
ep6_t0.5_test_time 1.463932752609253
Test Epoch6 threshold 0.6 Acc 0.9746714285714285, AUC 0.9984479546546936, avg_entr 0.009030926041305065
ep6_t0.6_test_time 1.463974952697754
Test Epoch6 threshold 0.7 Acc 0.9746714285714285, AUC 0.9984479546546936, avg_entr 0.009030926041305065
ep6_t0.7_test_time 1.4378926753997803
Test Epoch6 threshold 0.8 Acc 0.9746714285714285, AUC 0.9984479546546936, avg_entr 0.009030926041305065
ep6_t0.8_test_time 1.4570972919464111
Test Epoch6 threshold 0.9 Acc 0.9746714285714285, AUC 0.9984479546546936, avg_entr 0.009030926041305065
ep6_t0.9_test_time 1.4723842144012451
gc 0
Train Epoch7 Acc 0.9907535714285715 (554822/560000), AUC 0.9989837408065796
ep7_train_time 154.7118353843689
Test Epoch7 threshold 0.1 Acc 0.9763142857142857, AUC 0.9982785582542419, avg_entr 0.004060681909322739
ep7_t0.1_test_time 1.929419994354248
Test Epoch7 threshold 0.2 Acc 0.9750857142857143, AUC 0.9984506368637085, avg_entr 0.008751551620662212
ep7_t0.2_test_time 1.5283653736114502
Test Epoch7 threshold 0.3 Acc 0.9746714285714285, AUC 0.9984479546546936, avg_entr 0.00903088878840208
ep7_t0.3_test_time 1.4532127380371094
Test Epoch7 threshold 0.4 Acc 0.9746714285714285, AUC 0.9984479546546936, avg_entr 0.00903088878840208
ep7_t0.4_test_time 1.4661765098571777
Test Epoch7 threshold 0.5 Acc 0.9746714285714285, AUC 0.9984479546546936, avg_entr 0.00903088878840208
ep7_t0.5_test_time 1.4572818279266357
Test Epoch7 threshold 0.6 Acc 0.9746714285714285, AUC 0.9984479546546936, avg_entr 0.00903088878840208
ep7_t0.6_test_time 1.4662330150604248
Test Epoch7 threshold 0.7 Acc 0.9746714285714285, AUC 0.9984479546546936, avg_entr 0.00903088878840208
ep7_t0.7_test_time 1.4690630435943604
Test Epoch7 threshold 0.8 Acc 0.9746714285714285, AUC 0.9984479546546936, avg_entr 0.00903088878840208
ep7_t0.8_test_time 1.4476888179779053
Test Epoch7 threshold 0.9 Acc 0.9746714285714285, AUC 0.9984479546546936, avg_entr 0.00903088878840208
ep7_t0.9_test_time 1.465087890625
gc 0
Train Epoch8 Acc 0.9907303571428572 (554809/560000), AUC 0.998961865901947
ep8_train_time 154.70231533050537
Test Epoch8 threshold 0.1 Acc 0.9763142857142857, AUC 0.9982785582542419, avg_entr 0.004060683771967888
ep8_t0.1_test_time 1.8790299892425537
Test Epoch8 threshold 0.2 Acc 0.9750857142857143, AUC 0.9984506964683533, avg_entr 0.008751541376113892
ep8_t0.2_test_time 1.510448932647705
Test Epoch8 threshold 0.3 Acc 0.9746714285714285, AUC 0.9984479546546936, avg_entr 0.009030883200466633
ep8_t0.3_test_time 1.4663736820220947
Test Epoch8 threshold 0.4 Acc 0.9746714285714285, AUC 0.9984479546546936, avg_entr 0.009030883200466633
ep8_t0.4_test_time 1.4574663639068604
Test Epoch8 threshold 0.5 Acc 0.9746714285714285, AUC 0.9984479546546936, avg_entr 0.009030883200466633
ep8_t0.5_test_time 1.4819409847259521
Test Epoch8 threshold 0.6 Acc 0.9746714285714285, AUC 0.9984479546546936, avg_entr 0.009030883200466633
ep8_t0.6_test_time 1.4552812576293945
Test Epoch8 threshold 0.7 Acc 0.9746714285714285, AUC 0.9984479546546936, avg_entr 0.009030883200466633
ep8_t0.7_test_time 1.4500596523284912
Test Epoch8 threshold 0.8 Acc 0.9746714285714285, AUC 0.9984479546546936, avg_entr 0.009030883200466633
ep8_t0.8_test_time 1.4547045230865479
Test Epoch8 threshold 0.9 Acc 0.9746714285714285, AUC 0.9984479546546936, avg_entr 0.009030883200466633
ep8_t0.9_test_time 1.583846092224121
gc 0
Train Epoch9 Acc 0.9907660714285714 (554829/560000), AUC 0.9989752173423767
ep9_train_time 155.16231608390808
Test Epoch9 threshold 0.1 Acc 0.9763142857142857, AUC 0.9982783198356628, avg_entr 0.0040621766820549965
ep9_t0.1_test_time 1.8686347007751465
Test Epoch9 threshold 0.2 Acc 0.9750857142857143, AUC 0.9984506368637085, avg_entr 0.008751535788178444
ep9_t0.2_test_time 1.517996072769165
Test Epoch9 threshold 0.3 Acc 0.9746714285714285, AUC 0.9984479546546936, avg_entr 0.009030872024595737
ep9_t0.3_test_time 1.4643378257751465
Test Epoch9 threshold 0.4 Acc 0.9746714285714285, AUC 0.9984479546546936, avg_entr 0.009030872024595737
ep9_t0.4_test_time 1.4980709552764893
Test Epoch9 threshold 0.5 Acc 0.9746714285714285, AUC 0.9984479546546936, avg_entr 0.009030872024595737
ep9_t0.5_test_time 1.4629981517791748
Test Epoch9 threshold 0.6 Acc 0.9746714285714285, AUC 0.9984479546546936, avg_entr 0.009030872024595737
ep9_t0.6_test_time 1.4744510650634766
Test Epoch9 threshold 0.7 Acc 0.9746714285714285, AUC 0.9984479546546936, avg_entr 0.009030872024595737
ep9_t0.7_test_time 1.4679362773895264
Test Epoch9 threshold 0.8 Acc 0.9746714285714285, AUC 0.9984479546546936, avg_entr 0.009030872024595737
ep9_t0.8_test_time 1.4685907363891602
Test Epoch9 threshold 0.9 Acc 0.9746714285714285, AUC 0.9984479546546936, avg_entr 0.009030872024595737
ep9_t0.9_test_time 1.4583461284637451
Best AUC 0.9984530210494995
train_loss (2, 5, 10)
valid_acc (10, 9)
valid_AUC (10, 9)
train_acc (10,)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Start Testing
Load ckpt at ckpt/dbpedia_14_transformeral_l5_pad60_m5//dbpedia_14_transformeral_l5_prefix.pt
[[4715   36   19   14   12   63   39    7    4    6    4   19   18   44]
 [  40 4896    1    1    7    0   31    7    3    2    1    0    4    7]
 [  38   13 4595   18   54    3   10    3    4    1    1   86   22  152]
 [   3    2   23 4956   12    0    1    0    0    0    0    1    0    2]
 [  11   17   63   13 4864    8    7    1    2    0    0    1    2   11]
 [  37    1    2    2    1 4945    5    2    1    2    0    0    1    1]
 [  66   41    6    3   10   14 4798   39    9    5    1    1    5    2]
 [   0    0    0    0    2    0   13 4964   16    3    1    0    0    1]
 [   1    1    2    0    4    0   11   13 4968    0    0    0    0    0]
 [   1    0    0    2    0    0    0    7    0 4957   31    1    0    1]
 [  12    1    0    0    0    0    2    4    0   36 4945    0    0    0]
 [   7    0   32    2    0    0    0    0    0    0    0 4931   14   14]
 [   9    2   18    4    1    3    1    3    0    2    0   21 4892   44]
 [  33    4   68    7   12    4    3    9    1    5    2    7   43 4802]]
Figure(640x480)
tensor([1.1847e-07, 1.7025e-02, 4.4157e-04,  ..., 6.6461e-08, 2.0246e-03,
        3.4834e-06])
[[4791   36   20    5   11   40   38    2    0    2    1   14    6   34]
 [  42 4910    4    0   10    0   26    0    0    1    1    0    0    6]
 [  32   12 4718   11   70    1    6    0    0    1    2   45   11   91]
 [   4    1   15 4961   12    2    0    1    0    0    0    1    1    2]
 [   8    7   68    9 4885    8    6    1    1    2    0    0    2    3]
 [  40    0    0    1    0 4951    3    2    1    0    0    0    1    1]
 [  68   39    7    1    5   12 4824   29    5    4    0    1    2    3]
 [   3    1    0    0    0    1   17 4967    6    4    0    0    0    1]
 [   4    2    1    0    5    0    8   19 4961    0    0    0    0    0]
 [   1    0    2    1    0    1    0    6    0 4963   25    0    0    1]
 [  14    1    0    0    0    1    1    2    0   29 4952    0    0    0]
 [   7    0   33    2    0    0    0    0    0    0    0 4943    9    6]
 [  13    1   19    1    0    0    0    2    0    0    0   20 4890   54]
 [  36    4   62    3    6    3    2    0    0    0    2    6   35 4841]]
Figure(640x480)
tensor([5.2848e-08, 1.3558e-01, 1.7937e-07,  ..., 6.6218e-08, 9.8450e-08,
        8.6627e-08])
[[4800   33   22    4   10   41   36    3    0    2    1   12    4   32]
 [  47 4910    2    0    8    0   24    0    1    1    1    0    0    6]
 [  34   11 4719   14   67    1    6    0    1    0    1   45   10   91]
 [   4    1   15 4961   11    2    0    1    0    0    0    1    2    2]
 [   9    8   66   10 4881    8    6    1    2    2    0    0    2    5]
 [  38    0    0    1    0 4953    3    2    1    0    0    0    1    1]
 [  75   42    6    1    5   12 4816   28    5    4    0    1    2    3]
 [   3    1    0    0    0    1   18 4964    8    4    0    0    0    1]
 [   5    3    1    0    5    0    7   16 4963    0    0    0    0    0]
 [   1    0    2    1    0    1    0    6    0 4961   27    0    0    1]
 [  14    1    1    0    0    2    1    1    0   29 4951    0    0    0]
 [   9    0   33    2    0    0    0    0    0    0    0 4941    9    6]
 [  14    1   24    0    0    1    0    2    0    0    0   19 4883   56]
 [  37    2   62    3    5    3    2    0    0    0    1    6   33 4846]]
Figure(640x480)
tensor([2.1017e-07, 7.0777e-05, 2.1343e-07,  ..., 1.0421e-07, 1.0381e-07,
        9.9814e-08])
[[4804   32   22    4   10   40   36    2    0    2    1   11    4   32]
 [  49 4909    4    0    8    0   21    0    1    1    1    0    0    6]
 [  36   11 4722   14   65    1    6    0    1    0    1   45   10   88]
 [   4    1   15 4961   11    2    0    1    0    0    0    1    2    2]
 [  10    8   67   13 4875    8    6    1    2    2    0    0    2    6]
 [  38    0    0    1    0 4953    3    2    1    0    0    0    1    1]
 [  77   42    7    1    4   13 4815   26    5    4    0    1    2    3]
 [   3    1    0    0    0    1   18 4964    8    4    0    0    0    1]
 [   5    3    1    0    5    0    8   15 4963    0    0    0    0    0]
 [   1    0    2    1    0    1    0    6    0 4962   26    0    0    1]
 [  15    1    1    0    0    2    1    1    0   29 4950    0    0    0]
 [   9    0   33    2    0    0    0    0    0    0    0 4941    9    6]
 [  14    1   24    0    0    1    0    2    0    0    0   19 4883   56]
 [  37    2   63    3    3    3    2    0    0    0    1    6   31 4849]]
Figure(640x480)
tensor([1.1408e-07, 4.5760e-06, 1.1615e-07,  ..., 1.2790e-07, 1.3382e-07,
        1.2772e-07])
[[4804   32   22    4   10   40   36    2    0    2    1   11    4   32]
 [  50 4907    4    0    8    0   22    0    1    1    1    0    0    6]
 [  35   11 4719   14   65    1    5    0    1    1    1   45   10   92]
 [   4    1   15 4961   11    2    0    1    0    0    0    1    2    2]
 [   9    8   67   14 4875    8    6    1    2    2    0    0    2    6]
 [  38    0    0    1    0 4953    3    2    1    0    0    0    1    1]
 [  77   40    7    1    4   12 4818   26    5    4    0    1    2    3]
 [   3    1    0    0    0    1   18 4964    8    4    0    0    0    1]
 [   5    1    1    0    3    0    9   15 4966    0    0    0    0    0]
 [   1    0    2    0    0    1    0    6    0 4964   25    0    0    1]
 [  15    1    1    0    0    2    1    1    0   29 4950    0    0    0]
 [  10    0   33    2    0    0    0    0    0    0    0 4939    9    7]
 [  14    1   24    0    0    1    0    2    0    1    0   19 4882   56]
 [  37    2   62    3    3    2    2    0    0    0    1    6   31 4851]]
Figure(640x480)
tensor([1.3432e-07, 1.2392e-06, 1.3600e-07,  ..., 1.3129e-07, 1.3753e-07,
        1.2838e-07])
