total count words 102019
vocab size 30000
found 26754 words in glove
model: TransformerModelML(
  (layers): ModuleList(
    (0): EMBLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Embedding(30000, 300)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=4, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=4, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.weight 9000000
layers.0.ae.g.0.weight 512
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 512
layers.0.ae.h.0.bias 4
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
Total Trainable Params: 13672292
Start Training
gc 0
Train Epoch0 Acc 0.64635 (77562/120000), AUC 0.8620566129684448
Test Epoch0 layer0 Acc 0.9038157894736842, AUC 0.9756801128387451, avg_entr 0.2557937800884247
Save ckpt to ckpt/ag_news_transformeral_l5_pad175//ag_news_transformeral_l5.pt  ,ep 0
Test Epoch0 layer1 Acc 0.9057894736842105, AUC 0.977948009967804, avg_entr 0.16682308912277222
Save ckpt to ckpt/ag_news_transformeral_l5_pad175//ag_news_transformeral_l5.pt  ,ep 0
Test Epoch0 layer2 Acc 0.9042105263157895, AUC 0.9778587818145752, avg_entr 0.1624433696269989
Test Epoch0 layer3 Acc 0.9006578947368421, AUC 0.9780376553535461, avg_entr 0.16433243453502655
Save ckpt to ckpt/ag_news_transformeral_l5_pad175//ag_news_transformeral_l5.pt  ,ep 0
Test Epoch0 layer4 Acc 0.8943421052631579, AUC 0.9781714677810669, avg_entr 0.17440767586231232
Save ckpt to ckpt/ag_news_transformeral_l5_pad175//ag_news_transformeral_l5.pt  ,ep 0
gc 0
Train Epoch1 Acc 0.9196666666666666 (110360/120000), AUC 0.981298565864563
Test Epoch1 layer0 Acc 0.9131578947368421, AUC 0.9794772863388062, avg_entr 0.149338036775589
Save ckpt to ckpt/ag_news_transformeral_l5_pad175//ag_news_transformeral_l5.pt  ,ep 1
Test Epoch1 layer1 Acc 0.9186842105263158, AUC 0.9812328815460205, avg_entr 0.08084473013877869
Save ckpt to ckpt/ag_news_transformeral_l5_pad175//ag_news_transformeral_l5.pt  ,ep 1
Test Epoch1 layer2 Acc 0.9182894736842105, AUC 0.980516254901886, avg_entr 0.0699978843331337
Test Epoch1 layer3 Acc 0.9184210526315789, AUC 0.9804458618164062, avg_entr 0.06710641086101532
Test Epoch1 layer4 Acc 0.9185526315789474, AUC 0.9805530905723572, avg_entr 0.06286869198083878
gc 0
Train Epoch2 Acc 0.9348416666666667 (112181/120000), AUC 0.9869111776351929
Test Epoch2 layer0 Acc 0.9173684210526316, AUC 0.9808390140533447, avg_entr 0.11394704133272171
Test Epoch2 layer1 Acc 0.9219736842105263, AUC 0.9818191528320312, avg_entr 0.0455513596534729
Save ckpt to ckpt/ag_news_transformeral_l5_pad175//ag_news_transformeral_l5.pt  ,ep 2
Test Epoch2 layer2 Acc 0.9218421052631579, AUC 0.9829914569854736, avg_entr 0.04053518921136856
Save ckpt to ckpt/ag_news_transformeral_l5_pad175//ag_news_transformeral_l5.pt  ,ep 2
Test Epoch2 layer3 Acc 0.9214473684210527, AUC 0.9828237295150757, avg_entr 0.0390532910823822
Test Epoch2 layer4 Acc 0.9211842105263158, AUC 0.9825987219810486, avg_entr 0.03608124330639839
gc 0
Train Epoch3 Acc 0.9435166666666667 (113222/120000), AUC 0.989328920841217
Test Epoch3 layer0 Acc 0.9173684210526316, AUC 0.9814082980155945, avg_entr 0.09293966740369797
Test Epoch3 layer1 Acc 0.921578947368421, AUC 0.9814296364784241, avg_entr 0.03455702215433121
Test Epoch3 layer2 Acc 0.9214473684210527, AUC 0.9826365113258362, avg_entr 0.029720334336161613
Test Epoch3 layer3 Acc 0.9219736842105263, AUC 0.9834660887718201, avg_entr 0.027847416698932648
Save ckpt to ckpt/ag_news_transformeral_l5_pad175//ag_news_transformeral_l5.pt  ,ep 3
Test Epoch3 layer4 Acc 0.9219736842105263, AUC 0.9832711219787598, avg_entr 0.025450751185417175
gc 0
Train Epoch4 Acc 0.9489333333333333 (113872/120000), AUC 0.9904437065124512
Test Epoch4 layer0 Acc 0.9194736842105263, AUC 0.9817817807197571, avg_entr 0.08365219831466675
Test Epoch4 layer1 Acc 0.920921052631579, AUC 0.9805759191513062, avg_entr 0.029855255037546158
Test Epoch4 layer2 Acc 0.9197368421052632, AUC 0.9818419218063354, avg_entr 0.025534743443131447
Test Epoch4 layer3 Acc 0.9205263157894736, AUC 0.9826334714889526, avg_entr 0.024071592837572098
Test Epoch4 layer4 Acc 0.9203947368421053, AUC 0.9825103282928467, avg_entr 0.02224043384194374
gc 0
Train Epoch5 Acc 0.9529416666666667 (114353/120000), AUC 0.991351842880249
Test Epoch5 layer0 Acc 0.9175, AUC 0.9818524122238159, avg_entr 0.07564883679151535
Test Epoch5 layer1 Acc 0.9185526315789474, AUC 0.9791327118873596, avg_entr 0.026332486420869827
Test Epoch5 layer2 Acc 0.9182894736842105, AUC 0.980795681476593, avg_entr 0.021492615342140198
Test Epoch5 layer3 Acc 0.9192105263157895, AUC 0.9811309576034546, avg_entr 0.019804298877716064
Test Epoch5 layer4 Acc 0.9192105263157895, AUC 0.9811108708381653, avg_entr 0.01794780045747757
gc 0
Train Epoch6 Acc 0.956275 (114753/120000), AUC 0.9922586679458618
Test Epoch6 layer0 Acc 0.9176315789473685, AUC 0.9819148778915405, avg_entr 0.06778787821531296
Test Epoch6 layer1 Acc 0.9157894736842105, AUC 0.9789149761199951, avg_entr 0.025885028764605522
Test Epoch6 layer2 Acc 0.9163157894736842, AUC 0.9808593392372131, avg_entr 0.020483611151576042
Test Epoch6 layer3 Acc 0.9161842105263158, AUC 0.9816553592681885, avg_entr 0.01914592832326889
Test Epoch6 layer4 Acc 0.915921052631579, AUC 0.9817238450050354, avg_entr 0.017655273899435997
gc 0
Train Epoch7 Acc 0.958925 (115071/120000), AUC 0.9930688142776489
Test Epoch7 layer0 Acc 0.9168421052631579, AUC 0.9817683100700378, avg_entr 0.06319141387939453
Test Epoch7 layer1 Acc 0.9168421052631579, AUC 0.9766126871109009, avg_entr 0.022932572290301323
Test Epoch7 layer2 Acc 0.9163157894736842, AUC 0.979103147983551, avg_entr 0.018294665962457657
Test Epoch7 layer3 Acc 0.9156578947368421, AUC 0.980668306350708, avg_entr 0.017595874145627022
Test Epoch7 layer4 Acc 0.9153947368421053, AUC 0.9806050062179565, avg_entr 0.016179101541638374
gc 0
Train Epoch8 Acc 0.96115 (115338/120000), AUC 0.993779182434082
Test Epoch8 layer0 Acc 0.9164473684210527, AUC 0.9816639423370361, avg_entr 0.05963258445262909
Test Epoch8 layer1 Acc 0.9152631578947369, AUC 0.9773484468460083, avg_entr 0.021158885210752487
Test Epoch8 layer2 Acc 0.9153947368421053, AUC 0.9789389371871948, avg_entr 0.015471216291189194
Test Epoch8 layer3 Acc 0.9155263157894736, AUC 0.9807525873184204, avg_entr 0.013839028775691986
Test Epoch8 layer4 Acc 0.9152631578947369, AUC 0.9803504347801208, avg_entr 0.012482360005378723
gc 0
Train Epoch9 Acc 0.963025 (115563/120000), AUC 0.9942240118980408
Test Epoch9 layer0 Acc 0.9163157894736842, AUC 0.9816782474517822, avg_entr 0.05679521709680557
Test Epoch9 layer1 Acc 0.9161842105263158, AUC 0.9753488302230835, avg_entr 0.019662218168377876
Test Epoch9 layer2 Acc 0.9153947368421053, AUC 0.9763311147689819, avg_entr 0.014675427228212357
Test Epoch9 layer3 Acc 0.9152631578947369, AUC 0.9772332310676575, avg_entr 0.012983319349586964
Test Epoch9 layer4 Acc 0.9151315789473684, AUC 0.976503849029541, avg_entr 0.011718385852873325
gc 0
Train Epoch10 Acc 0.9638916666666667 (115667/120000), AUC 0.9944540858268738
Test Epoch10 layer0 Acc 0.9148684210526316, AUC 0.9815511703491211, avg_entr 0.0544026680290699
Test Epoch10 layer1 Acc 0.9131578947368421, AUC 0.9753644466400146, avg_entr 0.019352491945028305
Test Epoch10 layer2 Acc 0.9125, AUC 0.9782660007476807, avg_entr 0.013999918475747108
Test Epoch10 layer3 Acc 0.9126315789473685, AUC 0.9793976545333862, avg_entr 0.012290065176784992
Test Epoch10 layer4 Acc 0.9123684210526316, AUC 0.9781942963600159, avg_entr 0.011104434728622437
gc 0
Train Epoch11 Acc 0.9668583333333334 (116023/120000), AUC 0.9952501058578491
Test Epoch11 layer0 Acc 0.9177631578947368, AUC 0.9813879132270813, avg_entr 0.05266985297203064
Test Epoch11 layer1 Acc 0.9132894736842105, AUC 0.9750100374221802, avg_entr 0.01887686364352703
Test Epoch11 layer2 Acc 0.9139473684210526, AUC 0.9765415191650391, avg_entr 0.013931180350482464
Test Epoch11 layer3 Acc 0.9132894736842105, AUC 0.9777024388313293, avg_entr 0.012431114912033081
Test Epoch11 layer4 Acc 0.9130263157894737, AUC 0.9789677262306213, avg_entr 0.011238266713917255
gc 0
Train Epoch12 Acc 0.9674416666666666 (116093/120000), AUC 0.9953011274337769
Test Epoch12 layer0 Acc 0.9169736842105263, AUC 0.9812651872634888, avg_entr 0.05008943751454353
Test Epoch12 layer1 Acc 0.9125, AUC 0.9735471606254578, avg_entr 0.018279585987329483
Test Epoch12 layer2 Acc 0.9126315789473685, AUC 0.9748048186302185, avg_entr 0.01337458100169897
Test Epoch12 layer3 Acc 0.9126315789473685, AUC 0.9769235253334045, avg_entr 0.011749771423637867
Test Epoch12 layer4 Acc 0.9123684210526316, AUC 0.9780116081237793, avg_entr 0.010641585104167461
gc 0
Train Epoch13 Acc 0.968575 (116229/120000), AUC 0.9954706430435181
Test Epoch13 layer0 Acc 0.916578947368421, AUC 0.9811774492263794, avg_entr 0.048796236515045166
Test Epoch13 layer1 Acc 0.9131578947368421, AUC 0.9735358953475952, avg_entr 0.017306357622146606
Test Epoch13 layer2 Acc 0.9127631578947368, AUC 0.9743199348449707, avg_entr 0.012192799709737301
Test Epoch13 layer3 Acc 0.9125, AUC 0.9748448729515076, avg_entr 0.010365708731114864
Test Epoch13 layer4 Acc 0.9121052631578948, AUC 0.9759703278541565, avg_entr 0.009353023953735828
gc 0
Train Epoch14 Acc 0.9689416666666667 (116273/120000), AUC 0.995586097240448
Test Epoch14 layer0 Acc 0.916578947368421, AUC 0.981112003326416, avg_entr 0.046351172029972076
Test Epoch14 layer1 Acc 0.9123684210526316, AUC 0.9724658131599426, avg_entr 0.016472915187478065
Test Epoch14 layer2 Acc 0.9111842105263158, AUC 0.9735325574874878, avg_entr 0.011835718527436256
Test Epoch14 layer3 Acc 0.9114473684210527, AUC 0.9726659059524536, avg_entr 0.01004132442176342
Test Epoch14 layer4 Acc 0.9113157894736842, AUC 0.9739365577697754, avg_entr 0.009084619581699371
gc 0
Train Epoch15 Acc 0.9704 (116448/120000), AUC 0.9957797527313232
Test Epoch15 layer0 Acc 0.9160526315789473, AUC 0.9810265302658081, avg_entr 0.04588538035750389
Test Epoch15 layer1 Acc 0.9119736842105263, AUC 0.9724622964859009, avg_entr 0.01686262898147106
Test Epoch15 layer2 Acc 0.9114473684210527, AUC 0.9739933013916016, avg_entr 0.011742479167878628
Test Epoch15 layer3 Acc 0.9110526315789473, AUC 0.974382758140564, avg_entr 0.009872757829725742
Test Epoch15 layer4 Acc 0.9107894736842105, AUC 0.9757803678512573, avg_entr 0.008878041058778763
gc 0
Train Epoch16 Acc 0.970725 (116487/120000), AUC 0.9959622621536255
Test Epoch16 layer0 Acc 0.915921052631579, AUC 0.980995774269104, avg_entr 0.04464123398065567
Test Epoch16 layer1 Acc 0.9113157894736842, AUC 0.9721972346305847, avg_entr 0.016461260616779327
Test Epoch16 layer2 Acc 0.910921052631579, AUC 0.9730340242385864, avg_entr 0.011663360521197319
Test Epoch16 layer3 Acc 0.910921052631579, AUC 0.9727228879928589, avg_entr 0.009919454343616962
Test Epoch16 layer4 Acc 0.911578947368421, AUC 0.9735555052757263, avg_entr 0.008932676166296005
gc 0
Train Epoch17 Acc 0.97105 (116526/120000), AUC 0.9959286451339722
Test Epoch17 layer0 Acc 0.915921052631579, AUC 0.980920135974884, avg_entr 0.04387519881129265
Test Epoch17 layer1 Acc 0.9118421052631579, AUC 0.9725431203842163, avg_entr 0.01618560217320919
Test Epoch17 layer2 Acc 0.911578947368421, AUC 0.9746840000152588, avg_entr 0.011093318462371826
Test Epoch17 layer3 Acc 0.911578947368421, AUC 0.974869430065155, avg_entr 0.00918338168412447
Test Epoch17 layer4 Acc 0.9114473684210527, AUC 0.9752209186553955, avg_entr 0.00838268268853426
gc 0
Train Epoch18 Acc 0.9712916666666667 (116555/120000), AUC 0.9959349036216736
Test Epoch18 layer0 Acc 0.9164473684210527, AUC 0.9809142351150513, avg_entr 0.042069852352142334
Test Epoch18 layer1 Acc 0.9121052631578948, AUC 0.9721574783325195, avg_entr 0.015526654198765755
Test Epoch18 layer2 Acc 0.9121052631578948, AUC 0.9737098217010498, avg_entr 0.010825135745108128
Test Epoch18 layer3 Acc 0.9114473684210527, AUC 0.9726295471191406, avg_entr 0.009035329334437847
Test Epoch18 layer4 Acc 0.9117105263157895, AUC 0.973642110824585, avg_entr 0.008103100582957268
gc 0
Train Epoch19 Acc 0.9721666666666666 (116660/120000), AUC 0.9961473345756531
Test Epoch19 layer0 Acc 0.9153947368421053, AUC 0.9809204936027527, avg_entr 0.040946077555418015
Test Epoch19 layer1 Acc 0.9118421052631579, AUC 0.9720575213432312, avg_entr 0.01524786464869976
Test Epoch19 layer2 Acc 0.9114473684210527, AUC 0.973273515701294, avg_entr 0.010868953540921211
Test Epoch19 layer3 Acc 0.9110526315789473, AUC 0.9717931747436523, avg_entr 0.009251579642295837
Test Epoch19 layer4 Acc 0.9103947368421053, AUC 0.9724264740943909, avg_entr 0.008505296893417835
gc 0
Train Epoch20 Acc 0.9724916666666666 (116699/120000), AUC 0.9961433410644531
Test Epoch20 layer0 Acc 0.915921052631579, AUC 0.9809004068374634, avg_entr 0.04079451039433479
Test Epoch20 layer1 Acc 0.9107894736842105, AUC 0.9721497297286987, avg_entr 0.01478935033082962
Test Epoch20 layer2 Acc 0.9107894736842105, AUC 0.9733738303184509, avg_entr 0.010652824304997921
Test Epoch20 layer3 Acc 0.9105263157894737, AUC 0.9719759225845337, avg_entr 0.008901751600205898
Test Epoch20 layer4 Acc 0.9102631578947369, AUC 0.9729481935501099, avg_entr 0.008090225979685783
gc 0
Train Epoch21 Acc 0.9727583333333333 (116731/120000), AUC 0.9961603283882141
Test Epoch21 layer0 Acc 0.9156578947368421, AUC 0.9808673858642578, avg_entr 0.040749479085206985
Test Epoch21 layer1 Acc 0.910921052631579, AUC 0.9723527431488037, avg_entr 0.015067368745803833
Test Epoch21 layer2 Acc 0.9113157894736842, AUC 0.9744841456413269, avg_entr 0.010594644583761692
Test Epoch21 layer3 Acc 0.9105263157894737, AUC 0.9730071425437927, avg_entr 0.009055026806890965
Test Epoch21 layer4 Acc 0.9102631578947369, AUC 0.9733825325965881, avg_entr 0.00849136896431446
gc 0
Train Epoch22 Acc 0.9726583333333333 (116719/120000), AUC 0.9962279796600342
Test Epoch22 layer0 Acc 0.9153947368421053, AUC 0.980857253074646, avg_entr 0.03958065062761307
Test Epoch22 layer1 Acc 0.9107894736842105, AUC 0.9721054434776306, avg_entr 0.014721330255270004
Test Epoch22 layer2 Acc 0.910921052631579, AUC 0.9736241102218628, avg_entr 0.010376286692917347
Test Epoch22 layer3 Acc 0.9101315789473684, AUC 0.9720832109451294, avg_entr 0.008898276835680008
Test Epoch22 layer4 Acc 0.9101315789473684, AUC 0.9726840257644653, avg_entr 0.00824431050568819
gc 0
Train Epoch23 Acc 0.9729833333333333 (116758/120000), AUC 0.9961709380149841
Test Epoch23 layer0 Acc 0.9152631578947369, AUC 0.9808392524719238, avg_entr 0.03943011537194252
Test Epoch23 layer1 Acc 0.9107894736842105, AUC 0.971892774105072, avg_entr 0.01458687987178564
Test Epoch23 layer2 Acc 0.9105263157894737, AUC 0.9735349416732788, avg_entr 0.010241774842143059
Test Epoch23 layer3 Acc 0.9098684210526315, AUC 0.9716898798942566, avg_entr 0.008759668096899986
Test Epoch23 layer4 Acc 0.9093421052631578, AUC 0.971561074256897, avg_entr 0.0082180080935359
gc 0
Train Epoch24 Acc 0.972775 (116733/120000), AUC 0.9963158369064331
Test Epoch24 layer0 Acc 0.9156578947368421, AUC 0.9808198809623718, avg_entr 0.03859313577413559
Test Epoch24 layer1 Acc 0.9105263157894737, AUC 0.9718731045722961, avg_entr 0.014361229725182056
Test Epoch24 layer2 Acc 0.9103947368421053, AUC 0.9733753204345703, avg_entr 0.010030937381088734
Test Epoch24 layer3 Acc 0.9098684210526315, AUC 0.971601665019989, avg_entr 0.008421101607382298
Test Epoch24 layer4 Acc 0.9097368421052632, AUC 0.9719122052192688, avg_entr 0.007696114014834166
gc 0
Train Epoch25 Acc 0.9730833333333333 (116770/120000), AUC 0.9963617324829102
Test Epoch25 layer0 Acc 0.9152631578947369, AUC 0.9808067083358765, avg_entr 0.03857351839542389
Test Epoch25 layer1 Acc 0.9107894736842105, AUC 0.9716732501983643, avg_entr 0.014341836795210838
Test Epoch25 layer2 Acc 0.9106578947368421, AUC 0.9731960296630859, avg_entr 0.00980228092521429
Test Epoch25 layer3 Acc 0.9101315789473684, AUC 0.9714782238006592, avg_entr 0.008119413629174232
Test Epoch25 layer4 Acc 0.91, AUC 0.9715471863746643, avg_entr 0.007548080291599035
gc 0
Train Epoch26 Acc 0.9737583333333333 (116851/120000), AUC 0.9963384866714478
Test Epoch26 layer0 Acc 0.9155263157894736, AUC 0.9808015823364258, avg_entr 0.038072358816862106
Test Epoch26 layer1 Acc 0.9105263157894737, AUC 0.971820592880249, avg_entr 0.014211257919669151
Test Epoch26 layer2 Acc 0.9105263157894737, AUC 0.9735432267189026, avg_entr 0.009755391627550125
Test Epoch26 layer3 Acc 0.9101315789473684, AUC 0.9715230464935303, avg_entr 0.008140215650200844
Test Epoch26 layer4 Acc 0.9097368421052632, AUC 0.971448540687561, avg_entr 0.007535814307630062
gc 0
Train Epoch27 Acc 0.9734 (116808/120000), AUC 0.9964203834533691
Test Epoch27 layer0 Acc 0.9155263157894736, AUC 0.980791449546814, avg_entr 0.038175977766513824
Test Epoch27 layer1 Acc 0.9106578947368421, AUC 0.9716066122055054, avg_entr 0.014178208075463772
Test Epoch27 layer2 Acc 0.9105263157894737, AUC 0.9734065532684326, avg_entr 0.009679827839136124
Test Epoch27 layer3 Acc 0.9101315789473684, AUC 0.9720208644866943, avg_entr 0.008031818084418774
Test Epoch27 layer4 Acc 0.9097368421052632, AUC 0.9723225831985474, avg_entr 0.007436648476868868
gc 0
Train Epoch28 Acc 0.9735833333333334 (116830/120000), AUC 0.9963796138763428
Test Epoch28 layer0 Acc 0.9148684210526316, AUC 0.9807817935943604, avg_entr 0.038159117102622986
Test Epoch28 layer1 Acc 0.9106578947368421, AUC 0.9717674851417542, avg_entr 0.014081666246056557
Test Epoch28 layer2 Acc 0.9105263157894737, AUC 0.9735981225967407, avg_entr 0.009641681797802448
Test Epoch28 layer3 Acc 0.91, AUC 0.9719587564468384, avg_entr 0.00803250540047884
Test Epoch28 layer4 Acc 0.9097368421052632, AUC 0.9722261428833008, avg_entr 0.007404329255223274
gc 0
Train Epoch29 Acc 0.973675 (116841/120000), AUC 0.9964423775672913
Test Epoch29 layer0 Acc 0.9148684210526316, AUC 0.9807684421539307, avg_entr 0.03794187307357788
Test Epoch29 layer1 Acc 0.9103947368421053, AUC 0.9717880487442017, avg_entr 0.014083999209105968
Test Epoch29 layer2 Acc 0.91, AUC 0.9733259081840515, avg_entr 0.009640127420425415
Test Epoch29 layer3 Acc 0.9096052631578947, AUC 0.9711461067199707, avg_entr 0.007986572571098804
Test Epoch29 layer4 Acc 0.9092105263157895, AUC 0.9717103242874146, avg_entr 0.007418469991534948
Best AUC 0.9834660887718201
train_loss (2, 5, 30)
valid_acc (5, 30)
valid_AUC (5, 30)
train_acc (30,)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Start Testing
Load ckpt at ckpt/ag_news_transformeral_l5_pad175//ag_news_transformeral_l5.pt
[[1706   61   83   50]
 [   9 1877    6    8]
 [  51   21 1665  163]
 [  48   18  110 1724]]
Figure(640x480)
tensor([0.0761, 0.0017, 0.0577,  ..., 0.0523, 0.0076, 0.3156])
[[1716   53   73   58]
 [  10 1871    9   10]
 [  45   16 1683  156]
 [  44   10  112 1734]]
Figure(640x480)
tensor([0.0027, 0.0021, 0.0023,  ..., 0.0023, 0.0022, 0.0029])
[[1725   52   66   57]
 [  11 1870    7   12]
 [  46   17 1681  156]
 [  47   11  115 1727]]
Figure(640x480)
tensor([0.0036, 0.0025, 0.0029,  ..., 0.0026, 0.0029, 0.0029])
[[1726   51   66   57]
 [  10 1870    8   12]
 [  46   17 1682  155]
 [  46   11  114 1729]]
Figure(640x480)
tensor([0.0031, 0.0024, 0.0029,  ..., 0.0026, 0.0027, 0.0030])
[[1725   51   66   58]
 [  10 1870    8   12]
 [  46   17 1680  157]
 [  45   11  112 1732]]
Figure(640x480)
tensor([0.0030, 0.0024, 0.0029,  ..., 0.0024, 0.0027, 0.0030])
