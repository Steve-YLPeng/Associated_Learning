total count words 102019
vocab size 30000
found 26754 words in glove
model: TransformerModelML(
  (layers): ModuleList(
    (0): EMBLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Embedding(30000, 300)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=4, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=4, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.weight 9000000
layers.0.ae.g.0.weight 512
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 512
layers.0.ae.h.0.bias 4
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
Total Trainable Params: 13672292
Start Training
gc 0
Train Epoch0 Acc 0.6016166666666667 (72194/120000), AUC 0.834924578666687
Test Epoch0 layer0 Acc 0.9131578947368421, AUC 0.980388879776001, avg_entr 0.21312780678272247
Save ckpt to ckpt/ag_news_transformeral_l5_pad25//ag_news_transformeral_l5.pt  ,ep 0
Test Epoch0 layer1 Acc 0.9111842105263158, AUC 0.9794387817382812, avg_entr 0.17305469512939453
Test Epoch0 layer2 Acc 0.9078947368421053, AUC 0.9796464443206787, avg_entr 0.17824916541576385
Test Epoch0 layer3 Acc 0.8955263157894737, AUC 0.9799298644065857, avg_entr 0.2021946758031845
Test Epoch0 layer4 Acc 0.8764473684210526, AUC 0.980126142501831, avg_entr 0.23840615153312683
gc 0
Train Epoch1 Acc 0.9267416666666667 (111209/120000), AUC 0.9838194251060486
Test Epoch1 layer0 Acc 0.9131578947368421, AUC 0.981526255607605, avg_entr 0.1337319016456604
Save ckpt to ckpt/ag_news_transformeral_l5_pad25//ag_news_transformeral_l5.pt  ,ep 1
Test Epoch1 layer1 Acc 0.9155263157894736, AUC 0.9812794327735901, avg_entr 0.09360147267580032
Test Epoch1 layer2 Acc 0.9156578947368421, AUC 0.9810701608657837, avg_entr 0.07762838900089264
Test Epoch1 layer3 Acc 0.9168421052631579, AUC 0.981336236000061, avg_entr 0.06538041681051254
Test Epoch1 layer4 Acc 0.9164473684210527, AUC 0.9811005592346191, avg_entr 0.06177644804120064
gc 0
Train Epoch2 Acc 0.939325 (112719/120000), AUC 0.9881591796875
Test Epoch2 layer0 Acc 0.9138157894736842, AUC 0.9819915294647217, avg_entr 0.10644860565662384
Save ckpt to ckpt/ag_news_transformeral_l5_pad25//ag_news_transformeral_l5.pt  ,ep 2
Test Epoch2 layer1 Acc 0.9180263157894737, AUC 0.9814256429672241, avg_entr 0.052610673010349274
Test Epoch2 layer2 Acc 0.9182894736842105, AUC 0.9810963869094849, avg_entr 0.0396743044257164
Test Epoch2 layer3 Acc 0.9185526315789474, AUC 0.9798405170440674, avg_entr 0.03561810776591301
Test Epoch2 layer4 Acc 0.9184210526315789, AUC 0.9809201955795288, avg_entr 0.03335847705602646
gc 0
Train Epoch3 Acc 0.9465833333333333 (113590/120000), AUC 0.9898802042007446
Test Epoch3 layer0 Acc 0.9134210526315789, AUC 0.9815500974655151, avg_entr 0.09136898815631866
Test Epoch3 layer1 Acc 0.9173684210526316, AUC 0.979870080947876, avg_entr 0.03816838935017586
Test Epoch3 layer2 Acc 0.9169736842105263, AUC 0.9805396795272827, avg_entr 0.03063662350177765
Test Epoch3 layer3 Acc 0.9172368421052631, AUC 0.9796924591064453, avg_entr 0.02817041054368019
Test Epoch3 layer4 Acc 0.9169736842105263, AUC 0.980076789855957, avg_entr 0.02655418962240219
gc 0
Train Epoch4 Acc 0.95205 (114246/120000), AUC 0.9909439086914062
Test Epoch4 layer0 Acc 0.9138157894736842, AUC 0.9812697172164917, avg_entr 0.08206363767385483
Test Epoch4 layer1 Acc 0.9176315789473685, AUC 0.9799879193305969, avg_entr 0.03161723166704178
Test Epoch4 layer2 Acc 0.9175, AUC 0.9809865951538086, avg_entr 0.025675546377897263
Test Epoch4 layer3 Acc 0.9177631578947368, AUC 0.9815505743026733, avg_entr 0.023738570511341095
Test Epoch4 layer4 Acc 0.9181578947368421, AUC 0.981939435005188, avg_entr 0.02175661362707615
gc 0
Train Epoch5 Acc 0.9558 (114696/120000), AUC 0.991762638092041
Test Epoch5 layer0 Acc 0.9139473684210526, AUC 0.981080949306488, avg_entr 0.07626975327730179
Test Epoch5 layer1 Acc 0.9152631578947369, AUC 0.9785448312759399, avg_entr 0.028512034565210342
Test Epoch5 layer2 Acc 0.9164473684210527, AUC 0.9801868200302124, avg_entr 0.022536439821124077
Test Epoch5 layer3 Acc 0.916578947368421, AUC 0.9788058996200562, avg_entr 0.020590675994753838
Test Epoch5 layer4 Acc 0.9164473684210527, AUC 0.9780901074409485, avg_entr 0.018432725220918655
gc 0
Train Epoch6 Acc 0.9587916666666667 (115055/120000), AUC 0.9925981760025024
Test Epoch6 layer0 Acc 0.9139473684210526, AUC 0.9807240962982178, avg_entr 0.07075106352567673
Test Epoch6 layer1 Acc 0.9140789473684211, AUC 0.9763790965080261, avg_entr 0.024952800944447517
Test Epoch6 layer2 Acc 0.9139473684210526, AUC 0.9771536588668823, avg_entr 0.018421946093440056
Test Epoch6 layer3 Acc 0.9140789473684211, AUC 0.9776917695999146, avg_entr 0.01647338457405567
Test Epoch6 layer4 Acc 0.9139473684210526, AUC 0.9768761396408081, avg_entr 0.014568888582289219
gc 0
Train Epoch7 Acc 0.9635 (115620/120000), AUC 0.9941295981407166
Test Epoch7 layer0 Acc 0.9126315789473685, AUC 0.9804968237876892, avg_entr 0.06778058409690857
Test Epoch7 layer1 Acc 0.9125, AUC 0.9764986038208008, avg_entr 0.024518143385648727
Test Epoch7 layer2 Acc 0.9131578947368421, AUC 0.978684663772583, avg_entr 0.018386417999863625
Test Epoch7 layer3 Acc 0.9130263157894737, AUC 0.9786453247070312, avg_entr 0.016868485137820244
Test Epoch7 layer4 Acc 0.9130263157894737, AUC 0.9791534543037415, avg_entr 0.015120702795684338
gc 0
Train Epoch8 Acc 0.9648 (115776/120000), AUC 0.9943676590919495
Test Epoch8 layer0 Acc 0.9121052631578948, AUC 0.9801968336105347, avg_entr 0.06512650847434998
Test Epoch8 layer1 Acc 0.9118421052631579, AUC 0.9747156500816345, avg_entr 0.02393985725939274
Test Epoch8 layer2 Acc 0.9117105263157895, AUC 0.9765214323997498, avg_entr 0.01820305734872818
Test Epoch8 layer3 Acc 0.9114473684210527, AUC 0.9775053262710571, avg_entr 0.016735685989260674
Test Epoch8 layer4 Acc 0.9114473684210527, AUC 0.9781732559204102, avg_entr 0.01502812560647726
gc 0
Train Epoch9 Acc 0.9664083333333333 (115969/120000), AUC 0.9947798848152161
Test Epoch9 layer0 Acc 0.9122368421052631, AUC 0.9802201390266418, avg_entr 0.06250634789466858
Test Epoch9 layer1 Acc 0.9142105263157895, AUC 0.974221408367157, avg_entr 0.022170867770910263
Test Epoch9 layer2 Acc 0.9144736842105263, AUC 0.9770109057426453, avg_entr 0.016260184347629547
Test Epoch9 layer3 Acc 0.9146052631578947, AUC 0.9770774841308594, avg_entr 0.01469346322119236
Test Epoch9 layer4 Acc 0.9140789473684211, AUC 0.9763645529747009, avg_entr 0.013073922134935856
gc 0
Train Epoch10 Acc 0.967275 (116073/120000), AUC 0.9949985146522522
Test Epoch10 layer0 Acc 0.9106578947368421, AUC 0.9797732830047607, avg_entr 0.05909423530101776
Test Epoch10 layer1 Acc 0.9122368421052631, AUC 0.975021481513977, avg_entr 0.022087283432483673
Test Epoch10 layer2 Acc 0.911578947368421, AUC 0.9781219959259033, avg_entr 0.015369044616818428
Test Epoch10 layer3 Acc 0.9113157894736842, AUC 0.9790844917297363, avg_entr 0.01367083378136158
Test Epoch10 layer4 Acc 0.9114473684210527, AUC 0.9789958000183105, avg_entr 0.012226656079292297
gc 0
Train Epoch11 Acc 0.9699833333333333 (116398/120000), AUC 0.9955507516860962
Test Epoch11 layer0 Acc 0.9103947368421053, AUC 0.9799392819404602, avg_entr 0.0579555407166481
Test Epoch11 layer1 Acc 0.9097368421052632, AUC 0.9742389917373657, avg_entr 0.021941129118204117
Test Epoch11 layer2 Acc 0.9089473684210526, AUC 0.9762405753135681, avg_entr 0.015568258240818977
Test Epoch11 layer3 Acc 0.9093421052631578, AUC 0.9766886234283447, avg_entr 0.013806949369609356
Test Epoch11 layer4 Acc 0.9092105263157895, AUC 0.9748657941818237, avg_entr 0.012114854529500008
gc 0
Train Epoch12 Acc 0.9705916666666666 (116471/120000), AUC 0.9955753087997437
Test Epoch12 layer0 Acc 0.9121052631578948, AUC 0.979745626449585, avg_entr 0.056225065141916275
Test Epoch12 layer1 Acc 0.9121052631578948, AUC 0.9726820588111877, avg_entr 0.020706556737422943
Test Epoch12 layer2 Acc 0.9107894736842105, AUC 0.9729673862457275, avg_entr 0.01488422229886055
Test Epoch12 layer3 Acc 0.9090789473684211, AUC 0.972451388835907, avg_entr 0.013299998827278614
Test Epoch12 layer4 Acc 0.9088157894736842, AUC 0.9740347862243652, avg_entr 0.011591962538659573
gc 0
Train Epoch13 Acc 0.9711 (116532/120000), AUC 0.995819628238678
Test Epoch13 layer0 Acc 0.9111842105263158, AUC 0.979621946811676, avg_entr 0.05431227758526802
Test Epoch13 layer1 Acc 0.9114473684210527, AUC 0.9718279838562012, avg_entr 0.01978411339223385
Test Epoch13 layer2 Acc 0.9096052631578947, AUC 0.9733469486236572, avg_entr 0.013871102593839169
Test Epoch13 layer3 Acc 0.9094736842105263, AUC 0.9729322195053101, avg_entr 0.012153481133282185
Test Epoch13 layer4 Acc 0.9093421052631578, AUC 0.9708529710769653, avg_entr 0.010725189000368118
gc 0
Train Epoch14 Acc 0.9718166666666667 (116618/120000), AUC 0.9958530068397522
Test Epoch14 layer0 Acc 0.9103947368421053, AUC 0.9795711636543274, avg_entr 0.052085068076848984
Test Epoch14 layer1 Acc 0.9111842105263158, AUC 0.9724276661872864, avg_entr 0.01947292685508728
Test Epoch14 layer2 Acc 0.9103947368421053, AUC 0.973639965057373, avg_entr 0.013250024057924747
Test Epoch14 layer3 Acc 0.91, AUC 0.9751145839691162, avg_entr 0.011366693302989006
Test Epoch14 layer4 Acc 0.9098684210526315, AUC 0.9726746082305908, avg_entr 0.010008545592427254
gc 0
Train Epoch15 Acc 0.9731166666666666 (116774/120000), AUC 0.9961605072021484
Test Epoch15 layer0 Acc 0.910921052631579, AUC 0.9794532060623169, avg_entr 0.05063123255968094
Test Epoch15 layer1 Acc 0.9111842105263158, AUC 0.9709181189537048, avg_entr 0.01874157413840294
Test Epoch15 layer2 Acc 0.9101315789473684, AUC 0.9707896113395691, avg_entr 0.01255639549344778
Test Epoch15 layer3 Acc 0.9096052631578947, AUC 0.9701107144355774, avg_entr 0.010551256127655506
Test Epoch15 layer4 Acc 0.9092105263157895, AUC 0.9692338109016418, avg_entr 0.009216631762683392
gc 0
Train Epoch16 Acc 0.9735 (116820/120000), AUC 0.9962000846862793
Test Epoch16 layer0 Acc 0.9103947368421053, AUC 0.9793933629989624, avg_entr 0.049335550516843796
Test Epoch16 layer1 Acc 0.9121052631578948, AUC 0.9708636999130249, avg_entr 0.018339011818170547
Test Epoch16 layer2 Acc 0.9110526315789473, AUC 0.9720696806907654, avg_entr 0.012522129341959953
Test Epoch16 layer3 Acc 0.9107894736842105, AUC 0.9708918333053589, avg_entr 0.01069437526166439
Test Epoch16 layer4 Acc 0.9105263157894737, AUC 0.9712727665901184, avg_entr 0.009389122016727924
gc 0
Train Epoch17 Acc 0.9738583333333334 (116863/120000), AUC 0.9963007569313049
Test Epoch17 layer0 Acc 0.9106578947368421, AUC 0.9793515205383301, avg_entr 0.0482199564576149
Test Epoch17 layer1 Acc 0.9111842105263158, AUC 0.9711429476737976, avg_entr 0.01796817220747471
Test Epoch17 layer2 Acc 0.9098684210526315, AUC 0.9722132682800293, avg_entr 0.012129741720855236
Test Epoch17 layer3 Acc 0.9096052631578947, AUC 0.9712032079696655, avg_entr 0.010327018797397614
Test Epoch17 layer4 Acc 0.9093421052631578, AUC 0.9701573848724365, avg_entr 0.00918764341622591
gc 0
Train Epoch18 Acc 0.9741666666666666 (116900/120000), AUC 0.9963587522506714
Test Epoch18 layer0 Acc 0.9097368421052632, AUC 0.979337751865387, avg_entr 0.046448953449726105
Test Epoch18 layer1 Acc 0.9111842105263158, AUC 0.9708964824676514, avg_entr 0.01749352365732193
Test Epoch18 layer2 Acc 0.9106578947368421, AUC 0.971609890460968, avg_entr 0.011605056934058666
Test Epoch18 layer3 Acc 0.91, AUC 0.9705477952957153, avg_entr 0.009780216962099075
Test Epoch18 layer4 Acc 0.9093421052631578, AUC 0.9688524603843689, avg_entr 0.008741173893213272
gc 0
Train Epoch19 Acc 0.9748416666666667 (116981/120000), AUC 0.9963842630386353
Test Epoch19 layer0 Acc 0.91, AUC 0.9792879223823547, avg_entr 0.04563657194375992
Test Epoch19 layer1 Acc 0.9119736842105263, AUC 0.9705834984779358, avg_entr 0.017316602170467377
Test Epoch19 layer2 Acc 0.9105263157894737, AUC 0.9710050821304321, avg_entr 0.011767027899622917
Test Epoch19 layer3 Acc 0.9103947368421053, AUC 0.9693981409072876, avg_entr 0.01007607951760292
Test Epoch19 layer4 Acc 0.9103947368421053, AUC 0.9678354263305664, avg_entr 0.008924615569412708
gc 0
Train Epoch20 Acc 0.9750166666666666 (117002/120000), AUC 0.9964643716812134
Test Epoch20 layer0 Acc 0.9105263157894737, AUC 0.979284942150116, avg_entr 0.04468933492898941
Test Epoch20 layer1 Acc 0.9107894736842105, AUC 0.9704300761222839, avg_entr 0.01711326278746128
Test Epoch20 layer2 Acc 0.9098684210526315, AUC 0.9704738259315491, avg_entr 0.011670883744955063
Test Epoch20 layer3 Acc 0.9093421052631578, AUC 0.9698681831359863, avg_entr 0.009846673347055912
Test Epoch20 layer4 Acc 0.9086842105263158, AUC 0.9676504731178284, avg_entr 0.008647888898849487
gc 0
Train Epoch21 Acc 0.9750666666666666 (117008/120000), AUC 0.9965296387672424
Test Epoch21 layer0 Acc 0.9105263157894737, AUC 0.9792949557304382, avg_entr 0.04357292130589485
Test Epoch21 layer1 Acc 0.9111842105263158, AUC 0.9706773161888123, avg_entr 0.01678401604294777
Test Epoch21 layer2 Acc 0.9096052631578947, AUC 0.9716625809669495, avg_entr 0.011570388451218605
Test Epoch21 layer3 Acc 0.9089473684210526, AUC 0.9701276421546936, avg_entr 0.009740215726196766
Test Epoch21 layer4 Acc 0.9085526315789474, AUC 0.968571662902832, avg_entr 0.008640522137284279
gc 0
Train Epoch22 Acc 0.9753166666666667 (117038/120000), AUC 0.9964941740036011
Test Epoch22 layer0 Acc 0.9102631578947369, AUC 0.9792768359184265, avg_entr 0.042233020067214966
Test Epoch22 layer1 Acc 0.9106578947368421, AUC 0.970748782157898, avg_entr 0.016310233622789383
Test Epoch22 layer2 Acc 0.9090789473684211, AUC 0.9720453023910522, avg_entr 0.011035563424229622
Test Epoch22 layer3 Acc 0.9086842105263158, AUC 0.9708801507949829, avg_entr 0.009192432276904583
Test Epoch22 layer4 Acc 0.9086842105263158, AUC 0.9697149991989136, avg_entr 0.008081359788775444
gc 0
Train Epoch23 Acc 0.9757333333333333 (117088/120000), AUC 0.996567964553833
Test Epoch23 layer0 Acc 0.9106578947368421, AUC 0.9792485237121582, avg_entr 0.04182933643460274
Test Epoch23 layer1 Acc 0.9107894736842105, AUC 0.9704416990280151, avg_entr 0.016224751248955727
Test Epoch23 layer2 Acc 0.91, AUC 0.9713164567947388, avg_entr 0.010929477401077747
Test Epoch23 layer3 Acc 0.9097368421052632, AUC 0.9689863324165344, avg_entr 0.009164434857666492
Test Epoch23 layer4 Acc 0.9093421052631578, AUC 0.9677687287330627, avg_entr 0.00807205867022276
gc 0
Train Epoch24 Acc 0.97565 (117078/120000), AUC 0.9964704513549805
Test Epoch24 layer0 Acc 0.9101315789473684, AUC 0.9792377948760986, avg_entr 0.04129038378596306
Test Epoch24 layer1 Acc 0.9114473684210527, AUC 0.9703755378723145, avg_entr 0.01622246578335762
Test Epoch24 layer2 Acc 0.91, AUC 0.9711898565292358, avg_entr 0.010982655920088291
Test Epoch24 layer3 Acc 0.9094736842105263, AUC 0.9692273736000061, avg_entr 0.009245366789400578
Test Epoch24 layer4 Acc 0.9093421052631578, AUC 0.9674252867698669, avg_entr 0.008304154500365257
gc 0
Train Epoch25 Acc 0.97575 (117090/120000), AUC 0.9967027902603149
Test Epoch25 layer0 Acc 0.91, AUC 0.9792395830154419, avg_entr 0.040800780057907104
Test Epoch25 layer1 Acc 0.9114473684210527, AUC 0.9704341888427734, avg_entr 0.015803737565875053
Test Epoch25 layer2 Acc 0.9096052631578947, AUC 0.9717004299163818, avg_entr 0.010222064331173897
Test Epoch25 layer3 Acc 0.91, AUC 0.9695349931716919, avg_entr 0.008502190001308918
Test Epoch25 layer4 Acc 0.9096052631578947, AUC 0.9680995941162109, avg_entr 0.00765341566875577
gc 0
Train Epoch26 Acc 0.9756333333333334 (117076/120000), AUC 0.996732234954834
Test Epoch26 layer0 Acc 0.9105263157894737, AUC 0.9792250394821167, avg_entr 0.040469519793987274
Test Epoch26 layer1 Acc 0.9106578947368421, AUC 0.9704774618148804, avg_entr 0.015887590125203133
Test Epoch26 layer2 Acc 0.9092105263157895, AUC 0.9714020490646362, avg_entr 0.010690066032111645
Test Epoch26 layer3 Acc 0.9082894736842105, AUC 0.97039395570755, avg_entr 0.008937212638556957
Test Epoch26 layer4 Acc 0.9078947368421053, AUC 0.968292236328125, avg_entr 0.007903619669377804
gc 0
Train Epoch27 Acc 0.976125 (117135/120000), AUC 0.9968386888504028
Test Epoch27 layer0 Acc 0.9102631578947369, AUC 0.979221522808075, avg_entr 0.040197573602199554
Test Epoch27 layer1 Acc 0.9111842105263158, AUC 0.9703282713890076, avg_entr 0.015787504613399506
Test Epoch27 layer2 Acc 0.9093421052631578, AUC 0.9712462425231934, avg_entr 0.01045223418623209
Test Epoch27 layer3 Acc 0.9094736842105263, AUC 0.9685859084129333, avg_entr 0.008617614395916462
Test Epoch27 layer4 Acc 0.9086842105263158, AUC 0.9668540358543396, avg_entr 0.007700753398239613
gc 0
Train Epoch28 Acc 0.9762416666666667 (117149/120000), AUC 0.996772825717926
Test Epoch28 layer0 Acc 0.9101315789473684, AUC 0.9792056083679199, avg_entr 0.04000600427389145
Test Epoch28 layer1 Acc 0.910921052631579, AUC 0.9703490138053894, avg_entr 0.015821412205696106
Test Epoch28 layer2 Acc 0.9093421052631578, AUC 0.9714012742042542, avg_entr 0.010618406347930431
Test Epoch28 layer3 Acc 0.9093421052631578, AUC 0.9703442454338074, avg_entr 0.008881178684532642
Test Epoch28 layer4 Acc 0.9082894736842105, AUC 0.96844482421875, avg_entr 0.007968624122440815
gc 0
Train Epoch29 Acc 0.9763666666666667 (117164/120000), AUC 0.9968061447143555
Test Epoch29 layer0 Acc 0.9101315789473684, AUC 0.979194164276123, avg_entr 0.03979777917265892
Test Epoch29 layer1 Acc 0.9111842105263158, AUC 0.970180869102478, avg_entr 0.015723641961812973
Test Epoch29 layer2 Acc 0.9094736842105263, AUC 0.9709086418151855, avg_entr 0.010464278049767017
Test Epoch29 layer3 Acc 0.9092105263157895, AUC 0.9687482118606567, avg_entr 0.008658047765493393
Test Epoch29 layer4 Acc 0.9086842105263158, AUC 0.9668091535568237, avg_entr 0.007772591896355152
Best AUC 0.9819915294647217
train_loss (2, 5, 30)
valid_acc (5, 30)
valid_AUC (5, 30)
train_acc (30,)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Start Testing
Load ckpt at ckpt/ag_news_transformeral_l5_pad25//ag_news_transformeral_l5.pt
[[1711   59   85   45]
 [  17 1865    9    9]
 [  51   20 1675  154]
 [  55   16  135 1694]]
Figure(640x480)
tensor([0.1056, 0.0235, 0.1291,  ..., 0.0857, 0.0064, 0.0558])
[[1724   55   73   48]
 [  17 1863   10   10]
 [  47   14 1685  154]
 [  43   17  135 1705]]
Figure(640x480)
tensor([0.0066, 0.0051, 0.0127,  ..., 0.0121, 0.0030, 0.0210])
[[1726   53   74   47]
 [  16 1863   12    9]
 [  47   14 1691  148]
 [  41   19  141 1699]]
Figure(640x480)
tensor([0.0060, 0.0064, 0.0121,  ..., 0.0083, 0.0046, 0.0079])
[[1726   53   74   47]
 [  16 1862   12   10]
 [  47   14 1692  147]
 [  39   16  144 1701]]
Figure(640x480)
tensor([0.0068, 0.0072, 0.0108,  ..., 0.0068, 0.0056, 0.0078])
[[1728   53   72   47]
 [  16 1862   12   10]
 [  48   14 1695  143]
 [  43   16  146 1695]]
Figure(640x480)
tensor([0.0069, 0.0070, 0.0097,  ..., 0.0061, 0.0055, 0.0073])
