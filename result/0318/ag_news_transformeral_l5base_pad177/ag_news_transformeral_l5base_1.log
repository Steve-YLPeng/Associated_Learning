total count words 102019
vocab size 30000
train size 120000, valid size 3800, test size 3800
found 26754 words in glove
model: TransformerModelML(
  (layers): ModuleList(
    (0): EMBLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Embedding(30000, 300)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=4, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=4, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.weight 9000000
layers.0.ae.g.0.weight 512
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 512
layers.0.ae.h.0.bias 4
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
Total Trainable Params: 13672292
init_time 15.922812044999999
Start Training
gc 0
Train Epoch0 Acc 0.6475833333333333 (77710/120000), AUC 0.8680720329284668
ep0_train_time 150.016466232
Test Epoch0 layer4 Acc 0.9060526315789473, AUC 0.9795789122581482, avg_entr 0.1465812623500824, f1 0.9060525894165039
ep0_l4_test_time 1.3160991149999859
Save ckpt to ckpt/ag_news_transformeral_l5base_pad177//ag_news_transformeral_l5.pt  ,ep 0
gc 0
Train Epoch1 Acc 0.9195916666666667 (110351/120000), AUC 0.9813600778579712
ep1_train_time 157.283523145
Test Epoch1 layer4 Acc 0.9178947368421052, AUC 0.983397901058197, avg_entr 0.06495371460914612, f1 0.917894721031189
ep1_l4_test_time 1.254594553000004
Save ckpt to ckpt/ag_news_transformeral_l5base_pad177//ag_news_transformeral_l5.pt  ,ep 1
gc 0
Train Epoch2 Acc 0.934025 (112083/120000), AUC 0.9862977862358093
ep2_train_time 157.18544074799996
Test Epoch2 layer4 Acc 0.9213157894736842, AUC 0.9849324226379395, avg_entr 0.031695347279310226, f1 0.9213157892227173
ep2_l4_test_time 1.3419156059999864
Save ckpt to ckpt/ag_news_transformeral_l5base_pad177//ag_news_transformeral_l5.pt  ,ep 2
gc 0
Train Epoch3 Acc 0.942275 (113073/120000), AUC 0.988730788230896
ep3_train_time 157.32541117800002
Test Epoch3 layer4 Acc 0.9221052631578948, AUC 0.9828594923019409, avg_entr 0.023825235664844513, f1 0.9221052527427673
ep3_l4_test_time 1.3017605430000003
Save ckpt to ckpt/ag_news_transformeral_l5base_pad177//ag_news_transformeral_l5.pt  ,ep 3
gc 0
Train Epoch4 Acc 0.9479 (113748/120000), AUC 0.9902505278587341
ep4_train_time 157.35298027199997
Test Epoch4 layer4 Acc 0.9228947368421052, AUC 0.9835436344146729, avg_entr 0.018961913883686066, f1 0.9228947162628174
ep4_l4_test_time 1.3223304870000447
Save ckpt to ckpt/ag_news_transformeral_l5base_pad177//ag_news_transformeral_l5.pt  ,ep 4
gc 0
Train Epoch5 Acc 0.9516916666666667 (114203/120000), AUC 0.991431474685669
ep5_train_time 157.21051811400002
Test Epoch5 layer4 Acc 0.9213157894736842, AUC 0.9816071391105652, avg_entr 0.016412047669291496, f1 0.9213157892227173
ep5_l4_test_time 1.3038770110000542
gc 0
Train Epoch6 Acc 0.9554083333333333 (114649/120000), AUC 0.9919924736022949
ep6_train_time 157.22963852399994
Test Epoch6 layer4 Acc 0.921578947368421, AUC 0.9817496538162231, avg_entr 0.013034839183092117, f1 0.9215789437294006
ep6_l4_test_time 1.3039237590000994
gc 0
Train Epoch7 Acc 0.9578583333333334 (114943/120000), AUC 0.9930481910705566
ep7_train_time 157.18635578400017
Test Epoch7 layer4 Acc 0.9186842105263158, AUC 0.9816762208938599, avg_entr 0.012589697726070881, f1 0.918684184551239
ep7_l4_test_time 1.337799873999984
gc 0
Train Epoch8 Acc 0.9601666666666666 (115220/120000), AUC 0.9935088753700256
ep8_train_time 157.36238616699984
Test Epoch8 layer4 Acc 0.9192105263157895, AUC 0.9812418818473816, avg_entr 0.011118429712951183, f1 0.9192105531692505
ep8_l4_test_time 1.3229035209999438
gc 0
Train Epoch9 Acc 0.9623416666666667 (115481/120000), AUC 0.9940075278282166
ep9_train_time 139.995367256
Test Epoch9 layer4 Acc 0.9168421052631579, AUC 0.9807813763618469, avg_entr 0.01076594926416874, f1 0.9168421030044556
ep9_l4_test_time 1.3414339040000414
gc 0
Train Epoch10 Acc 0.964625 (115755/120000), AUC 0.9942058324813843
ep10_train_time 157.24567389699996
Test Epoch10 layer4 Acc 0.9168421052631579, AUC 0.9790142774581909, avg_entr 0.010789121501147747, f1 0.9168421030044556
ep10_l4_test_time 1.3099832819998483
gc 0
Train Epoch11 Acc 0.9663333333333334 (115960/120000), AUC 0.994914174079895
ep11_train_time 157.21546240599992
Test Epoch11 layer4 Acc 0.916578947368421, AUC 0.9778953194618225, avg_entr 0.009609001688659191, f1 0.9165789484977722
ep11_l4_test_time 1.303977648
gc 0
Train Epoch12 Acc 0.9683083333333333 (116197/120000), AUC 0.9949718117713928
ep12_train_time 157.22757969399981
Test Epoch12 layer4 Acc 0.9184210526315789, AUC 0.9761327505111694, avg_entr 0.0076166121289134026, f1 0.9184210896492004
ep12_l4_test_time 1.318415492999975
gc 0
Train Epoch13 Acc 0.9696666666666667 (116360/120000), AUC 0.995172381401062
ep13_train_time 157.42110650899986
Test Epoch13 layer4 Acc 0.916578947368421, AUC 0.9761454463005066, avg_entr 0.00727198226377368, f1 0.9165789484977722
ep13_l4_test_time 1.3043021519997637
gc 0
Train Epoch14 Acc 0.9715333333333334 (116584/120000), AUC 0.9956936836242676
ep14_train_time 157.18225882600018
Test Epoch14 layer4 Acc 0.9160526315789473, AUC 0.9754611253738403, avg_entr 0.007662634830921888, f1 0.9160526394844055
ep14_l4_test_time 1.3323000219997994
gc 0
Train Epoch15 Acc 0.9728333333333333 (116740/120000), AUC 0.9957178831100464
ep15_train_time 157.19960289499977
Test Epoch15 layer4 Acc 0.915, AUC 0.9743952751159668, avg_entr 0.006385669577866793, f1 0.9150000214576721
ep15_l4_test_time 1.3040703570000005
gc 0
Train Epoch16 Acc 0.97395 (116874/120000), AUC 0.9960070848464966
ep16_train_time 157.1744105480002
Test Epoch16 layer4 Acc 0.9131578947368421, AUC 0.9745078086853027, avg_entr 0.0070212786085903645, f1 0.9131578803062439
ep16_l4_test_time 1.305160837999665
gc 0
Train Epoch17 Acc 0.9754833333333334 (117058/120000), AUC 0.9960678815841675
ep17_train_time 157.17832232
Test Epoch17 layer4 Acc 0.9168421052631579, AUC 0.9727495908737183, avg_entr 0.006163320504128933, f1 0.9168421030044556
ep17_l4_test_time 1.3170640799999092
gc 0
Train Epoch18 Acc 0.9768666666666667 (117224/120000), AUC 0.9964595437049866
ep18_train_time 157.27583210900002
Test Epoch18 layer4 Acc 0.915, AUC 0.972489595413208, avg_entr 0.004197672009468079, f1 0.9150000214576721
ep18_l4_test_time 1.3182164849999936
gc 0
Train Epoch19 Acc 0.9780333333333333 (117364/120000), AUC 0.9965312480926514
ep19_train_time 140.11163016599994
Test Epoch19 layer4 Acc 0.9118421052631579, AUC 0.9719998836517334, avg_entr 0.005100307520478964, f1 0.9118421077728271
ep19_l4_test_time 1.2963745880001625
gc 0
Train Epoch20 Acc 0.9790333333333333 (117484/120000), AUC 0.9966387152671814
ep20_train_time 157.369149224
Test Epoch20 layer4 Acc 0.911578947368421, AUC 0.9697667360305786, avg_entr 0.006370913237333298, f1 0.9115789532661438
ep20_l4_test_time 1.3337771110000176
gc 0
Train Epoch21 Acc 0.9802833333333333 (117634/120000), AUC 0.996964156627655
ep21_train_time 157.19958407700005
Test Epoch21 layer4 Acc 0.9118421052631579, AUC 0.9689764976501465, avg_entr 0.004798246081918478, f1 0.9118421077728271
ep21_l4_test_time 1.3058326240002316
gc 0
Train Epoch22 Acc 0.9818 (117816/120000), AUC 0.9971849918365479
ep22_train_time 157.23767867900006
Test Epoch22 layer4 Acc 0.9071052631578947, AUC 0.9698152542114258, avg_entr 0.004846211988478899, f1 0.9071052670478821
ep22_l4_test_time 1.2443515499999194
gc 0
Train Epoch23 Acc 0.9824916666666667 (117899/120000), AUC 0.9970886707305908
ep23_train_time 157.3555955099996
Test Epoch23 layer4 Acc 0.9110526315789473, AUC 0.9681999087333679, avg_entr 0.0038324217312037945, f1 0.9110526442527771
ep23_l4_test_time 1.336967760999869
gc 0
Train Epoch24 Acc 0.9832666666666666 (117992/120000), AUC 0.9973211288452148
ep24_train_time 157.15471907599976
Test Epoch24 layer4 Acc 0.91, AUC 0.9686636924743652, avg_entr 0.005609962623566389, f1 0.9100000262260437
ep24_l4_test_time 1.3357370900002934
gc 0
Train Epoch25 Acc 0.984125 (118095/120000), AUC 0.997401237487793
ep25_train_time 157.2950992609999
Test Epoch25 layer4 Acc 0.9084210526315789, AUC 0.9688995480537415, avg_entr 0.0043777963146567345, f1 0.9084210395812988
ep25_l4_test_time 1.2967523429997527
gc 0
Train Epoch26 Acc 0.9849 (118188/120000), AUC 0.9975727796554565
ep26_train_time 157.42556597600014
Test Epoch26 layer4 Acc 0.9044736842105263, AUC 0.965959370136261, avg_entr 0.004815888125449419, f1 0.9044736623764038
ep26_l4_test_time 1.283799136000198
gc 0
Train Epoch27 Acc 0.985475 (118257/120000), AUC 0.9975717067718506
ep27_train_time 157.10899300499932
Test Epoch27 layer4 Acc 0.9076315789473685, AUC 0.9683672189712524, avg_entr 0.004782659467309713, f1 0.9076315760612488
ep27_l4_test_time 1.3455743690001327
gc 0
Train Epoch28 Acc 0.9863333333333333 (118360/120000), AUC 0.9977720975875854
ep28_train_time 157.3040104749998
Test Epoch28 layer4 Acc 0.9084210526315789, AUC 0.9661093354225159, avg_entr 0.004063774831593037, f1 0.9084210395812988
ep28_l4_test_time 1.3378598859999329
gc 0
Train Epoch29 Acc 0.987025 (118443/120000), AUC 0.9979197978973389
ep29_train_time 135.00392250999994
Test Epoch29 layer4 Acc 0.9102631578947369, AUC 0.9621667861938477, avg_entr 0.0045816972851753235, f1 0.910263180732727
ep29_l4_test_time 1.3091516719996434
Best AUC tensor(0.9229) 4
train_loss (2, 5, 30)
valid_acc (1, 30)
valid_AUC (1, 30)
train_acc (30,)
total_train+valid_time 4695.074860426
Start Testing
Load ckpt at ckpt/ag_news_transformeral_l5base_pad177//ag_news_transformeral_l5.pt
gc 9
Test layer0 Acc 0.911578947368421, AUC 0.9807285070419312, avg_entr 0.08463243395090103, f1 0.9115789532661438
l0_test_time 0.2574253379998481
gc 0
Test layer1 Acc 0.9136842105263158, AUC 0.9796686172485352, avg_entr 0.03317394480109215, f1 0.9136841893196106
l1_test_time 0.5404611650001243
gc 0
Test layer2 Acc 0.9147368421052632, AUC 0.9802753925323486, avg_entr 0.026961203664541245, f1 0.9147368669509888
l2_test_time 0.7984727619996193
gc 0
Test layer3 Acc 0.9147368421052632, AUC 0.9812895059585571, avg_entr 0.02545836567878723, f1 0.9147368669509888
l3_test_time 1.0242407289997573
gc 0
Test layer4 Acc 0.9147368421052632, AUC 0.981006920337677, avg_entr 0.023387989029288292, f1 0.9147368669509888
l4_test_time 1.3266230109993558
gc 0
Test threshold 0.1 Acc 0.9144736842105263, AUC 0.9765130877494812, avg_entr 0.026299506425857544, f1 0.9144737124443054
t0.1_test_time 1.322150775999944
gc 0
Test threshold 0.2 Acc 0.9121052631578948, AUC 0.9778756499290466, avg_entr 0.034492120146751404, f1 0.9121052622795105
t0.2_test_time 1.3281773859998793
gc 0
Test threshold 0.3 Acc 0.9105263157894737, AUC 0.9795190095901489, avg_entr 0.04651900380849838, f1 0.9105263352394104
t0.3_test_time 1.125334488999215
gc 0
Test threshold 0.4 Acc 0.9128947368421053, AUC 0.9805974960327148, avg_entr 0.05580078065395355, f1 0.9128947257995605
t0.4_test_time 0.7797910660001435
gc 0
Test threshold 0.5 Acc 0.911578947368421, AUC 0.9806576371192932, avg_entr 0.05979367345571518, f1 0.9115789532661438
t0.5_test_time 0.6567147730002034
gc 0
Test threshold 0.6 Acc 0.911578947368421, AUC 0.9807285070419312, avg_entr 0.06104939430952072, f1 0.9115789532661438
t0.6_test_time 0.6117134539999824
gc 0
Test threshold 0.7 Acc 0.911578947368421, AUC 0.9807285070419312, avg_entr 0.06104939430952072, f1 0.9115789532661438
t0.7_test_time 0.5799595420003243
gc 0
Test threshold 0.8 Acc 0.911578947368421, AUC 0.9807285070419312, avg_entr 0.06104939430952072, f1 0.9115789532661438
t0.8_test_time 0.569892121000521
gc 0
Test threshold 0.9 Acc 0.911578947368421, AUC 0.9807285070419312, avg_entr 0.06104939430952072, f1 0.9115789532661438
t0.9_test_time 0.5684894860005443
