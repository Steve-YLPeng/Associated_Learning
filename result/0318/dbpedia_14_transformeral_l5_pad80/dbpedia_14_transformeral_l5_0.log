total count words 887881
vocab size 30000
train size 560000, valid size 35000, test size 35000
found 28354 words in glove
model: TransformerModelML(
  (layers): ModuleList(
    (0): EMBLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Embedding(30000, 300)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=14, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=14, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.weight 9000000
layers.0.ae.g.0.weight 1792
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 1792
layers.0.ae.h.0.bias 14
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
Total Trainable Params: 13674862
init_time 56.22423434257507
Start Training
gc 0
Train Epoch0 Acc 0.8325160714285714 (466209/560000), AUC 0.9800885915756226
ep0_train_time 330.22374272346497
Test Epoch0 layer0 Acc 0.9727714285714286, AUC 0.9982050061225891, avg_entr 0.08559933304786682, f1 0.972771406173706
ep0_l0_test_time 1.529691457748413
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad80//dbpedia_14_transformeral_l5.pt  ,ep 0
Test Epoch0 layer1 Acc 0.9726857142857143, AUC 0.9984104037284851, avg_entr 0.03930015116930008, f1 0.972685694694519
ep0_l1_test_time 2.484273672103882
Test Epoch0 layer2 Acc 0.9733714285714286, AUC 0.9979444742202759, avg_entr 0.02705877460539341, f1 0.9733714461326599
ep0_l2_test_time 3.072113275527954
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad80//dbpedia_14_transformeral_l5.pt  ,ep 0
Test Epoch0 layer3 Acc 0.9731714285714286, AUC 0.9978967308998108, avg_entr 0.024819910526275635, f1 0.9731714129447937
ep0_l3_test_time 4.393231630325317
Test Epoch0 layer4 Acc 0.9730857142857143, AUC 0.9978402853012085, avg_entr 0.023705657571554184, f1 0.9730857014656067
ep0_l4_test_time 5.123030662536621
gc 0
Train Epoch1 Acc 0.9797857142857143 (548680/560000), AUC 0.9975991249084473
ep1_train_time 363.8990168571472
Test Epoch1 layer0 Acc 0.9736857142857143, AUC 0.9983624815940857, avg_entr 0.04713508114218712, f1 0.9736857414245605
ep1_l0_test_time 2.299699306488037
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad80//dbpedia_14_transformeral_l5.pt  ,ep 1
Test Epoch1 layer1 Acc 0.9780857142857143, AUC 0.9980751872062683, avg_entr 0.010570738464593887, f1 0.9780856966972351
ep1_l1_test_time 2.8783607482910156
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad80//dbpedia_14_transformeral_l5.pt  ,ep 1
Test Epoch1 layer2 Acc 0.9782285714285714, AUC 0.9979435205459595, avg_entr 0.006875623483210802, f1 0.9782285690307617
ep1_l2_test_time 4.005781173706055
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad80//dbpedia_14_transformeral_l5.pt  ,ep 1
Test Epoch1 layer3 Acc 0.9782, AUC 0.997814953327179, avg_entr 0.005966991186141968, f1 0.9782000184059143
ep1_l3_test_time 5.0754618644714355
Test Epoch1 layer4 Acc 0.9781428571428571, AUC 0.9977869391441345, avg_entr 0.0052329171448946, f1 0.9781428575515747
ep1_l4_test_time 6.2296953201293945
gc 0
Train Epoch2 Acc 0.9838642857142857 (550964/560000), AUC 0.9979941248893738
ep2_train_time 393.3718557357788
Test Epoch2 layer0 Acc 0.9736571428571429, AUC 0.9983946681022644, avg_entr 0.033372487872838974, f1 0.9736570715904236
ep2_l0_test_time 2.1477460861206055
Test Epoch2 layer1 Acc 0.9788857142857142, AUC 0.9978609085083008, avg_entr 0.006649641785770655, f1 0.9788857698440552
ep2_l1_test_time 2.696030855178833
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad80//dbpedia_14_transformeral_l5.pt  ,ep 2
Test Epoch2 layer2 Acc 0.9792571428571428, AUC 0.997596263885498, avg_entr 0.004352853633463383, f1 0.9792571663856506
ep2_l2_test_time 3.962402820587158
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad80//dbpedia_14_transformeral_l5.pt  ,ep 2
Test Epoch2 layer3 Acc 0.9793714285714286, AUC 0.9974616765975952, avg_entr 0.0035886671394109726, f1 0.9793714284896851
ep2_l3_test_time 5.105576992034912
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad80//dbpedia_14_transformeral_l5.pt  ,ep 2
Test Epoch2 layer4 Acc 0.9791714285714286, AUC 0.997370183467865, avg_entr 0.00317949615418911, f1 0.9791714549064636
ep2_l4_test_time 6.2190704345703125
gc 0
Train Epoch3 Acc 0.9856571428571429 (551968/560000), AUC 0.9981203079223633
ep3_train_time 393.33166575431824
Test Epoch3 layer0 Acc 0.9739428571428571, AUC 0.9984370470046997, avg_entr 0.02721789851784706, f1 0.9739428758621216
ep3_l0_test_time 2.341431140899658
Test Epoch3 layer1 Acc 0.9792285714285714, AUC 0.9976927042007446, avg_entr 0.005143554415553808, f1 0.9792285561561584
ep3_l1_test_time 2.9030728340148926
Test Epoch3 layer2 Acc 0.9792857142857143, AUC 0.9975178837776184, avg_entr 0.0032968870364129543, f1 0.979285717010498
ep3_l2_test_time 3.994403123855591
Test Epoch3 layer3 Acc 0.9792, AUC 0.9970446228981018, avg_entr 0.0027947903145104647, f1 0.9791999459266663
ep3_l3_test_time 5.101288557052612
Test Epoch3 layer4 Acc 0.9792285714285714, AUC 0.9969635009765625, avg_entr 0.002467781538143754, f1 0.9792285561561584
ep3_l4_test_time 6.235423564910889
gc 0
Train Epoch4 Acc 0.9869017857142857 (552665/560000), AUC 0.9981828927993774
ep4_train_time 393.43525886535645
Test Epoch4 layer0 Acc 0.9741142857142857, AUC 0.9984616041183472, avg_entr 0.02548416517674923, f1 0.9741142988204956
ep4_l0_test_time 2.2884013652801514
Test Epoch4 layer1 Acc 0.9793428571428572, AUC 0.9976000785827637, avg_entr 0.004432130604982376, f1 0.9793428778648376
ep4_l1_test_time 2.9170403480529785
Test Epoch4 layer2 Acc 0.9795714285714285, AUC 0.9973193407058716, avg_entr 0.0031473166309297085, f1 0.9795714020729065
ep4_l2_test_time 4.001415729522705
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad80//dbpedia_14_transformeral_l5.pt  ,ep 4
Test Epoch4 layer3 Acc 0.9794857142857143, AUC 0.9971975684165955, avg_entr 0.002653492847457528, f1 0.9794856905937195
ep4_l3_test_time 5.1213295459747314
Test Epoch4 layer4 Acc 0.9794285714285714, AUC 0.9968488812446594, avg_entr 0.002379555953666568, f1 0.9794285893440247
ep4_l4_test_time 5.890582323074341
gc 0
Train Epoch5 Acc 0.9878785714285714 (553212/560000), AUC 0.9985049366950989
ep5_train_time 393.3924558162689
Test Epoch5 layer0 Acc 0.9743714285714286, AUC 0.9984647631645203, avg_entr 0.02478179708123207, f1 0.9743714332580566
ep5_l0_test_time 2.3047258853912354
Test Epoch5 layer1 Acc 0.9790571428571428, AUC 0.9976759552955627, avg_entr 0.004146365914493799, f1 0.9790571331977844
ep5_l1_test_time 2.8912742137908936
Test Epoch5 layer2 Acc 0.9789714285714286, AUC 0.9973756074905396, avg_entr 0.0025415208656340837, f1 0.9789714217185974
ep5_l2_test_time 4.018973350524902
Test Epoch5 layer3 Acc 0.9788857142857142, AUC 0.9969285130500793, avg_entr 0.0021369908936321735, f1 0.9788857698440552
ep5_l3_test_time 5.1213600635528564
Test Epoch5 layer4 Acc 0.9790571428571428, AUC 0.9966753125190735, avg_entr 0.0019379296572878957, f1 0.9790571331977844
ep5_l4_test_time 6.20297646522522
gc 0
Train Epoch6 Acc 0.9884928571428572 (553556/560000), AUC 0.9986814260482788
ep6_train_time 393.33573174476624
Test Epoch6 layer0 Acc 0.9743428571428572, AUC 0.9984822273254395, avg_entr 0.02464236505329609, f1 0.9743428826332092
ep6_l0_test_time 2.339876890182495
Test Epoch6 layer1 Acc 0.9794, AUC 0.9976876378059387, avg_entr 0.004055793397128582, f1 0.9793999791145325
ep6_l1_test_time 2.8772215843200684
Test Epoch6 layer2 Acc 0.9796571428571429, AUC 0.9973861575126648, avg_entr 0.002618863247334957, f1 0.9796571135520935
ep6_l2_test_time 3.9878623485565186
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad80//dbpedia_14_transformeral_l5.pt  ,ep 6
Test Epoch6 layer3 Acc 0.9795142857142857, AUC 0.9969062805175781, avg_entr 0.0021982486359775066, f1 0.9795143008232117
ep6_l3_test_time 5.113611221313477
Test Epoch6 layer4 Acc 0.9795714285714285, AUC 0.9961700439453125, avg_entr 0.0019971008878201246, f1 0.9795714020729065
ep6_l4_test_time 6.1986095905303955
gc 0
Train Epoch7 Acc 0.9890607142857143 (553874/560000), AUC 0.9987519979476929
ep7_train_time 393.35767364501953
Test Epoch7 layer0 Acc 0.9747428571428571, AUC 0.9984674453735352, avg_entr 0.02398752048611641, f1 0.9747428297996521
ep7_l0_test_time 2.0274746417999268
Test Epoch7 layer1 Acc 0.9789428571428571, AUC 0.9975013732910156, avg_entr 0.004032035358250141, f1 0.97894287109375
ep7_l1_test_time 2.732402801513672
Test Epoch7 layer2 Acc 0.9783142857142857, AUC 0.9971562623977661, avg_entr 0.0025026993826031685, f1 0.9783142805099487
ep7_l2_test_time 3.9862797260284424
Test Epoch7 layer3 Acc 0.9782571428571428, AUC 0.9968417286872864, avg_entr 0.0022087907418608665, f1 0.9782571196556091
ep7_l3_test_time 5.103026390075684
Test Epoch7 layer4 Acc 0.9782857142857143, AUC 0.9962039589881897, avg_entr 0.0020045775454491377, f1 0.9782857298851013
ep7_l4_test_time 6.214874029159546
gc 0
Train Epoch8 Acc 0.9895125 (554127/560000), AUC 0.9987987875938416
ep8_train_time 393.4276821613312
Test Epoch8 layer0 Acc 0.9744571428571429, AUC 0.9984707236289978, avg_entr 0.02374066598713398, f1 0.9744571447372437
ep8_l0_test_time 2.311889886856079
Test Epoch8 layer1 Acc 0.979, AUC 0.9973058700561523, avg_entr 0.003996869549155235, f1 0.9789999723434448
ep8_l1_test_time 2.886087656021118
Test Epoch8 layer2 Acc 0.9789428571428571, AUC 0.9970765709877014, avg_entr 0.002511139027774334, f1 0.97894287109375
ep8_l2_test_time 4.017564058303833
Test Epoch8 layer3 Acc 0.9788857142857142, AUC 0.9966744780540466, avg_entr 0.0021639440674334764, f1 0.9788857698440552
ep8_l3_test_time 5.094454050064087
Test Epoch8 layer4 Acc 0.9788, AUC 0.996026337146759, avg_entr 0.002009566407650709, f1 0.9787999987602234
ep8_l4_test_time 6.240768194198608
gc 0
Train Epoch9 Acc 0.9899875 (554393/560000), AUC 0.9988363981246948
ep9_train_time 393.49478101730347
Test Epoch9 layer0 Acc 0.9747428571428571, AUC 0.9984851479530334, avg_entr 0.0238155797123909, f1 0.9747428297996521
ep9_l0_test_time 2.298567295074463
Test Epoch9 layer1 Acc 0.9794857142857143, AUC 0.9973655343055725, avg_entr 0.0038004459347575903, f1 0.9794856905937195
ep9_l1_test_time 2.8749279975891113
Test Epoch9 layer2 Acc 0.9792, AUC 0.997188150882721, avg_entr 0.0023437875788658857, f1 0.9791999459266663
ep9_l2_test_time 3.985523223876953
Test Epoch9 layer3 Acc 0.9791714285714286, AUC 0.9967320561408997, avg_entr 0.0019411769462749362, f1 0.9791714549064636
ep9_l3_test_time 5.105257511138916
Test Epoch9 layer4 Acc 0.9792285714285714, AUC 0.9959306716918945, avg_entr 0.0018420187989249825, f1 0.9792285561561584
ep9_l4_test_time 5.870312452316284
gc 0
Train Epoch10 Acc 0.9907482142857142 (554819/560000), AUC 0.9989040493965149
ep10_train_time 393.4586181640625
Test Epoch10 layer0 Acc 0.9745428571428572, AUC 0.9984696507453918, avg_entr 0.02346012555062771, f1 0.9745428562164307
ep10_l0_test_time 2.331406354904175
Test Epoch10 layer1 Acc 0.9787714285714286, AUC 0.9972659945487976, avg_entr 0.003991471603512764, f1 0.978771448135376
ep10_l1_test_time 2.9240331649780273
Test Epoch10 layer2 Acc 0.9787714285714286, AUC 0.9969328045845032, avg_entr 0.0025332393124699593, f1 0.978771448135376
ep10_l2_test_time 3.9879586696624756
Test Epoch10 layer3 Acc 0.9788, AUC 0.9965229034423828, avg_entr 0.0020875229965895414, f1 0.9787999987602234
ep10_l3_test_time 5.08948278427124
Test Epoch10 layer4 Acc 0.9787428571428571, AUC 0.9959374666213989, avg_entr 0.0019128323765471578, f1 0.9787428379058838
ep10_l4_test_time 6.228157043457031
gc 0
Train Epoch11 Acc 0.9909625 (554939/560000), AUC 0.998897135257721
ep11_train_time 393.471107006073
Test Epoch11 layer0 Acc 0.9746285714285714, AUC 0.998468816280365, avg_entr 0.02345796301960945, f1 0.9746285676956177
ep11_l0_test_time 2.3302974700927734
Test Epoch11 layer1 Acc 0.9790571428571428, AUC 0.9971312880516052, avg_entr 0.004038588143885136, f1 0.9790571331977844
ep11_l1_test_time 2.8996129035949707
Test Epoch11 layer2 Acc 0.9791142857142857, AUC 0.9968875646591187, avg_entr 0.0023011064622551203, f1 0.979114294052124
ep11_l2_test_time 3.995091438293457
Test Epoch11 layer3 Acc 0.9790571428571428, AUC 0.9965129494667053, avg_entr 0.0018567831721156836, f1 0.9790571331977844
ep11_l3_test_time 5.095857858657837
Test Epoch11 layer4 Acc 0.9788285714285714, AUC 0.9957823753356934, avg_entr 0.001704383990727365, f1 0.9788285493850708
ep11_l4_test_time 6.218692779541016
gc 0
Train Epoch12 Acc 0.9911375 (555037/560000), AUC 0.9989659190177917
ep12_train_time 372.0952694416046
Test Epoch12 layer0 Acc 0.9747428571428571, AUC 0.9984825253486633, avg_entr 0.023330653086304665, f1 0.9747428297996521
ep12_l0_test_time 2.2739028930664062
Test Epoch12 layer1 Acc 0.9790571428571428, AUC 0.9972211122512817, avg_entr 0.003802759572863579, f1 0.9790571331977844
ep12_l1_test_time 2.8720781803131104
Test Epoch12 layer2 Acc 0.9785428571428572, AUC 0.9968295097351074, avg_entr 0.002498834626749158, f1 0.9785428643226624
ep12_l2_test_time 3.988060474395752
Test Epoch12 layer3 Acc 0.9787142857142858, AUC 0.996648907661438, avg_entr 0.0020784016232937574, f1 0.9787142872810364
ep12_l3_test_time 5.060495615005493
Test Epoch12 layer4 Acc 0.9787142857142858, AUC 0.9958556890487671, avg_entr 0.001889180624857545, f1 0.9787142872810364
ep12_l4_test_time 6.224637508392334
gc 0
Train Epoch13 Acc 0.9914428571428572 (555208/560000), AUC 0.9989123940467834
ep13_train_time 393.0111918449402
Test Epoch13 layer0 Acc 0.9746857142857143, AUC 0.99847811460495, avg_entr 0.023500565439462662, f1 0.9746857285499573
ep13_l0_test_time 2.2609715461730957
Test Epoch13 layer1 Acc 0.9789714285714286, AUC 0.9972847700119019, avg_entr 0.003989685792475939, f1 0.9789714217185974
ep13_l1_test_time 2.6527090072631836
Test Epoch13 layer2 Acc 0.9788571428571429, AUC 0.9967683553695679, avg_entr 0.0022511209826916456, f1 0.978857159614563
ep13_l2_test_time 3.924602508544922
Test Epoch13 layer3 Acc 0.9789714285714286, AUC 0.9964151382446289, avg_entr 0.0019244692521169782, f1 0.9789714217185974
ep13_l3_test_time 5.081319332122803
Test Epoch13 layer4 Acc 0.9788857142857142, AUC 0.9957482218742371, avg_entr 0.0017749370308592916, f1 0.9788857698440552
ep13_l4_test_time 6.217550992965698
gc 0
Train Epoch14 Acc 0.9918053571428571 (555411/560000), AUC 0.9989390969276428
ep14_train_time 392.8983211517334
Test Epoch14 layer0 Acc 0.9745428571428572, AUC 0.9984694719314575, avg_entr 0.02354104444384575, f1 0.9745428562164307
ep14_l0_test_time 2.31672739982605
Test Epoch14 layer1 Acc 0.9787714285714286, AUC 0.9971488118171692, avg_entr 0.003931776620447636, f1 0.978771448135376
ep14_l1_test_time 2.882995843887329
Test Epoch14 layer2 Acc 0.9788285714285714, AUC 0.9968422651290894, avg_entr 0.002291911281645298, f1 0.9788285493850708
ep14_l2_test_time 3.9761688709259033
Test Epoch14 layer3 Acc 0.9788571428571429, AUC 0.9965583682060242, avg_entr 0.001905976445414126, f1 0.978857159614563
ep14_l3_test_time 5.087035417556763
Test Epoch14 layer4 Acc 0.9788571428571429, AUC 0.9958332777023315, avg_entr 0.0017513071652501822, f1 0.978857159614563
ep14_l4_test_time 6.210763454437256
gc 0
Train Epoch15 Acc 0.9919767857142857 (555507/560000), AUC 0.9989559054374695
ep15_train_time 393.0361485481262
Test Epoch15 layer0 Acc 0.9748571428571429, AUC 0.9984599947929382, avg_entr 0.023193882778286934, f1 0.9748571515083313
ep15_l0_test_time 2.3430304527282715
Test Epoch15 layer1 Acc 0.9787714285714286, AUC 0.9970707297325134, avg_entr 0.003959247842431068, f1 0.978771448135376
ep15_l1_test_time 2.8893768787384033
Test Epoch15 layer2 Acc 0.9784571428571428, AUC 0.9965863823890686, avg_entr 0.0022283599246293306, f1 0.9784571528434753
ep15_l2_test_time 3.9835236072540283
Test Epoch15 layer3 Acc 0.9784, AUC 0.9963458180427551, avg_entr 0.0017073940252885222, f1 0.9783999919891357
ep15_l3_test_time 5.085181951522827
Test Epoch15 layer4 Acc 0.9784571428571428, AUC 0.9955158829689026, avg_entr 0.0016214344650506973, f1 0.9784571528434753
ep15_l4_test_time 6.068408489227295
gc 0
Train Epoch16 Acc 0.9920714285714286 (555560/560000), AUC 0.9989595413208008
ep16_train_time 392.8712270259857
Test Epoch16 layer0 Acc 0.9746, AUC 0.9984679222106934, avg_entr 0.02337338961660862, f1 0.9746000170707703
ep16_l0_test_time 2.3436665534973145
Test Epoch16 layer1 Acc 0.9786285714285714, AUC 0.9971093535423279, avg_entr 0.004008122719824314, f1 0.9786285758018494
ep16_l1_test_time 2.906156301498413
Test Epoch16 layer2 Acc 0.9784285714285714, AUC 0.9966866374015808, avg_entr 0.002451501786708832, f1 0.9784285426139832
ep16_l2_test_time 3.9930548667907715
Test Epoch16 layer3 Acc 0.9785714285714285, AUC 0.9963840246200562, avg_entr 0.002059066668152809, f1 0.9785714149475098
ep16_l3_test_time 5.092409610748291
Test Epoch16 layer4 Acc 0.9786, AUC 0.9956881403923035, avg_entr 0.001905048731714487, f1 0.978600025177002
ep16_l4_test_time 6.193601846694946
gc 0
Train Epoch17 Acc 0.9922535714285714 (555662/560000), AUC 0.9989910125732422
ep17_train_time 393.0994825363159
Test Epoch17 layer0 Acc 0.9746571428571429, AUC 0.9984664916992188, avg_entr 0.023403992876410484, f1 0.9746571183204651
ep17_l0_test_time 2.334434747695923
Test Epoch17 layer1 Acc 0.9786571428571429, AUC 0.997042715549469, avg_entr 0.00407553743571043, f1 0.9786571264266968
ep17_l1_test_time 2.8973805904388428
Test Epoch17 layer2 Acc 0.9784, AUC 0.9966365098953247, avg_entr 0.0025186296552419662, f1 0.9783999919891357
ep17_l2_test_time 4.002151012420654
Test Epoch17 layer3 Acc 0.9784857142857143, AUC 0.9962610602378845, avg_entr 0.0019589581061154604, f1 0.9784857034683228
ep17_l3_test_time 5.091794490814209
Test Epoch17 layer4 Acc 0.9785142857142857, AUC 0.9955716133117676, avg_entr 0.0018044699681922793, f1 0.9785143136978149
ep17_l4_test_time 6.222980499267578
gc 0
Train Epoch18 Acc 0.9924125 (555751/560000), AUC 0.9990277290344238
ep18_train_time 393.03279995918274
Test Epoch18 layer0 Acc 0.9748, AUC 0.9984620809555054, avg_entr 0.023160062730312347, f1 0.9747999906539917
ep18_l0_test_time 2.3312370777130127
Test Epoch18 layer1 Acc 0.9786571428571429, AUC 0.9970529675483704, avg_entr 0.004028111230581999, f1 0.9786571264266968
ep18_l1_test_time 2.666969060897827
Test Epoch18 layer2 Acc 0.9782571428571428, AUC 0.99665367603302, avg_entr 0.0024499017745256424, f1 0.9782571196556091
ep18_l2_test_time 3.8561954498291016
Test Epoch18 layer3 Acc 0.9783428571428572, AUC 0.9962454438209534, avg_entr 0.0020216756965965033, f1 0.9783428311347961
ep18_l3_test_time 5.089511156082153
Test Epoch18 layer4 Acc 0.9783142857142857, AUC 0.995564341545105, avg_entr 0.0018884144956246018, f1 0.9783142805099487
ep18_l4_test_time 6.2141828536987305
gc 0
Train Epoch19 Acc 0.9924785714285714 (555788/560000), AUC 0.9990054965019226
ep19_train_time 393.0197219848633
Test Epoch19 layer0 Acc 0.9747714285714286, AUC 0.9984616637229919, avg_entr 0.02322504110634327, f1 0.9747714400291443
ep19_l0_test_time 2.3008882999420166
Test Epoch19 layer1 Acc 0.9786, AUC 0.9970980286598206, avg_entr 0.004003643058240414, f1 0.978600025177002
ep19_l1_test_time 2.8850393295288086
Test Epoch19 layer2 Acc 0.9784571428571428, AUC 0.9966865181922913, avg_entr 0.0024145031347870827, f1 0.9784571528434753
ep19_l2_test_time 3.9965648651123047
Test Epoch19 layer3 Acc 0.9787714285714286, AUC 0.9963083267211914, avg_entr 0.0019453257555142045, f1 0.978771448135376
ep19_l3_test_time 5.081983804702759
Test Epoch19 layer4 Acc 0.9786571428571429, AUC 0.9955969452857971, avg_entr 0.0017687251092866063, f1 0.9786571264266968
ep19_l4_test_time 6.217533588409424
gc 0
Train Epoch20 Acc 0.9925232142857143 (555813/560000), AUC 0.9989975690841675
ep20_train_time 393.2463753223419
Test Epoch20 layer0 Acc 0.9747428571428571, AUC 0.9984578490257263, avg_entr 0.023206638172268867, f1 0.9747428297996521
ep20_l0_test_time 2.289778709411621
Test Epoch20 layer1 Acc 0.9785428571428572, AUC 0.9969579577445984, avg_entr 0.003977674059569836, f1 0.9785428643226624
ep20_l1_test_time 2.876145601272583
Test Epoch20 layer2 Acc 0.9784, AUC 0.9965566396713257, avg_entr 0.002315519843250513, f1 0.9783999919891357
ep20_l2_test_time 4.010032653808594
Test Epoch20 layer3 Acc 0.9784285714285714, AUC 0.9961662292480469, avg_entr 0.0018603169592097402, f1 0.9784285426139832
ep20_l3_test_time 5.057178735733032
Test Epoch20 layer4 Acc 0.9784285714285714, AUC 0.9954676032066345, avg_entr 0.0017378799384459853, f1 0.9784285426139832
ep20_l4_test_time 6.125236511230469
gc 0
Train Epoch21 Acc 0.9926035714285715 (555858/560000), AUC 0.9989948272705078
ep21_train_time 392.9771430492401
Test Epoch21 layer0 Acc 0.9748285714285714, AUC 0.9984617233276367, avg_entr 0.02327851392328739, f1 0.9748286008834839
ep21_l0_test_time 2.3211801052093506
Test Epoch21 layer1 Acc 0.9786571428571429, AUC 0.9969705939292908, avg_entr 0.004036691039800644, f1 0.9786571264266968
ep21_l1_test_time 2.8901865482330322
Test Epoch21 layer2 Acc 0.9784285714285714, AUC 0.9966073632240295, avg_entr 0.0023092799820005894, f1 0.9784285426139832
ep21_l2_test_time 4.00278639793396
Test Epoch21 layer3 Acc 0.9784857142857143, AUC 0.9961668252944946, avg_entr 0.0019576966296881437, f1 0.9784857034683228
ep21_l3_test_time 5.099966764450073
Test Epoch21 layer4 Acc 0.9784571428571428, AUC 0.9955358505249023, avg_entr 0.001818134798668325, f1 0.9784571528434753
ep21_l4_test_time 6.22050666809082
gc 0
Train Epoch22 Acc 0.9926428571428572 (555880/560000), AUC 0.998986542224884
ep22_train_time 393.32312393188477
Test Epoch22 layer0 Acc 0.9748, AUC 0.9984608292579651, avg_entr 0.02319776453077793, f1 0.9747999906539917
ep22_l0_test_time 2.3113596439361572
Test Epoch22 layer1 Acc 0.9785142857142857, AUC 0.996938169002533, avg_entr 0.004021777305752039, f1 0.9785143136978149
ep22_l1_test_time 2.901231288909912
Test Epoch22 layer2 Acc 0.9783714285714286, AUC 0.9965490698814392, avg_entr 0.002295003505423665, f1 0.9783714413642883
ep22_l2_test_time 4.016201972961426
Test Epoch22 layer3 Acc 0.9784571428571428, AUC 0.9960821270942688, avg_entr 0.001850753091275692, f1 0.9784571528434753
ep22_l3_test_time 5.093909740447998
Test Epoch22 layer4 Acc 0.9784571428571428, AUC 0.99542236328125, avg_entr 0.0017872959142550826, f1 0.9784571528434753
ep22_l4_test_time 6.2070112228393555
gc 0
Train Epoch23 Acc 0.9927160714285714 (555921/560000), AUC 0.9990018010139465
ep23_train_time 393.16929149627686
Test Epoch23 layer0 Acc 0.9746571428571429, AUC 0.9984574913978577, avg_entr 0.023196248337626457, f1 0.9746571183204651
ep23_l0_test_time 2.3195362091064453
Test Epoch23 layer1 Acc 0.9784285714285714, AUC 0.9969176054000854, avg_entr 0.0040230415761470795, f1 0.9784285426139832
ep23_l1_test_time 2.722665309906006
Test Epoch23 layer2 Acc 0.9784, AUC 0.9965454339981079, avg_entr 0.0023697151336818933, f1 0.9783999919891357
ep23_l2_test_time 3.3911848068237305
Test Epoch23 layer3 Acc 0.9784285714285714, AUC 0.9961227178573608, avg_entr 0.0019014556892216206, f1 0.9784285426139832
ep23_l3_test_time 2.4832098484039307
Test Epoch23 layer4 Acc 0.9781714285714286, AUC 0.9954863786697388, avg_entr 0.0018023669254034758, f1 0.9781714081764221
ep23_l4_test_time 2.9911437034606934
gc 0
Train Epoch24 Acc 0.9927071428571429 (555916/560000), AUC 0.9990333914756775
ep24_train_time 378.93987822532654
Test Epoch24 layer0 Acc 0.9746857142857143, AUC 0.998461127281189, avg_entr 0.023248808458447456, f1 0.9746857285499573
ep24_l0_test_time 2.348435640335083
Test Epoch24 layer1 Acc 0.9784, AUC 0.996889054775238, avg_entr 0.004029168747365475, f1 0.9783999919891357
ep24_l1_test_time 2.8863699436187744
Test Epoch24 layer2 Acc 0.9782, AUC 0.996526300907135, avg_entr 0.0023876670747995377, f1 0.9782000184059143
ep24_l2_test_time 3.744001865386963
Test Epoch24 layer3 Acc 0.9783142857142857, AUC 0.9961881041526794, avg_entr 0.0019790716469287872, f1 0.9783142805099487
ep24_l3_test_time 4.951730489730835
Test Epoch24 layer4 Acc 0.9782285714285714, AUC 0.995395302772522, avg_entr 0.0019048860995098948, f1 0.9782285690307617
ep24_l4_test_time 6.201253414154053
gc 0
Train Epoch25 Acc 0.9927696428571429 (555951/560000), AUC 0.9990262985229492
ep25_train_time 392.9609978199005
Test Epoch25 layer0 Acc 0.9746857142857143, AUC 0.9984561204910278, avg_entr 0.023158740252256393, f1 0.9746857285499573
ep25_l0_test_time 2.293151617050171
Test Epoch25 layer1 Acc 0.9785142857142857, AUC 0.9969016313552856, avg_entr 0.004028544295579195, f1 0.9785143136978149
ep25_l1_test_time 2.8792102336883545
Test Epoch25 layer2 Acc 0.9783714285714286, AUC 0.996498703956604, avg_entr 0.002305069472640753, f1 0.9783714413642883
ep25_l2_test_time 4.0019214153289795
Test Epoch25 layer3 Acc 0.9783428571428572, AUC 0.9961188435554504, avg_entr 0.0019185768906027079, f1 0.9783428311347961
ep25_l3_test_time 5.076978445053101
Test Epoch25 layer4 Acc 0.9783428571428572, AUC 0.9954119324684143, avg_entr 0.0018136248691007495, f1 0.9783428311347961
ep25_l4_test_time 6.2074267864227295
gc 0
Train Epoch26 Acc 0.9927928571428571 (555964/560000), AUC 0.9990144968032837
ep26_train_time 392.83072805404663
Test Epoch26 layer0 Acc 0.9747428571428571, AUC 0.9984584450721741, avg_entr 0.023227035999298096, f1 0.9747428297996521
ep26_l0_test_time 2.296757459640503
Test Epoch26 layer1 Acc 0.9784, AUC 0.9969252943992615, avg_entr 0.004020991735160351, f1 0.9783999919891357
ep26_l1_test_time 2.90750789642334
Test Epoch26 layer2 Acc 0.9781714285714286, AUC 0.9965521693229675, avg_entr 0.0023615993559360504, f1 0.9781714081764221
ep26_l2_test_time 3.996572971343994
Test Epoch26 layer3 Acc 0.9782, AUC 0.9961482882499695, avg_entr 0.0019327994668856263, f1 0.9782000184059143
ep26_l3_test_time 5.114854574203491
Test Epoch26 layer4 Acc 0.9781428571428571, AUC 0.995448887348175, avg_entr 0.001835215138271451, f1 0.9781428575515747
ep26_l4_test_time 6.211242198944092
gc 0
Train Epoch27 Acc 0.9928571428571429 (556000/560000), AUC 0.9990181922912598
ep27_train_time 392.6219608783722
Test Epoch27 layer0 Acc 0.9746857142857143, AUC 0.998458981513977, avg_entr 0.023194577544927597, f1 0.9746857285499573
ep27_l0_test_time 2.3253390789031982
Test Epoch27 layer1 Acc 0.9782285714285714, AUC 0.9969082474708557, avg_entr 0.004006409086287022, f1 0.9782285690307617
ep27_l1_test_time 2.898763656616211
Test Epoch27 layer2 Acc 0.9781428571428571, AUC 0.9965670704841614, avg_entr 0.002359169302508235, f1 0.9781428575515747
ep27_l2_test_time 4.020025968551636
Test Epoch27 layer3 Acc 0.9783142857142857, AUC 0.9961193203926086, avg_entr 0.0019076102180406451, f1 0.9783142805099487
ep27_l3_test_time 5.10182785987854
Test Epoch27 layer4 Acc 0.9782571428571428, AUC 0.9954094290733337, avg_entr 0.0018374030478298664, f1 0.9782571196556091
ep27_l4_test_time 6.210317373275757
gc 0
Train Epoch28 Acc 0.9928196428571429 (555979/560000), AUC 0.9990081191062927
ep28_train_time 393.20017290115356
Test Epoch28 layer0 Acc 0.9746, AUC 0.9984604120254517, avg_entr 0.023224692791700363, f1 0.9746000170707703
ep28_l0_test_time 2.3215837478637695
Test Epoch28 layer1 Acc 0.9782285714285714, AUC 0.9969068765640259, avg_entr 0.003994069993495941, f1 0.9782285690307617
ep28_l1_test_time 2.923961877822876
Test Epoch28 layer2 Acc 0.9781714285714286, AUC 0.9965486526489258, avg_entr 0.0023568912874907255, f1 0.9781714081764221
ep28_l2_test_time 4.009236812591553
Test Epoch28 layer3 Acc 0.9782857142857143, AUC 0.9960709810256958, avg_entr 0.0018779789097607136, f1 0.9782857298851013
ep28_l3_test_time 5.099703550338745
Test Epoch28 layer4 Acc 0.9783142857142857, AUC 0.9954249262809753, avg_entr 0.0018143223132938147, f1 0.9783142805099487
ep28_l4_test_time 6.2266130447387695
gc 0
Train Epoch29 Acc 0.9929303571428572 (556041/560000), AUC 0.9990240931510925
ep29_train_time 393.111745595932
Test Epoch29 layer0 Acc 0.9745714285714285, AUC 0.9984590411186218, avg_entr 0.02319740504026413, f1 0.9745714068412781
ep29_l0_test_time 2.3688008785247803
Test Epoch29 layer1 Acc 0.9783142857142857, AUC 0.9969070553779602, avg_entr 0.003996253479272127, f1 0.9783142805099487
ep29_l1_test_time 2.9017913341522217
Test Epoch29 layer2 Acc 0.9782857142857143, AUC 0.9965203404426575, avg_entr 0.002331661991775036, f1 0.9782857298851013
ep29_l2_test_time 3.85935640335083
Test Epoch29 layer3 Acc 0.9782571428571428, AUC 0.9960755705833435, avg_entr 0.0018779441015794873, f1 0.9782571196556091
ep29_l3_test_time 4.854400396347046
Test Epoch29 layer4 Acc 0.9783142857142857, AUC 0.9953799247741699, avg_entr 0.0018369031604379416, f1 0.9783142805099487
ep29_l4_test_time 6.219287395477295
Best AUC tensor(0.9797) 6 2
train_loss (2, 5, 30)
valid_acc (5, 30)
valid_AUC (5, 30)
train_acc (30,)
total_train+valid_time 12271.836413145065
Start Testing
Load ckpt at ckpt/dbpedia_14_transformeral_l5_pad80//dbpedia_14_transformeral_l5.pt
Test layer0 Acc 0.9742857142857143, AUC 0.9982880353927612, avg_entr 0.024462923407554626, f1 0.9742856621742249
l0_test_time 2.3314316272735596
Test layer1 Acc 0.9807142857142858, AUC 0.9977054595947266, avg_entr 0.003960424102842808, f1 0.9807142615318298
l1_test_time 2.8838980197906494
Test layer2 Acc 0.9805714285714285, AUC 0.9976049661636353, avg_entr 0.0024497597478330135, f1 0.980571448802948
l2_test_time 3.9790048599243164
Test layer3 Acc 0.9806571428571429, AUC 0.9971476793289185, avg_entr 0.0020428672432899475, f1 0.980657160282135
l3_test_time 5.107415437698364
Test layer4 Acc 0.9806571428571429, AUC 0.9964503645896912, avg_entr 0.00182186474557966, f1 0.980657160282135
l4_test_time 6.198325872421265
