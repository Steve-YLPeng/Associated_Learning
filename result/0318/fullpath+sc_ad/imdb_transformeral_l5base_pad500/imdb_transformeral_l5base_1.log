total count words 222751
vocab size 30000
train size 40000, valid size 5000, test size 5000
found 27937 words in glove
model: TransformerModelML(
  (layers): ModuleList(
    (0): EMBLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Embedding(30000, 300)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=2, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=2, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.weight 9000000
layers.0.ae.g.0.weight 256
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 256
layers.0.ae.h.0.bias 2
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
Total Trainable Params: 13671778
init_time 22.472581102
Start Training
gc 0
Train Epoch0 Acc 0.514775 (20591/40000), AUC 0.5172037482261658
ep0_train_time 183.830781588
Test Epoch0 layer4 Acc 0.5508, AUC 0.8075135946273804, avg_entr 0.6842772364616394, f1 0.5508000254631042
ep0_l4_test_time 5.896123337999995
Save ckpt to ckpt/imdb_transformeral_l5base_pad500//imdb_transformeral_l5.pt  ,ep 0
gc 0
Train Epoch1 Acc 0.83505 (33402/40000), AUC 0.9120618104934692
ep1_train_time 164.751304668
Test Epoch1 layer4 Acc 0.889, AUC 0.954129695892334, avg_entr 0.13205549120903015, f1 0.8889999985694885
ep1_l4_test_time 5.860619996000025
Save ckpt to ckpt/imdb_transformeral_l5base_pad500//imdb_transformeral_l5.pt  ,ep 1
gc 0
Train Epoch2 Acc 0.911775 (36471/40000), AUC 0.9680399894714355
ep2_train_time 182.50367147900005
Test Epoch2 layer4 Acc 0.8846, AUC 0.9579991698265076, avg_entr 0.06419564038515091, f1 0.8845999836921692
ep2_l4_test_time 5.953278134000016
gc 0
Train Epoch3 Acc 0.9351 (37404/40000), AUC 0.9781680107116699
ep3_train_time 182.91775170300002
Test Epoch3 layer4 Acc 0.8856, AUC 0.9542771577835083, avg_entr 0.041867028921842575, f1 0.8855999708175659
ep3_l4_test_time 5.924081034999972
gc 0
Train Epoch4 Acc 0.947375 (37895/40000), AUC 0.983008861541748
ep4_train_time 182.81570671199995
Test Epoch4 layer4 Acc 0.8878, AUC 0.9524604082107544, avg_entr 0.038340672850608826, f1 0.8877999782562256
ep4_l4_test_time 5.8949791329999925
gc 0
Train Epoch5 Acc 0.9548 (38192/40000), AUC 0.9861065149307251
ep5_train_time 182.82029062599986
Test Epoch5 layer4 Acc 0.8892, AUC 0.9496220350265503, avg_entr 0.02569427154958248, f1 0.88919997215271
ep5_l4_test_time 5.816378550000081
Save ckpt to ckpt/imdb_transformeral_l5base_pad500//imdb_transformeral_l5.pt  ,ep 5
gc 0
Train Epoch6 Acc 0.959775 (38391/40000), AUC 0.9872565269470215
ep6_train_time 182.89692859800016
Test Epoch6 layer4 Acc 0.8726, AUC 0.9462215900421143, avg_entr 0.022007135674357414, f1 0.8726000189781189
ep6_l4_test_time 5.892497360000107
gc 0
Train Epoch7 Acc 0.96305 (38522/40000), AUC 0.9890914559364319
ep7_train_time 182.48982890399998
Test Epoch7 layer4 Acc 0.8778, AUC 0.9427448511123657, avg_entr 0.017035260796546936, f1 0.8778000473976135
ep7_l4_test_time 5.850175981999882
gc 0
Train Epoch8 Acc 0.9673 (38692/40000), AUC 0.9903613924980164
ep8_train_time 182.87167203000013
Test Epoch8 layer4 Acc 0.8774, AUC 0.9420950412750244, avg_entr 0.018951987847685814, f1 0.8773999810218811
ep8_l4_test_time 5.705715376999933
gc 0
Train Epoch9 Acc 0.969025 (38761/40000), AUC 0.9910875558853149
ep9_train_time 183.120859269
Test Epoch9 layer4 Acc 0.874, AUC 0.9392417669296265, avg_entr 0.012404651381075382, f1 0.8740000128746033
ep9_l4_test_time 5.883672867999849
gc 0
Train Epoch10 Acc 0.9728 (38912/40000), AUC 0.991990864276886
ep10_train_time 182.67265248900026
Test Epoch10 layer4 Acc 0.8744, AUC 0.9402742385864258, avg_entr 0.014136295765638351, f1 0.8744000792503357
ep10_l4_test_time 5.8486534170001505
gc 0
Train Epoch11 Acc 0.97475 (38990/40000), AUC 0.9932938814163208
ep11_train_time 164.71505283099987
Test Epoch11 layer4 Acc 0.8706, AUC 0.9376729726791382, avg_entr 0.01613553799688816, f1 0.8705999851226807
ep11_l4_test_time 5.8757720779999545
gc 0
Train Epoch12 Acc 0.9765 (39060/40000), AUC 0.9938870668411255
ep12_train_time 182.97360224900012
Test Epoch12 layer4 Acc 0.87, AUC 0.9371923208236694, avg_entr 0.015502910129725933, f1 0.8700000047683716
ep12_l4_test_time 5.916892011000073
gc 0
Train Epoch13 Acc 0.978825 (39153/40000), AUC 0.994106113910675
ep13_train_time 182.80562531899977
Test Epoch13 layer4 Acc 0.8666, AUC 0.9350528120994568, avg_entr 0.01406153291463852, f1 0.866599977016449
ep13_l4_test_time 5.907225897999979
gc 0
Train Epoch14 Acc 0.9808 (39232/40000), AUC 0.9954928159713745
ep14_train_time 182.80883219499992
Test Epoch14 layer4 Acc 0.8662, AUC 0.9337602853775024, avg_entr 0.016955852508544922, f1 0.8661999702453613
ep14_l4_test_time 5.8406877009997515
gc 0
Train Epoch15 Acc 0.981725 (39269/40000), AUC 0.9958728551864624
ep15_train_time 182.76994804600008
Test Epoch15 layer4 Acc 0.8622, AUC 0.9302959442138672, avg_entr 0.011941157281398773, f1 0.8622000217437744
ep15_l4_test_time 5.760898027999701
gc 0
Train Epoch16 Acc 0.9834 (39336/40000), AUC 0.9962965846061707
ep16_train_time 182.76259229400011
Test Epoch16 layer4 Acc 0.8614, AUC 0.9242831468582153, avg_entr 0.010951329953968525, f1 0.8614000082015991
ep16_l4_test_time 5.915192724000008
gc 0
Train Epoch17 Acc 0.984175 (39367/40000), AUC 0.996595025062561
ep17_train_time 182.61819870199997
Test Epoch17 layer4 Acc 0.863, AUC 0.9281446933746338, avg_entr 0.015575485303997993, f1 0.8629999756813049
ep17_l4_test_time 5.838598694000211
gc 0
Train Epoch18 Acc 0.98565 (39426/40000), AUC 0.9970166683197021
ep18_train_time 182.8134663339997
Test Epoch18 layer4 Acc 0.8538, AUC 0.9212619662284851, avg_entr 0.013003136962652206, f1 0.8537999987602234
ep18_l4_test_time 5.7681271780002135
gc 0
Train Epoch19 Acc 0.986275 (39451/40000), AUC 0.9971036911010742
ep19_train_time 182.88186481399998
Test Epoch19 layer4 Acc 0.8476, AUC 0.9198731184005737, avg_entr 0.01037699542939663, f1 0.847599983215332
ep19_l4_test_time 5.903619744000025
gc 0
Train Epoch20 Acc 0.986825 (39473/40000), AUC 0.9974119663238525
ep20_train_time 173.59717440800023
Test Epoch20 layer4 Acc 0.858, AUC 0.9163349270820618, avg_entr 0.009143650531768799, f1 0.8579999804496765
ep20_l4_test_time 3.117474303000108
gc 0
Train Epoch21 Acc 0.98735 (39494/40000), AUC 0.9976034164428711
ep21_train_time 177.0006905549999
Test Epoch21 layer4 Acc 0.8518, AUC 0.9162892699241638, avg_entr 0.009772083722054958, f1 0.8518000245094299
ep21_l4_test_time 5.844638307999958
gc 0
Train Epoch22 Acc 0.988325 (39533/40000), AUC 0.9978497624397278
ep22_train_time 183.04306639199967
Test Epoch22 layer4 Acc 0.852, AUC 0.9128363132476807, avg_entr 0.010370608419179916, f1 0.8519999980926514
ep22_l4_test_time 5.901565741000013
gc 0
Train Epoch23 Acc 0.98915 (39566/40000), AUC 0.9979240894317627
ep23_train_time 182.82506184300019
Test Epoch23 layer4 Acc 0.8518, AUC 0.9002827405929565, avg_entr 0.0086652971804142, f1 0.8518000245094299
ep23_l4_test_time 5.901574448000247
gc 0
Train Epoch24 Acc 0.98985 (39594/40000), AUC 0.9982149600982666
ep24_train_time 182.62757383499957
Test Epoch24 layer4 Acc 0.8486, AUC 0.8965059518814087, avg_entr 0.009008589200675488, f1 0.8485999703407288
ep24_l4_test_time 5.7733758369995485
gc 0
Train Epoch25 Acc 0.990625 (39625/40000), AUC 0.9985136389732361
ep25_train_time 182.89730277700073
Test Epoch25 layer4 Acc 0.843, AUC 0.8866431713104248, avg_entr 0.007188776507973671, f1 0.8429999947547913
ep25_l4_test_time 5.924975169999925
gc 0
Train Epoch26 Acc 0.991075 (39643/40000), AUC 0.9986259341239929
ep26_train_time 182.78685780900014
Test Epoch26 layer4 Acc 0.8442, AUC 0.8868733644485474, avg_entr 0.007567449007183313, f1 0.8442000150680542
ep26_l4_test_time 5.888156395000806
gc 0
Train Epoch27 Acc 0.991225 (39649/40000), AUC 0.9986844658851624
ep27_train_time 182.84943890900013
Test Epoch27 layer4 Acc 0.8452, AUC 0.8812522888183594, avg_entr 0.006658724043518305, f1 0.8452000021934509
ep27_l4_test_time 5.853604761000497
gc 0
Train Epoch28 Acc 0.99205 (39682/40000), AUC 0.9987309575080872
ep28_train_time 182.61623084199982
Test Epoch28 layer4 Acc 0.85, AUC 0.8832864761352539, avg_entr 0.005228062625974417, f1 0.8500000238418579
ep28_l4_test_time 5.802138430000014
gc 0
Train Epoch29 Acc 0.992425 (39697/40000), AUC 0.9990534782409668
ep29_train_time 182.78680502800034
Test Epoch29 layer4 Acc 0.8434, AUC 0.8671141862869263, avg_entr 0.006558059714734554, f1 0.8434000015258789
ep29_l4_test_time 5.898237893999976
Best AUC tensor(0.8892) 5
train_loss (2, 5, 30)
valid_acc (1, 30)
valid_AUC (1, 30)
train_acc (30,)
total_train+valid_time 5608.538896612
Start Testing
Load ckpt at ckpt/imdb_transformeral_l5base_pad500//imdb_transformeral_l5.pt
gc 9
Test layer0 Acc 0.8932, AUC 0.9548704624176025, avg_entr 0.16560186445713043, f1 0.8931999802589417
l0_test_time 0.7690179779992832
gc 0
Test layer1 Acc 0.8818, AUC 0.946485161781311, avg_entr 0.056097760796546936, f1 0.8818000555038452
l1_test_time 2.0176924919996964
gc 0
Test layer2 Acc 0.8806, AUC 0.9455900192260742, avg_entr 0.03354565426707268, f1 0.8805999755859375
l2_test_time 3.2931831449996025
gc 0
Test layer3 Acc 0.8802, AUC 0.9484759569168091, avg_entr 0.029577624052762985, f1 0.8802000284194946
l3_test_time 4.5327860980005426
gc 0
Test layer4 Acc 0.8806, AUC 0.9488284587860107, avg_entr 0.02722250111401081, f1 0.8805999755859375
l4_test_time 5.816767142000572
gc 0
Test threshold 0.1 Acc 0.8816, AUC 0.9344112873077393, avg_entr 0.05649367719888687, f1 0.881600022315979
t0.1_test_time 3.3009219659998053
gc 0
Test threshold 0.2 Acc 0.8822, AUC 0.9338196516036987, avg_entr 0.07171598821878433, f1 0.8822000026702881
t0.2_test_time 2.9678289379999114
gc 0
Test threshold 0.3 Acc 0.8836, AUC 0.9342146515846252, avg_entr 0.08777622133493423, f1 0.8835999965667725
t0.3_test_time 2.8539027139995596
gc 0
Test threshold 0.4 Acc 0.8834, AUC 0.9337435364723206, avg_entr 0.1033158078789711, f1 0.8834000825881958
t0.4_test_time 2.519446636999419
gc 0
Test threshold 0.5 Acc 0.8834, AUC 0.9341418147087097, avg_entr 0.11978964507579803, f1 0.8834000825881958
t0.5_test_time 2.6509708300000057
gc 0
Test threshold 0.6 Acc 0.884, AUC 0.9356018304824829, avg_entr 0.13371886312961578, f1 0.8840000033378601
t0.6_test_time 2.4182741180002267
gc 0
Test threshold 0.7 Acc 0.886, AUC 0.9375803470611572, avg_entr 0.1505984365940094, f1 0.8859999775886536
t0.7_test_time 2.319764693000252
gc 0
Test threshold 0.8 Acc 0.8874, AUC 0.9397202730178833, avg_entr 0.16858509182929993, f1 0.8873999714851379
t0.8_test_time 2.1567604539995955
gc 0
Test threshold 0.9 Acc 0.8892, AUC 0.9434589743614197, avg_entr 0.19195297360420227, f1 0.88919997215271
t0.9_test_time 1.9551508880003894
