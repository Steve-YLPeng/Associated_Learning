total count words 102019
vocab size 30000
train size 120000, valid size 3800, test size 3800
found 26754 words in glove
model: TransformerModelML(
  (layers): ModuleList(
    (0): EMBLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Embedding(30000, 300)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=4, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=4, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.weight 9000000
layers.0.ae.g.0.weight 512
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 512
layers.0.ae.h.0.bias 4
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
Total Trainable Params: 13672292
init_time 16.599960412999998
Start Training
gc 0
Train Epoch0 Acc 0.6332333333333333 (75988/120000), AUC 0.8597980737686157
ep0_train_time 160.21637487
Test Epoch0 layer4 Acc 0.9118421052631579, AUC 0.9794434309005737, avg_entr 0.14634577929973602, f1 0.9118421077728271
ep0_l4_test_time 1.3311103630000218
Save ckpt to ckpt/ag_news_transformeral_l5base_pad177//ag_news_transformeral_l5.pt  ,ep 0
gc 0
Train Epoch1 Acc 0.9198666666666667 (110384/120000), AUC 0.9814598560333252
ep1_train_time 159.334300077
Test Epoch1 layer4 Acc 0.9247368421052632, AUC 0.9824365973472595, avg_entr 0.05930260941386223, f1 0.9247368574142456
ep1_l4_test_time 1.33163835900001
Save ckpt to ckpt/ag_news_transformeral_l5base_pad177//ag_news_transformeral_l5.pt  ,ep 1
gc 0
Train Epoch2 Acc 0.934675 (112161/120000), AUC 0.9861427545547485
ep2_train_time 159.085154203
Test Epoch2 layer4 Acc 0.9263157894736842, AUC 0.9842135906219482, avg_entr 0.03434954583644867, f1 0.9263157844543457
ep2_l4_test_time 1.3007117700000208
Save ckpt to ckpt/ag_news_transformeral_l5base_pad177//ag_news_transformeral_l5.pt  ,ep 2
gc 0
Train Epoch3 Acc 0.9428833333333333 (113146/120000), AUC 0.9885555505752563
ep3_train_time 159.14042286700004
Test Epoch3 layer4 Acc 0.9247368421052632, AUC 0.9833394289016724, avg_entr 0.02477995678782463, f1 0.9247368574142456
ep3_l4_test_time 1.3127083919999905
gc 0
Train Epoch4 Acc 0.9482583333333333 (113791/120000), AUC 0.9902576804161072
ep4_train_time 159.14307287600002
Test Epoch4 layer4 Acc 0.9213157894736842, AUC 0.9827909469604492, avg_entr 0.019879579544067383, f1 0.9213157892227173
ep4_l4_test_time 1.3243834300000117
gc 0
Train Epoch5 Acc 0.9519 (114228/120000), AUC 0.9912192821502686
ep5_train_time 159.16027262600005
Test Epoch5 layer4 Acc 0.9202631578947369, AUC 0.9821581840515137, avg_entr 0.017209259793162346, f1 0.9202631711959839
ep5_l4_test_time 1.3239075080000475
gc 0
Train Epoch6 Acc 0.9556166666666667 (114674/120000), AUC 0.9921419024467468
ep6_train_time 159.1036522400001
Test Epoch6 layer4 Acc 0.9189473684210526, AUC 0.9787943363189697, avg_entr 0.014308109879493713, f1 0.9189472794532776
ep6_l4_test_time 1.3354196810000758
gc 0
Train Epoch7 Acc 0.9576833333333333 (114922/120000), AUC 0.9931284785270691
ep7_train_time 159.16428787600012
Test Epoch7 layer4 Acc 0.9189473684210526, AUC 0.9814934134483337, avg_entr 0.014737263321876526, f1 0.9189472794532776
ep7_l4_test_time 1.3329626679999365
gc 0
Train Epoch8 Acc 0.9607 (115284/120000), AUC 0.9936836957931519
ep8_train_time 159.15904141600004
Test Epoch8 layer4 Acc 0.9163157894736842, AUC 0.9771703481674194, avg_entr 0.013996911235153675, f1 0.9163157939910889
ep8_l4_test_time 1.3361498430001575
gc 0
Train Epoch9 Acc 0.9632916666666667 (115595/120000), AUC 0.9939655065536499
ep9_train_time 141.122411866
Test Epoch9 layer4 Acc 0.9163157894736842, AUC 0.975798487663269, avg_entr 0.012192095629870892, f1 0.9163157939910889
ep9_l4_test_time 1.3194502070000453
gc 0
Train Epoch10 Acc 0.965025 (115803/120000), AUC 0.9944257736206055
ep10_train_time 158.89781755700005
Test Epoch10 layer4 Acc 0.9160526315789473, AUC 0.977375864982605, avg_entr 0.009758012369275093, f1 0.9160526394844055
ep10_l4_test_time 1.3373512110001684
gc 0
Train Epoch11 Acc 0.9664916666666666 (115979/120000), AUC 0.9948940277099609
ep11_train_time 159.04033431899984
Test Epoch11 layer4 Acc 0.9160526315789473, AUC 0.9750138521194458, avg_entr 0.010045550763607025, f1 0.9160526394844055
ep11_l4_test_time 1.2968215429998509
gc 0
Train Epoch12 Acc 0.9683583333333333 (116203/120000), AUC 0.9951971769332886
ep12_train_time 159.07693060100019
Test Epoch12 layer4 Acc 0.9168421052631579, AUC 0.9757797122001648, avg_entr 0.008222430944442749, f1 0.9168421030044556
ep12_l4_test_time 1.3294863119999718
gc 0
Train Epoch13 Acc 0.9704916666666666 (116459/120000), AUC 0.9954531788825989
ep13_train_time 159.02820191
Test Epoch13 layer4 Acc 0.915, AUC 0.9733185172080994, avg_entr 0.00878226663917303, f1 0.9150000214576721
ep13_l4_test_time 1.3149867479996828
gc 0
Train Epoch14 Acc 0.9720083333333334 (116641/120000), AUC 0.9955930709838867
ep14_train_time 158.99091062300022
Test Epoch14 layer4 Acc 0.9121052631578948, AUC 0.9726085662841797, avg_entr 0.008176402188837528, f1 0.9121052622795105
ep14_l4_test_time 1.3251331129999926
gc 0
Train Epoch15 Acc 0.973075 (116769/120000), AUC 0.9958856105804443
ep15_train_time 159.16409398899987
Test Epoch15 layer4 Acc 0.9113157894736842, AUC 0.9682021737098694, avg_entr 0.007975012063980103, f1 0.9113157987594604
ep15_l4_test_time 1.2633846099997754
gc 0
Train Epoch16 Acc 0.9750333333333333 (117004/120000), AUC 0.9959574937820435
ep16_train_time 159.06485369099983
Test Epoch16 layer4 Acc 0.9121052631578948, AUC 0.9710507988929749, avg_entr 0.007391289342194796, f1 0.9121052622795105
ep16_l4_test_time 1.3303764970000884
gc 0
Train Epoch17 Acc 0.9763416666666667 (117161/120000), AUC 0.9961959719657898
ep17_train_time 159.1273281040003
Test Epoch17 layer4 Acc 0.9136842105263158, AUC 0.9676008224487305, avg_entr 0.005341781303286552, f1 0.9136841893196106
ep17_l4_test_time 1.3098592300002565
gc 0
Train Epoch18 Acc 0.97775 (117330/120000), AUC 0.9964647889137268
ep18_train_time 158.9485833069998
Test Epoch18 layer4 Acc 0.9102631578947369, AUC 0.9667819142341614, avg_entr 0.006956588011234999, f1 0.910263180732727
ep18_l4_test_time 1.3404028210002252
gc 0
Train Epoch19 Acc 0.979175 (117501/120000), AUC 0.9966448545455933
ep19_train_time 159.01549693399966
Test Epoch19 layer4 Acc 0.9134210526315789, AUC 0.9686803221702576, avg_entr 0.006729075685143471, f1 0.9134210348129272
ep19_l4_test_time 1.3361164719999579
gc 0
Train Epoch20 Acc 0.9804 (117648/120000), AUC 0.9967929124832153
ep20_train_time 141.13055811000004
Test Epoch20 layer4 Acc 0.9068421052631579, AUC 0.9657660126686096, avg_entr 0.006067159119993448, f1 0.9068421125411987
ep20_l4_test_time 1.3016721779999898
gc 0
Train Epoch21 Acc 0.9809333333333333 (117712/120000), AUC 0.9969559907913208
ep21_train_time 158.90175039899987
Test Epoch21 layer4 Acc 0.9063157894736842, AUC 0.9658464193344116, avg_entr 0.005158646032214165, f1 0.906315803527832
ep21_l4_test_time 1.3097559799998635
gc 0
Train Epoch22 Acc 0.9826083333333333 (117913/120000), AUC 0.9971617460250854
ep22_train_time 158.9482551899996
Test Epoch22 layer4 Acc 0.9086842105263158, AUC 0.9681857824325562, avg_entr 0.004313095472753048, f1 0.9086841940879822
ep22_l4_test_time 1.3261234860001423
gc 0
Train Epoch23 Acc 0.98315 (117978/120000), AUC 0.9972471594810486
ep23_train_time 159.0225185879999
Test Epoch23 layer4 Acc 0.9076315789473685, AUC 0.9642704725265503, avg_entr 0.0036844133865088224, f1 0.9076315760612488
ep23_l4_test_time 1.3255583690001913
gc 0
Train Epoch24 Acc 0.984175 (118101/120000), AUC 0.9973454475402832
ep24_train_time 159.08099665100008
Test Epoch24 layer4 Acc 0.9071052631578947, AUC 0.9645131230354309, avg_entr 0.00629548542201519, f1 0.9071052670478821
ep24_l4_test_time 1.2493039339997267
gc 0
Train Epoch25 Acc 0.985125 (118215/120000), AUC 0.9976637959480286
ep25_train_time 159.11159639200014
Test Epoch25 layer4 Acc 0.9042105263157895, AUC 0.9604241251945496, avg_entr 0.004466258455067873, f1 0.9042104482650757
ep25_l4_test_time 1.3316554330003783
gc 0
Train Epoch26 Acc 0.985675 (118281/120000), AUC 0.9976634383201599
ep26_train_time 159.0594467410001
Test Epoch26 layer4 Acc 0.9076315789473685, AUC 0.9628875851631165, avg_entr 0.004973425064235926, f1 0.9076315760612488
ep26_l4_test_time 1.336826462999852
gc 0
Train Epoch27 Acc 0.9863083333333333 (118357/120000), AUC 0.9977607131004333
ep27_train_time 159.0442321139999
Test Epoch27 layer4 Acc 0.9073684210526316, AUC 0.9606298804283142, avg_entr 0.0043801297433674335, f1 0.9073684215545654
ep27_l4_test_time 1.3390086219997102
gc 0
Train Epoch28 Acc 0.9873083333333333 (118477/120000), AUC 0.9979470372200012
ep28_train_time 159.09380938100003
Test Epoch28 layer4 Acc 0.9044736842105263, AUC 0.9636312127113342, avg_entr 0.004846202675253153, f1 0.9044736623764038
ep28_l4_test_time 1.3210022459998072
gc 0
Train Epoch29 Acc 0.98775 (118530/120000), AUC 0.9980046153068542
ep29_train_time 159.25409997799943
Test Epoch29 layer4 Acc 0.9073684210526316, AUC 0.962113618850708, avg_entr 0.0038208146579563618, f1 0.9073684215545654
ep29_l4_test_time 1.3080986519998987
Best AUC tensor(0.9263) 2
train_loss (2, 5, 30)
valid_acc (1, 30)
valid_AUC (1, 30)
train_acc (30,)
total_train+valid_time 4778.804721191
Start Testing
Load ckpt at ckpt/ag_news_transformeral_l5base_pad177//ag_news_transformeral_l5.pt
gc 9
Test layer0 Acc 0.91, AUC 0.9789837598800659, avg_entr 0.11813264340162277, f1 0.9100000262260437
l0_test_time 0.27279696200002945
gc 0
Test layer1 Acc 0.9160526315789473, AUC 0.9813364744186401, avg_entr 0.050286371260881424, f1 0.9160526394844055
l1_test_time 0.5652375839999877
gc 0
Test layer2 Acc 0.916578947368421, AUC 0.9812043309211731, avg_entr 0.04486981779336929, f1 0.9165789484977722
l2_test_time 0.8262550130002637
gc 0
Test layer3 Acc 0.9171052631578948, AUC 0.9818795919418335, avg_entr 0.04130300134420395, f1 0.9171052575111389
l3_test_time 1.0246251759999723
gc 0
Test layer4 Acc 0.9160526315789473, AUC 0.9820709824562073, avg_entr 0.03916095569729805, f1 0.9160526394844055
l4_test_time 1.3417699829997218
gc 0
Test threshold 0.1 Acc 0.9155263157894736, AUC 0.9768177270889282, avg_entr 0.04185814410448074, f1 0.9155263304710388
t0.1_test_time 1.2589348229994357
gc 0
Test threshold 0.2 Acc 0.9142105263157895, AUC 0.9764295816421509, avg_entr 0.052137281745672226, f1 0.9142104983329773
t0.2_test_time 1.1797587099999873
gc 0
Test threshold 0.3 Acc 0.9128947368421053, AUC 0.9779574871063232, avg_entr 0.06311105191707611, f1 0.9128947257995605
t0.3_test_time 1.2066564189999553
gc 0
Test threshold 0.4 Acc 0.911578947368421, AUC 0.9790222644805908, avg_entr 0.0761074349284172, f1 0.9115789532661438
t0.4_test_time 0.9347023890004493
gc 0
Test threshold 0.5 Acc 0.9113157894736842, AUC 0.9790120124816895, avg_entr 0.08330506086349487, f1 0.9113157987594604
t0.5_test_time 0.7541582119993109
gc 0
Test threshold 0.6 Acc 0.9102631578947369, AUC 0.9790151715278625, avg_entr 0.08519165962934494, f1 0.910263180732727
t0.6_test_time 0.5969553659997473
gc 0
Test threshold 0.7 Acc 0.91, AUC 0.9789837598800659, avg_entr 0.08521468192338943, f1 0.9100000262260437
t0.7_test_time 0.5847260780001307
gc 0
Test threshold 0.8 Acc 0.91, AUC 0.9789837598800659, avg_entr 0.08521468192338943, f1 0.9100000262260437
t0.8_test_time 0.5819219059994793
gc 0
Test threshold 0.9 Acc 0.91, AUC 0.9789837598800659, avg_entr 0.08521468192338943, f1 0.9100000262260437
t0.9_test_time 0.5808263310000257
