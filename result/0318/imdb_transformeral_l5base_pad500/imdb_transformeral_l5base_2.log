total count words 222751
vocab size 30000
train size 40000, valid size 5000, test size 5000
found 27937 words in glove
model: TransformerModelML(
  (layers): ModuleList(
    (0): EMBLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Embedding(30000, 300)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=2, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=2, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.weight 9000000
layers.0.ae.g.0.weight 256
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 256
layers.0.ae.h.0.bias 2
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
Total Trainable Params: 13671778
init_time 22.716380858999997
Start Training
gc 0
Train Epoch0 Acc 0.509825 (20393/40000), AUC 0.5164917707443237
ep0_train_time 182.31008006800002
Test Epoch0 layer4 Acc 0.7294, AUC 0.9135133028030396, avg_entr 0.6673220992088318, f1 0.7293999791145325
ep0_l4_test_time 5.711660145999986
Save ckpt to ckpt/imdb_transformeral_l5base_pad500//imdb_transformeral_l5.pt  ,ep 0
gc 0
Train Epoch1 Acc 0.859875 (34395/40000), AUC 0.9286117553710938
ep1_train_time 181.399131768
Test Epoch1 layer4 Acc 0.877, AUC 0.9546456336975098, avg_entr 0.12250947207212448, f1 0.8769999742507935
ep1_l4_test_time 5.821207018999985
Save ckpt to ckpt/imdb_transformeral_l5base_pad500//imdb_transformeral_l5.pt  ,ep 1
gc 0
Train Epoch2 Acc 0.9144 (36576/40000), AUC 0.9692882895469666
ep2_train_time 181.05909104300002
Test Epoch2 layer4 Acc 0.891, AUC 0.9594138860702515, avg_entr 0.05609285831451416, f1 0.890999972820282
ep2_l4_test_time 5.8202289170000086
Save ckpt to ckpt/imdb_transformeral_l5base_pad500//imdb_transformeral_l5.pt  ,ep 2
gc 0
Train Epoch3 Acc 0.9413 (37652/40000), AUC 0.9816566705703735
ep3_train_time 181.28516076799997
Test Epoch3 layer4 Acc 0.8928, AUC 0.9557642340660095, avg_entr 0.04168270528316498, f1 0.892799973487854
ep3_l4_test_time 5.770773011000074
Save ckpt to ckpt/imdb_transformeral_l5base_pad500//imdb_transformeral_l5.pt  ,ep 3
gc 0
Train Epoch4 Acc 0.95195 (38078/40000), AUC 0.9861248135566711
ep4_train_time 181.189754719
Test Epoch4 layer4 Acc 0.8844, AUC 0.954937219619751, avg_entr 0.03555287420749664, f1 0.8844000101089478
ep4_l4_test_time 5.880641042999969
gc 0
Train Epoch5 Acc 0.9578 (38312/40000), AUC 0.9884989261627197
ep5_train_time 181.49337744600007
Test Epoch5 layer4 Acc 0.8886, AUC 0.9536899328231812, avg_entr 0.032978788018226624, f1 0.8885999917984009
ep5_l4_test_time 5.7848828240000785
gc 0
Train Epoch6 Acc 0.963425 (38537/40000), AUC 0.9896750450134277
ep6_train_time 181.34114499899988
Test Epoch6 layer4 Acc 0.8848, AUC 0.9504445195198059, avg_entr 0.024137476459145546, f1 0.8848000168800354
ep6_l4_test_time 5.6489074099999925
gc 0
Train Epoch7 Acc 0.966925 (38677/40000), AUC 0.9910297989845276
ep7_train_time 163.68532452700015
Test Epoch7 layer4 Acc 0.882, AUC 0.9476727247238159, avg_entr 0.02443234622478485, f1 0.8820000290870667
ep7_l4_test_time 5.827886285999966
gc 0
Train Epoch8 Acc 0.969575 (38783/40000), AUC 0.9918115735054016
ep8_train_time 181.12427744299998
Test Epoch8 layer4 Acc 0.8744, AUC 0.9467878341674805, avg_entr 0.020705146715044975, f1 0.8744000792503357
ep8_l4_test_time 5.800828756000101
gc 0
Train Epoch9 Acc 0.9723 (38892/40000), AUC 0.9924488067626953
ep9_train_time 181.59186006000004
Test Epoch9 layer4 Acc 0.8772, AUC 0.9436996579170227, avg_entr 0.023277223110198975, f1 0.8772000074386597
ep9_l4_test_time 5.779083756000091
gc 0
Train Epoch10 Acc 0.974925 (38997/40000), AUC 0.9936747550964355
ep10_train_time 181.60561395600007
Test Epoch10 layer4 Acc 0.873, AUC 0.9430596828460693, avg_entr 0.01694723591208458, f1 0.8730000257492065
ep10_l4_test_time 5.829373955000392
gc 0
Train Epoch11 Acc 0.9766 (39064/40000), AUC 0.9942480325698853
ep11_train_time 181.33938053099973
Test Epoch11 layer4 Acc 0.868, AUC 0.9377824664115906, avg_entr 0.01581362634897232, f1 0.8679999709129333
ep11_l4_test_time 5.812108869999975
gc 0
Train Epoch12 Acc 0.9788 (39152/40000), AUC 0.9948163032531738
ep12_train_time 181.30188341800022
Test Epoch12 layer4 Acc 0.8654, AUC 0.9379098415374756, avg_entr 0.014177259989082813, f1 0.865399956703186
ep12_l4_test_time 5.847092234000229
gc 0
Train Epoch13 Acc 0.980775 (39231/40000), AUC 0.9954948425292969
ep13_train_time 181.5438568489999
Test Epoch13 layer4 Acc 0.859, AUC 0.9322649240493774, avg_entr 0.01372081134468317, f1 0.859000027179718
ep13_l4_test_time 5.736606308000319
gc 0
Train Epoch14 Acc 0.981875 (39275/40000), AUC 0.9958341121673584
ep14_train_time 181.27828244300008
Test Epoch14 layer4 Acc 0.867, AUC 0.9325774908065796, avg_entr 0.012804429978132248, f1 0.8669999837875366
ep14_l4_test_time 5.7272882970000865
gc 0
Train Epoch15 Acc 0.9829 (39316/40000), AUC 0.996180534362793
ep15_train_time 181.10366633700005
Test Epoch15 layer4 Acc 0.8676, AUC 0.9298809170722961, avg_entr 0.011365622282028198, f1 0.8675999641418457
ep15_l4_test_time 5.864877385
gc 0
Train Epoch16 Acc 0.984475 (39379/40000), AUC 0.9966546297073364
ep16_train_time 172.3528295340002
Test Epoch16 layer4 Acc 0.864, AUC 0.9220549464225769, avg_entr 0.010429215617477894, f1 0.8640000224113464
ep16_l4_test_time 3.071805786999903
gc 0
Train Epoch17 Acc 0.984725 (39389/40000), AUC 0.9969536066055298
ep17_train_time 131.42777733399998
Test Epoch17 layer4 Acc 0.8574, AUC 0.9220409989356995, avg_entr 0.009249920956790447, f1 0.8574000000953674
ep17_l4_test_time 4.45081015400001
gc 0
Train Epoch18 Acc 0.985825 (39433/40000), AUC 0.9974426627159119
ep18_train_time 136.5812042550001
Test Epoch18 layer4 Acc 0.8542, AUC 0.9138209819793701, avg_entr 0.01102102268487215, f1 0.854200005531311
ep18_l4_test_time 4.444580039999892
gc 0
Train Epoch19 Acc 0.986975 (39479/40000), AUC 0.9974818229675293
ep19_train_time 136.75481931100012
Test Epoch19 layer4 Acc 0.8542, AUC 0.8967462778091431, avg_entr 0.009759201668202877, f1 0.854200005531311
ep19_l4_test_time 4.324205462999998
gc 0
Train Epoch20 Acc 0.9878 (39512/40000), AUC 0.9975499510765076
ep20_train_time 105.21466265300023
Test Epoch20 layer4 Acc 0.8564, AUC 0.9107804298400879, avg_entr 0.008349447511136532, f1 0.8564000129699707
ep20_l4_test_time 3.05836043599993
gc 0
Train Epoch21 Acc 0.988125 (39525/40000), AUC 0.9978932738304138
ep21_train_time 91.80498254399981
Test Epoch21 layer4 Acc 0.8546, AUC 0.9047678709030151, avg_entr 0.012774579226970673, f1 0.8546000123023987
ep21_l4_test_time 3.0960231540002496
gc 0
Train Epoch22 Acc 0.98885 (39554/40000), AUC 0.998031497001648
ep22_train_time 92.28549640799974
Test Epoch22 layer4 Acc 0.8548, AUC 0.8985403776168823, avg_entr 0.008947782218456268, f1 0.8547999858856201
ep22_l4_test_time 3.2571770919998926
gc 0
Train Epoch23 Acc 0.989475 (39579/40000), AUC 0.9984182715415955
ep23_train_time 126.59818395000002
Test Epoch23 layer4 Acc 0.8494, AUC 0.8972537517547607, avg_entr 0.010666836984455585, f1 0.849399983882904
ep23_l4_test_time 3.0585267910000766
gc 0
Train Epoch24 Acc 0.990125 (39605/40000), AUC 0.9983386993408203
ep24_train_time 93.60329558400008
Test Epoch24 layer4 Acc 0.851, AUC 0.8919451832771301, avg_entr 0.0075616599060595036, f1 0.8510000109672546
ep24_l4_test_time 3.071659481000097
gc 0
Train Epoch25 Acc 0.9905 (39620/40000), AUC 0.9982643127441406
ep25_train_time 94.11888131900014
Test Epoch25 layer4 Acc 0.8466, AUC 0.885631799697876, avg_entr 0.008483365178108215, f1 0.8465999960899353
ep25_l4_test_time 3.071382735000043
gc 0
Train Epoch26 Acc 0.991175 (39647/40000), AUC 0.9987101554870605
ep26_train_time 94.03474774899951
Test Epoch26 layer4 Acc 0.8486, AUC 0.8702091574668884, avg_entr 0.006293072365224361, f1 0.8485999703407288
ep26_l4_test_time 3.1117237820008086
gc 0
Train Epoch27 Acc 0.991525 (39661/40000), AUC 0.9988287687301636
ep27_train_time 93.62888670800021
Test Epoch27 layer4 Acc 0.844, AUC 0.8683693408966064, avg_entr 0.00566498190164566, f1 0.8439999222755432
ep27_l4_test_time 3.3616203640003732
gc 0
Train Epoch28 Acc 0.99215 (39686/40000), AUC 0.9985875487327576
ep28_train_time 93.42545205499937
Test Epoch28 layer4 Acc 0.8406, AUC 0.8760721683502197, avg_entr 0.00809115543961525, f1 0.8405999541282654
ep28_l4_test_time 3.109526860999722
gc 0
Train Epoch29 Acc 0.99195 (39678/40000), AUC 0.9988927841186523
ep29_train_time 93.5371384730006
Test Epoch29 layer4 Acc 0.8402, AUC 0.8767196536064148, avg_entr 0.00831359252333641, f1 0.8402000069618225
ep29_l4_test_time 3.0834948510000686
Best AUC tensor(0.8928) 3
train_loss (2, 5, 30)
valid_acc (1, 30)
valid_AUC (1, 30)
train_acc (30,)
total_train+valid_time 4581.799261127
Start Testing
Load ckpt at ckpt/imdb_transformeral_l5base_pad500//imdb_transformeral_l5.pt
gc 9
Test layer0 Acc 0.8922, AUC 0.955601692199707, avg_entr 0.20002245903015137, f1 0.8921999931335449
l0_test_time 0.5653380329995343
gc 0
Test layer1 Acc 0.8924, AUC 0.9541728496551514, avg_entr 0.08590196818113327, f1 0.8923999667167664
l1_test_time 1.2700472790002095
gc 0
Test layer2 Acc 0.892, AUC 0.9532176852226257, avg_entr 0.05094284191727638, f1 0.8920000195503235
l2_test_time 1.9987011650000568
gc 0
Test layer3 Acc 0.8918, AUC 0.95368492603302, avg_entr 0.04577159136533737, f1 0.8917999863624573
l3_test_time 2.493501623999691
gc 0
Test layer4 Acc 0.8912, AUC 0.9541226625442505, avg_entr 0.044728219509124756, f1 0.8912000060081482
l4_test_time 3.0804520229994523
gc 0
Test threshold 0.1 Acc 0.8916, AUC 0.9445155262947083, avg_entr 0.07785549014806747, f1 0.8916000127792358
t0.1_test_time 1.2490304110006036
gc 0
Test threshold 0.2 Acc 0.8922, AUC 0.941943883895874, avg_entr 0.09499809890985489, f1 0.8921999931335449
t0.2_test_time 1.1165095470005326
gc 0
Test threshold 0.3 Acc 0.8928, AUC 0.9427656531333923, avg_entr 0.11320355534553528, f1 0.892799973487854
t0.3_test_time 1.0257698879995587
gc 0
Test threshold 0.4 Acc 0.8928, AUC 0.9435445070266724, avg_entr 0.13336391746997833, f1 0.892799973487854
t0.4_test_time 0.9510216590006166
gc 0
Test threshold 0.5 Acc 0.8934, AUC 0.9424736499786377, avg_entr 0.14828813076019287, f1 0.8934000134468079
t0.5_test_time 0.8974112269997931
gc 0
Test threshold 0.6 Acc 0.8942, AUC 0.9422659873962402, avg_entr 0.16651853919029236, f1 0.8942000269889832
t0.6_test_time 0.8599728290000712
gc 0
Test threshold 0.7 Acc 0.8954, AUC 0.9442057609558105, avg_entr 0.18482725322246552, f1 0.8953999876976013
t0.7_test_time 0.8182852110003296
gc 0
Test threshold 0.8 Acc 0.8954, AUC 0.9445958137512207, avg_entr 0.20555171370506287, f1 0.8953999876976013
t0.8_test_time 0.7618457220005439
gc 0
Test threshold 0.9 Acc 0.8986, AUC 0.947928786277771, avg_entr 0.23110322654247284, f1 0.8985999822616577
t0.9_test_time 0.6976441919996432
