total count words 222751
vocab size 30000
train size 40000, valid size 5000, test size 5000
found 27937 words in glove
model: TransformerModelML(
  (layers): ModuleList(
    (0): EMBLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Embedding(30000, 300)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=2, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=2, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.weight 9000000
layers.0.ae.g.0.weight 256
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 256
layers.0.ae.h.0.bias 2
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
Total Trainable Params: 13671778
init_time 22.429671089
Start Training
gc 0
Train Epoch0 Acc 0.505175 (20207/40000), AUC 0.49590030312538147
ep0_train_time 148.207240695
Test Epoch0 layer4 Acc 0.6736, AUC 0.8815505504608154, avg_entr 0.6793934106826782, f1 0.6736000180244446
ep0_l4_test_time 3.097784909000012
Save ckpt to ckpt/imdb_transformeral_l5base_pad500//imdb_transformeral_l5.pt  ,ep 0
gc 0
Train Epoch1 Acc 0.852075 (34083/40000), AUC 0.9241013526916504
ep1_train_time 127.04765244199999
Test Epoch1 layer4 Acc 0.8352, AUC 0.9534518122673035, avg_entr 0.1562718152999878, f1 0.8352000117301941
ep1_l4_test_time 3.058699255000022
Save ckpt to ckpt/imdb_transformeral_l5base_pad500//imdb_transformeral_l5.pt  ,ep 1
gc 0
Train Epoch2 Acc 0.9065 (36260/40000), AUC 0.9651625752449036
ep2_train_time 91.72847849300001
Test Epoch2 layer4 Acc 0.898, AUC 0.9589545726776123, avg_entr 0.0641016736626625, f1 0.8980000019073486
ep2_l4_test_time 3.057393122999997
Save ckpt to ckpt/imdb_transformeral_l5base_pad500//imdb_transformeral_l5.pt  ,ep 2
gc 0
Train Epoch3 Acc 0.93905 (37562/40000), AUC 0.9803330898284912
ep3_train_time 91.78680853899999
Test Epoch3 layer4 Acc 0.8914, AUC 0.955545961856842, avg_entr 0.04054989665746689, f1 0.8913999795913696
ep3_l4_test_time 3.0587846359999844
gc 0
Train Epoch4 Acc 0.950575 (38023/40000), AUC 0.9844021797180176
ep4_train_time 91.79764102400003
Test Epoch4 layer4 Acc 0.8922, AUC 0.9535602331161499, avg_entr 0.04081655293703079, f1 0.8921999931335449
ep4_l4_test_time 3.06564532699997
gc 0
Train Epoch5 Acc 0.956425 (38257/40000), AUC 0.9866325855255127
ep5_train_time 91.81650814700004
Test Epoch5 layer4 Acc 0.881, AUC 0.9510735273361206, avg_entr 0.034057725220918655, f1 0.8809999823570251
ep5_l4_test_time 3.0604505170000493
gc 0
Train Epoch6 Acc 0.96195 (38478/40000), AUC 0.9878277778625488
ep6_train_time 91.75269260899995
Test Epoch6 layer4 Acc 0.8734, AUC 0.9479165077209473, avg_entr 0.029507802799344063, f1 0.8733999729156494
ep6_l4_test_time 3.056834815000002
gc 0
Train Epoch7 Acc 0.96585 (38634/40000), AUC 0.9915211200714111
ep7_train_time 91.82876839599999
Test Epoch7 layer4 Acc 0.8848, AUC 0.9459433555603027, avg_entr 0.0289020836353302, f1 0.8848000168800354
ep7_l4_test_time 3.0638925250000284
gc 0
Train Epoch8 Acc 0.968325 (38733/40000), AUC 0.991965115070343
ep8_train_time 92.34265841799993
Test Epoch8 layer4 Acc 0.8796, AUC 0.9438784122467041, avg_entr 0.02414826676249504, f1 0.8795999884605408
ep8_l4_test_time 3.1076844870000286
gc 0
Train Epoch9 Acc 0.9714 (38856/40000), AUC 0.9923369884490967
ep9_train_time 91.80080067199992
Test Epoch9 layer4 Acc 0.876, AUC 0.9428738355636597, avg_entr 0.023702936246991158, f1 0.8760000467300415
ep9_l4_test_time 3.06038252999997
gc 0
Train Epoch10 Acc 0.973925 (38957/40000), AUC 0.9927778244018555
ep10_train_time 91.79931376899981
Test Epoch10 layer4 Acc 0.8728, AUC 0.9393861293792725, avg_entr 0.01784960739314556, f1 0.8727999925613403
ep10_l4_test_time 3.057397454000011
gc 0
Train Epoch11 Acc 0.976 (39040/40000), AUC 0.9944354891777039
ep11_train_time 91.75147294099997
Test Epoch11 layer4 Acc 0.872, AUC 0.9374845027923584, avg_entr 0.015760062262415886, f1 0.871999979019165
ep11_l4_test_time 3.0605496769999263
gc 0
Train Epoch12 Acc 0.9783 (39132/40000), AUC 0.9949408769607544
ep12_train_time 91.87355981799988
Test Epoch12 layer4 Acc 0.8728, AUC 0.9344261884689331, avg_entr 0.016410693526268005, f1 0.8727999925613403
ep12_l4_test_time 3.0614662069999667
gc 0
Train Epoch13 Acc 0.98025 (39210/40000), AUC 0.9950540065765381
ep13_train_time 91.90946595600008
Test Epoch13 layer4 Acc 0.867, AUC 0.933228611946106, avg_entr 0.01966560259461403, f1 0.8669999837875366
ep13_l4_test_time 3.0647690180001064
gc 0
Train Epoch14 Acc 0.981125 (39245/40000), AUC 0.995314359664917
ep14_train_time 127.24443515899998
Test Epoch14 layer4 Acc 0.8662, AUC 0.928038477897644, avg_entr 0.014282790012657642, f1 0.8661999702453613
ep14_l4_test_time 3.0882244809999975
gc 0
Train Epoch15 Acc 0.98215 (39286/40000), AUC 0.9961221814155579
ep15_train_time 148.481730901
Test Epoch15 layer4 Acc 0.8612, AUC 0.9257503747940063, avg_entr 0.015961427241563797, f1 0.8612000346183777
ep15_l4_test_time 6.058138124999914
gc 0
Train Epoch16 Acc 0.9831 (39324/40000), AUC 0.9962766766548157
ep16_train_time 180.8673724570001
Test Epoch16 layer4 Acc 0.853, AUC 0.9018224477767944, avg_entr 0.012195703573524952, f1 0.8529999852180481
ep16_l4_test_time 6.056860571000016
gc 0
Train Epoch17 Acc 0.9847 (39388/40000), AUC 0.9964649081230164
ep17_train_time 169.76689528200018
Test Epoch17 layer4 Acc 0.86, AUC 0.912642240524292, avg_entr 0.010808666236698627, f1 0.8600000143051147
ep17_l4_test_time 6.016578185000071
gc 0
Train Epoch18 Acc 0.9855 (39420/40000), AUC 0.9969149827957153
ep18_train_time 130.80361012699996
Test Epoch18 layer4 Acc 0.8588, AUC 0.9049807786941528, avg_entr 0.009891612455248833, f1 0.8587999939918518
ep18_l4_test_time 3.058092348000173
gc 0
Train Epoch19 Acc 0.98665 (39466/40000), AUC 0.9974446296691895
ep19_train_time 171.08743971100012
Test Epoch19 layer4 Acc 0.859, AUC 0.9029241800308228, avg_entr 0.011333033442497253, f1 0.859000027179718
ep19_l4_test_time 6.149375288999636
gc 0
Train Epoch20 Acc 0.9871 (39484/40000), AUC 0.9977091550827026
ep20_train_time 181.07563044400013
Test Epoch20 layer4 Acc 0.8536, AUC 0.8990819454193115, avg_entr 0.011341926641762257, f1 0.853600025177002
ep20_l4_test_time 6.049422637999669
gc 0
Train Epoch21 Acc 0.988 (39520/40000), AUC 0.9976915121078491
ep21_train_time 179.67372426199972
Test Epoch21 layer4 Acc 0.8548, AUC 0.88545161485672, avg_entr 0.007315895054489374, f1 0.8547999858856201
ep21_l4_test_time 5.1227002350001385
gc 0
Train Epoch22 Acc 0.9885 (39540/40000), AUC 0.9979355335235596
ep22_train_time 91.98741093499984
Test Epoch22 layer4 Acc 0.8532, AUC 0.8927958607673645, avg_entr 0.008826406672596931, f1 0.8532000184059143
ep22_l4_test_time 3.0642755100002432
gc 0
Train Epoch23 Acc 0.98895 (39558/40000), AUC 0.9978672862052917
ep23_train_time 91.84630486500009
Test Epoch23 layer4 Acc 0.849, AUC 0.8720930218696594, avg_entr 0.007292324677109718, f1 0.8489999771118164
ep23_l4_test_time 3.0591496950000874
gc 0
Train Epoch24 Acc 0.989675 (39587/40000), AUC 0.998090386390686
ep24_train_time 91.81518952299984
Test Epoch24 layer4 Acc 0.8472, AUC 0.8759101629257202, avg_entr 0.007866397500038147, f1 0.8471999764442444
ep24_l4_test_time 3.061383835000015
gc 0
Train Epoch25 Acc 0.99 (39600/40000), AUC 0.9981765747070312
ep25_train_time 91.84199717699994
Test Epoch25 layer4 Acc 0.846, AUC 0.8582116365432739, avg_entr 0.006070529110729694, f1 0.8460000157356262
ep25_l4_test_time 3.061554836000141
gc 0
Train Epoch26 Acc 0.990525 (39621/40000), AUC 0.9983013868331909
ep26_train_time 122.49913155900003
Test Epoch26 layer4 Acc 0.8498, AUC 0.8720098733901978, avg_entr 0.005597757641226053, f1 0.8497999906539917
ep26_l4_test_time 3.1417042009998113
gc 0
Train Epoch27 Acc 0.991275 (39651/40000), AUC 0.9984360337257385
ep27_train_time 167.25408807500025
Test Epoch27 layer4 Acc 0.8486, AUC 0.8702694177627563, avg_entr 0.007340262643992901, f1 0.8485999703407288
ep27_l4_test_time 3.1829942490003305
gc 0
Train Epoch28 Acc 0.991425 (39657/40000), AUC 0.9985803961753845
ep28_train_time 168.092968508
Test Epoch28 layer4 Acc 0.8416, AUC 0.8657755851745605, avg_entr 0.008640744723379612, f1 0.8416000008583069
ep28_l4_test_time 5.192698264000228
gc 0
Train Epoch29 Acc 0.9923 (39692/40000), AUC 0.998590350151062
ep29_train_time 153.37425760799988
Test Epoch29 layer4 Acc 0.8466, AUC 0.8591022491455078, avg_entr 0.006315167993307114, f1 0.8465999960899353
ep29_l4_test_time 5.392487167000127
Best AUC tensor(0.8980) 2
train_loss (2, 5, 30)
valid_acc (1, 30)
valid_AUC (1, 30)
train_acc (30,)
total_train+valid_time 3760.286178879
Start Testing
Load ckpt at ckpt/imdb_transformeral_l5base_pad500//imdb_transformeral_l5.pt
gc 9
Test layer0 Acc 0.893, AUC 0.95366370677948, avg_entr 0.2287653535604477, f1 0.8930000066757202
l0_test_time 0.7143973330003064
gc 0
Test layer1 Acc 0.892, AUC 0.9579589366912842, avg_entr 0.15099669992923737, f1 0.8920000195503235
l1_test_time 1.8658247299999857
gc 0
Test layer2 Acc 0.8936, AUC 0.9579876661300659, avg_entr 0.09064941108226776, f1 0.8935999870300293
l2_test_time 2.9417655430002014
gc 0
Test layer3 Acc 0.8926, AUC 0.9586062431335449, avg_entr 0.07830014824867249, f1 0.8925999999046326
l3_test_time 4.104348476999803
gc 0
Test layer4 Acc 0.894, AUC 0.9582371115684509, avg_entr 0.06857182085514069, f1 0.8939999938011169
l4_test_time 5.307704363000084
gc 0
Test threshold 0.1 Acc 0.894, AUC 0.9529582262039185, avg_entr 0.11465612798929214, f1 0.8939999938011169
t0.1_test_time 3.230759118000151
gc 0
Test threshold 0.2 Acc 0.894, AUC 0.9510365128517151, avg_entr 0.13662618398666382, f1 0.8939999938011169
t0.2_test_time 2.8958224530001644
gc 0
Test threshold 0.3 Acc 0.8944, AUC 0.9504403471946716, avg_entr 0.15900850296020508, f1 0.8944000005722046
t0.3_test_time 2.602324275999763
gc 0
Test threshold 0.4 Acc 0.8946, AUC 0.94959557056427, avg_entr 0.1768413633108139, f1 0.894599974155426
t0.4_test_time 2.4428461689999494
gc 0
Test threshold 0.5 Acc 0.895, AUC 0.9487520456314087, avg_entr 0.19776663184165955, f1 0.8949999809265137
t0.5_test_time 2.198331070000222
gc 0
Test threshold 0.6 Acc 0.8942, AUC 0.9497033953666687, avg_entr 0.21657173335552216, f1 0.8942000269889832
t0.6_test_time 2.1323471470000186
gc 0
Test threshold 0.7 Acc 0.8948, AUC 0.9489783048629761, avg_entr 0.23919031023979187, f1 0.8948000073432922
t0.7_test_time 2.0551620460000777
gc 0
Test threshold 0.8 Acc 0.8964, AUC 0.9489908814430237, avg_entr 0.26372572779655457, f1 0.896399974822998
t0.8_test_time 1.7175432539997928
gc 0
Test threshold 0.9 Acc 0.8974, AUC 0.9507594108581543, avg_entr 0.2880762815475464, f1 0.8974000215530396
t0.9_test_time 1.1076183729996956
