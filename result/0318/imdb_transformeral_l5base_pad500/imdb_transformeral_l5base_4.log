total count words 222751
vocab size 30000
train size 40000, valid size 5000, test size 5000
found 27937 words in glove
model: TransformerModelML(
  (layers): ModuleList(
    (0): EMBLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Embedding(30000, 300)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=2, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=2, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.weight 9000000
layers.0.ae.g.0.weight 256
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 256
layers.0.ae.h.0.bias 2
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
Total Trainable Params: 13671778
init_time 21.579174648
Start Training
gc 0
Train Epoch0 Acc 0.525425 (21017/40000), AUC 0.5315988659858704
ep0_train_time 92.010016492
Test Epoch0 layer4 Acc 0.6902, AUC 0.9011518955230713, avg_entr 0.6770188212394714, f1 0.6901999711990356
ep0_l4_test_time 3.055392832999999
Save ckpt to ckpt/imdb_transformeral_l5base_pad500//imdb_transformeral_l5.pt  ,ep 0
gc 0
Train Epoch1 Acc 0.8374 (33496/40000), AUC 0.9143698811531067
ep1_train_time 91.944737794
Test Epoch1 layer4 Acc 0.8896, AUC 0.9537473917007446, avg_entr 0.1427927315235138, f1 0.8895999193191528
ep1_l4_test_time 3.057032663000001
Save ckpt to ckpt/imdb_transformeral_l5base_pad500//imdb_transformeral_l5.pt  ,ep 1
gc 0
Train Epoch2 Acc 0.91 (36400/40000), AUC 0.9656142592430115
ep2_train_time 91.91299961799999
Test Epoch2 layer4 Acc 0.8974, AUC 0.9577010273933411, avg_entr 0.06197696551680565, f1 0.8974000215530396
ep2_l4_test_time 3.067775783000002
Save ckpt to ckpt/imdb_transformeral_l5base_pad500//imdb_transformeral_l5.pt  ,ep 2
gc 0
Train Epoch3 Acc 0.93755 (37502/40000), AUC 0.9790315628051758
ep3_train_time 91.83190040200003
Test Epoch3 layer4 Acc 0.8916, AUC 0.9551279544830322, avg_entr 0.04486639052629471, f1 0.8916000127792358
ep3_l4_test_time 3.0557866180000133
gc 0
Train Epoch4 Acc 0.947375 (37895/40000), AUC 0.9832075834274292
ep4_train_time 91.84685579399996
Test Epoch4 layer4 Acc 0.89, AUC 0.9521484971046448, avg_entr 0.03885611519217491, f1 0.8899999856948853
ep4_l4_test_time 3.057786036999971
gc 0
Train Epoch5 Acc 0.95665 (38266/40000), AUC 0.9883131980895996
ep5_train_time 91.75211705400005
Test Epoch5 layer4 Acc 0.8882, AUC 0.9512296915054321, avg_entr 0.036173272877931595, f1 0.888200044631958
ep5_l4_test_time 3.057372225999984
gc 0
Train Epoch6 Acc 0.962125 (38485/40000), AUC 0.9892551898956299
ep6_train_time 91.75611794999998
Test Epoch6 layer4 Acc 0.8874, AUC 0.9509139060974121, avg_entr 0.03284035250544548, f1 0.8873999714851379
ep6_l4_test_time 3.0573468420000154
gc 0
Train Epoch7 Acc 0.9663 (38652/40000), AUC 0.9913339614868164
ep7_train_time 91.72845163900001
Test Epoch7 layer4 Acc 0.8814, AUC 0.9474793672561646, avg_entr 0.027302630245685577, f1 0.8813999891281128
ep7_l4_test_time 3.071500699000012
gc 0
Train Epoch8 Acc 0.969 (38760/40000), AUC 0.9919075965881348
ep8_train_time 91.77437042700001
Test Epoch8 layer4 Acc 0.881, AUC 0.946545422077179, avg_entr 0.02609071135520935, f1 0.8809999823570251
ep8_l4_test_time 3.056997291000016
gc 0
Train Epoch9 Acc 0.97195 (38878/40000), AUC 0.9930583238601685
ep9_train_time 91.78017843500004
Test Epoch9 layer4 Acc 0.8748, AUC 0.9445019364356995, avg_entr 0.023646259680390358, f1 0.8748000264167786
ep9_l4_test_time 3.068006504999971
gc 0
Train Epoch10 Acc 0.97435 (38974/40000), AUC 0.9937251806259155
ep10_train_time 91.74347311399993
Test Epoch10 layer4 Acc 0.8724, AUC 0.9442850351333618, avg_entr 0.020437201485037804, f1 0.8723999857902527
ep10_l4_test_time 3.0579537419998815
gc 0
Train Epoch11 Acc 0.9768 (39072/40000), AUC 0.9942222237586975
ep11_train_time 91.755734308
Test Epoch11 layer4 Acc 0.873, AUC 0.9417895078659058, avg_entr 0.016225647181272507, f1 0.8730000257492065
ep11_l4_test_time 3.0558303919999616
gc 0
Train Epoch12 Acc 0.9783 (39132/40000), AUC 0.9948287010192871
ep12_train_time 91.75204796999992
Test Epoch12 layer4 Acc 0.8692, AUC 0.9380613565444946, avg_entr 0.01620706170797348, f1 0.8691999912261963
ep12_l4_test_time 3.066734007999912
gc 0
Train Epoch13 Acc 0.980575 (39223/40000), AUC 0.9951436519622803
ep13_train_time 91.78665282799989
Test Epoch13 layer4 Acc 0.8688, AUC 0.9361507892608643, avg_entr 0.013100935146212578, f1 0.8687999844551086
ep13_l4_test_time 3.055527032999862
gc 0
Train Epoch14 Acc 0.9815 (39260/40000), AUC 0.995275616645813
ep14_train_time 91.75762093599997
Test Epoch14 layer4 Acc 0.864, AUC 0.9350037574768066, avg_entr 0.01538792159408331, f1 0.8640000224113464
ep14_l4_test_time 3.0579377789999853
gc 0
Train Epoch15 Acc 0.982675 (39307/40000), AUC 0.9961634278297424
ep15_train_time 91.91243655500011
Test Epoch15 layer4 Acc 0.866, AUC 0.931777834892273, avg_entr 0.01128158625215292, f1 0.8659999966621399
ep15_l4_test_time 3.05672238000011
gc 0
Train Epoch16 Acc 0.98385 (39354/40000), AUC 0.9963676333427429
ep16_train_time 91.8273233609998
Test Epoch16 layer4 Acc 0.8644, AUC 0.9289220571517944, avg_entr 0.01518124807626009, f1 0.8644000291824341
ep16_l4_test_time 3.059853116999875
gc 0
Train Epoch17 Acc 0.985225 (39409/40000), AUC 0.9970955848693848
ep17_train_time 91.92540578900002
Test Epoch17 layer4 Acc 0.8624, AUC 0.9245690703392029, avg_entr 0.0129945557564497, f1 0.8623999953269958
ep17_l4_test_time 3.056777395999916
gc 0
Train Epoch18 Acc 0.986075 (39443/40000), AUC 0.9973509311676025
ep18_train_time 91.83697098599987
Test Epoch18 layer4 Acc 0.8612, AUC 0.9196034669876099, avg_entr 0.009491374716162682, f1 0.8612000346183777
ep18_l4_test_time 3.0597423480001
gc 0
Train Epoch19 Acc 0.986775 (39471/40000), AUC 0.9974914789199829
ep19_train_time 91.77836574699995
Test Epoch19 layer4 Acc 0.8594, AUC 0.9120478630065918, avg_entr 0.008212577551603317, f1 0.8593999743461609
ep19_l4_test_time 3.0558418699999947
gc 0
Train Epoch20 Acc 0.986675 (39467/40000), AUC 0.9973605871200562
ep20_train_time 91.94617838099998
Test Epoch20 layer4 Acc 0.8564, AUC 0.9111552238464355, avg_entr 0.00882146880030632, f1 0.8564000129699707
ep20_l4_test_time 3.0578366390000156
gc 0
Train Epoch21 Acc 0.987925 (39517/40000), AUC 0.9978345036506653
ep21_train_time 91.74281994499984
Test Epoch21 layer4 Acc 0.8548, AUC 0.9059363603591919, avg_entr 0.0054573314264416695, f1 0.8547999858856201
ep21_l4_test_time 3.0566100290002396
gc 0
Train Epoch22 Acc 0.988825 (39553/40000), AUC 0.9979159832000732
ep22_train_time 91.77718494500004
Test Epoch22 layer4 Acc 0.8572, AUC 0.903735876083374, avg_entr 0.0062282513827085495, f1 0.857200026512146
ep22_l4_test_time 3.056817619999947
gc 0
Train Epoch23 Acc 0.988975 (39559/40000), AUC 0.9981330633163452
ep23_train_time 91.78118288399992
Test Epoch23 layer4 Acc 0.8586, AUC 0.9071112871170044, avg_entr 0.00945680495351553, f1 0.8586000204086304
ep23_l4_test_time 3.065114685000026
gc 0
Train Epoch24 Acc 0.989825 (39593/40000), AUC 0.9985092878341675
ep24_train_time 91.84226729600005
Test Epoch24 layer4 Acc 0.849, AUC 0.9047831296920776, avg_entr 0.009992819279432297, f1 0.8489999771118164
ep24_l4_test_time 3.0634194709996336
gc 0
Train Epoch25 Acc 0.990225 (39609/40000), AUC 0.9985867738723755
ep25_train_time 91.7555417399999
Test Epoch25 layer4 Acc 0.8534, AUC 0.9065254926681519, avg_entr 0.009921970777213573, f1 0.8533999919891357
ep25_l4_test_time 3.0594722740002
gc 0
Train Epoch26 Acc 0.991025 (39641/40000), AUC 0.9985668659210205
ep26_train_time 91.79286452400038
Test Epoch26 layer4 Acc 0.8478, AUC 0.8866710662841797, avg_entr 0.008069006726145744, f1 0.8477999567985535
ep26_l4_test_time 3.057724140999653
gc 0
Train Epoch27 Acc 0.991675 (39667/40000), AUC 0.9985592365264893
ep27_train_time 91.78675709799973
Test Epoch27 layer4 Acc 0.8468, AUC 0.8863042593002319, avg_entr 0.006434943526983261, f1 0.8468000292778015
ep27_l4_test_time 3.0579061429998546
gc 0
Train Epoch28 Acc 0.99195 (39678/40000), AUC 0.9987243413925171
ep28_train_time 92.01388404900035
Test Epoch28 layer4 Acc 0.8474, AUC 0.8830450773239136, avg_entr 0.004750079475343227, f1 0.8474000096321106
ep28_l4_test_time 3.059309007000138
gc 0
Train Epoch29 Acc 0.9919 (39676/40000), AUC 0.9987978935241699
ep29_train_time 91.88070512000013
Test Epoch29 layer4 Acc 0.8474, AUC 0.8979877233505249, avg_entr 0.008525729179382324, f1 0.8474000096321106
ep29_l4_test_time 3.0559581779998553
Best AUC tensor(0.8974) 2
train_loss (2, 5, 30)
valid_acc (1, 30)
valid_AUC (1, 30)
train_acc (30,)
total_train+valid_time 2847.942908019
Start Testing
Load ckpt at ckpt/imdb_transformeral_l5base_pad500//imdb_transformeral_l5.pt
gc 9
Test layer0 Acc 0.8896, AUC 0.9531435966491699, avg_entr 0.2291608601808548, f1 0.8895999193191528
l0_test_time 0.5431204619999335
gc 0
Test layer1 Acc 0.8918, AUC 0.9574533104896545, avg_entr 0.16218724846839905, f1 0.8917999863624573
l1_test_time 1.17867941600025
gc 0
Test layer2 Acc 0.8984, AUC 0.956453800201416, avg_entr 0.08666189014911652, f1 0.8984000086784363
l2_test_time 1.8076873220002199
gc 0
Test layer3 Acc 0.8972, AUC 0.956977903842926, avg_entr 0.06282781064510345, f1 0.8971999883651733
l3_test_time 2.4323722179997276
gc 0
Test layer4 Acc 0.8966, AUC 0.9569412469863892, avg_entr 0.061414685100317, f1 0.8966000080108643
l4_test_time 3.06195648899984
gc 0
Test threshold 0.1 Acc 0.8966, AUC 0.9495564103126526, avg_entr 0.10391328483819962, f1 0.8966000080108643
t0.1_test_time 1.5055123949996414
gc 0
Test threshold 0.2 Acc 0.8966, AUC 0.9461889863014221, avg_entr 0.1272660195827484, f1 0.8966000080108643
t0.2_test_time 1.3077855199999249
gc 0
Test threshold 0.3 Acc 0.8968, AUC 0.9453950524330139, avg_entr 0.15095557272434235, f1 0.8967999815940857
t0.3_test_time 1.1873197149998305
gc 0
Test threshold 0.4 Acc 0.8968, AUC 0.9461901187896729, avg_entr 0.17518475651741028, f1 0.8967999815940857
t0.4_test_time 1.0950920229997791
gc 0
Test threshold 0.5 Acc 0.8968, AUC 0.9449471831321716, avg_entr 0.19819480180740356, f1 0.8967999815940857
t0.5_test_time 1.011371202999726
gc 0
Test threshold 0.6 Acc 0.8962, AUC 0.9443828463554382, avg_entr 0.22067475318908691, f1 0.8962000012397766
t0.6_test_time 0.9404184330001044
gc 0
Test threshold 0.7 Acc 0.897, AUC 0.9455424547195435, avg_entr 0.24625983834266663, f1 0.8970000147819519
t0.7_test_time 0.8729007209999509
gc 0
Test threshold 0.8 Acc 0.8972, AUC 0.9469972252845764, avg_entr 0.27105712890625, f1 0.8971999883651733
t0.8_test_time 0.8001678350001384
gc 0
Test threshold 0.9 Acc 0.8974, AUC 0.9504686594009399, avg_entr 0.29809486865997314, f1 0.8974000215530396
t0.9_test_time 0.7102938789998916
