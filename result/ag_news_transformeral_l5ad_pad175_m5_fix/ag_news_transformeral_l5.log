total count words 102019
vocab size 30000
found 26754 words in glove
Load ckpt from ckpt/ag_news_transformeral_l5_pad175_m4_fix//ag_news_transformeral_l5_prefix.pt
model: TransformerModelML(
  (layers): ModuleList(
    (0): EMBLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Embedding(30000, 300)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=4, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=4, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.weight 9000000
layers.0.ae.g.0.weight 512
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 512
layers.0.ae.h.0.bias 4
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
Total Trainable Params: 13672292
Start Training
train_mask {0, 1, 2, 3, 4}
gc 9
Train Epoch0 Acc 0.9145333333333333 (109744/120000), AUC 0.9844561815261841
ep0_train_time 80.39871025085449
Test Epoch0 threshold 0.1 Acc 0.9188157894736843, AUC 0.9790394306182861, avg_entr 0.00761523237451911
ep0_t0.1_test_time 0.4311225414276123
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m5_fix//ag_news_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.2 Acc 0.9188157894736843, AUC 0.9809990525245667, avg_entr 0.013021822087466717
ep0_t0.2_test_time 0.39199209213256836
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m5_fix//ag_news_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.3 Acc 0.9193421052631578, AUC 0.9819346070289612, avg_entr 0.022755496203899384
ep0_t0.3_test_time 0.34529852867126465
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m5_fix//ag_news_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.4 Acc 0.9188157894736843, AUC 0.9820528030395508, avg_entr 0.023977402597665787
ep0_t0.4_test_time 0.32743334770202637
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m5_fix//ag_news_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.5 Acc 0.9186842105263158, AUC 0.9821063280105591, avg_entr 0.025237644091248512
ep0_t0.5_test_time 0.31821656227111816
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m5_fix//ag_news_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.6 Acc 0.9186842105263158, AUC 0.9821076989173889, avg_entr 0.025484029203653336
ep0_t0.6_test_time 0.3174750804901123
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m5_fix//ag_news_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.7 Acc 0.9186842105263158, AUC 0.9821076989173889, avg_entr 0.025484029203653336
ep0_t0.7_test_time 0.31648707389831543
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m5_fix//ag_news_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.8 Acc 0.9186842105263158, AUC 0.9821076989173889, avg_entr 0.025484029203653336
ep0_t0.8_test_time 0.31652140617370605
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m5_fix//ag_news_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.9 Acc 0.9186842105263158, AUC 0.9821076989173889, avg_entr 0.025484029203653336
ep0_t0.9_test_time 0.3155553340911865
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m5_fix//ag_news_transformeral_l5_prefix.pt  ,ep 0
gc 0
Train Epoch1 Acc 0.9608333333333333 (115300/120000), AUC 0.9937900304794312
ep1_train_time 80.09284210205078
Test Epoch1 threshold 0.1 Acc 0.9173684210526316, AUC 0.9791030883789062, avg_entr 0.007629326079040766
ep1_t0.1_test_time 0.43503236770629883
Test Epoch1 threshold 0.2 Acc 0.9178947368421052, AUC 0.980812668800354, avg_entr 0.012919043190777302
ep1_t0.2_test_time 0.39356327056884766
Test Epoch1 threshold 0.3 Acc 0.9176315789473685, AUC 0.9819446802139282, avg_entr 0.022327449172735214
ep1_t0.3_test_time 0.34399986267089844
Test Epoch1 threshold 0.4 Acc 0.9176315789473685, AUC 0.9820704460144043, avg_entr 0.023954909294843674
ep1_t0.4_test_time 0.32372498512268066
Test Epoch1 threshold 0.5 Acc 0.9175, AUC 0.9821836948394775, avg_entr 0.02473374456167221
ep1_t0.5_test_time 0.318253755569458
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m5_fix//ag_news_transformeral_l5_prefix.pt  ,ep 1
Test Epoch1 threshold 0.6 Acc 0.9173684210526316, AUC 0.9821805953979492, avg_entr 0.025016089901328087
ep1_t0.6_test_time 0.3174123764038086
Test Epoch1 threshold 0.7 Acc 0.9173684210526316, AUC 0.9821805953979492, avg_entr 0.025016089901328087
ep1_t0.7_test_time 0.3140699863433838
Test Epoch1 threshold 0.8 Acc 0.9173684210526316, AUC 0.9821805953979492, avg_entr 0.025016089901328087
ep1_t0.8_test_time 0.3138008117675781
Test Epoch1 threshold 0.9 Acc 0.9173684210526316, AUC 0.9821805953979492, avg_entr 0.025016089901328087
ep1_t0.9_test_time 0.3134472370147705
gc 0
Train Epoch2 Acc 0.96365 (115638/120000), AUC 0.9946867823600769
ep2_train_time 80.09346413612366
Test Epoch2 threshold 0.1 Acc 0.9184210526315789, AUC 0.9785813689231873, avg_entr 0.007319357246160507
ep2_t0.1_test_time 0.4344053268432617
Test Epoch2 threshold 0.2 Acc 0.9186842105263158, AUC 0.9804192781448364, avg_entr 0.012502357363700867
ep2_t0.2_test_time 0.39123034477233887
Test Epoch2 threshold 0.3 Acc 0.9189473684210526, AUC 0.9820000529289246, avg_entr 0.02282801643013954
ep2_t0.3_test_time 0.3444356918334961
Test Epoch2 threshold 0.4 Acc 0.9188157894736843, AUC 0.9821026921272278, avg_entr 0.02424500323832035
ep2_t0.4_test_time 0.32794833183288574
Test Epoch2 threshold 0.5 Acc 0.9186842105263158, AUC 0.9821125268936157, avg_entr 0.02516702003777027
ep2_t0.5_test_time 0.31685876846313477
Test Epoch2 threshold 0.6 Acc 0.9182894736842105, AUC 0.9821177124977112, avg_entr 0.02553332969546318
ep2_t0.6_test_time 0.3153877258300781
Test Epoch2 threshold 0.7 Acc 0.9182894736842105, AUC 0.9821177124977112, avg_entr 0.02553332969546318
ep2_t0.7_test_time 0.3173699378967285
Test Epoch2 threshold 0.8 Acc 0.9182894736842105, AUC 0.9821177124977112, avg_entr 0.02553332969546318
ep2_t0.8_test_time 0.3140401840209961
Test Epoch2 threshold 0.9 Acc 0.9182894736842105, AUC 0.9821177124977112, avg_entr 0.02553332969546318
ep2_t0.9_test_time 0.31408047676086426
gc 0
Train Epoch3 Acc 0.9642333333333334 (115708/120000), AUC 0.9948246479034424
ep3_train_time 80.01173734664917
Test Epoch3 threshold 0.1 Acc 0.9180263157894737, AUC 0.9788485765457153, avg_entr 0.007351183798164129
ep3_t0.1_test_time 0.4283621311187744
Test Epoch3 threshold 0.2 Acc 0.9180263157894737, AUC 0.9806578755378723, avg_entr 0.012560867704451084
ep3_t0.2_test_time 0.38806796073913574
Test Epoch3 threshold 0.3 Acc 0.9180263157894737, AUC 0.9819172024726868, avg_entr 0.02238694205880165
ep3_t0.3_test_time 0.3453686237335205
Test Epoch3 threshold 0.4 Acc 0.9182894736842105, AUC 0.9821275472640991, avg_entr 0.02403278462588787
ep3_t0.4_test_time 0.32341670989990234
Test Epoch3 threshold 0.5 Acc 0.9186842105263158, AUC 0.9821445941925049, avg_entr 0.02482655458152294
ep3_t0.5_test_time 0.3182668685913086
Test Epoch3 threshold 0.6 Acc 0.9181578947368421, AUC 0.9821277856826782, avg_entr 0.025189053267240524
ep3_t0.6_test_time 0.3142518997192383
Test Epoch3 threshold 0.7 Acc 0.9181578947368421, AUC 0.9821277856826782, avg_entr 0.025189053267240524
ep3_t0.7_test_time 0.3137972354888916
Test Epoch3 threshold 0.8 Acc 0.9181578947368421, AUC 0.9821277856826782, avg_entr 0.025189053267240524
ep3_t0.8_test_time 0.31342387199401855
Test Epoch3 threshold 0.9 Acc 0.9181578947368421, AUC 0.9821277856826782, avg_entr 0.025189053267240524
ep3_t0.9_test_time 0.31422901153564453
gc 0
Train Epoch4 Acc 0.96425 (115710/120000), AUC 0.9949184656143188
ep4_train_time 80.09976291656494
Test Epoch4 threshold 0.1 Acc 0.9172368421052631, AUC 0.9785435795783997, avg_entr 0.007111124694347382
ep4_t0.1_test_time 0.4305434226989746
Test Epoch4 threshold 0.2 Acc 0.9176315789473685, AUC 0.9802371263504028, avg_entr 0.012357998639345169
ep4_t0.2_test_time 0.38759326934814453
Test Epoch4 threshold 0.3 Acc 0.9185526315789474, AUC 0.9819535613059998, avg_entr 0.022474374622106552
ep4_t0.3_test_time 0.34165239334106445
Test Epoch4 threshold 0.4 Acc 0.9180263157894737, AUC 0.9821483492851257, avg_entr 0.024140752851963043
ep4_t0.4_test_time 0.32541799545288086
Test Epoch4 threshold 0.5 Acc 0.9181578947368421, AUC 0.982148289680481, avg_entr 0.024916088208556175
ep4_t0.5_test_time 0.31789135932922363
Test Epoch4 threshold 0.6 Acc 0.9177631578947368, AUC 0.9821280837059021, avg_entr 0.02527809701859951
ep4_t0.6_test_time 0.3142228126525879
Test Epoch4 threshold 0.7 Acc 0.9177631578947368, AUC 0.9821280837059021, avg_entr 0.02527809701859951
ep4_t0.7_test_time 0.31392741203308105
Test Epoch4 threshold 0.8 Acc 0.9177631578947368, AUC 0.9821280837059021, avg_entr 0.02527809701859951
ep4_t0.8_test_time 0.3137357234954834
Test Epoch4 threshold 0.9 Acc 0.9177631578947368, AUC 0.9821280837059021, avg_entr 0.02527809701859951
ep4_t0.9_test_time 0.3139936923980713
gc 0
Train Epoch5 Acc 0.9645333333333334 (115744/120000), AUC 0.9948807954788208
ep5_train_time 80.03011202812195
Test Epoch5 threshold 0.1 Acc 0.9177631578947368, AUC 0.978736937046051, avg_entr 0.007226540707051754
ep5_t0.1_test_time 0.42977046966552734
Test Epoch5 threshold 0.2 Acc 0.9180263157894737, AUC 0.9803847670555115, avg_entr 0.01255102176219225
ep5_t0.2_test_time 0.38919973373413086
Test Epoch5 threshold 0.3 Acc 0.9184210526315789, AUC 0.9819545149803162, avg_entr 0.02240239456295967
ep5_t0.3_test_time 0.345400333404541
Test Epoch5 threshold 0.4 Acc 0.9181578947368421, AUC 0.9821462035179138, avg_entr 0.024148615077137947
ep5_t0.4_test_time 0.32354116439819336
Test Epoch5 threshold 0.5 Acc 0.9184210526315789, AUC 0.9821317791938782, avg_entr 0.024782560765743256
ep5_t0.5_test_time 0.32005786895751953
Test Epoch5 threshold 0.6 Acc 0.9180263157894737, AUC 0.9821241497993469, avg_entr 0.02520832233130932
ep5_t0.6_test_time 0.3149385452270508
Test Epoch5 threshold 0.7 Acc 0.9180263157894737, AUC 0.9821241497993469, avg_entr 0.02520832233130932
ep5_t0.7_test_time 0.3155519962310791
Test Epoch5 threshold 0.8 Acc 0.9180263157894737, AUC 0.9821241497993469, avg_entr 0.02520832233130932
ep5_t0.8_test_time 0.3154118061065674
Test Epoch5 threshold 0.9 Acc 0.9180263157894737, AUC 0.9821241497993469, avg_entr 0.02520832233130932
ep5_t0.9_test_time 0.31574010848999023
gc 0
Train Epoch6 Acc 0.9644666666666667 (115736/120000), AUC 0.9949147701263428
ep6_train_time 80.01745295524597
Test Epoch6 threshold 0.1 Acc 0.9175, AUC 0.9787389636039734, avg_entr 0.007242904510349035
ep6_t0.1_test_time 0.4293954372406006
Test Epoch6 threshold 0.2 Acc 0.9178947368421052, AUC 0.9801886081695557, avg_entr 0.012450761161744595
ep6_t0.2_test_time 0.38878512382507324
Test Epoch6 threshold 0.3 Acc 0.9181578947368421, AUC 0.9819456338882446, avg_entr 0.022450624033808708
ep6_t0.3_test_time 0.34609436988830566
Test Epoch6 threshold 0.4 Acc 0.9178947368421052, AUC 0.9821453094482422, avg_entr 0.02416173927485943
ep6_t0.4_test_time 0.3247556686401367
Test Epoch6 threshold 0.5 Acc 0.9181578947368421, AUC 0.9821310639381409, avg_entr 0.024794695898890495
ep6_t0.5_test_time 0.32114529609680176
Test Epoch6 threshold 0.6 Acc 0.9177631578947368, AUC 0.9821234941482544, avg_entr 0.025221118703484535
ep6_t0.6_test_time 0.31570959091186523
Test Epoch6 threshold 0.7 Acc 0.9177631578947368, AUC 0.9821234941482544, avg_entr 0.025221118703484535
ep6_t0.7_test_time 0.3157358169555664
Test Epoch6 threshold 0.8 Acc 0.9177631578947368, AUC 0.9821234941482544, avg_entr 0.025221118703484535
ep6_t0.8_test_time 0.3154454231262207
Test Epoch6 threshold 0.9 Acc 0.9177631578947368, AUC 0.9821234941482544, avg_entr 0.025221118703484535
ep6_t0.9_test_time 0.3151378631591797
gc 0
Train Epoch7 Acc 0.9645666666666667 (115748/120000), AUC 0.9948773384094238
ep7_train_time 80.17073965072632
Test Epoch7 threshold 0.1 Acc 0.9176315789473685, AUC 0.9787392020225525, avg_entr 0.007246784400194883
ep7_t0.1_test_time 0.42819666862487793
Test Epoch7 threshold 0.2 Acc 0.9180263157894737, AUC 0.9801887273788452, avg_entr 0.01245102845132351
ep7_t0.2_test_time 0.3890116214752197
Test Epoch7 threshold 0.3 Acc 0.9182894736842105, AUC 0.9819456338882446, avg_entr 0.022452177479863167
ep7_t0.3_test_time 0.34632062911987305
Test Epoch7 threshold 0.4 Acc 0.9180263157894737, AUC 0.9821455478668213, avg_entr 0.02416420355439186
ep7_t0.4_test_time 0.3253469467163086
Test Epoch7 threshold 0.5 Acc 0.9182894736842105, AUC 0.9821312427520752, avg_entr 0.024797018617391586
ep7_t0.5_test_time 0.3172917366027832
Test Epoch7 threshold 0.6 Acc 0.9178947368421052, AUC 0.9821237325668335, avg_entr 0.02522357739508152
ep7_t0.6_test_time 0.31499218940734863
Test Epoch7 threshold 0.7 Acc 0.9178947368421052, AUC 0.9821237325668335, avg_entr 0.02522357739508152
ep7_t0.7_test_time 0.3150324821472168
Test Epoch7 threshold 0.8 Acc 0.9178947368421052, AUC 0.9821237325668335, avg_entr 0.02522357739508152
ep7_t0.8_test_time 0.3136637210845947
Test Epoch7 threshold 0.9 Acc 0.9178947368421052, AUC 0.9821237325668335, avg_entr 0.02522357739508152
ep7_t0.9_test_time 0.31386733055114746
gc 0
Train Epoch8 Acc 0.9642916666666667 (115715/120000), AUC 0.9949156641960144
ep8_train_time 80.03483128547668
Test Epoch8 threshold 0.1 Acc 0.9176315789473685, AUC 0.9787388443946838, avg_entr 0.007246376480907202
ep8_t0.1_test_time 0.42775559425354004
Test Epoch8 threshold 0.2 Acc 0.9180263157894737, AUC 0.980188250541687, avg_entr 0.01245085522532463
ep8_t0.2_test_time 0.38766908645629883
Test Epoch8 threshold 0.3 Acc 0.9182894736842105, AUC 0.9819455146789551, avg_entr 0.02245226316154003
ep8_t0.3_test_time 0.34447598457336426
Test Epoch8 threshold 0.4 Acc 0.9180263157894737, AUC 0.9821454286575317, avg_entr 0.024164721369743347
ep8_t0.4_test_time 0.3236351013183594
Test Epoch8 threshold 0.5 Acc 0.9182894736842105, AUC 0.9821311831474304, avg_entr 0.02479749545454979
ep8_t0.5_test_time 0.3178412914276123
Test Epoch8 threshold 0.6 Acc 0.9178947368421052, AUC 0.9821235537528992, avg_entr 0.02522408217191696
ep8_t0.6_test_time 0.31374120712280273
Test Epoch8 threshold 0.7 Acc 0.9178947368421052, AUC 0.9821235537528992, avg_entr 0.02522408217191696
ep8_t0.7_test_time 0.31306886672973633
Test Epoch8 threshold 0.8 Acc 0.9178947368421052, AUC 0.9821235537528992, avg_entr 0.02522408217191696
ep8_t0.8_test_time 0.3133578300476074
Test Epoch8 threshold 0.9 Acc 0.9178947368421052, AUC 0.9821235537528992, avg_entr 0.02522408217191696
ep8_t0.9_test_time 0.31354284286499023
gc 0
Train Epoch9 Acc 0.9647 (115764/120000), AUC 0.9949036240577698
ep9_train_time 80.0535933971405
Test Epoch9 threshold 0.1 Acc 0.9176315789473685, AUC 0.9787383079528809, avg_entr 0.007245785556733608
ep9_t0.1_test_time 0.42756223678588867
Test Epoch9 threshold 0.2 Acc 0.9178947368421052, AUC 0.9801815152168274, avg_entr 0.012467932887375355
ep9_t0.2_test_time 0.3892483711242676
Test Epoch9 threshold 0.3 Acc 0.9182894736842105, AUC 0.9819453954696655, avg_entr 0.02245251275599003
ep9_t0.3_test_time 0.3461873531341553
Test Epoch9 threshold 0.4 Acc 0.9180263157894737, AUC 0.9821453094482422, avg_entr 0.024165121838450432
ep9_t0.4_test_time 0.3246729373931885
Test Epoch9 threshold 0.5 Acc 0.9182894736842105, AUC 0.9821310639381409, avg_entr 0.024797923862934113
ep9_t0.5_test_time 0.3175230026245117
Test Epoch9 threshold 0.6 Acc 0.9178947368421052, AUC 0.9821233749389648, avg_entr 0.025224534794688225
ep9_t0.6_test_time 0.3154337406158447
Test Epoch9 threshold 0.7 Acc 0.9178947368421052, AUC 0.9821233749389648, avg_entr 0.025224534794688225
ep9_t0.7_test_time 0.3148210048675537
Test Epoch9 threshold 0.8 Acc 0.9178947368421052, AUC 0.9821233749389648, avg_entr 0.025224534794688225
ep9_t0.8_test_time 0.3150203227996826
Test Epoch9 threshold 0.9 Acc 0.9178947368421052, AUC 0.9821233749389648, avg_entr 0.025224534794688225
ep9_t0.9_test_time 0.3146483898162842
Best AUC 0.9821836948394775
train_loss (2, 5, 10)
valid_acc (10, 9)
valid_AUC (10, 9)
train_acc (10,)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Start Testing
Load ckpt at ckpt/ag_news_transformeral_l5_pad175_m5_fix//ag_news_transformeral_l5_prefix.pt
[[1701   59   93   47]
 [  11 1867   12   10]
 [  42   15 1702  141]
 [  42   15  141 1702]]
Figure(640x480)
tensor([3.1542e-04, 2.5819e-08, 2.1125e-03,  ..., 5.5967e-02, 6.7533e-08,
        2.4995e-04])
[[1716   57   71   56]
 [  22 1856   11   11]
 [  57   17 1693  133]
 [  55   11  146 1688]]
Figure(640x480)
tensor([2.6353e-06, 3.5464e-08, 2.1237e-08,  ..., 4.2977e-08, 3.4399e-08,
        3.1603e-08])
[[1716   57   71   56]
 [  23 1853   13   11]
 [  59   17 1692  132]
 [  55   11  152 1682]]
Figure(640x480)
tensor([3.2687e-07, 1.5479e-07, 3.1971e-08,  ..., 4.8246e-08, 3.2574e-08,
        3.3539e-08])
[[1718   56   70   56]
 [  22 1853   13   12]
 [  58   17 1692  133]
 [  56   11  144 1689]]
Figure(640x480)
tensor([2.0687e-07, 3.6107e-08, 3.1872e-08,  ..., 4.4540e-08, 3.3570e-08,
        3.4655e-08])
[[1717   57   70   56]
 [  22 1854   12   12]
 [  57   18 1692  133]
 [  55   11  144 1690]]
Figure(640x480)
tensor([2.0743e-07, 4.0867e-08, 3.8667e-08,  ..., 3.9267e-08, 3.5120e-08,
        3.6333e-08])
