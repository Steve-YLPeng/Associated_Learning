total count words 222751
vocab size 30000
train size 40000, valid size 5000, test size 5000
found 27937 words in glove
model: TransformerALsideText(
  (layers): ModuleList(
    (0): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=2, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=2, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
  (emb_layers): ModuleList(
    (0): Embedding(30000, 300)
    (1): Embedding(30000, 300)
    (2): Embedding(30000, 300)
    (3): Embedding(30000, 300)
    (4): Embedding(30000, 300)
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.multi_headed_attention.linears.0.weight 90000
layers.0.enc.f.multi_headed_attention.linears.0.bias 300
layers.0.enc.f.multi_headed_attention.linears.1.weight 90000
layers.0.enc.f.multi_headed_attention.linears.1.bias 300
layers.0.enc.f.multi_headed_attention.linears.2.weight 90000
layers.0.enc.f.multi_headed_attention.linears.2.bias 300
layers.0.enc.f.multi_headed_attention.linears.3.weight 90000
layers.0.enc.f.multi_headed_attention.linears.3.bias 300
layers.0.enc.f.feed_forward.w_1.weight 90000
layers.0.enc.f.feed_forward.w_1.bias 300
layers.0.enc.f.feed_forward.w_2.weight 90000
layers.0.enc.f.feed_forward.w_2.bias 300
layers.0.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.0.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.0.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.0.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.0.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.0.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.0.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.0.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.0.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.0.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.0.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.0.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.0.enc.f.encoder.norm.a 300
layers.0.enc.f.encoder.norm.b 300
layers.0.ae.g.0.weight 256
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 256
layers.0.ae.h.0.bias 2
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
emb_layers.0.weight 9000000
emb_layers.1.weight 9000000
emb_layers.2.weight 9000000
emb_layers.3.weight 9000000
emb_layers.4.weight 9000000
Total Trainable Params: 50758378
init_time 20.942779535
Start Training
gc 0
Train Epoch0 Acc 0.5014 (20056/40000), AUC 0.49980950355529785
ep0_train_time 65.74035216099999
Test Epoch0 layer0 Acc 0.5694, AUC 0.6275955438613892, avg_entr 0.6847285628318787, f1 0.5694000124931335
ep0_l0_test_time 0.6486118990000023
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 0
Test Epoch0 layer1 Acc 0.5202, AUC 0.5482692718505859, avg_entr 0.6901082992553711, f1 0.5202000141143799
ep0_l1_test_time 0.831955863999994
Test Epoch0 layer2 Acc 0.5158, AUC 0.5284405946731567, avg_entr 0.6917695999145508, f1 0.5157999992370605
ep0_l2_test_time 1.139978100999997
Test Epoch0 layer3 Acc 0.5014, AUC 0.5060093402862549, avg_entr 0.6869567632675171, f1 0.5013999938964844
ep0_l3_test_time 1.6232138769999978
Test Epoch0 layer4 Acc 0.5, AUC 0.4866046905517578, avg_entr 0.6868873238563538, f1 0.5
ep0_l4_test_time 2.2509236319999957
gc 0
Train Epoch1 Acc 0.50615 (20246/40000), AUC 0.5097178816795349
ep1_train_time 65.53100036699999
Test Epoch1 layer0 Acc 0.645, AUC 0.741473913192749, avg_entr 0.5868164896965027, f1 0.6449999809265137
ep1_l0_test_time 0.6418155209999838
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 1
Test Epoch1 layer1 Acc 0.6428, AUC 0.7418602705001831, avg_entr 0.6197000741958618, f1 0.642799973487854
ep1_l1_test_time 0.8262312070000064
Test Epoch1 layer2 Acc 0.501, AUC 0.7318620681762695, avg_entr 0.6641256809234619, f1 0.5009999871253967
ep1_l2_test_time 1.1704737260000115
Test Epoch1 layer3 Acc 0.5, AUC 0.6911078691482544, avg_entr 0.6538254022598267, f1 0.5
ep1_l3_test_time 1.629380148999985
Test Epoch1 layer4 Acc 0.5, AUC 0.4863123595714569, avg_entr 0.6596341729164124, f1 0.5
ep1_l4_test_time 2.2525275190000116
gc 0
Train Epoch2 Acc 0.550775 (22031/40000), AUC 0.5729608535766602
ep2_train_time 65.54394560100002
Test Epoch2 layer0 Acc 0.7218, AUC 0.7991176843643188, avg_entr 0.4894247055053711, f1 0.7217999696731567
ep2_l0_test_time 0.6280565650000085
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 2
Test Epoch2 layer1 Acc 0.7332, AUC 0.8070180416107178, avg_entr 0.5050362944602966, f1 0.7332000136375427
ep2_l1_test_time 0.828348874999989
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 2
Test Epoch2 layer2 Acc 0.7172, AUC 0.8049498796463013, avg_entr 0.5300501585006714, f1 0.717199981212616
ep2_l2_test_time 1.1559057139999993
Test Epoch2 layer3 Acc 0.7216, AUC 0.8044978380203247, avg_entr 0.5837290287017822, f1 0.7215999960899353
ep2_l3_test_time 1.6241724799999986
Test Epoch2 layer4 Acc 0.7128, AUC 0.8037596344947815, avg_entr 0.6696709990501404, f1 0.7128000259399414
ep2_l4_test_time 2.2561897989999977
gc 0
Train Epoch3 Acc 0.636075 (25443/40000), AUC 0.6908017992973328
ep3_train_time 65.497653009
Test Epoch3 layer0 Acc 0.685, AUC 0.828894317150116, avg_entr 0.37611448764801025, f1 0.6850000023841858
ep3_l0_test_time 0.6256259870000349
Test Epoch3 layer1 Acc 0.6828, AUC 0.8448000550270081, avg_entr 0.3705454468727112, f1 0.6827999949455261
ep3_l1_test_time 0.8224488920000113
Test Epoch3 layer2 Acc 0.6682, AUC 0.842222273349762, avg_entr 0.393572598695755, f1 0.6682000160217285
ep3_l2_test_time 1.1428163900000072
Test Epoch3 layer3 Acc 0.6792, AUC 0.843317449092865, avg_entr 0.4392300844192505, f1 0.6791999936103821
ep3_l3_test_time 1.6250007819999723
Test Epoch3 layer4 Acc 0.7444, AUC 0.8430308103561401, avg_entr 0.5495196580886841, f1 0.7444000244140625
ep3_l4_test_time 2.258937712999966
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 3
gc 0
Train Epoch4 Acc 0.708475 (28339/40000), AUC 0.7844176292419434
ep4_train_time 65.490739975
Test Epoch4 layer0 Acc 0.7334, AUC 0.8455331921577454, avg_entr 0.3508743941783905, f1 0.7333999872207642
ep4_l0_test_time 0.623829236000006
Test Epoch4 layer1 Acc 0.7358, AUC 0.8586329221725464, avg_entr 0.3383401334285736, f1 0.73580002784729
ep4_l1_test_time 0.8168886159999715
Test Epoch4 layer2 Acc 0.7224, AUC 0.8583763837814331, avg_entr 0.35119739174842834, f1 0.7224000096321106
ep4_l2_test_time 1.1414341630000422
Test Epoch4 layer3 Acc 0.721, AUC 0.8588403463363647, avg_entr 0.37884896993637085, f1 0.7210000157356262
ep4_l3_test_time 1.624620063000009
Test Epoch4 layer4 Acc 0.7018, AUC 0.8594427108764648, avg_entr 0.41197797656059265, f1 0.7017999887466431
ep4_l4_test_time 2.26301313700003
gc 0
Train Epoch5 Acc 0.7694 (30776/40000), AUC 0.849576473236084
ep5_train_time 65.66149107799998
Test Epoch5 layer0 Acc 0.7694, AUC 0.8577190637588501, avg_entr 0.318880170583725, f1 0.7694000005722046
ep5_l0_test_time 0.6238701260000425
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 5
Test Epoch5 layer1 Acc 0.7826, AUC 0.8690557479858398, avg_entr 0.2771874666213989, f1 0.7825999855995178
ep5_l1_test_time 0.8480441319999841
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 5
Test Epoch5 layer2 Acc 0.7798, AUC 0.8701538443565369, avg_entr 0.27327531576156616, f1 0.7797999978065491
ep5_l2_test_time 1.154921024000032
Test Epoch5 layer3 Acc 0.7746, AUC 0.8708987236022949, avg_entr 0.2762877345085144, f1 0.7746000289916992
ep5_l3_test_time 1.6179952869999852
Test Epoch5 layer4 Acc 0.757, AUC 0.8707573413848877, avg_entr 0.2710230350494385, f1 0.7570000886917114
ep5_l4_test_time 2.251287015999992
gc 0
Train Epoch6 Acc 0.787225 (31489/40000), AUC 0.8705505132675171
ep6_train_time 65.51529869099994
Test Epoch6 layer0 Acc 0.7718, AUC 0.8633548021316528, avg_entr 0.2870628535747528, f1 0.7717999815940857
ep6_l0_test_time 0.6286924519999957
Test Epoch6 layer1 Acc 0.7746, AUC 0.8762375116348267, avg_entr 0.22964496910572052, f1 0.7746000289916992
ep6_l1_test_time 0.8304154280000375
Test Epoch6 layer2 Acc 0.772, AUC 0.878923237323761, avg_entr 0.21712656319141388, f1 0.7720000147819519
ep6_l2_test_time 1.1414662390000103
Test Epoch6 layer3 Acc 0.7664, AUC 0.8795930743217468, avg_entr 0.21447409689426422, f1 0.7663999199867249
ep6_l3_test_time 1.6236483630000293
Test Epoch6 layer4 Acc 0.765, AUC 0.8796747922897339, avg_entr 0.21334616839885712, f1 0.7649999856948853
ep6_l4_test_time 2.2600172869999824
gc 0
Train Epoch7 Acc 0.80905 (32362/40000), AUC 0.8891165256500244
ep7_train_time 65.66707315099995
Test Epoch7 layer0 Acc 0.778, AUC 0.8711568117141724, avg_entr 0.27646908164024353, f1 0.777999997138977
ep7_l0_test_time 0.6301570709999851
Test Epoch7 layer1 Acc 0.789, AUC 0.8845722079277039, avg_entr 0.20663276314735413, f1 0.7889999747276306
ep7_l1_test_time 0.8225522029999865
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 7
Test Epoch7 layer2 Acc 0.7912, AUC 0.8887444734573364, avg_entr 0.2004670351743698, f1 0.7911999821662903
ep7_l2_test_time 1.1571563129999731
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 7
Test Epoch7 layer3 Acc 0.7888, AUC 0.8892182111740112, avg_entr 0.19685740768909454, f1 0.7888000011444092
ep7_l3_test_time 1.6281078639999578
Test Epoch7 layer4 Acc 0.7818, AUC 0.8894813060760498, avg_entr 0.19997547566890717, f1 0.7817999720573425
ep7_l4_test_time 2.2552098780000733
gc 0
Train Epoch8 Acc 0.817675 (32707/40000), AUC 0.8945518732070923
ep8_train_time 65.46696805800002
Test Epoch8 layer0 Acc 0.7846, AUC 0.8707053065299988, avg_entr 0.2523099482059479, f1 0.784600019454956
ep8_l0_test_time 0.624434382000004
Test Epoch8 layer1 Acc 0.7964, AUC 0.8858966827392578, avg_entr 0.1752576231956482, f1 0.7964000105857849
ep8_l1_test_time 0.8173647500000243
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 8
Test Epoch8 layer2 Acc 0.7976, AUC 0.8900219202041626, avg_entr 0.16687123477458954, f1 0.7976000308990479
ep8_l2_test_time 1.1460005500000534
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 8
Test Epoch8 layer3 Acc 0.7988, AUC 0.8905124068260193, avg_entr 0.16293705999851227, f1 0.798799991607666
ep8_l3_test_time 1.624639982000076
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 8
Test Epoch8 layer4 Acc 0.7982, AUC 0.8907480239868164, avg_entr 0.16639435291290283, f1 0.7982000708580017
ep8_l4_test_time 2.260380696000084
gc 0
Train Epoch9 Acc 0.836225 (33449/40000), AUC 0.9136756658554077
ep9_train_time 65.48634433299992
Test Epoch9 layer0 Acc 0.7928, AUC 0.8757213354110718, avg_entr 0.2349967211484909, f1 0.7928000092506409
ep9_l0_test_time 0.6241960319999862
Test Epoch9 layer1 Acc 0.8088, AUC 0.8907103538513184, avg_entr 0.16441166400909424, f1 0.8087999820709229
ep9_l1_test_time 0.8254167430000052
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 9
Test Epoch9 layer2 Acc 0.8124, AUC 0.8955988883972168, avg_entr 0.15434211492538452, f1 0.8123999238014221
ep9_l2_test_time 1.1496896600000355
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 9
Test Epoch9 layer3 Acc 0.8138, AUC 0.896158754825592, avg_entr 0.15311072766780853, f1 0.8137999773025513
ep9_l3_test_time 1.6248817020000388
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 9
Test Epoch9 layer4 Acc 0.8134, AUC 0.8964736461639404, avg_entr 0.1505177617073059, f1 0.8133999109268188
ep9_l4_test_time 2.2583041310000453
gc 0
Train Epoch10 Acc 0.847 (33880/40000), AUC 0.920661211013794
ep10_train_time 65.47392174700008
Test Epoch10 layer0 Acc 0.7924, AUC 0.8767993450164795, avg_entr 0.22604118287563324, f1 0.7923999428749084
ep10_l0_test_time 0.622602841999992
Test Epoch10 layer1 Acc 0.8142, AUC 0.8926549553871155, avg_entr 0.150728240609169, f1 0.8141999840736389
ep10_l1_test_time 0.8193743560000257
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 10
Test Epoch10 layer2 Acc 0.8148, AUC 0.8962066769599915, avg_entr 0.1329813301563263, f1 0.8148000240325928
ep10_l2_test_time 1.1480660690000377
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 10
Test Epoch10 layer3 Acc 0.8092, AUC 0.8969837427139282, avg_entr 0.1295730471611023, f1 0.8092000484466553
ep10_l3_test_time 1.6328648690000591
Test Epoch10 layer4 Acc 0.8108, AUC 0.8973073959350586, avg_entr 0.12593109905719757, f1 0.8108000159263611
ep10_l4_test_time 2.2624725289999787
gc 0
Train Epoch11 Acc 0.862125 (34485/40000), AUC 0.9330141544342041
ep11_train_time 65.54144386300004
Test Epoch11 layer0 Acc 0.7886, AUC 0.8750211000442505, avg_entr 0.19851268827915192, f1 0.7886000275611877
ep11_l0_test_time 0.6240766219999614
Test Epoch11 layer1 Acc 0.8064, AUC 0.8909175395965576, avg_entr 0.13051559031009674, f1 0.8064000010490417
ep11_l1_test_time 0.8252104939999754
Test Epoch11 layer2 Acc 0.8086, AUC 0.8958637714385986, avg_entr 0.11657800525426865, f1 0.8086000084877014
ep11_l2_test_time 1.146819687000061
Test Epoch11 layer3 Acc 0.809, AUC 0.8961231708526611, avg_entr 0.11611048877239227, f1 0.8090000748634338
ep11_l3_test_time 1.63396443299996
Test Epoch11 layer4 Acc 0.8092, AUC 0.8967070579528809, avg_entr 0.11542636901140213, f1 0.8092000484466553
ep11_l4_test_time 2.2709438370000044
gc 0
Train Epoch12 Acc 0.869525 (34781/40000), AUC 0.9373745322227478
ep12_train_time 65.60356352600002
Test Epoch12 layer0 Acc 0.786, AUC 0.8767451047897339, avg_entr 0.20373591780662537, f1 0.7860000133514404
ep12_l0_test_time 0.6235708310000518
Test Epoch12 layer1 Acc 0.8076, AUC 0.8924978375434875, avg_entr 0.12280841171741486, f1 0.8076000213623047
ep12_l1_test_time 0.8161622659999921
Test Epoch12 layer2 Acc 0.8092, AUC 0.8979229927062988, avg_entr 0.10873017460107803, f1 0.8092000484466553
ep12_l2_test_time 1.1393531069999199
Test Epoch12 layer3 Acc 0.8106, AUC 0.8983790278434753, avg_entr 0.10790947079658508, f1 0.8105999231338501
ep12_l3_test_time 1.6237985090000393
Test Epoch12 layer4 Acc 0.8108, AUC 0.8987680673599243, avg_entr 0.10606042295694351, f1 0.8108000159263611
ep12_l4_test_time 2.2587228910000476
gc 0
Train Epoch13 Acc 0.882525 (35301/40000), AUC 0.9476562738418579
ep13_train_time 65.78565327800015
Test Epoch13 layer0 Acc 0.7934, AUC 0.8749727010726929, avg_entr 0.193875253200531, f1 0.79339998960495
ep13_l0_test_time 0.6383158339999682
Test Epoch13 layer1 Acc 0.809, AUC 0.8890050053596497, avg_entr 0.1193319708108902, f1 0.8090000748634338
ep13_l1_test_time 0.8227153059999637
Test Epoch13 layer2 Acc 0.8108, AUC 0.8940497636795044, avg_entr 0.10485778748989105, f1 0.8108000159263611
ep13_l2_test_time 1.1439676499999223
Test Epoch13 layer3 Acc 0.81, AUC 0.8947663307189941, avg_entr 0.10404417663812637, f1 0.809999942779541
ep13_l3_test_time 1.6234356649999881
Test Epoch13 layer4 Acc 0.8118, AUC 0.8953112363815308, avg_entr 0.10292923450469971, f1 0.8118000030517578
ep13_l4_test_time 2.259775349999927
gc 0
Train Epoch14 Acc 0.888525 (35541/40000), AUC 0.9501283168792725
ep14_train_time 65.59705817300005
Test Epoch14 layer0 Acc 0.7804, AUC 0.8743486404418945, avg_entr 0.18486687541007996, f1 0.7803999781608582
ep14_l0_test_time 0.6241896439998982
Test Epoch14 layer1 Acc 0.7988, AUC 0.887717604637146, avg_entr 0.10045760124921799, f1 0.798799991607666
ep14_l1_test_time 0.8182035410000026
Test Epoch14 layer2 Acc 0.8006, AUC 0.893478512763977, avg_entr 0.09045795351266861, f1 0.800599992275238
ep14_l2_test_time 1.140135755000074
Test Epoch14 layer3 Acc 0.8026, AUC 0.89417564868927, avg_entr 0.090479776263237, f1 0.802600085735321
ep14_l3_test_time 1.6240756770000644
Test Epoch14 layer4 Acc 0.8034, AUC 0.8945828676223755, avg_entr 0.08796989917755127, f1 0.8033999800682068
ep14_l4_test_time 2.257618735000051
gc 0
Train Epoch15 Acc 0.906525 (36261/40000), AUC 0.9603593349456787
ep15_train_time 65.74004764100005
Test Epoch15 layer0 Acc 0.7858, AUC 0.8727409839630127, avg_entr 0.17937935888767242, f1 0.7857999801635742
ep15_l0_test_time 0.6242680440000186
Test Epoch15 layer1 Acc 0.8, AUC 0.8846367597579956, avg_entr 0.0888291597366333, f1 0.8000000715255737
ep15_l1_test_time 0.8157044269999005
Test Epoch15 layer2 Acc 0.8028, AUC 0.8928688764572144, avg_entr 0.07663658261299133, f1 0.8027999997138977
ep15_l2_test_time 1.1404549090000273
Test Epoch15 layer3 Acc 0.8026, AUC 0.8930782079696655, avg_entr 0.07472048699855804, f1 0.802600085735321
ep15_l3_test_time 1.6231713160000254
Test Epoch15 layer4 Acc 0.8018, AUC 0.8931571245193481, avg_entr 0.07212286442518234, f1 0.801800012588501
ep15_l4_test_time 2.257837552999945
gc 0
Train Epoch16 Acc 0.9158 (36632/40000), AUC 0.9671096801757812
ep16_train_time 65.59754583999984
Test Epoch16 layer0 Acc 0.7882, AUC 0.8709651231765747, avg_entr 0.17209428548812866, f1 0.7882000207901001
ep16_l0_test_time 0.6328235159999167
Test Epoch16 layer1 Acc 0.7996, AUC 0.87981778383255, avg_entr 0.07927048206329346, f1 0.7996000051498413
ep16_l1_test_time 0.818765423000059
Test Epoch16 layer2 Acc 0.801, AUC 0.8882983326911926, avg_entr 0.06982991844415665, f1 0.8010000586509705
ep16_l2_test_time 1.136866519000023
Test Epoch16 layer3 Acc 0.8016, AUC 0.8879549503326416, avg_entr 0.06809353083372116, f1 0.8015999794006348
ep16_l3_test_time 1.6266733109998768
Test Epoch16 layer4 Acc 0.8012, AUC 0.8889108896255493, avg_entr 0.0661054477095604, f1 0.8011999726295471
ep16_l4_test_time 2.266545122000025
gc 0
Train Epoch17 Acc 0.9267 (37068/40000), AUC 0.9758113622665405
ep17_train_time 65.59659236700008
Test Epoch17 layer0 Acc 0.7838, AUC 0.8676755428314209, avg_entr 0.1720132976770401, f1 0.7838000059127808
ep17_l0_test_time 0.6226101380000273
Test Epoch17 layer1 Acc 0.795, AUC 0.8731323480606079, avg_entr 0.07879481464624405, f1 0.7950000166893005
ep17_l1_test_time 0.8175039230000039
Test Epoch17 layer2 Acc 0.798, AUC 0.885370135307312, avg_entr 0.06696915626525879, f1 0.7979999780654907
ep17_l2_test_time 1.1397486729999855
Test Epoch17 layer3 Acc 0.7984, AUC 0.8852323293685913, avg_entr 0.06516903638839722, f1 0.7983999848365784
ep17_l3_test_time 1.623022020999997
Test Epoch17 layer4 Acc 0.7996, AUC 0.8857783079147339, avg_entr 0.06206519901752472, f1 0.7996000051498413
ep17_l4_test_time 2.258387696999989
gc 0
Train Epoch18 Acc 0.93205 (37282/40000), AUC 0.9791563749313354
ep18_train_time 65.61435291599992
Test Epoch18 layer0 Acc 0.7878, AUC 0.8683475255966187, avg_entr 0.16751599311828613, f1 0.7877999544143677
ep18_l0_test_time 0.6306831379999949
Test Epoch18 layer1 Acc 0.7978, AUC 0.875187873840332, avg_entr 0.07080787420272827, f1 0.7978000044822693
ep18_l1_test_time 0.81581488300003
Test Epoch18 layer2 Acc 0.7998, AUC 0.886214554309845, avg_entr 0.06398683041334152, f1 0.7997999787330627
ep18_l2_test_time 1.1395504010001787
Test Epoch18 layer3 Acc 0.8012, AUC 0.8857475519180298, avg_entr 0.06314850598573685, f1 0.8011999726295471
ep18_l3_test_time 1.6218588250001176
Test Epoch18 layer4 Acc 0.8022, AUC 0.8866842985153198, avg_entr 0.061159905046224594, f1 0.8022000193595886
ep18_l4_test_time 2.258133080000107
gc 0
Train Epoch19 Acc 0.940475 (37619/40000), AUC 0.9821469783782959
ep19_train_time 65.69278086899999
Test Epoch19 layer0 Acc 0.784, AUC 0.8654791712760925, avg_entr 0.16327805817127228, f1 0.7839999794960022
ep19_l0_test_time 0.6222942919998786
Test Epoch19 layer1 Acc 0.796, AUC 0.870501697063446, avg_entr 0.06698166579008102, f1 0.796000063419342
ep19_l1_test_time 0.8177308469998934
Test Epoch19 layer2 Acc 0.798, AUC 0.882000744342804, avg_entr 0.05904906615614891, f1 0.7979999780654907
ep19_l2_test_time 1.1393691819998821
Test Epoch19 layer3 Acc 0.7984, AUC 0.8811894655227661, avg_entr 0.05813310667872429, f1 0.7983999848365784
ep19_l3_test_time 1.6234913929999948
Test Epoch19 layer4 Acc 0.7974, AUC 0.8817611932754517, avg_entr 0.056057605892419815, f1 0.7973999977111816
ep19_l4_test_time 2.2585334779998902
gc 0
Train Epoch20 Acc 0.9448 (37792/40000), AUC 0.9837141036987305
ep20_train_time 65.6304839710001
Test Epoch20 layer0 Acc 0.7836, AUC 0.8640278577804565, avg_entr 0.1588246077299118, f1 0.7835999727249146
ep20_l0_test_time 0.6230477320000318
Test Epoch20 layer1 Acc 0.7964, AUC 0.868028461933136, avg_entr 0.06513825058937073, f1 0.7964000105857849
ep20_l1_test_time 0.8171265009998478
Test Epoch20 layer2 Acc 0.8, AUC 0.8841186761856079, avg_entr 0.055788587778806686, f1 0.8000000715255737
ep20_l2_test_time 1.1434843110000656
Test Epoch20 layer3 Acc 0.8018, AUC 0.8828166127204895, avg_entr 0.054369017481803894, f1 0.801800012588501
ep20_l3_test_time 1.624114766000048
Test Epoch20 layer4 Acc 0.8016, AUC 0.8832545280456543, avg_entr 0.050395552068948746, f1 0.8015999794006348
ep20_l4_test_time 2.2608401059999323
gc 0
Train Epoch21 Acc 0.9477 (37908/40000), AUC 0.9863176345825195
ep21_train_time 65.60710009500008
Test Epoch21 layer0 Acc 0.7836, AUC 0.863028883934021, avg_entr 0.15926679968833923, f1 0.7835999727249146
ep21_l0_test_time 0.6263236649999726
Test Epoch21 layer1 Acc 0.797, AUC 0.8674198389053345, avg_entr 0.06663114577531815, f1 0.796999990940094
ep21_l1_test_time 0.8168067159999737
Test Epoch21 layer2 Acc 0.8002, AUC 0.8818976283073425, avg_entr 0.056346919387578964, f1 0.8001999855041504
ep21_l2_test_time 1.1411839709999185
Test Epoch21 layer3 Acc 0.8014, AUC 0.8808696269989014, avg_entr 0.05546493083238602, f1 0.8014000058174133
ep21_l3_test_time 1.6226416619999782
Test Epoch21 layer4 Acc 0.8012, AUC 0.8811880350112915, avg_entr 0.05195445567369461, f1 0.8011999726295471
ep21_l4_test_time 2.2619943459999377
gc 0
Train Epoch22 Acc 0.95105 (38042/40000), AUC 0.9877761602401733
ep22_train_time 65.59870838100005
Test Epoch22 layer0 Acc 0.7798, AUC 0.8622912168502808, avg_entr 0.15689918398857117, f1 0.7797999978065491
ep22_l0_test_time 0.6235695269999724
Test Epoch22 layer1 Acc 0.7958, AUC 0.8612179756164551, avg_entr 0.059550005942583084, f1 0.7958000302314758
ep22_l1_test_time 0.8185356940000474
Test Epoch22 layer2 Acc 0.798, AUC 0.8797264695167542, avg_entr 0.05063767731189728, f1 0.7979999780654907
ep22_l2_test_time 1.148054239999965
Test Epoch22 layer3 Acc 0.7994, AUC 0.8790321350097656, avg_entr 0.050277650356292725, f1 0.7993999123573303
ep22_l3_test_time 1.6301855980000255
Test Epoch22 layer4 Acc 0.7996, AUC 0.8796721696853638, avg_entr 0.04599510133266449, f1 0.7996000051498413
ep22_l4_test_time 2.26777653299996
gc 0
Train Epoch23 Acc 0.95455 (38182/40000), AUC 0.9889628887176514
ep23_train_time 65.87373089099992
Test Epoch23 layer0 Acc 0.7838, AUC 0.8620244264602661, avg_entr 0.15705935657024384, f1 0.7838000059127808
ep23_l0_test_time 0.6223752429998513
Test Epoch23 layer1 Acc 0.7922, AUC 0.859387993812561, avg_entr 0.05850927159190178, f1 0.7922000288963318
ep23_l1_test_time 0.8147984479999195
Test Epoch23 layer2 Acc 0.795, AUC 0.8757045269012451, avg_entr 0.05031822994351387, f1 0.7950000166893005
ep23_l2_test_time 1.1397137210001347
Test Epoch23 layer3 Acc 0.7954, AUC 0.8744878172874451, avg_entr 0.04878760501742363, f1 0.795400083065033
ep23_l3_test_time 1.621970680000004
Test Epoch23 layer4 Acc 0.797, AUC 0.8749326467514038, avg_entr 0.04517973214387894, f1 0.796999990940094
ep23_l4_test_time 2.259203697999965
gc 0
Train Epoch24 Acc 0.9565 (38260/40000), AUC 0.989309549331665
ep24_train_time 65.60905547899984
Test Epoch24 layer0 Acc 0.781, AUC 0.8609038591384888, avg_entr 0.15495999157428741, f1 0.781000018119812
ep24_l0_test_time 0.6269024059999992
Test Epoch24 layer1 Acc 0.791, AUC 0.8570935130119324, avg_entr 0.05603451654314995, f1 0.7910000681877136
ep24_l1_test_time 0.8159216260000903
Test Epoch24 layer2 Acc 0.7914, AUC 0.873826265335083, avg_entr 0.04590262845158577, f1 0.7914000153541565
ep24_l2_test_time 1.140858721999848
Test Epoch24 layer3 Acc 0.793, AUC 0.8724840879440308, avg_entr 0.04496308043599129, f1 0.7929999828338623
ep24_l3_test_time 1.6229214540001067
Test Epoch24 layer4 Acc 0.7926, AUC 0.874697208404541, avg_entr 0.04152936488389969, f1 0.7925999760627747
ep24_l4_test_time 2.259958665999875
gc 0
Train Epoch25 Acc 0.958225 (38329/40000), AUC 0.9903742074966431
ep25_train_time 65.69380196399993
Test Epoch25 layer0 Acc 0.7808, AUC 0.8602181673049927, avg_entr 0.15444689989089966, f1 0.7808000445365906
ep25_l0_test_time 0.6243494799998643
Test Epoch25 layer1 Acc 0.793, AUC 0.8543871641159058, avg_entr 0.055148378014564514, f1 0.7929999828338623
ep25_l1_test_time 0.8189440629998899
Test Epoch25 layer2 Acc 0.7934, AUC 0.8745790719985962, avg_entr 0.045755501836538315, f1 0.79339998960495
ep25_l2_test_time 1.15324299100007
Test Epoch25 layer3 Acc 0.7948, AUC 0.8733328580856323, avg_entr 0.04477987065911293, f1 0.7947999835014343
ep25_l3_test_time 1.6237549670001954
Test Epoch25 layer4 Acc 0.7952, AUC 0.8728642463684082, avg_entr 0.041080016642808914, f1 0.7952000498771667
ep25_l4_test_time 2.2602072080001108
gc 0
Train Epoch26 Acc 0.95845 (38338/40000), AUC 0.9910842776298523
ep26_train_time 65.59758139899986
Test Epoch26 layer0 Acc 0.7778, AUC 0.8594425916671753, avg_entr 0.15427958965301514, f1 0.7778000235557556
ep26_l0_test_time 0.6269276959999388
Test Epoch26 layer1 Acc 0.7906, AUC 0.855197548866272, avg_entr 0.05511439964175224, f1 0.7906000018119812
ep26_l1_test_time 0.817317292000098
Test Epoch26 layer2 Acc 0.7938, AUC 0.875450074672699, avg_entr 0.045346759259700775, f1 0.7937999963760376
ep26_l2_test_time 1.1437037340001552
Test Epoch26 layer3 Acc 0.7928, AUC 0.8739804625511169, avg_entr 0.04400176554918289, f1 0.7928000092506409
ep26_l3_test_time 1.6275863180001124
Test Epoch26 layer4 Acc 0.794, AUC 0.8751549124717712, avg_entr 0.04081049561500549, f1 0.7940000295639038
ep26_l4_test_time 2.2622841830000198
gc 0
Train Epoch27 Acc 0.9607 (38428/40000), AUC 0.9910062551498413
ep27_train_time 65.65059915300003
Test Epoch27 layer0 Acc 0.781, AUC 0.8591163754463196, avg_entr 0.15339522063732147, f1 0.781000018119812
ep27_l0_test_time 0.625745550999909
Test Epoch27 layer1 Acc 0.7914, AUC 0.8540525436401367, avg_entr 0.053455695509910583, f1 0.7914000153541565
ep27_l1_test_time 0.8259725580001032
Test Epoch27 layer2 Acc 0.793, AUC 0.8730356097221375, avg_entr 0.04440479725599289, f1 0.7929999828338623
ep27_l2_test_time 1.1429652369999985
Test Epoch27 layer3 Acc 0.7932, AUC 0.8716979026794434, avg_entr 0.04275831952691078, f1 0.7932000160217285
ep27_l3_test_time 1.6248634799999309
Test Epoch27 layer4 Acc 0.7932, AUC 0.8730501532554626, avg_entr 0.03930939361453056, f1 0.7932000160217285
ep27_l4_test_time 2.261382761999812
gc 0
Train Epoch28 Acc 0.962225 (38489/40000), AUC 0.9916931390762329
ep28_train_time 65.63914258299997
Test Epoch28 layer0 Acc 0.7814, AUC 0.8587512969970703, avg_entr 0.15385089814662933, f1 0.7814000248908997
ep28_l0_test_time 0.6246167969998169
Test Epoch28 layer1 Acc 0.7928, AUC 0.8532106876373291, avg_entr 0.05225303769111633, f1 0.7928000092506409
ep28_l1_test_time 0.8180513480001537
Test Epoch28 layer2 Acc 0.7936, AUC 0.8739630579948425, avg_entr 0.04337436705827713, f1 0.7936000227928162
ep28_l2_test_time 1.1389804979999099
Test Epoch28 layer3 Acc 0.794, AUC 0.872103214263916, avg_entr 0.042028430849313736, f1 0.7940000295639038
ep28_l3_test_time 1.6232149170000412
Test Epoch28 layer4 Acc 0.7936, AUC 0.8720588684082031, avg_entr 0.038098156452178955, f1 0.7936000227928162
ep28_l4_test_time 2.260707592000017
gc 0
Train Epoch29 Acc 0.962425 (38497/40000), AUC 0.9925673007965088
ep29_train_time 65.65001868299987
Test Epoch29 layer0 Acc 0.7802, AUC 0.8587745428085327, avg_entr 0.15241718292236328, f1 0.7801999449729919
ep29_l0_test_time 0.6258672170001773
Test Epoch29 layer1 Acc 0.7928, AUC 0.852967381477356, avg_entr 0.052426982671022415, f1 0.7928000092506409
ep29_l1_test_time 0.8160839970000779
Test Epoch29 layer2 Acc 0.793, AUC 0.8736110329627991, avg_entr 0.042870134115219116, f1 0.7929999828338623
ep29_l2_test_time 1.143585124999845
Test Epoch29 layer3 Acc 0.7942, AUC 0.8705472350120544, avg_entr 0.041410498321056366, f1 0.7942000031471252
ep29_l3_test_time 1.6223127529997328
Test Epoch29 layer4 Acc 0.794, AUC 0.8721624612808228, avg_entr 0.03774317353963852, f1 0.7940000295639038
ep29_l4_test_time 2.259865322999758
gc 0
Train Epoch30 Acc 0.963225 (38529/40000), AUC 0.992391049861908
ep30_train_time 65.59473031299967
Test Epoch30 layer0 Acc 0.779, AUC 0.8588166832923889, avg_entr 0.15222015976905823, f1 0.7789999842643738
ep30_l0_test_time 0.6222453270002006
Test Epoch30 layer1 Acc 0.7922, AUC 0.8560789823532104, avg_entr 0.054241832345724106, f1 0.7922000288963318
ep30_l1_test_time 0.8321976639999775
Test Epoch30 layer2 Acc 0.7934, AUC 0.875706136226654, avg_entr 0.043862901628017426, f1 0.79339998960495
ep30_l2_test_time 1.1386391170003662
Test Epoch30 layer3 Acc 0.7944, AUC 0.8737192749977112, avg_entr 0.042163122445344925, f1 0.7943999767303467
ep30_l3_test_time 1.6242940649999582
Test Epoch30 layer4 Acc 0.794, AUC 0.8742001056671143, avg_entr 0.037725310772657394, f1 0.7940000295639038
ep30_l4_test_time 2.2598017510003956
gc 0
Train Epoch31 Acc 0.963575 (38543/40000), AUC 0.992971658706665
ep31_train_time 65.60598867399995
Test Epoch31 layer0 Acc 0.7788, AUC 0.8586337566375732, avg_entr 0.1510549634695053, f1 0.7788000106811523
ep31_l0_test_time 0.6223269259999142
Test Epoch31 layer1 Acc 0.7924, AUC 0.8538252115249634, avg_entr 0.05265252664685249, f1 0.7923999428749084
ep31_l1_test_time 0.8170755490000374
Test Epoch31 layer2 Acc 0.7932, AUC 0.8749186992645264, avg_entr 0.043383464217185974, f1 0.7932000160217285
ep31_l2_test_time 1.1411648520002018
Test Epoch31 layer3 Acc 0.7928, AUC 0.8723939061164856, avg_entr 0.04174433648586273, f1 0.7928000092506409
ep31_l3_test_time 1.6235986320002667
Test Epoch31 layer4 Acc 0.7926, AUC 0.8730419874191284, avg_entr 0.03785218298435211, f1 0.7925999760627747
ep31_l4_test_time 2.2626712820001558
gc 0
Train Epoch32 Acc 0.9637 (38548/40000), AUC 0.9930272102355957
ep32_train_time 65.62255618800009
Test Epoch32 layer0 Acc 0.7784, AUC 0.8579810857772827, avg_entr 0.1512618511915207, f1 0.7784000039100647
ep32_l0_test_time 0.6242707270002938
Test Epoch32 layer1 Acc 0.7926, AUC 0.8519073724746704, avg_entr 0.051175523549318314, f1 0.7925999760627747
ep32_l1_test_time 0.815482098999837
Test Epoch32 layer2 Acc 0.7926, AUC 0.8720641136169434, avg_entr 0.04178126901388168, f1 0.7925999760627747
ep32_l2_test_time 1.13899738300006
Test Epoch32 layer3 Acc 0.7932, AUC 0.8708805441856384, avg_entr 0.04007447510957718, f1 0.7932000160217285
ep32_l3_test_time 1.6200783949998367
Test Epoch32 layer4 Acc 0.792, AUC 0.8710919618606567, avg_entr 0.03607814386487007, f1 0.7920000553131104
ep32_l4_test_time 2.2584475720000228
gc 0
Train Epoch33 Acc 0.9642 (38568/40000), AUC 0.9930002689361572
ep33_train_time 65.60787544000004
Test Epoch33 layer0 Acc 0.7796, AUC 0.8581000566482544, avg_entr 0.15024951100349426, f1 0.7796000838279724
ep33_l0_test_time 0.6234958380000535
Test Epoch33 layer1 Acc 0.792, AUC 0.8522531986236572, avg_entr 0.05028838664293289, f1 0.7920000553131104
ep33_l1_test_time 0.8182122130001517
Test Epoch33 layer2 Acc 0.7916, AUC 0.8729330897331238, avg_entr 0.04157666862010956, f1 0.7915999889373779
ep33_l2_test_time 1.139459786000316
Test Epoch33 layer3 Acc 0.7918, AUC 0.8711866140365601, avg_entr 0.039776869118213654, f1 0.7918000221252441
ep33_l3_test_time 1.6250066620000325
Test Epoch33 layer4 Acc 0.7922, AUC 0.8716322183609009, avg_entr 0.035661883652210236, f1 0.7922000288963318
ep33_l4_test_time 2.2611583319999227
gc 0
Train Epoch34 Acc 0.965075 (38603/40000), AUC 0.9937034845352173
ep34_train_time 65.80570290599962
Test Epoch34 layer0 Acc 0.7788, AUC 0.8578670620918274, avg_entr 0.1500115990638733, f1 0.7788000106811523
ep34_l0_test_time 0.633419338999829
Test Epoch34 layer1 Acc 0.79, AUC 0.8536112904548645, avg_entr 0.0508078932762146, f1 0.7900000214576721
ep34_l1_test_time 0.8253846490001706
Test Epoch34 layer2 Acc 0.7922, AUC 0.8729355335235596, avg_entr 0.04183969646692276, f1 0.7922000288963318
ep34_l2_test_time 1.14422807200026
Test Epoch34 layer3 Acc 0.793, AUC 0.8713271617889404, avg_entr 0.040391694754362106, f1 0.7929999828338623
ep34_l3_test_time 1.6221132690002378
Test Epoch34 layer4 Acc 0.7922, AUC 0.870805025100708, avg_entr 0.03657665476202965, f1 0.7922000288963318
ep34_l4_test_time 2.2604585130002306
gc 0
Train Epoch35 Acc 0.966525 (38661/40000), AUC 0.9932416677474976
ep35_train_time 65.7092111359998
Test Epoch35 layer0 Acc 0.779, AUC 0.8576771020889282, avg_entr 0.15027499198913574, f1 0.7789999842643738
ep35_l0_test_time 0.6257046480000099
Test Epoch35 layer1 Acc 0.7904, AUC 0.8497254252433777, avg_entr 0.05036887153983116, f1 0.7904000282287598
ep35_l1_test_time 0.815828263000185
Test Epoch35 layer2 Acc 0.7936, AUC 0.8717683553695679, avg_entr 0.04151545464992523, f1 0.7936000227928162
ep35_l2_test_time 1.1418161149999833
Test Epoch35 layer3 Acc 0.7946, AUC 0.8706424236297607, avg_entr 0.04016649350523949, f1 0.7946000695228577
ep35_l3_test_time 1.6224816799999644
Test Epoch35 layer4 Acc 0.794, AUC 0.8705777525901794, avg_entr 0.03690061718225479, f1 0.7940000295639038
ep35_l4_test_time 2.2595793509999567
gc 0
Train Epoch36 Acc 0.966825 (38673/40000), AUC 0.993840217590332
ep36_train_time 65.59425081900008
Test Epoch36 layer0 Acc 0.7788, AUC 0.8575152158737183, avg_entr 0.1500760316848755, f1 0.7788000106811523
ep36_l0_test_time 0.6254583889999594
Test Epoch36 layer1 Acc 0.7906, AUC 0.8503377437591553, avg_entr 0.050102680921554565, f1 0.7906000018119812
ep36_l1_test_time 0.8172702709998703
Test Epoch36 layer2 Acc 0.7906, AUC 0.8713876008987427, avg_entr 0.041100312024354935, f1 0.7906000018119812
ep36_l2_test_time 1.137750395999774
Test Epoch36 layer3 Acc 0.7916, AUC 0.8699983954429626, avg_entr 0.03932071104645729, f1 0.7915999889373779
ep36_l3_test_time 1.6273683339995841
Test Epoch36 layer4 Acc 0.7902, AUC 0.8702409267425537, avg_entr 0.03541019558906555, f1 0.7901999950408936
ep36_l4_test_time 2.262223479000113
gc 0
Train Epoch37 Acc 0.965825 (38633/40000), AUC 0.9935906529426575
ep37_train_time 65.58143880999978
Test Epoch37 layer0 Acc 0.778, AUC 0.8574613332748413, avg_entr 0.1500844657421112, f1 0.777999997138977
ep37_l0_test_time 0.6237685529999908
Test Epoch37 layer1 Acc 0.791, AUC 0.8496130704879761, avg_entr 0.04973669722676277, f1 0.7910000681877136
ep37_l1_test_time 0.8159248980000484
Test Epoch37 layer2 Acc 0.793, AUC 0.8710473775863647, avg_entr 0.040774647146463394, f1 0.7929999828338623
ep37_l2_test_time 1.1416606519997003
Test Epoch37 layer3 Acc 0.7938, AUC 0.8697828054428101, avg_entr 0.03908061608672142, f1 0.7937999963760376
ep37_l3_test_time 1.6230658049998965
Test Epoch37 layer4 Acc 0.7938, AUC 0.8700369596481323, avg_entr 0.03562270849943161, f1 0.7937999963760376
ep37_l4_test_time 2.257372325999768
gc 0
Train Epoch38 Acc 0.966025 (38641/40000), AUC 0.9932641983032227
ep38_train_time 65.6905082520002
Test Epoch38 layer0 Acc 0.7792, AUC 0.8575100898742676, avg_entr 0.14980050921440125, f1 0.7791999578475952
ep38_l0_test_time 0.6337703369999872
Test Epoch38 layer1 Acc 0.7916, AUC 0.8497962951660156, avg_entr 0.049908533692359924, f1 0.7915999889373779
ep38_l1_test_time 0.8184463349998623
Test Epoch38 layer2 Acc 0.793, AUC 0.8712809681892395, avg_entr 0.040764592587947845, f1 0.7929999828338623
ep38_l2_test_time 1.1383566320000682
Test Epoch38 layer3 Acc 0.793, AUC 0.8701901435852051, avg_entr 0.03907627612352371, f1 0.7929999828338623
ep38_l3_test_time 1.6250745560000723
Test Epoch38 layer4 Acc 0.7916, AUC 0.8700303435325623, avg_entr 0.035344984382390976, f1 0.7915999889373779
ep38_l4_test_time 2.258101185999749
gc 0
Train Epoch39 Acc 0.966075 (38643/40000), AUC 0.9933783411979675
ep39_train_time 65.62944579799978
Test Epoch39 layer0 Acc 0.7792, AUC 0.8575010299682617, avg_entr 0.14920489490032196, f1 0.7791999578475952
ep39_l0_test_time 0.6227995009999177
Test Epoch39 layer1 Acc 0.7902, AUC 0.8507031798362732, avg_entr 0.05006340146064758, f1 0.7901999950408936
ep39_l1_test_time 0.817470602999947
Test Epoch39 layer2 Acc 0.7916, AUC 0.8716690540313721, avg_entr 0.04096595570445061, f1 0.7915999889373779
ep39_l2_test_time 1.1394253849998677
Test Epoch39 layer3 Acc 0.7924, AUC 0.8704268932342529, avg_entr 0.03932153433561325, f1 0.7923999428749084
ep39_l3_test_time 1.6281971490002434
Test Epoch39 layer4 Acc 0.7916, AUC 0.870067834854126, avg_entr 0.03536809980869293, f1 0.7915999889373779
ep39_l4_test_time 2.26547090799977
gc 0
Train Epoch40 Acc 0.966475 (38659/40000), AUC 0.9937606453895569
ep40_train_time 65.68229669599987
Test Epoch40 layer0 Acc 0.7778, AUC 0.8574029207229614, avg_entr 0.14929179847240448, f1 0.7778000235557556
ep40_l0_test_time 0.6252036480000243
Test Epoch40 layer1 Acc 0.792, AUC 0.8496683835983276, avg_entr 0.04986441507935524, f1 0.7920000553131104
ep40_l1_test_time 0.8179903809996176
Test Epoch40 layer2 Acc 0.792, AUC 0.8711007833480835, avg_entr 0.0411064587533474, f1 0.7920000553131104
ep40_l2_test_time 1.1449555340000188
Test Epoch40 layer3 Acc 0.7916, AUC 0.870246171951294, avg_entr 0.0392962284386158, f1 0.7915999889373779
ep40_l3_test_time 1.6261168930000167
Test Epoch40 layer4 Acc 0.7916, AUC 0.8699513673782349, avg_entr 0.035275548696517944, f1 0.7915999889373779
ep40_l4_test_time 2.2598343260001457
gc 0
Train Epoch41 Acc 0.966325 (38653/40000), AUC 0.9938439130783081
ep41_train_time 65.61276507299999
Test Epoch41 layer0 Acc 0.7786, AUC 0.8573215007781982, avg_entr 0.14962226152420044, f1 0.7785999774932861
ep41_l0_test_time 0.624768837999909
Test Epoch41 layer1 Acc 0.7914, AUC 0.8494807481765747, avg_entr 0.049886688590049744, f1 0.7914000153541565
ep41_l1_test_time 0.8198381709999012
Test Epoch41 layer2 Acc 0.792, AUC 0.8709791898727417, avg_entr 0.041117407381534576, f1 0.7920000553131104
ep41_l2_test_time 1.1438738359997842
Test Epoch41 layer3 Acc 0.7924, AUC 0.870134711265564, avg_entr 0.03948696702718735, f1 0.7923999428749084
ep41_l3_test_time 1.6235274390000995
Test Epoch41 layer4 Acc 0.791, AUC 0.8698698282241821, avg_entr 0.03576984629034996, f1 0.7910000681877136
ep41_l4_test_time 2.263288112000282
gc 0
Train Epoch42 Acc 0.9666 (38664/40000), AUC 0.9938693046569824
ep42_train_time 65.61253536200002
Test Epoch42 layer0 Acc 0.779, AUC 0.8573889136314392, avg_entr 0.14906620979309082, f1 0.7789999842643738
ep42_l0_test_time 0.6286733400002049
Test Epoch42 layer1 Acc 0.7922, AUC 0.8500030636787415, avg_entr 0.04988119378685951, f1 0.7922000288963318
ep42_l1_test_time 0.8573236970000835
Test Epoch42 layer2 Acc 0.7926, AUC 0.8714229464530945, avg_entr 0.04119542986154556, f1 0.7925999760627747
ep42_l2_test_time 1.1427846589999717
Test Epoch42 layer3 Acc 0.7926, AUC 0.8701214790344238, avg_entr 0.039543211460113525, f1 0.7925999760627747
ep42_l3_test_time 1.6237953120003112
Test Epoch42 layer4 Acc 0.7922, AUC 0.869807779788971, avg_entr 0.035613950341939926, f1 0.7922000288963318
ep42_l4_test_time 2.260352839999996
gc 0
Train Epoch43 Acc 0.967225 (38689/40000), AUC 0.994091272354126
ep43_train_time 65.68901600399977
Test Epoch43 layer0 Acc 0.7784, AUC 0.8573193550109863, avg_entr 0.1490861177444458, f1 0.7784000039100647
ep43_l0_test_time 0.6350659069998983
Test Epoch43 layer1 Acc 0.7924, AUC 0.8497236967086792, avg_entr 0.049838487058877945, f1 0.7923999428749084
ep43_l1_test_time 0.8178265080000529
Test Epoch43 layer2 Acc 0.7932, AUC 0.8711770176887512, avg_entr 0.040817949920892715, f1 0.7932000160217285
ep43_l2_test_time 1.1560234100002162
Test Epoch43 layer3 Acc 0.7932, AUC 0.8698609471321106, avg_entr 0.03908156231045723, f1 0.7932000160217285
ep43_l3_test_time 1.6297147800000857
Test Epoch43 layer4 Acc 0.793, AUC 0.8696924448013306, avg_entr 0.0350879542529583, f1 0.7929999828338623
ep43_l4_test_time 2.260844299999917
gc 0
Train Epoch44 Acc 0.9675 (38700/40000), AUC 0.9939647912979126
ep44_train_time 65.671034426
Test Epoch44 layer0 Acc 0.7798, AUC 0.8572811484336853, avg_entr 0.1491488516330719, f1 0.7797999978065491
ep44_l0_test_time 0.6241799310000715
Test Epoch44 layer1 Acc 0.7918, AUC 0.8492403030395508, avg_entr 0.04974890127778053, f1 0.7918000221252441
ep44_l1_test_time 0.8189198689997284
Test Epoch44 layer2 Acc 0.7918, AUC 0.8707983493804932, avg_entr 0.0409432128071785, f1 0.7918000221252441
ep44_l2_test_time 1.1406914149997647
Test Epoch44 layer3 Acc 0.792, AUC 0.8695517778396606, avg_entr 0.039316002279520035, f1 0.7920000553131104
ep44_l3_test_time 1.6244673629998942
Test Epoch44 layer4 Acc 0.7914, AUC 0.8692759275436401, avg_entr 0.03557009622454643, f1 0.7914000153541565
ep44_l4_test_time 2.2596327169999313
gc 0
Train Epoch45 Acc 0.966925 (38677/40000), AUC 0.9939359426498413
ep45_train_time 65.62489754800026
Test Epoch45 layer0 Acc 0.779, AUC 0.8572655916213989, avg_entr 0.14883148670196533, f1 0.7789999842643738
ep45_l0_test_time 0.6324911009996868
Test Epoch45 layer1 Acc 0.7924, AUC 0.8490976095199585, avg_entr 0.04966659098863602, f1 0.7923999428749084
ep45_l1_test_time 0.816590000999895
Test Epoch45 layer2 Acc 0.792, AUC 0.8707205057144165, avg_entr 0.040901828557252884, f1 0.7920000553131104
ep45_l2_test_time 1.1425519370000075
Test Epoch45 layer3 Acc 0.7924, AUC 0.8693579435348511, avg_entr 0.0392741821706295, f1 0.7923999428749084
ep45_l3_test_time 1.6238586100002976
Test Epoch45 layer4 Acc 0.7914, AUC 0.8690570592880249, avg_entr 0.03547003120183945, f1 0.7914000153541565
ep45_l4_test_time 2.2592426629998954
gc 0
Train Epoch46 Acc 0.96655 (38662/40000), AUC 0.9938557147979736
ep46_train_time 65.63567345199999
Test Epoch46 layer0 Acc 0.7792, AUC 0.8572509288787842, avg_entr 0.1487383097410202, f1 0.7791999578475952
ep46_l0_test_time 0.6266471549997732
Test Epoch46 layer1 Acc 0.7916, AUC 0.8489960432052612, avg_entr 0.04977336525917053, f1 0.7915999889373779
ep46_l1_test_time 0.8173342320001211
Test Epoch46 layer2 Acc 0.792, AUC 0.8707691431045532, avg_entr 0.041075199842453, f1 0.7920000553131104
ep46_l2_test_time 1.1414541570002257
Test Epoch46 layer3 Acc 0.7932, AUC 0.8694488406181335, avg_entr 0.03929251804947853, f1 0.7932000160217285
ep46_l3_test_time 1.623626535999847
Test Epoch46 layer4 Acc 0.7914, AUC 0.8692647814750671, avg_entr 0.03584715723991394, f1 0.7914000153541565
ep46_l4_test_time 2.262257080999916
gc 0
Train Epoch47 Acc 0.96765 (38706/40000), AUC 0.9937391877174377
ep47_train_time 65.62530013199967
Test Epoch47 layer0 Acc 0.7784, AUC 0.8572244048118591, avg_entr 0.1486280858516693, f1 0.7784000039100647
ep47_l0_test_time 0.6238797930000146
Test Epoch47 layer1 Acc 0.791, AUC 0.8494859933853149, avg_entr 0.04972369596362114, f1 0.7910000681877136
ep47_l1_test_time 0.8347503449999749
Test Epoch47 layer2 Acc 0.7914, AUC 0.8710618019104004, avg_entr 0.0410708449780941, f1 0.7914000153541565
ep47_l2_test_time 1.1386603119999563
Test Epoch47 layer3 Acc 0.792, AUC 0.8696458339691162, avg_entr 0.039361197501420975, f1 0.7920000553131104
ep47_l3_test_time 1.6232012589998703
Test Epoch47 layer4 Acc 0.7912, AUC 0.8693708777427673, avg_entr 0.03548282012343407, f1 0.7911999821662903
ep47_l4_test_time 2.2599183689999336
gc 0
Train Epoch48 Acc 0.96715 (38686/40000), AUC 0.9941549897193909
ep48_train_time 65.64612935500008
Test Epoch48 layer0 Acc 0.7788, AUC 0.8571969270706177, avg_entr 0.1487487256526947, f1 0.7788000106811523
ep48_l0_test_time 0.6245531979998304
Test Epoch48 layer1 Acc 0.7922, AUC 0.8492319583892822, avg_entr 0.04962050914764404, f1 0.7922000288963318
ep48_l1_test_time 0.818204532999971
Test Epoch48 layer2 Acc 0.7924, AUC 0.8708270788192749, avg_entr 0.04094129800796509, f1 0.7923999428749084
ep48_l2_test_time 1.1504025730000649
Test Epoch48 layer3 Acc 0.793, AUC 0.8696898221969604, avg_entr 0.03913992643356323, f1 0.7929999828338623
ep48_l3_test_time 1.630689509000149
Test Epoch48 layer4 Acc 0.7922, AUC 0.8695763349533081, avg_entr 0.035396549850702286, f1 0.7922000288963318
ep48_l4_test_time 2.265698107999924
gc 0
Train Epoch49 Acc 0.96725 (38690/40000), AUC 0.9940344095230103
ep49_train_time 65.72453668800017
Test Epoch49 layer0 Acc 0.7788, AUC 0.8571934103965759, avg_entr 0.1488114893436432, f1 0.7788000106811523
ep49_l0_test_time 0.6267872420003187
Test Epoch49 layer1 Acc 0.7916, AUC 0.8490216732025146, avg_entr 0.04954318702220917, f1 0.7915999889373779
ep49_l1_test_time 0.8166587430000618
Test Epoch49 layer2 Acc 0.7922, AUC 0.8707985877990723, avg_entr 0.040728434920310974, f1 0.7922000288963318
ep49_l2_test_time 1.1407846730003257
Test Epoch49 layer3 Acc 0.7938, AUC 0.8696936368942261, avg_entr 0.03892868012189865, f1 0.7937999963760376
ep49_l3_test_time 1.6228007949998755
Test Epoch49 layer4 Acc 0.7924, AUC 0.8697192072868347, avg_entr 0.035366158932447433, f1 0.7923999428749084
ep49_l4_test_time 2.2646420750002108
Best AUC tensor(0.8148) 10 2
train_as_loss [[9.14460197e+01 6.03560980e+01 5.23104311e+01 5.05211428e+01
  4.99114017e+01 4.96369563e+01 4.94909683e+01 4.94043185e+01
  4.93487920e+01 4.93111204e+01 4.92844282e+01 4.92648485e+01
  4.92500827e+01 4.92386855e+01 4.92297175e+01 4.92241255e+01
  4.92208015e+01 4.92176985e+01 4.92148122e+01 4.92127670e+01
  4.92114301e+01 4.92100971e+01 4.92087740e+01 4.92077852e+01
  4.92071118e+01 4.92064167e+01 4.92057050e+01 4.92051593e+01
  4.92047752e+01 4.92043759e+01 4.92039563e+01 4.92036282e+01
  4.92034001e+01 4.92031534e+01 4.92028949e+01 4.92026903e+01
  4.92025460e+01 4.92023921e+01 4.92022263e+01 4.92020905e+01
  4.92020058e+01 4.92019004e+01 4.92017910e+01 4.92017192e+01
  4.92016440e+01 4.92015878e+01 4.92015128e+01 4.92014522e+01
  4.92014255e+01 4.92013786e+01]
 [2.33992358e+00 8.55555288e-04 2.21903363e-05 4.81633966e-06
  1.72347786e-06 7.84830598e-07 4.05998744e-07 2.28037748e-07
  1.38181855e-07 8.95880506e-08 5.75993656e-08 3.93882213e-08
  2.82859237e-08 2.01335217e-08 1.48773977e-08 1.15506768e-08
  9.64894882e-09 8.21125404e-09 7.28749313e-09 6.20944829e-09
  5.70926345e-09 5.18260066e-09 4.94270851e-09 4.42484252e-09
  4.20752196e-09 3.92261629e-09 3.74057273e-09 3.55669687e-09
  3.39931073e-09 3.29880996e-09 3.20144570e-09 3.04554404e-09
  2.99633602e-09 2.93009577e-09 2.81709156e-09 2.76604316e-09
  2.70851385e-09 2.65015319e-09 2.59473756e-09 2.57786390e-09
  2.50820078e-09 2.44742829e-09 2.36240104e-09 2.50928514e-09
  2.44945025e-09 2.32562957e-09 2.24678111e-09 2.56580611e-09
  2.50344388e-09 2.35384924e-09]
 [2.53977654e+00 3.22109487e-03 3.45152291e-05 6.53700586e-06
  2.12389573e-06 9.07306528e-07 4.50671789e-07 2.45953580e-07
  1.44234140e-07 9.71438144e-08 5.96053274e-08 4.14604969e-08
  3.06766254e-08 2.17959373e-08 1.67699005e-08 4.97223505e-09
  2.93451737e-09 9.13280792e-09 7.63392057e-09 2.22103534e-09
  1.47610928e-09 5.52348670e-09 5.09428694e-09 1.34446946e-09
  1.10766727e-09 4.04702065e-09 3.60336906e-09 9.65054700e-10
  8.32236687e-10 3.31369065e-09 3.07345333e-09 7.65314136e-10
  7.30193970e-10 2.88266812e-09 2.66486423e-09 6.76000654e-10
  6.59894329e-10 2.56412739e-09 2.45020927e-09 6.35095524e-10
  6.00621191e-10 2.31681225e-09 2.24550722e-09 6.26352748e-10
  5.72114932e-10 2.13384837e-09 2.09681305e-09 6.86267276e-10
  6.04986934e-10 2.09457416e-09]
 [2.65531420e+00 5.50167150e-03 2.08632943e-05 4.76307414e-06
  1.78219019e-06 8.22409733e-07 4.50511943e-07 2.55882246e-07
  1.53549650e-07 1.18736310e-07 7.03128045e-08 5.22463962e-08
  4.12257912e-08 2.91292623e-08 2.51847029e-08 1.05075194e-08
  5.06409097e-09 1.16557461e-08 9.87302608e-09 3.93919918e-09
  1.87012423e-09 6.59369774e-09 6.51425229e-09 1.97529197e-09
  1.40602020e-09 4.67757818e-09 4.18137056e-09 1.24930642e-09
  9.64948740e-10 3.76887816e-09 3.52769561e-09 9.50005978e-10
  8.43883231e-10 3.27478223e-09 3.00791182e-09 8.28604681e-10
  7.50428678e-10 2.88882942e-09 2.81182387e-09 7.76951364e-10
  7.05851100e-10 2.62261370e-09 2.60920225e-09 7.64292049e-10
  6.59681691e-10 2.37299379e-09 2.42073235e-09 8.34510918e-10
  6.84045371e-10 2.27399320e-09]
 [2.63280731e+00 8.79724176e-03 1.83473975e-05 4.62860576e-06
  1.79684112e-06 8.49820216e-07 5.17666772e-07 2.82018667e-07
  1.67407571e-07 1.59731501e-07 9.21768239e-08 7.18539044e-08
  6.73366797e-08 4.81133514e-08 4.73097354e-08 2.87918436e-08
  1.24689851e-08 1.48887165e-08 1.28702836e-08 7.74142621e-09
  2.16677895e-09 6.25360141e-09 7.29101335e-09 3.00020038e-09
  1.50341401e-09 3.87669300e-09 3.31625847e-09 1.37945790e-09
  7.63962703e-10 2.84895448e-09 2.65141763e-09 8.94729519e-10
  6.39767142e-10 2.39250343e-09 2.13713896e-09 6.82109560e-10
  5.52309163e-10 2.06195657e-09 2.00279853e-09 5.97848636e-10
  4.94629712e-10 1.84751257e-09 1.81239293e-09 6.11707879e-10
  5.02531267e-10 1.78224550e-09 1.80537572e-09 7.06625322e-10
  5.72884718e-10 1.83533874e-09]]
train_ae_loss [[4.06110844 2.89638029 3.87185933 4.38590198 4.65798569 4.75293608
  4.87319852 4.91525643 4.98811148 4.91916132 4.87234862 4.79024801
  4.69849577 4.58168584 4.56518663 4.11620142 3.94057376 3.77848788
  3.70657365 3.44981654 3.37814158 3.28746936 3.25551625 3.11840983
  3.08498899 3.0574683  3.02172251 2.9667309  2.95964909 2.92460602
  2.92261458 2.89031817 2.8932384  2.87750824 2.86696974 2.85715544
  2.84779755 2.85604282 2.84981969 2.84117991 2.84384857 2.84639759
  2.83445314 2.84339593 2.83214178 2.83808234 2.83693724 2.83433157
  2.83422155 2.82578634]
 [3.793934   2.80412072 3.75508563 4.08996664 4.25110852 4.21876908
  4.2665159  4.23662329 4.28322569 4.10464959 4.02436466 3.84853381
  3.74937831 3.55186399 3.46569077 2.86398086 2.6481464  2.46169631
  2.35991388 2.06271433 1.95785186 1.86941454 1.8132728  1.64980747
  1.60810507 1.58021368 1.53328042 1.50044651 1.46389881 1.42025998
  1.43504777 1.38182772 1.39036329 1.37540343 1.36523107 1.34728224
  1.33282908 1.35382221 1.34862891 1.33012322 1.33426554 1.34761244
  1.31526395 1.33715319 1.31588096 1.31939958 1.32667402 1.31745052
  1.31078206 1.31670665]
 [4.05413468 2.74550453 3.88248087 4.09290235 4.22756151 4.11248008
  4.12641577 4.05844071 4.10324377 3.89082739 3.803768   3.61917901
  3.51684176 3.3238707  3.24782333 2.75783482 2.51050525 2.24228276
  2.14676619 1.89134739 1.78484345 1.6767027  1.61884136 1.47701294
  1.42786872 1.39413604 1.35433528 1.33013033 1.2893167  1.24021569
  1.25702713 1.21314449 1.21578046 1.20142849 1.19082616 1.16704575
  1.157531   1.1766948  1.1716583  1.15625026 1.15980268 1.17635495
  1.14454074 1.16066277 1.13855476 1.1450915  1.15125221 1.13902226
  1.13346294 1.13382716]
 [4.41470298 2.56132044 3.82239137 4.01185444 4.04797063 3.83287496
  3.81285959 3.71367187 3.76251932 3.54621429 3.4664137  3.28929881
  3.20229578 3.02123073 2.95500397 2.55119065 2.28863713 2.0095806
  1.92030048 1.69558273 1.59729092 1.48938735 1.43613479 1.31218233
  1.26671469 1.23092747 1.19831705 1.174929   1.13739274 1.09139554
  1.10644732 1.06555097 1.06976858 1.05473369 1.04550485 1.02355676
  1.01579373 1.03052211 1.02698809 1.01325112 1.01832506 1.03023693
  1.00235479 1.01715103 0.99754413 1.00126118 1.00800888 0.99985366
  0.99225962 0.9912254 ]
 [5.05028227 2.46660151 3.78623355 4.29712546 4.08945411 3.73887669
  3.67768332 3.54195862 3.5877207  3.35091097 3.27143978 3.10053826
  3.01998645 2.84220593 2.78162192 2.43797426 2.15750851 1.86562307
  1.77818682 1.57368866 1.47884868 1.36326298 1.31102854 1.19724843
  1.15485419 1.11360331 1.09029358 1.06357604 1.02622429 0.98206768
  0.99422602 0.95678912 0.9640501  0.94431071 0.93761518 0.91843426
  0.91092198 0.922827   0.91942452 0.90784296 0.91153412 0.92171826
  0.89537565 0.90907057 0.89189635 0.89275233 0.90194193 0.89881106
  0.88720922 0.88587315]]
valid_acc (5, 50)
valid_AUC (5, 50)
train_acc (50,)
total_train+valid_time 3611.6348113410004
Start Testing
Load ckpt at ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt
gc 9
Test layer0 Acc 0.789, AUC 0.874197781085968, avg_entr 0.22433942556381226, f1 0.7889999747276306
l0_test_time 0.6294808019997618
gc 0
Test layer1 Acc 0.8048, AUC 0.8930367231369019, avg_entr 0.1506379097700119, f1 0.8047999739646912
l1_test_time 0.8306406460001199
gc 0
Test layer2 Acc 0.8116, AUC 0.8954347372055054, avg_entr 0.13348497450351715, f1 0.8116000294685364
l2_test_time 1.1600599530002
gc 0
Test layer3 Acc 0.812, AUC 0.8963812589645386, avg_entr 0.13008272647857666, f1 0.8119999766349792
l3_test_time 1.6364273340000182
gc 0
Test layer4 Acc 0.8116, AUC 0.896751880645752, avg_entr 0.1266266405582428, f1 0.8116000294685364
l4_test_time 2.26437186999965
gc 0
Test threshold 0.1 Acc 0.8112, AUC 0.8904129862785339, avg_entr 0.18557707965373993, f1 0.8112000226974487
t0.1_test_time 1.3149712879999242
gc 0
Test threshold 0.2 Acc 0.8102, AUC 0.8883838653564453, avg_entr 0.19739362597465515, f1 0.8101999759674072
t0.2_test_time 1.1783597039998313
gc 0
Test threshold 0.3 Acc 0.809, AUC 0.8857026100158691, avg_entr 0.2092386782169342, f1 0.8090000748634338
t0.3_test_time 1.0784762060002322
gc 0
Test threshold 0.4 Acc 0.8092, AUC 0.8853651285171509, avg_entr 0.22035159170627594, f1 0.8092000484466553
t0.4_test_time 1.0035680619998857
gc 0
Test threshold 0.5 Acc 0.808, AUC 0.8838366270065308, avg_entr 0.23222212493419647, f1 0.8080000281333923
t0.5_test_time 0.9475128039998708
gc 0
Test threshold 0.6 Acc 0.8072, AUC 0.8831892013549805, avg_entr 0.2458101063966751, f1 0.807200014591217
t0.6_test_time 0.8947605090002071
gc 0
Test threshold 0.7 Acc 0.806, AUC 0.8823224306106567, avg_entr 0.25876572728157043, f1 0.8059999942779541
t0.7_test_time 0.8456649220001964
gc 0
Test threshold 0.8 Acc 0.803, AUC 0.8807528018951416, avg_entr 0.2742742598056793, f1 0.8029999732971191
t0.8_test_time 0.7844132420000278
gc 0
Test threshold 0.9 Acc 0.8008, AUC 0.8780390620231628, avg_entr 0.2924351990222931, f1 0.8007999658584595
t0.9_test_time 0.7409293809996598
