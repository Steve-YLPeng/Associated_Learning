total count words 222751
vocab size 30000
train size 40000, valid size 5000, test size 5000
found 27937 words in glove
model: TransformerALsideText(
  (layers): ModuleList(
    (0): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=2, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=2, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
  (emb_layers): ModuleList(
    (0): Embedding(30000, 300)
    (1): Embedding(30000, 300)
    (2): Embedding(30000, 300)
    (3): Embedding(30000, 300)
    (4): Embedding(30000, 300)
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.multi_headed_attention.linears.0.weight 90000
layers.0.enc.f.multi_headed_attention.linears.0.bias 300
layers.0.enc.f.multi_headed_attention.linears.1.weight 90000
layers.0.enc.f.multi_headed_attention.linears.1.bias 300
layers.0.enc.f.multi_headed_attention.linears.2.weight 90000
layers.0.enc.f.multi_headed_attention.linears.2.bias 300
layers.0.enc.f.multi_headed_attention.linears.3.weight 90000
layers.0.enc.f.multi_headed_attention.linears.3.bias 300
layers.0.enc.f.feed_forward.w_1.weight 90000
layers.0.enc.f.feed_forward.w_1.bias 300
layers.0.enc.f.feed_forward.w_2.weight 90000
layers.0.enc.f.feed_forward.w_2.bias 300
layers.0.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.0.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.0.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.0.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.0.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.0.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.0.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.0.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.0.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.0.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.0.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.0.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.0.enc.f.encoder.norm.a 300
layers.0.enc.f.encoder.norm.b 300
layers.0.ae.g.0.weight 256
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 256
layers.0.ae.h.0.bias 2
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
emb_layers.0.weight 9000000
emb_layers.1.weight 9000000
emb_layers.2.weight 9000000
emb_layers.3.weight 9000000
emb_layers.4.weight 9000000
Total Trainable Params: 50758378
init_time 21.212369448
Start Training
gc 0
Train Epoch0 Acc 0.506975 (20279/40000), AUC 0.5078561902046204
ep0_train_time 65.633362266
Test Epoch0 layer0 Acc 0.5398, AUC 0.6047959327697754, avg_entr 0.6953128576278687, f1 0.5397999882698059
ep0_l0_test_time 0.6131224289999864
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 0
Test Epoch0 layer1 Acc 0.5336, AUC 0.5463118553161621, avg_entr 0.6903497576713562, f1 0.5335999727249146
ep0_l1_test_time 0.8124867399999971
Test Epoch0 layer2 Acc 0.5266, AUC 0.5284052491188049, avg_entr 0.693962574005127, f1 0.5266000032424927
ep0_l2_test_time 1.128845035999987
Test Epoch0 layer3 Acc 0.5, AUC 0.5343056917190552, avg_entr 0.6897245645523071, f1 0.5
ep0_l3_test_time 1.6098752480000087
Test Epoch0 layer4 Acc 0.5, AUC 0.5048513412475586, avg_entr 0.6956223845481873, f1 0.5
ep0_l4_test_time 2.2439188369999954
gc 0
Train Epoch1 Acc 0.50755 (20302/40000), AUC 0.5098053812980652
ep1_train_time 65.44818654899998
Test Epoch1 layer0 Acc 0.615, AUC 0.6835379600524902, avg_entr 0.6263331770896912, f1 0.6150000095367432
ep1_l0_test_time 0.6142143289999922
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 1
Test Epoch1 layer1 Acc 0.5912, AUC 0.6908448934555054, avg_entr 0.6453146934509277, f1 0.5911999940872192
ep1_l1_test_time 0.8210893039999974
Test Epoch1 layer2 Acc 0.5008, AUC 0.6550469398498535, avg_entr 0.6827874183654785, f1 0.5008000135421753
ep1_l2_test_time 1.129912754000003
Test Epoch1 layer3 Acc 0.5, AUC 0.611237645149231, avg_entr 0.6814925670623779, f1 0.5
ep1_l3_test_time 1.6129333549999956
Test Epoch1 layer4 Acc 0.5, AUC 0.552443265914917, avg_entr 0.6893378496170044, f1 0.5
ep1_l4_test_time 2.245038743000009
gc 0
Train Epoch2 Acc 0.527625 (21105/40000), AUC 0.5403031706809998
ep2_train_time 65.4777709
Test Epoch2 layer0 Acc 0.6926, AUC 0.7655704617500305, avg_entr 0.5131801962852478, f1 0.6926000118255615
ep2_l0_test_time 0.6149975890000121
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 2
Test Epoch2 layer1 Acc 0.687, AUC 0.7744511365890503, avg_entr 0.4971967339515686, f1 0.6869999766349792
ep2_l1_test_time 0.8137253879999946
Test Epoch2 layer2 Acc 0.6502, AUC 0.7739655375480652, avg_entr 0.4867021441459656, f1 0.6502000093460083
ep2_l2_test_time 1.12954593500001
Test Epoch2 layer3 Acc 0.5522, AUC 0.7753742337226868, avg_entr 0.4914643168449402, f1 0.5522000193595886
ep2_l3_test_time 1.6110219690000065
Test Epoch2 layer4 Acc 0.5672, AUC 0.7621990442276001, avg_entr 0.6705771684646606, f1 0.5672000050544739
ep2_l4_test_time 2.2435316260000207
gc 0
Train Epoch3 Acc 0.628375 (25135/40000), AUC 0.6737842559814453
ep3_train_time 65.53843455400002
Test Epoch3 layer0 Acc 0.714, AUC 0.7921693325042725, avg_entr 0.4169248938560486, f1 0.7139999866485596
ep3_l0_test_time 0.614671817000044
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 3
Test Epoch3 layer1 Acc 0.7326, AUC 0.8092029690742493, avg_entr 0.39776769280433655, f1 0.7325999140739441
ep3_l1_test_time 0.8243300469999895
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 3
Test Epoch3 layer2 Acc 0.7338, AUC 0.8109749555587769, avg_entr 0.41805240511894226, f1 0.7338000535964966
ep3_l2_test_time 1.1420589790000122
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 3
Test Epoch3 layer3 Acc 0.7292, AUC 0.8107048273086548, avg_entr 0.472978413105011, f1 0.7291999459266663
ep3_l3_test_time 1.6371457289999967
Test Epoch3 layer4 Acc 0.706, AUC 0.8109403848648071, avg_entr 0.5415137410163879, f1 0.7059999704360962
ep3_l4_test_time 2.246186755999986
gc 0
Train Epoch4 Acc 0.715425 (28617/40000), AUC 0.7903674840927124
ep4_train_time 65.46056911699998
Test Epoch4 layer0 Acc 0.7332, AUC 0.8235114812850952, avg_entr 0.3682428002357483, f1 0.7332000136375427
ep4_l0_test_time 0.6162535629999866
Test Epoch4 layer1 Acc 0.7508, AUC 0.8397517204284668, avg_entr 0.32989051938056946, f1 0.7508000135421753
ep4_l1_test_time 0.8069320680000374
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 4
Test Epoch4 layer2 Acc 0.7544, AUC 0.8409208059310913, avg_entr 0.31868594884872437, f1 0.7544000148773193
ep4_l2_test_time 1.1379187790000174
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 4
Test Epoch4 layer3 Acc 0.7526, AUC 0.8405768275260925, avg_entr 0.3233690559864044, f1 0.7526000142097473
ep4_l3_test_time 1.6188597600000207
Test Epoch4 layer4 Acc 0.755, AUC 0.8409738540649414, avg_entr 0.3335389792919159, f1 0.7549999356269836
ep4_l4_test_time 2.2567853500000297
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 4
gc 0
Train Epoch5 Acc 0.731 (29240/40000), AUC 0.8063288927078247
ep5_train_time 65.59714701400003
Test Epoch5 layer0 Acc 0.7636, AUC 0.8466933965682983, avg_entr 0.3316580057144165, f1 0.7635999917984009
ep5_l0_test_time 0.6211214630000086
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 5
Test Epoch5 layer1 Acc 0.7788, AUC 0.8621838688850403, avg_entr 0.30924731492996216, f1 0.7788000106811523
ep5_l1_test_time 0.8196973970000272
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 5
Test Epoch5 layer2 Acc 0.7756, AUC 0.8635284900665283, avg_entr 0.3008975684642792, f1 0.775600016117096
ep5_l2_test_time 1.1349162760000127
Test Epoch5 layer3 Acc 0.7706, AUC 0.8634573221206665, avg_entr 0.3008164167404175, f1 0.7706000208854675
ep5_l3_test_time 1.6119989950000218
Test Epoch5 layer4 Acc 0.7602, AUC 0.8638133406639099, avg_entr 0.3047759234905243, f1 0.7602000832557678
ep5_l4_test_time 2.247839311000007
gc 0
Train Epoch6 Acc 0.778875 (31155/40000), AUC 0.8611205816268921
ep6_train_time 65.57033109600007
Test Epoch6 layer0 Acc 0.7702, AUC 0.8581334352493286, avg_entr 0.30475854873657227, f1 0.7702000141143799
ep6_l0_test_time 0.6351264699999319
Test Epoch6 layer1 Acc 0.7856, AUC 0.8738094568252563, avg_entr 0.278781920671463, f1 0.7856000065803528
ep6_l1_test_time 0.8081733060000715
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 6
Test Epoch6 layer2 Acc 0.7872, AUC 0.875625491142273, avg_entr 0.2575051784515381, f1 0.7871999740600586
ep6_l2_test_time 1.1429482339999595
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 6
Test Epoch6 layer3 Acc 0.7854, AUC 0.8755272626876831, avg_entr 0.2545698285102844, f1 0.7853999733924866
ep6_l3_test_time 1.6204833309999458
Test Epoch6 layer4 Acc 0.7848, AUC 0.875795841217041, avg_entr 0.26433563232421875, f1 0.7847999930381775
ep6_l4_test_time 2.247585228000048
gc 0
Train Epoch7 Acc 0.784525 (31381/40000), AUC 0.8614730834960938
ep7_train_time 65.540879711
Test Epoch7 layer0 Acc 0.7656, AUC 0.8653277158737183, avg_entr 0.2750917971134186, f1 0.7655999660491943
ep7_l0_test_time 0.6156302269999969
Test Epoch7 layer1 Acc 0.7588, AUC 0.881275475025177, avg_entr 0.24880121648311615, f1 0.7588000297546387
ep7_l1_test_time 0.8082774499999914
Test Epoch7 layer2 Acc 0.7556, AUC 0.8824752569198608, avg_entr 0.21029430627822876, f1 0.7555999755859375
ep7_l2_test_time 1.1311385899999777
Test Epoch7 layer3 Acc 0.7438, AUC 0.8822352290153503, avg_entr 0.20105017721652985, f1 0.7437999248504639
ep7_l3_test_time 1.6194558700000243
Test Epoch7 layer4 Acc 0.736, AUC 0.8825737833976746, avg_entr 0.21160776913166046, f1 0.7360000014305115
ep7_l4_test_time 2.2545910930000446
gc 0
Train Epoch8 Acc 0.80845 (32338/40000), AUC 0.889555811882019
ep8_train_time 65.66428522399997
Test Epoch8 layer0 Acc 0.7884, AUC 0.8697853088378906, avg_entr 0.26312127709388733, f1 0.7884000539779663
ep8_l0_test_time 0.6303295250000929
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 8
Test Epoch8 layer1 Acc 0.8062, AUC 0.8862498998641968, avg_entr 0.23183611035346985, f1 0.8062000274658203
ep8_l1_test_time 0.82274747200006
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 8
Test Epoch8 layer2 Acc 0.8092, AUC 0.8884929418563843, avg_entr 0.16598236560821533, f1 0.8092000484466553
ep8_l2_test_time 1.1340271430000257
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 8
Test Epoch8 layer3 Acc 0.809, AUC 0.8887957334518433, avg_entr 0.15801262855529785, f1 0.8090000748634338
ep8_l3_test_time 1.618125430999953
Test Epoch8 layer4 Acc 0.8096, AUC 0.8890485763549805, avg_entr 0.16189785301685333, f1 0.8095999956130981
ep8_l4_test_time 2.247056245000067
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 8
gc 0
Train Epoch9 Acc 0.823525 (32941/40000), AUC 0.9024702310562134
ep9_train_time 65.47972839400006
Test Epoch9 layer0 Acc 0.7928, AUC 0.8725124597549438, avg_entr 0.2663862705230713, f1 0.7928000092506409
ep9_l0_test_time 0.6559302800000069
Test Epoch9 layer1 Acc 0.8092, AUC 0.8874639272689819, avg_entr 0.2524755001068115, f1 0.8092000484466553
ep9_l1_test_time 0.826070159999972
Test Epoch9 layer2 Acc 0.8122, AUC 0.8897510766983032, avg_entr 0.19032827019691467, f1 0.8122000098228455
ep9_l2_test_time 1.1369533099999671
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 9
Test Epoch9 layer3 Acc 0.8116, AUC 0.8899344205856323, avg_entr 0.20090383291244507, f1 0.8116000294685364
ep9_l3_test_time 1.6194242720000602
Test Epoch9 layer4 Acc 0.8114, AUC 0.890078067779541, avg_entr 0.21380263566970825, f1 0.8113999962806702
ep9_l4_test_time 2.249002146000066
gc 0
Train Epoch10 Acc 0.83585 (33434/40000), AUC 0.9133956432342529
ep10_train_time 65.694399997
Test Epoch10 layer0 Acc 0.7952, AUC 0.8759353160858154, avg_entr 0.23083510994911194, f1 0.7952000498771667
ep10_l0_test_time 0.6121703889999708
Test Epoch10 layer1 Acc 0.8126, AUC 0.892936110496521, avg_entr 0.19756342470645905, f1 0.8126000165939331
ep10_l1_test_time 0.8092104399999016
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 10
Test Epoch10 layer2 Acc 0.8144, AUC 0.8945738077163696, avg_entr 0.1331770271062851, f1 0.8144000172615051
ep10_l2_test_time 1.1433520540000472
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 10
Test Epoch10 layer3 Acc 0.813, AUC 0.8947392106056213, avg_entr 0.12865521013736725, f1 0.8130000233650208
ep10_l3_test_time 1.6214173270000174
Test Epoch10 layer4 Acc 0.8098, AUC 0.8949333429336548, avg_entr 0.13212847709655762, f1 0.8098000288009644
ep10_l4_test_time 2.246515024999894
gc 0
Train Epoch11 Acc 0.8444 (33776/40000), AUC 0.9180389046669006
ep11_train_time 65.61601801500001
Test Epoch11 layer0 Acc 0.7902, AUC 0.8777037858963013, avg_entr 0.22896474599838257, f1 0.7901999950408936
ep11_l0_test_time 0.6161980040000117
Test Epoch11 layer1 Acc 0.8088, AUC 0.8945854902267456, avg_entr 0.19671688973903656, f1 0.8087999820709229
ep11_l1_test_time 0.813242665999951
Test Epoch11 layer2 Acc 0.8092, AUC 0.8967360258102417, avg_entr 0.13562704622745514, f1 0.8092000484466553
ep11_l2_test_time 1.1310734749999938
Test Epoch11 layer3 Acc 0.8084, AUC 0.8973318338394165, avg_entr 0.1373390108346939, f1 0.8083999752998352
ep11_l3_test_time 1.6123606909999353
Test Epoch11 layer4 Acc 0.8086, AUC 0.8976532220840454, avg_entr 0.1423504650592804, f1 0.8086000084877014
ep11_l4_test_time 2.246605813999963
gc 0
Train Epoch12 Acc 0.85105 (34042/40000), AUC 0.9251810312271118
ep12_train_time 65.48862619500005
Test Epoch12 layer0 Acc 0.7936, AUC 0.8778094053268433, avg_entr 0.22052296996116638, f1 0.7936000227928162
ep12_l0_test_time 0.6170421680000118
Test Epoch12 layer1 Acc 0.8034, AUC 0.8935903310775757, avg_entr 0.17832374572753906, f1 0.8033999800682068
ep12_l1_test_time 0.8176783359999718
Test Epoch12 layer2 Acc 0.8012, AUC 0.8961243629455566, avg_entr 0.1203756332397461, f1 0.8011999726295471
ep12_l2_test_time 1.1328118059999497
Test Epoch12 layer3 Acc 0.802, AUC 0.8966201543807983, avg_entr 0.1170191615819931, f1 0.8019999861717224
ep12_l3_test_time 1.6164981030000263
Test Epoch12 layer4 Acc 0.7982, AUC 0.8970098495483398, avg_entr 0.11728157848119736, f1 0.7982000708580017
ep12_l4_test_time 2.247687042999928
gc 0
Train Epoch13 Acc 0.862975 (34519/40000), AUC 0.9360332489013672
ep13_train_time 65.63064558000008
Test Epoch13 layer0 Acc 0.7954, AUC 0.879454493522644, avg_entr 0.2081177681684494, f1 0.795400083065033
ep13_l0_test_time 0.615745392000008
Test Epoch13 layer1 Acc 0.8132, AUC 0.8936559557914734, avg_entr 0.14981777966022491, f1 0.8131999969482422
ep13_l1_test_time 0.818743257000051
Test Epoch13 layer2 Acc 0.8158, AUC 0.8969625234603882, avg_entr 0.1018279492855072, f1 0.8158000111579895
ep13_l2_test_time 1.1385694849998345
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 13
Test Epoch13 layer3 Acc 0.8148, AUC 0.8974226117134094, avg_entr 0.0988173708319664, f1 0.8148000240325928
ep13_l3_test_time 1.6367625280001903
Test Epoch13 layer4 Acc 0.816, AUC 0.897650420665741, avg_entr 0.09774796664714813, f1 0.8160000443458557
ep13_l4_test_time 2.2476119289999588
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 13
gc 0
Train Epoch14 Acc 0.870475 (34819/40000), AUC 0.93970787525177
ep14_train_time 65.55862013900014
Test Epoch14 layer0 Acc 0.7946, AUC 0.8783842325210571, avg_entr 0.20809607207775116, f1 0.7946000695228577
ep14_l0_test_time 0.6134233400000539
Test Epoch14 layer1 Acc 0.8076, AUC 0.893046498298645, avg_entr 0.1307668387889862, f1 0.8076000213623047
ep14_l1_test_time 0.8066804929999307
Test Epoch14 layer2 Acc 0.8094, AUC 0.8958165645599365, avg_entr 0.1087309941649437, f1 0.8094000220298767
ep14_l2_test_time 1.1300019870000142
Test Epoch14 layer3 Acc 0.811, AUC 0.8959832191467285, avg_entr 0.1077158972620964, f1 0.8109999895095825
ep14_l3_test_time 1.6128234190000512
Test Epoch14 layer4 Acc 0.8116, AUC 0.896167516708374, avg_entr 0.1068233773112297, f1 0.8116000294685364
ep14_l4_test_time 2.2462626550000095
gc 0
Train Epoch15 Acc 0.881475 (35259/40000), AUC 0.9472854137420654
ep15_train_time 65.47924676799994
Test Epoch15 layer0 Acc 0.7782, AUC 0.8763422966003418, avg_entr 0.18611542880535126, f1 0.7781999707221985
ep15_l0_test_time 0.6176823430000695
Test Epoch15 layer1 Acc 0.7858, AUC 0.8907402753829956, avg_entr 0.11312225461006165, f1 0.7857999801635742
ep15_l1_test_time 0.8155111510000097
Test Epoch15 layer2 Acc 0.7886, AUC 0.8928694725036621, avg_entr 0.09505066275596619, f1 0.7886000275611877
ep15_l2_test_time 1.13405003999992
Test Epoch15 layer3 Acc 0.788, AUC 0.8917320370674133, avg_entr 0.09407775104045868, f1 0.7879999876022339
ep15_l3_test_time 1.6158009189998666
Test Epoch15 layer4 Acc 0.7836, AUC 0.8918613195419312, avg_entr 0.09396755695343018, f1 0.7835999727249146
ep15_l4_test_time 2.2469618729999183
gc 0
Train Epoch16 Acc 0.894825 (35793/40000), AUC 0.9574075937271118
ep16_train_time 65.5335323920001
Test Epoch16 layer0 Acc 0.7902, AUC 0.875690221786499, avg_entr 0.18820321559906006, f1 0.7901999950408936
ep16_l0_test_time 0.6143160960000387
Test Epoch16 layer1 Acc 0.804, AUC 0.890094518661499, avg_entr 0.09112688153982162, f1 0.8040000200271606
ep16_l1_test_time 0.8186433690000285
Test Epoch16 layer2 Acc 0.8076, AUC 0.8932570219039917, avg_entr 0.08320634067058563, f1 0.8076000213623047
ep16_l2_test_time 1.1404724179999448
Test Epoch16 layer3 Acc 0.81, AUC 0.8939993381500244, avg_entr 0.08155932277441025, f1 0.809999942779541
ep16_l3_test_time 1.6486384560000715
Test Epoch16 layer4 Acc 0.8094, AUC 0.8941753506660461, avg_entr 0.08246677368879318, f1 0.8094000220298767
ep16_l4_test_time 2.2480900069999734
gc 0
Train Epoch17 Acc 0.90535 (36214/40000), AUC 0.9656367301940918
ep17_train_time 65.55437709800003
Test Epoch17 layer0 Acc 0.777, AUC 0.8760648965835571, avg_entr 0.1816246658563614, f1 0.7770000100135803
ep17_l0_test_time 0.617255840000098
Test Epoch17 layer1 Acc 0.7922, AUC 0.8896627426147461, avg_entr 0.08773768693208694, f1 0.7922000288963318
ep17_l1_test_time 0.8078801990000102
Test Epoch17 layer2 Acc 0.793, AUC 0.8949795961380005, avg_entr 0.08009848743677139, f1 0.7929999828338623
ep17_l2_test_time 1.1291172490000463
Test Epoch17 layer3 Acc 0.7902, AUC 0.896016001701355, avg_entr 0.0797596350312233, f1 0.7901999950408936
ep17_l3_test_time 1.6134638169999107
Test Epoch17 layer4 Acc 0.7902, AUC 0.8962980508804321, avg_entr 0.07982387393712997, f1 0.7901999950408936
ep17_l4_test_time 2.2477901150000434
gc 0
Train Epoch18 Acc 0.914375 (36575/40000), AUC 0.9692069888114929
ep18_train_time 65.48080737800001
Test Epoch18 layer0 Acc 0.788, AUC 0.8739394545555115, avg_entr 0.17309825122356415, f1 0.7879999876022339
ep18_l0_test_time 0.61363332000019
Test Epoch18 layer1 Acc 0.7958, AUC 0.8810110092163086, avg_entr 0.0738348737359047, f1 0.7958000302314758
ep18_l1_test_time 0.8092557930001476
Test Epoch18 layer2 Acc 0.7954, AUC 0.8878631591796875, avg_entr 0.06630171090364456, f1 0.795400083065033
ep18_l2_test_time 1.1297946009999578
Test Epoch18 layer3 Acc 0.7948, AUC 0.889073371887207, avg_entr 0.06477809697389603, f1 0.7947999835014343
ep18_l3_test_time 1.6131229069999335
Test Epoch18 layer4 Acc 0.792, AUC 0.8895666003227234, avg_entr 0.06445138156414032, f1 0.7920000553131104
ep18_l4_test_time 2.248116692999929
gc 0
Train Epoch19 Acc 0.92495 (36998/40000), AUC 0.9753956198692322
ep19_train_time 65.56175344000007
Test Epoch19 layer0 Acc 0.784, AUC 0.872154712677002, avg_entr 0.17024001479148865, f1 0.7839999794960022
ep19_l0_test_time 0.6143529660000695
Test Epoch19 layer1 Acc 0.8028, AUC 0.8814923167228699, avg_entr 0.07009878009557724, f1 0.8027999997138977
ep19_l1_test_time 0.8073684010000761
Test Epoch19 layer2 Acc 0.803, AUC 0.8881439566612244, avg_entr 0.06305551528930664, f1 0.8029999732971191
ep19_l2_test_time 1.1309979180000482
Test Epoch19 layer3 Acc 0.8044, AUC 0.8886560201644897, avg_entr 0.062232255935668945, f1 0.8044000267982483
ep19_l3_test_time 1.6118575990001318
Test Epoch19 layer4 Acc 0.8044, AUC 0.8892418146133423, avg_entr 0.06224735081195831, f1 0.8044000267982483
ep19_l4_test_time 2.2466570669998873
gc 0
Train Epoch20 Acc 0.9329 (37316/40000), AUC 0.9792566299438477
ep20_train_time 65.46879117200001
Test Epoch20 layer0 Acc 0.782, AUC 0.8698689937591553, avg_entr 0.16977056860923767, f1 0.7820000052452087
ep20_l0_test_time 0.6128861119998419
Test Epoch20 layer1 Acc 0.7932, AUC 0.8773380517959595, avg_entr 0.06792647391557693, f1 0.7932000160217285
ep20_l1_test_time 0.8080845639999552
Test Epoch20 layer2 Acc 0.796, AUC 0.8844335079193115, avg_entr 0.06087497994303703, f1 0.796000063419342
ep20_l2_test_time 1.128425365000112
Test Epoch20 layer3 Acc 0.798, AUC 0.8848940134048462, avg_entr 0.05893741548061371, f1 0.7979999780654907
ep20_l3_test_time 1.6122215939999478
Test Epoch20 layer4 Acc 0.8002, AUC 0.885554313659668, avg_entr 0.0592682808637619, f1 0.8001999855041504
ep20_l4_test_time 2.2452603000001545
gc 0
Train Epoch21 Acc 0.9368 (37472/40000), AUC 0.9818388223648071
ep21_train_time 65.54883598700007
Test Epoch21 layer0 Acc 0.7782, AUC 0.8679267764091492, avg_entr 0.1672763079404831, f1 0.7781999707221985
ep21_l0_test_time 0.6139844859999357
Test Epoch21 layer1 Acc 0.794, AUC 0.8758074045181274, avg_entr 0.06365273147821426, f1 0.7940000295639038
ep21_l1_test_time 0.8057518220000475
Test Epoch21 layer2 Acc 0.7958, AUC 0.884858250617981, avg_entr 0.05584177374839783, f1 0.7958000302314758
ep21_l2_test_time 1.1297303549999924
Test Epoch21 layer3 Acc 0.7972, AUC 0.8857364058494568, avg_entr 0.05509575083851814, f1 0.7971999645233154
ep21_l3_test_time 1.61942679699996
Test Epoch21 layer4 Acc 0.7974, AUC 0.8867145776748657, avg_entr 0.05528479069471359, f1 0.7973999977111816
ep21_l4_test_time 2.2554116359999625
gc 0
Train Epoch22 Acc 0.942125 (37685/40000), AUC 0.9835770130157471
ep22_train_time 65.51596013500011
Test Epoch22 layer0 Acc 0.7792, AUC 0.8652513027191162, avg_entr 0.16445700824260712, f1 0.7791999578475952
ep22_l0_test_time 0.6127760980000403
Test Epoch22 layer1 Acc 0.7926, AUC 0.873431921005249, avg_entr 0.06275270134210587, f1 0.7925999760627747
ep22_l1_test_time 0.8155613750000157
Test Epoch22 layer2 Acc 0.7952, AUC 0.8811166286468506, avg_entr 0.05555570125579834, f1 0.7952000498771667
ep22_l2_test_time 1.1271590339999875
Test Epoch22 layer3 Acc 0.7958, AUC 0.8818914294242859, avg_entr 0.05401290953159332, f1 0.7958000302314758
ep22_l3_test_time 1.6131439340001634
Test Epoch22 layer4 Acc 0.7948, AUC 0.8825756311416626, avg_entr 0.053792938590049744, f1 0.7947999835014343
ep22_l4_test_time 2.2476481060000424
gc 0
Train Epoch23 Acc 0.9444 (37776/40000), AUC 0.9847196340560913
ep23_train_time 65.49331207499995
Test Epoch23 layer0 Acc 0.7738, AUC 0.8651404976844788, avg_entr 0.16022555530071259, f1 0.7738000154495239
ep23_l0_test_time 0.6133099260000563
Test Epoch23 layer1 Acc 0.7808, AUC 0.8692644834518433, avg_entr 0.054098132997751236, f1 0.7808000445365906
ep23_l1_test_time 0.8054603609998594
Test Epoch23 layer2 Acc 0.7824, AUC 0.8793282508850098, avg_entr 0.04866129904985428, f1 0.7824000120162964
ep23_l2_test_time 1.1309944299998733
Test Epoch23 layer3 Acc 0.7816, AUC 0.880149781703949, avg_entr 0.048214469105005264, f1 0.7815999984741211
ep23_l3_test_time 1.618443968000065
Test Epoch23 layer4 Acc 0.7808, AUC 0.8801296949386597, avg_entr 0.047811359167099, f1 0.7808000445365906
ep23_l4_test_time 2.250140078000186
gc 0
Train Epoch24 Acc 0.947225 (37889/40000), AUC 0.9862399101257324
ep24_train_time 65.47180710399994
Test Epoch24 layer0 Acc 0.7748, AUC 0.8642967939376831, avg_entr 0.15664127469062805, f1 0.7748000025749207
ep24_l0_test_time 0.6155331550000938
Test Epoch24 layer1 Acc 0.7916, AUC 0.8731676340103149, avg_entr 0.057521238923072815, f1 0.7915999889373779
ep24_l1_test_time 0.807707358000016
Test Epoch24 layer2 Acc 0.7942, AUC 0.8814692497253418, avg_entr 0.05304332450032234, f1 0.7942000031471252
ep24_l2_test_time 1.128410242999962
Test Epoch24 layer3 Acc 0.796, AUC 0.8824911117553711, avg_entr 0.05232040584087372, f1 0.796000063419342
ep24_l3_test_time 1.6112274729998717
Test Epoch24 layer4 Acc 0.7956, AUC 0.8831398487091064, avg_entr 0.0521344356238842, f1 0.7955999970436096
ep24_l4_test_time 2.2460338949999823
gc 0
Train Epoch25 Acc 0.950175 (38007/40000), AUC 0.9879200458526611
ep25_train_time 65.47326483500001
Test Epoch25 layer0 Acc 0.776, AUC 0.8645473718643188, avg_entr 0.15759782493114471, f1 0.7759999632835388
ep25_l0_test_time 0.613544015999878
Test Epoch25 layer1 Acc 0.7928, AUC 0.8681548833847046, avg_entr 0.05298284441232681, f1 0.7928000092506409
ep25_l1_test_time 0.8066585540000233
Test Epoch25 layer2 Acc 0.796, AUC 0.8793810606002808, avg_entr 0.045133888721466064, f1 0.796000063419342
ep25_l2_test_time 1.1301120059999903
Test Epoch25 layer3 Acc 0.796, AUC 0.881710410118103, avg_entr 0.04248230159282684, f1 0.796000063419342
ep25_l3_test_time 1.6119658459999755
Test Epoch25 layer4 Acc 0.7958, AUC 0.8824435472488403, avg_entr 0.042263444513082504, f1 0.7958000302314758
ep25_l4_test_time 2.2454752319999898
gc 0
Train Epoch26 Acc 0.9525 (38100/40000), AUC 0.9889543056488037
ep26_train_time 65.46913010899993
Test Epoch26 layer0 Acc 0.7772, AUC 0.8645696043968201, avg_entr 0.15791010856628418, f1 0.777199923992157
ep26_l0_test_time 0.615102405000016
Test Epoch26 layer1 Acc 0.789, AUC 0.8696926832199097, avg_entr 0.055176008492708206, f1 0.7889999747276306
ep26_l1_test_time 0.8080421300001035
Test Epoch26 layer2 Acc 0.7932, AUC 0.878646969795227, avg_entr 0.04867985099554062, f1 0.7932000160217285
ep26_l2_test_time 1.129020103999892
Test Epoch26 layer3 Acc 0.7946, AUC 0.8811031579971313, avg_entr 0.04731941968202591, f1 0.7946000695228577
ep26_l3_test_time 1.6100856839998414
Test Epoch26 layer4 Acc 0.7946, AUC 0.8816978931427002, avg_entr 0.04688557609915733, f1 0.7946000695228577
ep26_l4_test_time 2.2470525820001512
gc 0
Train Epoch27 Acc 0.9565 (38260/40000), AUC 0.9894288778305054
ep27_train_time 65.48433785599991
Test Epoch27 layer0 Acc 0.774, AUC 0.8632454872131348, avg_entr 0.15622511506080627, f1 0.7739999890327454
ep27_l0_test_time 0.6125642819999939
Test Epoch27 layer1 Acc 0.7912, AUC 0.8677471876144409, avg_entr 0.05375376716256142, f1 0.7911999821662903
ep27_l1_test_time 0.8053792030000295
Test Epoch27 layer2 Acc 0.7956, AUC 0.8784873485565186, avg_entr 0.0468156673014164, f1 0.7955999970436096
ep27_l2_test_time 1.1296227660000113
Test Epoch27 layer3 Acc 0.7954, AUC 0.8807317614555359, avg_entr 0.0449003204703331, f1 0.795400083065033
ep27_l3_test_time 1.6156243780001205
Test Epoch27 layer4 Acc 0.7956, AUC 0.8815838098526001, avg_entr 0.04459867253899574, f1 0.7955999970436096
ep27_l4_test_time 2.2533015240001077
gc 0
Train Epoch28 Acc 0.955775 (38231/40000), AUC 0.9903154373168945
ep28_train_time 65.60729137700014
Test Epoch28 layer0 Acc 0.7748, AUC 0.8618547320365906, avg_entr 0.15798956155776978, f1 0.7748000025749207
ep28_l0_test_time 0.6179950519999693
Test Epoch28 layer1 Acc 0.7862, AUC 0.8655662536621094, avg_entr 0.05192708596587181, f1 0.7861999869346619
ep28_l1_test_time 0.8109459559996139
Test Epoch28 layer2 Acc 0.7904, AUC 0.8755942583084106, avg_entr 0.04674607887864113, f1 0.7904000282287598
ep28_l2_test_time 1.1297019550002005
Test Epoch28 layer3 Acc 0.7906, AUC 0.8782252073287964, avg_entr 0.045211732387542725, f1 0.7906000018119812
ep28_l3_test_time 1.6103727710001294
Test Epoch28 layer4 Acc 0.7904, AUC 0.8792585134506226, avg_entr 0.04498279467225075, f1 0.7904000282287598
ep28_l4_test_time 2.2494660890001796
gc 0
Train Epoch29 Acc 0.958825 (38353/40000), AUC 0.9910011291503906
ep29_train_time 65.46236068300004
Test Epoch29 layer0 Acc 0.7714, AUC 0.8622676134109497, avg_entr 0.154849112033844, f1 0.771399974822998
ep29_l0_test_time 0.6131747310000719
Test Epoch29 layer1 Acc 0.7844, AUC 0.8653611540794373, avg_entr 0.0500452034175396, f1 0.7843999266624451
ep29_l1_test_time 0.8084065060002104
Test Epoch29 layer2 Acc 0.7906, AUC 0.8753611445426941, avg_entr 0.04472324252128601, f1 0.7906000018119812
ep29_l2_test_time 1.1279181979998611
Test Epoch29 layer3 Acc 0.7914, AUC 0.8780950307846069, avg_entr 0.04379084333777428, f1 0.7914000153541565
ep29_l3_test_time 1.612961248999909
Test Epoch29 layer4 Acc 0.7906, AUC 0.8789088726043701, avg_entr 0.043632104992866516, f1 0.7906000018119812
ep29_l4_test_time 2.2461829079998097
gc 0
Train Epoch30 Acc 0.95815 (38326/40000), AUC 0.9909058809280396
ep30_train_time 65.60111709500006
Test Epoch30 layer0 Acc 0.7744, AUC 0.861987829208374, avg_entr 0.15562023222446442, f1 0.7743999361991882
ep30_l0_test_time 0.6238811049997821
Test Epoch30 layer1 Acc 0.7896, AUC 0.8642536401748657, avg_entr 0.04969712346792221, f1 0.7896000146865845
ep30_l1_test_time 0.8066148460002296
Test Epoch30 layer2 Acc 0.7928, AUC 0.874350905418396, avg_entr 0.04460684210062027, f1 0.7928000092506409
ep30_l2_test_time 1.131534190000366
Test Epoch30 layer3 Acc 0.7934, AUC 0.878101110458374, avg_entr 0.042863328009843826, f1 0.79339998960495
ep30_l3_test_time 1.61107789100015
Test Epoch30 layer4 Acc 0.7934, AUC 0.8793810606002808, avg_entr 0.042644187808036804, f1 0.79339998960495
ep30_l4_test_time 2.252910877999966
gc 0
Train Epoch31 Acc 0.96 (38400/40000), AUC 0.9913547039031982
ep31_train_time 65.57799495800009
Test Epoch31 layer0 Acc 0.7734, AUC 0.8618255853652954, avg_entr 0.15641894936561584, f1 0.7734000086784363
ep31_l0_test_time 0.623493714000233
Test Epoch31 layer1 Acc 0.7904, AUC 0.8650652170181274, avg_entr 0.05105537548661232, f1 0.7904000282287598
ep31_l1_test_time 0.8149392479999733
Test Epoch31 layer2 Acc 0.7908, AUC 0.8748974204063416, avg_entr 0.045226193964481354, f1 0.7907999753952026
ep31_l2_test_time 1.1276272730001438
Test Epoch31 layer3 Acc 0.792, AUC 0.8781776428222656, avg_entr 0.043536487966775894, f1 0.7920000553131104
ep31_l3_test_time 1.6130687350000699
Test Epoch31 layer4 Acc 0.7912, AUC 0.8794471621513367, avg_entr 0.04332356154918671, f1 0.7911999821662903
ep31_l4_test_time 2.248473546999776
gc 0
Train Epoch32 Acc 0.96065 (38426/40000), AUC 0.9916316270828247
ep32_train_time 65.54925461799985
Test Epoch32 layer0 Acc 0.7758, AUC 0.8609822988510132, avg_entr 0.15590758621692657, f1 0.7758000493049622
ep32_l0_test_time 0.6137725799999316
Test Epoch32 layer1 Acc 0.7884, AUC 0.8624952435493469, avg_entr 0.04961394891142845, f1 0.7884000539779663
ep32_l1_test_time 0.8058525510000436
Test Epoch32 layer2 Acc 0.7926, AUC 0.8728939294815063, avg_entr 0.04383261129260063, f1 0.7925999760627747
ep32_l2_test_time 1.1323614950001684
Test Epoch32 layer3 Acc 0.7928, AUC 0.8772021532058716, avg_entr 0.04249553382396698, f1 0.7928000092506409
ep32_l3_test_time 1.6129246360001162
Test Epoch32 layer4 Acc 0.7924, AUC 0.8786622881889343, avg_entr 0.04241127520799637, f1 0.7923999428749084
ep32_l4_test_time 2.245709365000039
gc 0
Train Epoch33 Acc 0.96155 (38462/40000), AUC 0.9919732809066772
ep33_train_time 65.5410056439996
Test Epoch33 layer0 Acc 0.7716, AUC 0.8614637851715088, avg_entr 0.15409955382347107, f1 0.771600067615509
ep33_l0_test_time 0.618734322000364
Test Epoch33 layer1 Acc 0.7864, AUC 0.8633444309234619, avg_entr 0.04887625202536583, f1 0.7864000201225281
ep33_l1_test_time 0.8098621170001934
Test Epoch33 layer2 Acc 0.7892, AUC 0.8730906844139099, avg_entr 0.04374029114842415, f1 0.7892000079154968
ep33_l2_test_time 1.1282411560000583
Test Epoch33 layer3 Acc 0.7896, AUC 0.877475380897522, avg_entr 0.0415322370827198, f1 0.7896000146865845
ep33_l3_test_time 1.613352549999945
Test Epoch33 layer4 Acc 0.789, AUC 0.878799557685852, avg_entr 0.04121723771095276, f1 0.7889999747276306
ep33_l4_test_time 2.245849240000098
gc 0
Train Epoch34 Acc 0.96095 (38438/40000), AUC 0.9920145273208618
ep34_train_time 65.47497245499972
Test Epoch34 layer0 Acc 0.7714, AUC 0.8612868785858154, avg_entr 0.15269683301448822, f1 0.771399974822998
ep34_l0_test_time 0.6160290839998197
Test Epoch34 layer1 Acc 0.7852, AUC 0.8627701997756958, avg_entr 0.04663456976413727, f1 0.7851999998092651
ep34_l1_test_time 0.8090542550003192
Test Epoch34 layer2 Acc 0.7884, AUC 0.8719923496246338, avg_entr 0.04083159193396568, f1 0.7884000539779663
ep34_l2_test_time 1.1311293230000956
Test Epoch34 layer3 Acc 0.7898, AUC 0.8768208622932434, avg_entr 0.03872700035572052, f1 0.7897999882698059
ep34_l3_test_time 1.6117741409998416
Test Epoch34 layer4 Acc 0.7904, AUC 0.878105878829956, avg_entr 0.03858661651611328, f1 0.7904000282287598
ep34_l4_test_time 2.2487017509997713
gc 0
Train Epoch35 Acc 0.96225 (38490/40000), AUC 0.9920737743377686
ep35_train_time 65.50252021400001
Test Epoch35 layer0 Acc 0.7716, AUC 0.8612246513366699, avg_entr 0.15382598340511322, f1 0.771600067615509
ep35_l0_test_time 0.6162179929997365
Test Epoch35 layer1 Acc 0.788, AUC 0.8618863821029663, avg_entr 0.04707442224025726, f1 0.7879999876022339
ep35_l1_test_time 0.8069602900000064
Test Epoch35 layer2 Acc 0.7906, AUC 0.8717097043991089, avg_entr 0.04197218269109726, f1 0.7906000018119812
ep35_l2_test_time 1.1268940850000035
Test Epoch35 layer3 Acc 0.79, AUC 0.8765633702278137, avg_entr 0.03976773843169212, f1 0.7900000214576721
ep35_l3_test_time 1.6110154160001002
Test Epoch35 layer4 Acc 0.7906, AUC 0.8779868483543396, avg_entr 0.03953887149691582, f1 0.7906000018119812
ep35_l4_test_time 2.245728837000115
gc 0
Train Epoch36 Acc 0.962 (38480/40000), AUC 0.9925086498260498
ep36_train_time 65.48149504399998
Test Epoch36 layer0 Acc 0.7718, AUC 0.8610228300094604, avg_entr 0.15297600626945496, f1 0.7717999815940857
ep36_l0_test_time 0.6136671190001834
Test Epoch36 layer1 Acc 0.7856, AUC 0.8619948029518127, avg_entr 0.04699176549911499, f1 0.7856000065803528
ep36_l1_test_time 0.8060167720000209
Test Epoch36 layer2 Acc 0.7898, AUC 0.8721057772636414, avg_entr 0.041550204157829285, f1 0.7897999882698059
ep36_l2_test_time 1.142028651000146
Test Epoch36 layer3 Acc 0.7888, AUC 0.8764801025390625, avg_entr 0.03987276181578636, f1 0.7888000011444092
ep36_l3_test_time 1.6195097740001074
Test Epoch36 layer4 Acc 0.7898, AUC 0.8778167963027954, avg_entr 0.0396801196038723, f1 0.7897999882698059
ep36_l4_test_time 2.246778082000219
gc 0
Train Epoch37 Acc 0.96245 (38498/40000), AUC 0.9926320910453796
ep37_train_time 65.52233876399987
Test Epoch37 layer0 Acc 0.7708, AUC 0.8610035181045532, avg_entr 0.15224924683570862, f1 0.7707999348640442
ep37_l0_test_time 0.6154968329997246
Test Epoch37 layer1 Acc 0.7844, AUC 0.8627350330352783, avg_entr 0.04715803638100624, f1 0.7843999266624451
ep37_l1_test_time 0.8074839030000476
Test Epoch37 layer2 Acc 0.7896, AUC 0.872348964214325, avg_entr 0.042184196412563324, f1 0.7896000146865845
ep37_l2_test_time 1.1314413879999847
Test Epoch37 layer3 Acc 0.7908, AUC 0.8770079612731934, avg_entr 0.039917461574077606, f1 0.7907999753952026
ep37_l3_test_time 1.6200383519999377
Test Epoch37 layer4 Acc 0.791, AUC 0.8783383369445801, avg_entr 0.03973020613193512, f1 0.7910000681877136
ep37_l4_test_time 2.2522611120002693
gc 0
Train Epoch38 Acc 0.96245 (38498/40000), AUC 0.9925936460494995
ep38_train_time 65.49879417200009
Test Epoch38 layer0 Acc 0.7732, AUC 0.860812783241272, avg_entr 0.1531313806772232, f1 0.7731999754905701
ep38_l0_test_time 0.6197768070001075
Test Epoch38 layer1 Acc 0.787, AUC 0.8632144927978516, avg_entr 0.04811984300613403, f1 0.7870000600814819
ep38_l1_test_time 0.8153219450000506
Test Epoch38 layer2 Acc 0.7896, AUC 0.8729987144470215, avg_entr 0.04298966005444527, f1 0.7896000146865845
ep38_l2_test_time 1.1374841599999854
Test Epoch38 layer3 Acc 0.7906, AUC 0.8770841360092163, avg_entr 0.04076381400227547, f1 0.7906000018119812
ep38_l3_test_time 1.6206266929998492
Test Epoch38 layer4 Acc 0.7898, AUC 0.8782732486724854, avg_entr 0.04074789583683014, f1 0.7897999882698059
ep38_l4_test_time 2.2519336069999554
gc 0
Train Epoch39 Acc 0.962725 (38509/40000), AUC 0.9926083087921143
ep39_train_time 65.55372017799982
Test Epoch39 layer0 Acc 0.7738, AUC 0.8609350919723511, avg_entr 0.1525679975748062, f1 0.7738000154495239
ep39_l0_test_time 0.6170531470002061
Test Epoch39 layer1 Acc 0.787, AUC 0.862345814704895, avg_entr 0.04779190197587013, f1 0.7870000600814819
ep39_l1_test_time 0.8079904080000233
Test Epoch39 layer2 Acc 0.7892, AUC 0.8726738691329956, avg_entr 0.04278137534856796, f1 0.7892000079154968
ep39_l2_test_time 1.13131636099979
Test Epoch39 layer3 Acc 0.7892, AUC 0.8773435950279236, avg_entr 0.04038414731621742, f1 0.7892000079154968
ep39_l3_test_time 1.6119941019996986
Test Epoch39 layer4 Acc 0.7894, AUC 0.8786953091621399, avg_entr 0.04033513367176056, f1 0.7893999814987183
ep39_l4_test_time 2.2501058749999174
gc 0
Train Epoch40 Acc 0.9632 (38528/40000), AUC 0.9925699234008789
ep40_train_time 65.5920091500002
Test Epoch40 layer0 Acc 0.771, AUC 0.860798180103302, avg_entr 0.15150487422943115, f1 0.7710000276565552
ep40_l0_test_time 0.6194767570000295
Test Epoch40 layer1 Acc 0.786, AUC 0.8623019456863403, avg_entr 0.04630345478653908, f1 0.7860000133514404
ep40_l1_test_time 0.8099360530000013
Test Epoch40 layer2 Acc 0.7904, AUC 0.8723026514053345, avg_entr 0.04081588611006737, f1 0.7904000282287598
ep40_l2_test_time 1.128456002999883
Test Epoch40 layer3 Acc 0.7922, AUC 0.8770101070404053, avg_entr 0.03878050670027733, f1 0.7922000288963318
ep40_l3_test_time 1.612365279000187
Test Epoch40 layer4 Acc 0.7926, AUC 0.8781588673591614, avg_entr 0.03855183348059654, f1 0.7925999760627747
ep40_l4_test_time 2.2461669070003154
gc 0
Train Epoch41 Acc 0.963075 (38523/40000), AUC 0.9928945302963257
ep41_train_time 65.51579177399981
Test Epoch41 layer0 Acc 0.7714, AUC 0.8607650995254517, avg_entr 0.15178406238555908, f1 0.771399974822998
ep41_l0_test_time 0.6137728730000163
Test Epoch41 layer1 Acc 0.7864, AUC 0.8624774217605591, avg_entr 0.046169381588697433, f1 0.7864000201225281
ep41_l1_test_time 0.8078259929998239
Test Epoch41 layer2 Acc 0.7902, AUC 0.8726198673248291, avg_entr 0.040954966098070145, f1 0.7901999950408936
ep41_l2_test_time 1.1300048119996973
Test Epoch41 layer3 Acc 0.792, AUC 0.8770462274551392, avg_entr 0.038821399211883545, f1 0.7920000553131104
ep41_l3_test_time 1.6183495720001702
Test Epoch41 layer4 Acc 0.7914, AUC 0.8781508207321167, avg_entr 0.03861403837800026, f1 0.7914000153541565
ep41_l4_test_time 2.2518129290001525
gc 0
Train Epoch42 Acc 0.9634 (38536/40000), AUC 0.9931308031082153
ep42_train_time 65.53436096699988
Test Epoch42 layer0 Acc 0.7734, AUC 0.8607933521270752, avg_entr 0.15195125341415405, f1 0.7734000086784363
ep42_l0_test_time 0.6135265749999235
Test Epoch42 layer1 Acc 0.7864, AUC 0.861102819442749, avg_entr 0.046065784990787506, f1 0.7864000201225281
ep42_l1_test_time 0.8114181490000192
Test Epoch42 layer2 Acc 0.7894, AUC 0.8718037605285645, avg_entr 0.04165484756231308, f1 0.7893999814987183
ep42_l2_test_time 1.1336305959998754
Test Epoch42 layer3 Acc 0.7896, AUC 0.8769243955612183, avg_entr 0.03936201333999634, f1 0.7896000146865845
ep42_l3_test_time 1.6170681570001761
Test Epoch42 layer4 Acc 0.7898, AUC 0.8782699704170227, avg_entr 0.03920033946633339, f1 0.7897999882698059
ep42_l4_test_time 2.25657591900017
gc 0
Train Epoch43 Acc 0.964 (38560/40000), AUC 0.9928200840950012
ep43_train_time 65.51424465800028
Test Epoch43 layer0 Acc 0.7712, AUC 0.8607493042945862, avg_entr 0.15162323415279388, f1 0.7712000012397766
ep43_l0_test_time 0.6121762719999424
Test Epoch43 layer1 Acc 0.786, AUC 0.8614129424095154, avg_entr 0.04577799513936043, f1 0.7860000133514404
ep43_l1_test_time 0.8054405509997196
Test Epoch43 layer2 Acc 0.7894, AUC 0.871673047542572, avg_entr 0.04094165191054344, f1 0.7893999814987183
ep43_l2_test_time 1.1359333699997478
Test Epoch43 layer3 Acc 0.7916, AUC 0.876763105392456, avg_entr 0.03858513385057449, f1 0.7915999889373779
ep43_l3_test_time 1.6113224489999993
Test Epoch43 layer4 Acc 0.7916, AUC 0.8780175447463989, avg_entr 0.03845930099487305, f1 0.7915999889373779
ep43_l4_test_time 2.2455663869995988
gc 0
Train Epoch44 Acc 0.963725 (38549/40000), AUC 0.9928550720214844
ep44_train_time 65.49918183699992
Test Epoch44 layer0 Acc 0.7722, AUC 0.860761821269989, avg_entr 0.1518506407737732, f1 0.7721999287605286
ep44_l0_test_time 0.6126578840003276
Test Epoch44 layer1 Acc 0.7858, AUC 0.8615144491195679, avg_entr 0.04573913663625717, f1 0.7857999801635742
ep44_l1_test_time 0.8088661969995883
Test Epoch44 layer2 Acc 0.7898, AUC 0.872097373008728, avg_entr 0.04063785821199417, f1 0.7897999882698059
ep44_l2_test_time 1.1371278559995517
Test Epoch44 layer3 Acc 0.7912, AUC 0.876799464225769, avg_entr 0.03828524425625801, f1 0.7911999821662903
ep44_l3_test_time 1.6188117579999926
Test Epoch44 layer4 Acc 0.791, AUC 0.8779839277267456, avg_entr 0.03807956725358963, f1 0.7910000681877136
ep44_l4_test_time 2.251193632000195
gc 0
Train Epoch45 Acc 0.9647 (38588/40000), AUC 0.992972731590271
ep45_train_time 65.70243625500007
Test Epoch45 layer0 Acc 0.7734, AUC 0.8607845306396484, avg_entr 0.15166732668876648, f1 0.7734000086784363
ep45_l0_test_time 0.6143583369998851
Test Epoch45 layer1 Acc 0.7864, AUC 0.8609869480133057, avg_entr 0.04563188552856445, f1 0.7864000201225281
ep45_l1_test_time 0.8095761969998421
Test Epoch45 layer2 Acc 0.7906, AUC 0.8709108829498291, avg_entr 0.04121537506580353, f1 0.7906000018119812
ep45_l2_test_time 1.129126043999804
Test Epoch45 layer3 Acc 0.7902, AUC 0.8765628337860107, avg_entr 0.03878942131996155, f1 0.7901999950408936
ep45_l3_test_time 1.6129501710001932
Test Epoch45 layer4 Acc 0.7896, AUC 0.8779373168945312, avg_entr 0.038697049021720886, f1 0.7896000146865845
ep45_l4_test_time 2.2518833829999494
gc 0
Train Epoch46 Acc 0.964075 (38563/40000), AUC 0.9930307865142822
ep46_train_time 65.51234618699982
Test Epoch46 layer0 Acc 0.7744, AUC 0.860690712928772, avg_entr 0.151960089802742, f1 0.7743999361991882
ep46_l0_test_time 0.6252064390000669
Test Epoch46 layer1 Acc 0.7872, AUC 0.8610601425170898, avg_entr 0.04591449722647667, f1 0.7871999740600586
ep46_l1_test_time 0.8078526879999117
Test Epoch46 layer2 Acc 0.7902, AUC 0.8711867332458496, avg_entr 0.041407350450754166, f1 0.7901999950408936
ep46_l2_test_time 1.1391420969998762
Test Epoch46 layer3 Acc 0.7898, AUC 0.876634955406189, avg_entr 0.039027146995067596, f1 0.7897999882698059
ep46_l3_test_time 1.6110146920000261
Test Epoch46 layer4 Acc 0.79, AUC 0.8779464364051819, avg_entr 0.038931164890527725, f1 0.7900000214576721
ep46_l4_test_time 2.2500129499999275
gc 0
Train Epoch47 Acc 0.964375 (38575/40000), AUC 0.9931521415710449
ep47_train_time 65.55122017099984
Test Epoch47 layer0 Acc 0.7738, AUC 0.8607093095779419, avg_entr 0.15172640979290009, f1 0.7738000154495239
ep47_l0_test_time 0.6155331269997077
Test Epoch47 layer1 Acc 0.787, AUC 0.8609524965286255, avg_entr 0.045681800693273544, f1 0.7870000600814819
ep47_l1_test_time 0.8112698380000438
Test Epoch47 layer2 Acc 0.791, AUC 0.8710272312164307, avg_entr 0.04113652929663658, f1 0.7910000681877136
ep47_l2_test_time 1.1321278069999607
Test Epoch47 layer3 Acc 0.7902, AUC 0.8766487836837769, avg_entr 0.038767650723457336, f1 0.7901999950408936
ep47_l3_test_time 1.6116646979999132
Test Epoch47 layer4 Acc 0.7908, AUC 0.8779962062835693, avg_entr 0.03865125775337219, f1 0.7907999753952026
ep47_l4_test_time 2.2574146819997623
gc 0
Train Epoch48 Acc 0.963875 (38555/40000), AUC 0.9932106733322144
ep48_train_time 65.56767292699988
Test Epoch48 layer0 Acc 0.7734, AUC 0.8606497049331665, avg_entr 0.15143509209156036, f1 0.7734000086784363
ep48_l0_test_time 0.6142020339998453
Test Epoch48 layer1 Acc 0.787, AUC 0.8609156608581543, avg_entr 0.045383021235466, f1 0.7870000600814819
ep48_l1_test_time 0.8068855639999128
Test Epoch48 layer2 Acc 0.7896, AUC 0.8710968494415283, avg_entr 0.04076356813311577, f1 0.7896000146865845
ep48_l2_test_time 1.130299286999616
Test Epoch48 layer3 Acc 0.791, AUC 0.8766070604324341, avg_entr 0.038346067070961, f1 0.7910000681877136
ep48_l3_test_time 1.6131046030000107
Test Epoch48 layer4 Acc 0.7912, AUC 0.8778850436210632, avg_entr 0.0382252000272274, f1 0.7911999821662903
ep48_l4_test_time 2.2483867400001145
gc 0
Train Epoch49 Acc 0.96405 (38562/40000), AUC 0.9931356906890869
ep49_train_time 65.51888782600008
Test Epoch49 layer0 Acc 0.7726, AUC 0.8606702089309692, avg_entr 0.15126259624958038, f1 0.772599995136261
ep49_l0_test_time 0.6151957709998896
Test Epoch49 layer1 Acc 0.7864, AUC 0.8609832525253296, avg_entr 0.045235149562358856, f1 0.7864000201225281
ep49_l1_test_time 0.8106028959996365
Test Epoch49 layer2 Acc 0.7902, AUC 0.8711689114570618, avg_entr 0.04073728248476982, f1 0.7901999950408936
ep49_l2_test_time 1.1288030659998185
Test Epoch49 layer3 Acc 0.791, AUC 0.876605749130249, avg_entr 0.03838703781366348, f1 0.7910000681877136
ep49_l3_test_time 1.6135799180001413
Test Epoch49 layer4 Acc 0.7912, AUC 0.8778793811798096, avg_entr 0.038240689784288406, f1 0.7911999821662903
ep49_l4_test_time 2.247614567000255
Best AUC tensor(0.8160) 13 4
train_as_loss [[8.57333310e+01 5.89676986e+01 5.21540742e+01 5.05023311e+01
  4.99131650e+01 4.96419674e+01 4.94958713e+01 4.94084998e+01
  4.93522267e+01 4.93139202e+01 4.92867142e+01 4.92667292e+01
  4.92516393e+01 4.92399828e+01 4.92308071e+01 4.92234670e+01
  4.92175123e+01 4.92126244e+01 4.92094765e+01 4.92075644e+01
  4.92057519e+01 4.92040391e+01 4.92028098e+01 4.92019984e+01
  4.92011845e+01 4.92003700e+01 4.91997572e+01 4.91993372e+01
  4.91989014e+01 4.91984542e+01 4.91981071e+01 4.91978684e+01
  4.91976134e+01 4.91973461e+01 4.91971379e+01 4.91969908e+01
  4.91968339e+01 4.91966679e+01 4.91965349e+01 4.91964444e+01
  4.91963437e+01 4.91962366e+01 4.91961562e+01 4.91960900e+01
  4.91960316e+01 4.91959569e+01 4.91959029e+01 4.91958680e+01
  4.91958208e+01 4.91957688e+01]
 [2.47964982e+00 3.71834824e-04 2.17910328e-05 5.20822787e-06
  1.98914232e-06 9.64338511e-07 4.89320355e-07 3.05190302e-07
  1.81692506e-07 4.36064231e-07 3.80172400e-07 6.12805075e-07
  4.47776480e-07 8.89221797e-07 9.94628301e-07 8.34688203e-07
  2.77052090e-07 3.57796617e-08 9.44382309e-09 6.51809925e-09
  2.23060521e-08 1.81203935e-08 4.69652929e-09 3.76156613e-09
  1.44254142e-08 1.25994636e-08 3.07880336e-09 2.74534824e-09
  1.07824453e-08 9.77456425e-09 2.40856239e-09 2.30620522e-09
  8.94293804e-09 8.27803828e-09 2.02965168e-09 1.98253837e-09
  7.62165033e-09 7.37436745e-09 1.95652350e-09 1.80452296e-09
  6.90004303e-09 6.70486641e-09 1.93440373e-09 1.67038098e-09
  6.20794613e-09 6.37842908e-09 1.86682274e-09 1.60584572e-09
  5.50955118e-09 5.77612298e-09]
 [2.59280273e+00 7.38176961e-04 3.14416690e-05 6.78038912e-06
  2.40096146e-06 1.10376784e-06 5.21014633e-07 3.21502730e-07
  1.89330768e-07 1.26796633e-07 8.74629608e-08 6.83094868e-08
  6.05022683e-08 8.67926583e-08 1.58960929e-07 1.46438074e-07
  1.19222936e-07 1.26669284e-08 1.29000217e-08 6.79354721e-09
  7.00390678e-09 5.46241265e-09 5.34612250e-09 3.55757036e-09
  3.88030547e-09 3.53547525e-09 3.25497130e-09 2.42492333e-09
  2.57698980e-09 2.33543181e-09 2.34293929e-09 1.99741957e-09
  2.03364799e-09 1.89065964e-09 1.92336453e-09 1.71368956e-09
  1.72903974e-09 1.62706700e-09 1.77371830e-09 1.59610045e-09
  1.55275901e-09 1.53826937e-09 1.70193888e-09 1.53357497e-09
  1.47594038e-09 1.45963016e-09 1.65356264e-09 1.53097612e-09
  1.44509298e-09 1.40924445e-09]
 [2.58082194e+00 1.89091369e-03 2.21240216e-05 5.61907333e-06
  2.20003330e-06 1.06740488e-06 4.90274123e-07 3.40798696e-07
  2.05914434e-07 1.44388097e-07 1.02688105e-07 8.00371462e-08
  8.23556403e-08 1.00886546e-07 1.85755274e-07 1.70075992e-07
  3.64834897e-07 2.37825460e-08 2.77015849e-08 1.02721407e-08
  1.18946692e-08 8.79356494e-09 8.70853887e-09 4.60324023e-09
  5.50580029e-09 5.07678943e-09 4.49487375e-09 2.90614268e-09
  3.31025359e-09 2.89162476e-09 2.93604595e-09 2.36963625e-09
  2.53305967e-09 2.26609853e-09 2.29703468e-09 2.00965437e-09
  2.07265197e-09 1.93282587e-09 2.05139108e-09 1.85916774e-09
  1.83153905e-09 1.81241655e-09 1.94384909e-09 1.76265966e-09
  1.71454492e-09 1.66484015e-09 1.91959522e-09 1.81077190e-09
  1.72757945e-09 1.66728199e-09]
 [2.63564257e+00 9.53555987e-03 2.17696531e-05 6.08317796e-06
  2.40502550e-06 1.18650310e-06 4.85289253e-07 3.99627853e-07
  2.36946392e-07 1.74719030e-07 1.31080293e-07 1.01135950e-07
  1.32040982e-07 1.21090070e-07 1.98506376e-07 1.89431597e-07
  2.34341658e-07 6.42205695e-08 6.30299214e-08 1.71344884e-08
  2.93099488e-08 2.40182877e-08 1.44066535e-08 5.19724863e-09
  1.46577533e-08 1.36769622e-08 5.59986827e-09 2.65804480e-09
  9.43490045e-09 8.52910665e-09 2.99702000e-09 2.05789775e-09
  7.37748370e-09 6.88655784e-09 2.10688142e-09 1.64195481e-09
  6.18883961e-09 6.05932471e-09 1.91020959e-09 1.53842233e-09
  5.48812117e-09 5.69264242e-09 1.91705137e-09 1.46956610e-09
  4.97890970e-09 5.43924913e-09 2.01939978e-09 1.56854070e-09
  4.79102257e-09 5.54212608e-09]]
train_ae_loss [[4.27613274 2.89122387 3.77157626 4.20497219 4.41037595 4.56842405
  4.59178314 4.70267235 4.69341172 4.68753094 4.64321386 4.67414675
  4.6350549  4.533626   4.46438872 4.34686599 4.25839768 4.09257206
  3.79991188 3.61686633 3.53317752 3.42889746 3.24751832 3.18417239
  3.1461729  3.06289899 2.99642478 2.92974873 2.92426998 2.90335285
  2.8546871  2.82855494 2.80918136 2.81524565 2.78131396 2.77623814
  2.77008409 2.76954411 2.75668775 2.75246268 2.74303624 2.75679907
  2.75171688 2.72723897 2.74743191 2.74837861 2.75487669 2.73439614
  2.72540374 2.74441898]
 [3.69388901 2.88952795 3.67073717 3.94349418 4.04199856 4.17391846
  4.11470481 4.25247401 4.16673598 4.13537563 4.04962189 4.07214326
  4.00442549 3.83376391 3.67494206 3.38041083 3.09159447 2.86210057
  2.59509467 2.3534339  2.18351485 2.05039669 1.880875   1.80427343
  1.73096898 1.63588272 1.57396853 1.51661992 1.47807866 1.44838812
  1.40049402 1.37928035 1.35345049 1.34708161 1.31245063 1.3043961
  1.2849546  1.30621216 1.29251283 1.2679374  1.26688545 1.2842986
  1.26848583 1.24686062 1.2768656  1.25671177 1.27015577 1.24312111
  1.24372756 1.27366056]
 [4.81774158 2.77037499 3.5927226  3.77137    3.77103515 3.92971646
  3.7740585  3.90665529 3.68264044 3.60980966 3.4818348  3.48638787
  3.4137533  3.24490795 3.19685927 2.99161393 2.78723395 2.55048666
  2.28634667 2.065565   1.92176547 1.79988758 1.63423499 1.5674571
  1.51493738 1.41763329 1.35550776 1.31020143 1.27661846 1.24724946
  1.20020064 1.18341263 1.16158786 1.15658596 1.1230537  1.11452366
  1.10262868 1.11427767 1.10590152 1.08256349 1.08253945 1.09863158
  1.08088578 1.06042087 1.09167559 1.07033447 1.0829442  1.05851773
  1.06149508 1.08905165]
 [4.27354546 2.41307352 3.4067438  3.70034653 3.48676998 3.65466161
  3.42613048 3.56936684 3.30530161 3.22894066 3.11036885 3.11514223
  3.04436793 2.88262493 2.84306033 2.65580374 2.48652949 2.25505509
  2.02376478 1.81875031 1.6919899  1.58557967 1.43547986 1.37709265
  1.33392495 1.24247277 1.18808652 1.14864546 1.11966413 1.09339215
  1.04994158 1.03619172 1.01743069 1.01196806 0.9830925  0.97430595
  0.96586569 0.97338999 0.96891176 0.94667538 0.94811015 0.96145895
  0.94464534 0.92723957 0.95607671 0.93562445 0.94641785 0.92501451
  0.92991955 0.95320047]
 [4.45360495 2.04308321 2.90964862 3.42042906 3.0230694  3.16301259
  2.8798574  3.02115418 2.75099603 2.67412876 2.57301481 2.57678287
  2.51502816 2.36577129 2.3369208  2.17986784 1.98700739 1.81499007
  1.657125   1.4826374  1.36739892 1.28056875 1.16932627 1.12254418
  1.07802664 1.0070573  0.96675693 0.93512774 0.90825711 0.88735276
  0.85375753 0.84275162 0.82638864 0.82174986 0.79904709 0.79220497
  0.78456774 0.79035839 0.78730264 0.769328   0.77022558 0.78093675
  0.76777826 0.75392042 0.77644016 0.76015439 0.76903175 0.75138108
  0.75524258 0.77432118]]
valid_acc (5, 50)
valid_AUC (5, 50)
train_acc (50,)
total_train+valid_time 3605.822714422
Start Testing
Load ckpt at ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt
gc 9
Test layer0 Acc 0.7928, AUC 0.8760168552398682, avg_entr 0.20298393070697784, f1 0.7928000092506409
l0_test_time 0.6168200689999139
gc 0
Test layer1 Acc 0.811, AUC 0.8936922550201416, avg_entr 0.14581499993801117, f1 0.8109999895095825
l1_test_time 0.8110571499996695
gc 0
Test layer2 Acc 0.814, AUC 0.8970356583595276, avg_entr 0.09888697415590286, f1 0.8139999508857727
l2_test_time 1.1345247680001194
gc 0
Test layer3 Acc 0.8132, AUC 0.8970420360565186, avg_entr 0.09624029695987701, f1 0.8131999969482422
l3_test_time 1.6146412489997601
gc 0
Test layer4 Acc 0.8144, AUC 0.8971378207206726, avg_entr 0.09519216418266296, f1 0.8144000172615051
l4_test_time 2.250793347999661
gc 0
Test threshold 0.1 Acc 0.8142, AUC 0.8872498869895935, avg_entr 0.14763961732387543, f1 0.8141999840736389
t0.1_test_time 1.196839008999632
gc 0
Test threshold 0.2 Acc 0.814, AUC 0.8858500719070435, avg_entr 0.1612224131822586, f1 0.8139999508857727
t0.2_test_time 1.0707804949997808
gc 0
Test threshold 0.3 Acc 0.8146, AUC 0.8850132822990417, avg_entr 0.1757538616657257, f1 0.8145999908447266
t0.3_test_time 0.9894225900002311
gc 0
Test threshold 0.4 Acc 0.814, AUC 0.8840065002441406, avg_entr 0.18804344534873962, f1 0.8139999508857727
t0.4_test_time 0.9344024940000963
gc 0
Test threshold 0.5 Acc 0.8124, AUC 0.8833667635917664, avg_entr 0.20107069611549377, f1 0.8123999238014221
t0.5_test_time 0.8892320960003417
gc 0
Test threshold 0.6 Acc 0.811, AUC 0.8820763826370239, avg_entr 0.2142835259437561, f1 0.8109999895095825
t0.6_test_time 0.846243991999927
gc 0
Test threshold 0.7 Acc 0.81, AUC 0.8806943297386169, avg_entr 0.2254549264907837, f1 0.809999942779541
t0.7_test_time 0.8120436610001889
gc 0
Test threshold 0.8 Acc 0.807, AUC 0.8796358108520508, avg_entr 0.24038739502429962, f1 0.8069999814033508
t0.8_test_time 0.7700755730002129
gc 0
Test threshold 0.9 Acc 0.8032, AUC 0.8792739510536194, avg_entr 0.25648099184036255, f1 0.8032000660896301
t0.9_test_time 0.7257029610000245
