total count words 102019
vocab size 30000
train size 120000, valid size 3800, test size 3800
found 26754 words in glove
model: TransformerALsideText(
  (layers): ModuleList(
    (0): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=4, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=4, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
  (emb_layers): ModuleList(
    (0): Embedding(30000, 300)
    (1): Embedding(30000, 300)
    (2): Embedding(30000, 300)
    (3): Embedding(30000, 300)
    (4): Embedding(30000, 300)
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.multi_headed_attention.linears.0.weight 90000
layers.0.enc.f.multi_headed_attention.linears.0.bias 300
layers.0.enc.f.multi_headed_attention.linears.1.weight 90000
layers.0.enc.f.multi_headed_attention.linears.1.bias 300
layers.0.enc.f.multi_headed_attention.linears.2.weight 90000
layers.0.enc.f.multi_headed_attention.linears.2.bias 300
layers.0.enc.f.multi_headed_attention.linears.3.weight 90000
layers.0.enc.f.multi_headed_attention.linears.3.bias 300
layers.0.enc.f.feed_forward.w_1.weight 90000
layers.0.enc.f.feed_forward.w_1.bias 300
layers.0.enc.f.feed_forward.w_2.weight 90000
layers.0.enc.f.feed_forward.w_2.bias 300
layers.0.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.0.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.0.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.0.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.0.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.0.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.0.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.0.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.0.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.0.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.0.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.0.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.0.enc.f.encoder.norm.a 300
layers.0.enc.f.encoder.norm.b 300
layers.0.ae.g.0.weight 512
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 512
layers.0.ae.h.0.bias 4
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
emb_layers.0.weight 9000000
emb_layers.1.weight 9000000
emb_layers.2.weight 9000000
emb_layers.3.weight 9000000
emb_layers.4.weight 9000000
Total Trainable Params: 50758892
init_time 15.102032680000002
Start Training
gc 0
Train Epoch0 Acc 0.278275 (33393/120000), AUC 0.5182530879974365
ep0_train_time 20.8302329
Test Epoch0 layer0 Acc 0.6792105263157895, AUC 0.8750883936882019, avg_entr 0.8980346918106079, f1 0.6792105436325073
ep0_l0_test_time 0.06933338700000036
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 0
Test Epoch0 layer1 Acc 0.7113157894736842, AUC 0.9053250551223755, avg_entr 0.8473332524299622, f1 0.7113158106803894
ep0_l1_test_time 0.08974921499999766
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 0
Test Epoch0 layer2 Acc 0.6639473684210526, AUC 0.8999570608139038, avg_entr 0.8633137345314026, f1 0.663947343826294
ep0_l2_test_time 0.11212013299999768
Test Epoch0 layer3 Acc 0.6065789473684211, AUC 0.8925975561141968, avg_entr 0.9532718658447266, f1 0.6065789461135864
ep0_l3_test_time 0.13646334100000246
Test Epoch0 layer4 Acc 0.6563157894736842, AUC 0.8724450469017029, avg_entr 1.1584434509277344, f1 0.656315803527832
ep0_l4_test_time 0.16886742700000212
gc 0
Train Epoch1 Acc 0.6892666666666667 (82712/120000), AUC 0.8749710321426392
ep1_train_time 20.384619927000003
Test Epoch1 layer0 Acc 0.7563157894736842, AUC 0.9210762977600098, avg_entr 0.5030598044395447, f1 0.7563157677650452
ep1_l0_test_time 0.06739360099999914
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 1
Test Epoch1 layer1 Acc 0.7992105263157895, AUC 0.9426963925361633, avg_entr 0.43338489532470703, f1 0.7992105484008789
ep1_l1_test_time 0.08952429500000392
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 1
Test Epoch1 layer2 Acc 0.7963157894736842, AUC 0.9476327896118164, avg_entr 0.4144085645675659, f1 0.7963157892227173
ep1_l2_test_time 0.11139746799999983
Test Epoch1 layer3 Acc 0.7944736842105263, AUC 0.9483063817024231, avg_entr 0.4257965087890625, f1 0.7944737076759338
ep1_l3_test_time 0.13317818500000556
Test Epoch1 layer4 Acc 0.7844736842105263, AUC 0.9481790065765381, avg_entr 0.4462983012199402, f1 0.784473717212677
ep1_l4_test_time 0.1691115169999975
gc 0
Train Epoch2 Acc 0.8227833333333333 (98734/120000), AUC 0.9469906091690063
ep2_train_time 20.437671793
Test Epoch2 layer0 Acc 0.7913157894736842, AUC 0.9401558637619019, avg_entr 0.33056285977363586, f1 0.7913157939910889
ep2_l0_test_time 0.06794270599999663
Test Epoch2 layer1 Acc 0.8231578947368421, AUC 0.9567590355873108, avg_entr 0.2546471059322357, f1 0.8231579065322876
ep2_l1_test_time 0.0830511759999979
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 2
Test Epoch2 layer2 Acc 0.828421052631579, AUC 0.9590624570846558, avg_entr 0.2096700221300125, f1 0.8284210562705994
ep2_l2_test_time 0.11350963699999284
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 2
Test Epoch2 layer3 Acc 0.8268421052631579, AUC 0.959415078163147, avg_entr 0.1950932741165161, f1 0.8268421292304993
ep2_l3_test_time 0.140433189999996
Test Epoch2 layer4 Acc 0.8247368421052632, AUC 0.958225429058075, avg_entr 0.19043049216270447, f1 0.8247368335723877
ep2_l4_test_time 0.17084832999999833
gc 0
Train Epoch3 Acc 0.8588416666666666 (103061/120000), AUC 0.9595981240272522
ep3_train_time 20.422915618999994
Test Epoch3 layer0 Acc 0.8021052631578948, AUC 0.9449554085731506, avg_entr 0.26019373536109924, f1 0.8021052479743958
ep3_l0_test_time 0.0682760170000023
Test Epoch3 layer1 Acc 0.8318421052631579, AUC 0.9585437774658203, avg_entr 0.1686113178730011, f1 0.8318421840667725
ep3_l1_test_time 0.08293951899999286
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 3
Test Epoch3 layer2 Acc 0.8394736842105263, AUC 0.9599807858467102, avg_entr 0.14120379090309143, f1 0.8394736647605896
ep3_l2_test_time 0.11504062100000567
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 3
Test Epoch3 layer3 Acc 0.8423684210526315, AUC 0.9597896933555603, avg_entr 0.12800127267837524, f1 0.8423683643341064
ep3_l3_test_time 0.14139907800000628
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 3
Test Epoch3 layer4 Acc 0.8413157894736842, AUC 0.957456648349762, avg_entr 0.1259605437517166, f1 0.8413158059120178
ep3_l4_test_time 0.17278865899999118
gc 0
Train Epoch4 Acc 0.8792583333333334 (105511/120000), AUC 0.9672486186027527
ep4_train_time 20.439623554999997
Test Epoch4 layer0 Acc 0.8113157894736842, AUC 0.9468353986740112, avg_entr 0.2191823273897171, f1 0.8113157749176025
ep4_l0_test_time 0.06830650199999866
Test Epoch4 layer1 Acc 0.8476315789473684, AUC 0.9625831842422485, avg_entr 0.1322629451751709, f1 0.847631573677063
ep4_l1_test_time 0.08283483800001079
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 4
Test Epoch4 layer2 Acc 0.8557894736842105, AUC 0.9642428159713745, avg_entr 0.10605184733867645, f1 0.8557894825935364
ep4_l2_test_time 0.11202501900000073
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 4
Test Epoch4 layer3 Acc 0.8555263157894737, AUC 0.9641226530075073, avg_entr 0.10179268568754196, f1 0.855526328086853
ep4_l3_test_time 0.1408978910000087
Test Epoch4 layer4 Acc 0.855, AUC 0.9627500772476196, avg_entr 0.09770089387893677, f1 0.8550000190734863
ep4_l4_test_time 0.17033056200000374
gc 0
Train Epoch5 Acc 0.8956083333333333 (107473/120000), AUC 0.9726665019989014
ep5_train_time 20.498179590999996
Test Epoch5 layer0 Acc 0.8173684210526316, AUC 0.9489632248878479, avg_entr 0.19508038461208344, f1 0.8173684477806091
ep5_l0_test_time 0.06829217699998935
Test Epoch5 layer1 Acc 0.8465789473684211, AUC 0.9621210098266602, avg_entr 0.10819634795188904, f1 0.8465789556503296
ep5_l1_test_time 0.08274709699998084
Test Epoch5 layer2 Acc 0.8547368421052631, AUC 0.9658262729644775, avg_entr 0.0876409262418747, f1 0.854736864566803
ep5_l2_test_time 0.10532597499999952
Test Epoch5 layer3 Acc 0.8557894736842105, AUC 0.96417236328125, avg_entr 0.08077538758516312, f1 0.8557894825935364
ep5_l3_test_time 0.13423141600000577
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 5
Test Epoch5 layer4 Acc 0.8542105263157894, AUC 0.9625412821769714, avg_entr 0.07778529822826385, f1 0.8542105555534363
ep5_l4_test_time 0.17852045900002622
gc 0
Train Epoch6 Acc 0.906425 (108771/120000), AUC 0.9771401882171631
ep6_train_time 20.45697951699998
Test Epoch6 layer0 Acc 0.8257894736842105, AUC 0.9507896900177002, avg_entr 0.16725462675094604, f1 0.8257894515991211
ep6_l0_test_time 0.0677423939999926
Test Epoch6 layer1 Acc 0.8544736842105263, AUC 0.9635956287384033, avg_entr 0.08976592123508453, f1 0.8544737100601196
ep6_l1_test_time 0.08390540999999985
Test Epoch6 layer2 Acc 0.8594736842105263, AUC 0.9642729759216309, avg_entr 0.07244483381509781, f1 0.859473705291748
ep6_l2_test_time 0.10536855000000855
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 6
Test Epoch6 layer3 Acc 0.8592105263157894, AUC 0.9622705578804016, avg_entr 0.06465647369623184, f1 0.8592105507850647
ep6_l3_test_time 0.1419109219999939
Test Epoch6 layer4 Acc 0.8586842105263158, AUC 0.9606584310531616, avg_entr 0.059270117431879044, f1 0.8586841821670532
ep6_l4_test_time 0.17067911799998114
gc 0
Train Epoch7 Acc 0.916025 (109923/120000), AUC 0.9809303283691406
ep7_train_time 20.389026030999986
Test Epoch7 layer0 Acc 0.828421052631579, AUC 0.9509303569793701, avg_entr 0.15270353853702545, f1 0.8284210562705994
ep7_l0_test_time 0.06805281100000116
Test Epoch7 layer1 Acc 0.8592105263157894, AUC 0.9641745686531067, avg_entr 0.07558920979499817, f1 0.8592105507850647
ep7_l1_test_time 0.08318159199998831
Test Epoch7 layer2 Acc 0.863421052631579, AUC 0.9663133025169373, avg_entr 0.058431848883628845, f1 0.8634210228919983
ep7_l2_test_time 0.10554702900000734
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 7
Test Epoch7 layer3 Acc 0.8644736842105263, AUC 0.9655734896659851, avg_entr 0.05126205459237099, f1 0.8644736409187317
ep7_l3_test_time 0.1424610589999986
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 7
Test Epoch7 layer4 Acc 0.863421052631579, AUC 0.9631115198135376, avg_entr 0.046774886548519135, f1 0.8634210228919983
ep7_l4_test_time 0.18191515699999172
gc 0
Train Epoch8 Acc 0.9247166666666666 (110966/120000), AUC 0.98308265209198
ep8_train_time 20.407811770999984
Test Epoch8 layer0 Acc 0.8315789473684211, AUC 0.9517351388931274, avg_entr 0.14304262399673462, f1 0.8315790295600891
ep8_l0_test_time 0.06825431900000467
Test Epoch8 layer1 Acc 0.8610526315789474, AUC 0.9641292095184326, avg_entr 0.06506223976612091, f1 0.8610526323318481
ep8_l1_test_time 0.08303521100000921
Test Epoch8 layer2 Acc 0.8668421052631579, AUC 0.9655047059059143, avg_entr 0.04999003931879997, f1 0.8668420910835266
ep8_l2_test_time 0.10538690600000677
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 8
Test Epoch8 layer3 Acc 0.8671052631578947, AUC 0.9638240337371826, avg_entr 0.04395759850740433, f1 0.86710524559021
ep8_l3_test_time 0.14585542899999382
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 8
Test Epoch8 layer4 Acc 0.8660526315789474, AUC 0.9611377120018005, avg_entr 0.039427753537893295, f1 0.8660526275634766
ep8_l4_test_time 0.17911791199998106
gc 0
Train Epoch9 Acc 0.9323833333333333 (111886/120000), AUC 0.9852399826049805
ep9_train_time 20.43170693900001
Test Epoch9 layer0 Acc 0.83, AUC 0.9526464343070984, avg_entr 0.1377348154783249, f1 0.8299999833106995
ep9_l0_test_time 0.0685429860000113
Test Epoch9 layer1 Acc 0.8586842105263158, AUC 0.963241696357727, avg_entr 0.06327559798955917, f1 0.8586841821670532
ep9_l1_test_time 0.08323102500000346
Test Epoch9 layer2 Acc 0.8689473684210526, AUC 0.9651969075202942, avg_entr 0.04570406302809715, f1 0.8689473867416382
ep9_l2_test_time 0.10588942799998335
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 9
Test Epoch9 layer3 Acc 0.868421052631579, AUC 0.964327871799469, avg_entr 0.039357174187898636, f1 0.8684210777282715
ep9_l3_test_time 0.14094450700000039
Test Epoch9 layer4 Acc 0.8686842105263158, AUC 0.9605810642242432, avg_entr 0.03436233103275299, f1 0.8686841726303101
ep9_l4_test_time 0.17033372700001337
gc 0
Train Epoch10 Acc 0.9374583333333333 (112495/120000), AUC 0.9870795011520386
ep10_train_time 20.42890756899999
Test Epoch10 layer0 Acc 0.8357894736842105, AUC 0.9518781900405884, avg_entr 0.12905359268188477, f1 0.8357895016670227
ep10_l0_test_time 0.07961701399997878
Test Epoch10 layer1 Acc 0.8607894736842105, AUC 0.9638807773590088, avg_entr 0.05838407576084137, f1 0.8607894778251648
ep10_l1_test_time 0.08373516600002517
Test Epoch10 layer2 Acc 0.868421052631579, AUC 0.9638668894767761, avg_entr 0.04191780462861061, f1 0.8684210777282715
ep10_l2_test_time 0.10558204500000556
Test Epoch10 layer3 Acc 0.866578947368421, AUC 0.9630045890808105, avg_entr 0.035316143184900284, f1 0.8665789365768433
ep10_l3_test_time 0.1342715700000099
Test Epoch10 layer4 Acc 0.8673684210526316, AUC 0.9592383503913879, avg_entr 0.03142678737640381, f1 0.8673684000968933
ep10_l4_test_time 0.17036374100001694
gc 0
Train Epoch11 Acc 0.942475 (113097/120000), AUC 0.9886201620101929
ep11_train_time 20.38317058999999
Test Epoch11 layer0 Acc 0.8313157894736842, AUC 0.9517514705657959, avg_entr 0.12329479306936264, f1 0.831315815448761
ep11_l0_test_time 0.0682059499999923
Test Epoch11 layer1 Acc 0.8578947368421053, AUC 0.9618498086929321, avg_entr 0.05627361312508583, f1 0.8578947186470032
ep11_l1_test_time 0.08273503000003757
Test Epoch11 layer2 Acc 0.8631578947368421, AUC 0.9610422849655151, avg_entr 0.04112494736909866, f1 0.8631578683853149
ep11_l2_test_time 0.10553681099997902
Test Epoch11 layer3 Acc 0.861578947368421, AUC 0.9593158960342407, avg_entr 0.03573354706168175, f1 0.8615789413452148
ep11_l3_test_time 0.13413944099994524
Test Epoch11 layer4 Acc 0.8605263157894737, AUC 0.9579476714134216, avg_entr 0.03226565569639206, f1 0.8605263233184814
ep11_l4_test_time 0.17024261199998136
gc 0
Train Epoch12 Acc 0.9534416666666666 (114413/120000), AUC 0.9922130107879639
ep12_train_time 20.430326982999986
Test Epoch12 layer0 Acc 0.8286842105263158, AUC 0.9514167308807373, avg_entr 0.11439947038888931, f1 0.8286842107772827
ep12_l0_test_time 0.06852013299999271
Test Epoch12 layer1 Acc 0.8594736842105263, AUC 0.9611911177635193, avg_entr 0.05115561559796333, f1 0.859473705291748
ep12_l1_test_time 0.08304446900001494
Test Epoch12 layer2 Acc 0.8644736842105263, AUC 0.9600270390510559, avg_entr 0.03650780767202377, f1 0.8644736409187317
ep12_l2_test_time 0.10601888500002588
Test Epoch12 layer3 Acc 0.866578947368421, AUC 0.9574121236801147, avg_entr 0.029976533725857735, f1 0.8665789365768433
ep12_l3_test_time 0.1342550549999828
Test Epoch12 layer4 Acc 0.8657894736842106, AUC 0.95558762550354, avg_entr 0.026037588715553284, f1 0.8657894730567932
ep12_l4_test_time 0.17006355300003406
gc 0
Train Epoch13 Acc 0.9572416666666667 (114869/120000), AUC 0.9929583072662354
ep13_train_time 20.35962849599997
Test Epoch13 layer0 Acc 0.8310526315789474, AUC 0.951117753982544, avg_entr 0.1089189201593399, f1 0.8310526013374329
ep13_l0_test_time 0.06802154700000074
Test Epoch13 layer1 Acc 0.8581578947368421, AUC 0.959764301776886, avg_entr 0.04672932252287865, f1 0.8581578731536865
ep13_l1_test_time 0.08273080700001856
Test Epoch13 layer2 Acc 0.8631578947368421, AUC 0.9588284492492676, avg_entr 0.03189302980899811, f1 0.8631578683853149
ep13_l2_test_time 0.10510434000002533
Test Epoch13 layer3 Acc 0.8644736842105263, AUC 0.9584843516349792, avg_entr 0.026732120662927628, f1 0.8644736409187317
ep13_l3_test_time 0.1348634299999958
Test Epoch13 layer4 Acc 0.8644736842105263, AUC 0.9558922052383423, avg_entr 0.023942317813634872, f1 0.8644736409187317
ep13_l4_test_time 0.1702418539999826
gc 0
Train Epoch14 Acc 0.9633583333333333 (115603/120000), AUC 0.9942569732666016
ep14_train_time 20.43278491799998
Test Epoch14 layer0 Acc 0.8344736842105264, AUC 0.9511127471923828, avg_entr 0.1041877418756485, f1 0.8344736695289612
ep14_l0_test_time 0.06786033499997757
Test Epoch14 layer1 Acc 0.8557894736842105, AUC 0.9574382305145264, avg_entr 0.042793575674295425, f1 0.8557894825935364
ep14_l1_test_time 0.08282884199996943
Test Epoch14 layer2 Acc 0.8605263157894737, AUC 0.9588900804519653, avg_entr 0.027826933190226555, f1 0.8605263233184814
ep14_l2_test_time 0.10541317799999206
Test Epoch14 layer3 Acc 0.8618421052631579, AUC 0.958500325679779, avg_entr 0.024834316223859787, f1 0.8618420958518982
ep14_l3_test_time 0.1340976429999614
Test Epoch14 layer4 Acc 0.8621052631578947, AUC 0.9546799659729004, avg_entr 0.02178436890244484, f1 0.8621052503585815
ep14_l4_test_time 0.16969592399999556
gc 0
Train Epoch15 Acc 0.9671583333333333 (116059/120000), AUC 0.9948878288269043
ep15_train_time 20.63650433600003
Test Epoch15 layer0 Acc 0.8342105263157895, AUC 0.9511000514030457, avg_entr 0.10231602936983109, f1 0.8342105150222778
ep15_l0_test_time 0.06797997400002487
Test Epoch15 layer1 Acc 0.8547368421052631, AUC 0.9562881588935852, avg_entr 0.04549625143408775, f1 0.854736864566803
ep15_l1_test_time 0.08283934700000373
Test Epoch15 layer2 Acc 0.855, AUC 0.9534562826156616, avg_entr 0.028569886460900307, f1 0.8550000190734863
ep15_l2_test_time 0.10530068200000642
Test Epoch15 layer3 Acc 0.8560526315789474, AUC 0.956809401512146, avg_entr 0.02500702254474163, f1 0.8560526371002197
ep15_l3_test_time 0.1344483979999609
Test Epoch15 layer4 Acc 0.8552631578947368, AUC 0.9536211490631104, avg_entr 0.021395402029156685, f1 0.8552631735801697
ep15_l4_test_time 0.1702785469999526
gc 0
Train Epoch16 Acc 0.9711833333333333 (116542/120000), AUC 0.9959189891815186
ep16_train_time 20.37920709399998
Test Epoch16 layer0 Acc 0.8315789473684211, AUC 0.9507407546043396, avg_entr 0.09600555896759033, f1 0.8315790295600891
ep16_l0_test_time 0.07333920899998247
Test Epoch16 layer1 Acc 0.8555263157894737, AUC 0.9576142430305481, avg_entr 0.03907394036650658, f1 0.855526328086853
ep16_l1_test_time 0.0873714370000016
Test Epoch16 layer2 Acc 0.8610526315789474, AUC 0.9549852013587952, avg_entr 0.023409493267536163, f1 0.8610526323318481
ep16_l2_test_time 0.10622952400001395
Test Epoch16 layer3 Acc 0.86, AUC 0.9575542211532593, avg_entr 0.020951280370354652, f1 0.8600000143051147
ep16_l3_test_time 0.13459873900001185
Test Epoch16 layer4 Acc 0.8597368421052631, AUC 0.9520066976547241, avg_entr 0.018784532323479652, f1 0.8597368597984314
ep16_l4_test_time 0.17024780600002032
gc 0
Train Epoch17 Acc 0.9736583333333333 (116839/120000), AUC 0.996293842792511
ep17_train_time 20.387743215
Test Epoch17 layer0 Acc 0.8315789473684211, AUC 0.951216459274292, avg_entr 0.09824564307928085, f1 0.8315790295600891
ep17_l0_test_time 0.06847522199996092
Test Epoch17 layer1 Acc 0.8560526315789474, AUC 0.9549322128295898, avg_entr 0.04129192978143692, f1 0.8560526371002197
ep17_l1_test_time 0.08310245999996368
Test Epoch17 layer2 Acc 0.8563157894736843, AUC 0.9534082412719727, avg_entr 0.02423913963139057, f1 0.8563157916069031
ep17_l2_test_time 0.10538551899998083
Test Epoch17 layer3 Acc 0.8589473684210527, AUC 0.9539893269538879, avg_entr 0.01987667940557003, f1 0.8589473962783813
ep17_l3_test_time 0.13419685800005254
Test Epoch17 layer4 Acc 0.8565789473684211, AUC 0.9498878717422485, avg_entr 0.01841643452644348, f1 0.8565789461135864
ep17_l4_test_time 0.17046936200000573
gc 0
Train Epoch18 Acc 0.9760166666666666 (117122/120000), AUC 0.9965903759002686
ep18_train_time 20.38728757499996
Test Epoch18 layer0 Acc 0.8305263157894737, AUC 0.9507206082344055, avg_entr 0.09328203648328781, f1 0.8305262923240662
ep18_l0_test_time 0.0681784629999811
Test Epoch18 layer1 Acc 0.855, AUC 0.9549037218093872, avg_entr 0.04063086211681366, f1 0.8550000190734863
ep18_l1_test_time 0.08291885500000262
Test Epoch18 layer2 Acc 0.8578947368421053, AUC 0.9497829675674438, avg_entr 0.022568155080080032, f1 0.8578947186470032
ep18_l2_test_time 0.10531328100000792
Test Epoch18 layer3 Acc 0.8592105263157894, AUC 0.9531143307685852, avg_entr 0.018528832122683525, f1 0.8592105507850647
ep18_l3_test_time 0.13427364400001807
Test Epoch18 layer4 Acc 0.8597368421052631, AUC 0.9452075958251953, avg_entr 0.01657254621386528, f1 0.8597368597984314
ep18_l4_test_time 0.1702265220000072
gc 0
Train Epoch19 Acc 0.9773166666666666 (117278/120000), AUC 0.9968687891960144
ep19_train_time 20.41872694799997
Test Epoch19 layer0 Acc 0.8310526315789474, AUC 0.9509251713752747, avg_entr 0.09246847033500671, f1 0.8310526013374329
ep19_l0_test_time 0.06802335899999434
Test Epoch19 layer1 Acc 0.855, AUC 0.9531707763671875, avg_entr 0.03673971816897392, f1 0.8550000190734863
ep19_l1_test_time 0.08294491100002688
Test Epoch19 layer2 Acc 0.8565789473684211, AUC 0.95164954662323, avg_entr 0.020113974809646606, f1 0.8565789461135864
ep19_l2_test_time 0.10558779500001947
Test Epoch19 layer3 Acc 0.8563157894736843, AUC 0.9512887001037598, avg_entr 0.015624932013452053, f1 0.8563157916069031
ep19_l3_test_time 0.1344674410000266
Test Epoch19 layer4 Acc 0.8560526315789474, AUC 0.9417504072189331, avg_entr 0.013155617751181126, f1 0.8560526371002197
ep19_l4_test_time 0.17033300600002121
Best AUC tensor(0.8689) 9 2
train_as_loss [[4.56171563e+02 3.56753935e+02 3.51133162e+02 3.49869965e+02
  3.49388170e+02 3.49155374e+02 3.49026611e+02 3.48948925e+02
  3.48899173e+02 3.48865907e+02 3.48842944e+02 3.48826706e+02
  3.48815007e+02 3.48806466e+02 3.48801446e+02 3.48798467e+02
  3.48795696e+02 3.48793188e+02 3.48791441e+02 3.48790320e+02]
 [1.51632871e+00 1.14597789e-05 8.89262900e-07 1.86401036e-07
  3.14807862e-07 9.57039317e-07 3.72101406e-06 3.17554386e-05
  1.27324535e-04 2.14698404e-04 4.18721062e-04 2.65101811e-04
  1.86046628e-06 1.33382734e-09 1.11342950e-04 7.82544089e-05
  1.31157465e-08 5.09574545e-10 2.37427118e-09 4.26171835e-05]
 [1.81296676e+00 1.28221128e-05 1.06393168e-06 2.43270857e-07
  1.09526499e-07 2.71154338e-07 2.47062179e-06 4.23817803e-05
  2.15887280e-04 4.52183136e-04 8.31619255e-04 6.02859129e-04
  1.99998015e-06 4.02204820e-09 2.77838328e-04 1.59078359e-04
  2.62760111e-08 1.45152956e-09 3.35777202e-09 6.76837448e-05]
 [1.83272459e+00 1.14579466e-05 1.04201918e-06 2.89756307e-07
  1.52833292e-07 3.08998790e-07 2.80596818e-06 5.62474670e-05
  3.15393964e-04 7.19470593e-04 1.31202918e-03 1.03075671e-03
  2.59801084e-06 1.66838167e-08 4.08005758e-04 3.16445035e-04
  5.04175862e-08 4.67498161e-09 7.12397308e-09 8.12606266e-05]
 [1.75635935e+00 1.54607900e-05 1.63500870e-06 5.68628228e-07
  3.46971293e-07 4.28206391e-07 3.19498222e-06 7.04728673e-05
  4.61944526e-04 1.09521691e-03 2.00277875e-03 1.86158327e-03
  2.56477577e-06 5.51667992e-08 5.31324017e-04 5.28516259e-04
  1.15598073e-07 1.55035245e-08 1.98929342e-08 9.38123104e-05]]
train_ae_loss [[16.62594808 17.00115058 16.36467042 15.43735934 14.96091814 14.62306253
  14.39351262 14.22672173 14.04416131 13.8776796  13.77616305 13.64784188
  13.50496696 13.39178256 12.36299925 12.09063051 11.97635506 11.85765964
  11.28529937 11.14497104]
 [14.56493733 14.41060894 13.36648907 12.05945083 10.96376256 10.05128463
   9.36811129  8.82647927  8.31864704  7.89712313  7.58236878  7.24555934
   6.52630315  6.28165354  5.52477597  5.18507769  4.81321028  4.62767093
   4.24520077  4.09310025]
 [15.18350536 14.49750011 12.71273594 11.32092232 10.30438205  9.39185454
   8.65351436  8.03025462  7.43680766  6.94235008  6.58579478  6.19824135
   5.2522802   4.97332636  4.24881462  3.9331358   3.49550484  3.27187335
   2.93713044  2.80957764]
 [15.53908845 14.78064168 12.18981671 10.74497471  9.76916801  8.77233802
   8.01135128  7.3837675   6.83201562  6.33919827  5.98757589  5.58263108
   4.57221188  4.30907457  3.65337487  3.36198919  2.9347113   2.72247719
   2.41798498  2.30968718]
 [15.28593734 14.88746529 10.7607451   9.25352109  8.41286679  7.60113152
   6.94766333  6.35213479  5.82179491  5.33639554  5.03450654  4.62621114
   3.7035145   3.48103287  2.9532763   2.71355236  2.34249316  2.17243034
   1.93241841  1.84796055]]
valid_acc (5, 20)
valid_AUC (5, 20)
train_acc (20,)
total_train+valid_time 425.748917614
Start Testing
Load ckpt at ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt
gc 9
Test layer0 Acc 0.8252631578947368, AUC 0.9520171284675598, avg_entr 0.13901279866695404, f1 0.8252631425857544
l0_test_time 0.06783433300000752
gc 0
Test layer1 Acc 0.8544736842105263, AUC 0.9609194397926331, avg_entr 0.062179405242204666, f1 0.8544737100601196
l1_test_time 0.08406346999998959
gc 0
Test layer2 Acc 0.8602631578947368, AUC 0.9636138677597046, avg_entr 0.04478192701935768, f1 0.8602631688117981
l2_test_time 0.10741316099995402
gc 0
Test layer3 Acc 0.863421052631579, AUC 0.9631151556968689, avg_entr 0.038263723254203796, f1 0.8634210228919983
l3_test_time 0.13614471099998582
gc 0
Test layer4 Acc 0.8628947368421053, AUC 0.9594134092330933, avg_entr 0.033558886498212814, f1 0.8628947138786316
l4_test_time 0.17241561700001284
gc 0
Test threshold 0.1 Acc 0.8594736842105263, AUC 0.956002950668335, avg_entr 0.03441278636455536, f1 0.859473705291748
t0.1_test_time 0.12325510099998382
gc 0
Test threshold 0.2 Acc 0.8552631578947368, AUC 0.9553176164627075, avg_entr 0.045569632202386856, f1 0.8552631735801697
t0.2_test_time 0.12067801400002054
gc 0
Test threshold 0.3 Acc 0.8468421052631578, AUC 0.9546379446983337, avg_entr 0.05986688658595085, f1 0.8468421101570129
t0.3_test_time 0.11816189699999313
gc 0
Test threshold 0.4 Acc 0.8326315789473684, AUC 0.9529767036437988, avg_entr 0.08259192109107971, f1 0.832631528377533
t0.4_test_time 0.10033905099999174
gc 0
Test threshold 0.5 Acc 0.8271052631578948, AUC 0.9522453546524048, avg_entr 0.09622418135404587, f1 0.8271052837371826
t0.5_test_time 0.08892491099999233
gc 0
Test threshold 0.6 Acc 0.8252631578947368, AUC 0.9520171284675598, avg_entr 0.10027654469013214, f1 0.8252631425857544
t0.6_test_time 0.08452723099998138
gc 0
Test threshold 0.7 Acc 0.8252631578947368, AUC 0.9520171284675598, avg_entr 0.10027654469013214, f1 0.8252631425857544
t0.7_test_time 0.0795734720000496
gc 0
Test threshold 0.8 Acc 0.8252631578947368, AUC 0.9520171284675598, avg_entr 0.10027654469013214, f1 0.8252631425857544
t0.8_test_time 0.08017204599997285
gc 0
Test threshold 0.9 Acc 0.8252631578947368, AUC 0.9520171284675598, avg_entr 0.10027654469013214, f1 0.8252631425857544
t0.9_test_time 0.07996117200002573
