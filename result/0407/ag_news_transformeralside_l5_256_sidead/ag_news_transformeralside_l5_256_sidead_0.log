total count words 102019
vocab size 30000
train size 120000, valid size 3800, test size 3800
found 26754 words in glove
model: TransformerALsideText(
  (layers): ModuleList(
    (0): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=4, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=4, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
  (emb_layers): ModuleList(
    (0): Embedding(30000, 300)
    (1): Embedding(30000, 300)
    (2): Embedding(30000, 300)
    (3): Embedding(30000, 300)
    (4): Embedding(30000, 300)
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.multi_headed_attention.linears.0.weight 90000
layers.0.enc.f.multi_headed_attention.linears.0.bias 300
layers.0.enc.f.multi_headed_attention.linears.1.weight 90000
layers.0.enc.f.multi_headed_attention.linears.1.bias 300
layers.0.enc.f.multi_headed_attention.linears.2.weight 90000
layers.0.enc.f.multi_headed_attention.linears.2.bias 300
layers.0.enc.f.multi_headed_attention.linears.3.weight 90000
layers.0.enc.f.multi_headed_attention.linears.3.bias 300
layers.0.enc.f.feed_forward.w_1.weight 90000
layers.0.enc.f.feed_forward.w_1.bias 300
layers.0.enc.f.feed_forward.w_2.weight 90000
layers.0.enc.f.feed_forward.w_2.bias 300
layers.0.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.0.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.0.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.0.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.0.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.0.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.0.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.0.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.0.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.0.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.0.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.0.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.0.enc.f.encoder.norm.a 300
layers.0.enc.f.encoder.norm.b 300
layers.0.ae.g.0.weight 512
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 512
layers.0.ae.h.0.bias 4
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
emb_layers.0.weight 9000000
emb_layers.1.weight 9000000
emb_layers.2.weight 9000000
emb_layers.3.weight 9000000
emb_layers.4.weight 9000000
Total Trainable Params: 50758892
init_time 15.134910252000001
Start Training
gc 0
Train Epoch0 Acc 0.276275 (33153/120000), AUC 0.5194100141525269
ep0_train_time 20.631891299000003
Test Epoch0 layer0 Acc 0.6842105263157895, AUC 0.8730555176734924, avg_entr 0.9059767127037048, f1 0.6842105388641357
ep0_l0_test_time 0.06918409099999678
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 0
Test Epoch0 layer1 Acc 0.7105263157894737, AUC 0.9011079668998718, avg_entr 0.8413475155830383, f1 0.7105262875556946
ep0_l1_test_time 0.08570497099999841
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 0
Test Epoch0 layer2 Acc 0.6721052631578948, AUC 0.8954540491104126, avg_entr 0.8467659950256348, f1 0.6721052527427673
ep0_l2_test_time 0.11118334300000043
Test Epoch0 layer3 Acc 0.6884210526315789, AUC 0.870206356048584, avg_entr 0.9831405282020569, f1 0.6884210705757141
ep0_l3_test_time 0.13269306899999833
Test Epoch0 layer4 Acc 0.5628947368421052, AUC 0.8466917872428894, avg_entr 1.142667531967163, f1 0.5628947615623474
ep0_l4_test_time 0.16787942999999927
gc 0
Train Epoch1 Acc 0.6817666666666666 (81812/120000), AUC 0.870212972164154
ep1_train_time 20.283141688
Test Epoch1 layer0 Acc 0.7610526315789473, AUC 0.9220758676528931, avg_entr 0.49627426266670227, f1 0.761052668094635
ep1_l0_test_time 0.06735101900000018
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 1
Test Epoch1 layer1 Acc 0.786578947368421, AUC 0.9413205981254578, avg_entr 0.40588417649269104, f1 0.7865789532661438
ep1_l1_test_time 0.08832045000000477
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 1
Test Epoch1 layer2 Acc 0.7786842105263158, AUC 0.9419411420822144, avg_entr 0.39442452788352966, f1 0.7786842584609985
ep1_l2_test_time 0.11240157399999617
Test Epoch1 layer3 Acc 0.7660526315789473, AUC 0.9420766830444336, avg_entr 0.39631903171539307, f1 0.7660526037216187
ep1_l3_test_time 0.1325325709999987
Test Epoch1 layer4 Acc 0.7518421052631579, AUC 0.9425745010375977, avg_entr 0.4204050600528717, f1 0.7518420219421387
ep1_l4_test_time 0.16959046799999555
gc 0
Train Epoch2 Acc 0.8137916666666667 (97655/120000), AUC 0.9411581754684448
ep2_train_time 20.354627811000007
Test Epoch2 layer0 Acc 0.795, AUC 0.9405402541160583, avg_entr 0.3235500752925873, f1 0.7950000166893005
ep2_l0_test_time 0.0670328860000069
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 2
Test Epoch2 layer1 Acc 0.825, AUC 0.9547808170318604, avg_entr 0.2364451289176941, f1 0.824999988079071
ep2_l1_test_time 0.0891280240000043
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 2
Test Epoch2 layer2 Acc 0.8263157894736842, AUC 0.956598699092865, avg_entr 0.203547865152359, f1 0.8263157606124878
ep2_l2_test_time 0.11138476800000774
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 2
Test Epoch2 layer3 Acc 0.8276315789473684, AUC 0.9561944007873535, avg_entr 0.1909037083387375, f1 0.8276315927505493
ep2_l3_test_time 0.13742740499999684
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 2
Test Epoch2 layer4 Acc 0.828421052631579, AUC 0.95612633228302, avg_entr 0.1857357621192932, f1 0.8284210562705994
ep2_l4_test_time 0.1704255020000005
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 2
gc 0
Train Epoch3 Acc 0.8508333333333333 (102100/120000), AUC 0.9562742710113525
ep3_train_time 20.32536896699999
Test Epoch3 layer0 Acc 0.8060526315789474, AUC 0.9463838338851929, avg_entr 0.24754822254180908, f1 0.8060526251792908
ep3_l0_test_time 0.06775378899999396
Test Epoch3 layer1 Acc 0.83, AUC 0.9589473605155945, avg_entr 0.16226212680339813, f1 0.8299999833106995
ep3_l1_test_time 0.08186767699999109
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 3
Test Epoch3 layer2 Acc 0.8326315789473684, AUC 0.9603250622749329, avg_entr 0.14898313581943512, f1 0.832631528377533
ep3_l2_test_time 0.1124425940000009
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 3
Test Epoch3 layer3 Acc 0.8328947368421052, AUC 0.9574117660522461, avg_entr 0.1430589258670807, f1 0.8328947424888611
ep3_l3_test_time 0.14070085800000243
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 3
Test Epoch3 layer4 Acc 0.8292105263157895, AUC 0.9584890604019165, avg_entr 0.14842306077480316, f1 0.8292105197906494
ep3_l4_test_time 0.17036321299998747
gc 0
Train Epoch4 Acc 0.873175 (104781/120000), AUC 0.9653009176254272
ep4_train_time 20.334641543000004
Test Epoch4 layer0 Acc 0.8223684210526315, AUC 0.9500792026519775, avg_entr 0.21492807567119598, f1 0.8223684430122375
ep4_l0_test_time 0.06769182400000773
Test Epoch4 layer1 Acc 0.8478947368421053, AUC 0.9630023241043091, avg_entr 0.12839604914188385, f1 0.8478947281837463
ep4_l1_test_time 0.08154563099999734
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 4
Test Epoch4 layer2 Acc 0.8539473684210527, AUC 0.9644452929496765, avg_entr 0.11260531842708588, f1 0.8539473414421082
ep4_l2_test_time 0.11218680499999323
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 4
Test Epoch4 layer3 Acc 0.8568421052631578, AUC 0.9625482559204102, avg_entr 0.1042260229587555, f1 0.8568421006202698
ep4_l3_test_time 0.1380933760000005
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 4
Test Epoch4 layer4 Acc 0.8563157894736843, AUC 0.9624587297439575, avg_entr 0.10018827766180038, f1 0.8563157916069031
ep4_l4_test_time 0.17303465000000529
gc 0
Train Epoch5 Acc 0.8887166666666667 (106646/120000), AUC 0.971652090549469
ep5_train_time 20.615250704999994
Test Epoch5 layer0 Acc 0.8168421052631579, AUC 0.9511234760284424, avg_entr 0.18289467692375183, f1 0.8168420791625977
ep5_l0_test_time 0.06727502600000435
Test Epoch5 layer1 Acc 0.8494736842105263, AUC 0.9603419303894043, avg_entr 0.10002584010362625, f1 0.8494736552238464
ep5_l1_test_time 0.082310753999991
Test Epoch5 layer2 Acc 0.8507894736842105, AUC 0.9635196328163147, avg_entr 0.08973243832588196, f1 0.850789487361908
ep5_l2_test_time 0.10403308600001537
Test Epoch5 layer3 Acc 0.8489473684210527, AUC 0.9622174501419067, avg_entr 0.0820387452840805, f1 0.8489473462104797
ep5_l3_test_time 0.13315213899997502
Test Epoch5 layer4 Acc 0.848421052631579, AUC 0.9596072435379028, avg_entr 0.08189049363136292, f1 0.848421037197113
ep5_l4_test_time 0.16837730699998588
gc 0
Train Epoch6 Acc 0.900625 (108075/120000), AUC 0.9749664068222046
ep6_train_time 20.458390250999997
Test Epoch6 layer0 Acc 0.8255263157894737, AUC 0.9525017738342285, avg_entr 0.1667143851518631, f1 0.825526237487793
ep6_l0_test_time 0.06782053999998539
Test Epoch6 layer1 Acc 0.8526315789473684, AUC 0.9637500047683716, avg_entr 0.08450723439455032, f1 0.8526315689086914
ep6_l1_test_time 0.08186853300000507
Test Epoch6 layer2 Acc 0.8581578947368421, AUC 0.9668264389038086, avg_entr 0.07172853499650955, f1 0.8581578731536865
ep6_l2_test_time 0.10371904699999845
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 6
Test Epoch6 layer3 Acc 0.86, AUC 0.9669786691665649, avg_entr 0.06346436589956284, f1 0.8600000143051147
ep6_l3_test_time 0.14083633599997825
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 6
Test Epoch6 layer4 Acc 0.8597368421052631, AUC 0.963715672492981, avg_entr 0.05886484682559967, f1 0.8597368597984314
ep6_l4_test_time 0.17486733800001275
gc 0
Train Epoch7 Acc 0.9105666666666666 (109268/120000), AUC 0.9791523218154907
ep7_train_time 20.43398645000002
Test Epoch7 layer0 Acc 0.825, AUC 0.9519621133804321, avg_entr 0.1509084552526474, f1 0.824999988079071
ep7_l0_test_time 0.06766305000002149
Test Epoch7 layer1 Acc 0.8510526315789474, AUC 0.9634526968002319, avg_entr 0.07256843894720078, f1 0.8510526418685913
ep7_l1_test_time 0.0829334670000037
Test Epoch7 layer2 Acc 0.8552631578947368, AUC 0.964114785194397, avg_entr 0.061349570751190186, f1 0.8552631735801697
ep7_l2_test_time 0.10456086499999628
Test Epoch7 layer3 Acc 0.8573684210526316, AUC 0.9638463258743286, avg_entr 0.0552176870405674, f1 0.8573684096336365
ep7_l3_test_time 0.133065031000001
Test Epoch7 layer4 Acc 0.8573684210526316, AUC 0.9624994993209839, avg_entr 0.05374644324183464, f1 0.8573684096336365
ep7_l4_test_time 0.16859132600001203
gc 0
Train Epoch8 Acc 0.9201 (110412/120000), AUC 0.982234001159668
ep8_train_time 20.31996323200002
Test Epoch8 layer0 Acc 0.8310526315789474, AUC 0.9526090621948242, avg_entr 0.1452755630016327, f1 0.8310526013374329
ep8_l0_test_time 0.06712962499997843
Test Epoch8 layer1 Acc 0.8576315789473684, AUC 0.9649832248687744, avg_entr 0.07246360927820206, f1 0.8576316237449646
ep8_l1_test_time 0.08172260399999232
Test Epoch8 layer2 Acc 0.8644736842105263, AUC 0.9662764072418213, avg_entr 0.05892207846045494, f1 0.8644736409187317
ep8_l2_test_time 0.10344577399999366
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 8
Test Epoch8 layer3 Acc 0.8642105263157894, AUC 0.9659209847450256, avg_entr 0.05103975534439087, f1 0.8642105460166931
ep8_l3_test_time 0.13985267399999657
Test Epoch8 layer4 Acc 0.865, AUC 0.9625903367996216, avg_entr 0.04688069224357605, f1 0.8650000095367432
ep8_l4_test_time 0.16830580100000248
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 8
gc 0
Train Epoch9 Acc 0.9279416666666667 (111353/120000), AUC 0.9842188358306885
ep9_train_time 20.315666159000017
Test Epoch9 layer0 Acc 0.8328947368421052, AUC 0.9526576995849609, avg_entr 0.12904664874076843, f1 0.8328947424888611
ep9_l0_test_time 0.0669740130000207
Test Epoch9 layer1 Acc 0.861578947368421, AUC 0.9628255367279053, avg_entr 0.06109951436519623, f1 0.8615789413452148
ep9_l1_test_time 0.08175673000002348
Test Epoch9 layer2 Acc 0.868421052631579, AUC 0.9644159078598022, avg_entr 0.04827817529439926, f1 0.8684210777282715
ep9_l2_test_time 0.10386171299998637
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 9
Test Epoch9 layer3 Acc 0.8668421052631579, AUC 0.9625922441482544, avg_entr 0.042827852070331573, f1 0.8668420910835266
ep9_l3_test_time 0.14207511500001146
Test Epoch9 layer4 Acc 0.8681578947368421, AUC 0.9604718685150146, avg_entr 0.03929998353123665, f1 0.8681579232215881
ep9_l4_test_time 0.16867465400000015
gc 0
Train Epoch10 Acc 0.93555 (112266/120000), AUC 0.9863035678863525
ep10_train_time 20.338310890999992
Test Epoch10 layer0 Acc 0.8310526315789474, AUC 0.9522411227226257, avg_entr 0.11935634166002274, f1 0.8310526013374329
ep10_l0_test_time 0.06760025500000211
Test Epoch10 layer1 Acc 0.8555263157894737, AUC 0.9609266519546509, avg_entr 0.05442564934492111, f1 0.855526328086853
ep10_l1_test_time 0.0819102419999922
Test Epoch10 layer2 Acc 0.8628947368421053, AUC 0.9641397595405579, avg_entr 0.04317229986190796, f1 0.8628947138786316
ep10_l2_test_time 0.10411710399998242
Test Epoch10 layer3 Acc 0.8626315789473684, AUC 0.9636422991752625, avg_entr 0.03806842863559723, f1 0.862631618976593
ep10_l3_test_time 0.13440456400002176
Test Epoch10 layer4 Acc 0.8618421052631579, AUC 0.9627079963684082, avg_entr 0.03573635593056679, f1 0.8618420958518982
ep10_l4_test_time 0.1688902770000027
gc 0
Train Epoch11 Acc 0.9422916666666666 (113075/120000), AUC 0.9899551868438721
ep11_train_time 20.298675898000027
Test Epoch11 layer0 Acc 0.8307894736842105, AUC 0.9530975818634033, avg_entr 0.11474178731441498, f1 0.8307894468307495
ep11_l0_test_time 0.06786403199998858
Test Epoch11 layer1 Acc 0.8536842105263158, AUC 0.9581186175346375, avg_entr 0.05116172134876251, f1 0.8536841869354248
ep11_l1_test_time 0.08177832999996326
Test Epoch11 layer2 Acc 0.8576315789473684, AUC 0.9597086906433105, avg_entr 0.03990599513053894, f1 0.8576316237449646
ep11_l2_test_time 0.1038342669999679
Test Epoch11 layer3 Acc 0.8605263157894737, AUC 0.957621693611145, avg_entr 0.03295161575078964, f1 0.8605263233184814
ep11_l3_test_time 0.13308486800002584
Test Epoch11 layer4 Acc 0.8607894736842105, AUC 0.9542572498321533, avg_entr 0.0303950235247612, f1 0.8607894778251648
ep11_l4_test_time 0.16867155500000308
gc 0
Train Epoch12 Acc 0.9484833333333333 (113818/120000), AUC 0.9907301664352417
ep12_train_time 20.311391947000004
Test Epoch12 layer0 Acc 0.8281578947368421, AUC 0.9530302882194519, avg_entr 0.11016374081373215, f1 0.828157901763916
ep12_l0_test_time 0.06753144100002828
Test Epoch12 layer1 Acc 0.8594736842105263, AUC 0.9606319069862366, avg_entr 0.04768625646829605, f1 0.859473705291748
ep12_l1_test_time 0.08184253699999999
Test Epoch12 layer2 Acc 0.8707894736842106, AUC 0.9619592428207397, avg_entr 0.03467441350221634, f1 0.8707894682884216
ep12_l2_test_time 0.10396936600000117
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 12
Test Epoch12 layer3 Acc 0.868421052631579, AUC 0.9610604047775269, avg_entr 0.028561333194375038, f1 0.8684210777282715
ep12_l3_test_time 0.14022189000002072
Test Epoch12 layer4 Acc 0.8678947368421053, AUC 0.9593154191970825, avg_entr 0.02559497021138668, f1 0.86789470911026
ep12_l4_test_time 0.16826365599996507
gc 0
Train Epoch13 Acc 0.95485 (114582/120000), AUC 0.992455244064331
ep13_train_time 20.364710773000013
Test Epoch13 layer0 Acc 0.8297368421052631, AUC 0.9536459445953369, avg_entr 0.10394234955310822, f1 0.8297368288040161
ep13_l0_test_time 0.06708439400000543
Test Epoch13 layer1 Acc 0.855, AUC 0.9596055150032043, avg_entr 0.0477653369307518, f1 0.8550000190734863
ep13_l1_test_time 0.0820603109999638
Test Epoch13 layer2 Acc 0.8621052631578947, AUC 0.9600035548210144, avg_entr 0.0371873565018177, f1 0.8621052503585815
ep13_l2_test_time 0.10376278300003605
Test Epoch13 layer3 Acc 0.8636842105263158, AUC 0.9610475897789001, avg_entr 0.0309328380972147, f1 0.8636842370033264
ep13_l3_test_time 0.1328462179999974
Test Epoch13 layer4 Acc 0.8636842105263158, AUC 0.9587054252624512, avg_entr 0.028016306459903717, f1 0.8636842370033264
ep13_l4_test_time 0.16819524999999658
gc 0
Train Epoch14 Acc 0.95845 (115014/120000), AUC 0.9929874539375305
ep14_train_time 20.36839022700002
Test Epoch14 layer0 Acc 0.83, AUC 0.951313853263855, avg_entr 0.09926499426364899, f1 0.8299999833106995
ep14_l0_test_time 0.06777102900002774
Test Epoch14 layer1 Acc 0.8576315789473684, AUC 0.9576917886734009, avg_entr 0.042004168033599854, f1 0.8576316237449646
ep14_l1_test_time 0.08323066299999482
Test Epoch14 layer2 Acc 0.8623684210526316, AUC 0.9580442905426025, avg_entr 0.03088558465242386, f1 0.8623684048652649
ep14_l2_test_time 0.10398749199998747
Test Epoch14 layer3 Acc 0.8644736842105263, AUC 0.9576870203018188, avg_entr 0.024640822783112526, f1 0.8644736409187317
ep14_l3_test_time 0.1329724619999979
Test Epoch14 layer4 Acc 0.8642105263157894, AUC 0.9547242522239685, avg_entr 0.021874161437153816, f1 0.8642105460166931
ep14_l4_test_time 0.16911343399999623
gc 0
Train Epoch15 Acc 0.9630333333333333 (115564/120000), AUC 0.9946702122688293
ep15_train_time 20.455222402000004
Test Epoch15 layer0 Acc 0.8302631578947368, AUC 0.9510740041732788, avg_entr 0.09555996209383011, f1 0.8302631378173828
ep15_l0_test_time 0.06722971400000688
Test Epoch15 layer1 Acc 0.8557894736842105, AUC 0.9586712718009949, avg_entr 0.03673330694437027, f1 0.8557894825935364
ep15_l1_test_time 0.08166105600002993
Test Epoch15 layer2 Acc 0.8621052631578947, AUC 0.9580289125442505, avg_entr 0.030613403767347336, f1 0.8621052503585815
ep15_l2_test_time 0.10355172399999901
Test Epoch15 layer3 Acc 0.8605263157894737, AUC 0.9570310115814209, avg_entr 0.02542494609951973, f1 0.8605263233184814
ep15_l3_test_time 0.13275543499997866
Test Epoch15 layer4 Acc 0.8594736842105263, AUC 0.955855131149292, avg_entr 0.024067562073469162, f1 0.859473705291748
ep15_l4_test_time 0.168196303000002
gc 0
Train Epoch16 Acc 0.9650166666666666 (115802/120000), AUC 0.994931697845459
ep16_train_time 20.260550969000008
Test Epoch16 layer0 Acc 0.8257894736842105, AUC 0.9504112005233765, avg_entr 0.09265722334384918, f1 0.8257894515991211
ep16_l0_test_time 0.06872758200000817
Test Epoch16 layer1 Acc 0.8486842105263158, AUC 0.9538396596908569, avg_entr 0.03722798079252243, f1 0.8486841917037964
ep16_l1_test_time 0.08309616999997615
Test Epoch16 layer2 Acc 0.8597368421052631, AUC 0.9541081786155701, avg_entr 0.027577580884099007, f1 0.8597368597984314
ep16_l2_test_time 0.10391845099996999
Test Epoch16 layer3 Acc 0.8560526315789474, AUC 0.9537598490715027, avg_entr 0.022324493154883385, f1 0.8560526371002197
ep16_l3_test_time 0.13257291400003623
Test Epoch16 layer4 Acc 0.855, AUC 0.9487597942352295, avg_entr 0.020357225090265274, f1 0.8550000190734863
ep16_l4_test_time 0.16773816599999236
gc 0
Train Epoch17 Acc 0.9691083333333333 (116293/120000), AUC 0.9956451654434204
ep17_train_time 20.295086024
Test Epoch17 layer0 Acc 0.8294736842105264, AUC 0.949299156665802, avg_entr 0.08832176774740219, f1 0.829473614692688
ep17_l0_test_time 0.06743268300004956
Test Epoch17 layer1 Acc 0.8565789473684211, AUC 0.953136682510376, avg_entr 0.03587529808282852, f1 0.8565789461135864
ep17_l1_test_time 0.08187120799999548
Test Epoch17 layer2 Acc 0.8607894736842105, AUC 0.9556710124015808, avg_entr 0.02581874281167984, f1 0.8607894778251648
ep17_l2_test_time 0.1036159889999908
Test Epoch17 layer3 Acc 0.86, AUC 0.9534468650817871, avg_entr 0.019843116402626038, f1 0.8600000143051147
ep17_l3_test_time 0.1326574229999551
Test Epoch17 layer4 Acc 0.8586842105263158, AUC 0.9528800249099731, avg_entr 0.017826257273554802, f1 0.8586841821670532
ep17_l4_test_time 0.16854474199999459
gc 0
Train Epoch18 Acc 0.973175 (116781/120000), AUC 0.996375322341919
ep18_train_time 20.355406142999982
Test Epoch18 layer0 Acc 0.8302631578947368, AUC 0.9487013816833496, avg_entr 0.08218740671873093, f1 0.8302631378173828
ep18_l0_test_time 0.06713377800002718
Test Epoch18 layer1 Acc 0.8526315789473684, AUC 0.9494742155075073, avg_entr 0.03357992693781853, f1 0.8526315689086914
ep18_l1_test_time 0.08273404900000969
Test Epoch18 layer2 Acc 0.858421052631579, AUC 0.9495173096656799, avg_entr 0.02146674133837223, f1 0.8584210276603699
ep18_l2_test_time 0.10419551100000035
Test Epoch18 layer3 Acc 0.8576315789473684, AUC 0.9482703804969788, avg_entr 0.01668793149292469, f1 0.8576316237449646
ep18_l3_test_time 0.13305777700003318
Test Epoch18 layer4 Acc 0.8578947368421053, AUC 0.9450989961624146, avg_entr 0.014821373857557774, f1 0.8578947186470032
ep18_l4_test_time 0.16804332699996394
gc 0
Train Epoch19 Acc 0.97595 (117114/120000), AUC 0.9967541694641113
ep19_train_time 20.57731279799998
Test Epoch19 layer0 Acc 0.8271052631578948, AUC 0.9478132128715515, avg_entr 0.08217351138591766, f1 0.8271052837371826
ep19_l0_test_time 0.06738471400001345
Test Epoch19 layer1 Acc 0.8528947368421053, AUC 0.9496691823005676, avg_entr 0.032316818833351135, f1 0.8528947234153748
ep19_l1_test_time 0.08169456000001674
Test Epoch19 layer2 Acc 0.8597368421052631, AUC 0.9500883221626282, avg_entr 0.02165820635855198, f1 0.8597368597984314
ep19_l2_test_time 0.10383685199997217
Test Epoch19 layer3 Acc 0.858421052631579, AUC 0.9482074975967407, avg_entr 0.018780400976538658, f1 0.8584210276603699
ep19_l3_test_time 0.13300657800004956
Test Epoch19 layer4 Acc 0.8571052631578947, AUC 0.9452970027923584, avg_entr 0.016964871436357498, f1 0.8571051955223083
ep19_l4_test_time 0.1685017120000225
Best AUC tensor(0.8708) 12 2
train_as_loss [[4.53905294e+02 3.56642588e+02 3.51114268e+02 3.49863741e+02
  3.49385434e+02 3.49153905e+02 3.49025701e+02 3.48948294e+02
  3.48898700e+02 3.48865535e+02 3.48842642e+02 3.48826458e+02
  3.48814803e+02 3.48806295e+02 3.48800006e+02 3.48795314e+02
  3.48791804e+02 3.48789136e+02 3.48787553e+02 3.48786585e+02]
 [2.07987187e+00 1.69567883e-05 1.31084175e-06 2.73479384e-07
  2.39456486e-07 1.11646021e-06 1.90527570e-06 9.78181023e-06
  3.39966340e-05 8.39547874e-05 2.27295355e-04 2.83081469e-04
  2.86043831e-04 4.60750835e-06 1.19609195e-09 2.20414662e-05
  1.52392650e-04 8.75011048e-07 5.10418202e-10 5.15796558e-10]
 [1.61275310e+00 1.23267673e-05 1.03930890e-06 2.35852557e-07
  1.00793386e-07 5.48253590e-07 1.96043771e-06 2.39976075e-05
  1.17345877e-04 2.90673440e-04 7.05289113e-04 1.40325160e-04
  1.67625147e-04 2.52427240e-05 5.11743531e-09 1.17897794e-05
  1.07930351e-04 5.23052678e-06 2.93897959e-09 1.05096272e-09]
 [1.54077741e+00 1.32236856e-05 1.21900923e-06 3.38165115e-07
  1.41220843e-07 5.62579689e-07 2.26117130e-06 5.59233007e-05
  2.80382076e-04 6.78649116e-04 1.44336680e-03 1.31907068e-04
  1.66118819e-04 9.67804899e-05 3.27062414e-08 1.04518471e-05
  1.10412824e-04 2.80000845e-05 2.38186817e-08 3.05907379e-09]
 [1.64179052e+00 1.72984797e-05 1.82520001e-06 6.93497525e-07
  3.00708305e-07 6.45469647e-07 2.62859949e-06 1.04260466e-04
  4.57923316e-04 1.07965537e-03 2.12537872e-03 1.14293429e-04
  1.41829410e-04 1.83780707e-04 1.43985428e-07 8.19206544e-06
  1.04732596e-04 6.62473010e-05 1.31749727e-07 9.38697895e-09]]
train_ae_loss [[16.67813549 16.80287148 16.30287472 15.394836   14.83994143 14.51547633
  14.34947855 14.1421591  13.97997635 13.8072266  13.70056331 13.56770112
  13.44220892 13.35274639 13.19544526 13.10349933 13.02744461 12.97883413
  11.92257983 11.53219328]
 [14.63942831 14.16190723 13.21621663 12.25375817 11.21340655 10.33996964
   9.76621681  9.17512082  8.69632841  8.27404403  7.94713564  7.61870594
   7.30898768  6.66991948  6.34766021  6.16269727  5.96170271  5.62324133
   4.88970088  4.56624749]
 [15.4957752  14.15065156 12.52123368 11.37674023 10.23324287  9.27881345
   8.62274652  7.94387811  7.39131905  6.88114903  6.50916415  5.9439872
   5.60861307  4.99668377  4.63625632  4.30596397  4.10394039  3.79934901
   3.20664176  2.90632358]
 [15.39713211 13.91013221 11.46797475 10.27137968  9.22755137  8.35328584
   7.71299228  7.01229966  6.4471821   5.94577566  5.58205362  4.95705768
   4.64606413  4.09303547  3.75893907  3.42614386  3.26166781  2.98233652
   2.49594884  2.22855535]
 [14.63468704 14.1974223  10.50180768  9.24563125  8.23994456  7.4233458
   6.87396156  6.23293483  5.68678948  5.21701531  4.87404024  4.24229633
   3.9768481   3.48249553  3.20147754  2.88806992  2.7524356   2.51453325
   2.1054982   1.8667814 ]]
valid_acc (5, 20)
valid_AUC (5, 20)
train_acc (20,)
total_train+valid_time 425.062407895
Start Testing
Load ckpt at ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt
gc 9
Test layer0 Acc 0.8181578947368421, AUC 0.9497470259666443, avg_entr 0.11747924238443375, f1 0.818157970905304
l0_test_time 0.06629171600002337
gc 0
Test layer1 Acc 0.8452631578947368, AUC 0.9576489925384521, avg_entr 0.0554877445101738, f1 0.8452631831169128
l1_test_time 0.08342673300001024
gc 0
Test layer2 Acc 0.8536842105263158, AUC 0.9609208106994629, avg_entr 0.040351856499910355, f1 0.8536841869354248
l2_test_time 0.10541720700001633
gc 0
Test layer3 Acc 0.8547368421052631, AUC 0.9602378606796265, avg_entr 0.0337168388068676, f1 0.854736864566803
l3_test_time 0.1346527449999826
gc 0
Test layer4 Acc 0.8536842105263158, AUC 0.9570251703262329, avg_entr 0.03055788204073906, f1 0.8536841869354248
l4_test_time 0.1703740109999785
gc 0
Test threshold 0.1 Acc 0.8481578947368421, AUC 0.9542399048805237, avg_entr 0.026983076706528664, f1 0.8481578826904297
t0.1_test_time 0.12038676000003079
gc 0
Test threshold 0.2 Acc 0.8428947368421053, AUC 0.953822672367096, avg_entr 0.035160668194293976, f1 0.8428947329521179
t0.2_test_time 0.11778844099995922
gc 0
Test threshold 0.3 Acc 0.8336842105263158, AUC 0.9526916146278381, avg_entr 0.05050965026021004, f1 0.8336842060089111
t0.3_test_time 0.115255392999984
gc 0
Test threshold 0.4 Acc 0.8242105263157895, AUC 0.9506738781929016, avg_entr 0.06943722814321518, f1 0.8242105841636658
t0.4_test_time 0.09976719200000161
gc 0
Test threshold 0.5 Acc 0.8189473684210526, AUC 0.9499253630638123, avg_entr 0.08238175511360168, f1 0.8189473748207092
t0.5_test_time 0.08636772400001291
gc 0
Test threshold 0.6 Acc 0.8181578947368421, AUC 0.9497470259666443, avg_entr 0.08474336564540863, f1 0.818157970905304
t0.6_test_time 0.07832755499998711
gc 0
Test threshold 0.7 Acc 0.8181578947368421, AUC 0.9497470259666443, avg_entr 0.08474336564540863, f1 0.818157970905304
t0.7_test_time 0.07823432400005004
gc 0
Test threshold 0.8 Acc 0.8181578947368421, AUC 0.9497470259666443, avg_entr 0.08474336564540863, f1 0.818157970905304
t0.8_test_time 0.0784899159999668
gc 0
Test threshold 0.9 Acc 0.8181578947368421, AUC 0.9497470259666443, avg_entr 0.08474336564540863, f1 0.818157970905304
t0.9_test_time 0.07868122200000016
