total count words 102019
vocab size 30000
train size 120000, valid size 3800, test size 3800
found 26754 words in glove
model: TransformerALsideText(
  (layers): ModuleList(
    (0): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=4, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=4, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
  (emb_layers): ModuleList(
    (0): Embedding(30000, 300)
    (1): Embedding(30000, 300)
    (2): Embedding(30000, 300)
    (3): Embedding(30000, 300)
    (4): Embedding(30000, 300)
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.multi_headed_attention.linears.0.weight 90000
layers.0.enc.f.multi_headed_attention.linears.0.bias 300
layers.0.enc.f.multi_headed_attention.linears.1.weight 90000
layers.0.enc.f.multi_headed_attention.linears.1.bias 300
layers.0.enc.f.multi_headed_attention.linears.2.weight 90000
layers.0.enc.f.multi_headed_attention.linears.2.bias 300
layers.0.enc.f.multi_headed_attention.linears.3.weight 90000
layers.0.enc.f.multi_headed_attention.linears.3.bias 300
layers.0.enc.f.feed_forward.w_1.weight 90000
layers.0.enc.f.feed_forward.w_1.bias 300
layers.0.enc.f.feed_forward.w_2.weight 90000
layers.0.enc.f.feed_forward.w_2.bias 300
layers.0.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.0.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.0.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.0.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.0.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.0.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.0.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.0.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.0.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.0.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.0.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.0.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.0.enc.f.encoder.norm.a 300
layers.0.enc.f.encoder.norm.b 300
layers.0.ae.g.0.weight 512
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 512
layers.0.ae.h.0.bias 4
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
emb_layers.0.weight 9000000
emb_layers.1.weight 9000000
emb_layers.2.weight 9000000
emb_layers.3.weight 9000000
emb_layers.4.weight 9000000
Total Trainable Params: 50758892
init_time 15.156176604
Start Training
gc 0
Train Epoch0 Acc 0.26429166666666665 (31715/120000), AUC 0.510231614112854
ep0_train_time 20.960892886
Test Epoch0 layer0 Acc 0.6586842105263158, AUC 0.8723708391189575, avg_entr 0.8910453915596008, f1 0.6586841940879822
ep0_l0_test_time 0.06921733000000074
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 0
Test Epoch0 layer1 Acc 0.7255263157894737, AUC 0.9028588533401489, avg_entr 0.8564508557319641, f1 0.7255263328552246
ep0_l1_test_time 0.09184698099999622
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 0
Test Epoch0 layer2 Acc 0.7007894736842105, AUC 0.8805520534515381, avg_entr 0.9779245257377625, f1 0.7007894515991211
ep0_l2_test_time 0.11683834299999774
Test Epoch0 layer3 Acc 0.3605263157894737, AUC 0.8581151962280273, avg_entr 1.1012824773788452, f1 0.36052629351615906
ep0_l3_test_time 0.13243331399999647
Test Epoch0 layer4 Acc 0.24763157894736842, AUC 0.810758113861084, avg_entr 1.18417227268219, f1 0.24763157963752747
ep0_l4_test_time 0.1698298200000039
gc 0
Train Epoch1 Acc 0.6395666666666666 (76748/120000), AUC 0.850441575050354
ep1_train_time 20.447455840000003
Test Epoch1 layer0 Acc 0.7573684210526316, AUC 0.9211371541023254, avg_entr 0.512752115726471, f1 0.7573684453964233
ep1_l0_test_time 0.06747971300000444
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 1
Test Epoch1 layer1 Acc 0.7968421052631579, AUC 0.942270815372467, avg_entr 0.4529828429222107, f1 0.796842098236084
ep1_l1_test_time 0.0931991169999975
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 1
Test Epoch1 layer2 Acc 0.8002631578947368, AUC 0.945078432559967, avg_entr 0.4271204173564911, f1 0.8002631664276123
ep1_l2_test_time 0.11174374899999862
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 1
Test Epoch1 layer3 Acc 0.7944736842105263, AUC 0.9440845847129822, avg_entr 0.45083996653556824, f1 0.7944737076759338
ep1_l3_test_time 0.1411919309999945
Test Epoch1 layer4 Acc 0.7886842105263158, AUC 0.9437099099159241, avg_entr 0.5236850380897522, f1 0.7886841893196106
ep1_l4_test_time 0.16945652200000438
gc 0
Train Epoch2 Acc 0.8117666666666666 (97412/120000), AUC 0.9424775242805481
ep2_train_time 20.356493702
Test Epoch2 layer0 Acc 0.7836842105263158, AUC 0.9388499855995178, avg_entr 0.33799293637275696, f1 0.7836841940879822
ep2_l0_test_time 0.06682836999999608
Test Epoch2 layer1 Acc 0.8181578947368421, AUC 0.9551130533218384, avg_entr 0.27480122447013855, f1 0.818157970905304
ep2_l1_test_time 0.08196822700000439
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 2
Test Epoch2 layer2 Acc 0.8231578947368421, AUC 0.9568276405334473, avg_entr 0.25046274065971375, f1 0.8231579065322876
ep2_l2_test_time 0.1097322800000029
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 2
Test Epoch2 layer3 Acc 0.8210526315789474, AUC 0.9562565088272095, avg_entr 0.2365909069776535, f1 0.821052610874176
ep2_l3_test_time 0.13617922500000645
Test Epoch2 layer4 Acc 0.8176315789473684, AUC 0.9562317728996277, avg_entr 0.23891179263591766, f1 0.8176316618919373
ep2_l4_test_time 0.1689238359999905
gc 0
Train Epoch3 Acc 0.8555916666666666 (102671/120000), AUC 0.9591158628463745
ep3_train_time 20.348999097000004
Test Epoch3 layer0 Acc 0.8052631578947368, AUC 0.9445470571517944, avg_entr 0.2569616734981537, f1 0.8052631616592407
ep3_l0_test_time 0.06718068400000732
Test Epoch3 layer1 Acc 0.8294736842105264, AUC 0.9597629904747009, avg_entr 0.18261775374412537, f1 0.829473614692688
ep3_l1_test_time 0.0819005040000036
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 3
Test Epoch3 layer2 Acc 0.8326315789473684, AUC 0.9605180025100708, avg_entr 0.16425202786922455, f1 0.832631528377533
ep3_l2_test_time 0.12217176699999754
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 3
Test Epoch3 layer3 Acc 0.8294736842105264, AUC 0.958734393119812, avg_entr 0.14524048566818237, f1 0.829473614692688
ep3_l3_test_time 0.13879896800000324
Test Epoch3 layer4 Acc 0.8307894736842105, AUC 0.9582923650741577, avg_entr 0.13687002658843994, f1 0.8307894468307495
ep3_l4_test_time 0.16878364199999396
gc 0
Train Epoch4 Acc 0.8755166666666667 (105062/120000), AUC 0.9667982459068298
ep4_train_time 20.305689287000007
Test Epoch4 layer0 Acc 0.8160526315789474, AUC 0.946873128414154, avg_entr 0.21327470242977142, f1 0.8160526156425476
ep4_l0_test_time 0.06719875200001013
Test Epoch4 layer1 Acc 0.8436842105263158, AUC 0.9616639614105225, avg_entr 0.1289381980895996, f1 0.843684196472168
ep4_l1_test_time 0.08269524800000738
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 4
Test Epoch4 layer2 Acc 0.8497368421052631, AUC 0.9631829857826233, avg_entr 0.10547837615013123, f1 0.8497368693351746
ep4_l2_test_time 0.12107575900000711
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 4
Test Epoch4 layer3 Acc 0.8513157894736842, AUC 0.9623429775238037, avg_entr 0.09515358507633209, f1 0.8513157963752747
ep4_l3_test_time 0.1387900489999936
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 4
Test Epoch4 layer4 Acc 0.8528947368421053, AUC 0.9617669582366943, avg_entr 0.08812564611434937, f1 0.8528947234153748
ep4_l4_test_time 0.1793480980000055
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 4
gc 0
Train Epoch5 Acc 0.8909416666666666 (106913/120000), AUC 0.9721106886863708
ep5_train_time 20.409514149000003
Test Epoch5 layer0 Acc 0.8163157894736842, AUC 0.9484668374061584, avg_entr 0.18708910048007965, f1 0.8163158297538757
ep5_l0_test_time 0.06692516199998977
Test Epoch5 layer1 Acc 0.8518421052631578, AUC 0.9612784385681152, avg_entr 0.10544487833976746, f1 0.8518421053886414
ep5_l1_test_time 0.08151097000001073
Test Epoch5 layer2 Acc 0.8621052631578947, AUC 0.9645893573760986, avg_entr 0.08643260598182678, f1 0.8621052503585815
ep5_l2_test_time 0.10409967999999026
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 5
Test Epoch5 layer3 Acc 0.8623684210526316, AUC 0.9647760987281799, avg_entr 0.07844957709312439, f1 0.8623684048652649
ep5_l3_test_time 0.1410338389999879
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 5
Test Epoch5 layer4 Acc 0.861578947368421, AUC 0.9627691507339478, avg_entr 0.07533933967351913, f1 0.8615789413452148
ep5_l4_test_time 0.17489278200000058
gc 0
Train Epoch6 Acc 0.9027916666666667 (108335/120000), AUC 0.976915717124939
ep6_train_time 20.318769685999996
Test Epoch6 layer0 Acc 0.8157894736842105, AUC 0.9493518471717834, avg_entr 0.17057651281356812, f1 0.8157894611358643
ep6_l0_test_time 0.06712709300001052
Test Epoch6 layer1 Acc 0.853421052631579, AUC 0.9631372690200806, avg_entr 0.09204242378473282, f1 0.8534210324287415
ep6_l1_test_time 0.08143634699999325
Test Epoch6 layer2 Acc 0.861578947368421, AUC 0.9640936851501465, avg_entr 0.07312832027673721, f1 0.8615789413452148
ep6_l2_test_time 0.10413556899999321
Test Epoch6 layer3 Acc 0.8623684210526316, AUC 0.9640013575553894, avg_entr 0.06622539460659027, f1 0.8623684048652649
ep6_l3_test_time 0.1327571950000106
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 6
Test Epoch6 layer4 Acc 0.8613157894736843, AUC 0.9624472856521606, avg_entr 0.06101877987384796, f1 0.8613157868385315
ep6_l4_test_time 0.176400569000009
gc 0
Train Epoch7 Acc 0.9133666666666667 (109604/120000), AUC 0.97999107837677
ep7_train_time 20.32710562599999
Test Epoch7 layer0 Acc 0.8252631578947368, AUC 0.9504525065422058, avg_entr 0.15355467796325684, f1 0.8252631425857544
ep7_l0_test_time 0.06721357699998975
Test Epoch7 layer1 Acc 0.8489473684210527, AUC 0.9602475166320801, avg_entr 0.08012470602989197, f1 0.8489473462104797
ep7_l1_test_time 0.08185717800000702
Test Epoch7 layer2 Acc 0.8515789473684211, AUC 0.964259147644043, avg_entr 0.06341376155614853, f1 0.851578950881958
ep7_l2_test_time 0.10418975700000033
Test Epoch7 layer3 Acc 0.853421052631579, AUC 0.9640378355979919, avg_entr 0.05836771801114082, f1 0.8534210324287415
ep7_l3_test_time 0.13347715599999788
Test Epoch7 layer4 Acc 0.8494736842105263, AUC 0.962689220905304, avg_entr 0.05459285527467728, f1 0.8494736552238464
ep7_l4_test_time 0.16888433599999075
gc 0
Train Epoch8 Acc 0.9218666666666666 (110624/120000), AUC 0.9825026392936707
ep8_train_time 20.550922385999996
Test Epoch8 layer0 Acc 0.8239473684210527, AUC 0.951565146446228, avg_entr 0.14800919592380524, f1 0.8239473700523376
ep8_l0_test_time 0.06741380500000105
Test Epoch8 layer1 Acc 0.8460526315789474, AUC 0.96339350938797, avg_entr 0.0780305340886116, f1 0.8460525870323181
ep8_l1_test_time 0.08212800800001219
Test Epoch8 layer2 Acc 0.8523684210526316, AUC 0.9658414125442505, avg_entr 0.05992938578128815, f1 0.8523684144020081
ep8_l2_test_time 0.10450619499999902
Test Epoch8 layer3 Acc 0.8526315789473684, AUC 0.9639836549758911, avg_entr 0.053030699491500854, f1 0.8526315689086914
ep8_l3_test_time 0.13288836799998194
Test Epoch8 layer4 Acc 0.8507894736842105, AUC 0.9617334604263306, avg_entr 0.0494934506714344, f1 0.850789487361908
ep8_l4_test_time 0.1689940379999939
gc 0
Train Epoch9 Acc 0.9300416666666667 (111605/120000), AUC 0.9847699999809265
ep9_train_time 20.401556804999984
Test Epoch9 layer0 Acc 0.8260526315789474, AUC 0.9502602815628052, avg_entr 0.1362331360578537, f1 0.8260526061058044
ep9_l0_test_time 0.0672863659999905
Test Epoch9 layer1 Acc 0.8505263157894737, AUC 0.9609756469726562, avg_entr 0.06577073037624359, f1 0.8505263328552246
ep9_l1_test_time 0.081964653
Test Epoch9 layer2 Acc 0.8592105263157894, AUC 0.9623924493789673, avg_entr 0.04546637460589409, f1 0.8592105507850647
ep9_l2_test_time 0.10464487499999109
Test Epoch9 layer3 Acc 0.86, AUC 0.9625232219696045, avg_entr 0.03962603956460953, f1 0.8600000143051147
ep9_l3_test_time 0.13405482199999597
Test Epoch9 layer4 Acc 0.8586842105263158, AUC 0.9600890874862671, avg_entr 0.03493960201740265, f1 0.8586841821670532
ep9_l4_test_time 0.16961845099999096
gc 0
Train Epoch10 Acc 0.9371 (112452/120000), AUC 0.987994372844696
ep10_train_time 20.418080597
Test Epoch10 layer0 Acc 0.8289473684210527, AUC 0.9524362087249756, avg_entr 0.12274836748838425, f1 0.8289473652839661
ep10_l0_test_time 0.06745176500001548
Test Epoch10 layer1 Acc 0.8573684210526316, AUC 0.9618692398071289, avg_entr 0.05831345170736313, f1 0.8573684096336365
ep10_l1_test_time 0.08189255800002115
Test Epoch10 layer2 Acc 0.8647368421052631, AUC 0.9657014012336731, avg_entr 0.04156453534960747, f1 0.8647368550300598
ep10_l2_test_time 0.10431198199998448
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 10
Test Epoch10 layer3 Acc 0.8644736842105263, AUC 0.9649626612663269, avg_entr 0.03735259175300598, f1 0.8644736409187317
ep10_l3_test_time 0.14916381300000126
Test Epoch10 layer4 Acc 0.8644736842105263, AUC 0.9630125761032104, avg_entr 0.03386898338794708, f1 0.8644736409187317
ep10_l4_test_time 0.16920503000000053
gc 0
Train Epoch11 Acc 0.9441416666666667 (113297/120000), AUC 0.989744246006012
ep11_train_time 20.358799241000014
Test Epoch11 layer0 Acc 0.8213157894736842, AUC 0.9505828619003296, avg_entr 0.11884231120347977, f1 0.8213157653808594
ep11_l0_test_time 0.06777091000003566
Test Epoch11 layer1 Acc 0.8578947368421053, AUC 0.9608657360076904, avg_entr 0.05569663643836975, f1 0.8578947186470032
ep11_l1_test_time 0.08195664000004399
Test Epoch11 layer2 Acc 0.8626315789473684, AUC 0.9594123363494873, avg_entr 0.03673539310693741, f1 0.862631618976593
ep11_l2_test_time 0.10465297599995438
Test Epoch11 layer3 Acc 0.863421052631579, AUC 0.9592667818069458, avg_entr 0.03220590204000473, f1 0.8634210228919983
ep11_l3_test_time 0.1333780819999788
Test Epoch11 layer4 Acc 0.8631578947368421, AUC 0.9585086107254028, avg_entr 0.02801828272640705, f1 0.8631578683853149
ep11_l4_test_time 0.1691627550000021
gc 0
Train Epoch12 Acc 0.9482833333333334 (113794/120000), AUC 0.9903051853179932
ep12_train_time 20.346029636000026
Test Epoch12 layer0 Acc 0.8265789473684211, AUC 0.950833797454834, avg_entr 0.11575204133987427, f1 0.8265789747238159
ep12_l0_test_time 0.06714883199998667
Test Epoch12 layer1 Acc 0.8521052631578947, AUC 0.9602190256118774, avg_entr 0.0541682094335556, f1 0.8521052598953247
ep12_l1_test_time 0.08197362900000371
Test Epoch12 layer2 Acc 0.8573684210526316, AUC 0.96140056848526, avg_entr 0.03787540644407272, f1 0.8573684096336365
ep12_l2_test_time 0.1042249120000065
Test Epoch12 layer3 Acc 0.858421052631579, AUC 0.9588032364845276, avg_entr 0.03335948288440704, f1 0.8584210276603699
ep12_l3_test_time 0.13285199000000603
Test Epoch12 layer4 Acc 0.8573684210526316, AUC 0.9554848670959473, avg_entr 0.029642488807439804, f1 0.8573684096336365
ep12_l4_test_time 0.16846238900001254
gc 0
Train Epoch13 Acc 0.9556166666666667 (114674/120000), AUC 0.9927361011505127
ep13_train_time 20.29943123700002
Test Epoch13 layer0 Acc 0.8326315789473684, AUC 0.9512381553649902, avg_entr 0.10976093262434006, f1 0.832631528377533
ep13_l0_test_time 0.06847411000001102
Test Epoch13 layer1 Acc 0.8605263157894737, AUC 0.961047351360321, avg_entr 0.04988710954785347, f1 0.8605263233184814
ep13_l1_test_time 0.08179111300000841
Test Epoch13 layer2 Acc 0.868421052631579, AUC 0.9623180627822876, avg_entr 0.029407957568764687, f1 0.8684210777282715
ep13_l2_test_time 0.1041239300000143
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 13
Test Epoch13 layer3 Acc 0.8668421052631579, AUC 0.9607674479484558, avg_entr 0.02439943328499794, f1 0.8668420910835266
ep13_l3_test_time 0.14535885699996243
Test Epoch13 layer4 Acc 0.8678947368421053, AUC 0.961815595626831, avg_entr 0.020971547812223434, f1 0.86789470911026
ep13_l4_test_time 0.1694951119999928
gc 0
Train Epoch14 Acc 0.9603416666666666 (115241/120000), AUC 0.9934942722320557
ep14_train_time 20.351085091000016
Test Epoch14 layer0 Acc 0.8281578947368421, AUC 0.9494768381118774, avg_entr 0.10266631096601486, f1 0.828157901763916
ep14_l0_test_time 0.06717406299998174
Test Epoch14 layer1 Acc 0.8560526315789474, AUC 0.9599905610084534, avg_entr 0.04645637795329094, f1 0.8560526371002197
ep14_l1_test_time 0.08174032900001293
Test Epoch14 layer2 Acc 0.861578947368421, AUC 0.9600275754928589, avg_entr 0.025859812274575233, f1 0.8615789413452148
ep14_l2_test_time 0.1041990079999664
Test Epoch14 layer3 Acc 0.8631578947368421, AUC 0.9574501514434814, avg_entr 0.022792305797338486, f1 0.8631578683853149
ep14_l3_test_time 0.13266236300000855
Test Epoch14 layer4 Acc 0.861578947368421, AUC 0.9562799334526062, avg_entr 0.020486095920205116, f1 0.8615789413452148
ep14_l4_test_time 0.16878356299997677
gc 0
Train Epoch15 Acc 0.9662583333333333 (115951/120000), AUC 0.9950310587882996
ep15_train_time 20.415231000000006
Test Epoch15 layer0 Acc 0.8315789473684211, AUC 0.9518911242485046, avg_entr 0.10042904317378998, f1 0.8315790295600891
ep15_l0_test_time 0.06704646400004322
Test Epoch15 layer1 Acc 0.8576315789473684, AUC 0.9583271145820618, avg_entr 0.04609212651848793, f1 0.8576316237449646
ep15_l1_test_time 0.08185743999996475
Test Epoch15 layer2 Acc 0.8594736842105263, AUC 0.9585419297218323, avg_entr 0.028359033167362213, f1 0.859473705291748
ep15_l2_test_time 0.10409369199999219
Test Epoch15 layer3 Acc 0.8594736842105263, AUC 0.9569118618965149, avg_entr 0.025837648659944534, f1 0.859473705291748
ep15_l3_test_time 0.132850526000027
Test Epoch15 layer4 Acc 0.8607894736842105, AUC 0.955472469329834, avg_entr 0.022649837657809258, f1 0.8607894778251648
ep15_l4_test_time 0.1684887959999628
gc 0
Train Epoch16 Acc 0.9693333333333334 (116320/120000), AUC 0.995441734790802
ep16_train_time 20.31741873499999
Test Epoch16 layer0 Acc 0.8313157894736842, AUC 0.9510302543640137, avg_entr 0.09889412671327591, f1 0.831315815448761
ep16_l0_test_time 0.06710461400001577
Test Epoch16 layer1 Acc 0.8576315789473684, AUC 0.9614554643630981, avg_entr 0.043791547417640686, f1 0.8576316237449646
ep16_l1_test_time 0.08192822899997054
Test Epoch16 layer2 Acc 0.8613157894736843, AUC 0.9581084847450256, avg_entr 0.0248889047652483, f1 0.8613157868385315
ep16_l2_test_time 0.1041271469999856
Test Epoch16 layer3 Acc 0.8644736842105263, AUC 0.9582716822624207, avg_entr 0.022032033652067184, f1 0.8644736409187317
ep16_l3_test_time 0.13296384000000216
Test Epoch16 layer4 Acc 0.8644736842105263, AUC 0.9560867547988892, avg_entr 0.019285641610622406, f1 0.8644736409187317
ep16_l4_test_time 0.16898391399996626
gc 0
Train Epoch17 Acc 0.9727916666666667 (116735/120000), AUC 0.9962552189826965
ep17_train_time 20.58684309199998
Test Epoch17 layer0 Acc 0.8302631578947368, AUC 0.9496486186981201, avg_entr 0.0938636064529419, f1 0.8302631378173828
ep17_l0_test_time 0.06758792099998345
Test Epoch17 layer1 Acc 0.8586842105263158, AUC 0.9574731588363647, avg_entr 0.04150735214352608, f1 0.8586841821670532
ep17_l1_test_time 0.08187659999998687
Test Epoch17 layer2 Acc 0.865, AUC 0.9578830003738403, avg_entr 0.023274866864085197, f1 0.8650000095367432
ep17_l2_test_time 0.10436122599998043
Test Epoch17 layer3 Acc 0.865, AUC 0.9554167985916138, avg_entr 0.019408391788601875, f1 0.8650000095367432
ep17_l3_test_time 0.13286659999999983
Test Epoch17 layer4 Acc 0.8642105263157894, AUC 0.9550188183784485, avg_entr 0.016833702102303505, f1 0.8642105460166931
ep17_l4_test_time 0.16872989099999813
gc 0
Train Epoch18 Acc 0.9745333333333334 (116944/120000), AUC 0.9965529441833496
ep18_train_time 20.36255961500001
Test Epoch18 layer0 Acc 0.8302631578947368, AUC 0.9491912126541138, avg_entr 0.09143074601888657, f1 0.8302631378173828
ep18_l0_test_time 0.06694103099999893
Test Epoch18 layer1 Acc 0.8547368421052631, AUC 0.9587761163711548, avg_entr 0.038796599954366684, f1 0.854736864566803
ep18_l1_test_time 0.08187401999998656
Test Epoch18 layer2 Acc 0.8581578947368421, AUC 0.9539958238601685, avg_entr 0.02507077530026436, f1 0.8581578731536865
ep18_l2_test_time 0.10418262899997899
Test Epoch18 layer3 Acc 0.8592105263157894, AUC 0.9536059498786926, avg_entr 0.018617987632751465, f1 0.8592105507850647
ep18_l3_test_time 0.1332446910000158
Test Epoch18 layer4 Acc 0.8589473684210527, AUC 0.9504753351211548, avg_entr 0.016093146055936813, f1 0.8589473962783813
ep18_l4_test_time 0.17055032100000744
gc 0
Train Epoch19 Acc 0.9775333333333334 (117304/120000), AUC 0.9972598552703857
ep19_train_time 20.435904613000048
Test Epoch19 layer0 Acc 0.8323684210526315, AUC 0.9496196508407593, avg_entr 0.09031207114458084, f1 0.8323684930801392
ep19_l0_test_time 0.06783096800000976
Test Epoch19 layer1 Acc 0.855, AUC 0.9571378827095032, avg_entr 0.0386875756084919, f1 0.8550000190734863
ep19_l1_test_time 0.08215792900000451
Test Epoch19 layer2 Acc 0.8586842105263158, AUC 0.9542517066001892, avg_entr 0.020295657217502594, f1 0.8586841821670532
ep19_l2_test_time 0.1038288469999884
Test Epoch19 layer3 Acc 0.8607894736842105, AUC 0.9530150890350342, avg_entr 0.017957955598831177, f1 0.8607894778251648
ep19_l3_test_time 0.13273215899999968
Test Epoch19 layer4 Acc 0.8607894736842105, AUC 0.9512763619422913, avg_entr 0.01526804268360138, f1 0.8607894778251648
ep19_l4_test_time 0.1686406190000298
Best AUC tensor(0.8684) 13 2
train_as_loss [[4.56391977e+02 3.56704537e+02 3.51120392e+02 3.49864766e+02
  3.49385659e+02 3.49154053e+02 3.49025888e+02 3.48948525e+02
  3.48898952e+02 3.48865793e+02 3.48842891e+02 3.48826686e+02
  3.48815011e+02 3.48806474e+02 3.48800165e+02 3.48796427e+02
  3.48794199e+02 3.48792108e+02 3.48790211e+02 3.48788904e+02]
 [1.79884620e+00 2.15058273e-05 1.54139269e-06 3.03006897e-07
  8.98814869e-08 3.33338182e-08 2.45966228e-07 2.05834701e-06
  4.56735474e-05 7.01698377e-05 2.07290317e-04 3.88831644e-04
  2.81955553e-04 7.51254461e-06 1.11676576e-09 2.56235907e-06
  1.43825730e-04 1.33064829e-06 4.35182387e-10 5.65422600e-10]
 [1.81979577e+00 1.84507321e-05 1.35882769e-06 2.73960618e-07
  8.79117009e-08 3.67259242e-08 8.99707632e-08 2.12734480e-06
  1.01599278e-04 1.89759141e-04 6.08998122e-04 1.11779469e-03
  4.83491767e-04 1.73289022e-05 3.16240640e-09 2.25866144e-06
  3.12229452e-04 2.07232620e-06 1.21853114e-09 1.64024529e-09]
 [1.53755311e+00 1.40681231e-05 1.15728264e-06 2.83841365e-07
  1.19251585e-07 7.10819640e-08 1.33297766e-07 4.88558800e-06
  2.60047652e-04 4.60089797e-04 3.58900755e-04 7.11974831e-04
  3.25326709e-04 8.13096267e-05 2.16732755e-08 1.51451014e-06
  2.07148352e-04 9.20846768e-06 7.49106016e-09 2.77025795e-09]
 [1.83293969e+00 1.64385068e-05 1.63460436e-06 5.02689835e-07
  2.81232534e-07 2.09973789e-07 2.51874438e-07 7.39392214e-06
  5.54094433e-04 7.92422998e-04 3.27484877e-04 6.47623181e-04
  3.17525214e-04 2.69421890e-04 1.23687583e-07 1.27059955e-06
  1.92587354e-04 2.80531035e-05 5.25031851e-08 7.51399523e-09]]
train_ae_loss [[16.65796862 16.39609578 16.02846757 15.08138843 14.45204711 14.09072966
  13.82506236 13.66895919 13.47429505 13.30728182 13.193783   13.02485223
  12.93222866 12.83681477 12.74646368 11.76962527 11.47024003 11.36465661
  11.31960919 10.77532082]
 [13.96346718 13.32834393 12.61130779 11.53630217 10.42278268  9.59827594
   8.91367414  8.41505348  7.97152529  7.60771046  7.20019446  6.84125745
   6.5310724   5.91383762  5.65465522  4.96683179  4.65092039  4.29557252
   4.15149522  3.78644711]
 [15.15371927 12.98593873 11.41728535 10.11188615  9.04738553  8.29674004
   7.5663101   7.00427576  6.51066419  6.11803191  5.70857877  5.3074388
   4.99230159  4.29645776  4.00810439  3.43921719  3.19087682  2.80551519
   2.66532918  2.38829907]
 [14.42750833 12.72771293 10.04336275  8.53291647  7.55364291  6.85695024
   6.20628327  5.7410212   5.3187989   4.95624048  4.48126766  4.14475174
   3.89783296  3.34265707  3.08633872  2.57876837  2.3927306   2.10335981
   1.97284125  1.75378667]
 [14.35123656 14.39056907 10.09197603  8.46454104  7.59983417  6.9355645
   6.24431441  5.70192458  5.24808562  4.85499881  4.29245829  3.9533683
   3.71331127  3.16686738  2.9083238   2.41103532  2.23482556  1.95872152
   1.82867512  1.61804524]]
valid_acc (5, 20)
valid_AUC (5, 20)
train_acc (20,)
total_train+valid_time 425.066920326
Start Testing
Load ckpt at ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt
gc 9
Test layer0 Acc 0.8239473684210527, AUC 0.9502695798873901, avg_entr 0.11011514812707901, f1 0.8239473700523376
l0_test_time 0.06651748400003044
gc 0
Test layer1 Acc 0.8521052631578947, AUC 0.9576613903045654, avg_entr 0.047123223543167114, f1 0.8521052598953247
l1_test_time 0.08300745599996162
gc 0
Test layer2 Acc 0.858421052631579, AUC 0.9605357646942139, avg_entr 0.03143687546253204, f1 0.8584210276603699
l2_test_time 0.10577836299995624
gc 0
Test layer3 Acc 0.8592105263157894, AUC 0.9582154750823975, avg_entr 0.026604652404785156, f1 0.8592105507850647
l3_test_time 0.13457238499995583
gc 0
Test layer4 Acc 0.8597368421052631, AUC 0.9596946835517883, avg_entr 0.023346180096268654, f1 0.8597368597984314
l4_test_time 0.17153774499996643
gc 0
Test threshold 0.1 Acc 0.8547368421052631, AUC 0.9558978080749512, avg_entr 0.02397041581571102, f1 0.854736864566803
t0.1_test_time 0.12017729799998733
gc 0
Test threshold 0.2 Acc 0.848421052631579, AUC 0.9546045064926147, avg_entr 0.033728115260601044, f1 0.848421037197113
t0.2_test_time 0.11763533300000972
gc 0
Test threshold 0.3 Acc 0.8376315789473684, AUC 0.9529666900634766, avg_entr 0.04924950748682022, f1 0.8376315832138062
t0.3_test_time 0.11119235600000366
gc 0
Test threshold 0.4 Acc 0.8305263157894737, AUC 0.9517731070518494, avg_entr 0.06752308458089828, f1 0.8305262923240662
t0.4_test_time 0.09907910200001879
gc 0
Test threshold 0.5 Acc 0.825, AUC 0.9507399201393127, avg_entr 0.07720987498760223, f1 0.824999988079071
t0.5_test_time 0.08723732900000414
gc 0
Test threshold 0.6 Acc 0.8239473684210527, AUC 0.9502695798873901, avg_entr 0.07943128794431686, f1 0.8239473700523376
t0.6_test_time 0.07867791900002885
gc 0
Test threshold 0.7 Acc 0.8239473684210527, AUC 0.9502695798873901, avg_entr 0.07943128794431686, f1 0.8239473700523376
t0.7_test_time 0.07888217300001088
gc 0
Test threshold 0.8 Acc 0.8239473684210527, AUC 0.9502695798873901, avg_entr 0.07943128794431686, f1 0.8239473700523376
t0.8_test_time 0.07856387399999676
gc 0
Test threshold 0.9 Acc 0.8239473684210527, AUC 0.9502695798873901, avg_entr 0.07943128794431686, f1 0.8239473700523376
t0.9_test_time 0.07866654200000767
