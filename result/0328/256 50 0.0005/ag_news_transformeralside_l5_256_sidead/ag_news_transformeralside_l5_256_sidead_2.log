total count words 102019
vocab size 30000
train size 120000, valid size 3800, test size 3800
found 26754 words in glove
model: TransformerALsideText(
  (layers): ModuleList(
    (0): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=4, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=4, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
  (emb_layers): ModuleList(
    (0): Embedding(30000, 300)
    (1): Embedding(30000, 300)
    (2): Embedding(30000, 300)
    (3): Embedding(30000, 300)
    (4): Embedding(30000, 300)
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.multi_headed_attention.linears.0.weight 90000
layers.0.enc.f.multi_headed_attention.linears.0.bias 300
layers.0.enc.f.multi_headed_attention.linears.1.weight 90000
layers.0.enc.f.multi_headed_attention.linears.1.bias 300
layers.0.enc.f.multi_headed_attention.linears.2.weight 90000
layers.0.enc.f.multi_headed_attention.linears.2.bias 300
layers.0.enc.f.multi_headed_attention.linears.3.weight 90000
layers.0.enc.f.multi_headed_attention.linears.3.bias 300
layers.0.enc.f.feed_forward.w_1.weight 90000
layers.0.enc.f.feed_forward.w_1.bias 300
layers.0.enc.f.feed_forward.w_2.weight 90000
layers.0.enc.f.feed_forward.w_2.bias 300
layers.0.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.0.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.0.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.0.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.0.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.0.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.0.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.0.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.0.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.0.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.0.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.0.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.0.enc.f.encoder.norm.a 300
layers.0.enc.f.encoder.norm.b 300
layers.0.ae.g.0.weight 512
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 512
layers.0.ae.h.0.bias 4
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
emb_layers.0.weight 9000000
emb_layers.1.weight 9000000
emb_layers.2.weight 9000000
emb_layers.3.weight 9000000
emb_layers.4.weight 9000000
Total Trainable Params: 50758892
init_time 25.333318416
Start Training
gc 0
Train Epoch0 Acc 0.282125 (33855/120000), AUC 0.5307342410087585
ep0_train_time 28.104701579
Test Epoch0 layer0 Acc 0.6639473684210526, AUC 0.8711545467376709, avg_entr 0.8967963457107544, f1 0.663947343826294
ep0_l0_test_time 0.15982560200000506
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 0
Test Epoch0 layer1 Acc 0.6771052631578948, AUC 0.8962419629096985, avg_entr 0.8323367834091187, f1 0.6771052479743958
ep0_l1_test_time 0.17807063900000486
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 0
Test Epoch0 layer2 Acc 0.6202631578947368, AUC 0.889997124671936, avg_entr 0.8530765771865845, f1 0.6202631592750549
ep0_l2_test_time 0.19714105100000268
Test Epoch0 layer3 Acc 0.36447368421052634, AUC 0.871183454990387, avg_entr 0.8549829125404358, f1 0.3644736409187317
ep0_l3_test_time 0.22382956900000295
Test Epoch0 layer4 Acc 0.3476315789473684, AUC 0.8136298656463623, avg_entr 1.0836248397827148, f1 0.347631573677063
ep0_l4_test_time 0.23291677199999583
gc 0
Train Epoch1 Acc 0.6681833333333334 (80182/120000), AUC 0.8631372451782227
ep1_train_time 25.437782708000007
Test Epoch1 layer0 Acc 0.7602631578947369, AUC 0.9217095971107483, avg_entr 0.5014350414276123, f1 0.7602631449699402
ep1_l0_test_time 0.1570162900000014
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 1
Test Epoch1 layer1 Acc 0.7928947368421052, AUC 0.940189003944397, avg_entr 0.41262882947921753, f1 0.792894721031189
ep1_l1_test_time 0.15089257099999998
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 1
Test Epoch1 layer2 Acc 0.7889473684210526, AUC 0.9431263208389282, avg_entr 0.4067898690700531, f1 0.788947343826294
ep1_l2_test_time 0.16872452300000873
Test Epoch1 layer3 Acc 0.786578947368421, AUC 0.9436489939689636, avg_entr 0.4109291136264801, f1 0.7865789532661438
ep1_l3_test_time 0.2214612020000004
Test Epoch1 layer4 Acc 0.7828947368421053, AUC 0.9432424306869507, avg_entr 0.42568525671958923, f1 0.7828947305679321
ep1_l4_test_time 0.2656931610000015
gc 0
Train Epoch2 Acc 0.81155 (97386/120000), AUC 0.9414900541305542
ep2_train_time 25.905372352
Test Epoch2 layer0 Acc 0.7926315789473685, AUC 0.9401535391807556, avg_entr 0.3380153477191925, f1 0.7926315665245056
ep2_l0_test_time 0.1263893399999887
Test Epoch2 layer1 Acc 0.8115789473684211, AUC 0.9519978165626526, avg_entr 0.2650275230407715, f1 0.8115789294242859
ep2_l1_test_time 0.1404810339999898
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 2
Test Epoch2 layer2 Acc 0.8052631578947368, AUC 0.9540210962295532, avg_entr 0.2381594330072403, f1 0.8052631616592407
ep2_l2_test_time 0.16453850099999556
Test Epoch2 layer3 Acc 0.8018421052631579, AUC 0.9535847902297974, avg_entr 0.2200157791376114, f1 0.8018420338630676
ep2_l3_test_time 0.1948581059999981
Test Epoch2 layer4 Acc 0.8010526315789473, AUC 0.9518103003501892, avg_entr 0.2191995084285736, f1 0.8010526299476624
ep2_l4_test_time 0.23022770599999376
gc 0
Train Epoch3 Acc 0.8517166666666667 (102206/120000), AUC 0.9593362808227539
ep3_train_time 26.25560859800001
Test Epoch3 layer0 Acc 0.8110526315789474, AUC 0.9467735886573792, avg_entr 0.2582845389842987, f1 0.8110526204109192
ep3_l0_test_time 0.12154583700001353
Test Epoch3 layer1 Acc 0.8428947368421053, AUC 0.9600588083267212, avg_entr 0.1746145635843277, f1 0.8428947329521179
ep3_l1_test_time 0.14043578999999795
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 3
Test Epoch3 layer2 Acc 0.8468421052631578, AUC 0.9616995453834534, avg_entr 0.14977090060710907, f1 0.8468421101570129
ep3_l2_test_time 0.16098910499999874
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 3
Test Epoch3 layer3 Acc 0.8460526315789474, AUC 0.962544322013855, avg_entr 0.14002838730812073, f1 0.8460525870323181
ep3_l3_test_time 0.19460237800001323
Test Epoch3 layer4 Acc 0.845, AUC 0.9610002636909485, avg_entr 0.14220228791236877, f1 0.8449999690055847
ep3_l4_test_time 0.23167999100002135
gc 0
Train Epoch4 Acc 0.874425 (104931/120000), AUC 0.9666457176208496
ep4_train_time 26.136994079999994
Test Epoch4 layer0 Acc 0.8139473684210526, AUC 0.9493118524551392, avg_entr 0.21902363002300262, f1 0.8139473795890808
ep4_l0_test_time 0.12559828199999856
Test Epoch4 layer1 Acc 0.8463157894736842, AUC 0.95897376537323, avg_entr 0.12707111239433289, f1 0.8463158011436462
ep4_l1_test_time 0.14317973699999698
Test Epoch4 layer2 Acc 0.8481578947368421, AUC 0.9621720314025879, avg_entr 0.11304385215044022, f1 0.8481578826904297
ep4_l2_test_time 0.16395782000000736
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 4
Test Epoch4 layer3 Acc 0.8452631578947368, AUC 0.9630252122879028, avg_entr 0.10543348640203476, f1 0.8452631831169128
ep4_l3_test_time 0.1976360429999886
Test Epoch4 layer4 Acc 0.8426315789473684, AUC 0.960835337638855, avg_entr 0.10097678750753403, f1 0.8426315784454346
ep4_l4_test_time 0.2308231840000019
gc 0
Train Epoch5 Acc 0.8886583333333333 (106639/120000), AUC 0.970253050327301
ep5_train_time 25.563470010999993
Test Epoch5 layer0 Acc 0.8234210526315789, AUC 0.9513903856277466, avg_entr 0.18878589570522308, f1 0.823421061038971
ep5_l0_test_time 0.12726972700002648
Test Epoch5 layer1 Acc 0.855, AUC 0.964234471321106, avg_entr 0.10128236562013626, f1 0.8550000190734863
ep5_l1_test_time 0.14095318099998622
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 5
Test Epoch5 layer2 Acc 0.8586842105263158, AUC 0.9661414623260498, avg_entr 0.08303479105234146, f1 0.8586841821670532
ep5_l2_test_time 0.16458156499999177
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 5
Test Epoch5 layer3 Acc 0.8610526315789474, AUC 0.9665563106536865, avg_entr 0.07787179946899414, f1 0.8610526323318481
ep5_l3_test_time 0.1994330370000057
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 5
Test Epoch5 layer4 Acc 0.86, AUC 0.9647383093833923, avg_entr 0.07148401439189911, f1 0.8600000143051147
ep5_l4_test_time 0.26343834000002175
gc 0
Train Epoch6 Acc 0.901225 (108147/120000), AUC 0.9758322238922119
ep6_train_time 25.788896668999996
Test Epoch6 layer0 Acc 0.8223684210526315, AUC 0.952810525894165, avg_entr 0.1737525910139084, f1 0.8223684430122375
ep6_l0_test_time 0.1252232410000147
Test Epoch6 layer1 Acc 0.858421052631579, AUC 0.9645615220069885, avg_entr 0.08667472749948502, f1 0.8584210276603699
ep6_l1_test_time 0.14102089799999362
Test Epoch6 layer2 Acc 0.8647368421052631, AUC 0.9677143692970276, avg_entr 0.07284696400165558, f1 0.8647368550300598
ep6_l2_test_time 0.16732740800000556
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 6
Test Epoch6 layer3 Acc 0.8623684210526316, AUC 0.9677491784095764, avg_entr 0.06893229484558105, f1 0.8623684048652649
ep6_l3_test_time 0.19892173799999568
Test Epoch6 layer4 Acc 0.861578947368421, AUC 0.9625644087791443, avg_entr 0.06366967409849167, f1 0.8615789413452148
ep6_l4_test_time 0.2273276790000125
gc 0
Train Epoch7 Acc 0.9118166666666667 (109418/120000), AUC 0.9786579608917236
ep7_train_time 26.876849816999993
Test Epoch7 layer0 Acc 0.8255263157894737, AUC 0.9532210826873779, avg_entr 0.15687882900238037, f1 0.825526237487793
ep7_l0_test_time 0.12827731400000175
Test Epoch7 layer1 Acc 0.8578947368421053, AUC 0.9650498032569885, avg_entr 0.07350897043943405, f1 0.8578947186470032
ep7_l1_test_time 0.13891386899999247
Test Epoch7 layer2 Acc 0.8642105263157894, AUC 0.9672891497612, avg_entr 0.06037662550806999, f1 0.8642105460166931
ep7_l2_test_time 0.16807280000000446
Test Epoch7 layer3 Acc 0.8657894736842106, AUC 0.9662125110626221, avg_entr 0.05399384722113609, f1 0.8657894730567932
ep7_l3_test_time 0.1975359940000203
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 7
Test Epoch7 layer4 Acc 0.8663157894736843, AUC 0.9641786813735962, avg_entr 0.04789560288190842, f1 0.8663158416748047
ep7_l4_test_time 0.23512730699999906
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 7
gc 0
Train Epoch8 Acc 0.9210833333333334 (110530/120000), AUC 0.9827858209609985
ep8_train_time 25.77663665700001
Test Epoch8 layer0 Acc 0.8260526315789474, AUC 0.9528944492340088, avg_entr 0.14531072974205017, f1 0.8260526061058044
ep8_l0_test_time 0.12613382000000684
Test Epoch8 layer1 Acc 0.8576315789473684, AUC 0.9625149965286255, avg_entr 0.06345584988594055, f1 0.8576316237449646
ep8_l1_test_time 0.14193487900001855
Test Epoch8 layer2 Acc 0.8626315789473684, AUC 0.9657586216926575, avg_entr 0.05133121460676193, f1 0.862631618976593
ep8_l2_test_time 0.1690197790000525
Test Epoch8 layer3 Acc 0.8628947368421053, AUC 0.9649139046669006, avg_entr 0.045940812677145004, f1 0.8628947138786316
ep8_l3_test_time 0.19718010699995148
Test Epoch8 layer4 Acc 0.8623684210526316, AUC 0.9586984515190125, avg_entr 0.04010036587715149, f1 0.8623684048652649
ep8_l4_test_time 0.22934068299997534
gc 0
Train Epoch9 Acc 0.92815 (111378/120000), AUC 0.9843998551368713
ep9_train_time 25.979165010999964
Test Epoch9 layer0 Acc 0.8307894736842105, AUC 0.9515901803970337, avg_entr 0.13434793055057526, f1 0.8307894468307495
ep9_l0_test_time 0.12967923200000087
Test Epoch9 layer1 Acc 0.8657894736842106, AUC 0.9632759094238281, avg_entr 0.06092273071408272, f1 0.8657894730567932
ep9_l1_test_time 0.14189470200000187
Test Epoch9 layer2 Acc 0.8673684210526316, AUC 0.9646670818328857, avg_entr 0.047637615352869034, f1 0.8673684000968933
ep9_l2_test_time 0.16617497999999387
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 9
Test Epoch9 layer3 Acc 0.8663157894736843, AUC 0.9653847217559814, avg_entr 0.044217512011528015, f1 0.8663158416748047
ep9_l3_test_time 0.1933869019999861
Test Epoch9 layer4 Acc 0.8642105263157894, AUC 0.963246762752533, avg_entr 0.039756666868925095, f1 0.8642105460166931
ep9_l4_test_time 0.23353846999998495
gc 0
Train Epoch10 Acc 0.9356916666666667 (112283/120000), AUC 0.9876253008842468
ep10_train_time 25.481274971999994
Test Epoch10 layer0 Acc 0.828421052631579, AUC 0.9524577856063843, avg_entr 0.12865084409713745, f1 0.8284210562705994
ep10_l0_test_time 0.13152735599999232
Test Epoch10 layer1 Acc 0.8555263157894737, AUC 0.9645880460739136, avg_entr 0.056736111640930176, f1 0.855526328086853
ep10_l1_test_time 0.14650827299999492
Test Epoch10 layer2 Acc 0.8639473684210527, AUC 0.9645752906799316, avg_entr 0.04296368733048439, f1 0.8639474511146545
ep10_l2_test_time 0.17479379700000663
Test Epoch10 layer3 Acc 0.8613157894736843, AUC 0.9641069769859314, avg_entr 0.037420254200696945, f1 0.8613157868385315
ep10_l3_test_time 0.2003462580000246
Test Epoch10 layer4 Acc 0.8607894736842105, AUC 0.96314537525177, avg_entr 0.033757440745830536, f1 0.8607894778251648
ep10_l4_test_time 0.23737667799997553
gc 0
Train Epoch11 Acc 0.9426916666666667 (113123/120000), AUC 0.9899997115135193
ep11_train_time 25.646092655000018
Test Epoch11 layer0 Acc 0.8278947368421052, AUC 0.9518926739692688, avg_entr 0.12142404168844223, f1 0.8278947472572327
ep11_l0_test_time 0.12419514900000195
Test Epoch11 layer1 Acc 0.8465789473684211, AUC 0.9591376781463623, avg_entr 0.054411206394433975, f1 0.8465789556503296
ep11_l1_test_time 0.14325170600000092
Test Epoch11 layer2 Acc 0.8518421052631578, AUC 0.9583679437637329, avg_entr 0.03994926065206528, f1 0.8518421053886414
ep11_l2_test_time 0.16704906100000017
Test Epoch11 layer3 Acc 0.8518421052631578, AUC 0.9573290348052979, avg_entr 0.03457843139767647, f1 0.8518421053886414
ep11_l3_test_time 0.1931624630000215
Test Epoch11 layer4 Acc 0.8523684210526316, AUC 0.9563852548599243, avg_entr 0.03045588918030262, f1 0.8523684144020081
ep11_l4_test_time 0.23234933099996624
gc 0
Train Epoch12 Acc 0.95235 (114282/120000), AUC 0.9918317794799805
ep12_train_time 25.807935876999977
Test Epoch12 layer0 Acc 0.8268421052631579, AUC 0.9530900716781616, avg_entr 0.11526334285736084, f1 0.8268421292304993
ep12_l0_test_time 0.12301163500001167
Test Epoch12 layer1 Acc 0.8557894736842105, AUC 0.9612383842468262, avg_entr 0.04916764795780182, f1 0.8557894825935364
ep12_l1_test_time 0.1419720470000243
Test Epoch12 layer2 Acc 0.8613157894736843, AUC 0.9621122479438782, avg_entr 0.03554409742355347, f1 0.8613157868385315
ep12_l2_test_time 0.1647811369999772
Test Epoch12 layer3 Acc 0.8605263157894737, AUC 0.9606995582580566, avg_entr 0.030194930732250214, f1 0.8605263233184814
ep12_l3_test_time 0.19788551400000642
Test Epoch12 layer4 Acc 0.8605263157894737, AUC 0.9579309821128845, avg_entr 0.0262661911547184, f1 0.8605263233184814
ep12_l4_test_time 0.23045890299999883
gc 0
Train Epoch13 Acc 0.95705 (114846/120000), AUC 0.9927240610122681
ep13_train_time 26.216595944999995
Test Epoch13 layer0 Acc 0.8321052631578948, AUC 0.9512267708778381, avg_entr 0.10854782164096832, f1 0.832105278968811
ep13_l0_test_time 0.12987552499998856
Test Epoch13 layer1 Acc 0.8547368421052631, AUC 0.9588882923126221, avg_entr 0.04385504499077797, f1 0.854736864566803
ep13_l1_test_time 0.14233384099998148
Test Epoch13 layer2 Acc 0.8581578947368421, AUC 0.9584574699401855, avg_entr 0.03186763450503349, f1 0.8581578731536865
ep13_l2_test_time 0.15945629499998404
Test Epoch13 layer3 Acc 0.8613157894736843, AUC 0.9581766724586487, avg_entr 0.025894751772284508, f1 0.8613157868385315
ep13_l3_test_time 0.19894866499998898
Test Epoch13 layer4 Acc 0.8605263157894737, AUC 0.9550731778144836, avg_entr 0.02252611145377159, f1 0.8605263233184814
ep13_l4_test_time 0.23469796699998824
gc 0
Train Epoch14 Acc 0.96065 (115278/120000), AUC 0.9939337968826294
ep14_train_time 25.87745110200001
Test Epoch14 layer0 Acc 0.8292105263157895, AUC 0.9518604278564453, avg_entr 0.10597572475671768, f1 0.8292105197906494
ep14_l0_test_time 0.12346108899998853
Test Epoch14 layer1 Acc 0.8557894736842105, AUC 0.9578598141670227, avg_entr 0.04413861408829689, f1 0.8557894825935364
ep14_l1_test_time 0.14272730499999398
Test Epoch14 layer2 Acc 0.8621052631578947, AUC 0.9586217403411865, avg_entr 0.02981296367943287, f1 0.8621052503585815
ep14_l2_test_time 0.16479068199998892
Test Epoch14 layer3 Acc 0.8623684210526316, AUC 0.9591686129570007, avg_entr 0.02574881911277771, f1 0.8623684048652649
ep14_l3_test_time 0.1955626370000232
Test Epoch14 layer4 Acc 0.8626315789473684, AUC 0.9578733444213867, avg_entr 0.02301779016852379, f1 0.862631618976593
ep14_l4_test_time 0.2299421899999743
gc 0
Train Epoch15 Acc 0.9651 (115812/120000), AUC 0.9952749013900757
ep15_train_time 25.695962597000005
Test Epoch15 layer0 Acc 0.8321052631578948, AUC 0.9521899223327637, avg_entr 0.105027936398983, f1 0.832105278968811
ep15_l0_test_time 0.127814160000014
Test Epoch15 layer1 Acc 0.8560526315789474, AUC 0.9578096866607666, avg_entr 0.040836650878190994, f1 0.8560526371002197
ep15_l1_test_time 0.14106967000003579
Test Epoch15 layer2 Acc 0.8602631578947368, AUC 0.9582352638244629, avg_entr 0.0265186820179224, f1 0.8602631688117981
ep15_l2_test_time 0.16886002799998323
Test Epoch15 layer3 Acc 0.8626315789473684, AUC 0.9602428674697876, avg_entr 0.023378731682896614, f1 0.862631618976593
ep15_l3_test_time 0.19185835299998644
Test Epoch15 layer4 Acc 0.863421052631579, AUC 0.9571429491043091, avg_entr 0.020488707348704338, f1 0.8634210228919983
ep15_l4_test_time 0.2331026159999965
gc 0
Train Epoch16 Acc 0.9689083333333334 (116269/120000), AUC 0.9955780506134033
ep16_train_time 25.38276318999999
Test Epoch16 layer0 Acc 0.8257894736842105, AUC 0.9502657055854797, avg_entr 0.10094086825847626, f1 0.8257894515991211
ep16_l0_test_time 0.15846285300000318
Test Epoch16 layer1 Acc 0.8544736842105263, AUC 0.9583583474159241, avg_entr 0.040862418711185455, f1 0.8544737100601196
ep16_l1_test_time 0.17362565400003405
Test Epoch16 layer2 Acc 0.8597368421052631, AUC 0.9556370973587036, avg_entr 0.02837235853075981, f1 0.8597368597984314
ep16_l2_test_time 0.19845997099997703
Test Epoch16 layer3 Acc 0.8605263157894737, AUC 0.9566029906272888, avg_entr 0.0235474593937397, f1 0.8605263233184814
ep16_l3_test_time 0.2273277589999907
Test Epoch16 layer4 Acc 0.86, AUC 0.9541894197463989, avg_entr 0.020847974345088005, f1 0.8600000143051147
ep16_l4_test_time 0.26079482700004064
gc 0
Train Epoch17 Acc 0.9715833333333334 (116590/120000), AUC 0.9960358738899231
ep17_train_time 25.801629702999946
Test Epoch17 layer0 Acc 0.8286842105263158, AUC 0.950886607170105, avg_entr 0.10017550736665726, f1 0.8286842107772827
ep17_l0_test_time 0.12189494500000819
Test Epoch17 layer1 Acc 0.8513157894736842, AUC 0.9553031921386719, avg_entr 0.041478607803583145, f1 0.8513157963752747
ep17_l1_test_time 0.14288714499991784
Test Epoch17 layer2 Acc 0.8568421052631578, AUC 0.9550968408584595, avg_entr 0.028396690264344215, f1 0.8568421006202698
ep17_l2_test_time 0.16133654999998726
Test Epoch17 layer3 Acc 0.858421052631579, AUC 0.9566270709037781, avg_entr 0.02521047741174698, f1 0.8584210276603699
ep17_l3_test_time 0.1940163560000201
Test Epoch17 layer4 Acc 0.8581578947368421, AUC 0.9560367465019226, avg_entr 0.022732848301529884, f1 0.8581578731536865
ep17_l4_test_time 0.23382522499991865
gc 0
Train Epoch18 Acc 0.9734083333333333 (116809/120000), AUC 0.9964978098869324
ep18_train_time 26.404884913999922
Test Epoch18 layer0 Acc 0.8315789473684211, AUC 0.9504063129425049, avg_entr 0.09907301515340805, f1 0.8315790295600891
ep18_l0_test_time 0.12878720900005192
Test Epoch18 layer1 Acc 0.8492105263157895, AUC 0.9555012583732605, avg_entr 0.04028850421309471, f1 0.8492104411125183
ep18_l1_test_time 0.14359261099991727
Test Epoch18 layer2 Acc 0.8539473684210527, AUC 0.9535315632820129, avg_entr 0.0261737909168005, f1 0.8539473414421082
ep18_l2_test_time 0.16847587899997052
Test Epoch18 layer3 Acc 0.8560526315789474, AUC 0.9542089700698853, avg_entr 0.020482193678617477, f1 0.8560526371002197
ep18_l3_test_time 0.19480755699999008
Test Epoch18 layer4 Acc 0.8557894736842105, AUC 0.9509366750717163, avg_entr 0.01790142059326172, f1 0.8557894825935364
ep18_l4_test_time 0.23067102499999237
gc 0
Train Epoch19 Acc 0.9746083333333333 (116953/120000), AUC 0.9966664910316467
ep19_train_time 26.554667920000043
Test Epoch19 layer0 Acc 0.8307894736842105, AUC 0.9514390230178833, avg_entr 0.09769301861524582, f1 0.8307894468307495
ep19_l0_test_time 0.12434813900006247
Test Epoch19 layer1 Acc 0.8539473684210527, AUC 0.9552046060562134, avg_entr 0.03931460902094841, f1 0.8539473414421082
ep19_l1_test_time 0.14167911399999866
Test Epoch19 layer2 Acc 0.8555263157894737, AUC 0.9510725736618042, avg_entr 0.02570108324289322, f1 0.855526328086853
ep19_l2_test_time 0.16625630700002603
Test Epoch19 layer3 Acc 0.8555263157894737, AUC 0.9542443752288818, avg_entr 0.020523641258478165, f1 0.855526328086853
ep19_l3_test_time 0.1970376629999464
Test Epoch19 layer4 Acc 0.8565789473684211, AUC 0.9520635008811951, avg_entr 0.01790187880396843, f1 0.8565789461135864
ep19_l4_test_time 0.2311530940000921
Best AUC tensor(0.8674) 9 2
train_as_loss [[4.62356776e+02 3.56988431e+02 3.51160262e+02 3.49877826e+02
  3.49391766e+02 3.49157503e+02 3.49028060e+02 3.48949990e+02
  3.48899987e+02 3.48866545e+02 3.48843449e+02 3.48827112e+02
  3.48817730e+02 3.48812239e+02 3.48807184e+02 3.48802650e+02
  3.48799567e+02 3.48797525e+02 3.48795470e+02 3.48793484e+02]
 [1.84931210e+00 1.66292363e-05 1.23861594e-06 2.50892500e-07
  7.39477102e-08 1.22460681e-07 3.20582534e-06 1.60043004e-05
  7.31358566e-05 1.62014925e-04 2.67027094e-04 2.74837008e-04
  1.06610393e-06 5.28669707e-10 1.07516819e-09 1.34724399e-04
  1.56722696e-07 2.65405509e-10 3.41987719e-10 3.59463612e-05]
 [1.69759061e+00 1.38274503e-05 1.01309948e-06 2.29104528e-07
  7.30283439e-08 3.82521157e-08 1.90374691e-06 2.07426294e-05
  1.81042989e-04 4.29678272e-04 7.16196775e-04 1.50711495e-04
  3.18518605e-06 1.88581261e-09 5.45792846e-09 8.30488649e-05
  1.12597739e-06 1.40501712e-09 2.59452950e-05 2.11847467e-05]
 [2.06556206e+00 1.63783984e-05 1.47965370e-06 3.89379497e-07
  1.43549461e-07 8.10593299e-08 2.00146947e-06 2.93787306e-05
  3.26265993e-04 7.91656850e-04 1.21648675e-03 1.31245492e-04
  6.16618457e-06 8.83751461e-09 1.97244631e-08 7.04742207e-05
  3.24844246e-06 8.90179889e-09 6.05961785e-05 1.77882093e-05]
 [2.12692052e+00 1.71998449e-05 1.67367949e-06 6.31854437e-07
  2.92357957e-07 2.28261086e-07 2.17127807e-06 5.36305163e-05
  5.85932466e-04 1.47401232e-03 7.48280523e-04 1.48557947e-04
  1.35303701e-05 4.37584891e-08 1.79218932e-08 7.08255257e-05
  1.10746207e-05 6.19647369e-08 3.96016480e-05 1.94663780e-05]]
train_ae_loss [[16.28140901 16.24433472 15.95854938 14.8827698  14.31411066 13.94139792
  13.69654052 13.49795958 13.28604373 13.18735392 13.03070292 12.91419453
  11.99012348 11.719041   11.54596264 11.49875288 10.99262932 10.90980628
  10.8162113  10.79834249]
 [13.93931638 14.01312516 13.2162108  12.04684706 10.88015331 10.04085213
   9.29747559  8.7979464   8.24591764  7.9018708   7.5255675   7.16878164
   6.05328867  5.67554269  5.37629125  5.2306605   4.649226    4.46340492
   4.30302872  4.20074338]
 [13.72743225 13.31393418 11.8316462  10.47280416  9.48005235  8.81025928
   8.03794309  7.50682691  6.89297189  6.52969378  6.15195356  5.60127542
   4.65180509  4.30834702  3.97810976  3.71884865  3.25136494  3.09186286
   2.92614316  2.75062442]
 [14.22451612 13.68789094 11.15144716  9.56234458  8.57748849  7.86856349
   7.13633912  6.68112673  6.11765178  5.79323918  5.43189532  4.82711458
   3.9830576   3.67170667  3.37084113  3.10300271  2.68476309  2.54045264
   2.40931679  2.22522257]
 [14.41017989 15.29412047 11.38364757  9.61721012  8.79765849  8.29044032
   7.46878822  6.91690576  6.17702601  5.80091496  5.2617568   4.70347791
   3.88096196  3.57964663  3.2145816   2.96467744  2.56761927  2.42472525
   2.27876537  2.10408895]]
valid_acc (5, 20)
valid_AUC (5, 20)
train_acc (20,)
total_train+valid_time 545.818041545
Start Testing
Load ckpt at ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt
gc 9
Test layer0 Acc 0.8181578947368421, AUC 0.9497692584991455, avg_entr 0.13516701757907867, f1 0.818157970905304
l0_test_time 0.1277834449999773
gc 0
Test layer1 Acc 0.8539473684210527, AUC 0.9601519107818604, avg_entr 0.0637347474694252, f1 0.8539473414421082
l1_test_time 0.14277069599995684
gc 0
Test layer2 Acc 0.8542105263157894, AUC 0.9611638784408569, avg_entr 0.05031140148639679, f1 0.8542105555534363
l2_test_time 0.16522135900004287
gc 0
Test layer3 Acc 0.8557894736842105, AUC 0.9628161191940308, avg_entr 0.04760061949491501, f1 0.8557894825935364
l3_test_time 0.19628633600007106
gc 0
Test layer4 Acc 0.8547368421052631, AUC 0.9607334733009338, avg_entr 0.04242664575576782, f1 0.854736864566803
l4_test_time 0.23273020700003144
gc 0
Test threshold 0.1 Acc 0.8510526315789474, AUC 0.9547617435455322, avg_entr 0.03741561621427536, f1 0.8510526418685913
t0.1_test_time 0.24325807799993981
gc 0
Test threshold 0.2 Acc 0.8473684210526315, AUC 0.954761803150177, avg_entr 0.04672401398420334, f1 0.8473684191703796
t0.2_test_time 0.23601598000004742
gc 0
Test threshold 0.3 Acc 0.8428947368421053, AUC 0.9540613293647766, avg_entr 0.059872228652238846, f1 0.8428947329521179
t0.3_test_time 0.23104840099995272
gc 0
Test threshold 0.4 Acc 0.8305263157894737, AUC 0.9519106149673462, avg_entr 0.08026286214590073, f1 0.8305262923240662
t0.4_test_time 0.2020281679999698
gc 0
Test threshold 0.5 Acc 0.8210526315789474, AUC 0.9503554105758667, avg_entr 0.09372379630804062, f1 0.821052610874176
t0.5_test_time 0.17710737999993853
gc 0
Test threshold 0.6 Acc 0.8181578947368421, AUC 0.9497692584991455, avg_entr 0.09750238806009293, f1 0.818157970905304
t0.6_test_time 0.15318697899999734
gc 0
Test threshold 0.7 Acc 0.8181578947368421, AUC 0.9497692584991455, avg_entr 0.09750238806009293, f1 0.818157970905304
t0.7_test_time 0.14893591500003822
gc 0
Test threshold 0.8 Acc 0.8181578947368421, AUC 0.9497692584991455, avg_entr 0.09750238806009293, f1 0.818157970905304
t0.8_test_time 0.14849005099995338
gc 0
Test threshold 0.9 Acc 0.8181578947368421, AUC 0.9497692584991455, avg_entr 0.09750238806009293, f1 0.818157970905304
t0.9_test_time 0.1496827780000558
