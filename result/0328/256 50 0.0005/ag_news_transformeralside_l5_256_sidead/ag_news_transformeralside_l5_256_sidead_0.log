total count words 102019
vocab size 30000
train size 120000, valid size 3800, test size 3800
found 26754 words in glove
model: TransformerALsideText(
  (layers): ModuleList(
    (0): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=4, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=4, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
  (emb_layers): ModuleList(
    (0): Embedding(30000, 300)
    (1): Embedding(30000, 300)
    (2): Embedding(30000, 300)
    (3): Embedding(30000, 300)
    (4): Embedding(30000, 300)
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.multi_headed_attention.linears.0.weight 90000
layers.0.enc.f.multi_headed_attention.linears.0.bias 300
layers.0.enc.f.multi_headed_attention.linears.1.weight 90000
layers.0.enc.f.multi_headed_attention.linears.1.bias 300
layers.0.enc.f.multi_headed_attention.linears.2.weight 90000
layers.0.enc.f.multi_headed_attention.linears.2.bias 300
layers.0.enc.f.multi_headed_attention.linears.3.weight 90000
layers.0.enc.f.multi_headed_attention.linears.3.bias 300
layers.0.enc.f.feed_forward.w_1.weight 90000
layers.0.enc.f.feed_forward.w_1.bias 300
layers.0.enc.f.feed_forward.w_2.weight 90000
layers.0.enc.f.feed_forward.w_2.bias 300
layers.0.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.0.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.0.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.0.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.0.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.0.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.0.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.0.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.0.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.0.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.0.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.0.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.0.enc.f.encoder.norm.a 300
layers.0.enc.f.encoder.norm.b 300
layers.0.ae.g.0.weight 512
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 512
layers.0.ae.h.0.bias 4
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
emb_layers.0.weight 9000000
emb_layers.1.weight 9000000
emb_layers.2.weight 9000000
emb_layers.3.weight 9000000
emb_layers.4.weight 9000000
Total Trainable Params: 50758892
init_time 25.29032533
Start Training
gc 0
Train Epoch0 Acc 0.2745166666666667 (32942/120000), AUC 0.5205211639404297
ep0_train_time 26.695293537999998
Test Epoch0 layer0 Acc 0.6734210526315789, AUC 0.8773804903030396, avg_entr 0.8950145840644836, f1 0.6734210252761841
ep0_l0_test_time 0.12984747699999843
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 0
Test Epoch0 layer1 Acc 0.708421052631579, AUC 0.8997287750244141, avg_entr 0.8692513704299927, f1 0.7084210515022278
ep0_l1_test_time 0.13922867400000172
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 0
Test Epoch0 layer2 Acc 0.5815789473684211, AUC 0.8878965377807617, avg_entr 0.8705323934555054, f1 0.5815789699554443
ep0_l2_test_time 0.16578203200000274
Test Epoch0 layer3 Acc 0.33052631578947367, AUC 0.8573968410491943, avg_entr 0.7526129484176636, f1 0.33052632212638855
ep0_l3_test_time 0.1950908709999979
Test Epoch0 layer4 Acc 0.3763157894736842, AUC 0.8429052233695984, avg_entr 1.1018588542938232, f1 0.37631580233573914
ep0_l4_test_time 0.22924471299999993
gc 0
Train Epoch1 Acc 0.6793166666666667 (81518/120000), AUC 0.8746188879013062
ep1_train_time 25.548318267
Test Epoch1 layer0 Acc 0.7636842105263157, AUC 0.923313558101654, avg_entr 0.5198975801467896, f1 0.7636842131614685
ep1_l0_test_time 0.12812219599999253
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 1
Test Epoch1 layer1 Acc 0.7989473684210526, AUC 0.9425618648529053, avg_entr 0.4408664107322693, f1 0.7989473938941956
ep1_l1_test_time 0.1431494690000079
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 1
Test Epoch1 layer2 Acc 0.8015789473684211, AUC 0.9459971785545349, avg_entr 0.41400137543678284, f1 0.801578938961029
ep1_l2_test_time 0.1653798359999996
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 1
Test Epoch1 layer3 Acc 0.7955263157894736, AUC 0.9468686580657959, avg_entr 0.4131975769996643, f1 0.7955263257026672
ep1_l3_test_time 0.19456516600000384
Test Epoch1 layer4 Acc 0.79, AUC 0.9463571906089783, avg_entr 0.4130050539970398, f1 0.7900000214576721
ep1_l4_test_time 0.23098631700000283
gc 0
Train Epoch2 Acc 0.8207333333333333 (98488/120000), AUC 0.9471919536590576
ep2_train_time 26.478550240999994
Test Epoch2 layer0 Acc 0.7878947368421053, AUC 0.9398704767227173, avg_entr 0.34242546558380127, f1 0.7878947257995605
ep2_l0_test_time 0.12295199499999399
Test Epoch2 layer1 Acc 0.8173684210526316, AUC 0.95546555519104, avg_entr 0.27209901809692383, f1 0.8173684477806091
ep2_l1_test_time 0.1370615279999896
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 2
Test Epoch2 layer2 Acc 0.8255263157894737, AUC 0.9580886960029602, avg_entr 0.23935404419898987, f1 0.825526237487793
ep2_l2_test_time 0.16709705799999597
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 2
Test Epoch2 layer3 Acc 0.8255263157894737, AUC 0.958398699760437, avg_entr 0.23519647121429443, f1 0.825526237487793
ep2_l3_test_time 0.1952312930000062
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 2
Test Epoch2 layer4 Acc 0.8271052631578948, AUC 0.9572276473045349, avg_entr 0.22658255696296692, f1 0.8271052837371826
ep2_l4_test_time 0.23296810399999401
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 2
gc 0
Train Epoch3 Acc 0.85875 (103050/120000), AUC 0.9607802629470825
ep3_train_time 25.878369923999983
Test Epoch3 layer0 Acc 0.8089473684210526, AUC 0.9459801912307739, avg_entr 0.253021776676178, f1 0.8089473843574524
ep3_l0_test_time 0.12638238999997498
Test Epoch3 layer1 Acc 0.8405263157894737, AUC 0.9608564376831055, avg_entr 0.17518256604671478, f1 0.8405263423919678
ep3_l1_test_time 0.14114834500000484
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 3
Test Epoch3 layer2 Acc 0.8463157894736842, AUC 0.9625083804130554, avg_entr 0.1466531753540039, f1 0.8463158011436462
ep3_l2_test_time 0.16547346900000548
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 3
Test Epoch3 layer3 Acc 0.8436842105263158, AUC 0.9609240889549255, avg_entr 0.13488474488258362, f1 0.843684196472168
ep3_l3_test_time 0.19582930000001397
Test Epoch3 layer4 Acc 0.8444736842105263, AUC 0.9594675302505493, avg_entr 0.12951622903347015, f1 0.844473659992218
ep3_l4_test_time 0.2310030529999949
gc 0
Train Epoch4 Acc 0.8783833333333333 (105406/120000), AUC 0.9676975607872009
ep4_train_time 25.554954337000026
Test Epoch4 layer0 Acc 0.8142105263157895, AUC 0.9496701955795288, avg_entr 0.2128182351589203, f1 0.8142105340957642
ep4_l0_test_time 0.12636691899999164
Test Epoch4 layer1 Acc 0.8481578947368421, AUC 0.962966799736023, avg_entr 0.12735579907894135, f1 0.8481578826904297
ep4_l1_test_time 0.14498551099998735
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 4
Test Epoch4 layer2 Acc 0.8465789473684211, AUC 0.9659344553947449, avg_entr 0.11258384585380554, f1 0.8465789556503296
ep4_l2_test_time 0.16731458800001064
Test Epoch4 layer3 Acc 0.8452631578947368, AUC 0.9644953012466431, avg_entr 0.10843203961849213, f1 0.8452631831169128
ep4_l3_test_time 0.19825694900001167
Test Epoch4 layer4 Acc 0.8447368421052631, AUC 0.9622895121574402, avg_entr 0.10788607597351074, f1 0.8447368144989014
ep4_l4_test_time 0.23269807500000184
gc 0
Train Epoch5 Acc 0.893425 (107211/120000), AUC 0.9724252223968506
ep5_train_time 25.723172926000018
Test Epoch5 layer0 Acc 0.8186842105263158, AUC 0.9507456421852112, avg_entr 0.1865517795085907, f1 0.8186842203140259
ep5_l0_test_time 0.12604258200002505
Test Epoch5 layer1 Acc 0.8521052631578947, AUC 0.9646324515342712, avg_entr 0.10059135407209396, f1 0.8521052598953247
ep5_l1_test_time 0.14269000199999482
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 5
Test Epoch5 layer2 Acc 0.8576315789473684, AUC 0.9672515392303467, avg_entr 0.07959826290607452, f1 0.8576316237449646
ep5_l2_test_time 0.1669458069999905
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 5
Test Epoch5 layer3 Acc 0.8589473684210527, AUC 0.9664996862411499, avg_entr 0.0715010017156601, f1 0.8589473962783813
ep5_l3_test_time 0.19709106799999176
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 5
Test Epoch5 layer4 Acc 0.8589473684210527, AUC 0.9656369686126709, avg_entr 0.06872108578681946, f1 0.8589473962783813
ep5_l4_test_time 0.23519534599998337
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 5
gc 0
Train Epoch6 Acc 0.9045583333333334 (108547/120000), AUC 0.976062536239624
ep6_train_time 26.801562638000007
Test Epoch6 layer0 Acc 0.8239473684210527, AUC 0.9514927268028259, avg_entr 0.16818927228450775, f1 0.8239473700523376
ep6_l0_test_time 0.12705997899999488
Test Epoch6 layer1 Acc 0.8528947368421053, AUC 0.9651678204536438, avg_entr 0.08576782792806625, f1 0.8528947234153748
ep6_l1_test_time 0.141258726999979
Test Epoch6 layer2 Acc 0.8621052631578947, AUC 0.9661417007446289, avg_entr 0.0670030489563942, f1 0.8621052503585815
ep6_l2_test_time 0.16375161499999535
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 6
Test Epoch6 layer3 Acc 0.8605263157894737, AUC 0.9640130400657654, avg_entr 0.05812252685427666, f1 0.8605263233184814
ep6_l3_test_time 0.19363520600001038
Test Epoch6 layer4 Acc 0.8610526315789474, AUC 0.9626950025558472, avg_entr 0.05283983424305916, f1 0.8610526323318481
ep6_l4_test_time 0.23415961199998492
gc 0
Train Epoch7 Acc 0.9135166666666666 (109622/120000), AUC 0.9789949059486389
ep7_train_time 25.376094508999984
Test Epoch7 layer0 Acc 0.8273684210526315, AUC 0.9523435235023499, avg_entr 0.15768292546272278, f1 0.827368438243866
ep7_l0_test_time 0.1588085289999981
Test Epoch7 layer1 Acc 0.8597368421052631, AUC 0.9655436873435974, avg_entr 0.079332135617733, f1 0.8597368597984314
ep7_l1_test_time 0.17693285499998979
Test Epoch7 layer2 Acc 0.865, AUC 0.9687682390213013, avg_entr 0.06359554082155228, f1 0.8650000095367432
ep7_l2_test_time 0.19758246199998553
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 7
Test Epoch7 layer3 Acc 0.8644736842105263, AUC 0.9687998294830322, avg_entr 0.05643299221992493, f1 0.8644736409187317
ep7_l3_test_time 0.22589333799999167
Test Epoch7 layer4 Acc 0.8644736842105263, AUC 0.9676864743232727, avg_entr 0.050743184983730316, f1 0.8644736409187317
ep7_l4_test_time 0.2609929429999909
gc 0
Train Epoch8 Acc 0.9233916666666667 (110807/120000), AUC 0.9824615716934204
ep8_train_time 25.821907193000015
Test Epoch8 layer0 Acc 0.8189473684210526, AUC 0.9502807855606079, avg_entr 0.14325852692127228, f1 0.8189473748207092
ep8_l0_test_time 0.12685713600001236
Test Epoch8 layer1 Acc 0.8505263157894737, AUC 0.9625649452209473, avg_entr 0.07051842659711838, f1 0.8505263328552246
ep8_l1_test_time 0.1440820280000139
Test Epoch8 layer2 Acc 0.8589473684210527, AUC 0.9646080732345581, avg_entr 0.05382309481501579, f1 0.8589473962783813
ep8_l2_test_time 0.1671856019999609
Test Epoch8 layer3 Acc 0.8560526315789474, AUC 0.9636183381080627, avg_entr 0.04818984121084213, f1 0.8560526371002197
ep8_l3_test_time 0.18905562999998438
Test Epoch8 layer4 Acc 0.8544736842105263, AUC 0.961144208908081, avg_entr 0.04505979269742966, f1 0.8544737100601196
ep8_l4_test_time 0.2334634709999932
gc 0
Train Epoch9 Acc 0.930325 (111639/120000), AUC 0.9846529960632324
ep9_train_time 25.756434775000002
Test Epoch9 layer0 Acc 0.8252631578947368, AUC 0.9515522718429565, avg_entr 0.13526879251003265, f1 0.8252631425857544
ep9_l0_test_time 0.12431410400000686
Test Epoch9 layer1 Acc 0.8544736842105263, AUC 0.9637545347213745, avg_entr 0.06794918328523636, f1 0.8544737100601196
ep9_l1_test_time 0.14331353000000036
Test Epoch9 layer2 Acc 0.8631578947368421, AUC 0.9643876552581787, avg_entr 0.050961412489414215, f1 0.8631578683853149
ep9_l2_test_time 0.1664155929999538
Test Epoch9 layer3 Acc 0.863421052631579, AUC 0.964648962020874, avg_entr 0.044820480048656464, f1 0.8634210228919983
ep9_l3_test_time 0.19516719500001045
Test Epoch9 layer4 Acc 0.863421052631579, AUC 0.9635586738586426, avg_entr 0.03983883187174797, f1 0.8634210228919983
ep9_l4_test_time 0.23113650499999494
gc 0
Train Epoch10 Acc 0.9373583333333333 (112483/120000), AUC 0.9873449206352234
ep10_train_time 25.431464489999996
Test Epoch10 layer0 Acc 0.8307894736842105, AUC 0.9518857002258301, avg_entr 0.12752647697925568, f1 0.8307894468307495
ep10_l0_test_time 0.12670068799997125
Test Epoch10 layer1 Acc 0.8576315789473684, AUC 0.9621192812919617, avg_entr 0.05683903768658638, f1 0.8576316237449646
ep10_l1_test_time 0.14149749499995323
Test Epoch10 layer2 Acc 0.8642105263157894, AUC 0.9623450040817261, avg_entr 0.04251573234796524, f1 0.8642105460166931
ep10_l2_test_time 0.16811838199998874
Test Epoch10 layer3 Acc 0.8673684210526316, AUC 0.9614126682281494, avg_entr 0.03589048236608505, f1 0.8673684000968933
ep10_l3_test_time 0.19228734000000713
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 10
Test Epoch10 layer4 Acc 0.8671052631578947, AUC 0.9604895114898682, avg_entr 0.03086736798286438, f1 0.86710524559021
ep10_l4_test_time 0.2315050639999754
gc 0
Train Epoch11 Acc 0.9443416666666666 (113321/120000), AUC 0.9887299537658691
ep11_train_time 25.683522342999993
Test Epoch11 layer0 Acc 0.8255263157894737, AUC 0.951897144317627, avg_entr 0.11669530719518661, f1 0.825526237487793
ep11_l0_test_time 0.12711182299995016
Test Epoch11 layer1 Acc 0.8513157894736842, AUC 0.9588671922683716, avg_entr 0.05428287759423256, f1 0.8513157963752747
ep11_l1_test_time 0.14297444099997847
Test Epoch11 layer2 Acc 0.8557894736842105, AUC 0.9592837691307068, avg_entr 0.04080897942185402, f1 0.8557894825935364
ep11_l2_test_time 0.1659366769999906
Test Epoch11 layer3 Acc 0.858421052631579, AUC 0.9568008184432983, avg_entr 0.035108909010887146, f1 0.8584210276603699
ep11_l3_test_time 0.19627302100002453
Test Epoch11 layer4 Acc 0.8594736842105263, AUC 0.9549720287322998, avg_entr 0.03047364018857479, f1 0.859473705291748
ep11_l4_test_time 0.23273289200000136
gc 0
Train Epoch12 Acc 0.956325 (114759/120000), AUC 0.9924282431602478
ep12_train_time 25.603786904999993
Test Epoch12 layer0 Acc 0.8318421052631579, AUC 0.9528559446334839, avg_entr 0.11141832917928696, f1 0.8318421840667725
ep12_l0_test_time 0.12630325100002437
Test Epoch12 layer1 Acc 0.8544736842105263, AUC 0.9591180086135864, avg_entr 0.04816100373864174, f1 0.8544737100601196
ep12_l1_test_time 0.14172733100002688
Test Epoch12 layer2 Acc 0.8618421052631579, AUC 0.9575685858726501, avg_entr 0.03478891775012016, f1 0.8618420958518982
ep12_l2_test_time 0.1646662510000283
Test Epoch12 layer3 Acc 0.863421052631579, AUC 0.9572905898094177, avg_entr 0.02914676070213318, f1 0.8634210228919983
ep12_l3_test_time 0.19041890500000136
Test Epoch12 layer4 Acc 0.8631578947368421, AUC 0.9551193118095398, avg_entr 0.02594170905649662, f1 0.8631578683853149
ep12_l4_test_time 0.23294776699998465
gc 0
Train Epoch13 Acc 0.9605416666666666 (115265/120000), AUC 0.9934830665588379
ep13_train_time 25.364115317000028
Test Epoch13 layer0 Acc 0.83, AUC 0.9534039497375488, avg_entr 0.10943864285945892, f1 0.8299999833106995
ep13_l0_test_time 0.12426532299997461
Test Epoch13 layer1 Acc 0.8571052631578947, AUC 0.9607264995574951, avg_entr 0.04891325533390045, f1 0.8571051955223083
ep13_l1_test_time 0.14110378200001605
Test Epoch13 layer2 Acc 0.8597368421052631, AUC 0.9590930938720703, avg_entr 0.03447820246219635, f1 0.8597368597984314
ep13_l2_test_time 0.16549349699999993
Test Epoch13 layer3 Acc 0.8607894736842105, AUC 0.9595248699188232, avg_entr 0.029511461034417152, f1 0.8607894778251648
ep13_l3_test_time 0.1967143379999925
Test Epoch13 layer4 Acc 0.86, AUC 0.9586297273635864, avg_entr 0.026544036343693733, f1 0.8600000143051147
ep13_l4_test_time 0.23198655699997062
gc 0
Train Epoch14 Acc 0.96385 (115662/120000), AUC 0.9939577579498291
ep14_train_time 25.821303866999983
Test Epoch14 layer0 Acc 0.8302631578947368, AUC 0.9521299004554749, avg_entr 0.10276272892951965, f1 0.8302631378173828
ep14_l0_test_time 0.12636120400003392
Test Epoch14 layer1 Acc 0.8552631578947368, AUC 0.9584636092185974, avg_entr 0.04560314118862152, f1 0.8552631735801697
ep14_l1_test_time 0.13935995000002777
Test Epoch14 layer2 Acc 0.8586842105263158, AUC 0.9566351175308228, avg_entr 0.028800852596759796, f1 0.8586841821670532
ep14_l2_test_time 0.1680771800000116
Test Epoch14 layer3 Acc 0.8589473684210527, AUC 0.9561945796012878, avg_entr 0.02352532185614109, f1 0.8589473962783813
ep14_l3_test_time 0.19563222599998653
Test Epoch14 layer4 Acc 0.8581578947368421, AUC 0.9552034139633179, avg_entr 0.021056784316897392, f1 0.8581578731536865
ep14_l4_test_time 0.23145870199999763
gc 0
Train Epoch15 Acc 0.966025 (115923/120000), AUC 0.9945736527442932
ep15_train_time 25.537045765000016
Test Epoch15 layer0 Acc 0.8234210526315789, AUC 0.9511815309524536, avg_entr 0.1011827141046524, f1 0.823421061038971
ep15_l0_test_time 0.12127888500003792
Test Epoch15 layer1 Acc 0.8518421052631578, AUC 0.9577447175979614, avg_entr 0.04329533129930496, f1 0.8518421053886414
ep15_l1_test_time 0.14399570599999834
Test Epoch15 layer2 Acc 0.8573684210526316, AUC 0.9535057544708252, avg_entr 0.02626953087747097, f1 0.8573684096336365
ep15_l2_test_time 0.16352255100002822
Test Epoch15 layer3 Acc 0.8568421052631578, AUC 0.9515053033828735, avg_entr 0.023046383634209633, f1 0.8568421006202698
ep15_l3_test_time 0.20057531399999107
Test Epoch15 layer4 Acc 0.8555263157894737, AUC 0.949329674243927, avg_entr 0.02046200819313526, f1 0.855526328086853
ep15_l4_test_time 0.22778238600000122
gc 0
Train Epoch16 Acc 0.9712083333333333 (116545/120000), AUC 0.9959184527397156
ep16_train_time 25.856780239999978
Test Epoch16 layer0 Acc 0.8294736842105264, AUC 0.9511495232582092, avg_entr 0.0971013754606247, f1 0.829473614692688
ep16_l0_test_time 0.12407317199995305
Test Epoch16 layer1 Acc 0.8536842105263158, AUC 0.9569116830825806, avg_entr 0.04244616627693176, f1 0.8536841869354248
ep16_l1_test_time 0.13728456999996297
Test Epoch16 layer2 Acc 0.8610526315789474, AUC 0.9533877372741699, avg_entr 0.026883313432335854, f1 0.8610526323318481
ep16_l2_test_time 0.16447103799998786
Test Epoch16 layer3 Acc 0.861578947368421, AUC 0.9519158005714417, avg_entr 0.022525634616613388, f1 0.8615789413452148
ep16_l3_test_time 0.197825747999957
Test Epoch16 layer4 Acc 0.8607894736842105, AUC 0.9484269618988037, avg_entr 0.01956670731306076, f1 0.8607894778251648
ep16_l4_test_time 0.2286045940000463
gc 0
Train Epoch17 Acc 0.9732083333333333 (116785/120000), AUC 0.9961164593696594
ep17_train_time 25.33385097500002
Test Epoch17 layer0 Acc 0.8263157894736842, AUC 0.9500155448913574, avg_entr 0.09625416994094849, f1 0.8263157606124878
ep17_l0_test_time 0.12845224800003052
Test Epoch17 layer1 Acc 0.8555263157894737, AUC 0.9563617706298828, avg_entr 0.0407983772456646, f1 0.855526328086853
ep17_l1_test_time 0.1409322480000128
Test Epoch17 layer2 Acc 0.8623684210526316, AUC 0.9506757855415344, avg_entr 0.025806309655308723, f1 0.8623684048652649
ep17_l2_test_time 0.16473710900004335
Test Epoch17 layer3 Acc 0.8623684210526316, AUC 0.9526343941688538, avg_entr 0.020898735150694847, f1 0.8623684048652649
ep17_l3_test_time 0.19542363900006876
Test Epoch17 layer4 Acc 0.861578947368421, AUC 0.9468139410018921, avg_entr 0.01835356093943119, f1 0.8615789413452148
ep17_l4_test_time 0.23044224900002064
gc 0
Train Epoch18 Acc 0.9752583333333333 (117031/120000), AUC 0.9965444207191467
ep18_train_time 25.52536331300007
Test Epoch18 layer0 Acc 0.83, AUC 0.9496251344680786, avg_entr 0.09088663756847382, f1 0.8299999833106995
ep18_l0_test_time 0.12761806999992586
Test Epoch18 layer1 Acc 0.85, AUC 0.9536623358726501, avg_entr 0.03777829930186272, f1 0.8500000238418579
ep18_l1_test_time 0.1427713580000045
Test Epoch18 layer2 Acc 0.8568421052631578, AUC 0.9517092704772949, avg_entr 0.023286376148462296, f1 0.8568421006202698
ep18_l2_test_time 0.16490056999998615
Test Epoch18 layer3 Acc 0.8571052631578947, AUC 0.9542879462242126, avg_entr 0.017533184960484505, f1 0.8571051955223083
ep18_l3_test_time 0.19412812200005192
Test Epoch18 layer4 Acc 0.8571052631578947, AUC 0.950821042060852, avg_entr 0.015592495910823345, f1 0.8571051955223083
ep18_l4_test_time 0.23122071399996003
gc 0
Train Epoch19 Acc 0.9774166666666667 (117290/120000), AUC 0.9966830611228943
ep19_train_time 26.99557245599999
Test Epoch19 layer0 Acc 0.828421052631579, AUC 0.9499416351318359, avg_entr 0.0904470831155777, f1 0.8284210562705994
ep19_l0_test_time 0.12692257699995935
Test Epoch19 layer1 Acc 0.8521052631578947, AUC 0.9541426301002502, avg_entr 0.037369903177022934, f1 0.8521052598953247
ep19_l1_test_time 0.13972580000006474
Test Epoch19 layer2 Acc 0.8560526315789474, AUC 0.9483965635299683, avg_entr 0.02156764268875122, f1 0.8560526371002197
ep19_l2_test_time 0.1659948340000028
Test Epoch19 layer3 Acc 0.853421052631579, AUC 0.949331521987915, avg_entr 0.019271990284323692, f1 0.8534210324287415
ep19_l3_test_time 0.19528566999997565
Test Epoch19 layer4 Acc 0.8518421052631578, AUC 0.9429068565368652, avg_entr 0.016959451138973236, f1 0.8518421053886414
ep19_l4_test_time 0.23253738599998997
Best AUC tensor(0.8674) 10 3
train_as_loss [[4.57387998e+02 3.57011445e+02 3.51208760e+02 3.49901987e+02
  3.49404434e+02 3.49164520e+02 3.49032084e+02 3.48952331e+02
  3.48901345e+02 3.48867323e+02 3.48843870e+02 3.48827318e+02
  3.48817834e+02 3.48812290e+02 3.48807195e+02 3.48802629e+02
  3.48798639e+02 3.48795212e+02 3.48792968e+02 3.48791515e+02]
 [1.56934108e+00 1.14854294e-05 8.12426626e-07 1.58617176e-07
  4.78294316e-08 1.80254117e-08 1.34120307e-06 4.52023562e-05
  1.34104760e-04 1.95368093e-04 3.78278759e-04 4.24935407e-04
  3.66196051e-05 4.63589140e-10 4.61982533e-10 1.23958749e-04
  1.12638551e-06 6.22408993e-10 7.83628027e-10 4.72952399e-05]
 [1.50406552e+00 1.19403797e-05 9.89595552e-07 2.09554376e-07
  7.08226472e-08 3.08946111e-08 1.50342933e-06 9.52152032e-05
  2.96055279e-04 4.01797527e-04 7.32623737e-04 8.72368055e-04
  7.23675291e-05 1.21474321e-09 1.60937739e-09 2.28778132e-04
  4.21132555e-06 1.77475686e-09 2.29024103e-09 8.55549390e-05]
 [1.96818311e+00 1.28096040e-05 1.15062007e-06 2.73138618e-07
  1.11534379e-07 6.31104989e-08 1.52046065e-06 1.16473963e-04
  3.92196274e-04 5.19871056e-04 1.02512650e-03 1.16267806e-03
  8.56726523e-05 4.17119023e-09 6.55979493e-09 3.28281475e-04
  7.36443358e-06 6.38807872e-09 8.30975820e-09 1.18735832e-04]
 [1.64789530e+00 1.31827713e-05 1.39919977e-06 3.94603891e-07
  2.13406384e-07 1.70768576e-07 1.63642146e-06 1.63483590e-04
  6.53893160e-04 8.65680632e-04 1.63772026e-03 1.63436613e-03
  1.20214395e-04 1.75867203e-08 3.19095727e-08 5.04941206e-04
  1.27945463e-05 2.63625999e-08 3.18567822e-08 1.61309279e-04]]
train_ae_loss [[16.65309587 16.24129638 15.71044756 14.72275551 14.17088785 13.86511694
  13.69579577 13.542191   13.36036672 13.18846978 13.01803674 12.91439418
  11.97837155 11.68264385 11.60336532 11.48869203 11.43496255 11.39755969
  10.85549394 10.72147581]
 [15.0522968  14.1926838  13.22434261 12.01462161 10.86300091  9.99545378
   9.43622055  8.99081433  8.48109022  8.10608066  7.64577206  7.2803804
   6.13514366  5.69373862  5.53630682  5.314678    4.96390263  4.80757335
   4.42219793  4.24477733]
 [14.67392994 13.93765583 12.19157439 10.82423059  9.80955703  8.97357211
   8.27418785  7.75172947  7.15048419  6.73964517  6.20155503  5.81109822
   4.62671556  4.1860865   4.02645806  3.78831324  3.38590282  3.2091707
   2.87143062  2.72149327]
 [15.69493483 14.33061592 11.66315636 10.10515219  9.19818793  8.42547607
   7.77722946  7.29779987  6.66337022  6.22084083  5.64863723  5.24320662
   4.0265393   3.60952178  3.46892671  3.24430664  2.83351067  2.67363864
   2.36016152  2.22733082]
 [17.69916446 17.32031898 12.65390406 10.80076113  9.89158852  9.08876778
   8.35151093  7.78270342  7.04780491  6.53398383  5.89091587  5.42419612
   4.08686374  3.66339658  3.51905193  3.29128797  2.84366881  2.67833529
   2.36001944  2.22846332]]
valid_acc (5, 20)
valid_AUC (5, 20)
train_acc (20,)
total_train+valid_time 543.014000707
Start Testing
Load ckpt at ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt
gc 9
Test layer0 Acc 0.8215789473684211, AUC 0.9485516548156738, avg_entr 0.12331192195415497, f1 0.8215789198875427
l0_test_time 0.12773544900005618
gc 0
Test layer1 Acc 0.8497368421052631, AUC 0.9560375213623047, avg_entr 0.05700160935521126, f1 0.8497368693351746
l1_test_time 0.14700952600003347
gc 0
Test layer2 Acc 0.855, AUC 0.9578536748886108, avg_entr 0.0415799655020237, f1 0.8550000190734863
l2_test_time 0.1970876310000449
gc 0
Test layer3 Acc 0.8578947368421053, AUC 0.9573037624359131, avg_entr 0.0374637097120285, f1 0.8578947186470032
l3_test_time 0.22628292199999578
gc 0
Test layer4 Acc 0.8571052631578947, AUC 0.9552655220031738, avg_entr 0.032614972442388535, f1 0.8571051955223083
l4_test_time 0.2620603990000063
gc 0
Test threshold 0.1 Acc 0.8539473684210527, AUC 0.9517394304275513, avg_entr 0.03096090257167816, f1 0.8539473414421082
t0.1_test_time 0.3581982999999127
gc 0
Test threshold 0.2 Acc 0.8505263157894737, AUC 0.951623797416687, avg_entr 0.04011187329888344, f1 0.8505263328552246
t0.2_test_time 0.3551020759999801
gc 0
Test threshold 0.3 Acc 0.8428947368421053, AUC 0.9504866003990173, avg_entr 0.05435134842991829, f1 0.8428947329521179
t0.3_test_time 0.3456260130000146
gc 0
Test threshold 0.4 Acc 0.8289473684210527, AUC 0.949674129486084, avg_entr 0.07421177625656128, f1 0.8289473652839661
t0.4_test_time 0.2929362829999036
gc 0
Test threshold 0.5 Acc 0.8234210526315789, AUC 0.9487398862838745, avg_entr 0.08661696314811707, f1 0.823421061038971
t0.5_test_time 0.24283605499999794
gc 0
Test threshold 0.6 Acc 0.8215789473684211, AUC 0.9485516548156738, avg_entr 0.0889507457613945, f1 0.8215789198875427
t0.6_test_time 0.2037241199999471
gc 0
Test threshold 0.7 Acc 0.8215789473684211, AUC 0.9485516548156738, avg_entr 0.0889507457613945, f1 0.8215789198875427
t0.7_test_time 0.20279976400001942
gc 0
Test threshold 0.8 Acc 0.8215789473684211, AUC 0.9485516548156738, avg_entr 0.0889507457613945, f1 0.8215789198875427
t0.8_test_time 0.2078157009999586
gc 0
Test threshold 0.9 Acc 0.8215789473684211, AUC 0.9485516548156738, avg_entr 0.0889507457613945, f1 0.8215789198875427
t0.9_test_time 0.20654543199998443
