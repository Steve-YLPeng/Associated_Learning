total count words 102019
vocab size 30000
train size 120000, valid size 3800, test size 3800
found 26754 words in glove
model: TransformerALsideText(
  (layers): ModuleList(
    (0): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=4, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=4, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
  (emb_layers): ModuleList(
    (0): Embedding(30000, 300)
    (1): Embedding(30000, 300)
    (2): Embedding(30000, 300)
    (3): Embedding(30000, 300)
    (4): Embedding(30000, 300)
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.multi_headed_attention.linears.0.weight 90000
layers.0.enc.f.multi_headed_attention.linears.0.bias 300
layers.0.enc.f.multi_headed_attention.linears.1.weight 90000
layers.0.enc.f.multi_headed_attention.linears.1.bias 300
layers.0.enc.f.multi_headed_attention.linears.2.weight 90000
layers.0.enc.f.multi_headed_attention.linears.2.bias 300
layers.0.enc.f.multi_headed_attention.linears.3.weight 90000
layers.0.enc.f.multi_headed_attention.linears.3.bias 300
layers.0.enc.f.feed_forward.w_1.weight 90000
layers.0.enc.f.feed_forward.w_1.bias 300
layers.0.enc.f.feed_forward.w_2.weight 90000
layers.0.enc.f.feed_forward.w_2.bias 300
layers.0.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.0.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.0.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.0.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.0.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.0.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.0.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.0.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.0.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.0.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.0.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.0.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.0.enc.f.encoder.norm.a 300
layers.0.enc.f.encoder.norm.b 300
layers.0.ae.g.0.weight 512
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 512
layers.0.ae.h.0.bias 4
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
emb_layers.0.weight 9000000
emb_layers.1.weight 9000000
emb_layers.2.weight 9000000
emb_layers.3.weight 9000000
emb_layers.4.weight 9000000
Total Trainable Params: 50758892
init_time 25.923853494
Start Training
gc 0
Train Epoch0 Acc 0.273125 (32775/120000), AUC 0.5152538418769836
ep0_train_time 26.439578981
Test Epoch0 layer0 Acc 0.6776315789473685, AUC 0.8736288547515869, avg_entr 0.891431450843811, f1 0.6776315569877625
ep0_l0_test_time 0.12640416800000054
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 0
Test Epoch0 layer1 Acc 0.718421052631579, AUC 0.9074259996414185, avg_entr 0.8458185195922852, f1 0.7184210419654846
ep0_l1_test_time 0.14099559599999623
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 0
Test Epoch0 layer2 Acc 0.6855263157894737, AUC 0.9005799293518066, avg_entr 0.9119107723236084, f1 0.6855263113975525
ep0_l2_test_time 0.16702676299999553
Test Epoch0 layer3 Acc 0.6865789473684211, AUC 0.8753334283828735, avg_entr 1.1102708578109741, f1 0.6865789294242859
ep0_l3_test_time 0.19386928800000192
Test Epoch0 layer4 Acc 0.42947368421052634, AUC 0.8342037200927734, avg_entr 1.168392300605774, f1 0.4294736981391907
ep0_l4_test_time 0.23057032799999888
gc 0
Train Epoch1 Acc 0.6774416666666667 (81293/120000), AUC 0.865825355052948
ep1_train_time 25.409278030000003
Test Epoch1 layer0 Acc 0.7639473684210526, AUC 0.9224823713302612, avg_entr 0.49464523792266846, f1 0.7639473676681519
ep1_l0_test_time 0.12408077499999592
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 1
Test Epoch1 layer1 Acc 0.8026315789473685, AUC 0.9427721500396729, avg_entr 0.3909517526626587, f1 0.8026315569877625
ep1_l1_test_time 0.1413740520000033
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 1
Test Epoch1 layer2 Acc 0.7989473684210526, AUC 0.9455416202545166, avg_entr 0.36984047293663025, f1 0.7989473938941956
ep1_l2_test_time 0.1631308899999908
Test Epoch1 layer3 Acc 0.7968421052631579, AUC 0.9447041749954224, avg_entr 0.39599472284317017, f1 0.796842098236084
ep1_l3_test_time 0.19805946599998947
Test Epoch1 layer4 Acc 0.7960526315789473, AUC 0.9418777823448181, avg_entr 0.41873762011528015, f1 0.7960526347160339
ep1_l4_test_time 0.22578209099999924
gc 0
Train Epoch2 Acc 0.8148166666666666 (97778/120000), AUC 0.942192018032074
ep2_train_time 26.076001125000005
Test Epoch2 layer0 Acc 0.7986842105263158, AUC 0.9426589012145996, avg_entr 0.3187442719936371, f1 0.7986842393875122
ep2_l0_test_time 0.1365457640000045
Test Epoch2 layer1 Acc 0.8271052631578948, AUC 0.9566609859466553, avg_entr 0.2538086473941803, f1 0.8271052837371826
ep2_l1_test_time 0.1464121690000013
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 2
Test Epoch2 layer2 Acc 0.8305263157894737, AUC 0.95838862657547, avg_entr 0.22212162613868713, f1 0.8305262923240662
ep2_l2_test_time 0.16738066599999968
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 2
Test Epoch2 layer3 Acc 0.8310526315789474, AUC 0.9589390158653259, avg_entr 0.20879004895687103, f1 0.8310526013374329
ep2_l3_test_time 0.1909650890000023
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 2
Test Epoch2 layer4 Acc 0.8321052631578948, AUC 0.9582284092903137, avg_entr 0.2004302442073822, f1 0.832105278968811
ep2_l4_test_time 0.23389844800000503
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 2
gc 0
Train Epoch3 Acc 0.8537916666666666 (102455/120000), AUC 0.9583351016044617
ep3_train_time 26.140478509999994
Test Epoch3 layer0 Acc 0.8094736842105263, AUC 0.9484350085258484, avg_entr 0.24550379812717438, f1 0.8094737529754639
ep3_l0_test_time 0.12761940199999344
Test Epoch3 layer1 Acc 0.8436842105263158, AUC 0.9613785743713379, avg_entr 0.17738083004951477, f1 0.843684196472168
ep3_l1_test_time 0.14373396400000615
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 3
Test Epoch3 layer2 Acc 0.8497368421052631, AUC 0.9626786708831787, avg_entr 0.1488567441701889, f1 0.8497368693351746
ep3_l2_test_time 0.16392225999999255
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 3
Test Epoch3 layer3 Acc 0.8523684210526316, AUC 0.9618157148361206, avg_entr 0.13590407371520996, f1 0.8523684144020081
ep3_l3_test_time 0.19347612499998945
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 3
Test Epoch3 layer4 Acc 0.8507894736842105, AUC 0.9613893628120422, avg_entr 0.13430793583393097, f1 0.850789487361908
ep3_l4_test_time 0.2320193740000036
gc 0
Train Epoch4 Acc 0.875125 (105015/120000), AUC 0.965623140335083
ep4_train_time 25.668868626000005
Test Epoch4 layer0 Acc 0.8155263157894737, AUC 0.9504886865615845, avg_entr 0.20949658751487732, f1 0.8155263066291809
ep4_l0_test_time 0.1258606709999981
Test Epoch4 layer1 Acc 0.8510526315789474, AUC 0.9629731178283691, avg_entr 0.13130588829517365, f1 0.8510526418685913
ep4_l1_test_time 0.14290415100001042
Test Epoch4 layer2 Acc 0.8576315789473684, AUC 0.9658406972885132, avg_entr 0.1081845760345459, f1 0.8576316237449646
ep4_l2_test_time 0.1666816219999987
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 4
Test Epoch4 layer3 Acc 0.8563157894736843, AUC 0.9650286436080933, avg_entr 0.09809545427560806, f1 0.8563157916069031
ep4_l3_test_time 0.223113761999997
Test Epoch4 layer4 Acc 0.8563157894736843, AUC 0.963927686214447, avg_entr 0.09360339492559433, f1 0.8563157916069031
ep4_l4_test_time 0.2377105970000173
gc 0
Train Epoch5 Acc 0.8922 (107064/120000), AUC 0.9722818732261658
ep5_train_time 26.046693036999983
Test Epoch5 layer0 Acc 0.8265789473684211, AUC 0.9515315294265747, avg_entr 0.18344226479530334, f1 0.8265789747238159
ep5_l0_test_time 0.13207473800000002
Test Epoch5 layer1 Acc 0.855, AUC 0.9636994004249573, avg_entr 0.10008960962295532, f1 0.8550000190734863
ep5_l1_test_time 0.1478443579999862
Test Epoch5 layer2 Acc 0.8631578947368421, AUC 0.9665155410766602, avg_entr 0.07891708612442017, f1 0.8631578683853149
ep5_l2_test_time 0.1703312159999939
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 5
Test Epoch5 layer3 Acc 0.8631578947368421, AUC 0.9651124477386475, avg_entr 0.07097885012626648, f1 0.8631578683853149
ep5_l3_test_time 0.20311318500000652
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 5
Test Epoch5 layer4 Acc 0.8626315789473684, AUC 0.9632644653320312, avg_entr 0.06298904865980148, f1 0.862631618976593
ep5_l4_test_time 0.2335215269999935
gc 0
Train Epoch6 Acc 0.9038583333333333 (108463/120000), AUC 0.9763956069946289
ep6_train_time 25.592260814000014
Test Epoch6 layer0 Acc 0.8273684210526315, AUC 0.9530755281448364, avg_entr 0.17608505487442017, f1 0.827368438243866
ep6_l0_test_time 0.1247509300000047
Test Epoch6 layer1 Acc 0.8542105263157894, AUC 0.9661612510681152, avg_entr 0.09290627390146255, f1 0.8542105555534363
ep6_l1_test_time 0.13626936700001124
Test Epoch6 layer2 Acc 0.8628947368421053, AUC 0.9682504534721375, avg_entr 0.07051068544387817, f1 0.8628947138786316
ep6_l2_test_time 0.1640960370000073
Test Epoch6 layer3 Acc 0.8639473684210527, AUC 0.9663626551628113, avg_entr 0.062181275337934494, f1 0.8639474511146545
ep6_l3_test_time 0.1963832449999927
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 6
Test Epoch6 layer4 Acc 0.8628947368421053, AUC 0.9646264314651489, avg_entr 0.05994962155818939, f1 0.8628947138786316
ep6_l4_test_time 0.23488311099998782
gc 0
Train Epoch7 Acc 0.9119583333333333 (109435/120000), AUC 0.9790127277374268
ep7_train_time 25.683152882
Test Epoch7 layer0 Acc 0.8260526315789474, AUC 0.9542967081069946, avg_entr 0.14880800247192383, f1 0.8260526061058044
ep7_l0_test_time 0.12765931699999555
Test Epoch7 layer1 Acc 0.855, AUC 0.9642829895019531, avg_entr 0.08313045650720596, f1 0.8550000190734863
ep7_l1_test_time 0.14080293300000335
Test Epoch7 layer2 Acc 0.8652631578947368, AUC 0.9682189226150513, avg_entr 0.06506601721048355, f1 0.8652631640434265
ep7_l2_test_time 0.16513893900000198
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 7
Test Epoch7 layer3 Acc 0.8652631578947368, AUC 0.9656556844711304, avg_entr 0.05724851414561272, f1 0.8652631640434265
ep7_l3_test_time 0.19623105200000168
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 7
Test Epoch7 layer4 Acc 0.8642105263157894, AUC 0.963320255279541, avg_entr 0.04965840280056, f1 0.8642105460166931
ep7_l4_test_time 0.23318196399998214
gc 0
Train Epoch8 Acc 0.9224 (110688/120000), AUC 0.9823980331420898
ep8_train_time 26.07990081599999
Test Epoch8 layer0 Acc 0.8313157894736842, AUC 0.955237865447998, avg_entr 0.14196763932704926, f1 0.831315815448761
ep8_l0_test_time 0.1261614740000141
Test Epoch8 layer1 Acc 0.8626315789473684, AUC 0.9643913507461548, avg_entr 0.07075915485620499, f1 0.862631618976593
ep8_l1_test_time 0.14025424199996905
Test Epoch8 layer2 Acc 0.8705263157894737, AUC 0.9673688411712646, avg_entr 0.05361754819750786, f1 0.8705263137817383
ep8_l2_test_time 0.16448659500002805
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 8
Test Epoch8 layer3 Acc 0.8718421052631579, AUC 0.9647033214569092, avg_entr 0.04434825852513313, f1 0.8718421459197998
ep8_l3_test_time 0.19553214700005128
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 8
Test Epoch8 layer4 Acc 0.8713157894736843, AUC 0.9624663591384888, avg_entr 0.03814998269081116, f1 0.8713157176971436
ep8_l4_test_time 0.2324241210000082
gc 0
Train Epoch9 Acc 0.92885 (111462/120000), AUC 0.9836099743843079
ep9_train_time 25.674616345000004
Test Epoch9 layer0 Acc 0.8276315789473684, AUC 0.9539440274238586, avg_entr 0.1342751681804657, f1 0.8276315927505493
ep9_l0_test_time 0.12987971000001153
Test Epoch9 layer1 Acc 0.8523684210526316, AUC 0.963747501373291, avg_entr 0.0648544654250145, f1 0.8523684144020081
ep9_l1_test_time 0.13428312700000333
Test Epoch9 layer2 Acc 0.8626315789473684, AUC 0.9668846130371094, avg_entr 0.051262423396110535, f1 0.862631618976593
ep9_l2_test_time 0.1669892190000155
Test Epoch9 layer3 Acc 0.8607894736842105, AUC 0.9630527496337891, avg_entr 0.044329117983579636, f1 0.8607894778251648
ep9_l3_test_time 0.19353971599997521
Test Epoch9 layer4 Acc 0.858421052631579, AUC 0.9645439386367798, avg_entr 0.03900887817144394, f1 0.8584210276603699
ep9_l4_test_time 0.23533938799999987
gc 0
Train Epoch10 Acc 0.9355833333333333 (112270/120000), AUC 0.9863878488540649
ep10_train_time 25.914100913000027
Test Epoch10 layer0 Acc 0.8257894736842105, AUC 0.9526640176773071, avg_entr 0.12785977125167847, f1 0.8257894515991211
ep10_l0_test_time 0.1287156340000024
Test Epoch10 layer1 Acc 0.8513157894736842, AUC 0.9587385058403015, avg_entr 0.06254061311483383, f1 0.8513157963752747
ep10_l1_test_time 0.18189744500000415
Test Epoch10 layer2 Acc 0.8586842105263158, AUC 0.9640325307846069, avg_entr 0.046362511813640594, f1 0.8586841821670532
ep10_l2_test_time 0.20381193399998665
Test Epoch10 layer3 Acc 0.8563157894736843, AUC 0.961367666721344, avg_entr 0.03951449692249298, f1 0.8563157916069031
ep10_l3_test_time 0.22980612600002814
Test Epoch10 layer4 Acc 0.8557894736842105, AUC 0.9607643485069275, avg_entr 0.034114688634872437, f1 0.8557894825935364
ep10_l4_test_time 0.2656957689999899
gc 0
Train Epoch11 Acc 0.9452333333333334 (113428/120000), AUC 0.9904927611351013
ep11_train_time 26.77899328999996
Test Epoch11 layer0 Acc 0.8247368421052632, AUC 0.9531430602073669, avg_entr 0.11784936487674713, f1 0.8247368335723877
ep11_l0_test_time 0.12294440300001952
Test Epoch11 layer1 Acc 0.8544736842105263, AUC 0.961143434047699, avg_entr 0.05677495151758194, f1 0.8544737100601196
ep11_l1_test_time 0.14459428399999297
Test Epoch11 layer2 Acc 0.8676315789473684, AUC 0.9620509147644043, avg_entr 0.04368910193443298, f1 0.8676315546035767
ep11_l2_test_time 0.16147551500000645
Test Epoch11 layer3 Acc 0.8663157894736843, AUC 0.9609290957450867, avg_entr 0.035749297589063644, f1 0.8663158416748047
ep11_l3_test_time 0.19895976899999823
Test Epoch11 layer4 Acc 0.8660526315789474, AUC 0.9597026705741882, avg_entr 0.031507741659879684, f1 0.8660526275634766
ep11_l4_test_time 0.2335629250000011
gc 0
Train Epoch12 Acc 0.9512416666666667 (114149/120000), AUC 0.9919776916503906
ep12_train_time 25.584010966999983
Test Epoch12 layer0 Acc 0.8242105263157895, AUC 0.9513458013534546, avg_entr 0.11134514212608337, f1 0.8242105841636658
ep12_l0_test_time 0.1588578110000185
Test Epoch12 layer1 Acc 0.8526315789473684, AUC 0.9592167735099792, avg_entr 0.05273590236902237, f1 0.8526315689086914
ep12_l1_test_time 0.17869737399996666
Test Epoch12 layer2 Acc 0.86, AUC 0.9618957042694092, avg_entr 0.039081647992134094, f1 0.8600000143051147
ep12_l2_test_time 0.20168450400001348
Test Epoch12 layer3 Acc 0.8597368421052631, AUC 0.9606280326843262, avg_entr 0.032932981848716736, f1 0.8597368597984314
ep12_l3_test_time 0.23114906199998586
Test Epoch12 layer4 Acc 0.858421052631579, AUC 0.9598653316497803, avg_entr 0.028982045128941536, f1 0.8584210276603699
ep12_l4_test_time 0.2660670970000183
gc 0
Train Epoch13 Acc 0.9575416666666666 (114905/120000), AUC 0.9924060106277466
ep13_train_time 25.710012561999974
Test Epoch13 layer0 Acc 0.8305263157894737, AUC 0.9520846605300903, avg_entr 0.10753006488084793, f1 0.8305262923240662
ep13_l0_test_time 0.12548425900001803
Test Epoch13 layer1 Acc 0.8573684210526316, AUC 0.9582006931304932, avg_entr 0.051766205579042435, f1 0.8573684096336365
ep13_l1_test_time 0.14321965999999975
Test Epoch13 layer2 Acc 0.8628947368421053, AUC 0.9608151912689209, avg_entr 0.03149494528770447, f1 0.8628947138786316
ep13_l2_test_time 0.16765031500000305
Test Epoch13 layer3 Acc 0.8623684210526316, AUC 0.958532452583313, avg_entr 0.02743428200483322, f1 0.8623684048652649
ep13_l3_test_time 0.19643126799996935
Test Epoch13 layer4 Acc 0.8623684210526316, AUC 0.9525777101516724, avg_entr 0.024208448827266693, f1 0.8623684048652649
ep13_l4_test_time 0.23088558599999942
gc 0
Train Epoch14 Acc 0.961725 (115407/120000), AUC 0.9934383034706116
ep14_train_time 25.73650737899999
Test Epoch14 layer0 Acc 0.8260526315789474, AUC 0.9518113136291504, avg_entr 0.10417358577251434, f1 0.8260526061058044
ep14_l0_test_time 0.12423916900002041
Test Epoch14 layer1 Acc 0.853421052631579, AUC 0.9595231413841248, avg_entr 0.04502033442258835, f1 0.8534210324287415
ep14_l1_test_time 0.14291698199997427
Test Epoch14 layer2 Acc 0.86, AUC 0.959106981754303, avg_entr 0.03159134462475777, f1 0.8600000143051147
ep14_l2_test_time 0.1651152100000104
Test Epoch14 layer3 Acc 0.858421052631579, AUC 0.9557784795761108, avg_entr 0.027187712490558624, f1 0.8584210276603699
ep14_l3_test_time 0.1957159509999542
Test Epoch14 layer4 Acc 0.8592105263157894, AUC 0.9514712691307068, avg_entr 0.023170659318566322, f1 0.8592105507850647
ep14_l4_test_time 0.23415215800002898
gc 0
Train Epoch15 Acc 0.96615 (115938/120000), AUC 0.9954149723052979
ep15_train_time 25.581905610999968
Test Epoch15 layer0 Acc 0.8297368421052631, AUC 0.9513279795646667, avg_entr 0.10045436024665833, f1 0.8297368288040161
ep15_l0_test_time 0.15631314800003793
Test Epoch15 layer1 Acc 0.8542105263157894, AUC 0.9584362506866455, avg_entr 0.041872281581163406, f1 0.8542105555534363
ep15_l1_test_time 0.1796207959999947
Test Epoch15 layer2 Acc 0.8605263157894737, AUC 0.9567012190818787, avg_entr 0.030491838231682777, f1 0.8605263233184814
ep15_l2_test_time 0.20200645500000292
Test Epoch15 layer3 Acc 0.8552631578947368, AUC 0.9549611806869507, avg_entr 0.024120256304740906, f1 0.8552631735801697
ep15_l3_test_time 0.23010615600003348
Test Epoch15 layer4 Acc 0.8539473684210527, AUC 0.9530948400497437, avg_entr 0.020940113812685013, f1 0.8539473414421082
ep15_l4_test_time 0.26784709700001486
gc 0
Train Epoch16 Acc 0.9686416666666666 (116237/120000), AUC 0.9953548312187195
ep16_train_time 25.45404567500003
Test Epoch16 layer0 Acc 0.8305263157894737, AUC 0.9506060481071472, avg_entr 0.0968208909034729, f1 0.8305262923240662
ep16_l0_test_time 0.12449937700000646
Test Epoch16 layer1 Acc 0.8513157894736842, AUC 0.9566777348518372, avg_entr 0.043350737541913986, f1 0.8513157963752747
ep16_l1_test_time 0.14406220200004327
Test Epoch16 layer2 Acc 0.8610526315789474, AUC 0.9563450813293457, avg_entr 0.02807026356458664, f1 0.8610526323318481
ep16_l2_test_time 0.16511262599999554
Test Epoch16 layer3 Acc 0.8610526315789474, AUC 0.9549994468688965, avg_entr 0.02305237017571926, f1 0.8610526323318481
ep16_l3_test_time 0.1977183359999799
Test Epoch16 layer4 Acc 0.8605263157894737, AUC 0.9536938667297363, avg_entr 0.019302785396575928, f1 0.8605263233184814
ep16_l4_test_time 0.2340541929999631
gc 0
Train Epoch17 Acc 0.972125 (116655/120000), AUC 0.9959924221038818
ep17_train_time 25.66423672100001
Test Epoch17 layer0 Acc 0.8297368421052631, AUC 0.950340986251831, avg_entr 0.0943455845117569, f1 0.8297368288040161
ep17_l0_test_time 0.12432460899992748
Test Epoch17 layer1 Acc 0.8521052631578947, AUC 0.9544756412506104, avg_entr 0.04052518308162689, f1 0.8521052598953247
ep17_l1_test_time 0.14348681000001307
Test Epoch17 layer2 Acc 0.8621052631578947, AUC 0.9544098377227783, avg_entr 0.028605755418539047, f1 0.8621052503585815
ep17_l2_test_time 0.16474223499994878
Test Epoch17 layer3 Acc 0.8602631578947368, AUC 0.9527719616889954, avg_entr 0.022734535858035088, f1 0.8602631688117981
ep17_l3_test_time 0.19727524799998264
Test Epoch17 layer4 Acc 0.8592105263157894, AUC 0.9515871405601501, avg_entr 0.019918469712138176, f1 0.8592105507850647
ep17_l4_test_time 0.2322834110000258
gc 0
Train Epoch18 Acc 0.9738083333333334 (116857/120000), AUC 0.9961178302764893
ep18_train_time 25.355585190000056
Test Epoch18 layer0 Acc 0.8292105263157895, AUC 0.9496512413024902, avg_entr 0.09289760142564774, f1 0.8292105197906494
ep18_l0_test_time 0.1239815510000426
Test Epoch18 layer1 Acc 0.8536842105263158, AUC 0.9544247388839722, avg_entr 0.04078475013375282, f1 0.8536841869354248
ep18_l1_test_time 0.13377356400008011
Test Epoch18 layer2 Acc 0.8581578947368421, AUC 0.952866792678833, avg_entr 0.030534539371728897, f1 0.8581578731536865
ep18_l2_test_time 0.1661851310000202
Test Epoch18 layer3 Acc 0.8578947368421053, AUC 0.9533238410949707, avg_entr 0.02204793691635132, f1 0.8578947186470032
ep18_l3_test_time 0.20111768300000676
Test Epoch18 layer4 Acc 0.8568421052631578, AUC 0.9506377577781677, avg_entr 0.01875143311917782, f1 0.8568421006202698
ep18_l4_test_time 0.23351577099992937
gc 0
Train Epoch19 Acc 0.9758583333333334 (117103/120000), AUC 0.9967736601829529
ep19_train_time 25.830592375000037
Test Epoch19 layer0 Acc 0.8307894736842105, AUC 0.9488670825958252, avg_entr 0.08970551192760468, f1 0.8307894468307495
ep19_l0_test_time 0.12952931200004514
Test Epoch19 layer1 Acc 0.8489473684210527, AUC 0.9554906487464905, avg_entr 0.03818757086992264, f1 0.8489473462104797
ep19_l1_test_time 0.14033661099995243
Test Epoch19 layer2 Acc 0.8592105263157894, AUC 0.9526082277297974, avg_entr 0.025908038020133972, f1 0.8592105507850647
ep19_l2_test_time 0.1673488559999896
Test Epoch19 layer3 Acc 0.8547368421052631, AUC 0.9527767300605774, avg_entr 0.019664341583848, f1 0.854736864566803
ep19_l3_test_time 0.1920127370000273
Test Epoch19 layer4 Acc 0.8542105263157894, AUC 0.9498919248580933, avg_entr 0.017014451324939728, f1 0.8542105555534363
ep19_l4_test_time 0.23418185499997435
Best AUC tensor(0.8718) 8 3
train_as_loss [[4.56175985e+02 3.56573306e+02 3.51077638e+02 3.49844385e+02
  3.49374025e+02 3.49146704e+02 3.49020936e+02 3.48945052e+02
  3.48896444e+02 3.48863943e+02 3.48841507e+02 3.48825641e+02
  3.48814211e+02 3.48807576e+02 3.48803655e+02 3.48800030e+02
  3.48796752e+02 3.48794526e+02 3.48793033e+02 3.48791532e+02]
 [1.79093366e+00 1.34916713e-05 9.93706068e-07 2.01735522e-07
  6.16112782e-08 2.36920978e-08 1.71509862e-06 2.56762889e-05
  7.26289741e-05 1.71294761e-04 3.00656868e-04 4.85684145e-06
  1.56724385e-09 1.03638552e-09 1.20027155e-04 5.45699163e-07
  5.50462846e-10 7.96664600e-10 3.48815946e-05 7.89880503e-08]
 [2.14229536e+00 2.33990688e-05 1.81924774e-06 3.80047667e-07
  1.20098241e-07 4.92400659e-08 2.03040287e-06 3.33897347e-05
  9.49200639e-05 2.93511734e-04 4.88933442e-04 1.54438573e-05
  4.01557827e-09 3.26418957e-09 2.18966799e-04 1.08913646e-06
  1.37346368e-09 1.70106780e-09 6.06507817e-05 2.39640846e-07]
 [2.03977213e+00 1.93048596e-05 1.66645010e-06 3.85716044e-07
  1.47638252e-07 7.77063645e-08 1.68080846e-06 4.35461376e-05
  1.52350166e-04 5.15704517e-04 8.02964229e-04 3.31531103e-05
  1.53455757e-08 1.47058957e-08 3.57101979e-04 1.52472102e-06
  5.03706345e-09 6.43404649e-09 1.01247419e-04 5.61847192e-07]
 [2.02788183e+00 1.94532159e-05 1.91777636e-06 5.26972188e-07
  2.58886883e-07 1.73457611e-07 2.15203846e-06 5.11295863e-05
  1.86906176e-04 7.00629451e-04 1.10948824e-03 4.56718393e-05
  5.19405954e-08 5.69376229e-08 5.17855446e-04 2.15397032e-06
  1.72113474e-08 2.26416376e-08 1.36344537e-04 1.03502836e-06]]
train_ae_loss [[16.68441177 16.44283986 15.96540316 14.91635901 14.32119259 13.85126702
  13.68112174 13.51385728 13.29601153 13.16530568 13.02832519 12.89756206
  12.72974703 11.79996072 11.54581618 11.39434612 11.33952483 10.8000977
  10.70309787 10.63060975]
 [13.80300962 13.09859728 12.53758992 11.55598319 10.54338483  9.60239152
   9.07933281  8.67735274  8.20353665  7.84143396  7.45361112  6.89999313
   6.58822472  5.85206102  5.54045686  5.13226871  5.00762661  4.60218749
   4.46119474  4.25455223]
 [13.82312553 12.55069006 11.41157725 10.22687924  9.21228465  8.23779944
   7.6337235   7.16111268  6.65168579  6.27398111  5.84844399  5.17464677
   4.81760227  4.19343729  3.90692388  3.43585032  3.31072164  2.97731136
   2.85095634  2.61496984]
 [13.781651   12.62202813 10.54904626  9.11955051  8.20476343  7.31308161
   6.77868003  6.32599132  5.79873029  5.44810269  5.01796372  4.28539362
   3.95831366  3.43156905  3.17643014  2.71621525  2.600727    2.33530834
   2.22450832  1.99933113]
 [14.8897445  15.30226704 11.26887046  9.57915861  8.64928751  7.73110185
   7.17435442  6.6849936   6.08307959  5.71731394  5.23182283  4.39013727
   4.04450655  3.52336129  3.25241155  2.74568814  2.62904444  2.36402374
   2.25087783  2.00797021]]
valid_acc (5, 20)
valid_AUC (5, 20)
train_acc (20,)
total_train+valid_time 543.108812409
Start Testing
Load ckpt at ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt
gc 9
Test layer0 Acc 0.8184210526315789, AUC 0.9487811326980591, avg_entr 0.1489877551794052, f1 0.8184210658073425
l0_test_time 0.12035194600002797
gc 0
Test layer1 Acc 0.8463157894736842, AUC 0.9627370238304138, avg_entr 0.07341320067644119, f1 0.8463158011436462
l1_test_time 0.1440477969999847
gc 0
Test layer2 Acc 0.8552631578947368, AUC 0.9643533825874329, avg_entr 0.056280717253685, f1 0.8552631735801697
l2_test_time 0.16626998100002766
gc 0
Test layer3 Acc 0.8557894736842105, AUC 0.962464451789856, avg_entr 0.048519719392061234, f1 0.8557894825935364
l3_test_time 0.19463566700005686
gc 0
Test layer4 Acc 0.8552631578947368, AUC 0.9613178968429565, avg_entr 0.041882164776325226, f1 0.8552631735801697
l4_test_time 0.23108023300005698
gc 0
Test threshold 0.1 Acc 0.8507894736842105, AUC 0.956045925617218, avg_entr 0.04035257548093796, f1 0.850789487361908
t0.1_test_time 0.24373889299999973
gc 0
Test threshold 0.2 Acc 0.8481578947368421, AUC 0.9546593427658081, avg_entr 0.050875090062618256, f1 0.8481578826904297
t0.2_test_time 0.23045183199997155
gc 0
Test threshold 0.3 Acc 0.8413157894736842, AUC 0.9535203576087952, avg_entr 0.06352858990430832, f1 0.8413158059120178
t0.3_test_time 0.23510766399999738
gc 0
Test threshold 0.4 Acc 0.8305263157894737, AUC 0.9517682790756226, avg_entr 0.08646073937416077, f1 0.8305262923240662
t0.4_test_time 0.21177824000005785
gc 0
Test threshold 0.5 Acc 0.8197368421052632, AUC 0.949200451374054, avg_entr 0.10321035236120224, f1 0.8197368383407593
t0.5_test_time 0.1720903029998908
gc 0
Test threshold 0.6 Acc 0.8184210526315789, AUC 0.9487811326980591, avg_entr 0.10747194290161133, f1 0.8184210658073425
t0.6_test_time 0.14797396300002674
gc 0
Test threshold 0.7 Acc 0.8184210526315789, AUC 0.9487811326980591, avg_entr 0.10747194290161133, f1 0.8184210658073425
t0.7_test_time 0.15170804199999566
gc 0
Test threshold 0.8 Acc 0.8184210526315789, AUC 0.9487811326980591, avg_entr 0.10747194290161133, f1 0.8184210658073425
t0.8_test_time 0.15059339499998714
gc 0
Test threshold 0.9 Acc 0.8184210526315789, AUC 0.9487811326980591, avg_entr 0.10747194290161133, f1 0.8184210658073425
t0.9_test_time 0.14746532200001639
