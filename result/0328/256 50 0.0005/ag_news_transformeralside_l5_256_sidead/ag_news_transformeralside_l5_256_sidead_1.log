total count words 102019
vocab size 30000
train size 120000, valid size 3800, test size 3800
found 26754 words in glove
model: TransformerALsideText(
  (layers): ModuleList(
    (0): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=4, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=4, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
  (emb_layers): ModuleList(
    (0): Embedding(30000, 300)
    (1): Embedding(30000, 300)
    (2): Embedding(30000, 300)
    (3): Embedding(30000, 300)
    (4): Embedding(30000, 300)
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.multi_headed_attention.linears.0.weight 90000
layers.0.enc.f.multi_headed_attention.linears.0.bias 300
layers.0.enc.f.multi_headed_attention.linears.1.weight 90000
layers.0.enc.f.multi_headed_attention.linears.1.bias 300
layers.0.enc.f.multi_headed_attention.linears.2.weight 90000
layers.0.enc.f.multi_headed_attention.linears.2.bias 300
layers.0.enc.f.multi_headed_attention.linears.3.weight 90000
layers.0.enc.f.multi_headed_attention.linears.3.bias 300
layers.0.enc.f.feed_forward.w_1.weight 90000
layers.0.enc.f.feed_forward.w_1.bias 300
layers.0.enc.f.feed_forward.w_2.weight 90000
layers.0.enc.f.feed_forward.w_2.bias 300
layers.0.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.0.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.0.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.0.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.0.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.0.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.0.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.0.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.0.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.0.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.0.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.0.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.0.enc.f.encoder.norm.a 300
layers.0.enc.f.encoder.norm.b 300
layers.0.ae.g.0.weight 512
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 512
layers.0.ae.h.0.bias 4
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
emb_layers.0.weight 9000000
emb_layers.1.weight 9000000
emb_layers.2.weight 9000000
emb_layers.3.weight 9000000
emb_layers.4.weight 9000000
Total Trainable Params: 50758892
init_time 25.450506929
Start Training
gc 0
Train Epoch0 Acc 0.287625 (34515/120000), AUC 0.5359757542610168
ep0_train_time 26.235774842
Test Epoch0 layer0 Acc 0.6892105263157895, AUC 0.8769853115081787, avg_entr 0.8874552845954895, f1 0.6892105340957642
ep0_l0_test_time 0.1275335240000004
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 0
Test Epoch0 layer1 Acc 0.7113157894736842, AUC 0.9051816463470459, avg_entr 0.796660840511322, f1 0.7113158106803894
ep0_l1_test_time 0.14198002399999865
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 0
Test Epoch0 layer2 Acc 0.7007894736842105, AUC 0.9054669141769409, avg_entr 0.7917852997779846, f1 0.7007894515991211
ep0_l2_test_time 0.16755778199999583
Test Epoch0 layer3 Acc 0.6805263157894736, AUC 0.9007865786552429, avg_entr 0.8928519487380981, f1 0.6805263161659241
ep0_l3_test_time 0.19376166900000413
Test Epoch0 layer4 Acc 0.3813157894736842, AUC 0.8718420267105103, avg_entr 0.9310793876647949, f1 0.38131576776504517
ep0_l4_test_time 0.23217884099999964
gc 0
Train Epoch1 Acc 0.7120333333333333 (85444/120000), AUC 0.8847426772117615
ep1_train_time 25.184594332000003
Test Epoch1 layer0 Acc 0.76, AUC 0.9227849841117859, avg_entr 0.48160403966903687, f1 0.7599999904632568
ep1_l0_test_time 0.12690538400001117
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 1
Test Epoch1 layer1 Acc 0.7978947368421052, AUC 0.9400646686553955, avg_entr 0.3759710490703583, f1 0.7978947162628174
ep1_l1_test_time 0.1415914579999935
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 1
Test Epoch1 layer2 Acc 0.7984210526315789, AUC 0.9448773860931396, avg_entr 0.3568037152290344, f1 0.7984210252761841
ep1_l2_test_time 0.16768357000000833
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 1
Test Epoch1 layer3 Acc 0.7947368421052632, AUC 0.9447569251060486, avg_entr 0.3610859513282776, f1 0.7947368621826172
ep1_l3_test_time 0.1967254959999991
Test Epoch1 layer4 Acc 0.7871052631578948, AUC 0.944214940071106, avg_entr 0.3800731599330902, f1 0.7871052622795105
ep1_l4_test_time 0.23429011700000046
gc 0
Train Epoch2 Acc 0.8189166666666666 (98270/120000), AUC 0.9441088438034058
ep2_train_time 25.762401972999996
Test Epoch2 layer0 Acc 0.7889473684210526, AUC 0.9408615827560425, avg_entr 0.3198510706424713, f1 0.788947343826294
ep2_l0_test_time 0.1278203970000078
Test Epoch2 layer1 Acc 0.8223684210526315, AUC 0.953280508518219, avg_entr 0.23979942500591278, f1 0.8223684430122375
ep2_l1_test_time 0.14088923100000272
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 2
Test Epoch2 layer2 Acc 0.8281578947368421, AUC 0.9563864469528198, avg_entr 0.21242588758468628, f1 0.828157901763916
ep2_l2_test_time 0.16775984200000948
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 2
Test Epoch2 layer3 Acc 0.83, AUC 0.9572185277938843, avg_entr 0.19245825707912445, f1 0.8299999833106995
ep2_l3_test_time 0.19867211599999735
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 2
Test Epoch2 layer4 Acc 0.8281578947368421, AUC 0.9570436477661133, avg_entr 0.18447625637054443, f1 0.828157901763916
ep2_l4_test_time 0.23506659999999613
gc 0
Train Epoch3 Acc 0.8560416666666667 (102725/120000), AUC 0.9604711532592773
ep3_train_time 25.464754113000012
Test Epoch3 layer0 Acc 0.8139473684210526, AUC 0.9464681148529053, avg_entr 0.24229729175567627, f1 0.8139473795890808
ep3_l0_test_time 0.161068207999989
Test Epoch3 layer1 Acc 0.8413157894736842, AUC 0.9577531814575195, avg_entr 0.16785970330238342, f1 0.8413158059120178
ep3_l1_test_time 0.18101949900000136
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 3
Test Epoch3 layer2 Acc 0.8473684210526315, AUC 0.960722029209137, avg_entr 0.14351946115493774, f1 0.8473684191703796
ep3_l2_test_time 0.1998149490000003
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 3
Test Epoch3 layer3 Acc 0.8460526315789474, AUC 0.9607457518577576, avg_entr 0.13519980013370514, f1 0.8460525870323181
ep3_l3_test_time 0.2294726480000122
Test Epoch3 layer4 Acc 0.8468421052631578, AUC 0.9606590270996094, avg_entr 0.13221094012260437, f1 0.8468421101570129
ep3_l4_test_time 0.2357300379999856
gc 0
Train Epoch4 Acc 0.8759666666666667 (105116/120000), AUC 0.967029333114624
ep4_train_time 25.492903323999997
Test Epoch4 layer0 Acc 0.8155263157894737, AUC 0.9486750364303589, avg_entr 0.19862160086631775, f1 0.8155263066291809
ep4_l0_test_time 0.12323703699999555
Test Epoch4 layer1 Acc 0.8463157894736842, AUC 0.9592494368553162, avg_entr 0.11876662075519562, f1 0.8463158011436462
ep4_l1_test_time 0.14027106000000344
Test Epoch4 layer2 Acc 0.8494736842105263, AUC 0.9618852734565735, avg_entr 0.10455892980098724, f1 0.8494736552238464
ep4_l2_test_time 0.1670196199999907
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 4
Test Epoch4 layer3 Acc 0.8473684210526315, AUC 0.9622606039047241, avg_entr 0.09438037127256393, f1 0.8473684191703796
ep4_l3_test_time 0.19748082000000977
Test Epoch4 layer4 Acc 0.8473684210526315, AUC 0.9626293182373047, avg_entr 0.09272389113903046, f1 0.8473684191703796
ep4_l4_test_time 0.23123578799999223
gc 0
Train Epoch5 Acc 0.89155 (106986/120000), AUC 0.9726632237434387
ep5_train_time 26.338333355000003
Test Epoch5 layer0 Acc 0.8236842105263158, AUC 0.9495586156845093, avg_entr 0.1882443130016327, f1 0.8236842155456543
ep5_l0_test_time 0.16117361600001345
Test Epoch5 layer1 Acc 0.8539473684210527, AUC 0.962461531162262, avg_entr 0.09871196746826172, f1 0.8539473414421082
ep5_l1_test_time 0.17557976400001962
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 5
Test Epoch5 layer2 Acc 0.8586842105263158, AUC 0.9649844765663147, avg_entr 0.0844033882021904, f1 0.8586841821670532
ep5_l2_test_time 0.20139426399998683
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 5
Test Epoch5 layer3 Acc 0.8602631578947368, AUC 0.9651016592979431, avg_entr 0.07659998536109924, f1 0.8602631688117981
ep5_l3_test_time 0.22742988699999955
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 5
Test Epoch5 layer4 Acc 0.8607894736842105, AUC 0.9645258188247681, avg_entr 0.07052729278802872, f1 0.8607894778251648
ep5_l4_test_time 0.23467920599998138
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 5
gc 0
Train Epoch6 Acc 0.9031583333333333 (108379/120000), AUC 0.9766366481781006
ep6_train_time 26.01423106300001
Test Epoch6 layer0 Acc 0.8255263157894737, AUC 0.9501118659973145, avg_entr 0.16314919292926788, f1 0.825526237487793
ep6_l0_test_time 0.12821930399999815
Test Epoch6 layer1 Acc 0.8589473684210527, AUC 0.9629094004631042, avg_entr 0.08055153489112854, f1 0.8589473962783813
ep6_l1_test_time 0.14369493600000283
Test Epoch6 layer2 Acc 0.8652631578947368, AUC 0.9650104641914368, avg_entr 0.06377244740724564, f1 0.8652631640434265
ep6_l2_test_time 0.16504959200000258
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 6
Test Epoch6 layer3 Acc 0.8652631578947368, AUC 0.9649239778518677, avg_entr 0.05749461054801941, f1 0.8652631640434265
ep6_l3_test_time 0.1931852230000004
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 6
Test Epoch6 layer4 Acc 0.8652631578947368, AUC 0.9639589786529541, avg_entr 0.05076930671930313, f1 0.8652631640434265
ep6_l4_test_time 0.23131757999999536
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 6
gc 0
Train Epoch7 Acc 0.911775 (109413/120000), AUC 0.9792312383651733
ep7_train_time 26.132131216000005
Test Epoch7 layer0 Acc 0.8210526315789474, AUC 0.9513501524925232, avg_entr 0.15188010036945343, f1 0.821052610874176
ep7_l0_test_time 0.1609060420000219
Test Epoch7 layer1 Acc 0.8523684210526316, AUC 0.9607641100883484, avg_entr 0.07290270924568176, f1 0.8523684144020081
ep7_l1_test_time 0.17868206899998995
Test Epoch7 layer2 Acc 0.8602631578947368, AUC 0.9628572463989258, avg_entr 0.059241458773612976, f1 0.8602631688117981
ep7_l2_test_time 0.19983344299998862
Test Epoch7 layer3 Acc 0.8586842105263158, AUC 0.9605857133865356, avg_entr 0.051342546939849854, f1 0.8586841821670532
ep7_l3_test_time 0.2288219980000008
Test Epoch7 layer4 Acc 0.8571052631578947, AUC 0.9588602185249329, avg_entr 0.04473846033215523, f1 0.8571051955223083
ep7_l4_test_time 0.27032659899998635
gc 0
Train Epoch8 Acc 0.9200666666666667 (110408/120000), AUC 0.9818272590637207
ep8_train_time 26.774727974
Test Epoch8 layer0 Acc 0.8297368421052631, AUC 0.9524059295654297, avg_entr 0.14576268196105957, f1 0.8297368288040161
ep8_l0_test_time 0.12847862299997814
Test Epoch8 layer1 Acc 0.8581578947368421, AUC 0.9614577889442444, avg_entr 0.06674934923648834, f1 0.8581578731536865
ep8_l1_test_time 0.1419556999999827
Test Epoch8 layer2 Acc 0.8642105263157894, AUC 0.9637354612350464, avg_entr 0.0546279102563858, f1 0.8642105460166931
ep8_l2_test_time 0.1670153530000107
Test Epoch8 layer3 Acc 0.8605263157894737, AUC 0.9620707631111145, avg_entr 0.050479624420404434, f1 0.8605263233184814
ep8_l3_test_time 0.19659313100004283
Test Epoch8 layer4 Acc 0.8589473684210527, AUC 0.9605951309204102, avg_entr 0.04671860858798027, f1 0.8589473962783813
ep8_l4_test_time 0.2296936240000491
gc 0
Train Epoch9 Acc 0.9280416666666667 (111365/120000), AUC 0.9837875962257385
ep9_train_time 25.82784915299999
Test Epoch9 layer0 Acc 0.8305263157894737, AUC 0.9536137580871582, avg_entr 0.12999822199344635, f1 0.8305262923240662
ep9_l0_test_time 0.1569245169999931
Test Epoch9 layer1 Acc 0.8626315789473684, AUC 0.9627261161804199, avg_entr 0.06443897634744644, f1 0.862631618976593
ep9_l1_test_time 0.18147712400002547
Test Epoch9 layer2 Acc 0.8668421052631579, AUC 0.9657561182975769, avg_entr 0.04781120643019676, f1 0.8668420910835266
ep9_l2_test_time 0.20000440699999444
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 9
Test Epoch9 layer3 Acc 0.8626315789473684, AUC 0.9668763279914856, avg_entr 0.04113950580358505, f1 0.862631618976593
ep9_l3_test_time 0.22490917599998284
Test Epoch9 layer4 Acc 0.8652631578947368, AUC 0.9637097716331482, avg_entr 0.03662564978003502, f1 0.8652631640434265
ep9_l4_test_time 0.26449859900003503
gc 0
Train Epoch10 Acc 0.934075 (112089/120000), AUC 0.9872117042541504
ep10_train_time 25.722063571000035
Test Epoch10 layer0 Acc 0.8331578947368421, AUC 0.9531340599060059, avg_entr 0.12287534773349762, f1 0.8331578969955444
ep10_l0_test_time 0.12675300899996955
Test Epoch10 layer1 Acc 0.8621052631578947, AUC 0.960762619972229, avg_entr 0.05658524110913277, f1 0.8621052503585815
ep10_l1_test_time 0.1461718080000196
Test Epoch10 layer2 Acc 0.8668421052631579, AUC 0.9657163619995117, avg_entr 0.04538522660732269, f1 0.8668420910835266
ep10_l2_test_time 0.1665345160000129
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 10
Test Epoch10 layer3 Acc 0.8678947368421053, AUC 0.9647730588912964, avg_entr 0.03890109807252884, f1 0.86789470911026
ep10_l3_test_time 0.1990036480000299
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 10
Test Epoch10 layer4 Acc 0.8681578947368421, AUC 0.9632490277290344, avg_entr 0.033358439803123474, f1 0.8681579232215881
ep10_l4_test_time 0.23264548200000945
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 10
gc 0
Train Epoch11 Acc 0.9433916666666666 (113207/120000), AUC 0.9897031188011169
ep11_train_time 25.537461786999984
Test Epoch11 layer0 Acc 0.835, AUC 0.951582133769989, avg_entr 0.117533840239048, f1 0.8349999785423279
ep11_l0_test_time 0.15837691199999426
Test Epoch11 layer1 Acc 0.8605263157894737, AUC 0.9595234990119934, avg_entr 0.050434403121471405, f1 0.8605263233184814
ep11_l1_test_time 0.1784858100000406
Test Epoch11 layer2 Acc 0.8671052631578947, AUC 0.9614143371582031, avg_entr 0.04000410810112953, f1 0.86710524559021
ep11_l2_test_time 0.19889970200000562
Test Epoch11 layer3 Acc 0.8686842105263158, AUC 0.9602618217468262, avg_entr 0.03553003817796707, f1 0.8686841726303101
ep11_l3_test_time 0.22868583700000045
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 11
Test Epoch11 layer4 Acc 0.8671052631578947, AUC 0.9587093591690063, avg_entr 0.031813837587833405, f1 0.86710524559021
ep11_l4_test_time 0.2664674260000197
gc 0
Train Epoch12 Acc 0.9467333333333333 (113608/120000), AUC 0.9898175597190857
ep12_train_time 25.781458262
Test Epoch12 layer0 Acc 0.8276315789473684, AUC 0.9510802626609802, avg_entr 0.11436908692121506, f1 0.8276315927505493
ep12_l0_test_time 0.12591743100000485
Test Epoch12 layer1 Acc 0.8571052631578947, AUC 0.9580330848693848, avg_entr 0.050567131489515305, f1 0.8571051955223083
ep12_l1_test_time 0.13416491699996413
Test Epoch12 layer2 Acc 0.8621052631578947, AUC 0.961446225643158, avg_entr 0.03845807537436485, f1 0.8621052503585815
ep12_l2_test_time 0.16505847499996662
Test Epoch12 layer3 Acc 0.8639473684210527, AUC 0.9600123763084412, avg_entr 0.031610243022441864, f1 0.8639474511146545
ep12_l3_test_time 0.19837417399998003
Test Epoch12 layer4 Acc 0.8607894736842105, AUC 0.9589499831199646, avg_entr 0.028516260907053947, f1 0.8607894778251648
ep12_l4_test_time 0.2315712779999899
gc 0
Train Epoch13 Acc 0.951875 (114225/120000), AUC 0.9915614724159241
ep13_train_time 25.935386477999998
Test Epoch13 layer0 Acc 0.8302631578947368, AUC 0.9523601531982422, avg_entr 0.10785612463951111, f1 0.8302631378173828
ep13_l0_test_time 0.12479293000001235
Test Epoch13 layer1 Acc 0.8628947368421053, AUC 0.9574151039123535, avg_entr 0.047199271619319916, f1 0.8628947138786316
ep13_l1_test_time 0.1405223809999825
Test Epoch13 layer2 Acc 0.8692105263157894, AUC 0.9610403180122375, avg_entr 0.03376036137342453, f1 0.8692105412483215
ep13_l2_test_time 0.16746804399997473
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 13
Test Epoch13 layer3 Acc 0.8723684210526316, AUC 0.9595809578895569, avg_entr 0.027970625087618828, f1 0.8723683953285217
ep13_l3_test_time 0.1952926390000016
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 13
Test Epoch13 layer4 Acc 0.871578947368421, AUC 0.9575785994529724, avg_entr 0.024432316422462463, f1 0.8715789318084717
ep13_l4_test_time 0.23115718299999344
gc 0
Train Epoch14 Acc 0.9605083333333333 (115261/120000), AUC 0.9939380884170532
ep14_train_time 25.552817160000018
Test Epoch14 layer0 Acc 0.8347368421052631, AUC 0.952658474445343, avg_entr 0.10497094690799713, f1 0.8347368240356445
ep14_l0_test_time 0.12693262800002003
Test Epoch14 layer1 Acc 0.8652631578947368, AUC 0.956932008266449, avg_entr 0.04307938739657402, f1 0.8652631640434265
ep14_l1_test_time 0.1455583649999994
Test Epoch14 layer2 Acc 0.8731578947368421, AUC 0.9590540528297424, avg_entr 0.026841532438993454, f1 0.8731579184532166
ep14_l2_test_time 0.16535775300002342
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 14
Test Epoch14 layer3 Acc 0.8742105263157894, AUC 0.9580895304679871, avg_entr 0.021609261631965637, f1 0.87421053647995
ep14_l3_test_time 0.22797757200004298
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 14
Test Epoch14 layer4 Acc 0.8752631578947369, AUC 0.9573330283164978, avg_entr 0.018581535667181015, f1 0.8752631545066833
ep14_l4_test_time 0.26503051600002436
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 14
gc 0
Train Epoch15 Acc 0.9652833333333334 (115834/120000), AUC 0.9950295686721802
ep15_train_time 25.298646631999986
Test Epoch15 layer0 Acc 0.8357894736842105, AUC 0.9511232972145081, avg_entr 0.10066952556371689, f1 0.8357895016670227
ep15_l0_test_time 0.12665868200002706
Test Epoch15 layer1 Acc 0.8589473684210527, AUC 0.954487144947052, avg_entr 0.042598657310009, f1 0.8589473962783813
ep15_l1_test_time 0.14108203800003594
Test Epoch15 layer2 Acc 0.8681578947368421, AUC 0.954860270023346, avg_entr 0.026854289695620537, f1 0.8681579232215881
ep15_l2_test_time 0.163369533999969
Test Epoch15 layer3 Acc 0.8689473684210526, AUC 0.9559837579727173, avg_entr 0.022003568708896637, f1 0.8689473867416382
ep15_l3_test_time 0.1967374300000415
Test Epoch15 layer4 Acc 0.8681578947368421, AUC 0.9550681114196777, avg_entr 0.01944890059530735, f1 0.8681579232215881
ep15_l4_test_time 0.22948519300001635
gc 0
Train Epoch16 Acc 0.9672916666666667 (116075/120000), AUC 0.9950032234191895
ep16_train_time 25.946560513999998
Test Epoch16 layer0 Acc 0.835, AUC 0.9504392743110657, avg_entr 0.10003825277090073, f1 0.8349999785423279
ep16_l0_test_time 0.12403083100002732
Test Epoch16 layer1 Acc 0.8592105263157894, AUC 0.9549752473831177, avg_entr 0.04203064739704132, f1 0.8592105507850647
ep16_l1_test_time 0.1696202140000196
Test Epoch16 layer2 Acc 0.868421052631579, AUC 0.9575868844985962, avg_entr 0.026236865669488907, f1 0.8684210777282715
ep16_l2_test_time 0.20144979299999477
Test Epoch16 layer3 Acc 0.8681578947368421, AUC 0.9579529762268066, avg_entr 0.020554745569825172, f1 0.8681579232215881
ep16_l3_test_time 0.22911147399997844
Test Epoch16 layer4 Acc 0.8692105263157894, AUC 0.955327033996582, avg_entr 0.018348325043916702, f1 0.8692105412483215
ep16_l4_test_time 0.2655703190000054
gc 0
Train Epoch17 Acc 0.9697583333333334 (116371/120000), AUC 0.9954864978790283
ep17_train_time 25.841321685999958
Test Epoch17 layer0 Acc 0.8315789473684211, AUC 0.9514984488487244, avg_entr 0.09930479526519775, f1 0.8315790295600891
ep17_l0_test_time 0.16000728200003778
Test Epoch17 layer1 Acc 0.858421052631579, AUC 0.9540810585021973, avg_entr 0.04091634601354599, f1 0.8584210276603699
ep17_l1_test_time 0.18037291999996796
Test Epoch17 layer2 Acc 0.8647368421052631, AUC 0.9566915035247803, avg_entr 0.02705724537372589, f1 0.8647368550300598
ep17_l2_test_time 0.20078090799995607
Test Epoch17 layer3 Acc 0.865, AUC 0.9563202261924744, avg_entr 0.022321702912449837, f1 0.8650000095367432
ep17_l3_test_time 0.2302989110000908
Test Epoch17 layer4 Acc 0.8652631578947368, AUC 0.9549161791801453, avg_entr 0.020565928891301155, f1 0.8652631640434265
ep17_l4_test_time 0.26726071400003093
gc 0
Train Epoch18 Acc 0.9739166666666667 (116870/120000), AUC 0.996577262878418
ep18_train_time 25.239580142000023
Test Epoch18 layer0 Acc 0.8328947368421052, AUC 0.9500718116760254, avg_entr 0.09416578710079193, f1 0.8328947424888611
ep18_l0_test_time 0.13292642299995805
Test Epoch18 layer1 Acc 0.8576315789473684, AUC 0.9514063000679016, avg_entr 0.039458803832530975, f1 0.8576316237449646
ep18_l1_test_time 0.1448932150000246
Test Epoch18 layer2 Acc 0.8621052631578947, AUC 0.9544026851654053, avg_entr 0.025709204375743866, f1 0.8621052503585815
ep18_l2_test_time 0.1638097380000545
Test Epoch18 layer3 Acc 0.8623684210526316, AUC 0.9541574716567993, avg_entr 0.019599778577685356, f1 0.8623684048652649
ep18_l3_test_time 0.19601107100004356
Test Epoch18 layer4 Acc 0.8613157894736843, AUC 0.9525384306907654, avg_entr 0.017300235107541084, f1 0.8613157868385315
ep18_l4_test_time 0.2300222929999336
gc 0
Train Epoch19 Acc 0.9754416666666667 (117053/120000), AUC 0.9967520833015442
ep19_train_time 26.75689468600001
Test Epoch19 layer0 Acc 0.8355263157894737, AUC 0.9496236443519592, avg_entr 0.09691780060529709, f1 0.8355263471603394
ep19_l0_test_time 0.12419078700008868
Test Epoch19 layer1 Acc 0.8613157894736843, AUC 0.9499095678329468, avg_entr 0.03569420427083969, f1 0.8613157868385315
ep19_l1_test_time 0.14121560000000954
Test Epoch19 layer2 Acc 0.8642105263157894, AUC 0.9538116455078125, avg_entr 0.023197028785943985, f1 0.8642105460166931
ep19_l2_test_time 0.16523298600009184
Test Epoch19 layer3 Acc 0.866578947368421, AUC 0.9546226859092712, avg_entr 0.01915944553911686, f1 0.8665789365768433
ep19_l3_test_time 0.1997583710000299
Test Epoch19 layer4 Acc 0.865, AUC 0.9533759951591492, avg_entr 0.01707260124385357, f1 0.8650000095367432
ep19_l4_test_time 0.22969477100002678
Best AUC tensor(0.8753) 14 4
train_as_loss [[4.51149169e+02 3.56402860e+02 3.51014876e+02 3.49810135e+02
  3.49352862e+02 3.49132712e+02 3.49011286e+02 3.48938190e+02
  3.48891466e+02 3.48860276e+02 3.48838769e+02 3.48823582e+02
  3.48812658e+02 3.48804674e+02 3.48800008e+02 3.48797220e+02
  3.48794631e+02 3.48792302e+02 3.48790680e+02 3.48789608e+02]
 [1.58534023e+00 1.59880269e-05 1.14191028e-06 2.22819924e-07
  6.77151058e-08 2.55726667e-08 1.62254253e-07 1.07323736e-05
  5.94621763e-05 1.94653880e-04 2.03752298e-04 1.48024321e-07
  1.64932134e-09 2.39935833e-05 1.17378765e-04 2.80425620e-08
  5.01231660e-10 5.18207183e-10 2.87022846e-05 1.22449194e-08]
 [1.55498453e+00 1.44615483e-05 1.14588914e-06 2.36647489e-07
  7.82258189e-08 3.40006582e-08 1.02371337e-07 1.96408026e-05
  1.46873148e-04 4.19681577e-04 4.50499095e-04 2.24351620e-06
  3.33938584e-04 6.43420952e-04 1.24842044e-04 2.51924817e-07
  1.89131393e-09 1.03073373e-04 1.81042071e-05 1.06705233e-07]
 [1.76831354e+00 1.68936232e-05 1.52739578e-06 3.52394541e-07
  1.44236106e-07 8.22623894e-08 1.36654720e-07 3.06329566e-05
  2.85568955e-04 7.45739393e-04 7.60044439e-04 2.26028155e-05
  7.75063819e-04 1.19217449e-03 2.08678564e-04 1.09562910e-06
  1.25327700e-08 1.94613559e-04 1.54420409e-05 4.84158025e-07]
 [1.53308394e+00 1.37411454e-05 1.59534611e-06 4.98868384e-07
  3.02128491e-07 2.61113571e-07 2.72108541e-07 7.51630613e-05
  5.92728795e-04 1.29928752e-03 4.87877158e-04 1.65819409e-05
  5.09397007e-04 8.65675245e-04 1.59207274e-04 8.00155968e-07
  1.72018162e-08 1.46513178e-04 1.14967790e-05 3.74812591e-07]]
train_ae_loss [[17.25848003 17.55138687 16.98986935 15.99087264 15.36994037 14.96413246
  14.67383854 14.53216709 14.3566509  14.1656527  14.08897146 13.8697902
  13.87713971 13.69122861 12.61481769 12.32424639 12.1477901  12.10402275
  11.51688533 11.3737394 ]
 [14.96719771 14.40529007 13.55787701 12.44809694 11.41493271 10.47038729
   9.76117622  9.26699481  8.78579322  8.3591917   8.00219585  7.28297536
   7.12573344  6.84034042  5.97886225  5.47327964  5.24915728  5.12811979
   4.6943526   4.4249463 ]
 [15.79625794 14.16985328 12.79645335 11.45225954 10.43246039  9.50617982
   8.74022241  8.17120001  7.59761109  7.09528345  6.6923798   5.93652251
   5.76641569  5.39132596  4.44800052  3.96862182  3.73290867  3.60179298
   3.13928246  2.89106606]
 [15.95744169 13.71611367 11.59876729 10.05335582  9.12086117  8.22591385
   7.50068511  6.97636964  6.4247637   5.94617809  5.55619377  4.83597641
   4.70489034  4.33538937  3.46487655  3.0610954   2.85668492  2.74535113
   2.34454799  2.13995927]
 [16.36542279 14.66540514 11.27156869  9.5632287   8.6379747   7.81766589
   7.18346536  6.67216376  6.12505719  5.65899289  5.12034565  4.44992279
   4.33921063  3.99504346  3.18387284  2.81159028  2.61754402  2.51227291
   2.1390046   1.95071512]]
valid_acc (5, 20)
valid_AUC (5, 20)
train_acc (20,)
total_train+valid_time 547.90356307
Start Testing
Load ckpt at ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt
gc 9
Test layer0 Acc 0.8297368421052631, AUC 0.9501564502716064, avg_entr 0.0976845845580101, f1 0.8297368288040161
l0_test_time 0.12677682300000015
gc 0
Test layer1 Acc 0.855, AUC 0.956086277961731, avg_entr 0.03898780047893524, f1 0.8550000190734863
l1_test_time 0.13691249400005745
gc 0
Test layer2 Acc 0.8607894736842105, AUC 0.95855712890625, avg_entr 0.02783108688890934, f1 0.8607894778251648
l2_test_time 0.16546076099996299
gc 0
Test layer3 Acc 0.8626315789473684, AUC 0.9594290256500244, avg_entr 0.024461662396788597, f1 0.862631618976593
l3_test_time 0.19950653399996554
gc 0
Test layer4 Acc 0.8621052631578947, AUC 0.9564625024795532, avg_entr 0.020739039406180382, f1 0.8621052503585815
l4_test_time 0.23152461799998036
gc 0
Test threshold 0.1 Acc 0.8589473684210527, AUC 0.9516804218292236, avg_entr 0.019897043704986572, f1 0.8589473962783813
t0.1_test_time 0.23335359299994707
gc 0
Test threshold 0.2 Acc 0.853421052631579, AUC 0.9520384073257446, avg_entr 0.02843713015317917, f1 0.8534210324287415
t0.2_test_time 0.22898634600005607
gc 0
Test threshold 0.3 Acc 0.8426315789473684, AUC 0.9505837559700012, avg_entr 0.04547657445073128, f1 0.8426315784454346
t0.3_test_time 0.21940213300001687
gc 0
Test threshold 0.4 Acc 0.8326315789473684, AUC 0.9499928951263428, avg_entr 0.06083250045776367, f1 0.832631528377533
t0.4_test_time 0.19077451199996176
gc 0
Test threshold 0.5 Acc 0.8302631578947368, AUC 0.9501330852508545, avg_entr 0.0690208449959755, f1 0.8302631378173828
t0.5_test_time 0.16506909199995334
gc 0
Test threshold 0.6 Acc 0.8297368421052631, AUC 0.9501564502716064, avg_entr 0.07046454399824142, f1 0.8297368288040161
t0.6_test_time 0.14515330800008996
gc 0
Test threshold 0.7 Acc 0.8297368421052631, AUC 0.9501564502716064, avg_entr 0.07046454399824142, f1 0.8297368288040161
t0.7_test_time 0.14088420099994892
gc 0
Test threshold 0.8 Acc 0.8297368421052631, AUC 0.9501564502716064, avg_entr 0.07046454399824142, f1 0.8297368288040161
t0.8_test_time 0.15348325000002205
gc 0
Test threshold 0.9 Acc 0.8297368421052631, AUC 0.9501564502716064, avg_entr 0.07046454399824142, f1 0.8297368288040161
t0.9_test_time 0.14699077399995986
