total count words 102019
vocab size 30000
train size 120000, valid size 3800, test size 3800
found 26754 words in glove
model: TransformerALsideText(
  (layers): ModuleList(
    (0): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=4, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=4, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
  (emb_layers): ModuleList(
    (0): Embedding(30000, 300)
    (1): Embedding(30000, 300)
    (2): Embedding(30000, 300)
    (3): Embedding(30000, 300)
    (4): Embedding(30000, 300)
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.multi_headed_attention.linears.0.weight 90000
layers.0.enc.f.multi_headed_attention.linears.0.bias 300
layers.0.enc.f.multi_headed_attention.linears.1.weight 90000
layers.0.enc.f.multi_headed_attention.linears.1.bias 300
layers.0.enc.f.multi_headed_attention.linears.2.weight 90000
layers.0.enc.f.multi_headed_attention.linears.2.bias 300
layers.0.enc.f.multi_headed_attention.linears.3.weight 90000
layers.0.enc.f.multi_headed_attention.linears.3.bias 300
layers.0.enc.f.feed_forward.w_1.weight 90000
layers.0.enc.f.feed_forward.w_1.bias 300
layers.0.enc.f.feed_forward.w_2.weight 90000
layers.0.enc.f.feed_forward.w_2.bias 300
layers.0.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.0.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.0.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.0.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.0.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.0.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.0.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.0.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.0.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.0.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.0.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.0.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.0.enc.f.encoder.norm.a 300
layers.0.enc.f.encoder.norm.b 300
layers.0.ae.g.0.weight 512
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 512
layers.0.ae.h.0.bias 4
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
emb_layers.0.weight 9000000
emb_layers.1.weight 9000000
emb_layers.2.weight 9000000
emb_layers.3.weight 9000000
emb_layers.4.weight 9000000
Total Trainable Params: 50758892
init_time 25.522843407
Start Training
gc 0
Train Epoch0 Acc 0.270775 (32493/120000), AUC 0.5143393278121948
ep0_train_time 26.739214075000003
Test Epoch0 layer0 Acc 0.6734210526315789, AUC 0.873268723487854, avg_entr 0.9031135439872742, f1 0.6734210252761841
ep0_l0_test_time 0.13397066999999652
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 0
Test Epoch0 layer1 Acc 0.7018421052631579, AUC 0.9021242260932922, avg_entr 0.8126485347747803, f1 0.7018421292304993
ep0_l1_test_time 0.14967372000000267
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 0
Test Epoch0 layer2 Acc 0.7105263157894737, AUC 0.9028008580207825, avg_entr 0.8357724547386169, f1 0.7105262875556946
ep0_l2_test_time 0.17348655199999996
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 0
Test Epoch0 layer3 Acc 0.6197368421052631, AUC 0.8936114311218262, avg_entr 0.9785929918289185, f1 0.6197368502616882
ep0_l3_test_time 0.20585941600000268
Test Epoch0 layer4 Acc 0.43, AUC 0.844509482383728, avg_entr 1.198615550994873, f1 0.4300000071525574
ep0_l4_test_time 0.23927162199999685
gc 0
Train Epoch1 Acc 0.67505 (81006/120000), AUC 0.8664019107818604
ep1_train_time 25.522842926000003
Test Epoch1 layer0 Acc 0.7589473684210526, AUC 0.9212518930435181, avg_entr 0.48912572860717773, f1 0.7589473724365234
ep1_l0_test_time 0.12622869000000492
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 1
Test Epoch1 layer1 Acc 0.7957894736842105, AUC 0.9404692649841309, avg_entr 0.4049728214740753, f1 0.7957894802093506
ep1_l1_test_time 0.1420416529999926
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 1
Test Epoch1 layer2 Acc 0.7981578947368421, AUC 0.9458405375480652, avg_entr 0.38065698742866516, f1 0.7981578707695007
ep1_l2_test_time 0.16955905700000073
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 1
Test Epoch1 layer3 Acc 0.791578947368421, AUC 0.9463778734207153, avg_entr 0.38680681586265564, f1 0.7915789484977722
ep1_l3_test_time 0.19825187200000016
Test Epoch1 layer4 Acc 0.7813157894736842, AUC 0.9460852146148682, avg_entr 0.4309772551059723, f1 0.781315803527832
ep1_l4_test_time 0.23362973300000078
gc 0
Train Epoch2 Acc 0.81685 (98022/120000), AUC 0.9428635239601135
ep2_train_time 25.773208487000005
Test Epoch2 layer0 Acc 0.7913157894736842, AUC 0.94100421667099, avg_entr 0.31311506032943726, f1 0.7913157939910889
ep2_l0_test_time 0.13119447600000456
Test Epoch2 layer1 Acc 0.8084210526315789, AUC 0.9522014856338501, avg_entr 0.2423163652420044, f1 0.8084211349487305
ep2_l1_test_time 0.1433384719999964
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 2
Test Epoch2 layer2 Acc 0.8155263157894737, AUC 0.9557912945747375, avg_entr 0.20611834526062012, f1 0.8155263066291809
ep2_l2_test_time 0.16860886899999628
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 2
Test Epoch2 layer3 Acc 0.8131578947368421, AUC 0.9552231431007385, avg_entr 0.198004812002182, f1 0.8131579160690308
ep2_l3_test_time 0.19775587100001246
Test Epoch2 layer4 Acc 0.8128947368421052, AUC 0.9552932381629944, avg_entr 0.20526103675365448, f1 0.8128947615623474
ep2_l4_test_time 0.23556789799999933
gc 0
Train Epoch3 Acc 0.8549 (102588/120000), AUC 0.9597179889678955
ep3_train_time 26.474134458999984
Test Epoch3 layer0 Acc 0.8065789473684211, AUC 0.9459388256072998, avg_entr 0.24690508842468262, f1 0.8065789341926575
ep3_l0_test_time 0.12878427399999737
Test Epoch3 layer1 Acc 0.8342105263157895, AUC 0.958115816116333, avg_entr 0.17454218864440918, f1 0.8342105150222778
ep3_l1_test_time 0.14645814499999688
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 3
Test Epoch3 layer2 Acc 0.8436842105263158, AUC 0.9600764513015747, avg_entr 0.14187340438365936, f1 0.843684196472168
ep3_l2_test_time 0.20124667100000693
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 3
Test Epoch3 layer3 Acc 0.8442105263157895, AUC 0.9589629173278809, avg_entr 0.13323350250720978, f1 0.8442105054855347
ep3_l3_test_time 0.2286771900000133
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 3
Test Epoch3 layer4 Acc 0.8463157894736842, AUC 0.9597249031066895, avg_entr 0.12925758957862854, f1 0.8463158011436462
ep3_l4_test_time 0.26571670800001357
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 3
gc 0
Train Epoch4 Acc 0.87695 (105234/120000), AUC 0.96681809425354
ep4_train_time 25.775472142000012
Test Epoch4 layer0 Acc 0.8147368421052632, AUC 0.9490675330162048, avg_entr 0.2102876901626587, f1 0.8147368431091309
ep4_l0_test_time 0.12691399100000922
Test Epoch4 layer1 Acc 0.8421052631578947, AUC 0.9617264270782471, avg_entr 0.1242779940366745, f1 0.8421053290367126
ep4_l1_test_time 0.1446805280000092
Test Epoch4 layer2 Acc 0.8505263157894737, AUC 0.9640056490898132, avg_entr 0.10626527667045593, f1 0.8505263328552246
ep4_l2_test_time 0.17250483899999836
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 4
Test Epoch4 layer3 Acc 0.8492105263157895, AUC 0.9635443687438965, avg_entr 0.10003006458282471, f1 0.8492104411125183
ep4_l3_test_time 0.20032092100001364
Test Epoch4 layer4 Acc 0.8494736842105263, AUC 0.9632290601730347, avg_entr 0.09944997727870941, f1 0.8494736552238464
ep4_l4_test_time 0.23051430600000344
gc 0
Train Epoch5 Acc 0.8925166666666666 (107102/120000), AUC 0.9728204011917114
ep5_train_time 25.650198472
Test Epoch5 layer0 Acc 0.8163157894736842, AUC 0.950187087059021, avg_entr 0.18160878121852875, f1 0.8163158297538757
ep5_l0_test_time 0.16003481600000669
Test Epoch5 layer1 Acc 0.8463157894736842, AUC 0.9620515704154968, avg_entr 0.09931203722953796, f1 0.8463158011436462
ep5_l1_test_time 0.17469340600001715
Test Epoch5 layer2 Acc 0.8547368421052631, AUC 0.9631860852241516, avg_entr 0.08504021912813187, f1 0.854736864566803
ep5_l2_test_time 0.20562941000000023
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 5
Test Epoch5 layer3 Acc 0.8557894736842105, AUC 0.9630248546600342, avg_entr 0.07665377110242844, f1 0.8557894825935364
ep5_l3_test_time 0.23662173499999994
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 5
Test Epoch5 layer4 Acc 0.8555263157894737, AUC 0.9616490602493286, avg_entr 0.06890033930540085, f1 0.855526328086853
ep5_l4_test_time 0.2720687349999764
gc 0
Train Epoch6 Acc 0.9044833333333333 (108538/120000), AUC 0.9770107269287109
ep6_train_time 26.10952426899999
Test Epoch6 layer0 Acc 0.8286842105263158, AUC 0.9526928663253784, avg_entr 0.17372703552246094, f1 0.8286842107772827
ep6_l0_test_time 0.13055197500000304
Test Epoch6 layer1 Acc 0.848421052631579, AUC 0.9647619128227234, avg_entr 0.09015229344367981, f1 0.848421037197113
ep6_l1_test_time 0.14315519900000595
Test Epoch6 layer2 Acc 0.8563157894736843, AUC 0.9669121503829956, avg_entr 0.07198195904493332, f1 0.8563157916069031
ep6_l2_test_time 0.16995256499998845
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 6
Test Epoch6 layer3 Acc 0.8571052631578947, AUC 0.9646139144897461, avg_entr 0.06217876449227333, f1 0.8571051955223083
ep6_l3_test_time 0.19782046000000264
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 6
Test Epoch6 layer4 Acc 0.8573684210526316, AUC 0.9633304476737976, avg_entr 0.0572950504720211, f1 0.8573684096336365
ep6_l4_test_time 0.23433003200000257
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 6
gc 0
Train Epoch7 Acc 0.91295 (109554/120000), AUC 0.9798378348350525
ep7_train_time 25.414556108
Test Epoch7 layer0 Acc 0.8247368421052632, AUC 0.9521490931510925, avg_entr 0.15655340254306793, f1 0.8247368335723877
ep7_l0_test_time 0.12699655999998072
Test Epoch7 layer1 Acc 0.8465789473684211, AUC 0.961334228515625, avg_entr 0.08166129142045975, f1 0.8465789556503296
ep7_l1_test_time 0.14516489600001137
Test Epoch7 layer2 Acc 0.8505263157894737, AUC 0.9625340700149536, avg_entr 0.06491665542125702, f1 0.8505263328552246
ep7_l2_test_time 0.14929698199998143
Test Epoch7 layer3 Acc 0.8528947368421053, AUC 0.9604784846305847, avg_entr 0.05640529468655586, f1 0.8528947234153748
ep7_l3_test_time 0.19737810899999886
Test Epoch7 layer4 Acc 0.8513157894736842, AUC 0.9591778516769409, avg_entr 0.04991525784134865, f1 0.8513157963752747
ep7_l4_test_time 0.23320828199999255
gc 0
Train Epoch8 Acc 0.9222083333333333 (110665/120000), AUC 0.9829154014587402
ep8_train_time 26.12087241200001
Test Epoch8 layer0 Acc 0.8273684210526315, AUC 0.9524909853935242, avg_entr 0.14365601539611816, f1 0.827368438243866
ep8_l0_test_time 0.12677410099996678
Test Epoch8 layer1 Acc 0.8542105263157894, AUC 0.9619231224060059, avg_entr 0.06515385210514069, f1 0.8542105555534363
ep8_l1_test_time 0.14354180800000904
Test Epoch8 layer2 Acc 0.8602631578947368, AUC 0.9640553593635559, avg_entr 0.05041343346238136, f1 0.8602631688117981
ep8_l2_test_time 0.1647079930000359
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 8
Test Epoch8 layer3 Acc 0.8594736842105263, AUC 0.9606443047523499, avg_entr 0.043313588947057724, f1 0.859473705291748
ep8_l3_test_time 0.19909296499997708
Test Epoch8 layer4 Acc 0.8571052631578947, AUC 0.9574475884437561, avg_entr 0.03946360945701599, f1 0.8571051955223083
ep8_l4_test_time 0.23196528800002625
gc 0
Train Epoch9 Acc 0.9302083333333333 (111625/120000), AUC 0.9851658344268799
ep9_train_time 25.439580173000024
Test Epoch9 layer0 Acc 0.8257894736842105, AUC 0.9523648619651794, avg_entr 0.1318395882844925, f1 0.8257894515991211
ep9_l0_test_time 0.12668268499999158
Test Epoch9 layer1 Acc 0.8531578947368421, AUC 0.9620988368988037, avg_entr 0.06396400928497314, f1 0.8531579375267029
ep9_l1_test_time 0.14119954299997062
Test Epoch9 layer2 Acc 0.861578947368421, AUC 0.964557409286499, avg_entr 0.04992258548736572, f1 0.8615789413452148
ep9_l2_test_time 0.16390646700000389
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 9
Test Epoch9 layer3 Acc 0.8628947368421053, AUC 0.961122989654541, avg_entr 0.044135984033346176, f1 0.8628947138786316
ep9_l3_test_time 0.23180079700000533
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 9
Test Epoch9 layer4 Acc 0.8628947368421053, AUC 0.9590780735015869, avg_entr 0.040998443961143494, f1 0.8628947138786316
ep9_l4_test_time 0.24157411900000625
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 9
gc 0
Train Epoch10 Acc 0.9377166666666666 (112526/120000), AUC 0.9872688055038452
ep10_train_time 26.030483791999984
Test Epoch10 layer0 Acc 0.8313157894736842, AUC 0.9513805508613586, avg_entr 0.12170367687940598, f1 0.831315815448761
ep10_l0_test_time 0.12854092300000275
Test Epoch10 layer1 Acc 0.8592105263157894, AUC 0.9613335132598877, avg_entr 0.05594755709171295, f1 0.8592105507850647
ep10_l1_test_time 0.14454143299997213
Test Epoch10 layer2 Acc 0.8636842105263158, AUC 0.9660248160362244, avg_entr 0.04285454377532005, f1 0.8636842370033264
ep10_l2_test_time 0.16933793100002958
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 10
Test Epoch10 layer3 Acc 0.8652631578947368, AUC 0.9643572568893433, avg_entr 0.0378100723028183, f1 0.8652631640434265
ep10_l3_test_time 0.19692699400002311
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 10
Test Epoch10 layer4 Acc 0.8660526315789474, AUC 0.9630308151245117, avg_entr 0.034739430993795395, f1 0.8660526275634766
ep10_l4_test_time 0.2326962629999798
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 10
gc 0
Train Epoch11 Acc 0.949625 (113955/120000), AUC 0.9912567138671875
ep11_train_time 26.26691295400002
Test Epoch11 layer0 Acc 0.8289473684210527, AUC 0.9515870809555054, avg_entr 0.11476019024848938, f1 0.8289473652839661
ep11_l0_test_time 0.12936641700002838
Test Epoch11 layer1 Acc 0.8565789473684211, AUC 0.9577957391738892, avg_entr 0.052638016641139984, f1 0.8565789461135864
ep11_l1_test_time 0.14271294299999226
Test Epoch11 layer2 Acc 0.865, AUC 0.9618467688560486, avg_entr 0.03751897066831589, f1 0.8650000095367432
ep11_l2_test_time 0.16654834700000265
Test Epoch11 layer3 Acc 0.8655263157894737, AUC 0.9608115553855896, avg_entr 0.03393574059009552, f1 0.8655263185501099
ep11_l3_test_time 0.19522151299997859
Test Epoch11 layer4 Acc 0.8655263157894737, AUC 0.958831787109375, avg_entr 0.03168636932969093, f1 0.8655263185501099
ep11_l4_test_time 0.23198889899998676
gc 0
Train Epoch12 Acc 0.9549666666666666 (114596/120000), AUC 0.9922323226928711
ep12_train_time 25.93686988600001
Test Epoch12 layer0 Acc 0.8342105263157895, AUC 0.9512828588485718, avg_entr 0.11096829921007156, f1 0.8342105150222778
ep12_l0_test_time 0.1286348649999809
Test Epoch12 layer1 Acc 0.8573684210526316, AUC 0.959052562713623, avg_entr 0.04952661693096161, f1 0.8573684096336365
ep12_l1_test_time 0.14069339799999625
Test Epoch12 layer2 Acc 0.8621052631578947, AUC 0.9608138799667358, avg_entr 0.03572002053260803, f1 0.8621052503585815
ep12_l2_test_time 0.16342818000003945
Test Epoch12 layer3 Acc 0.86, AUC 0.9586672782897949, avg_entr 0.030904026702046394, f1 0.8600000143051147
ep12_l3_test_time 0.1974409919999971
Test Epoch12 layer4 Acc 0.8589473684210527, AUC 0.9572418332099915, avg_entr 0.028574395924806595, f1 0.8589473962783813
ep12_l4_test_time 0.23508995600002436
gc 0
Train Epoch13 Acc 0.957925 (114951/120000), AUC 0.9927415251731873
ep13_train_time 25.268351077000034
Test Epoch13 layer0 Acc 0.8365789473684211, AUC 0.9519195556640625, avg_entr 0.10615287721157074, f1 0.8365789651870728
ep13_l0_test_time 0.1287135499999863
Test Epoch13 layer1 Acc 0.8528947368421053, AUC 0.956167459487915, avg_entr 0.04657766968011856, f1 0.8528947234153748
ep13_l1_test_time 0.14278584000004457
Test Epoch13 layer2 Acc 0.8597368421052631, AUC 0.9575556516647339, avg_entr 0.03669160231947899, f1 0.8597368597984314
ep13_l2_test_time 0.164825038999993
Test Epoch13 layer3 Acc 0.8592105263157894, AUC 0.9541555643081665, avg_entr 0.0312321949750185, f1 0.8592105507850647
ep13_l3_test_time 0.1963109899999722
Test Epoch13 layer4 Acc 0.8576315789473684, AUC 0.9521278142929077, avg_entr 0.028990009799599648, f1 0.8576316237449646
ep13_l4_test_time 0.23265937000002168
gc 0
Train Epoch14 Acc 0.9609333333333333 (115312/120000), AUC 0.9936181306838989
ep14_train_time 25.612052360000007
Test Epoch14 layer0 Acc 0.8302631578947368, AUC 0.951012372970581, avg_entr 0.10401439666748047, f1 0.8302631378173828
ep14_l0_test_time 0.126393921999977
Test Epoch14 layer1 Acc 0.8521052631578947, AUC 0.9567697048187256, avg_entr 0.048765599727630615, f1 0.8521052598953247
ep14_l1_test_time 0.14170204999999214
Test Epoch14 layer2 Acc 0.8592105263157894, AUC 0.9569331407546997, avg_entr 0.036117181181907654, f1 0.8592105507850647
ep14_l2_test_time 0.16837593499997183
Test Epoch14 layer3 Acc 0.8610526315789474, AUC 0.9540736675262451, avg_entr 0.030229460448026657, f1 0.8610526323318481
ep14_l3_test_time 0.1984213430000068
Test Epoch14 layer4 Acc 0.86, AUC 0.9541462659835815, avg_entr 0.028830623254179955, f1 0.8600000143051147
ep14_l4_test_time 0.2312966899999651
gc 0
Train Epoch15 Acc 0.9666583333333333 (115999/120000), AUC 0.9949296712875366
ep15_train_time 25.263123895999968
Test Epoch15 layer0 Acc 0.8318421052631579, AUC 0.9504973292350769, avg_entr 0.09984112530946732, f1 0.8318421840667725
ep15_l0_test_time 0.12817053899999564
Test Epoch15 layer1 Acc 0.8552631578947368, AUC 0.9556817412376404, avg_entr 0.04302332550287247, f1 0.8552631735801697
ep15_l1_test_time 0.1437920120000058
Test Epoch15 layer2 Acc 0.8618421052631579, AUC 0.9556720852851868, avg_entr 0.032770443707704544, f1 0.8618420958518982
ep15_l2_test_time 0.16808547499999804
Test Epoch15 layer3 Acc 0.8581578947368421, AUC 0.9542672038078308, avg_entr 0.02750515751540661, f1 0.8581578731536865
ep15_l3_test_time 0.19807430499997736
Test Epoch15 layer4 Acc 0.8560526315789474, AUC 0.9532039165496826, avg_entr 0.02503369003534317, f1 0.8560526371002197
ep15_l4_test_time 0.2327227829999856
gc 0
Train Epoch16 Acc 0.9690583333333334 (116287/120000), AUC 0.9955944418907166
ep16_train_time 25.653094140000007
Test Epoch16 layer0 Acc 0.8318421052631579, AUC 0.950814962387085, avg_entr 0.09917907416820526, f1 0.8318421840667725
ep16_l0_test_time 0.13161495599996442
Test Epoch16 layer1 Acc 0.8518421052631578, AUC 0.9559089541435242, avg_entr 0.04471396654844284, f1 0.8518421053886414
ep16_l1_test_time 0.14427358800003276
Test Epoch16 layer2 Acc 0.8523684210526316, AUC 0.9529528617858887, avg_entr 0.030812958255410194, f1 0.8523684144020081
ep16_l2_test_time 0.1668940980000002
Test Epoch16 layer3 Acc 0.8539473684210527, AUC 0.9525321125984192, avg_entr 0.023714488372206688, f1 0.8539473414421082
ep16_l3_test_time 0.19828144299998485
Test Epoch16 layer4 Acc 0.8547368421052631, AUC 0.9520830512046814, avg_entr 0.021147729828953743, f1 0.854736864566803
ep16_l4_test_time 0.23155923500002018
gc 0
Train Epoch17 Acc 0.9714166666666667 (116570/120000), AUC 0.995579183101654
ep17_train_time 26.31324090999999
Test Epoch17 layer0 Acc 0.8336842105263158, AUC 0.9504210948944092, avg_entr 0.09805208444595337, f1 0.8336842060089111
ep17_l0_test_time 0.12627306700005647
Test Epoch17 layer1 Acc 0.8515789473684211, AUC 0.9531101584434509, avg_entr 0.044971615076065063, f1 0.851578950881958
ep17_l1_test_time 0.14710293699999966
Test Epoch17 layer2 Acc 0.8560526315789474, AUC 0.9529722929000854, avg_entr 0.03005470708012581, f1 0.8560526371002197
ep17_l2_test_time 0.1573633970000401
Test Epoch17 layer3 Acc 0.8542105263157894, AUC 0.9526095390319824, avg_entr 0.024380726739764214, f1 0.8542105555534363
ep17_l3_test_time 0.19588295999994898
Test Epoch17 layer4 Acc 0.8544736842105263, AUC 0.9486180543899536, avg_entr 0.02234383299946785, f1 0.8544737100601196
ep17_l4_test_time 0.23607977899996513
gc 0
Train Epoch18 Acc 0.9724333333333334 (116692/120000), AUC 0.9960130453109741
ep18_train_time 25.64602663300002
Test Epoch18 layer0 Acc 0.8326315789473684, AUC 0.9498995542526245, avg_entr 0.09477467834949493, f1 0.832631528377533
ep18_l0_test_time 0.12439542399999937
Test Epoch18 layer1 Acc 0.8505263157894737, AUC 0.9538082480430603, avg_entr 0.04316290467977524, f1 0.8505263328552246
ep18_l1_test_time 0.14538797800003067
Test Epoch18 layer2 Acc 0.8555263157894737, AUC 0.9492771029472351, avg_entr 0.028643563389778137, f1 0.855526328086853
ep18_l2_test_time 0.16667223299998568
Test Epoch18 layer3 Acc 0.855, AUC 0.9454894065856934, avg_entr 0.023561807349324226, f1 0.8550000190734863
ep18_l3_test_time 0.20360634299993308
Test Epoch18 layer4 Acc 0.853421052631579, AUC 0.9403965473175049, avg_entr 0.021163159981369972, f1 0.8534210324287415
ep18_l4_test_time 0.23232826499997827
gc 0
Train Epoch19 Acc 0.97455 (116946/120000), AUC 0.996554434299469
ep19_train_time 25.856014106999964
Test Epoch19 layer0 Acc 0.8328947368421052, AUC 0.9503253698348999, avg_entr 0.09462319314479828, f1 0.8328947424888611
ep19_l0_test_time 0.16089330600004814
Test Epoch19 layer1 Acc 0.8528947368421053, AUC 0.9532675743103027, avg_entr 0.04057948291301727, f1 0.8528947234153748
ep19_l1_test_time 0.17826627300007658
Test Epoch19 layer2 Acc 0.8573684210526316, AUC 0.9484395980834961, avg_entr 0.02684273198246956, f1 0.8573684096336365
ep19_l2_test_time 0.20004594499994255
Test Epoch19 layer3 Acc 0.8576315789473684, AUC 0.9494381546974182, avg_entr 0.023082895204424858, f1 0.8576316237449646
ep19_l3_test_time 0.2309521050000285
Test Epoch19 layer4 Acc 0.8563157894736843, AUC 0.9457874894142151, avg_entr 0.021745964884757996, f1 0.8563157916069031
ep19_l4_test_time 0.26867258899994795
Best AUC tensor(0.8661) 10 4
train_as_loss [[4.52564360e+02 3.56677019e+02 3.51109105e+02 3.49854873e+02
  3.49377842e+02 3.49148026e+02 3.49021248e+02 3.48944946e+02
  3.48896175e+02 3.48863633e+02 3.48841202e+02 3.48828557e+02
  3.48821251e+02 3.48814578e+02 3.48808648e+02 3.48804635e+02
  3.48801997e+02 3.48799363e+02 3.48796816e+02 3.48794951e+02]
 [1.75336104e+00 1.94298085e-05 1.52270785e-06 3.15502040e-07
  9.72727970e-08 3.62971601e-08 1.58432887e-08 9.74844577e-06
  7.07490577e-05 1.46570602e-04 2.24307549e-04 1.13837613e-06
  8.31484407e-10 2.25519395e-08 1.36161865e-04 2.50961483e-06
  3.53390789e-10 3.63992922e-10 1.94315715e-05 4.31290566e-07]
 [1.67980145e+00 1.52456499e-05 1.13552355e-06 2.41144332e-07
  8.14779904e-08 3.31422131e-08 1.79708549e-08 1.23073018e-05
  1.50316501e-04 3.67984979e-04 5.97804913e-04 2.90027693e-06
  1.78234823e-09 2.67532710e-08 3.32746843e-04 7.84431074e-06
  7.23460464e-10 1.05505124e-09 4.16047765e-05 1.50006486e-06]
 [1.61320374e+00 1.90026901e-05 1.59112587e-06 3.75491294e-07
  1.53933722e-07 7.05310487e-08 4.92999631e-08 2.13528295e-05
  3.09044235e-04 6.71423229e-04 1.05809993e-03 4.32385416e-06
  6.19757226e-09 3.20669751e-08 5.24025219e-04 1.86054070e-05
  2.38102574e-09 3.70351713e-09 6.91783792e-05 2.18759149e-06]
 [1.84339223e+00 2.38208854e-05 2.31486987e-06 6.83035995e-07
  4.05378683e-07 2.23611287e-07 1.79808129e-07 2.78670330e-05
  4.50479539e-04 9.37178583e-04 1.57312670e-03 5.35023672e-06
  2.16331079e-08 5.70174527e-08 7.25672974e-04 3.41217695e-05
  8.23866399e-09 1.27922004e-08 9.59624365e-05 3.19900052e-06]]
train_ae_loss [[16.69007114 15.89049506 15.20533049 14.21733902 13.74574883 13.47416025
  13.26701913 13.14621196 12.94478404 12.81394152 12.62506731 11.76781768
  11.52181521 11.41649969 11.32752726 10.85277215 10.71400948 10.70455167
  10.69934214 10.42350136]
 [13.71480702 13.10379846 12.24744022 11.22380907 10.13895406  9.21292267
   8.60963759  8.17419141  7.68932689  7.33193428  6.92682239  5.90486137
   5.59063098  5.36442757  5.16911738  4.63156764  4.42828073  4.36077889
   4.25611162  3.95385559]
 [14.5856854  12.75773057 11.23861243  9.9829156   8.92954122  8.04398863
   7.4523235   6.99016149  6.48506925  6.09984719  5.64606633  4.59673106
   4.2800784   4.05081251  3.82669459  3.27857415  3.07043101  2.96693842
   2.90276904  2.58107246]
 [13.4016072  12.13459854  9.7837409   8.56157266  7.65040404  6.86310653
   6.3122546   5.87670462  5.41228019  5.03218047  4.60992491  3.63236095
   3.37143412  3.17987945  2.98075617  2.49508667  2.3265566   2.2341015
   2.18685922  1.90658835]
 [13.97707538 13.87756864 10.04222721  8.56388716  7.65391893  6.86839632
   6.25282411  5.79672476  5.29424775  4.89516127  4.45314521  3.43655304
   3.19655561  3.0074938   2.81511783  2.33044533  2.17260647  2.08494342
   2.04439807  1.76642383]]
valid_acc (5, 20)
valid_AUC (5, 20)
train_acc (20,)
total_train+valid_time 546.254350769
Start Testing
Load ckpt at ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt
gc 9
Test layer0 Acc 0.8234210526315789, AUC 0.9492230415344238, avg_entr 0.1231461688876152, f1 0.823421061038971
l0_test_time 0.1604715109999688
gc 0
Test layer1 Acc 0.8573684210526316, AUC 0.9619534015655518, avg_entr 0.055099885910749435, f1 0.8573684096336365
l1_test_time 0.17841671500002576
gc 0
Test layer2 Acc 0.8652631578947368, AUC 0.9636780619621277, avg_entr 0.04042227193713188, f1 0.8652631640434265
l2_test_time 0.20114877500009243
gc 0
Test layer3 Acc 0.8686842105263158, AUC 0.9641337990760803, avg_entr 0.03474222868680954, f1 0.8686841726303101
l3_test_time 0.22685825100006696
gc 0
Test layer4 Acc 0.868421052631579, AUC 0.9615718126296997, avg_entr 0.03147844970226288, f1 0.8684210777282715
l4_test_time 0.26630645699992783
gc 0
Test threshold 0.1 Acc 0.8618421052631579, AUC 0.9564538598060608, avg_entr 0.029130995273590088, f1 0.8618420958518982
t0.1_test_time 0.36967765099996086
gc 0
Test threshold 0.2 Acc 0.8563157894736843, AUC 0.9555479288101196, avg_entr 0.039128486067056656, f1 0.8563157916069031
t0.2_test_time 0.3553464960000383
gc 0
Test threshold 0.3 Acc 0.8473684210526315, AUC 0.9531627893447876, avg_entr 0.05418957397341728, f1 0.8473684191703796
t0.3_test_time 0.34782479199998306
gc 0
Test threshold 0.4 Acc 0.8355263157894737, AUC 0.9505893588066101, avg_entr 0.07473209500312805, f1 0.8355263471603394
t0.4_test_time 0.3041424159999906
gc 0
Test threshold 0.5 Acc 0.8252631578947368, AUC 0.9495111703872681, avg_entr 0.08610334992408752, f1 0.8252631425857544
t0.5_test_time 0.23843419299998914
gc 0
Test threshold 0.6 Acc 0.8234210526315789, AUC 0.9492230415344238, avg_entr 0.08883117884397507, f1 0.823421061038971
t0.6_test_time 0.20662988700007645
gc 0
Test threshold 0.7 Acc 0.8234210526315789, AUC 0.9492230415344238, avg_entr 0.08883117884397507, f1 0.823421061038971
t0.7_test_time 0.20819956299999376
gc 0
Test threshold 0.8 Acc 0.8234210526315789, AUC 0.9492230415344238, avg_entr 0.08883117884397507, f1 0.823421061038971
t0.8_test_time 0.2064550599999393
gc 0
Test threshold 0.9 Acc 0.8234210526315789, AUC 0.9492230415344238, avg_entr 0.08883117884397507, f1 0.823421061038971
t0.9_test_time 0.20704567299992505
