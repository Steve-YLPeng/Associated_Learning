total count words 102019
vocab size 30000
train size 120000, valid size 3800, test size 3800
found 26754 words in glove
model: TransformerALsideText(
  (layers): ModuleList(
    (0): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=4, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=4, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
  (emb_layers): ModuleList(
    (0): Embedding(30000, 300)
    (1): Embedding(30000, 300)
    (2): Embedding(30000, 300)
    (3): Embedding(30000, 300)
    (4): Embedding(30000, 300)
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.multi_headed_attention.linears.0.weight 90000
layers.0.enc.f.multi_headed_attention.linears.0.bias 300
layers.0.enc.f.multi_headed_attention.linears.1.weight 90000
layers.0.enc.f.multi_headed_attention.linears.1.bias 300
layers.0.enc.f.multi_headed_attention.linears.2.weight 90000
layers.0.enc.f.multi_headed_attention.linears.2.bias 300
layers.0.enc.f.multi_headed_attention.linears.3.weight 90000
layers.0.enc.f.multi_headed_attention.linears.3.bias 300
layers.0.enc.f.feed_forward.w_1.weight 90000
layers.0.enc.f.feed_forward.w_1.bias 300
layers.0.enc.f.feed_forward.w_2.weight 90000
layers.0.enc.f.feed_forward.w_2.bias 300
layers.0.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.0.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.0.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.0.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.0.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.0.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.0.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.0.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.0.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.0.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.0.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.0.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.0.enc.f.encoder.norm.a 300
layers.0.enc.f.encoder.norm.b 300
layers.0.ae.g.0.weight 512
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 512
layers.0.ae.h.0.bias 4
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
emb_layers.0.weight 9000000
emb_layers.1.weight 9000000
emb_layers.2.weight 9000000
emb_layers.3.weight 9000000
emb_layers.4.weight 9000000
Total Trainable Params: 50758892
init_time 15.745753574000002
Start Training
gc 0
Train Epoch0 Acc 0.27589166666666665 (33107/120000), AUC 0.5205562114715576
ep0_train_time 60.982673979
Test Epoch0 layer0 Acc 0.796578947368421, AUC 0.9364603757858276, avg_entr 0.7484337091445923, f1 0.7965789437294006
ep0_l0_test_time 0.1864748809999952
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 0
Test Epoch0 layer1 Acc 0.7644736842105263, AUC 0.935975193977356, avg_entr 0.7513796091079712, f1 0.7644737362861633
ep0_l1_test_time 0.24127363300000582
Test Epoch0 layer2 Acc 0.7371052631578947, AUC 0.9350385665893555, avg_entr 0.7797363996505737, f1 0.7371053099632263
ep0_l2_test_time 0.30648749799999564
Test Epoch0 layer3 Acc 0.756578947368421, AUC 0.9222898483276367, avg_entr 0.9717165231704712, f1 0.7565789222717285
ep0_l3_test_time 0.4044419330000011
Test Epoch0 layer4 Acc 0.4978947368421053, AUC 0.8947498798370361, avg_entr 1.1318479776382446, f1 0.4978947341442108
ep0_l4_test_time 0.5314337289999997
gc 0
Train Epoch1 Acc 0.737675 (88521/120000), AUC 0.9060553908348083
ep1_train_time 60.590029017000006
Test Epoch1 layer0 Acc 0.8378947368421052, AUC 0.957716703414917, avg_entr 0.38590049743652344, f1 0.8378947377204895
ep1_l0_test_time 0.18546018000000686
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 1
Test Epoch1 layer1 Acc 0.8318421052631579, AUC 0.9599189162254333, avg_entr 0.33658093214035034, f1 0.8318421840667725
ep1_l1_test_time 0.232529387999989
Test Epoch1 layer2 Acc 0.8305263157894737, AUC 0.9601203799247742, avg_entr 0.32857248187065125, f1 0.8305262923240662
ep1_l2_test_time 0.3041995729999769
Test Epoch1 layer3 Acc 0.8236842105263158, AUC 0.960310697555542, avg_entr 0.3482930660247803, f1 0.8236842155456543
ep1_l3_test_time 0.4036958650000031
Test Epoch1 layer4 Acc 0.8205263157894737, AUC 0.9599257707595825, avg_entr 0.3699047565460205, f1 0.8205263018608093
ep1_l4_test_time 0.5310945350000225
gc 0
Train Epoch2 Acc 0.8616833333333334 (103402/120000), AUC 0.9607681035995483
ep2_train_time 60.516708059999985
Test Epoch2 layer0 Acc 0.8594736842105263, AUC 0.9646462202072144, avg_entr 0.2437678575515747, f1 0.859473705291748
ep2_l0_test_time 0.18432417500000042
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 2
Test Epoch2 layer1 Acc 0.8623684210526316, AUC 0.9653019905090332, avg_entr 0.1640418916940689, f1 0.8623684048652649
ep2_l1_test_time 0.23154588099998819
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 2
Test Epoch2 layer2 Acc 0.8618421052631579, AUC 0.9653673768043518, avg_entr 0.15818698704242706, f1 0.8618420958518982
ep2_l2_test_time 0.30651504299999033
Test Epoch2 layer3 Acc 0.8618421052631579, AUC 0.9656027555465698, avg_entr 0.15462425351142883, f1 0.8618420958518982
ep2_l3_test_time 0.4063682169999936
Test Epoch2 layer4 Acc 0.8607894736842105, AUC 0.9659009575843811, avg_entr 0.15490448474884033, f1 0.8607894778251648
ep2_l4_test_time 0.5314835929999902
gc 0
Train Epoch3 Acc 0.883225 (105987/120000), AUC 0.9683078527450562
ep3_train_time 60.552806495
Test Epoch3 layer0 Acc 0.8726315789473684, AUC 0.9684127569198608, avg_entr 0.17403651773929596, f1 0.8726315498352051
ep3_l0_test_time 0.18457830900001682
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 3
Test Epoch3 layer1 Acc 0.8739473684210526, AUC 0.9699992537498474, avg_entr 0.11578456312417984, f1 0.8739473819732666
ep3_l1_test_time 0.2327830029999518
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 3
Test Epoch3 layer2 Acc 0.875, AUC 0.9708060026168823, avg_entr 0.11183573305606842, f1 0.875
ep3_l2_test_time 0.30460981400000264
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 3
Test Epoch3 layer3 Acc 0.8744736842105263, AUC 0.9710642099380493, avg_entr 0.11117874830961227, f1 0.8744736909866333
ep3_l3_test_time 0.40438446399997474
Test Epoch3 layer4 Acc 0.8744736842105263, AUC 0.9708384275436401, avg_entr 0.10777796804904938, f1 0.8744736909866333
ep3_l4_test_time 0.5302179509999974
gc 0
Train Epoch4 Acc 0.8959583333333333 (107515/120000), AUC 0.9722838401794434
ep4_train_time 60.688030889000004
Test Epoch4 layer0 Acc 0.8744736842105263, AUC 0.9685396552085876, avg_entr 0.1442408561706543, f1 0.8744736909866333
ep4_l0_test_time 0.18474171899998737
Test Epoch4 layer1 Acc 0.8763157894736842, AUC 0.9693644046783447, avg_entr 0.09014732390642166, f1 0.8763157725334167
ep4_l1_test_time 0.2323824590000072
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 4
Test Epoch4 layer2 Acc 0.8776315789473684, AUC 0.9707507491111755, avg_entr 0.08733676373958588, f1 0.8776316046714783
ep4_l2_test_time 0.30543867099999034
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 4
Test Epoch4 layer3 Acc 0.8773684210526316, AUC 0.9705467224121094, avg_entr 0.08523659408092499, f1 0.8773684501647949
ep4_l3_test_time 0.40444631700000855
Test Epoch4 layer4 Acc 0.876578947368421, AUC 0.9701850414276123, avg_entr 0.08190838247537613, f1 0.8765789270401001
ep4_l4_test_time 0.5306775550000111
gc 0
Train Epoch5 Acc 0.9063583333333334 (108763/120000), AUC 0.9752671718597412
ep5_train_time 60.59455309499998
Test Epoch5 layer0 Acc 0.8721052631578947, AUC 0.9706153273582458, avg_entr 0.12558303773403168, f1 0.8721052408218384
ep5_l0_test_time 0.18469081099999585
Test Epoch5 layer1 Acc 0.8723684210526316, AUC 0.9720094799995422, avg_entr 0.0759354904294014, f1 0.8723683953285217
ep5_l1_test_time 0.2315630450000299
Test Epoch5 layer2 Acc 0.8697368421052631, AUC 0.9721503257751465, avg_entr 0.06973686069250107, f1 0.8697368502616882
ep5_l2_test_time 0.3035612489999835
Test Epoch5 layer3 Acc 0.8702631578947368, AUC 0.97164386510849, avg_entr 0.0648999884724617, f1 0.8702631592750549
ep5_l3_test_time 0.40340193300005467
Test Epoch5 layer4 Acc 0.8705263157894737, AUC 0.9704822301864624, avg_entr 0.06077295169234276, f1 0.8705263137817383
ep5_l4_test_time 0.5313072120000015
gc 0
Train Epoch6 Acc 0.9142666666666667 (109712/120000), AUC 0.9780289530754089
ep6_train_time 60.47667800199997
Test Epoch6 layer0 Acc 0.878421052631579, AUC 0.9699988961219788, avg_entr 0.11327266693115234, f1 0.8784210681915283
ep6_l0_test_time 0.1854923079999935
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 6
Test Epoch6 layer1 Acc 0.8789473684210526, AUC 0.9687511920928955, avg_entr 0.06406574696302414, f1 0.878947377204895
ep6_l1_test_time 0.23365229999996018
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 6
Test Epoch6 layer2 Acc 0.8786842105263157, AUC 0.9692118763923645, avg_entr 0.058209553360939026, f1 0.8786842226982117
ep6_l2_test_time 0.30502274499997384
Test Epoch6 layer3 Acc 0.8786842105263157, AUC 0.9673260450363159, avg_entr 0.0540148951113224, f1 0.8786842226982117
ep6_l3_test_time 0.4048290040000211
Test Epoch6 layer4 Acc 0.8752631578947369, AUC 0.9637651443481445, avg_entr 0.051210299134254456, f1 0.8752631545066833
ep6_l4_test_time 0.5311136450000049
gc 0
Train Epoch7 Acc 0.9209 (110508/120000), AUC 0.9806486964225769
ep7_train_time 60.64734425600005
Test Epoch7 layer0 Acc 0.883421052631579, AUC 0.9710514545440674, avg_entr 0.09951259940862656, f1 0.8834210634231567
ep7_l0_test_time 0.18969195099998615
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 7
Test Epoch7 layer1 Acc 0.8828947368421053, AUC 0.9709420204162598, avg_entr 0.0546787828207016, f1 0.88289475440979
ep7_l1_test_time 0.2334567649999144
Test Epoch7 layer2 Acc 0.8831578947368421, AUC 0.9712145328521729, avg_entr 0.050082020461559296, f1 0.8831579089164734
ep7_l2_test_time 0.3049669920000042
Test Epoch7 layer3 Acc 0.8818421052631579, AUC 0.9709774255752563, avg_entr 0.046685200184583664, f1 0.8818420767784119
ep7_l3_test_time 0.4048443489999727
Test Epoch7 layer4 Acc 0.8828947368421053, AUC 0.9690106511116028, avg_entr 0.04356886446475983, f1 0.88289475440979
ep7_l4_test_time 0.5313550270000178
gc 0
Train Epoch8 Acc 0.9283916666666666 (111407/120000), AUC 0.9840493202209473
ep8_train_time 60.69304753100005
Test Epoch8 layer0 Acc 0.8828947368421053, AUC 0.9718762040138245, avg_entr 0.09012399613857269, f1 0.88289475440979
ep8_l0_test_time 0.18546495499992943
Test Epoch8 layer1 Acc 0.8818421052631579, AUC 0.9704453349113464, avg_entr 0.045934777706861496, f1 0.8818420767784119
ep8_l1_test_time 0.24188468899990312
Test Epoch8 layer2 Acc 0.881578947368421, AUC 0.9704339504241943, avg_entr 0.03901783749461174, f1 0.8815789222717285
ep8_l2_test_time 0.3061649279999301
Test Epoch8 layer3 Acc 0.8826315789473684, AUC 0.969138503074646, avg_entr 0.035288598388433456, f1 0.8826315999031067
ep8_l3_test_time 0.4066427029999886
Test Epoch8 layer4 Acc 0.8818421052631579, AUC 0.9674409627914429, avg_entr 0.031617552042007446, f1 0.8818420767784119
ep8_l4_test_time 0.532596443999978
gc 0
Train Epoch9 Acc 0.93505 (112206/120000), AUC 0.9861299991607666
ep9_train_time 60.77986591800004
Test Epoch9 layer0 Acc 0.883421052631579, AUC 0.9712661504745483, avg_entr 0.08340416848659515, f1 0.8834210634231567
ep9_l0_test_time 0.19121153700007198
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 9
Test Epoch9 layer1 Acc 0.886578947368421, AUC 0.9684575200080872, avg_entr 0.03810271993279457, f1 0.8865789771080017
ep9_l1_test_time 0.2360530309999831
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 9
Test Epoch9 layer2 Acc 0.8860526315789473, AUC 0.9687023758888245, avg_entr 0.031918685883283615, f1 0.886052668094635
ep9_l2_test_time 0.30467346599994016
Test Epoch9 layer3 Acc 0.8873684210526316, AUC 0.9666258096694946, avg_entr 0.02821369282901287, f1 0.8873685002326965
ep9_l3_test_time 0.4047250219999796
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 9
Test Epoch9 layer4 Acc 0.886578947368421, AUC 0.9644010663032532, avg_entr 0.0252995528280735, f1 0.8865789771080017
ep9_l4_test_time 0.5325464339999826
gc 0
Train Epoch10 Acc 0.9399 (112788/120000), AUC 0.9886263608932495
ep10_train_time 60.50210956000001
Test Epoch10 layer0 Acc 0.8818421052631579, AUC 0.9707999229431152, avg_entr 0.08019645512104034, f1 0.8818420767784119
ep10_l0_test_time 0.18453479900006187
Test Epoch10 layer1 Acc 0.8821052631578947, AUC 0.9691816568374634, avg_entr 0.03719048202037811, f1 0.88210529088974
ep10_l1_test_time 0.23189771700003803
Test Epoch10 layer2 Acc 0.8821052631578947, AUC 0.9698836207389832, avg_entr 0.032209672033786774, f1 0.88210529088974
ep10_l2_test_time 0.30345829000009417
Test Epoch10 layer3 Acc 0.8823684210526316, AUC 0.9688453078269958, avg_entr 0.029357530176639557, f1 0.8823684453964233
ep10_l3_test_time 0.40379051499996876
Test Epoch10 layer4 Acc 0.8821052631578947, AUC 0.963586688041687, avg_entr 0.02578878030180931, f1 0.88210529088974
ep10_l4_test_time 0.5301169690000052
gc 0
Train Epoch11 Acc 0.9446916666666667 (113363/120000), AUC 0.9893349409103394
ep11_train_time 60.63338392100002
Test Epoch11 layer0 Acc 0.8889473684210526, AUC 0.9723860025405884, avg_entr 0.07651171088218689, f1 0.8889473676681519
ep11_l0_test_time 0.18507298600002287
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 11
Test Epoch11 layer1 Acc 0.8868421052631579, AUC 0.9681040644645691, avg_entr 0.035896629095077515, f1 0.8868421316146851
ep11_l1_test_time 0.2320715589999054
Test Epoch11 layer2 Acc 0.8857894736842106, AUC 0.9677650928497314, avg_entr 0.029846040531992912, f1 0.8857894539833069
ep11_l2_test_time 0.3036004780000212
Test Epoch11 layer3 Acc 0.8855263157894737, AUC 0.9669474363327026, avg_entr 0.027440793812274933, f1 0.8855262398719788
ep11_l3_test_time 0.40472942199994577
Test Epoch11 layer4 Acc 0.8860526315789473, AUC 0.9655774831771851, avg_entr 0.023751698434352875, f1 0.886052668094635
ep11_l4_test_time 0.5314249019999124
gc 0
Train Epoch12 Acc 0.9495083333333333 (113941/120000), AUC 0.991278886795044
ep12_train_time 60.572352234999926
Test Epoch12 layer0 Acc 0.886578947368421, AUC 0.9719173312187195, avg_entr 0.07341822236776352, f1 0.8865789771080017
ep12_l0_test_time 0.18511896500001512
Test Epoch12 layer1 Acc 0.8847368421052632, AUC 0.9686986207962036, avg_entr 0.03427868336439133, f1 0.8847368359565735
ep12_l1_test_time 0.23593509799991352
Test Epoch12 layer2 Acc 0.8839473684210526, AUC 0.9687501788139343, avg_entr 0.029319940134882927, f1 0.8839473724365234
ep12_l2_test_time 0.30359902999998667
Test Epoch12 layer3 Acc 0.8842105263157894, AUC 0.9672239422798157, avg_entr 0.02723442204296589, f1 0.8842105865478516
ep12_l3_test_time 0.403985233999947
Test Epoch12 layer4 Acc 0.8839473684210526, AUC 0.9644581079483032, avg_entr 0.024114470928907394, f1 0.8839473724365234
ep12_l4_test_time 0.5301807199999757
gc 0
Train Epoch13 Acc 0.9527916666666667 (114335/120000), AUC 0.991618275642395
ep13_train_time 60.497549795000054
Test Epoch13 layer0 Acc 0.8844736842105263, AUC 0.9705055952072144, avg_entr 0.06868202239274979, f1 0.8844736814498901
ep13_l0_test_time 0.18440308899994307
Test Epoch13 layer1 Acc 0.8839473684210526, AUC 0.9665825963020325, avg_entr 0.029660748317837715, f1 0.8839473724365234
ep13_l1_test_time 0.2318882660000554
Test Epoch13 layer2 Acc 0.8821052631578947, AUC 0.9688321948051453, avg_entr 0.02407604642212391, f1 0.88210529088974
ep13_l2_test_time 0.3019415039999558
Test Epoch13 layer3 Acc 0.8818421052631579, AUC 0.9691034555435181, avg_entr 0.022055555135011673, f1 0.8818420767784119
ep13_l3_test_time 0.40390096099997663
Test Epoch13 layer4 Acc 0.8821052631578947, AUC 0.9675388336181641, avg_entr 0.019696984440088272, f1 0.88210529088974
ep13_l4_test_time 0.5318692600000077
gc 0
Train Epoch14 Acc 0.956225 (114747/120000), AUC 0.9928096532821655
ep14_train_time 60.647620867
Test Epoch14 layer0 Acc 0.888421052631579, AUC 0.9717515707015991, avg_entr 0.06436246633529663, f1 0.8884210586547852
ep14_l0_test_time 0.18509096499997213
Test Epoch14 layer1 Acc 0.8844736842105263, AUC 0.9667006731033325, avg_entr 0.02893764153122902, f1 0.8844736814498901
ep14_l1_test_time 0.23224033700000746
Test Epoch14 layer2 Acc 0.8836842105263157, AUC 0.970619261264801, avg_entr 0.02466234192252159, f1 0.8836842179298401
ep14_l2_test_time 0.3034477840000136
Test Epoch14 layer3 Acc 0.8839473684210526, AUC 0.969460129737854, avg_entr 0.022937921807169914, f1 0.8839473724365234
ep14_l3_test_time 0.4042160320000221
Test Epoch14 layer4 Acc 0.883421052631579, AUC 0.9676055312156677, avg_entr 0.020658161491155624, f1 0.8834210634231567
ep14_l4_test_time 0.5301459379999187
gc 0
Train Epoch15 Acc 0.9593 (115116/120000), AUC 0.9935472011566162
ep15_train_time 60.72736457099995
Test Epoch15 layer0 Acc 0.8818421052631579, AUC 0.9710586071014404, avg_entr 0.06258516758680344, f1 0.8818420767784119
ep15_l0_test_time 0.18633622599998034
Test Epoch15 layer1 Acc 0.8821052631578947, AUC 0.9670020937919617, avg_entr 0.025009097531437874, f1 0.88210529088974
ep15_l1_test_time 0.23224992899997687
Test Epoch15 layer2 Acc 0.8826315789473684, AUC 0.9689241647720337, avg_entr 0.0210257638245821, f1 0.8826315999031067
ep15_l2_test_time 0.3044280799999797
Test Epoch15 layer3 Acc 0.8823684210526316, AUC 0.9669623374938965, avg_entr 0.01953926496207714, f1 0.8823684453964233
ep15_l3_test_time 0.4040289000000712
Test Epoch15 layer4 Acc 0.8826315789473684, AUC 0.9645125865936279, avg_entr 0.017319360747933388, f1 0.8826315999031067
ep15_l4_test_time 0.5309348780000391
gc 0
Train Epoch16 Acc 0.9654666666666667 (115856/120000), AUC 0.995058536529541
ep16_train_time 60.47528969299992
Test Epoch16 layer0 Acc 0.8955263157894737, AUC 0.9706311225891113, avg_entr 0.062087055295705795, f1 0.8955262899398804
ep16_l0_test_time 0.18783101799999713
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 16
Test Epoch16 layer1 Acc 0.8881578947368421, AUC 0.9618480801582336, avg_entr 0.027171598747372627, f1 0.8881579041481018
ep16_l1_test_time 0.2319059669998751
Test Epoch16 layer2 Acc 0.8878947368421053, AUC 0.9635411500930786, avg_entr 0.023107437416911125, f1 0.8878947496414185
ep16_l2_test_time 0.303393208999978
Test Epoch16 layer3 Acc 0.8876315789473684, AUC 0.9599432945251465, avg_entr 0.02132544107735157, f1 0.8876315951347351
ep16_l3_test_time 0.4037708500000008
Test Epoch16 layer4 Acc 0.8873684210526316, AUC 0.9610276222229004, avg_entr 0.01927654817700386, f1 0.8873685002326965
ep16_l4_test_time 0.5301379110001108
gc 0
Train Epoch17 Acc 0.9683 (116196/120000), AUC 0.9954087734222412
ep17_train_time 60.69862417299987
Test Epoch17 layer0 Acc 0.8905263157894737, AUC 0.9695611000061035, avg_entr 0.058347806334495544, f1 0.890526294708252
ep17_l0_test_time 0.18520771999988028
Test Epoch17 layer1 Acc 0.8863157894736842, AUC 0.956558883190155, avg_entr 0.022875485941767693, f1 0.8863158226013184
ep17_l1_test_time 0.2330340940000042
Test Epoch17 layer2 Acc 0.8868421052631579, AUC 0.9588544368743896, avg_entr 0.01886054128408432, f1 0.8868421316146851
ep17_l2_test_time 0.30441540199990413
Test Epoch17 layer3 Acc 0.886578947368421, AUC 0.9566219449043274, avg_entr 0.01711384207010269, f1 0.8865789771080017
ep17_l3_test_time 0.403883174999919
Test Epoch17 layer4 Acc 0.886578947368421, AUC 0.9553200006484985, avg_entr 0.015398935414850712, f1 0.8865789771080017
ep17_l4_test_time 0.5308294449998812
gc 0
Train Epoch18 Acc 0.9702666666666667 (116432/120000), AUC 0.9960303902626038
ep18_train_time 60.488220401000035
Test Epoch18 layer0 Acc 0.8913157894736842, AUC 0.9699098467826843, avg_entr 0.05778617411851883, f1 0.8913158178329468
ep18_l0_test_time 0.18497152899999492
Test Epoch18 layer1 Acc 0.886578947368421, AUC 0.9589110612869263, avg_entr 0.02399921976029873, f1 0.8865789771080017
ep18_l1_test_time 0.23190942199994424
Test Epoch18 layer2 Acc 0.8873684210526316, AUC 0.962505578994751, avg_entr 0.01940227299928665, f1 0.8873685002326965
ep18_l2_test_time 0.30327545199997985
Test Epoch18 layer3 Acc 0.8876315789473684, AUC 0.9615402817726135, avg_entr 0.017770446836948395, f1 0.8876315951347351
ep18_l3_test_time 0.4039683660000719
Test Epoch18 layer4 Acc 0.8868421052631579, AUC 0.9578409194946289, avg_entr 0.015800654888153076, f1 0.8868421316146851
ep18_l4_test_time 0.5411652899999808
gc 0
Train Epoch19 Acc 0.9721583333333333 (116659/120000), AUC 0.9964330792427063
ep19_train_time 60.746902913999975
Test Epoch19 layer0 Acc 0.8905263157894737, AUC 0.9691318273544312, avg_entr 0.05710989609360695, f1 0.890526294708252
ep19_l0_test_time 0.18499963400017805
Test Epoch19 layer1 Acc 0.8892105263157895, AUC 0.9568237066268921, avg_entr 0.02329152077436447, f1 0.8892105221748352
ep19_l1_test_time 0.23190898600000764
Test Epoch19 layer2 Acc 0.8894736842105263, AUC 0.9594181776046753, avg_entr 0.018652720376849174, f1 0.8894736766815186
ep19_l2_test_time 0.3042642119999073
Test Epoch19 layer3 Acc 0.8892105263157895, AUC 0.9579285383224487, avg_entr 0.017306730151176453, f1 0.8892105221748352
ep19_l3_test_time 0.40725414100006674
Test Epoch19 layer4 Acc 0.888421052631579, AUC 0.9562380909919739, avg_entr 0.015572684817016125, f1 0.8884210586547852
ep19_l4_test_time 0.5318232380000154
gc 0
Train Epoch20 Acc 0.97505 (117006/120000), AUC 0.996960461139679
ep20_train_time 60.62040594599989
Test Epoch20 layer0 Acc 0.8934210526315789, AUC 0.9694479703903198, avg_entr 0.05510382354259491, f1 0.8934210538864136
ep20_l0_test_time 0.18495573499990314
Test Epoch20 layer1 Acc 0.8863157894736842, AUC 0.9548838138580322, avg_entr 0.022185225039720535, f1 0.8863158226013184
ep20_l1_test_time 0.2322748970000248
Test Epoch20 layer2 Acc 0.885, AUC 0.960839033126831, avg_entr 0.018008751794695854, f1 0.8849999904632568
ep20_l2_test_time 0.30411677600000075
Test Epoch20 layer3 Acc 0.8844736842105263, AUC 0.9598187208175659, avg_entr 0.016775276511907578, f1 0.8844736814498901
ep20_l3_test_time 0.4071327789999941
Test Epoch20 layer4 Acc 0.8852631578947369, AUC 0.9585263729095459, avg_entr 0.01485645305365324, f1 0.8852631449699402
ep20_l4_test_time 0.5309918309999375
gc 0
Train Epoch21 Acc 0.97675 (117210/120000), AUC 0.9971244931221008
ep21_train_time 60.44732580799996
Test Epoch21 layer0 Acc 0.8918421052631579, AUC 0.9694231748580933, avg_entr 0.05324598029255867, f1 0.8918421268463135
ep21_l0_test_time 0.18531919399993058
Test Epoch21 layer1 Acc 0.8839473684210526, AUC 0.9530161023139954, avg_entr 0.020739281550049782, f1 0.8839473724365234
ep21_l1_test_time 0.23218081100003474
Test Epoch21 layer2 Acc 0.8836842105263157, AUC 0.9564619064331055, avg_entr 0.016929401084780693, f1 0.8836842179298401
ep21_l2_test_time 0.30296970099993814
Test Epoch21 layer3 Acc 0.883421052631579, AUC 0.9561759233474731, avg_entr 0.015774957835674286, f1 0.8834210634231567
ep21_l3_test_time 0.4054472899999837
Test Epoch21 layer4 Acc 0.8842105263157894, AUC 0.9543596506118774, avg_entr 0.014158535748720169, f1 0.8842105865478516
ep21_l4_test_time 0.5309921069999746
gc 0
Train Epoch22 Acc 0.9777 (117324/120000), AUC 0.9973145723342896
ep22_train_time 60.61342139399994
Test Epoch22 layer0 Acc 0.8921052631578947, AUC 0.9694623947143555, avg_entr 0.05316058546304703, f1 0.8921052813529968
ep22_l0_test_time 0.1853877820001344
Test Epoch22 layer1 Acc 0.8821052631578947, AUC 0.9545133709907532, avg_entr 0.021143754944205284, f1 0.88210529088974
ep22_l1_test_time 0.23141605299997536
Test Epoch22 layer2 Acc 0.8821052631578947, AUC 0.9569387435913086, avg_entr 0.016428744420409203, f1 0.88210529088974
ep22_l2_test_time 0.30275760700010323
Test Epoch22 layer3 Acc 0.8818421052631579, AUC 0.955955982208252, avg_entr 0.015248697251081467, f1 0.8818420767784119
ep22_l3_test_time 0.4036830210000062
Test Epoch22 layer4 Acc 0.8818421052631579, AUC 0.9567514061927795, avg_entr 0.013853000476956367, f1 0.8818420767784119
ep22_l4_test_time 0.5299795100002029
gc 0
Train Epoch23 Acc 0.9780916666666667 (117371/120000), AUC 0.9975790977478027
ep23_train_time 60.70515221099981
Test Epoch23 layer0 Acc 0.8928947368421053, AUC 0.968840479850769, avg_entr 0.051656950265169144, f1 0.8928947448730469
ep23_l0_test_time 0.18442660000005162
Test Epoch23 layer1 Acc 0.886578947368421, AUC 0.9526206254959106, avg_entr 0.01928313635289669, f1 0.8865789771080017
ep23_l1_test_time 0.23281693599983555
Test Epoch23 layer2 Acc 0.8857894736842106, AUC 0.9554873704910278, avg_entr 0.015533452853560448, f1 0.8857894539833069
ep23_l2_test_time 0.3039112720000503
Test Epoch23 layer3 Acc 0.8860526315789473, AUC 0.9555050134658813, avg_entr 0.014319371432065964, f1 0.886052668094635
ep23_l3_test_time 0.40464178399997763
Test Epoch23 layer4 Acc 0.885, AUC 0.953141987323761, avg_entr 0.01300567202270031, f1 0.8849999904632568
ep23_l4_test_time 0.5311324900001182
gc 0
Train Epoch24 Acc 0.9797666666666667 (117572/120000), AUC 0.9977656602859497
ep24_train_time 60.63954085799992
Test Epoch24 layer0 Acc 0.8931578947368422, AUC 0.9687050580978394, avg_entr 0.0502319298684597, f1 0.8931578993797302
ep24_l0_test_time 0.1872315250000156
Test Epoch24 layer1 Acc 0.8857894736842106, AUC 0.9539996981620789, avg_entr 0.018083928152918816, f1 0.8857894539833069
ep24_l1_test_time 0.23124026100003903
Test Epoch24 layer2 Acc 0.8857894736842106, AUC 0.9570336937904358, avg_entr 0.013903047889471054, f1 0.8857894539833069
ep24_l2_test_time 0.3026585210000121
Test Epoch24 layer3 Acc 0.8857894736842106, AUC 0.9546334147453308, avg_entr 0.012715875171124935, f1 0.8857894539833069
ep24_l3_test_time 0.40365335000001323
Test Epoch24 layer4 Acc 0.8863157894736842, AUC 0.9533817172050476, avg_entr 0.011454152874648571, f1 0.8863158226013184
ep24_l4_test_time 0.5290715210001053
gc 0
Train Epoch25 Acc 0.9805666666666667 (117668/120000), AUC 0.9976581931114197
ep25_train_time 60.791107409000006
Test Epoch25 layer0 Acc 0.8928947368421053, AUC 0.968742311000824, avg_entr 0.04991530254483223, f1 0.8928947448730469
ep25_l0_test_time 0.18525941200005036
Test Epoch25 layer1 Acc 0.8839473684210526, AUC 0.9520783424377441, avg_entr 0.018144968897104263, f1 0.8839473724365234
ep25_l1_test_time 0.23243362200014417
Test Epoch25 layer2 Acc 0.8823684210526316, AUC 0.9564739465713501, avg_entr 0.014578900299966335, f1 0.8823684453964233
ep25_l2_test_time 0.3061636439999802
Test Epoch25 layer3 Acc 0.8826315789473684, AUC 0.955134928226471, avg_entr 0.013279144652187824, f1 0.8826315999031067
ep25_l3_test_time 0.4065970509998351
Test Epoch25 layer4 Acc 0.8828947368421053, AUC 0.9517599940299988, avg_entr 0.011957531794905663, f1 0.88289475440979
ep25_l4_test_time 0.5373429100000067
gc 0
Train Epoch26 Acc 0.9809833333333333 (117718/120000), AUC 0.997806191444397
ep26_train_time 61.05120132899992
Test Epoch26 layer0 Acc 0.8913157894736842, AUC 0.968437910079956, avg_entr 0.048892512917518616, f1 0.8913158178329468
ep26_l0_test_time 0.18371326600004068
Test Epoch26 layer1 Acc 0.883421052631579, AUC 0.9512481093406677, avg_entr 0.018915686756372452, f1 0.8834210634231567
ep26_l1_test_time 0.23113623799986271
Test Epoch26 layer2 Acc 0.8828947368421053, AUC 0.9543560743331909, avg_entr 0.014453401789069176, f1 0.88289475440979
ep26_l2_test_time 0.3028308770001331
Test Epoch26 layer3 Acc 0.8826315789473684, AUC 0.9533175826072693, avg_entr 0.01335698552429676, f1 0.8826315999031067
ep26_l3_test_time 0.4037299060000805
Test Epoch26 layer4 Acc 0.8842105263157894, AUC 0.9510550498962402, avg_entr 0.01226806640625, f1 0.8842105865478516
ep26_l4_test_time 0.5302418539999962
gc 0
Train Epoch27 Acc 0.980725 (117687/120000), AUC 0.9979936480522156
ep27_train_time 60.54865320199997
Test Epoch27 layer0 Acc 0.8921052631578947, AUC 0.9676023125648499, avg_entr 0.04879467561841011, f1 0.8921052813529968
ep27_l0_test_time 0.18464162700001907
Test Epoch27 layer1 Acc 0.8818421052631579, AUC 0.9507260918617249, avg_entr 0.01953628472983837, f1 0.8818420767784119
ep27_l1_test_time 0.23161706099995172
Test Epoch27 layer2 Acc 0.8807894736842106, AUC 0.9562053680419922, avg_entr 0.015571169555187225, f1 0.8807894587516785
ep27_l2_test_time 0.30334119300005113
Test Epoch27 layer3 Acc 0.8805263157894737, AUC 0.9559512138366699, avg_entr 0.014468749985098839, f1 0.8805263042449951
ep27_l3_test_time 0.4030238580000969
Test Epoch27 layer4 Acc 0.8802631578947369, AUC 0.9532216787338257, avg_entr 0.013375205919146538, f1 0.880263090133667
ep27_l4_test_time 0.5299379159998807
gc 0
Train Epoch28 Acc 0.982225 (117867/120000), AUC 0.9981324672698975
ep28_train_time 60.521691835999945
Test Epoch28 layer0 Acc 0.8910526315789473, AUC 0.9681403636932373, avg_entr 0.04805298149585724, f1 0.8910526037216187
ep28_l0_test_time 0.18470244399986768
Test Epoch28 layer1 Acc 0.8826315789473684, AUC 0.9500097036361694, avg_entr 0.018855242058634758, f1 0.8826315999031067
ep28_l1_test_time 0.23205917499990392
Test Epoch28 layer2 Acc 0.8810526315789474, AUC 0.9536773562431335, avg_entr 0.014558597467839718, f1 0.8810526132583618
ep28_l2_test_time 0.3033253879998483
Test Epoch28 layer3 Acc 0.8813157894736842, AUC 0.9524117708206177, avg_entr 0.01342364028096199, f1 0.8813157677650452
ep28_l3_test_time 0.40481795999994574
Test Epoch28 layer4 Acc 0.8813157894736842, AUC 0.9503563046455383, avg_entr 0.012193613685667515, f1 0.8813157677650452
ep28_l4_test_time 0.5312229670000761
gc 0
Train Epoch29 Acc 0.9825833333333334 (117910/120000), AUC 0.9980707168579102
ep29_train_time 60.85649681299992
Test Epoch29 layer0 Acc 0.8902631578947369, AUC 0.9680500030517578, avg_entr 0.04725339263677597, f1 0.8902631402015686
ep29_l0_test_time 0.18453229799979454
Test Epoch29 layer1 Acc 0.8823684210526316, AUC 0.9498292803764343, avg_entr 0.018181510269641876, f1 0.8823684453964233
ep29_l1_test_time 0.23190127599991683
Test Epoch29 layer2 Acc 0.8818421052631579, AUC 0.9534915089607239, avg_entr 0.014551224187016487, f1 0.8818420767784119
ep29_l2_test_time 0.3035514200000762
Test Epoch29 layer3 Acc 0.8813157894736842, AUC 0.952067494392395, avg_entr 0.013318208046257496, f1 0.8813157677650452
ep29_l3_test_time 0.40351011600000675
Test Epoch29 layer4 Acc 0.881578947368421, AUC 0.951635479927063, avg_entr 0.012010633014142513, f1 0.8815789222717285
ep29_l4_test_time 0.5295159310001054
gc 0
Train Epoch30 Acc 0.98275 (117930/120000), AUC 0.9981929063796997
ep30_train_time 60.66915139799994
Test Epoch30 layer0 Acc 0.891578947368421, AUC 0.9680482745170593, avg_entr 0.0458652526140213, f1 0.8915789723396301
ep30_l0_test_time 0.1845115999999507
Test Epoch30 layer1 Acc 0.8847368421052632, AUC 0.9504031538963318, avg_entr 0.018594320863485336, f1 0.8847368359565735
ep30_l1_test_time 0.23222999599988725
Test Epoch30 layer2 Acc 0.8839473684210526, AUC 0.9535501003265381, avg_entr 0.014980236068367958, f1 0.8839473724365234
ep30_l2_test_time 0.30420826800013856
Test Epoch30 layer3 Acc 0.8839473684210526, AUC 0.9529885053634644, avg_entr 0.013992379419505596, f1 0.8839473724365234
ep30_l3_test_time 0.40275620599982176
Test Epoch30 layer4 Acc 0.8839473684210526, AUC 0.9506783485412598, avg_entr 0.012737395241856575, f1 0.8839473724365234
ep30_l4_test_time 0.5299961460000304
gc 0
Train Epoch31 Acc 0.98295 (117954/120000), AUC 0.9982607960700989
ep31_train_time 60.685224856000104
Test Epoch31 layer0 Acc 0.8910526315789473, AUC 0.9677351117134094, avg_entr 0.04531112685799599, f1 0.8910526037216187
ep31_l0_test_time 0.19618811600003028
Test Epoch31 layer1 Acc 0.8831578947368421, AUC 0.9493141770362854, avg_entr 0.018701447173953056, f1 0.8831579089164734
ep31_l1_test_time 0.24513448799984872
Test Epoch31 layer2 Acc 0.881578947368421, AUC 0.9538512229919434, avg_entr 0.015345370396971703, f1 0.8815789222717285
ep31_l2_test_time 0.3050438640000266
Test Epoch31 layer3 Acc 0.8818421052631579, AUC 0.9524808526039124, avg_entr 0.014199494384229183, f1 0.8818420767784119
ep31_l3_test_time 0.4098254780001298
Test Epoch31 layer4 Acc 0.881578947368421, AUC 0.9502701759338379, avg_entr 0.012971822172403336, f1 0.8815789222717285
ep31_l4_test_time 0.5339149050000742
gc 0
Train Epoch32 Acc 0.9834166666666667 (118010/120000), AUC 0.9981553554534912
ep32_train_time 60.78724427799989
Test Epoch32 layer0 Acc 0.8918421052631579, AUC 0.9675710201263428, avg_entr 0.04497186094522476, f1 0.8918421268463135
ep32_l0_test_time 0.18506206800020664
Test Epoch32 layer1 Acc 0.8805263157894737, AUC 0.9492548704147339, avg_entr 0.017927350476384163, f1 0.8805263042449951
ep32_l1_test_time 0.2324379710003086
Test Epoch32 layer2 Acc 0.8802631578947369, AUC 0.9525021314620972, avg_entr 0.014692069962620735, f1 0.880263090133667
ep32_l2_test_time 0.3030005959999471
Test Epoch32 layer3 Acc 0.8805263157894737, AUC 0.9514100551605225, avg_entr 0.013783836737275124, f1 0.8805263042449951
ep32_l3_test_time 0.4041766179998376
Test Epoch32 layer4 Acc 0.8805263157894737, AUC 0.9500309228897095, avg_entr 0.012501269578933716, f1 0.8805263042449951
ep32_l4_test_time 0.530406906999815
gc 0
Train Epoch33 Acc 0.983275 (117993/120000), AUC 0.998184323310852
ep33_train_time 60.42069824400005
Test Epoch33 layer0 Acc 0.8910526315789473, AUC 0.9676312208175659, avg_entr 0.04499950259923935, f1 0.8910526037216187
ep33_l0_test_time 0.18531913299966618
Test Epoch33 layer1 Acc 0.8810526315789474, AUC 0.9490547776222229, avg_entr 0.017671732231974602, f1 0.8810526132583618
ep33_l1_test_time 0.23193115599997327
Test Epoch33 layer2 Acc 0.8813157894736842, AUC 0.9512315988540649, avg_entr 0.01373482495546341, f1 0.8813157677650452
ep33_l2_test_time 0.303611680999893
Test Epoch33 layer3 Acc 0.881578947368421, AUC 0.9508986473083496, avg_entr 0.01275261864066124, f1 0.8815789222717285
ep33_l3_test_time 0.4029701589997785
Test Epoch33 layer4 Acc 0.8813157894736842, AUC 0.9495142102241516, avg_entr 0.011608365923166275, f1 0.8813157677650452
ep33_l4_test_time 0.530097810999905
gc 0
Train Epoch34 Acc 0.9836666666666667 (118040/120000), AUC 0.9982914328575134
ep34_train_time 60.707716621000145
Test Epoch34 layer0 Acc 0.8918421052631579, AUC 0.9677549600601196, avg_entr 0.04428013041615486, f1 0.8918421268463135
ep34_l0_test_time 0.18451263900033155
Test Epoch34 layer1 Acc 0.8826315789473684, AUC 0.9496808648109436, avg_entr 0.017286404967308044, f1 0.8826315999031067
ep34_l1_test_time 0.23249735599983978
Test Epoch34 layer2 Acc 0.8826315789473684, AUC 0.9527196884155273, avg_entr 0.013304985128343105, f1 0.8826315999031067
ep34_l2_test_time 0.30369574299993474
Test Epoch34 layer3 Acc 0.8823684210526316, AUC 0.9511203765869141, avg_entr 0.01235960703343153, f1 0.8823684453964233
ep34_l3_test_time 0.40382297599990125
Test Epoch34 layer4 Acc 0.883421052631579, AUC 0.949079692363739, avg_entr 0.011082394048571587, f1 0.8834210634231567
ep34_l4_test_time 0.5299299079997581
gc 0
Train Epoch35 Acc 0.9830833333333333 (117970/120000), AUC 0.9982295632362366
ep35_train_time 60.66149098400001
Test Epoch35 layer0 Acc 0.891578947368421, AUC 0.9679210186004639, avg_entr 0.04377645626664162, f1 0.8915789723396301
ep35_l0_test_time 0.1862566289996721
Test Epoch35 layer1 Acc 0.8823684210526316, AUC 0.9485765695571899, avg_entr 0.017460156232118607, f1 0.8823684453964233
ep35_l1_test_time 0.23209524999992937
Test Epoch35 layer2 Acc 0.8831578947368421, AUC 0.9515408873558044, avg_entr 0.013253314420580864, f1 0.8831579089164734
ep35_l2_test_time 0.30338589799976035
Test Epoch35 layer3 Acc 0.883421052631579, AUC 0.9513982534408569, avg_entr 0.012196189723908901, f1 0.8834210634231567
ep35_l3_test_time 0.4042119249997995
Test Epoch35 layer4 Acc 0.8836842105263157, AUC 0.9489504098892212, avg_entr 0.011128826066851616, f1 0.8836842179298401
ep35_l4_test_time 0.5299243119998209
gc 0
Train Epoch36 Acc 0.9835583333333333 (118027/120000), AUC 0.9983060359954834
ep36_train_time 60.64549344299985
Test Epoch36 layer0 Acc 0.8913157894736842, AUC 0.9676449298858643, avg_entr 0.04394073411822319, f1 0.8913158178329468
ep36_l0_test_time 0.18569137400027103
Test Epoch36 layer1 Acc 0.8821052631578947, AUC 0.949608325958252, avg_entr 0.017364846542477608, f1 0.88210529088974
ep36_l1_test_time 0.2341907899999569
Test Epoch36 layer2 Acc 0.881578947368421, AUC 0.9526874423027039, avg_entr 0.013839442282915115, f1 0.8815789222717285
ep36_l2_test_time 0.3095742719997361
Test Epoch36 layer3 Acc 0.881578947368421, AUC 0.9518024325370789, avg_entr 0.013003206811845303, f1 0.8815789222717285
ep36_l3_test_time 0.404187075000209
Test Epoch36 layer4 Acc 0.8818421052631579, AUC 0.949744462966919, avg_entr 0.011893441900610924, f1 0.8818420767784119
ep36_l4_test_time 0.5310646879997876
gc 0
Train Epoch37 Acc 0.9839 (118068/120000), AUC 0.9982190132141113
ep37_train_time 60.52149478699994
Test Epoch37 layer0 Acc 0.8921052631578947, AUC 0.9678723812103271, avg_entr 0.04314858466386795, f1 0.8921052813529968
ep37_l0_test_time 0.18606274900002973
Test Epoch37 layer1 Acc 0.8831578947368421, AUC 0.9498831033706665, avg_entr 0.016850806772708893, f1 0.8831579089164734
ep37_l1_test_time 0.23163660399995933
Test Epoch37 layer2 Acc 0.8823684210526316, AUC 0.9525238871574402, avg_entr 0.012658806517720222, f1 0.8823684453964233
ep37_l2_test_time 0.3028484230003414
Test Epoch37 layer3 Acc 0.8828947368421053, AUC 0.9517698884010315, avg_entr 0.011638285592198372, f1 0.88289475440979
ep37_l3_test_time 0.4026773119999234
Test Epoch37 layer4 Acc 0.8826315789473684, AUC 0.9493414163589478, avg_entr 0.010526232421398163, f1 0.8826315999031067
ep37_l4_test_time 0.5298057179998068
gc 0
Train Epoch38 Acc 0.9841666666666666 (118100/120000), AUC 0.9984232187271118
ep38_train_time 60.592048803000125
Test Epoch38 layer0 Acc 0.8918421052631579, AUC 0.967731237411499, avg_entr 0.042971108108758926, f1 0.8918421268463135
ep38_l0_test_time 0.18470843800014336
Test Epoch38 layer1 Acc 0.8828947368421053, AUC 0.9499230980873108, avg_entr 0.016997354105114937, f1 0.88289475440979
ep38_l1_test_time 0.23238260400012223
Test Epoch38 layer2 Acc 0.8828947368421053, AUC 0.9536937475204468, avg_entr 0.012675853446125984, f1 0.88289475440979
ep38_l2_test_time 0.30325541299998804
Test Epoch38 layer3 Acc 0.8828947368421053, AUC 0.9522433876991272, avg_entr 0.011744107119739056, f1 0.88289475440979
ep38_l3_test_time 0.40362765999998373
Test Epoch38 layer4 Acc 0.8831578947368421, AUC 0.9502391815185547, avg_entr 0.010695894248783588, f1 0.8831579089164734
ep38_l4_test_time 0.529090792000261
gc 0
Train Epoch39 Acc 0.9838916666666667 (118067/120000), AUC 0.9983205795288086
ep39_train_time 60.60364029199991
Test Epoch39 layer0 Acc 0.8918421052631579, AUC 0.9678872227668762, avg_entr 0.04248413071036339, f1 0.8918421268463135
ep39_l0_test_time 0.18555706300003294
Test Epoch39 layer1 Acc 0.8831578947368421, AUC 0.9499921798706055, avg_entr 0.016189496964216232, f1 0.8831579089164734
ep39_l1_test_time 0.23263357799987716
Test Epoch39 layer2 Acc 0.883421052631579, AUC 0.9526114463806152, avg_entr 0.011954565532505512, f1 0.8834210634231567
ep39_l2_test_time 0.30329425100035223
Test Epoch39 layer3 Acc 0.883421052631579, AUC 0.9522831439971924, avg_entr 0.011003459803760052, f1 0.8834210634231567
ep39_l3_test_time 0.4039585460000126
Test Epoch39 layer4 Acc 0.883421052631579, AUC 0.9489338397979736, avg_entr 0.009971590712666512, f1 0.8834210634231567
ep39_l4_test_time 0.529975750000176
gc 0
Train Epoch40 Acc 0.9840666666666666 (118088/120000), AUC 0.9985292553901672
ep40_train_time 60.580991410000024
Test Epoch40 layer0 Acc 0.8913157894736842, AUC 0.9677637815475464, avg_entr 0.04271351918578148, f1 0.8913158178329468
ep40_l0_test_time 0.1843485330000476
Test Epoch40 layer1 Acc 0.8823684210526316, AUC 0.9495027661323547, avg_entr 0.016738450154662132, f1 0.8823684453964233
ep40_l1_test_time 0.23122958400017524
Test Epoch40 layer2 Acc 0.8818421052631579, AUC 0.9523711204528809, avg_entr 0.012852777726948261, f1 0.8818420767784119
ep40_l2_test_time 0.30255711299969335
Test Epoch40 layer3 Acc 0.8826315789473684, AUC 0.9516605734825134, avg_entr 0.011950052343308926, f1 0.8826315999031067
ep40_l3_test_time 0.4027918210003918
Test Epoch40 layer4 Acc 0.8826315789473684, AUC 0.9489427804946899, avg_entr 0.010976259596645832, f1 0.8826315999031067
ep40_l4_test_time 0.5317559279997113
gc 0
Train Epoch41 Acc 0.98375 (118050/120000), AUC 0.9984513521194458
ep41_train_time 60.78832804900003
Test Epoch41 layer0 Acc 0.8921052631578947, AUC 0.967896044254303, avg_entr 0.042393315583467484, f1 0.8921052813529968
ep41_l0_test_time 0.18477332000020397
Test Epoch41 layer1 Acc 0.8818421052631579, AUC 0.9498975872993469, avg_entr 0.01646980457007885, f1 0.8818420767784119
ep41_l1_test_time 0.232376911999836
Test Epoch41 layer2 Acc 0.8821052631578947, AUC 0.9530549049377441, avg_entr 0.01265537180006504, f1 0.88210529088974
ep41_l2_test_time 0.3030447779997303
Test Epoch41 layer3 Acc 0.8821052631578947, AUC 0.9519468545913696, avg_entr 0.011616935953497887, f1 0.88210529088974
ep41_l3_test_time 0.40395336100027635
Test Epoch41 layer4 Acc 0.8823684210526316, AUC 0.94951331615448, avg_entr 0.010564645752310753, f1 0.8823684453964233
ep41_l4_test_time 0.5316079979997994
gc 0
Train Epoch42 Acc 0.9841333333333333 (118096/120000), AUC 0.9984102845191956
ep42_train_time 60.63250609099987
Test Epoch42 layer0 Acc 0.891578947368421, AUC 0.9678144454956055, avg_entr 0.04199884831905365, f1 0.8915789723396301
ep42_l0_test_time 0.18436571399979584
Test Epoch42 layer1 Acc 0.8826315789473684, AUC 0.9496907591819763, avg_entr 0.016326550394296646, f1 0.8826315999031067
ep42_l1_test_time 0.23157335799987777
Test Epoch42 layer2 Acc 0.8828947368421053, AUC 0.9529147148132324, avg_entr 0.012452489696443081, f1 0.88289475440979
ep42_l2_test_time 0.3027972260001661
Test Epoch42 layer3 Acc 0.8836842105263157, AUC 0.9517466425895691, avg_entr 0.011493146419525146, f1 0.8836842179298401
ep42_l3_test_time 0.40230270299980475
Test Epoch42 layer4 Acc 0.8831578947368421, AUC 0.9498559236526489, avg_entr 0.010391791351139545, f1 0.8831579089164734
ep42_l4_test_time 0.5296235549999437
gc 0
Train Epoch43 Acc 0.98395 (118074/120000), AUC 0.9983261227607727
ep43_train_time 60.507228284999655
Test Epoch43 layer0 Acc 0.891578947368421, AUC 0.9678206443786621, avg_entr 0.04212654381990433, f1 0.8915789723396301
ep43_l0_test_time 0.18715341000006447
Test Epoch43 layer1 Acc 0.8821052631578947, AUC 0.9500059485435486, avg_entr 0.016810987144708633, f1 0.88210529088974
ep43_l1_test_time 0.23170963399979883
Test Epoch43 layer2 Acc 0.8826315789473684, AUC 0.9532403945922852, avg_entr 0.012744270265102386, f1 0.8826315999031067
ep43_l2_test_time 0.30284195900003397
Test Epoch43 layer3 Acc 0.8828947368421053, AUC 0.9523762464523315, avg_entr 0.011785872280597687, f1 0.88289475440979
ep43_l3_test_time 0.4035479940002915
Test Epoch43 layer4 Acc 0.8828947368421053, AUC 0.9495433568954468, avg_entr 0.01070602796971798, f1 0.88289475440979
ep43_l4_test_time 0.5303127480001422
gc 0
Train Epoch44 Acc 0.9844416666666667 (118133/120000), AUC 0.9983905553817749
ep44_train_time 60.65926013599983
Test Epoch44 layer0 Acc 0.891578947368421, AUC 0.9677440524101257, avg_entr 0.04184683412313461, f1 0.8915789723396301
ep44_l0_test_time 0.1845620610001788
Test Epoch44 layer1 Acc 0.8823684210526316, AUC 0.9493640661239624, avg_entr 0.01670178212225437, f1 0.8823684453964233
ep44_l1_test_time 0.23153143299987278
Test Epoch44 layer2 Acc 0.8823684210526316, AUC 0.9528504014015198, avg_entr 0.012639986351132393, f1 0.8823684453964233
ep44_l2_test_time 0.3028263680002965
Test Epoch44 layer3 Acc 0.8828947368421053, AUC 0.9521251916885376, avg_entr 0.01166460383683443, f1 0.88289475440979
ep44_l3_test_time 0.40250665500025207
Test Epoch44 layer4 Acc 0.8828947368421053, AUC 0.9487416744232178, avg_entr 0.010574636049568653, f1 0.88289475440979
ep44_l4_test_time 0.5296075020000899
gc 0
Train Epoch45 Acc 0.9841083333333334 (118093/120000), AUC 0.9985249638557434
ep45_train_time 60.67789674000005
Test Epoch45 layer0 Acc 0.891578947368421, AUC 0.9677785634994507, avg_entr 0.04167170822620392, f1 0.8915789723396301
ep45_l0_test_time 0.1862695699996948
Test Epoch45 layer1 Acc 0.8821052631578947, AUC 0.9496750831604004, avg_entr 0.016686391085386276, f1 0.88210529088974
ep45_l1_test_time 0.23191088600015064
Test Epoch45 layer2 Acc 0.8818421052631579, AUC 0.9531834125518799, avg_entr 0.012876590713858604, f1 0.8818420767784119
ep45_l2_test_time 0.30466225300006045
Test Epoch45 layer3 Acc 0.8823684210526316, AUC 0.9522877335548401, avg_entr 0.01197875663638115, f1 0.8823684453964233
ep45_l3_test_time 0.4045372559999123
Test Epoch45 layer4 Acc 0.8823684210526316, AUC 0.9488722085952759, avg_entr 0.010907085612416267, f1 0.8823684453964233
ep45_l4_test_time 0.5300243649999175
gc 0
Train Epoch46 Acc 0.9839166666666667 (118070/120000), AUC 0.9984517097473145
ep46_train_time 60.72570255399978
Test Epoch46 layer0 Acc 0.8913157894736842, AUC 0.9677212238311768, avg_entr 0.04156237468123436, f1 0.8913158178329468
ep46_l0_test_time 0.18773632500005988
Test Epoch46 layer1 Acc 0.8821052631578947, AUC 0.9499467611312866, avg_entr 0.016586564481258392, f1 0.88210529088974
ep46_l1_test_time 0.23299732400028006
Test Epoch46 layer2 Acc 0.881578947368421, AUC 0.9530768394470215, avg_entr 0.012758827768266201, f1 0.8815789222717285
ep46_l2_test_time 0.3041670749998957
Test Epoch46 layer3 Acc 0.881578947368421, AUC 0.9527407288551331, avg_entr 0.011830318719148636, f1 0.8815789222717285
ep46_l3_test_time 0.4046616730001915
Test Epoch46 layer4 Acc 0.8823684210526316, AUC 0.9492525458335876, avg_entr 0.010745526291429996, f1 0.8823684453964233
ep46_l4_test_time 0.5303237660000377
gc 0
Train Epoch47 Acc 0.9842083333333334 (118105/120000), AUC 0.9983856678009033
ep47_train_time 60.48201194600006
Test Epoch47 layer0 Acc 0.8918421052631579, AUC 0.9677889943122864, avg_entr 0.04149245098233223, f1 0.8918421268463135
ep47_l0_test_time 0.18990743300037138
Test Epoch47 layer1 Acc 0.8826315789473684, AUC 0.9498170614242554, avg_entr 0.016469361260533333, f1 0.8826315999031067
ep47_l1_test_time 0.23303209000005154
Test Epoch47 layer2 Acc 0.8823684210526316, AUC 0.9525066018104553, avg_entr 0.01248653419315815, f1 0.8823684453964233
ep47_l2_test_time 0.30420683800002735
Test Epoch47 layer3 Acc 0.8828947368421053, AUC 0.9520843029022217, avg_entr 0.011518225073814392, f1 0.88289475440979
ep47_l3_test_time 0.40334013600022445
Test Epoch47 layer4 Acc 0.8831578947368421, AUC 0.9491801261901855, avg_entr 0.01048146840184927, f1 0.8831579089164734
ep47_l4_test_time 0.5302021640000021
gc 0
Train Epoch48 Acc 0.9840166666666667 (118082/120000), AUC 0.9983491897583008
ep48_train_time 60.681435990999944
Test Epoch48 layer0 Acc 0.891578947368421, AUC 0.9677907228469849, avg_entr 0.041406869888305664, f1 0.8915789723396301
ep48_l0_test_time 0.18608009900026445
Test Epoch48 layer1 Acc 0.8826315789473684, AUC 0.9498679637908936, avg_entr 0.016460463404655457, f1 0.8826315999031067
ep48_l1_test_time 0.23236299300015162
Test Epoch48 layer2 Acc 0.881578947368421, AUC 0.9527146220207214, avg_entr 0.012521881610155106, f1 0.8815789222717285
ep48_l2_test_time 0.3040969639996547
Test Epoch48 layer3 Acc 0.881578947368421, AUC 0.9522958993911743, avg_entr 0.011583109386265278, f1 0.8815789222717285
ep48_l3_test_time 0.40357030799987115
Test Epoch48 layer4 Acc 0.8818421052631579, AUC 0.9492009878158569, avg_entr 0.010556736961007118, f1 0.8818420767784119
ep48_l4_test_time 0.5303820129997803
gc 0
Train Epoch49 Acc 0.98445 (118134/120000), AUC 0.9984911680221558
ep49_train_time 60.669720827999754
Test Epoch49 layer0 Acc 0.8921052631578947, AUC 0.9677996635437012, avg_entr 0.04136081039905548, f1 0.8921052813529968
ep49_l0_test_time 0.18551148299957276
Test Epoch49 layer1 Acc 0.8821052631578947, AUC 0.9497285485267639, avg_entr 0.016382640227675438, f1 0.88210529088974
ep49_l1_test_time 0.2327797479997571
Test Epoch49 layer2 Acc 0.881578947368421, AUC 0.9526143074035645, avg_entr 0.01230503711849451, f1 0.8815789222717285
ep49_l2_test_time 0.3035478790002344
Test Epoch49 layer3 Acc 0.881578947368421, AUC 0.952003002166748, avg_entr 0.011346179991960526, f1 0.8815789222717285
ep49_l3_test_time 0.40452924999999595
Test Epoch49 layer4 Acc 0.881578947368421, AUC 0.9487690329551697, avg_entr 0.010350796394050121, f1 0.8815789222717285
ep49_l4_test_time 0.5293262549998872
Best AUC tensor(0.8955) 16 0
train_as_loss [[4.57602305e+02 3.56750768e+02 3.51115729e+02 3.49858912e+02
  3.49381050e+02 3.49150542e+02 3.49023196e+02 3.48946437e+02
  3.48897318e+02 3.48864500e+02 3.48841862e+02 3.48825867e+02
  3.48814356e+02 3.48805946e+02 3.48799738e+02 3.48795116e+02
  3.48792361e+02 3.48790704e+02 3.48789169e+02 3.48787747e+02
  3.48786773e+02 3.48786155e+02 3.48785475e+02 3.48784804e+02
  3.48784422e+02 3.48783983e+02 3.48783687e+02 3.48783356e+02
  3.48782978e+02 3.48782821e+02 3.48782739e+02 3.48782500e+02
  3.48782190e+02 3.48782104e+02 3.48781960e+02 3.48781900e+02
  3.48781863e+02 3.48781829e+02 3.48781776e+02 3.48781647e+02
  3.48781609e+02 3.48781559e+02 3.48781342e+02 3.48781279e+02
  3.48781251e+02 3.48781229e+02 3.48781163e+02 3.48781116e+02
  3.48781084e+02 3.48781084e+02]
 [1.86068683e+00 1.81792703e-05 1.38603183e-06 2.80693028e-07
  1.65319580e-07 1.29342036e-06 6.36283193e-06 3.07763881e-05
  8.06562183e-05 1.65507531e-04 5.99273696e-06 2.59125223e-09
  1.51651914e-05 8.11874011e-05 2.15480232e-07 1.38932658e-09
  7.19979716e-10 2.59486393e-05 3.02486871e-07 5.19740872e-10
  1.07126656e-08 9.69536273e-06 1.98062943e-08 3.17023637e-10
  2.47360071e-10 2.52594306e-06 2.48735485e-10 2.21246218e-10
  2.53648961e-10 6.38761739e-07 4.69771115e-09 1.70763322e-10
  1.29381698e-10 1.31466894e-07 1.48410862e-10 1.34633077e-10
  1.03346200e-10 7.26964537e-09 2.61798286e-10 9.64089949e-11
  7.94638926e-11 1.93989894e-10 8.72886405e-11 6.83598327e-11
  5.18036645e-11 5.63993443e-11 4.37010325e-11 4.13660340e-11
  2.86836181e-11 2.65635662e-11]
 [1.69585155e+00 1.92723015e-05 1.53968526e-06 3.19411075e-07
  1.05832251e-07 3.30092016e-07 4.89951629e-06 3.70279720e-05
  1.51661690e-04 3.51889212e-04 1.45602930e-05 5.77109795e-09
  1.53476059e-05 2.17113730e-04 9.74756331e-07 3.27406802e-09
  2.90652567e-09 6.08052246e-05 3.61822452e-07 1.38420130e-09
  1.31668678e-08 2.05938340e-05 1.62886088e-07 8.80570825e-10
  9.92017198e-10 5.14501011e-06 5.85055663e-10 6.62847024e-10
  8.66144676e-10 1.19762659e-06 2.55092646e-08 4.96845448e-10
  5.66035558e-10 2.48423614e-07 3.78395568e-10 4.29482303e-10
  4.50629451e-10 1.50920596e-08 1.21319980e-09 3.47083478e-10
  3.98623992e-10 7.35429959e-10 2.68515337e-10 2.57235712e-10
  2.56843808e-10 3.23271104e-10 1.24464546e-10 1.29759738e-10
  1.17258307e-10 1.17512173e-10]
 [1.81641748e+00 1.57906215e-05 1.60093088e-06 4.05500086e-07
  1.58599016e-07 3.09754357e-07 4.63178882e-06 4.60832992e-05
  2.43935806e-04 5.44247962e-04 2.19848155e-05 2.09394380e-08
  1.30041832e-05 3.31954491e-04 1.84820978e-06 1.06349130e-08
  1.28329922e-08 9.60918663e-05 3.42773048e-07 4.57848855e-09
  1.75261175e-08 2.86753612e-05 3.06690040e-07 3.01557636e-09
  4.02994520e-09 6.88307895e-06 1.72502530e-09 2.36922891e-09
  3.39671558e-09 1.60397677e-06 6.36509920e-08 1.66232339e-09
  2.48192608e-09 3.49345605e-07 1.12771148e-09 1.55059066e-09
  1.78409625e-09 2.42088734e-08 3.36157797e-09 1.21429395e-09
  1.60193173e-09 2.48049276e-09 8.27871799e-10 9.22777822e-10
  9.73297897e-10 1.19524850e-09 3.62507203e-10 4.18912045e-10
  3.99905162e-10 3.93776676e-10]
 [1.72117421e+00 1.54868177e-05 1.91423505e-06 6.27484165e-07
  3.22698771e-07 5.69811310e-07 7.93237054e-06 8.17828704e-05
  1.97325701e-04 4.31236064e-04 5.92205894e-05 1.08368897e-07
  1.08696443e-05 2.51142753e-04 6.16921829e-06 6.14286423e-08
  1.83565474e-08 7.73192755e-05 5.37805731e-07 2.89026301e-08
  1.65668775e-08 2.25663911e-05 1.20615515e-06 2.02280528e-08
  5.26248172e-09 5.44514295e-06 9.02947655e-09 1.70481340e-08
  4.22031419e-09 1.27703450e-06 2.82225859e-07 1.10129329e-08
  2.84308371e-09 2.84514823e-07 5.83346430e-09 1.01879647e-08
  2.11890247e-09 1.86179073e-08 1.51376204e-08 6.92724115e-09
  1.63143118e-09 2.13337697e-09 3.47788667e-09 4.17595579e-09
  8.55736113e-10 1.02114268e-09 1.21973927e-09 1.38832251e-09
  3.61159630e-10 3.53820374e-10]]
train_ae_loss [[12.6639588  11.79663578 11.64744502 11.2219242  10.846453   10.53864425
  10.31536789 10.13742698  9.96947885  9.76262489  9.65387478  9.52055256
   9.32923941  9.23237262  9.12499901  9.00215887  8.18451219  7.97715793
   7.85561155  7.77691865  7.32716995  7.23208821  7.16050722  7.13206175
   6.91695594  6.87979271  6.8634262   6.84001908  6.74932664  6.7552024
   6.73624504  6.72900819  6.71525026  6.70753115  6.72215383  6.7586438
   6.75317855  6.77312901  6.78972468  6.79492283  6.79504744  6.82372868
   6.81974508  6.85966201  6.86550718  6.85941988  6.8786345   6.87432573
   6.89452774  6.90555287]
 [13.15637625 11.50097932 10.16374588  9.49769059  9.03998797  8.58120213
   8.14422417  7.71178147  7.3231641   6.89613088  6.46549131  6.17571731
   5.85066111  5.61635121  5.28899833  5.08746451  4.27162639  3.97289219
   3.76754652  3.64625794  3.18729725  3.07413858  2.92593892  2.86549596
   2.72050549  2.62486116  2.56438633  2.49796077  2.43337597  2.44310837
   2.38871599  2.32257928  2.31294008  2.31057825  2.27634862  2.26531096
   2.28649622  2.27613075  2.29348979  2.27900187  2.29332368  2.2674784
   2.27253961  2.30241693  2.28489255  2.27916416  2.26171766  2.28547138
   2.26885865  2.27233259]
 [14.17634192 10.98491358  9.0413659   8.39443023  7.87051846  7.35605632
   6.95034921  6.55971147  6.22125791  5.8190353   5.38692956  5.12005692
   4.82604731  4.61593118  4.30183169  4.12063305  3.43046172  3.1812944
   2.99507214  2.88047767  2.50068981  2.39856033  2.27016352  2.22085333
   2.10454264  2.01709762  1.96247822  1.90760294  1.86018072  1.86169354
   1.8168483   1.7583258   1.74744695  1.7424934   1.71638283  1.69584359
   1.72257843  1.70593163  1.72274812  1.71254174  1.72742238  1.69588781
   1.70762898  1.72900196  1.71244656  1.7028615   1.69555046  1.70763063
   1.69663703  1.69385022]
 [14.42438938 11.14267584  8.42870207  7.80901687  7.33730541  6.82556763
   6.35067021  5.94827471  5.61739677  5.22337244  4.77847609  4.53482045
   4.26485382  4.07951808  3.76695615  3.6053681   2.99833817  2.77815102
   2.60525557  2.5018177   2.17126198  2.08265079  1.96354155  1.91997948
   1.82222883  1.74289949  1.6898557   1.64255901  1.60180961  1.60221972
   1.56022363  1.50724973  1.50026836  1.4915661   1.46987213  1.45236384
   1.4768585   1.4584863   1.47160429  1.46516579  1.47560457  1.44752214
   1.46052501  1.47572554  1.46075498  1.45481704  1.44736389  1.45635617
   1.44471285  1.44444136]
 [14.85807844 12.78967442  8.48085459  7.76361228  7.32968509  6.93083212
   6.46280987  5.97040357  5.40260878  5.01272996  4.61689741  4.3693437
   4.03421662  3.84972475  3.56640907  3.40733021  2.80904434  2.6008032
   2.44700974  2.34731687  2.02529015  1.94355686  1.83590178  1.79159566
   1.69586442  1.62099464  1.57308768  1.52777765  1.48664413  1.48755394
   1.44784702  1.39877689  1.38973822  1.38241504  1.36190715  1.3432002
   1.36618333  1.3485703   1.36021566  1.35333859  1.36233632  1.33470144
   1.34731601  1.3618704   1.34787999  1.34145309  1.3341378   1.34257152
   1.33204187  1.33119178]]
valid_acc (5, 50)
valid_AUC (5, 50)
train_acc (50,)
total_train+valid_time 3121.624348225
Start Testing
Load ckpt at ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt
gc 9
Test layer0 Acc 0.8778947368421053, AUC 0.9678746461868286, avg_entr 0.06559230387210846, f1 0.8778947591781616
l0_test_time 0.18660468799998853
gc 0
Test layer1 Acc 0.8760526315789474, AUC 0.9592929482460022, avg_entr 0.02710876241326332, f1 0.8760526180267334
l1_test_time 0.23371285099983652
gc 0
Test layer2 Acc 0.875, AUC 0.9612154960632324, avg_entr 0.02213461510837078, f1 0.875
l2_test_time 0.30573351600014576
gc 0
Test layer3 Acc 0.8739473684210526, AUC 0.9566836357116699, avg_entr 0.02032811939716339, f1 0.8739473819732666
l3_test_time 0.40741340300019147
gc 0
Test layer4 Acc 0.8742105263157894, AUC 0.958602786064148, avg_entr 0.01834682933986187, f1 0.87421053647995
l4_test_time 0.5328031039998677
gc 0
Test threshold 0.1 Acc 0.8742105263157894, AUC 0.9627736806869507, avg_entr 0.01950400322675705, f1 0.87421053647995
t0.1_test_time 0.24716450000005352
gc 0
Test threshold 0.2 Acc 0.873421052631579, AUC 0.9642682075500488, avg_entr 0.024381699040532112, f1 0.8734210729598999
t0.2_test_time 0.24026934899984553
gc 0
Test threshold 0.3 Acc 0.8757894736842106, AUC 0.9662445783615112, avg_entr 0.032395459711551666, f1 0.87578946352005
t0.3_test_time 0.2327905539996209
gc 0
Test threshold 0.4 Acc 0.8773684210526316, AUC 0.9672431349754333, avg_entr 0.04036836326122284, f1 0.8773684501647949
t0.4_test_time 0.22147746800010282
gc 0
Test threshold 0.5 Acc 0.8776315789473684, AUC 0.9677910804748535, avg_entr 0.045743681490421295, f1 0.8776316046714783
t0.5_test_time 0.20539433699968868
gc 0
Test threshold 0.6 Acc 0.8778947368421053, AUC 0.9678746461868286, avg_entr 0.0473148413002491, f1 0.8778947591781616
t0.6_test_time 0.19832645000042248
gc 0
Test threshold 0.7 Acc 0.8778947368421053, AUC 0.9678746461868286, avg_entr 0.0473148413002491, f1 0.8778947591781616
t0.7_test_time 0.1978218869999182
gc 0
Test threshold 0.8 Acc 0.8778947368421053, AUC 0.9678746461868286, avg_entr 0.0473148413002491, f1 0.8778947591781616
t0.8_test_time 0.19778061399983926
gc 0
Test threshold 0.9 Acc 0.8778947368421053, AUC 0.9678746461868286, avg_entr 0.0473148413002491, f1 0.8778947591781616
t0.9_test_time 0.1974361399998088
