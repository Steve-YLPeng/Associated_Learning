total count words 102019
vocab size 30000
train size 120000, valid size 3800, test size 3800
found 26754 words in glove
model: TransformerALsideText(
  (layers): ModuleList(
    (0): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=4, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=4, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
  (emb_layers): ModuleList(
    (0): Embedding(30000, 300)
    (1): Embedding(30000, 300)
    (2): Embedding(30000, 300)
    (3): Embedding(30000, 300)
    (4): Embedding(30000, 300)
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.multi_headed_attention.linears.0.weight 90000
layers.0.enc.f.multi_headed_attention.linears.0.bias 300
layers.0.enc.f.multi_headed_attention.linears.1.weight 90000
layers.0.enc.f.multi_headed_attention.linears.1.bias 300
layers.0.enc.f.multi_headed_attention.linears.2.weight 90000
layers.0.enc.f.multi_headed_attention.linears.2.bias 300
layers.0.enc.f.multi_headed_attention.linears.3.weight 90000
layers.0.enc.f.multi_headed_attention.linears.3.bias 300
layers.0.enc.f.feed_forward.w_1.weight 90000
layers.0.enc.f.feed_forward.w_1.bias 300
layers.0.enc.f.feed_forward.w_2.weight 90000
layers.0.enc.f.feed_forward.w_2.bias 300
layers.0.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.0.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.0.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.0.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.0.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.0.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.0.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.0.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.0.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.0.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.0.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.0.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.0.enc.f.encoder.norm.a 300
layers.0.enc.f.encoder.norm.b 300
layers.0.ae.g.0.weight 512
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 512
layers.0.ae.h.0.bias 4
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
emb_layers.0.weight 9000000
emb_layers.1.weight 9000000
emb_layers.2.weight 9000000
emb_layers.3.weight 9000000
emb_layers.4.weight 9000000
Total Trainable Params: 50758892
init_time 15.826502685
Start Training
gc 0
Train Epoch0 Acc 0.2911166666666667 (34934/120000), AUC 0.5395805835723877
ep0_train_time 61.168169778999996
Test Epoch0 layer0 Acc 0.8034210526315789, AUC 0.9382847547531128, avg_entr 0.7335964441299438, f1 0.803421139717102
ep0_l0_test_time 0.19022483300000204
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 0
Test Epoch0 layer1 Acc 0.7989473684210526, AUC 0.9385948181152344, avg_entr 0.7426040768623352, f1 0.7989473938941956
ep0_l1_test_time 0.23778296999999782
Test Epoch0 layer2 Acc 0.7852631578947369, AUC 0.9389544725418091, avg_entr 0.7680526971817017, f1 0.785263180732727
ep0_l2_test_time 0.3092872100000079
Test Epoch0 layer3 Acc 0.7457894736842106, AUC 0.936616063117981, avg_entr 0.8537859916687012, f1 0.7457894682884216
ep0_l3_test_time 0.4097685670000004
Test Epoch0 layer4 Acc 0.7463157894736843, AUC 0.9170044660568237, avg_entr 1.081836223602295, f1 0.7463157773017883
ep0_l4_test_time 0.5367946510000081
gc 0
Train Epoch1 Acc 0.777525 (93303/120000), AUC 0.9235585331916809
ep1_train_time 60.74293605400001
Test Epoch1 layer0 Acc 0.8355263157894737, AUC 0.9597965478897095, avg_entr 0.3724152147769928, f1 0.8355263471603394
ep1_l0_test_time 0.1901578499999914
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 1
Test Epoch1 layer1 Acc 0.8326315789473684, AUC 0.9619075655937195, avg_entr 0.2920111119747162, f1 0.832631528377533
ep1_l1_test_time 0.23694872200002237
Test Epoch1 layer2 Acc 0.8336842105263158, AUC 0.962334394454956, avg_entr 0.2913808524608612, f1 0.8336842060089111
ep1_l2_test_time 0.30856864199998313
Test Epoch1 layer3 Acc 0.8286842105263158, AUC 0.9620577692985535, avg_entr 0.2847820818424225, f1 0.8286842107772827
ep1_l3_test_time 0.4096010179999894
Test Epoch1 layer4 Acc 0.8252631578947368, AUC 0.9619299173355103, avg_entr 0.29714030027389526, f1 0.8252631425857544
ep1_l4_test_time 0.5348684779999928
gc 0
Train Epoch2 Acc 0.8624583333333333 (103495/120000), AUC 0.961143434047699
ep2_train_time 60.75501414299998
Test Epoch2 layer0 Acc 0.8628947368421053, AUC 0.9662975072860718, avg_entr 0.24226197600364685, f1 0.8628947138786316
ep2_l0_test_time 0.18997960999999464
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 2
Test Epoch2 layer1 Acc 0.865, AUC 0.9671955704689026, avg_entr 0.16123701632022858, f1 0.8650000095367432
ep2_l1_test_time 0.23762824500002466
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 2
Test Epoch2 layer2 Acc 0.8652631578947368, AUC 0.9678146243095398, avg_entr 0.15581703186035156, f1 0.8652631640434265
ep2_l2_test_time 0.308913050000001
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 2
Test Epoch2 layer3 Acc 0.863421052631579, AUC 0.9678371548652649, avg_entr 0.15731681883335114, f1 0.8634210228919983
ep2_l3_test_time 0.4095591909999996
Test Epoch2 layer4 Acc 0.8623684210526316, AUC 0.9682745933532715, avg_entr 0.16173161566257477, f1 0.8623684048652649
ep2_l4_test_time 0.5365419609999833
gc 0
Train Epoch3 Acc 0.883675 (106041/120000), AUC 0.9687157869338989
ep3_train_time 60.698726644000004
Test Epoch3 layer0 Acc 0.8673684210526316, AUC 0.9700324535369873, avg_entr 0.17166060209274292, f1 0.8673684000968933
ep3_l0_test_time 0.1905549520000136
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 3
Test Epoch3 layer1 Acc 0.8710526315789474, AUC 0.9720273017883301, avg_entr 0.1113760769367218, f1 0.871052622795105
ep3_l1_test_time 0.23777128099999345
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 3
Test Epoch3 layer2 Acc 0.8710526315789474, AUC 0.9717473387718201, avg_entr 0.1054162085056305, f1 0.871052622795105
ep3_l2_test_time 0.30813190899999654
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 3
Test Epoch3 layer3 Acc 0.8707894736842106, AUC 0.9725847244262695, avg_entr 0.10151755809783936, f1 0.8707894682884216
ep3_l3_test_time 0.4110073709999824
Test Epoch3 layer4 Acc 0.8697368421052631, AUC 0.9714981317520142, avg_entr 0.1005890890955925, f1 0.8697368502616882
ep3_l4_test_time 0.5365601679999941
gc 0
Train Epoch4 Acc 0.8975166666666666 (107702/120000), AUC 0.9735530614852905
ep4_train_time 60.73805529499998
Test Epoch4 layer0 Acc 0.8697368421052631, AUC 0.970700740814209, avg_entr 0.14173316955566406, f1 0.8697368502616882
ep4_l0_test_time 0.19112367200000335
Test Epoch4 layer1 Acc 0.8718421052631579, AUC 0.9714909791946411, avg_entr 0.08962195366621017, f1 0.8718421459197998
ep4_l1_test_time 0.23894421300002477
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 4
Test Epoch4 layer2 Acc 0.8726315789473684, AUC 0.9715806841850281, avg_entr 0.08798622339963913, f1 0.8726315498352051
ep4_l2_test_time 0.30806402399997523
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 4
Test Epoch4 layer3 Acc 0.8731578947368421, AUC 0.9713835716247559, avg_entr 0.08312565833330154, f1 0.8731579184532166
ep4_l3_test_time 0.4089272940000228
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 4
Test Epoch4 layer4 Acc 0.8723684210526316, AUC 0.968387246131897, avg_entr 0.08340483158826828, f1 0.8723683953285217
ep4_l4_test_time 0.5362369110000031
gc 0
Train Epoch5 Acc 0.907125 (108855/120000), AUC 0.9762259125709534
ep5_train_time 60.73977433700003
Test Epoch5 layer0 Acc 0.8752631578947369, AUC 0.9715260863304138, avg_entr 0.12616141140460968, f1 0.8752631545066833
ep5_l0_test_time 0.19140848899996854
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 5
Test Epoch5 layer1 Acc 0.875, AUC 0.9738203883171082, avg_entr 0.07824105024337769, f1 0.875
ep5_l1_test_time 0.24916022600001497
Test Epoch5 layer2 Acc 0.8752631578947369, AUC 0.9738524556159973, avg_entr 0.07175253331661224, f1 0.8752631545066833
ep5_l2_test_time 0.3117690340000081
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 5
Test Epoch5 layer3 Acc 0.8752631578947369, AUC 0.9734450578689575, avg_entr 0.06906136870384216, f1 0.8752631545066833
ep5_l3_test_time 0.4087810540000305
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 5
Test Epoch5 layer4 Acc 0.8736842105263158, AUC 0.970730185508728, avg_entr 0.06721123307943344, f1 0.8736842274665833
ep5_l4_test_time 0.5352914400000373
gc 0
Train Epoch6 Acc 0.9154416666666667 (109853/120000), AUC 0.9792040586471558
ep6_train_time 60.766605701
Test Epoch6 layer0 Acc 0.8771052631578947, AUC 0.9710500240325928, avg_entr 0.10500425100326538, f1 0.8771052360534668
ep6_l0_test_time 0.19068155200000092
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 6
Test Epoch6 layer1 Acc 0.8789473684210526, AUC 0.971268892288208, avg_entr 0.05989334359765053, f1 0.878947377204895
ep6_l1_test_time 0.2431512300000236
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 6
Test Epoch6 layer2 Acc 0.8781578947368421, AUC 0.9709903597831726, avg_entr 0.05308467522263527, f1 0.878157913684845
ep6_l2_test_time 0.31856998599999997
Test Epoch6 layer3 Acc 0.8773684210526316, AUC 0.9707527160644531, avg_entr 0.049323298037052155, f1 0.8773684501647949
ep6_l3_test_time 0.4099179880000179
Test Epoch6 layer4 Acc 0.8768421052631579, AUC 0.968891978263855, avg_entr 0.044688545167446136, f1 0.8768420815467834
ep6_l4_test_time 0.5371325500000239
gc 0
Train Epoch7 Acc 0.9232416666666666 (110789/120000), AUC 0.9819513559341431
ep7_train_time 60.743462763000025
Test Epoch7 layer0 Acc 0.8797368421052632, AUC 0.971293568611145, avg_entr 0.09633274376392365, f1 0.8797368407249451
ep7_l0_test_time 0.19157841900005224
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 7
Test Epoch7 layer1 Acc 0.8810526315789474, AUC 0.9724956750869751, avg_entr 0.054908253252506256, f1 0.8810526132583618
ep7_l1_test_time 0.2369366269999773
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 7
Test Epoch7 layer2 Acc 0.8807894736842106, AUC 0.9723768830299377, avg_entr 0.047391630709171295, f1 0.8807894587516785
ep7_l2_test_time 0.31083822400000827
Test Epoch7 layer3 Acc 0.8813157894736842, AUC 0.972568690776825, avg_entr 0.04350978136062622, f1 0.8813157677650452
ep7_l3_test_time 0.41138808400000926
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 7
Test Epoch7 layer4 Acc 0.8807894736842106, AUC 0.9716346263885498, avg_entr 0.03964982554316521, f1 0.8807894587516785
ep7_l4_test_time 0.5363836469999796
gc 0
Train Epoch8 Acc 0.9297916666666667 (111575/120000), AUC 0.983903706073761
ep8_train_time 60.89746865500001
Test Epoch8 layer0 Acc 0.881578947368421, AUC 0.9710863828659058, avg_entr 0.0878777801990509, f1 0.8815789222717285
ep8_l0_test_time 0.1908158179999191
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 8
Test Epoch8 layer1 Acc 0.8847368421052632, AUC 0.9691454768180847, avg_entr 0.04591863602399826, f1 0.8847368359565735
ep8_l1_test_time 0.23688218500001312
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 8
Test Epoch8 layer2 Acc 0.8842105263157894, AUC 0.9709896445274353, avg_entr 0.03993788734078407, f1 0.8842105865478516
ep8_l2_test_time 0.3078287209999644
Test Epoch8 layer3 Acc 0.8842105263157894, AUC 0.9694585204124451, avg_entr 0.03699742257595062, f1 0.8842105865478516
ep8_l3_test_time 0.40816498799995315
Test Epoch8 layer4 Acc 0.8828947368421053, AUC 0.9666534662246704, avg_entr 0.032669100910425186, f1 0.88289475440979
ep8_l4_test_time 0.5361388000000034
gc 0
Train Epoch9 Acc 0.9348166666666666 (112178/120000), AUC 0.9855706691741943
ep9_train_time 60.82058850599992
Test Epoch9 layer0 Acc 0.8855263157894737, AUC 0.9709078669548035, avg_entr 0.08217542618513107, f1 0.8855262398719788
ep9_l0_test_time 0.19136468099998183
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 9
Test Epoch9 layer1 Acc 0.8828947368421053, AUC 0.9701254367828369, avg_entr 0.04371507465839386, f1 0.88289475440979
ep9_l1_test_time 0.23918013000002247
Test Epoch9 layer2 Acc 0.8839473684210526, AUC 0.9704697728157043, avg_entr 0.03836001455783844, f1 0.8839473724365234
ep9_l2_test_time 0.3149388680000129
Test Epoch9 layer3 Acc 0.8836842105263157, AUC 0.9692694544792175, avg_entr 0.03600435331463814, f1 0.8836842179298401
ep9_l3_test_time 0.4086700909999763
Test Epoch9 layer4 Acc 0.8831578947368421, AUC 0.9651904702186584, avg_entr 0.03253567963838577, f1 0.8831579089164734
ep9_l4_test_time 0.5361358019999898
gc 0
Train Epoch10 Acc 0.9447833333333333 (113374/120000), AUC 0.9885563254356384
ep10_train_time 60.718466557000056
Test Epoch10 layer0 Acc 0.8839473684210526, AUC 0.9720537066459656, avg_entr 0.07896529138088226, f1 0.8839473724365234
ep10_l0_test_time 0.18992878599999585
Test Epoch10 layer1 Acc 0.8821052631578947, AUC 0.9689503908157349, avg_entr 0.03831940516829491, f1 0.88210529088974
ep10_l1_test_time 0.2454359089999798
Test Epoch10 layer2 Acc 0.8823684210526316, AUC 0.9727141857147217, avg_entr 0.03210941329598427, f1 0.8823684453964233
ep10_l2_test_time 0.3091974909999635
Test Epoch10 layer3 Acc 0.8821052631578947, AUC 0.9713022708892822, avg_entr 0.029786348342895508, f1 0.88210529088974
ep10_l3_test_time 0.4127878500000861
Test Epoch10 layer4 Acc 0.8810526315789474, AUC 0.9699640274047852, avg_entr 0.02656593546271324, f1 0.8810526132583618
ep10_l4_test_time 0.5418880989999479
gc 0
Train Epoch11 Acc 0.9496583333333334 (113959/120000), AUC 0.9891986846923828
ep11_train_time 60.909081302000004
Test Epoch11 layer0 Acc 0.8839473684210526, AUC 0.9710111618041992, avg_entr 0.07452951371669769, f1 0.8839473724365234
ep11_l0_test_time 0.19080852199999754
Test Epoch11 layer1 Acc 0.878421052631579, AUC 0.9671921730041504, avg_entr 0.034243516623973846, f1 0.8784210681915283
ep11_l1_test_time 0.23740758199994616
Test Epoch11 layer2 Acc 0.878421052631579, AUC 0.9680503606796265, avg_entr 0.028263095766305923, f1 0.8784210681915283
ep11_l2_test_time 0.3092913080000699
Test Epoch11 layer3 Acc 0.8776315789473684, AUC 0.9656888246536255, avg_entr 0.026189280673861504, f1 0.8776316046714783
ep11_l3_test_time 0.4104250639999236
Test Epoch11 layer4 Acc 0.8776315789473684, AUC 0.9656467437744141, avg_entr 0.022946273908019066, f1 0.8776316046714783
ep11_l4_test_time 0.5359347709999156
gc 0
Train Epoch12 Acc 0.9520416666666667 (114245/120000), AUC 0.99092698097229
ep12_train_time 60.75874768999995
Test Epoch12 layer0 Acc 0.8831578947368421, AUC 0.9709405899047852, avg_entr 0.073127880692482, f1 0.8831579089164734
ep12_l0_test_time 0.19066896399999678
Test Epoch12 layer1 Acc 0.8813157894736842, AUC 0.9659669399261475, avg_entr 0.035230010747909546, f1 0.8813157677650452
ep12_l1_test_time 0.23789202900002238
Test Epoch12 layer2 Acc 0.8818421052631579, AUC 0.9676914811134338, avg_entr 0.029995093122124672, f1 0.8818420767784119
ep12_l2_test_time 0.3093026210000289
Test Epoch12 layer3 Acc 0.8813157894736842, AUC 0.9646978974342346, avg_entr 0.027918217703700066, f1 0.8813157677650452
ep12_l3_test_time 0.4106956440000431
Test Epoch12 layer4 Acc 0.8805263157894737, AUC 0.9643458127975464, avg_entr 0.02536345086991787, f1 0.8805263042449951
ep12_l4_test_time 0.5357907370000703
gc 0
Train Epoch13 Acc 0.95385 (114462/120000), AUC 0.9912480115890503
ep13_train_time 60.77443857200001
Test Epoch13 layer0 Acc 0.881578947368421, AUC 0.9716302156448364, avg_entr 0.06979093700647354, f1 0.8815789222717285
ep13_l0_test_time 0.19004250099999354
Test Epoch13 layer1 Acc 0.8821052631578947, AUC 0.9659602642059326, avg_entr 0.031014464795589447, f1 0.88210529088974
ep13_l1_test_time 0.23694150099993294
Test Epoch13 layer2 Acc 0.883421052631579, AUC 0.9682028293609619, avg_entr 0.025586260482668877, f1 0.8834210634231567
ep13_l2_test_time 0.30781856699991295
Test Epoch13 layer3 Acc 0.883421052631579, AUC 0.9675753712654114, avg_entr 0.023450827226042747, f1 0.8834210634231567
ep13_l3_test_time 0.40969425299999784
Test Epoch13 layer4 Acc 0.8823684210526316, AUC 0.9662408232688904, avg_entr 0.020743684843182564, f1 0.8823684453964233
ep13_l4_test_time 0.53600533000008
gc 0
Train Epoch14 Acc 0.9574833333333334 (114898/120000), AUC 0.9930738806724548
ep14_train_time 60.828924967000034
Test Epoch14 layer0 Acc 0.8786842105263157, AUC 0.9705490469932556, avg_entr 0.06940314918756485, f1 0.8786842226982117
ep14_l0_test_time 0.19009812599995257
Test Epoch14 layer1 Acc 0.8828947368421053, AUC 0.9642941951751709, avg_entr 0.03155916556715965, f1 0.88289475440979
ep14_l1_test_time 0.23686663899991345
Test Epoch14 layer2 Acc 0.8823684210526316, AUC 0.9670418500900269, avg_entr 0.02742179110646248, f1 0.8823684453964233
ep14_l2_test_time 0.3075602810000646
Test Epoch14 layer3 Acc 0.881578947368421, AUC 0.9640318155288696, avg_entr 0.02578669972717762, f1 0.8815789222717285
ep14_l3_test_time 0.40941080400000374
Test Epoch14 layer4 Acc 0.8823684210526316, AUC 0.9633058905601501, avg_entr 0.023620963096618652, f1 0.8823684453964233
ep14_l4_test_time 0.5360921950000375
gc 0
Train Epoch15 Acc 0.9614166666666667 (115370/120000), AUC 0.9935048222541809
ep15_train_time 60.831342421000045
Test Epoch15 layer0 Acc 0.8836842105263157, AUC 0.9708432555198669, avg_entr 0.06495830416679382, f1 0.8836842179298401
ep15_l0_test_time 0.18951998500006084
Test Epoch15 layer1 Acc 0.8821052631578947, AUC 0.9652731418609619, avg_entr 0.02579572983086109, f1 0.88210529088974
ep15_l1_test_time 0.2362267299999985
Test Epoch15 layer2 Acc 0.8826315789473684, AUC 0.9683170914649963, avg_entr 0.020448753610253334, f1 0.8826315999031067
ep15_l2_test_time 0.3085133530000803
Test Epoch15 layer3 Acc 0.8831578947368421, AUC 0.9673983454704285, avg_entr 0.018825791776180267, f1 0.8831579089164734
ep15_l3_test_time 0.41165433399999074
Test Epoch15 layer4 Acc 0.8826315789473684, AUC 0.9660550355911255, avg_entr 0.016619134694337845, f1 0.8826315999031067
ep15_l4_test_time 0.5382881499999712
gc 0
Train Epoch16 Acc 0.9634333333333334 (115612/120000), AUC 0.9943206906318665
ep16_train_time 60.842795182000145
Test Epoch16 layer0 Acc 0.8828947368421053, AUC 0.9700332880020142, avg_entr 0.06582639366388321, f1 0.88289475440979
ep16_l0_test_time 0.19008756499988522
Test Epoch16 layer1 Acc 0.8828947368421053, AUC 0.9640569090843201, avg_entr 0.02824636548757553, f1 0.88289475440979
ep16_l1_test_time 0.23899899200000618
Test Epoch16 layer2 Acc 0.8821052631578947, AUC 0.9670260548591614, avg_entr 0.023922912776470184, f1 0.88210529088974
ep16_l2_test_time 0.3090309659999093
Test Epoch16 layer3 Acc 0.8805263157894737, AUC 0.9661022424697876, avg_entr 0.022174779325723648, f1 0.8805263042449951
ep16_l3_test_time 0.4101458119998824
Test Epoch16 layer4 Acc 0.8802631578947369, AUC 0.9636794328689575, avg_entr 0.020040178671479225, f1 0.880263090133667
ep16_l4_test_time 0.5366519480000989
gc 0
Train Epoch17 Acc 0.9647416666666667 (115769/120000), AUC 0.9943567514419556
ep17_train_time 60.76510338099979
Test Epoch17 layer0 Acc 0.8813157894736842, AUC 0.9701175689697266, avg_entr 0.06275571137666702, f1 0.8813157677650452
ep17_l0_test_time 0.19106958499992288
Test Epoch17 layer1 Acc 0.8794736842105263, AUC 0.9626478552818298, avg_entr 0.02492721565067768, f1 0.8794736862182617
ep17_l1_test_time 0.23977463999995052
Test Epoch17 layer2 Acc 0.8794736842105263, AUC 0.9654703140258789, avg_entr 0.020628752186894417, f1 0.8794736862182617
ep17_l2_test_time 0.3084923520000302
Test Epoch17 layer3 Acc 0.8792105263157894, AUC 0.9637171626091003, avg_entr 0.01909947767853737, f1 0.8792105317115784
ep17_l3_test_time 0.4099004579998109
Test Epoch17 layer4 Acc 0.8792105263157894, AUC 0.9646191596984863, avg_entr 0.017310749739408493, f1 0.8792105317115784
ep17_l4_test_time 0.5365889319998587
gc 0
Train Epoch18 Acc 0.9657916666666667 (115895/120000), AUC 0.9949571490287781
ep18_train_time 60.8947643250001
Test Epoch18 layer0 Acc 0.8805263157894737, AUC 0.9704796671867371, avg_entr 0.06319782137870789, f1 0.8805263042449951
ep18_l0_test_time 0.1890071350001108
Test Epoch18 layer1 Acc 0.8823684210526316, AUC 0.9635986089706421, avg_entr 0.02576899155974388, f1 0.8823684453964233
ep18_l1_test_time 0.23607584799992765
Test Epoch18 layer2 Acc 0.8813157894736842, AUC 0.9660627841949463, avg_entr 0.021681614220142365, f1 0.8813157677650452
ep18_l2_test_time 0.30840849500009426
Test Epoch18 layer3 Acc 0.8813157894736842, AUC 0.964913547039032, avg_entr 0.020456587895751, f1 0.8813157677650452
ep18_l3_test_time 0.4119067649999124
Test Epoch18 layer4 Acc 0.8807894736842106, AUC 0.9613131284713745, avg_entr 0.018883774057030678, f1 0.8807894587516785
ep18_l4_test_time 0.5368987219999326
gc 0
Train Epoch19 Acc 0.9680916666666667 (116171/120000), AUC 0.9951492547988892
ep19_train_time 60.89985079500002
Test Epoch19 layer0 Acc 0.8821052631578947, AUC 0.9701334238052368, avg_entr 0.06154054403305054, f1 0.88210529088974
ep19_l0_test_time 0.18947223300006044
Test Epoch19 layer1 Acc 0.8823684210526316, AUC 0.9623623490333557, avg_entr 0.024427024647593498, f1 0.8823684453964233
ep19_l1_test_time 0.23781930399991325
Test Epoch19 layer2 Acc 0.8823684210526316, AUC 0.9650282859802246, avg_entr 0.020028095692396164, f1 0.8823684453964233
ep19_l2_test_time 0.30858220999994046
Test Epoch19 layer3 Acc 0.8826315789473684, AUC 0.9627168774604797, avg_entr 0.018585653975605965, f1 0.8826315999031067
ep19_l3_test_time 0.41094200800012004
Test Epoch19 layer4 Acc 0.8823684210526316, AUC 0.9612264037132263, avg_entr 0.016773557290434837, f1 0.8823684453964233
ep19_l4_test_time 0.5376429440000265
gc 0
Train Epoch20 Acc 0.9696 (116352/120000), AUC 0.9957317113876343
ep20_train_time 60.73819933499999
Test Epoch20 layer0 Acc 0.8831578947368421, AUC 0.9695641994476318, avg_entr 0.059501562267541885, f1 0.8831579089164734
ep20_l0_test_time 0.1904104059999554
Test Epoch20 layer1 Acc 0.8818421052631579, AUC 0.9606696367263794, avg_entr 0.022675737738609314, f1 0.8818420767784119
ep20_l1_test_time 0.23710423900001842
Test Epoch20 layer2 Acc 0.8821052631578947, AUC 0.9643239974975586, avg_entr 0.01811356283724308, f1 0.88210529088974
ep20_l2_test_time 0.31289253399995687
Test Epoch20 layer3 Acc 0.8823684210526316, AUC 0.9627646803855896, avg_entr 0.017052525654435158, f1 0.8823684453964233
ep20_l3_test_time 0.4105177769999955
Test Epoch20 layer4 Acc 0.8828947368421053, AUC 0.962108314037323, avg_entr 0.015598668716847897, f1 0.88289475440979
ep20_l4_test_time 0.5358502479998606
gc 0
Train Epoch21 Acc 0.9697333333333333 (116368/120000), AUC 0.9958750605583191
ep21_train_time 60.71187756799986
Test Epoch21 layer0 Acc 0.8818421052631579, AUC 0.969597339630127, avg_entr 0.058505311608314514, f1 0.8818420767784119
ep21_l0_test_time 0.1902091939998627
Test Epoch21 layer1 Acc 0.8802631578947369, AUC 0.957867443561554, avg_entr 0.02111992798745632, f1 0.880263090133667
ep21_l1_test_time 0.23719656899993424
Test Epoch21 layer2 Acc 0.8805263157894737, AUC 0.9595816135406494, avg_entr 0.016430646181106567, f1 0.8805263042449951
ep21_l2_test_time 0.3078737829998772
Test Epoch21 layer3 Acc 0.8805263157894737, AUC 0.9571030139923096, avg_entr 0.015389843843877316, f1 0.8805263042449951
ep21_l3_test_time 0.408787416999985
Test Epoch21 layer4 Acc 0.8802631578947369, AUC 0.9557267427444458, avg_entr 0.014168433845043182, f1 0.880263090133667
ep21_l4_test_time 0.5360143679999965
gc 0
Train Epoch22 Acc 0.9705166666666667 (116462/120000), AUC 0.9960106611251831
ep22_train_time 60.84158538199995
Test Epoch22 layer0 Acc 0.8844736842105263, AUC 0.9697569608688354, avg_entr 0.057718705385923386, f1 0.8844736814498901
ep22_l0_test_time 0.19201486799988743
Test Epoch22 layer1 Acc 0.88, AUC 0.9580374360084534, avg_entr 0.021746782585978508, f1 0.8799999952316284
ep22_l1_test_time 0.24172411900008228
Test Epoch22 layer2 Acc 0.8797368421052632, AUC 0.9632235169410706, avg_entr 0.0168979000300169, f1 0.8797368407249451
ep22_l2_test_time 0.30850744999997914
Test Epoch22 layer3 Acc 0.8794736842105263, AUC 0.9618894457817078, avg_entr 0.015728088095784187, f1 0.8794736862182617
ep22_l3_test_time 0.4089519310000469
Test Epoch22 layer4 Acc 0.8794736842105263, AUC 0.9605226516723633, avg_entr 0.014259272255003452, f1 0.8794736862182617
ep22_l4_test_time 0.5362027240000771
gc 0
Train Epoch23 Acc 0.9723416666666667 (116681/120000), AUC 0.9961374998092651
ep23_train_time 60.90172979800013
Test Epoch23 layer0 Acc 0.881578947368421, AUC 0.9701178073883057, avg_entr 0.058095093816518784, f1 0.8815789222717285
ep23_l0_test_time 0.18948895199991966
Test Epoch23 layer1 Acc 0.8797368421052632, AUC 0.9576923251152039, avg_entr 0.022301290184259415, f1 0.8797368407249451
ep23_l1_test_time 0.23673970499999086
Test Epoch23 layer2 Acc 0.8797368421052632, AUC 0.9639984369277954, avg_entr 0.018097691237926483, f1 0.8797368407249451
ep23_l2_test_time 0.3081024090001847
Test Epoch23 layer3 Acc 0.8789473684210526, AUC 0.9628424644470215, avg_entr 0.016890963539481163, f1 0.878947377204895
ep23_l3_test_time 0.4089709419999963
Test Epoch23 layer4 Acc 0.878421052631579, AUC 0.9607968330383301, avg_entr 0.015313034877181053, f1 0.8784210681915283
ep23_l4_test_time 0.5376028580001275
gc 0
Train Epoch24 Acc 0.9717416666666666 (116609/120000), AUC 0.9964094161987305
ep24_train_time 60.70269955100002
Test Epoch24 layer0 Acc 0.8810526315789474, AUC 0.9695055484771729, avg_entr 0.056154098361730576, f1 0.8810526132583618
ep24_l0_test_time 0.18956555500017203
Test Epoch24 layer1 Acc 0.878421052631579, AUC 0.9554361701011658, avg_entr 0.020480036735534668, f1 0.8784210681915283
ep24_l1_test_time 0.23648312699992857
Test Epoch24 layer2 Acc 0.8778947368421053, AUC 0.959912896156311, avg_entr 0.016766516491770744, f1 0.8778947591781616
ep24_l2_test_time 0.30789690400001746
Test Epoch24 layer3 Acc 0.8778947368421053, AUC 0.9592255353927612, avg_entr 0.01573270745575428, f1 0.8778947591781616
ep24_l3_test_time 0.40940253899998424
Test Epoch24 layer4 Acc 0.8778947368421053, AUC 0.9597799777984619, avg_entr 0.014480998739600182, f1 0.8778947591781616
ep24_l4_test_time 0.5359265970000706
gc 0
Train Epoch25 Acc 0.971925 (116631/120000), AUC 0.9965711832046509
ep25_train_time 60.831254959000034
Test Epoch25 layer0 Acc 0.8826315789473684, AUC 0.9697439074516296, avg_entr 0.056010328233242035, f1 0.8826315999031067
ep25_l0_test_time 0.18990311399988968
Test Epoch25 layer1 Acc 0.8810526315789474, AUC 0.9559783935546875, avg_entr 0.020727770403027534, f1 0.8810526132583618
ep25_l1_test_time 0.2379710380000688
Test Epoch25 layer2 Acc 0.8810526315789474, AUC 0.9616913795471191, avg_entr 0.016584746539592743, f1 0.8810526132583618
ep25_l2_test_time 0.3090659700001197
Test Epoch25 layer3 Acc 0.881578947368421, AUC 0.9608423709869385, avg_entr 0.015499166212975979, f1 0.8815789222717285
ep25_l3_test_time 0.4107875069998954
Test Epoch25 layer4 Acc 0.8821052631578947, AUC 0.9602263569831848, avg_entr 0.014191346243023872, f1 0.88210529088974
ep25_l4_test_time 0.5374110889999884
gc 0
Train Epoch26 Acc 0.9735416666666666 (116825/120000), AUC 0.9968007206916809
ep26_train_time 61.00619027200014
Test Epoch26 layer0 Acc 0.8807894736842106, AUC 0.9695199728012085, avg_entr 0.055386774241924286, f1 0.8807894587516785
ep26_l0_test_time 0.19050176300015664
Test Epoch26 layer1 Acc 0.8794736842105263, AUC 0.956187903881073, avg_entr 0.020912833511829376, f1 0.8794736862182617
ep26_l1_test_time 0.23757520100002694
Test Epoch26 layer2 Acc 0.8792105263157894, AUC 0.9607361555099487, avg_entr 0.017022397369146347, f1 0.8792105317115784
ep26_l2_test_time 0.3084120220000841
Test Epoch26 layer3 Acc 0.8792105263157894, AUC 0.9599636793136597, avg_entr 0.016031160950660706, f1 0.8792105317115784
ep26_l3_test_time 0.40943666599991957
Test Epoch26 layer4 Acc 0.8794736842105263, AUC 0.9572123289108276, avg_entr 0.0147811658680439, f1 0.8794736862182617
ep26_l4_test_time 0.5356550860001335
gc 0
Train Epoch27 Acc 0.973825 (116859/120000), AUC 0.9967483878135681
ep27_train_time 60.81673295399992
Test Epoch27 layer0 Acc 0.883421052631579, AUC 0.9692307710647583, avg_entr 0.055207815021276474, f1 0.8834210634231567
ep27_l0_test_time 0.18931485500002054
Test Epoch27 layer1 Acc 0.8802631578947369, AUC 0.9558464288711548, avg_entr 0.020005598664283752, f1 0.880263090133667
ep27_l1_test_time 0.236640141999942
Test Epoch27 layer2 Acc 0.8805263157894737, AUC 0.9606869220733643, avg_entr 0.015837067738175392, f1 0.8805263042449951
ep27_l2_test_time 0.31254693200003203
Test Epoch27 layer3 Acc 0.8805263157894737, AUC 0.9585734605789185, avg_entr 0.014897705055773258, f1 0.8805263042449951
ep27_l3_test_time 0.4092217239999627
Test Epoch27 layer4 Acc 0.8805263157894737, AUC 0.958094596862793, avg_entr 0.013728968799114227, f1 0.8805263042449951
ep27_l4_test_time 0.5348131200000807
gc 0
Train Epoch28 Acc 0.9742833333333333 (116914/120000), AUC 0.9967982172966003
ep28_train_time 60.821554645000106
Test Epoch28 layer0 Acc 0.883421052631579, AUC 0.9692988395690918, avg_entr 0.054535627365112305, f1 0.8834210634231567
ep28_l0_test_time 0.1894324659999711
Test Epoch28 layer1 Acc 0.8802631578947369, AUC 0.9561666250228882, avg_entr 0.019962068647146225, f1 0.880263090133667
ep28_l1_test_time 0.23673818999986906
Test Epoch28 layer2 Acc 0.8797368421052632, AUC 0.9618901014328003, avg_entr 0.015775199979543686, f1 0.8797368407249451
ep28_l2_test_time 0.30848769399995035
Test Epoch28 layer3 Acc 0.8805263157894737, AUC 0.9603477120399475, avg_entr 0.014906192198395729, f1 0.8805263042449951
ep28_l3_test_time 0.4090169240000705
Test Epoch28 layer4 Acc 0.8807894736842106, AUC 0.9590123891830444, avg_entr 0.013738268986344337, f1 0.8807894587516785
ep28_l4_test_time 0.5352189590000762
gc 0
Train Epoch29 Acc 0.9735666666666667 (116828/120000), AUC 0.9968997240066528
ep29_train_time 60.72081716399998
Test Epoch29 layer0 Acc 0.8823684210526316, AUC 0.9695531129837036, avg_entr 0.05375944823026657, f1 0.8823684453964233
ep29_l0_test_time 0.18875955499993324
Test Epoch29 layer1 Acc 0.8789473684210526, AUC 0.9555835723876953, avg_entr 0.020723160356283188, f1 0.878947377204895
ep29_l1_test_time 0.23570502899997336
Test Epoch29 layer2 Acc 0.8794736842105263, AUC 0.9589486718177795, avg_entr 0.016853298991918564, f1 0.8794736862182617
ep29_l2_test_time 0.30704905499987944
Test Epoch29 layer3 Acc 0.8789473684210526, AUC 0.9570192098617554, avg_entr 0.015739085152745247, f1 0.878947377204895
ep29_l3_test_time 0.4092490100001669
Test Epoch29 layer4 Acc 0.8792105263157894, AUC 0.9555647969245911, avg_entr 0.014283026568591595, f1 0.8792105317115784
ep29_l4_test_time 0.5347487850001471
gc 0
Train Epoch30 Acc 0.974725 (116967/120000), AUC 0.9969339370727539
ep30_train_time 60.67698752699994
Test Epoch30 layer0 Acc 0.8813157894736842, AUC 0.9694007635116577, avg_entr 0.05260711535811424, f1 0.8813157677650452
ep30_l0_test_time 0.18959471400012262
Test Epoch30 layer1 Acc 0.878421052631579, AUC 0.9556110501289368, avg_entr 0.019679637625813484, f1 0.8784210681915283
ep30_l1_test_time 0.23583263000000443
Test Epoch30 layer2 Acc 0.8778947368421053, AUC 0.9605345726013184, avg_entr 0.015766242519021034, f1 0.8778947591781616
ep30_l2_test_time 0.3075969950000399
Test Epoch30 layer3 Acc 0.8773684210526316, AUC 0.9592336416244507, avg_entr 0.014833252876996994, f1 0.8773684501647949
ep30_l3_test_time 0.40866493299995454
Test Epoch30 layer4 Acc 0.8773684210526316, AUC 0.9575039148330688, avg_entr 0.013700965791940689, f1 0.8773684501647949
ep30_l4_test_time 0.5349423699999534
gc 0
Train Epoch31 Acc 0.9743916666666667 (116927/120000), AUC 0.9969956874847412
ep31_train_time 60.9170566979999
Test Epoch31 layer0 Acc 0.8807894736842106, AUC 0.9691616296768188, avg_entr 0.052430279552936554, f1 0.8807894587516785
ep31_l0_test_time 0.19051527800002077
Test Epoch31 layer1 Acc 0.8781578947368421, AUC 0.9556061029434204, avg_entr 0.019259173423051834, f1 0.878157913684845
ep31_l1_test_time 0.23823727800004235
Test Epoch31 layer2 Acc 0.8781578947368421, AUC 0.9616015553474426, avg_entr 0.015062928199768066, f1 0.878157913684845
ep31_l2_test_time 0.30801754200001596
Test Epoch31 layer3 Acc 0.8778947368421053, AUC 0.9605082869529724, avg_entr 0.014099569991230965, f1 0.8778947591781616
ep31_l3_test_time 0.4100095930000407
Test Epoch31 layer4 Acc 0.8778947368421053, AUC 0.9582139253616333, avg_entr 0.012864504009485245, f1 0.8778947591781616
ep31_l4_test_time 0.5373286060000737
gc 0
Train Epoch32 Acc 0.9739 (116868/120000), AUC 0.9969633221626282
ep32_train_time 60.816692728000135
Test Epoch32 layer0 Acc 0.8818421052631579, AUC 0.9692424535751343, avg_entr 0.052101608365774155, f1 0.8818420767784119
ep32_l0_test_time 0.18944403700015755
Test Epoch32 layer1 Acc 0.8789473684210526, AUC 0.9545912742614746, avg_entr 0.01891721971333027, f1 0.878947377204895
ep32_l1_test_time 0.23603998999988107
Test Epoch32 layer2 Acc 0.878421052631579, AUC 0.9600787162780762, avg_entr 0.014823743142187595, f1 0.8784210681915283
ep32_l2_test_time 0.30739687999994203
Test Epoch32 layer3 Acc 0.878421052631579, AUC 0.959078311920166, avg_entr 0.013999033719301224, f1 0.8784210681915283
ep32_l3_test_time 0.4082111980001173
Test Epoch32 layer4 Acc 0.878421052631579, AUC 0.9578016400337219, avg_entr 0.012965274974703789, f1 0.8784210681915283
ep32_l4_test_time 0.534381263999876
gc 0
Train Epoch33 Acc 0.9743666666666667 (116924/120000), AUC 0.9971537590026855
ep33_train_time 60.864670875999764
Test Epoch33 layer0 Acc 0.8802631578947369, AUC 0.9693442583084106, avg_entr 0.051642633974552155, f1 0.880263090133667
ep33_l0_test_time 0.18952961000013602
Test Epoch33 layer1 Acc 0.878421052631579, AUC 0.9549442529678345, avg_entr 0.019371699541807175, f1 0.8784210681915283
ep33_l1_test_time 0.23672536799995214
Test Epoch33 layer2 Acc 0.8781578947368421, AUC 0.9601632952690125, avg_entr 0.015398260205984116, f1 0.878157913684845
ep33_l2_test_time 0.30807439500040346
Test Epoch33 layer3 Acc 0.8781578947368421, AUC 0.9588024020195007, avg_entr 0.014341345988214016, f1 0.878157913684845
ep33_l3_test_time 0.4095911529998375
Test Epoch33 layer4 Acc 0.8781578947368421, AUC 0.9573719501495361, avg_entr 0.013036319985985756, f1 0.878157913684845
ep33_l4_test_time 0.5358276769998156
gc 0
Train Epoch34 Acc 0.9749333333333333 (116992/120000), AUC 0.9970065951347351
ep34_train_time 60.73446961599984
Test Epoch34 layer0 Acc 0.8818421052631579, AUC 0.9692203998565674, avg_entr 0.051526494324207306, f1 0.8818420767784119
ep34_l0_test_time 0.19251396999970893
Test Epoch34 layer1 Acc 0.878421052631579, AUC 0.9554102420806885, avg_entr 0.019065510481595993, f1 0.8784210681915283
ep34_l1_test_time 0.235988494999674
Test Epoch34 layer2 Acc 0.8781578947368421, AUC 0.9605798721313477, avg_entr 0.014930393546819687, f1 0.878157913684845
ep34_l2_test_time 0.30698561399958635
Test Epoch34 layer3 Acc 0.878421052631579, AUC 0.9589612483978271, avg_entr 0.014077268540859222, f1 0.8784210681915283
ep34_l3_test_time 0.40782639000008203
Test Epoch34 layer4 Acc 0.878421052631579, AUC 0.9577658176422119, avg_entr 0.012902142480015755, f1 0.8784210681915283
ep34_l4_test_time 0.5342960980001408
gc 0
Train Epoch35 Acc 0.9746916666666666 (116963/120000), AUC 0.9970842599868774
ep35_train_time 60.71817151899995
Test Epoch35 layer0 Acc 0.881578947368421, AUC 0.9692574739456177, avg_entr 0.05135664716362953, f1 0.8815789222717285
ep35_l0_test_time 0.19180015500023728
Test Epoch35 layer1 Acc 0.878421052631579, AUC 0.9554369449615479, avg_entr 0.01949271932244301, f1 0.8784210681915283
ep35_l1_test_time 0.2380291990002661
Test Epoch35 layer2 Acc 0.8781578947368421, AUC 0.9614018201828003, avg_entr 0.015448843128979206, f1 0.878157913684845
ep35_l2_test_time 0.3083951650000927
Test Epoch35 layer3 Acc 0.8778947368421053, AUC 0.9600355625152588, avg_entr 0.01453748531639576, f1 0.8778947591781616
ep35_l3_test_time 0.40977533899967966
Test Epoch35 layer4 Acc 0.8781578947368421, AUC 0.9572544097900391, avg_entr 0.013325662352144718, f1 0.878157913684845
ep35_l4_test_time 0.5362429680003515
gc 0
Train Epoch36 Acc 0.974925 (116991/120000), AUC 0.9971030950546265
ep36_train_time 61.04702520000001
Test Epoch36 layer0 Acc 0.8813157894736842, AUC 0.9693018198013306, avg_entr 0.050862718373537064, f1 0.8813157677650452
ep36_l0_test_time 0.1899446329998682
Test Epoch36 layer1 Acc 0.8792105263157894, AUC 0.9558234214782715, avg_entr 0.01914059929549694, f1 0.8792105317115784
ep36_l1_test_time 0.23700223999958325
Test Epoch36 layer2 Acc 0.8789473684210526, AUC 0.9610227346420288, avg_entr 0.015155047178268433, f1 0.878947377204895
ep36_l2_test_time 0.3080186830002276
Test Epoch36 layer3 Acc 0.8786842105263157, AUC 0.9591529369354248, avg_entr 0.014286206103861332, f1 0.8786842226982117
ep36_l3_test_time 0.40965674300014143
Test Epoch36 layer4 Acc 0.8786842105263157, AUC 0.9576754570007324, avg_entr 0.013132826425135136, f1 0.8786842226982117
ep36_l4_test_time 0.534710086999894
gc 0
Train Epoch37 Acc 0.97495 (116994/120000), AUC 0.9971649646759033
ep37_train_time 60.76286878200017
Test Epoch37 layer0 Acc 0.8821052631578947, AUC 0.9692368507385254, avg_entr 0.05095258727669716, f1 0.88210529088974
ep37_l0_test_time 0.19062630899998112
Test Epoch37 layer1 Acc 0.8786842105263157, AUC 0.9553928971290588, avg_entr 0.019303614273667336, f1 0.8786842226982117
ep37_l1_test_time 0.2364226869999584
Test Epoch37 layer2 Acc 0.8789473684210526, AUC 0.9606792330741882, avg_entr 0.01513657532632351, f1 0.878947377204895
ep37_l2_test_time 0.30738885399978244
Test Epoch37 layer3 Acc 0.8786842105263157, AUC 0.9590886831283569, avg_entr 0.01423320546746254, f1 0.8786842226982117
ep37_l3_test_time 0.4095943359998273
Test Epoch37 layer4 Acc 0.8786842105263157, AUC 0.9573496580123901, avg_entr 0.013075574301183224, f1 0.8786842226982117
ep37_l4_test_time 0.5344892480002272
gc 0
Train Epoch38 Acc 0.9746666666666667 (116960/120000), AUC 0.9971311688423157
ep38_train_time 60.672619611999835
Test Epoch38 layer0 Acc 0.8823684210526316, AUC 0.9692224264144897, avg_entr 0.05031706392765045, f1 0.8823684453964233
ep38_l0_test_time 0.1888530759997593
Test Epoch38 layer1 Acc 0.8786842105263157, AUC 0.9555420875549316, avg_entr 0.01897897943854332, f1 0.8786842226982117
ep38_l1_test_time 0.23680634099991948
Test Epoch38 layer2 Acc 0.8786842105263157, AUC 0.960564136505127, avg_entr 0.0150413503870368, f1 0.8786842226982117
ep38_l2_test_time 0.3077503310000793
Test Epoch38 layer3 Acc 0.8786842105263157, AUC 0.958625316619873, avg_entr 0.01407265942543745, f1 0.8786842226982117
ep38_l3_test_time 0.4079132300003039
Test Epoch38 layer4 Acc 0.8789473684210526, AUC 0.9562559127807617, avg_entr 0.012921334244310856, f1 0.878947377204895
ep38_l4_test_time 0.5365004739996948
gc 0
Train Epoch39 Acc 0.9746666666666667 (116960/120000), AUC 0.9971117377281189
ep39_train_time 60.634166341999844
Test Epoch39 layer0 Acc 0.8823684210526316, AUC 0.9692506790161133, avg_entr 0.05023059621453285, f1 0.8823684453964233
ep39_l0_test_time 0.18892432400025427
Test Epoch39 layer1 Acc 0.8792105263157894, AUC 0.9556995034217834, avg_entr 0.019122950732707977, f1 0.8792105317115784
ep39_l1_test_time 0.23646500399991055
Test Epoch39 layer2 Acc 0.8786842105263157, AUC 0.9612154960632324, avg_entr 0.015105661004781723, f1 0.8786842226982117
ep39_l2_test_time 0.307681354000124
Test Epoch39 layer3 Acc 0.878421052631579, AUC 0.9593659043312073, avg_entr 0.01423668209463358, f1 0.8784210681915283
ep39_l3_test_time 0.4090508750000481
Test Epoch39 layer4 Acc 0.8786842105263157, AUC 0.9578456282615662, avg_entr 0.013095173984766006, f1 0.8786842226982117
ep39_l4_test_time 0.5360539390003396
gc 0
Train Epoch40 Acc 0.9744833333333334 (116938/120000), AUC 0.9972981810569763
ep40_train_time 60.86611592400004
Test Epoch40 layer0 Acc 0.8826315789473684, AUC 0.9692506790161133, avg_entr 0.05002462863922119, f1 0.8826315999031067
ep40_l0_test_time 0.18961505300012504
Test Epoch40 layer1 Acc 0.88, AUC 0.9554827809333801, avg_entr 0.018867522478103638, f1 0.8799999952316284
ep40_l1_test_time 0.23650941200003217
Test Epoch40 layer2 Acc 0.8789473684210526, AUC 0.960929274559021, avg_entr 0.01485425140708685, f1 0.878947377204895
ep40_l2_test_time 0.30812153000033504
Test Epoch40 layer3 Acc 0.8792105263157894, AUC 0.959251344203949, avg_entr 0.013916220515966415, f1 0.8792105317115784
ep40_l3_test_time 0.4097329839996746
Test Epoch40 layer4 Acc 0.8792105263157894, AUC 0.9573490619659424, avg_entr 0.012756205163896084, f1 0.8792105317115784
ep40_l4_test_time 0.5364839909998409
gc 0
Train Epoch41 Acc 0.9749833333333333 (116998/120000), AUC 0.9972344636917114
ep41_train_time 60.67016058599984
Test Epoch41 layer0 Acc 0.8818421052631579, AUC 0.9692099094390869, avg_entr 0.049921829253435135, f1 0.8818420767784119
ep41_l0_test_time 0.18988733600008345
Test Epoch41 layer1 Acc 0.8792105263157894, AUC 0.9557124376296997, avg_entr 0.018821951001882553, f1 0.8792105317115784
ep41_l1_test_time 0.24518530499972258
Test Epoch41 layer2 Acc 0.8792105263157894, AUC 0.9614553451538086, avg_entr 0.01485360972583294, f1 0.8792105317115784
ep41_l2_test_time 0.30809678199966584
Test Epoch41 layer3 Acc 0.8792105263157894, AUC 0.9600083827972412, avg_entr 0.014017879031598568, f1 0.8792105317115784
ep41_l3_test_time 0.40919918100007635
Test Epoch41 layer4 Acc 0.8792105263157894, AUC 0.9584041237831116, avg_entr 0.012926950119435787, f1 0.8792105317115784
ep41_l4_test_time 0.5351184559999638
gc 0
Train Epoch42 Acc 0.9750083333333334 (117001/120000), AUC 0.9971739053726196
ep42_train_time 60.868328654999914
Test Epoch42 layer0 Acc 0.8818421052631579, AUC 0.9692284464836121, avg_entr 0.04966958239674568, f1 0.8818420767784119
ep42_l0_test_time 0.1910500150002008
Test Epoch42 layer1 Acc 0.8794736842105263, AUC 0.9552704095840454, avg_entr 0.018686532974243164, f1 0.8794736862182617
ep42_l1_test_time 0.23684328599983928
Test Epoch42 layer2 Acc 0.8786842105263157, AUC 0.9605553150177002, avg_entr 0.014771388843655586, f1 0.8786842226982117
ep42_l2_test_time 0.3082041509996998
Test Epoch42 layer3 Acc 0.8789473684210526, AUC 0.9588877558708191, avg_entr 0.01387122180312872, f1 0.878947377204895
ep42_l3_test_time 0.40981263699995907
Test Epoch42 layer4 Acc 0.8789473684210526, AUC 0.9571885466575623, avg_entr 0.012730047106742859, f1 0.878947377204895
ep42_l4_test_time 0.5345943349998379
gc 0
Train Epoch43 Acc 0.975125 (117015/120000), AUC 0.9973326921463013
ep43_train_time 60.98157166300007
Test Epoch43 layer0 Acc 0.8821052631578947, AUC 0.9691863059997559, avg_entr 0.04948768764734268, f1 0.88210529088974
ep43_l0_test_time 0.18922243199995137
Test Epoch43 layer1 Acc 0.8794736842105263, AUC 0.9552196264266968, avg_entr 0.018542353063821793, f1 0.8794736862182617
ep43_l1_test_time 0.23656845200002863
Test Epoch43 layer2 Acc 0.8786842105263157, AUC 0.9603064060211182, avg_entr 0.014581494964659214, f1 0.8786842226982117
ep43_l2_test_time 0.30698470299967084
Test Epoch43 layer3 Acc 0.8789473684210526, AUC 0.9585810303688049, avg_entr 0.013664375059306622, f1 0.878947377204895
ep43_l3_test_time 0.40862935199993444
Test Epoch43 layer4 Acc 0.8789473684210526, AUC 0.9563470482826233, avg_entr 0.012515570037066936, f1 0.878947377204895
ep43_l4_test_time 0.534193508000044
gc 0
Train Epoch44 Acc 0.9753833333333334 (117046/120000), AUC 0.9972215890884399
ep44_train_time 60.88307376400007
Test Epoch44 layer0 Acc 0.8823684210526316, AUC 0.9692159295082092, avg_entr 0.04940938577055931, f1 0.8823684453964233
ep44_l0_test_time 0.18895406199999343
Test Epoch44 layer1 Acc 0.8792105263157894, AUC 0.9552960991859436, avg_entr 0.018717246130108833, f1 0.8792105317115784
ep44_l1_test_time 0.2356747369999539
Test Epoch44 layer2 Acc 0.878421052631579, AUC 0.9604155421257019, avg_entr 0.014753987081348896, f1 0.8784210681915283
ep44_l2_test_time 0.30678622699997504
Test Epoch44 layer3 Acc 0.8786842105263157, AUC 0.958919882774353, avg_entr 0.013851040974259377, f1 0.8786842226982117
ep44_l3_test_time 0.40744463100008943
Test Epoch44 layer4 Acc 0.8786842105263157, AUC 0.9568167328834534, avg_entr 0.012684639543294907, f1 0.8786842226982117
ep44_l4_test_time 0.5340637939998487
gc 0
Train Epoch45 Acc 0.9752666666666666 (117032/120000), AUC 0.9971590042114258
ep45_train_time 60.75266916700002
Test Epoch45 layer0 Acc 0.8821052631578947, AUC 0.9691684246063232, avg_entr 0.04936467111110687, f1 0.88210529088974
ep45_l0_test_time 0.19000604199982263
Test Epoch45 layer1 Acc 0.8797368421052632, AUC 0.955313503742218, avg_entr 0.01858913153409958, f1 0.8797368407249451
ep45_l1_test_time 0.23645971899986762
Test Epoch45 layer2 Acc 0.8792105263157894, AUC 0.9606920480728149, avg_entr 0.014631365425884724, f1 0.8792105317115784
ep45_l2_test_time 0.3082905749997735
Test Epoch45 layer3 Acc 0.8794736842105263, AUC 0.9590362310409546, avg_entr 0.013756824657320976, f1 0.8794736862182617
ep45_l3_test_time 0.40887049100001605
Test Epoch45 layer4 Acc 0.8794736842105263, AUC 0.9575514793395996, avg_entr 0.012624193914234638, f1 0.8794736862182617
ep45_l4_test_time 0.53451289800023
gc 0
Train Epoch46 Acc 0.9749333333333333 (116992/120000), AUC 0.9971861839294434
ep46_train_time 60.79974767199974
Test Epoch46 layer0 Acc 0.8823684210526316, AUC 0.9692036509513855, avg_entr 0.04921009764075279, f1 0.8823684453964233
ep46_l0_test_time 0.18895965899992007
Test Epoch46 layer1 Acc 0.8789473684210526, AUC 0.955053448677063, avg_entr 0.018526440486311913, f1 0.878947377204895
ep46_l1_test_time 0.23691795100012314
Test Epoch46 layer2 Acc 0.8786842105263157, AUC 0.9603757262229919, avg_entr 0.014643238857388496, f1 0.8786842226982117
ep46_l2_test_time 0.3077936609997778
Test Epoch46 layer3 Acc 0.8786842105263157, AUC 0.958673894405365, avg_entr 0.013763591647148132, f1 0.8786842226982117
ep46_l3_test_time 0.40805897099971844
Test Epoch46 layer4 Acc 0.8789473684210526, AUC 0.9561547040939331, avg_entr 0.01260521449148655, f1 0.878947377204895
ep46_l4_test_time 0.535130178000145
gc 0
Train Epoch47 Acc 0.9755916666666666 (117071/120000), AUC 0.997315526008606
ep47_train_time 60.71849826200014
Test Epoch47 layer0 Acc 0.8818421052631579, AUC 0.9691891670227051, avg_entr 0.04914555326104164, f1 0.8818420767784119
ep47_l0_test_time 0.1919829819998995
Test Epoch47 layer1 Acc 0.8797368421052632, AUC 0.9551538228988647, avg_entr 0.018466200679540634, f1 0.8797368407249451
ep47_l1_test_time 0.2375133650002681
Test Epoch47 layer2 Acc 0.8789473684210526, AUC 0.9604644179344177, avg_entr 0.014538846909999847, f1 0.878947377204895
ep47_l2_test_time 0.30791384399981325
Test Epoch47 layer3 Acc 0.8789473684210526, AUC 0.9588663578033447, avg_entr 0.013661136850714684, f1 0.878947377204895
ep47_l3_test_time 0.4083311610002056
Test Epoch47 layer4 Acc 0.8789473684210526, AUC 0.9567074775695801, avg_entr 0.012543030083179474, f1 0.878947377204895
ep47_l4_test_time 0.53568745199982
gc 0
Train Epoch48 Acc 0.9748916666666667 (116987/120000), AUC 0.9971721172332764
ep48_train_time 60.80699324599982
Test Epoch48 layer0 Acc 0.8826315789473684, AUC 0.9692133069038391, avg_entr 0.04914926737546921, f1 0.8826315999031067
ep48_l0_test_time 0.1891585789999226
Test Epoch48 layer1 Acc 0.8789473684210526, AUC 0.9552209377288818, avg_entr 0.01847437024116516, f1 0.878947377204895
ep48_l1_test_time 0.23690561399962462
Test Epoch48 layer2 Acc 0.8786842105263157, AUC 0.9605453610420227, avg_entr 0.014578009024262428, f1 0.8786842226982117
ep48_l2_test_time 0.30794171099978485
Test Epoch48 layer3 Acc 0.8786842105263157, AUC 0.9588231444358826, avg_entr 0.01371266320347786, f1 0.8786842226982117
ep48_l3_test_time 0.4093769080000129
Test Epoch48 layer4 Acc 0.8786842105263157, AUC 0.956632137298584, avg_entr 0.012592429295182228, f1 0.8786842226982117
ep48_l4_test_time 0.5352722990000984
gc 0
Train Epoch49 Acc 0.9748166666666667 (116978/120000), AUC 0.9971938133239746
ep49_train_time 60.89951728599999
Test Epoch49 layer0 Acc 0.8823684210526316, AUC 0.9692124724388123, avg_entr 0.049131106585264206, f1 0.8823684453964233
ep49_l0_test_time 0.18961365199993452
Test Epoch49 layer1 Acc 0.8789473684210526, AUC 0.9552446603775024, avg_entr 0.018497979268431664, f1 0.878947377204895
ep49_l1_test_time 0.23677902999997968
Test Epoch49 layer2 Acc 0.8786842105263157, AUC 0.9606614112854004, avg_entr 0.014628715813159943, f1 0.8786842226982117
ep49_l2_test_time 0.3086298700000043
Test Epoch49 layer3 Acc 0.8786842105263157, AUC 0.9589145183563232, avg_entr 0.013767226599156857, f1 0.8786842226982117
ep49_l3_test_time 0.4086851199999728
Test Epoch49 layer4 Acc 0.8789473684210526, AUC 0.956762433052063, avg_entr 0.012636824510991573, f1 0.878947377204895
ep49_l4_test_time 0.5353256990001682
Best AUC tensor(0.8855) 9 0
train_as_loss [[4.53437860e+02 3.56648623e+02 3.51118253e+02 3.49864940e+02
  3.49385879e+02 3.49154175e+02 3.49025933e+02 3.48948527e+02
  3.48898935e+02 3.48865767e+02 3.48847432e+02 3.48836968e+02
  3.48827504e+02 3.48819154e+02 3.48811948e+02 3.48807196e+02
  3.48804116e+02 3.48801087e+02 3.48798188e+02 3.48796091e+02
  3.48794679e+02 3.48793175e+02 3.48791686e+02 3.48790591e+02
  3.48789795e+02 3.48788951e+02 3.48788083e+02 3.48787374e+02
  3.48786988e+02 3.48786423e+02 3.48785900e+02 3.48785487e+02
  3.48785290e+02 3.48784864e+02 3.48784580e+02 3.48784410e+02
  3.48784158e+02 3.48783903e+02 3.48783729e+02 3.48783670e+02
  3.48783631e+02 3.48783507e+02 3.48783421e+02 3.48783365e+02
  3.48783133e+02 3.48783069e+02 3.48783034e+02 3.48783009e+02
  3.48783008e+02 3.48782953e+02]
 [2.00379030e+00 1.82687652e-05 1.43046333e-06 2.95317502e-07
  9.00754773e-08 2.27745310e-07 2.20128581e-06 6.31920582e-06
  4.86794687e-05 1.07106281e-04 5.05024790e-09 1.03770207e-09
  9.19305085e-10 4.17921945e-05 2.69048034e-06 5.02315975e-10
  8.24029358e-10 2.27002319e-05 1.10083014e-07 2.70853119e-10
  2.10979539e-10 5.47460493e-06 1.35694608e-09 1.79989441e-10
  1.48229327e-08 2.38867992e-06 1.77328260e-07 1.27515825e-10
  1.25938127e-10 7.31920268e-07 2.44420766e-10 1.00410615e-10
  9.28761377e-11 6.04269044e-08 1.47350533e-09 7.39768369e-11
  8.73120784e-11 2.31865004e-08 3.30286329e-10 5.04375940e-11
  5.57513766e-11 7.47443446e-11 5.24079078e-11 3.33901040e-11
  3.58938138e-11 4.25937247e-11 2.92106943e-11 1.97770832e-11
  2.02077818e-11 2.11477280e-11]
 [1.72902584e+00 1.26302009e-05 1.11768183e-06 2.50001799e-07
  8.21426854e-08 5.27993877e-08 2.14341522e-06 1.33549191e-05
  1.30864315e-04 4.01811969e-04 1.56562529e-08 2.22916734e-09
  2.97216027e-09 1.22038021e-04 4.83547138e-06 1.90171092e-09
  2.37545744e-09 5.48849094e-05 2.24645924e-07 1.05605285e-09
  9.22458453e-10 1.25767279e-05 2.87363142e-09 7.48531437e-10
  1.58830234e-08 5.12074196e-06 4.29346087e-07 4.57483041e-10
  5.28689791e-10 1.69553261e-06 5.46253562e-10 3.23540139e-10
  3.53617868e-10 8.65311087e-08 4.04400197e-09 2.89235166e-10
  4.04588577e-10 4.40229882e-08 1.07447919e-09 2.04391023e-10
  2.64899074e-10 4.14312820e-10 1.81880116e-10 1.52345121e-10
  1.79988001e-10 2.44837775e-10 9.30043196e-11 8.11966626e-11
  9.38877577e-11 9.92641357e-11]
 [2.11686441e+00 2.09330682e-05 2.02723165e-06 4.87061638e-07
  1.71344249e-07 1.05450039e-07 1.60464754e-06 1.92199843e-05
  1.71893872e-04 7.22688429e-04 5.02513275e-08 8.56274791e-09
  1.35840820e-08 1.94104541e-04 6.28519853e-06 7.43806638e-09
  7.90733696e-09 8.12178941e-05 4.17163544e-07 4.56079447e-09
  4.46899988e-09 1.99090932e-05 6.85132825e-09 3.16741644e-09
  2.06345850e-08 7.32181688e-06 9.02472988e-07 1.90953152e-09
  2.42344041e-09 2.77475503e-06 1.61352828e-09 1.22149542e-09
  1.42710972e-09 1.11908906e-07 7.35294062e-09 1.20566262e-09
  1.70995936e-09 6.87308116e-08 3.13844593e-09 8.20022007e-10
  1.17444461e-09 1.96741854e-09 6.99793807e-10 5.89613860e-10
  7.77461885e-10 1.13375202e-09 3.07574158e-10 2.92471555e-10
  3.57579987e-10 4.15033854e-10]
 [1.93565215e+00 1.90867576e-05 2.22803818e-06 7.23442879e-07
  3.20311859e-07 2.35890730e-07 1.93798258e-06 2.62934229e-05
  2.21198800e-04 1.12954607e-03 4.54218759e-07 6.14127240e-08
  6.06524288e-08 2.72744719e-04 5.13125389e-05 6.59861398e-08
  2.84098707e-08 1.06808623e-04 3.14174217e-06 4.17319868e-08
  1.69368430e-08 2.62641953e-05 5.61690531e-08 3.21830823e-08
  3.10679689e-08 9.76520670e-06 6.24990445e-06 1.80025267e-08
  6.22473921e-09 3.60193048e-06 1.04841452e-08 9.38971579e-09
  3.95354273e-09 1.28105844e-07 5.68080352e-08 1.03957847e-08
  5.19607718e-09 8.69658297e-08 2.57837529e-08 7.16863581e-09
  3.22585344e-09 5.70809694e-09 4.98008405e-09 4.60561358e-09
  2.06517588e-09 2.94891610e-09 1.88918131e-09 1.89381959e-09
  8.21341776e-10 9.89073206e-10]]
train_ae_loss [[12.57991016 12.07818731 12.15029703 11.68531117 11.28162085 11.00953588
  10.74206268 10.49478164 10.29294442 10.11709511  9.35314657  9.10480417
   8.98993961  8.93479566  8.80482588  8.35996811  8.26541846  8.22582459
   8.18507271  7.94551241  7.91484324  7.9452139   7.92143386  7.82511018
   7.83281484  7.87265538  7.86067139  7.82781032  7.82658684  7.86886143
   7.90290042  7.91424685  7.92449246  7.96641774  7.97194122  7.99336499
   8.00732005  8.02620741  8.05861087  8.05449555  8.0800211   8.08587303
   8.11220023  8.1197634   8.09538167  8.15725519  8.15586897  8.1736204
   8.1745234   8.15508808]
 [13.74922416 11.58295148 10.32037331  9.71234412  9.1580986   8.72793333
   8.26281743  7.73383606  7.346063    6.96032809  5.95986185  5.57457301
   5.37643526  5.2426279   4.89071776  4.373619    4.28036353  4.1539815
   4.00808864  3.71091381  3.65409214  3.65009901  3.52404171  3.4066865
   3.39063532  3.40068136  3.3764916   3.29101473  3.26496646  3.29720535
   3.29175166  3.24142493  3.21332829  3.27077311  3.23568944  3.25248031
   3.25284144  3.21420763  3.25479818  3.26236729  3.26353396  3.2436453
   3.27384605  3.26043968  3.24851082  3.26053968  3.27551725  3.29822094
   3.28730691  3.26088694]
 [15.83759885 12.25036639 10.45778834  9.76858881  9.13525735  8.65127086
   8.12085897  7.51881689  7.07605014  6.67044167  5.62418354  5.23472903
   5.03288175  4.90650812  4.51443207  4.00800492  3.91900074  3.79284169
   3.6356683   3.3397715   3.27963173  3.27791307  3.14041191  3.0273235
   3.00874859  3.01027079  2.98433101  2.89939475  2.87120638  2.90257877
   2.88675749  2.83472096  2.80503236  2.85767261  2.81799004  2.83584843
   2.82697136  2.78854117  2.8223188   2.83488641  2.83508817  2.81532497
   2.83745097  2.82516796  2.81321893  2.81771237  2.83253021  2.8540794
   2.84833698  2.82291766]
 [16.53275466 12.37725978  9.88339046  9.15673784  8.49636858  7.98168164
   7.40951151  6.82920855  6.41705895  6.03708014  5.04244681  4.69417599
   4.50968009  4.40499019  4.01481425  3.56052999  3.48423711  3.37043582
   3.21261186  2.94922726  2.89528058  2.89283249  2.761185    2.66008522
   2.64360484  2.64154257  2.61445083  2.53770206  2.51280649  2.53828217
   2.52177707  2.47435911  2.44728954  2.49172271  2.45558893  2.47019433
   2.46079838  2.4260452   2.45611198  2.46662344  2.46553982  2.44739141
   2.46535846  2.45602838  2.44608736  2.44807671  2.4626126   2.47796246
   2.47472253  2.4512179 ]
 [17.21717012 13.67081903  9.94148381  9.1745634   8.50600002  8.04793422
   7.48234117  6.83534664  6.35828201  5.93966622  5.0137062   4.66239953
   4.35270868  4.25064539  3.86960252  3.44913119  3.32985779  3.22572028
   3.07226796  2.82749114  2.75291153  2.75120339  2.62442979  2.52550597
   2.50397073  2.50013242  2.47368307  2.39888997  2.37346233  2.39465812
   2.37683935  2.33316797  2.30418742  2.34717099  2.30916824  2.32439048
   2.31643824  2.27950849  2.30918463  2.31839865  2.31865969  2.30182876
   2.31536545  2.30501257  2.29539429  2.29927059  2.31020928  2.32539957
   2.32159602  2.30048801]]
valid_acc (5, 50)
valid_AUC (5, 50)
train_acc (50,)
total_train+valid_time 3132.6868473649997
Start Testing
Load ckpt at ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt
gc 9
Test layer0 Acc 0.88, AUC 0.9673855900764465, avg_entr 0.0871489867568016, f1 0.8799999952316284
l0_test_time 0.19820319900009054
gc 0
Test layer1 Acc 0.8818421052631579, AUC 0.9683424234390259, avg_entr 0.04527090862393379, f1 0.8818420767784119
l1_test_time 0.2370432359998631
gc 0
Test layer2 Acc 0.8821052631578947, AUC 0.9687578678131104, avg_entr 0.04019307345151901, f1 0.88210529088974
l2_test_time 0.30850302699991516
gc 0
Test layer3 Acc 0.8821052631578947, AUC 0.9674755930900574, avg_entr 0.03797721117734909, f1 0.88210529088974
l3_test_time 0.4092331510000804
gc 0
Test layer4 Acc 0.8818421052631579, AUC 0.9628610610961914, avg_entr 0.035150788724422455, f1 0.8818420767784119
l4_test_time 0.535195924999698
gc 0
Test threshold 0.1 Acc 0.8818421052631579, AUC 0.9652467966079712, avg_entr 0.03356211259961128, f1 0.8818420767784119
t0.1_test_time 0.2621807099999387
gc 0
Test threshold 0.2 Acc 0.8818421052631579, AUC 0.9655965566635132, avg_entr 0.03962685167789459, f1 0.8818420767784119
t0.2_test_time 0.250846034999995
gc 0
Test threshold 0.3 Acc 0.8821052631578947, AUC 0.9665983319282532, avg_entr 0.045999277383089066, f1 0.88210529088974
t0.3_test_time 0.24338688499983618
gc 0
Test threshold 0.4 Acc 0.8821052631578947, AUC 0.967257559299469, avg_entr 0.05567270517349243, f1 0.88210529088974
t0.4_test_time 0.23545869999998104
gc 0
Test threshold 0.5 Acc 0.8818421052631579, AUC 0.9674166440963745, avg_entr 0.061031926423311234, f1 0.8818420767784119
t0.5_test_time 0.21127487300009307
gc 0
Test threshold 0.6 Acc 0.88, AUC 0.9673855900764465, avg_entr 0.06286469846963882, f1 0.8799999952316284
t0.6_test_time 0.20188422000001083
gc 0
Test threshold 0.7 Acc 0.88, AUC 0.9673855900764465, avg_entr 0.06286469846963882, f1 0.8799999952316284
t0.7_test_time 0.20226787000001423
gc 0
Test threshold 0.8 Acc 0.88, AUC 0.9673855900764465, avg_entr 0.06286469846963882, f1 0.8799999952316284
t0.8_test_time 0.20221523700001853
gc 0
Test threshold 0.9 Acc 0.88, AUC 0.9673855900764465, avg_entr 0.06286469846963882, f1 0.8799999952316284
t0.9_test_time 0.20299478500010082
