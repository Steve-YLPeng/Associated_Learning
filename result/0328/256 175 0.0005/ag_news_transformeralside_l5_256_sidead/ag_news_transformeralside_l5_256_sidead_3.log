total count words 102019
vocab size 30000
train size 120000, valid size 3800, test size 3800
found 26754 words in glove
model: TransformerALsideText(
  (layers): ModuleList(
    (0): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=4, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=4, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
  (emb_layers): ModuleList(
    (0): Embedding(30000, 300)
    (1): Embedding(30000, 300)
    (2): Embedding(30000, 300)
    (3): Embedding(30000, 300)
    (4): Embedding(30000, 300)
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.multi_headed_attention.linears.0.weight 90000
layers.0.enc.f.multi_headed_attention.linears.0.bias 300
layers.0.enc.f.multi_headed_attention.linears.1.weight 90000
layers.0.enc.f.multi_headed_attention.linears.1.bias 300
layers.0.enc.f.multi_headed_attention.linears.2.weight 90000
layers.0.enc.f.multi_headed_attention.linears.2.bias 300
layers.0.enc.f.multi_headed_attention.linears.3.weight 90000
layers.0.enc.f.multi_headed_attention.linears.3.bias 300
layers.0.enc.f.feed_forward.w_1.weight 90000
layers.0.enc.f.feed_forward.w_1.bias 300
layers.0.enc.f.feed_forward.w_2.weight 90000
layers.0.enc.f.feed_forward.w_2.bias 300
layers.0.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.0.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.0.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.0.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.0.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.0.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.0.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.0.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.0.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.0.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.0.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.0.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.0.enc.f.encoder.norm.a 300
layers.0.enc.f.encoder.norm.b 300
layers.0.ae.g.0.weight 512
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 512
layers.0.ae.h.0.bias 4
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
emb_layers.0.weight 9000000
emb_layers.1.weight 9000000
emb_layers.2.weight 9000000
emb_layers.3.weight 9000000
emb_layers.4.weight 9000000
Total Trainable Params: 50758892
init_time 15.765690281000001
Start Training
gc 0
Train Epoch0 Acc 0.28224166666666667 (33869/120000), AUC 0.52420973777771
ep0_train_time 61.01324131199999
Test Epoch0 layer0 Acc 0.8, AUC 0.9363651275634766, avg_entr 0.7576897740364075, f1 0.8000000715255737
ep0_l0_test_time 0.19548759599999244
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 0
Test Epoch0 layer1 Acc 0.7739473684210526, AUC 0.9364930987358093, avg_entr 0.7501228451728821, f1 0.7739473581314087
ep0_l1_test_time 0.23753200400000196
Test Epoch0 layer2 Acc 0.7381578947368421, AUC 0.9371078014373779, avg_entr 0.7741804718971252, f1 0.7381578683853149
ep0_l2_test_time 0.30847748399999375
Test Epoch0 layer3 Acc 0.765, AUC 0.9335588812828064, avg_entr 0.9189814329147339, f1 0.7649999856948853
ep0_l3_test_time 0.4087981829999876
Test Epoch0 layer4 Acc 0.671578947368421, AUC 0.9116154909133911, avg_entr 1.1494128704071045, f1 0.6715789437294006
ep0_l4_test_time 0.5364679249999966
gc 0
Train Epoch1 Acc 0.7668 (92016/120000), AUC 0.9192973971366882
ep1_train_time 60.883168427
Test Epoch1 layer0 Acc 0.845, AUC 0.9581215977668762, avg_entr 0.3863132894039154, f1 0.8449999690055847
ep1_l0_test_time 0.18953187599998955
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 1
Test Epoch1 layer1 Acc 0.8478947368421053, AUC 0.9602718949317932, avg_entr 0.3105210065841675, f1 0.8478947281837463
ep1_l1_test_time 0.23845601099998248
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 1
Test Epoch1 layer2 Acc 0.8473684210526315, AUC 0.9606949090957642, avg_entr 0.31166571378707886, f1 0.8473684191703796
ep1_l2_test_time 0.308813157000003
Test Epoch1 layer3 Acc 0.8431578947368421, AUC 0.9609774351119995, avg_entr 0.3117821514606476, f1 0.8431578874588013
ep1_l3_test_time 0.4095377360000043
Test Epoch1 layer4 Acc 0.8386842105263158, AUC 0.9608460068702698, avg_entr 0.3187016546726227, f1 0.8386842012405396
ep1_l4_test_time 0.5366045689999908
gc 0
Train Epoch2 Acc 0.8611416666666667 (103337/120000), AUC 0.961227536201477
ep2_train_time 60.719071351999986
Test Epoch2 layer0 Acc 0.8644736842105263, AUC 0.9647740125656128, avg_entr 0.24533072113990784, f1 0.8644736409187317
ep2_l0_test_time 0.19000462599998968
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 2
Test Epoch2 layer1 Acc 0.8660526315789474, AUC 0.9657809734344482, avg_entr 0.17189161479473114, f1 0.8660526275634766
ep2_l1_test_time 0.2367319699999939
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 2
Test Epoch2 layer2 Acc 0.8642105263157894, AUC 0.965546190738678, avg_entr 0.16591887176036835, f1 0.8642105460166931
ep2_l2_test_time 0.308151166000016
Test Epoch2 layer3 Acc 0.8652631578947368, AUC 0.9664312601089478, avg_entr 0.16153597831726074, f1 0.8652631640434265
ep2_l3_test_time 0.4090376929999877
Test Epoch2 layer4 Acc 0.865, AUC 0.9666454195976257, avg_entr 0.15995952486991882, f1 0.8650000095367432
ep2_l4_test_time 0.535878729999979
gc 0
Train Epoch3 Acc 0.8834416666666667 (106013/120000), AUC 0.969238817691803
ep3_train_time 60.879810366000015
Test Epoch3 layer0 Acc 0.8692105263157894, AUC 0.9680404663085938, avg_entr 0.17931751906871796, f1 0.8692105412483215
ep3_l0_test_time 0.19075158999999076
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 3
Test Epoch3 layer1 Acc 0.8668421052631579, AUC 0.9683100581169128, avg_entr 0.11472109705209732, f1 0.8668420910835266
ep3_l1_test_time 0.2371082639999713
Test Epoch3 layer2 Acc 0.8671052631578947, AUC 0.968683123588562, avg_entr 0.11141844838857651, f1 0.86710524559021
ep3_l2_test_time 0.30908446400002276
Test Epoch3 layer3 Acc 0.8678947368421053, AUC 0.968970775604248, avg_entr 0.10953445732593536, f1 0.86789470911026
ep3_l3_test_time 0.4092623259999755
Test Epoch3 layer4 Acc 0.8681578947368421, AUC 0.9679446220397949, avg_entr 0.10189687460660934, f1 0.8681579232215881
ep3_l4_test_time 0.5352628279999863
gc 0
Train Epoch4 Acc 0.8958416666666666 (107501/120000), AUC 0.9739298820495605
ep4_train_time 60.78057328900002
Test Epoch4 layer0 Acc 0.866578947368421, AUC 0.9694773554801941, avg_entr 0.15661559998989105, f1 0.8665789365768433
ep4_l0_test_time 0.1984723759999838
Test Epoch4 layer1 Acc 0.8694736842105263, AUC 0.9708284139633179, avg_entr 0.10121225565671921, f1 0.8694736957550049
ep4_l1_test_time 0.2374608970000054
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 4
Test Epoch4 layer2 Acc 0.8676315789473684, AUC 0.9703599214553833, avg_entr 0.10005652904510498, f1 0.8676315546035767
ep4_l2_test_time 0.30885572900001534
Test Epoch4 layer3 Acc 0.8671052631578947, AUC 0.9696457386016846, avg_entr 0.09613318741321564, f1 0.86710524559021
ep4_l3_test_time 0.4091669010000487
Test Epoch4 layer4 Acc 0.8660526315789474, AUC 0.9677420258522034, avg_entr 0.08637029677629471, f1 0.8660526275634766
ep4_l4_test_time 0.5347714079999832
gc 0
Train Epoch5 Acc 0.9057583333333333 (108691/120000), AUC 0.9766385555267334
ep5_train_time 60.69396005499999
Test Epoch5 layer0 Acc 0.8686842105263158, AUC 0.9708300828933716, avg_entr 0.12485729157924652, f1 0.8686841726303101
ep5_l0_test_time 0.18932688899997174
Test Epoch5 layer1 Acc 0.8692105263157894, AUC 0.9708492159843445, avg_entr 0.07903517782688141, f1 0.8692105412483215
ep5_l1_test_time 0.23626717599995573
Test Epoch5 layer2 Acc 0.8710526315789474, AUC 0.9721537232398987, avg_entr 0.0740807056427002, f1 0.871052622795105
ep5_l2_test_time 0.3080585460000407
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 5
Test Epoch5 layer3 Acc 0.8702631578947368, AUC 0.9709166288375854, avg_entr 0.06819408386945724, f1 0.8702631592750549
ep5_l3_test_time 0.4083155760000068
Test Epoch5 layer4 Acc 0.8686842105263158, AUC 0.9692469835281372, avg_entr 0.0643395483493805, f1 0.8686841726303101
ep5_l4_test_time 0.5336659600000075
gc 0
Train Epoch6 Acc 0.91445 (109734/120000), AUC 0.9797152280807495
ep6_train_time 60.638935597
Test Epoch6 layer0 Acc 0.876578947368421, AUC 0.9722101092338562, avg_entr 0.11299528181552887, f1 0.8765789270401001
ep6_l0_test_time 0.18921312100002297
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 6
Test Epoch6 layer1 Acc 0.8773684210526316, AUC 0.9721388816833496, avg_entr 0.06683513522148132, f1 0.8773684501647949
ep6_l1_test_time 0.23574178100000154
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 6
Test Epoch6 layer2 Acc 0.8763157894736842, AUC 0.9719451069831848, avg_entr 0.06024239957332611, f1 0.8763157725334167
ep6_l2_test_time 0.3100714380000227
Test Epoch6 layer3 Acc 0.8742105263157894, AUC 0.9711381793022156, avg_entr 0.05408475548028946, f1 0.87421053647995
ep6_l3_test_time 0.4121677679999607
Test Epoch6 layer4 Acc 0.8728947368421053, AUC 0.9691282510757446, avg_entr 0.04924523085355759, f1 0.8728947639465332
ep6_l4_test_time 0.5360763780000184
gc 0
Train Epoch7 Acc 0.92215 (110658/120000), AUC 0.9817548990249634
ep7_train_time 60.65720622900005
Test Epoch7 layer0 Acc 0.8818421052631579, AUC 0.97175532579422, avg_entr 0.09925101697444916, f1 0.8818420767784119
ep7_l0_test_time 0.18982225399997787
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 7
Test Epoch7 layer1 Acc 0.8831578947368421, AUC 0.9722051620483398, avg_entr 0.054529692977666855, f1 0.8831579089164734
ep7_l1_test_time 0.23707232100002784
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 7
Test Epoch7 layer2 Acc 0.8821052631578947, AUC 0.9714904427528381, avg_entr 0.04692624509334564, f1 0.88210529088974
ep7_l2_test_time 0.30758116399999835
Test Epoch7 layer3 Acc 0.8826315789473684, AUC 0.970504641532898, avg_entr 0.04217173531651497, f1 0.8826315999031067
ep7_l3_test_time 0.4074622120000413
Test Epoch7 layer4 Acc 0.8836842105263157, AUC 0.9684312343597412, avg_entr 0.0360550656914711, f1 0.8836842179298401
ep7_l4_test_time 0.5341070000000627
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 7
gc 0
Train Epoch8 Acc 0.9277416666666667 (111329/120000), AUC 0.9832942485809326
ep8_train_time 60.837505207999925
Test Epoch8 layer0 Acc 0.8807894736842106, AUC 0.9728289842605591, avg_entr 0.09321905672550201, f1 0.8807894587516785
ep8_l0_test_time 0.19009627200000523
Test Epoch8 layer1 Acc 0.8826315789473684, AUC 0.972381591796875, avg_entr 0.04890253022313118, f1 0.8826315999031067
ep8_l1_test_time 0.23698776000003363
Test Epoch8 layer2 Acc 0.883421052631579, AUC 0.9717466831207275, avg_entr 0.04170344024896622, f1 0.8834210634231567
ep8_l2_test_time 0.30733219700005066
Test Epoch8 layer3 Acc 0.8844736842105263, AUC 0.9708675742149353, avg_entr 0.036697663366794586, f1 0.8844736814498901
ep8_l3_test_time 0.4110579120000466
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 8
Test Epoch8 layer4 Acc 0.8836842105263157, AUC 0.9683496952056885, avg_entr 0.03191397711634636, f1 0.8836842179298401
ep8_l4_test_time 0.5340839110000388
gc 0
Train Epoch9 Acc 0.9343833333333333 (112126/120000), AUC 0.9855851531028748
ep9_train_time 60.958170058000064
Test Epoch9 layer0 Acc 0.881578947368421, AUC 0.9725227952003479, avg_entr 0.08615780621767044, f1 0.8815789222717285
ep9_l0_test_time 0.18958256400003393
Test Epoch9 layer1 Acc 0.8818421052631579, AUC 0.9694516658782959, avg_entr 0.04505452513694763, f1 0.8818420767784119
ep9_l1_test_time 0.2361844069999961
Test Epoch9 layer2 Acc 0.8810526315789474, AUC 0.9695281982421875, avg_entr 0.03856321796774864, f1 0.8810526132583618
ep9_l2_test_time 0.3075223029999279
Test Epoch9 layer3 Acc 0.8805263157894737, AUC 0.9662650227546692, avg_entr 0.03471904247999191, f1 0.8805263042449951
ep9_l3_test_time 0.4095281979999754
Test Epoch9 layer4 Acc 0.8805263157894737, AUC 0.9645376205444336, avg_entr 0.030970335006713867, f1 0.8805263042449951
ep9_l4_test_time 0.5345612620000111
gc 0
Train Epoch10 Acc 0.9397083333333334 (112765/120000), AUC 0.9881067872047424
ep10_train_time 60.927083484000036
Test Epoch10 layer0 Acc 0.8821052631578947, AUC 0.9723565578460693, avg_entr 0.07830101251602173, f1 0.88210529088974
ep10_l0_test_time 0.18957162200001676
Test Epoch10 layer1 Acc 0.8810526315789474, AUC 0.9682232141494751, avg_entr 0.037199247628450394, f1 0.8810526132583618
ep10_l1_test_time 0.2362914659998978
Test Epoch10 layer2 Acc 0.881578947368421, AUC 0.9703322052955627, avg_entr 0.030428029596805573, f1 0.8815789222717285
ep10_l2_test_time 0.3074641740000743
Test Epoch10 layer3 Acc 0.8810526315789474, AUC 0.9672833681106567, avg_entr 0.0276730228215456, f1 0.8810526132583618
ep10_l3_test_time 0.40944506699997874
Test Epoch10 layer4 Acc 0.8818421052631579, AUC 0.9667753577232361, avg_entr 0.0252629816532135, f1 0.8818420767784119
ep10_l4_test_time 0.5346906349999472
gc 0
Train Epoch11 Acc 0.943125 (113175/120000), AUC 0.9890950918197632
ep11_train_time 60.71820047599999
Test Epoch11 layer0 Acc 0.8876315789473684, AUC 0.9719193577766418, avg_entr 0.07543811947107315, f1 0.8876315951347351
ep11_l0_test_time 0.1887709330000007
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 11
Test Epoch11 layer1 Acc 0.8855263157894737, AUC 0.9688085317611694, avg_entr 0.03764752671122551, f1 0.8855262398719788
ep11_l1_test_time 0.237235657000042
Test Epoch11 layer2 Acc 0.8852631578947369, AUC 0.9710754156112671, avg_entr 0.031531594693660736, f1 0.8852631449699402
ep11_l2_test_time 0.30820107099998495
Test Epoch11 layer3 Acc 0.8855263157894737, AUC 0.9700849056243896, avg_entr 0.028362061828374863, f1 0.8855262398719788
ep11_l3_test_time 0.40887800400003016
Test Epoch11 layer4 Acc 0.8863157894736842, AUC 0.9685235023498535, avg_entr 0.02611530013382435, f1 0.8863158226013184
ep11_l4_test_time 0.5351510570000073
gc 0
Train Epoch12 Acc 0.948625 (113835/120000), AUC 0.990390956401825
ep12_train_time 60.80374129699999
Test Epoch12 layer0 Acc 0.8831578947368421, AUC 0.971172034740448, avg_entr 0.06929891556501389, f1 0.8831579089164734
ep12_l0_test_time 0.1972744629999852
Test Epoch12 layer1 Acc 0.8821052631578947, AUC 0.9663714170455933, avg_entr 0.029599424451589584, f1 0.88210529088974
ep12_l1_test_time 0.2368133710000393
Test Epoch12 layer2 Acc 0.8826315789473684, AUC 0.9693880081176758, avg_entr 0.023154614493250847, f1 0.8826315999031067
ep12_l2_test_time 0.3080286120000437
Test Epoch12 layer3 Acc 0.8826315789473684, AUC 0.968007504940033, avg_entr 0.021127477288246155, f1 0.8826315999031067
ep12_l3_test_time 0.4115957999999864
Test Epoch12 layer4 Acc 0.8826315789473684, AUC 0.9655805826187134, avg_entr 0.0191029105335474, f1 0.8826315999031067
ep12_l4_test_time 0.5354213370000025
gc 0
Train Epoch13 Acc 0.9555666666666667 (114668/120000), AUC 0.992058277130127
ep13_train_time 60.82835760699993
Test Epoch13 layer0 Acc 0.885, AUC 0.9716871976852417, avg_entr 0.06936798989772797, f1 0.8849999904632568
ep13_l0_test_time 0.18947725299995
Test Epoch13 layer1 Acc 0.8826315789473684, AUC 0.9627540111541748, avg_entr 0.030126385390758514, f1 0.8826315999031067
ep13_l1_test_time 0.2360086040000624
Test Epoch13 layer2 Acc 0.8826315789473684, AUC 0.9678775072097778, avg_entr 0.024415506049990654, f1 0.8826315999031067
ep13_l2_test_time 0.3072736960000384
Test Epoch13 layer3 Acc 0.8821052631578947, AUC 0.9660786390304565, avg_entr 0.02206096425652504, f1 0.88210529088974
ep13_l3_test_time 0.4098226840000052
Test Epoch13 layer4 Acc 0.8831578947368421, AUC 0.9659038782119751, avg_entr 0.020227540284395218, f1 0.8831579089164734
ep13_l4_test_time 0.5343289669999649
gc 0
Train Epoch14 Acc 0.9592916666666667 (115115/120000), AUC 0.993808388710022
ep14_train_time 60.822172600000044
Test Epoch14 layer0 Acc 0.8873684210526316, AUC 0.9718655347824097, avg_entr 0.06598100066184998, f1 0.8873685002326965
ep14_l0_test_time 0.189413060999982
Test Epoch14 layer1 Acc 0.8828947368421053, AUC 0.963835597038269, avg_entr 0.03144485503435135, f1 0.88289475440979
ep14_l1_test_time 0.2376950579999857
Test Epoch14 layer2 Acc 0.8842105263157894, AUC 0.9678452014923096, avg_entr 0.025848837569355965, f1 0.8842105865478516
ep14_l2_test_time 0.30819121900003665
Test Epoch14 layer3 Acc 0.883421052631579, AUC 0.9664281606674194, avg_entr 0.02349003776907921, f1 0.8834210634231567
ep14_l3_test_time 0.409804548000011
Test Epoch14 layer4 Acc 0.8839473684210526, AUC 0.9645663499832153, avg_entr 0.02175622433423996, f1 0.8839473724365234
ep14_l4_test_time 0.5346322940000618
gc 0
Train Epoch15 Acc 0.9620666666666666 (115448/120000), AUC 0.9940585494041443
ep15_train_time 60.82155944400006
Test Epoch15 layer0 Acc 0.8839473684210526, AUC 0.9713582396507263, avg_entr 0.06257731467485428, f1 0.8839473724365234
ep15_l0_test_time 0.19055228600007013
Test Epoch15 layer1 Acc 0.8831578947368421, AUC 0.9638599753379822, avg_entr 0.022241216152906418, f1 0.8831579089164734
ep15_l1_test_time 0.23684487599996373
Test Epoch15 layer2 Acc 0.8839473684210526, AUC 0.9676649570465088, avg_entr 0.01784122735261917, f1 0.8839473724365234
ep15_l2_test_time 0.3070362789999308
Test Epoch15 layer3 Acc 0.883421052631579, AUC 0.9657171964645386, avg_entr 0.016454573720693588, f1 0.8834210634231567
ep15_l3_test_time 0.408238630000028
Test Epoch15 layer4 Acc 0.8828947368421053, AUC 0.9607124328613281, avg_entr 0.015280235558748245, f1 0.88289475440979
ep15_l4_test_time 0.5347963670000127
gc 0
Train Epoch16 Acc 0.96375 (115650/120000), AUC 0.9942130446434021
ep16_train_time 60.588930643000026
Test Epoch16 layer0 Acc 0.8842105263157894, AUC 0.971225917339325, avg_entr 0.06354202330112457, f1 0.8842105865478516
ep16_l0_test_time 0.18937446599989016
Test Epoch16 layer1 Acc 0.878421052631579, AUC 0.9649662375450134, avg_entr 0.02966655045747757, f1 0.8784210681915283
ep16_l1_test_time 0.235931058999995
Test Epoch16 layer2 Acc 0.8778947368421053, AUC 0.9677379131317139, avg_entr 0.02443077228963375, f1 0.8778947591781616
ep16_l2_test_time 0.30815405500015913
Test Epoch16 layer3 Acc 0.8776315789473684, AUC 0.9660190343856812, avg_entr 0.02290530875325203, f1 0.8776316046714783
ep16_l3_test_time 0.40946911600008207
Test Epoch16 layer4 Acc 0.8776315789473684, AUC 0.9643734693527222, avg_entr 0.021236222237348557, f1 0.8776316046714783
ep16_l4_test_time 0.5357432690000223
gc 0
Train Epoch17 Acc 0.9675583333333333 (116107/120000), AUC 0.9951891899108887
ep17_train_time 60.69152552499986
Test Epoch17 layer0 Acc 0.8873684210526316, AUC 0.9700076580047607, avg_entr 0.06297232210636139, f1 0.8873685002326965
ep17_l0_test_time 0.19228708100013137
Test Epoch17 layer1 Acc 0.8836842105263157, AUC 0.9585903882980347, avg_entr 0.025863366201519966, f1 0.8836842179298401
ep17_l1_test_time 0.23825670600012927
Test Epoch17 layer2 Acc 0.8839473684210526, AUC 0.9631948471069336, avg_entr 0.020238442346453667, f1 0.8839473724365234
ep17_l2_test_time 0.30826348100004
Test Epoch17 layer3 Acc 0.8839473684210526, AUC 0.9630826711654663, avg_entr 0.018006352707743645, f1 0.8839473724365234
ep17_l3_test_time 0.40885390499988716
Test Epoch17 layer4 Acc 0.8836842105263157, AUC 0.9612206816673279, avg_entr 0.016192380338907242, f1 0.8836842179298401
ep17_l4_test_time 0.5341932639998959
gc 0
Train Epoch18 Acc 0.9701416666666667 (116417/120000), AUC 0.9957379698753357
ep18_train_time 60.72422974900019
Test Epoch18 layer0 Acc 0.8839473684210526, AUC 0.9700635671615601, avg_entr 0.05865031108260155, f1 0.8839473724365234
ep18_l0_test_time 0.19281926999997268
Test Epoch18 layer1 Acc 0.8828947368421053, AUC 0.9641155004501343, avg_entr 0.02477802149951458, f1 0.88289475440979
ep18_l1_test_time 0.23625262399991698
Test Epoch18 layer2 Acc 0.8821052631578947, AUC 0.9683716297149658, avg_entr 0.020327355712652206, f1 0.88210529088974
ep18_l2_test_time 0.31195510299994567
Test Epoch18 layer3 Acc 0.88, AUC 0.9663549661636353, avg_entr 0.018897000700235367, f1 0.8799999952316284
ep18_l3_test_time 0.4086588909999591
Test Epoch18 layer4 Acc 0.8807894736842106, AUC 0.9645591974258423, avg_entr 0.017695780843496323, f1 0.8807894587516785
ep18_l4_test_time 0.5353100339998491
gc 0
Train Epoch19 Acc 0.9709166666666667 (116510/120000), AUC 0.996036946773529
ep19_train_time 60.82381003299997
Test Epoch19 layer0 Acc 0.8871052631578947, AUC 0.9695491194725037, avg_entr 0.057984694838523865, f1 0.8871052861213684
ep19_l0_test_time 0.1969301589999759
Test Epoch19 layer1 Acc 0.8828947368421053, AUC 0.958199143409729, avg_entr 0.021747810766100883, f1 0.88289475440979
ep19_l1_test_time 0.24301536300004045
Test Epoch19 layer2 Acc 0.8823684210526316, AUC 0.9641686677932739, avg_entr 0.017017176374793053, f1 0.8823684453964233
ep19_l2_test_time 0.3084167659999366
Test Epoch19 layer3 Acc 0.8826315789473684, AUC 0.9628973603248596, avg_entr 0.015453752130270004, f1 0.8826315999031067
ep19_l3_test_time 0.4087565459999496
Test Epoch19 layer4 Acc 0.8821052631578947, AUC 0.9595386385917664, avg_entr 0.01434221863746643, f1 0.88210529088974
ep19_l4_test_time 0.5357873250000011
gc 0
Train Epoch20 Acc 0.9715666666666667 (116588/120000), AUC 0.9964452981948853
ep20_train_time 60.67286140800002
Test Epoch20 layer0 Acc 0.8842105263157894, AUC 0.9689798355102539, avg_entr 0.060613155364990234, f1 0.8842105865478516
ep20_l0_test_time 0.18869638099999975
Test Epoch20 layer1 Acc 0.8781578947368421, AUC 0.9608145356178284, avg_entr 0.024140208959579468, f1 0.878157913684845
ep20_l1_test_time 0.23640680499988775
Test Epoch20 layer2 Acc 0.8781578947368421, AUC 0.9646170735359192, avg_entr 0.018486864864826202, f1 0.878157913684845
ep20_l2_test_time 0.31066351799995573
Test Epoch20 layer3 Acc 0.878421052631579, AUC 0.9647709131240845, avg_entr 0.01679416187107563, f1 0.8784210681915283
ep20_l3_test_time 0.4091874910000115
Test Epoch20 layer4 Acc 0.8778947368421053, AUC 0.9648603200912476, avg_entr 0.01560197863727808, f1 0.8778947591781616
ep20_l4_test_time 0.5360436040000423
gc 0
Train Epoch21 Acc 0.9744583333333333 (116935/120000), AUC 0.9965784549713135
ep21_train_time 60.81463513400013
Test Epoch21 layer0 Acc 0.8844736842105263, AUC 0.9687070250511169, avg_entr 0.058111052960157394, f1 0.8844736814498901
ep21_l0_test_time 0.19021596900006443
Test Epoch21 layer1 Acc 0.8810526315789474, AUC 0.9590142965316772, avg_entr 0.02249741367995739, f1 0.8810526132583618
ep21_l1_test_time 0.23782430699998258
Test Epoch21 layer2 Acc 0.8813157894736842, AUC 0.9621450901031494, avg_entr 0.017473651096224785, f1 0.8813157677650452
ep21_l2_test_time 0.30894006700009413
Test Epoch21 layer3 Acc 0.8810526315789474, AUC 0.9604488611221313, avg_entr 0.016330016776919365, f1 0.8810526132583618
ep21_l3_test_time 0.4100871980001557
Test Epoch21 layer4 Acc 0.8810526315789474, AUC 0.9612568616867065, avg_entr 0.015111858956515789, f1 0.8810526132583618
ep21_l4_test_time 0.5361017000000174
gc 0
Train Epoch22 Acc 0.974825 (116979/120000), AUC 0.9969682693481445
ep22_train_time 60.63779264799996
Test Epoch22 layer0 Acc 0.8860526315789473, AUC 0.9681075811386108, avg_entr 0.05659397691488266, f1 0.886052668094635
ep22_l0_test_time 0.18908154799987642
Test Epoch22 layer1 Acc 0.8813157894736842, AUC 0.957735002040863, avg_entr 0.021678274497389793, f1 0.8813157677650452
ep22_l1_test_time 0.23747588900005212
Test Epoch22 layer2 Acc 0.8813157894736842, AUC 0.9612241387367249, avg_entr 0.0163046196103096, f1 0.8813157677650452
ep22_l2_test_time 0.3081240129999969
Test Epoch22 layer3 Acc 0.8818421052631579, AUC 0.9595956802368164, avg_entr 0.014531717635691166, f1 0.8818420767784119
ep22_l3_test_time 0.4096303060000537
Test Epoch22 layer4 Acc 0.8821052631578947, AUC 0.9580804109573364, avg_entr 0.013357303105294704, f1 0.88210529088974
ep22_l4_test_time 0.5380492760000379
gc 0
Train Epoch23 Acc 0.9755 (117060/120000), AUC 0.9970120191574097
ep23_train_time 60.654398348000086
Test Epoch23 layer0 Acc 0.8855263157894737, AUC 0.9683864116668701, avg_entr 0.05682215467095375, f1 0.8855262398719788
ep23_l0_test_time 0.1899456270000428
Test Epoch23 layer1 Acc 0.8807894736842106, AUC 0.9593443870544434, avg_entr 0.021859269589185715, f1 0.8807894587516785
ep23_l1_test_time 0.2361335620000773
Test Epoch23 layer2 Acc 0.8807894736842106, AUC 0.9645040035247803, avg_entr 0.016786744818091393, f1 0.8807894587516785
ep23_l2_test_time 0.30803225799991196
Test Epoch23 layer3 Acc 0.8810526315789474, AUC 0.9631697535514832, avg_entr 0.015562977641820908, f1 0.8810526132583618
ep23_l3_test_time 0.40908785200008424
Test Epoch23 layer4 Acc 0.8807894736842106, AUC 0.9630226492881775, avg_entr 0.014675536192953587, f1 0.8807894587516785
ep23_l4_test_time 0.5372577209998326
gc 0
Train Epoch24 Acc 0.9763333333333334 (117160/120000), AUC 0.9969602823257446
ep24_train_time 60.76745777799988
Test Epoch24 layer0 Acc 0.8847368421052632, AUC 0.968148946762085, avg_entr 0.05617240443825722, f1 0.8847368359565735
ep24_l0_test_time 0.19361583900013102
Test Epoch24 layer1 Acc 0.8805263157894737, AUC 0.9575414657592773, avg_entr 0.021924331784248352, f1 0.8805263042449951
ep24_l1_test_time 0.23771709700008614
Test Epoch24 layer2 Acc 0.8807894736842106, AUC 0.9642499089241028, avg_entr 0.01738567277789116, f1 0.8807894587516785
ep24_l2_test_time 0.30866257000002406
Test Epoch24 layer3 Acc 0.8810526315789474, AUC 0.964165449142456, avg_entr 0.015835624188184738, f1 0.8810526132583618
ep24_l3_test_time 0.4102073379999638
Test Epoch24 layer4 Acc 0.8805263157894737, AUC 0.9628923535346985, avg_entr 0.014549024403095245, f1 0.8805263042449951
ep24_l4_test_time 0.5359500790000311
gc 0
Train Epoch25 Acc 0.9766333333333334 (117196/120000), AUC 0.9973717927932739
ep25_train_time 60.59632316600005
Test Epoch25 layer0 Acc 0.8852631578947369, AUC 0.9673794507980347, avg_entr 0.054440900683403015, f1 0.8852631449699402
ep25_l0_test_time 0.18981233599993175
Test Epoch25 layer1 Acc 0.8802631578947369, AUC 0.9561816453933716, avg_entr 0.020175345242023468, f1 0.880263090133667
ep25_l1_test_time 0.23609579300000405
Test Epoch25 layer2 Acc 0.8792105263157894, AUC 0.9613446593284607, avg_entr 0.015202268958091736, f1 0.8792105317115784
ep25_l2_test_time 0.3073591799998212
Test Epoch25 layer3 Acc 0.8802631578947369, AUC 0.9621405005455017, avg_entr 0.013875751756131649, f1 0.880263090133667
ep25_l3_test_time 0.40917261799995686
Test Epoch25 layer4 Acc 0.88, AUC 0.9591647386550903, avg_entr 0.013108768500387669, f1 0.8799999952316284
ep25_l4_test_time 0.5401663879999887
gc 0
Train Epoch26 Acc 0.977425 (117291/120000), AUC 0.9973160624504089
ep26_train_time 60.770573658999865
Test Epoch26 layer0 Acc 0.8847368421052632, AUC 0.9675487279891968, avg_entr 0.05392426624894142, f1 0.8847368359565735
ep26_l0_test_time 0.19117472099992483
Test Epoch26 layer1 Acc 0.8802631578947369, AUC 0.9545899629592896, avg_entr 0.020517883822321892, f1 0.880263090133667
ep26_l1_test_time 0.23658747199988284
Test Epoch26 layer2 Acc 0.88, AUC 0.9604078531265259, avg_entr 0.015580355189740658, f1 0.8799999952316284
ep26_l2_test_time 0.30768170400006056
Test Epoch26 layer3 Acc 0.8802631578947369, AUC 0.9593775272369385, avg_entr 0.014077882282435894, f1 0.880263090133667
ep26_l3_test_time 0.40914333600017017
Test Epoch26 layer4 Acc 0.8802631578947369, AUC 0.9582862854003906, avg_entr 0.01282423548400402, f1 0.880263090133667
ep26_l4_test_time 0.5349565389999498
gc 0
Train Epoch27 Acc 0.9778916666666667 (117347/120000), AUC 0.9973523616790771
ep27_train_time 60.76032308699996
Test Epoch27 layer0 Acc 0.8857894736842106, AUC 0.9674819707870483, avg_entr 0.05371500179171562, f1 0.8857894539833069
ep27_l0_test_time 0.19039284699988457
Test Epoch27 layer1 Acc 0.8821052631578947, AUC 0.9550210237503052, avg_entr 0.02011561021208763, f1 0.88210529088974
ep27_l1_test_time 0.23661277299993344
Test Epoch27 layer2 Acc 0.8831578947368421, AUC 0.9608067274093628, avg_entr 0.015201859176158905, f1 0.8831579089164734
ep27_l2_test_time 0.30794081299995923
Test Epoch27 layer3 Acc 0.8826315789473684, AUC 0.9608231782913208, avg_entr 0.013787863776087761, f1 0.8826315999031067
ep27_l3_test_time 0.4090675050001664
Test Epoch27 layer4 Acc 0.8826315789473684, AUC 0.9590824842453003, avg_entr 0.012808609753847122, f1 0.8826315999031067
ep27_l4_test_time 0.535232914000062
gc 0
Train Epoch28 Acc 0.9783416666666667 (117401/120000), AUC 0.9975153207778931
ep28_train_time 60.777672145
Test Epoch28 layer0 Acc 0.888421052631579, AUC 0.9676010012626648, avg_entr 0.0522962287068367, f1 0.8884210586547852
ep28_l0_test_time 0.19055388700007825
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 28
Test Epoch28 layer1 Acc 0.8810526315789474, AUC 0.9551194906234741, avg_entr 0.0190750602632761, f1 0.8810526132583618
ep28_l1_test_time 0.2365460799999255
Test Epoch28 layer2 Acc 0.8810526315789474, AUC 0.9614713191986084, avg_entr 0.014685271307826042, f1 0.8810526132583618
ep28_l2_test_time 0.3079373929999747
Test Epoch28 layer3 Acc 0.881578947368421, AUC 0.9615408182144165, avg_entr 0.013547154143452644, f1 0.8815789222717285
ep28_l3_test_time 0.4131826270001966
Test Epoch28 layer4 Acc 0.8813157894736842, AUC 0.9584426879882812, avg_entr 0.012824726291000843, f1 0.8813157677650452
ep28_l4_test_time 0.5349880619999112
gc 0
Train Epoch29 Acc 0.9785083333333333 (117421/120000), AUC 0.9974134564399719
ep29_train_time 61.0012946830002
Test Epoch29 layer0 Acc 0.8873684210526316, AUC 0.9673895835876465, avg_entr 0.052417948842048645, f1 0.8873685002326965
ep29_l0_test_time 0.1902810659998977
Test Epoch29 layer1 Acc 0.8805263157894737, AUC 0.9532628059387207, avg_entr 0.019472794607281685, f1 0.8805263042449951
ep29_l1_test_time 0.23624101800010067
Test Epoch29 layer2 Acc 0.881578947368421, AUC 0.958892285823822, avg_entr 0.014004521071910858, f1 0.8815789222717285
ep29_l2_test_time 0.30903664499987826
Test Epoch29 layer3 Acc 0.8810526315789474, AUC 0.9583855867385864, avg_entr 0.012652342207729816, f1 0.8810526132583618
ep29_l3_test_time 0.41057393400001274
Test Epoch29 layer4 Acc 0.8807894736842106, AUC 0.9557843208312988, avg_entr 0.01193237118422985, f1 0.8807894587516785
ep29_l4_test_time 0.5361986410000554
gc 0
Train Epoch30 Acc 0.978675 (117441/120000), AUC 0.9976127743721008
ep30_train_time 60.62622069200006
Test Epoch30 layer0 Acc 0.8857894736842106, AUC 0.9674155712127686, avg_entr 0.051257818937301636, f1 0.8857894539833069
ep30_l0_test_time 0.1908779489999688
Test Epoch30 layer1 Acc 0.8823684210526316, AUC 0.9537476897239685, avg_entr 0.018718266859650612, f1 0.8823684453964233
ep30_l1_test_time 0.23803776900012963
Test Epoch30 layer2 Acc 0.881578947368421, AUC 0.9597081542015076, avg_entr 0.01369813084602356, f1 0.8815789222717285
ep30_l2_test_time 0.30901069699984873
Test Epoch30 layer3 Acc 0.8813157894736842, AUC 0.9602043628692627, avg_entr 0.012714876793324947, f1 0.8813157677650452
ep30_l3_test_time 0.4117903159999514
Test Epoch30 layer4 Acc 0.8813157894736842, AUC 0.9565808773040771, avg_entr 0.011877336539328098, f1 0.8813157677650452
ep30_l4_test_time 0.5357484169999225
gc 0
Train Epoch31 Acc 0.9791 (117492/120000), AUC 0.9977056980133057
ep31_train_time 60.728766509000025
Test Epoch31 layer0 Acc 0.8855263157894737, AUC 0.9675395488739014, avg_entr 0.05090527608990669, f1 0.8855262398719788
ep31_l0_test_time 0.18974979599988728
Test Epoch31 layer1 Acc 0.8805263157894737, AUC 0.9553167819976807, avg_entr 0.01799711212515831, f1 0.8805263042449951
ep31_l1_test_time 0.23679823500015118
Test Epoch31 layer2 Acc 0.8810526315789474, AUC 0.9609293937683105, avg_entr 0.014385458081960678, f1 0.8810526132583618
ep31_l2_test_time 0.3080031249999138
Test Epoch31 layer3 Acc 0.8802631578947369, AUC 0.9613668918609619, avg_entr 0.013104376383125782, f1 0.880263090133667
ep31_l3_test_time 0.40911564199996064
Test Epoch31 layer4 Acc 0.8802631578947369, AUC 0.958226203918457, avg_entr 0.01228069793432951, f1 0.880263090133667
ep31_l4_test_time 0.5353823389998524
gc 0
Train Epoch32 Acc 0.9793 (117516/120000), AUC 0.9976429343223572
ep32_train_time 60.6546217959999
Test Epoch32 layer0 Acc 0.8868421052631579, AUC 0.9673802852630615, avg_entr 0.04949643835425377, f1 0.8868421316146851
ep32_l0_test_time 0.19698209400030464
Test Epoch32 layer1 Acc 0.8805263157894737, AUC 0.953371524810791, avg_entr 0.01893777959048748, f1 0.8805263042449951
ep32_l1_test_time 0.23681171999987782
Test Epoch32 layer2 Acc 0.8810526315789474, AUC 0.9604880809783936, avg_entr 0.013931798748672009, f1 0.8810526132583618
ep32_l2_test_time 0.3082420389996514
Test Epoch32 layer3 Acc 0.8810526315789474, AUC 0.9607491493225098, avg_entr 0.01286389958113432, f1 0.8810526132583618
ep32_l3_test_time 0.40917180499991446
Test Epoch32 layer4 Acc 0.8813157894736842, AUC 0.9576568603515625, avg_entr 0.012006322853267193, f1 0.8813157677650452
ep32_l4_test_time 0.5397413599998799
gc 0
Train Epoch33 Acc 0.9792166666666666 (117506/120000), AUC 0.9977843761444092
ep33_train_time 60.75776373499957
Test Epoch33 layer0 Acc 0.8857894736842106, AUC 0.9673190712928772, avg_entr 0.04950256645679474, f1 0.8857894539833069
ep33_l0_test_time 0.18981621200009613
Test Epoch33 layer1 Acc 0.8813157894736842, AUC 0.9542704224586487, avg_entr 0.017973488196730614, f1 0.8813157677650452
ep33_l1_test_time 0.23690934900014327
Test Epoch33 layer2 Acc 0.8810526315789474, AUC 0.9605498909950256, avg_entr 0.013166090473532677, f1 0.8810526132583618
ep33_l2_test_time 0.30877852300000086
Test Epoch33 layer3 Acc 0.8813157894736842, AUC 0.9612290859222412, avg_entr 0.011913828551769257, f1 0.8813157677650452
ep33_l3_test_time 0.40935536500001035
Test Epoch33 layer4 Acc 0.8807894736842106, AUC 0.9576920866966248, avg_entr 0.011167338117957115, f1 0.8807894587516785
ep33_l4_test_time 0.5359236550002606
gc 0
Train Epoch34 Acc 0.9795 (117540/120000), AUC 0.9977594614028931
ep34_train_time 60.680115689000104
Test Epoch34 layer0 Acc 0.885, AUC 0.9672226905822754, avg_entr 0.049320150166749954, f1 0.8849999904632568
ep34_l0_test_time 0.18969904699997642
Test Epoch34 layer1 Acc 0.8813157894736842, AUC 0.9541122317314148, avg_entr 0.017702139914035797, f1 0.8813157677650452
ep34_l1_test_time 0.23646406500029116
Test Epoch34 layer2 Acc 0.8805263157894737, AUC 0.95981365442276, avg_entr 0.013355477713048458, f1 0.8805263042449951
ep34_l2_test_time 0.30873916199971063
Test Epoch34 layer3 Acc 0.881578947368421, AUC 0.9609752893447876, avg_entr 0.012274757958948612, f1 0.8815789222717285
ep34_l3_test_time 0.40862755899979675
Test Epoch34 layer4 Acc 0.8810526315789474, AUC 0.9583017230033875, avg_entr 0.011600160039961338, f1 0.8810526132583618
ep34_l4_test_time 0.5352914790000796
gc 0
Train Epoch35 Acc 0.9796666666666667 (117560/120000), AUC 0.9978091716766357
ep35_train_time 60.74918739199984
Test Epoch35 layer0 Acc 0.8855263157894737, AUC 0.9672201871871948, avg_entr 0.04901186749339104, f1 0.8855262398719788
ep35_l0_test_time 0.19048972699965816
Test Epoch35 layer1 Acc 0.8786842105263157, AUC 0.9532217979431152, avg_entr 0.017503798007965088, f1 0.8786842226982117
ep35_l1_test_time 0.23763015200029258
Test Epoch35 layer2 Acc 0.8797368421052632, AUC 0.9594894647598267, avg_entr 0.012941081076860428, f1 0.8797368407249451
ep35_l2_test_time 0.30897955399996135
Test Epoch35 layer3 Acc 0.8794736842105263, AUC 0.9595933556556702, avg_entr 0.011924435384571552, f1 0.8794736862182617
ep35_l3_test_time 0.4083033219999379
Test Epoch35 layer4 Acc 0.8786842105263157, AUC 0.9555429220199585, avg_entr 0.011098373681306839, f1 0.8786842226982117
ep35_l4_test_time 0.5347537419997934
gc 0
Train Epoch36 Acc 0.979375 (117525/120000), AUC 0.9977955222129822
ep36_train_time 60.675064584999745
Test Epoch36 layer0 Acc 0.8855263157894737, AUC 0.9673637747764587, avg_entr 0.04872795566916466, f1 0.8855262398719788
ep36_l0_test_time 0.1887114979999751
Test Epoch36 layer1 Acc 0.8792105263157894, AUC 0.9543370008468628, avg_entr 0.017115077003836632, f1 0.8792105317115784
ep36_l1_test_time 0.23568285200008177
Test Epoch36 layer2 Acc 0.8797368421052632, AUC 0.9611082077026367, avg_entr 0.013079619966447353, f1 0.8797368407249451
ep36_l2_test_time 0.3073413420001998
Test Epoch36 layer3 Acc 0.88, AUC 0.9615316390991211, avg_entr 0.011870570480823517, f1 0.8799999952316284
ep36_l3_test_time 0.4085575740000422
Test Epoch36 layer4 Acc 0.8794736842105263, AUC 0.9590922594070435, avg_entr 0.011128314770758152, f1 0.8794736862182617
ep36_l4_test_time 0.5345030910002606
gc 0
Train Epoch37 Acc 0.9794583333333333 (117535/120000), AUC 0.9978433847427368
ep37_train_time 60.78034283099987
Test Epoch37 layer0 Acc 0.8868421052631579, AUC 0.9672373533248901, avg_entr 0.048415638506412506, f1 0.8868421316146851
ep37_l0_test_time 0.18962495899995702
Test Epoch37 layer1 Acc 0.8797368421052632, AUC 0.9533495306968689, avg_entr 0.016880741342902184, f1 0.8797368407249451
ep37_l1_test_time 0.23664875500026028
Test Epoch37 layer2 Acc 0.8797368421052632, AUC 0.9591726064682007, avg_entr 0.0126145388931036, f1 0.8797368407249451
ep37_l2_test_time 0.3074233259999346
Test Epoch37 layer3 Acc 0.8797368421052632, AUC 0.960616946220398, avg_entr 0.011532229371368885, f1 0.8797368407249451
ep37_l3_test_time 0.4110675110000557
Test Epoch37 layer4 Acc 0.8797368421052632, AUC 0.9578197002410889, avg_entr 0.010830575600266457, f1 0.8797368407249451
ep37_l4_test_time 0.5352945019999424
gc 0
Train Epoch38 Acc 0.979875 (117585/120000), AUC 0.9977871775627136
ep38_train_time 60.86528386300006
Test Epoch38 layer0 Acc 0.8857894736842106, AUC 0.9671840071678162, avg_entr 0.04818226769566536, f1 0.8857894539833069
ep38_l0_test_time 0.19201201899977605
Test Epoch38 layer1 Acc 0.8805263157894737, AUC 0.9534484148025513, avg_entr 0.01673475094139576, f1 0.8805263042449951
ep38_l1_test_time 0.23669271500011746
Test Epoch38 layer2 Acc 0.8802631578947369, AUC 0.9595833420753479, avg_entr 0.01209291722625494, f1 0.880263090133667
ep38_l2_test_time 0.30802912999979526
Test Epoch38 layer3 Acc 0.8805263157894737, AUC 0.9599339962005615, avg_entr 0.010947443544864655, f1 0.8805263042449951
ep38_l3_test_time 0.41000781899992944
Test Epoch38 layer4 Acc 0.8805263157894737, AUC 0.9567642211914062, avg_entr 0.0101486686617136, f1 0.8805263042449951
ep38_l4_test_time 0.5365392500002599
gc 0
Train Epoch39 Acc 0.9796666666666667 (117560/120000), AUC 0.9977484941482544
ep39_train_time 60.8197461929999
Test Epoch39 layer0 Acc 0.8863157894736842, AUC 0.96724534034729, avg_entr 0.047993648797273636, f1 0.8863158226013184
ep39_l0_test_time 0.18966484300017328
Test Epoch39 layer1 Acc 0.8792105263157894, AUC 0.9528462886810303, avg_entr 0.016642412170767784, f1 0.8792105317115784
ep39_l1_test_time 0.24542157799987763
Test Epoch39 layer2 Acc 0.8786842105263157, AUC 0.9586595296859741, avg_entr 0.012376736849546432, f1 0.8786842226982117
ep39_l2_test_time 0.31716538800037597
Test Epoch39 layer3 Acc 0.8794736842105263, AUC 0.9601415395736694, avg_entr 0.011373498477041721, f1 0.8794736862182617
ep39_l3_test_time 0.40959983999982796
Test Epoch39 layer4 Acc 0.8797368421052632, AUC 0.9570725560188293, avg_entr 0.010749023407697678, f1 0.8797368407249451
ep39_l4_test_time 0.5357345699999314
gc 0
Train Epoch40 Acc 0.9799166666666667 (117590/120000), AUC 0.9977803230285645
ep40_train_time 60.99202562800019
Test Epoch40 layer0 Acc 0.886578947368421, AUC 0.9672379493713379, avg_entr 0.047839365899562836, f1 0.8865789771080017
ep40_l0_test_time 0.18965376799997102
Test Epoch40 layer1 Acc 0.878421052631579, AUC 0.9532104730606079, avg_entr 0.016395049169659615, f1 0.8784210681915283
ep40_l1_test_time 0.23657748499999798
Test Epoch40 layer2 Acc 0.8789473684210526, AUC 0.9594802856445312, avg_entr 0.01205927599221468, f1 0.878947377204895
ep40_l2_test_time 0.3079082039998866
Test Epoch40 layer3 Acc 0.8786842105263157, AUC 0.9608079195022583, avg_entr 0.011097828857600689, f1 0.8786842226982117
ep40_l3_test_time 0.40824594499963496
Test Epoch40 layer4 Acc 0.878421052631579, AUC 0.9573884606361389, avg_entr 0.01050281897187233, f1 0.8784210681915283
ep40_l4_test_time 0.5346937620001881
gc 0
Train Epoch41 Acc 0.9799833333333333 (117598/120000), AUC 0.9978563785552979
ep41_train_time 60.675192762999814
Test Epoch41 layer0 Acc 0.8860526315789473, AUC 0.9672344923019409, avg_entr 0.04747467488050461, f1 0.886052668094635
ep41_l0_test_time 0.18916257599994424
Test Epoch41 layer1 Acc 0.8792105263157894, AUC 0.9529317021369934, avg_entr 0.016277140006422997, f1 0.8792105317115784
ep41_l1_test_time 0.2364118589998725
Test Epoch41 layer2 Acc 0.8792105263157894, AUC 0.9590743780136108, avg_entr 0.011833561584353447, f1 0.8792105317115784
ep41_l2_test_time 0.3079008670001713
Test Epoch41 layer3 Acc 0.8794736842105263, AUC 0.9602037668228149, avg_entr 0.010857406072318554, f1 0.8794736862182617
ep41_l3_test_time 0.40948947500010036
Test Epoch41 layer4 Acc 0.8792105263157894, AUC 0.9571663737297058, avg_entr 0.010177552700042725, f1 0.8792105317115784
ep41_l4_test_time 0.5352813929998774
gc 0
Train Epoch42 Acc 0.9798833333333333 (117586/120000), AUC 0.9977720975875854
ep42_train_time 60.842403162999744
Test Epoch42 layer0 Acc 0.8855263157894737, AUC 0.9672104716300964, avg_entr 0.047481101006269455, f1 0.8855262398719788
ep42_l0_test_time 0.19061431700038156
Test Epoch42 layer1 Acc 0.8789473684210526, AUC 0.9527603387832642, avg_entr 0.01607310026884079, f1 0.878947377204895
ep42_l1_test_time 0.23731358699978955
Test Epoch42 layer2 Acc 0.8797368421052632, AUC 0.959235429763794, avg_entr 0.011709077283740044, f1 0.8797368407249451
ep42_l2_test_time 0.3091351579996626
Test Epoch42 layer3 Acc 0.8797368421052632, AUC 0.9591693878173828, avg_entr 0.010747007094323635, f1 0.8797368407249451
ep42_l3_test_time 0.410043823000251
Test Epoch42 layer4 Acc 0.8794736842105263, AUC 0.955711841583252, avg_entr 0.010145888663828373, f1 0.8794736862182617
ep42_l4_test_time 0.5351151980003124
gc 0
Train Epoch43 Acc 0.9803 (117636/120000), AUC 0.9978816509246826
ep43_train_time 60.71964448500012
Test Epoch43 layer0 Acc 0.8860526315789473, AUC 0.9672580361366272, avg_entr 0.04736123979091644, f1 0.886052668094635
ep43_l0_test_time 0.19096547499975713
Test Epoch43 layer1 Acc 0.8789473684210526, AUC 0.9531662464141846, avg_entr 0.016268650069832802, f1 0.878947377204895
ep43_l1_test_time 0.23862657399968157
Test Epoch43 layer2 Acc 0.8786842105263157, AUC 0.9592815041542053, avg_entr 0.012231660075485706, f1 0.8786842226982117
ep43_l2_test_time 0.30994685399991795
Test Epoch43 layer3 Acc 0.878421052631579, AUC 0.9599863290786743, avg_entr 0.011234588921070099, f1 0.8784210681915283
ep43_l3_test_time 0.41087571500020204
Test Epoch43 layer4 Acc 0.878421052631579, AUC 0.9570670127868652, avg_entr 0.010610884986817837, f1 0.8784210681915283
ep43_l4_test_time 0.5370658150000054
gc 0
Train Epoch44 Acc 0.9798666666666667 (117584/120000), AUC 0.9978002309799194
ep44_train_time 60.89118829099971
Test Epoch44 layer0 Acc 0.8863157894736842, AUC 0.9672136902809143, avg_entr 0.047122351825237274, f1 0.8863158226013184
ep44_l0_test_time 0.1911636710001403
Test Epoch44 layer1 Acc 0.8786842105263157, AUC 0.9532656669616699, avg_entr 0.016164002940058708, f1 0.8786842226982117
ep44_l1_test_time 0.23724762500023644
Test Epoch44 layer2 Acc 0.8797368421052632, AUC 0.9598785638809204, avg_entr 0.011989540420472622, f1 0.8797368407249451
ep44_l2_test_time 0.3086708159999034
Test Epoch44 layer3 Acc 0.8797368421052632, AUC 0.9605006575584412, avg_entr 0.011033318936824799, f1 0.8797368407249451
ep44_l3_test_time 0.4095668569998452
Test Epoch44 layer4 Acc 0.8792105263157894, AUC 0.9570195078849792, avg_entr 0.010388030670583248, f1 0.8792105317115784
ep44_l4_test_time 0.5362051520000932
gc 0
Train Epoch45 Acc 0.97995 (117594/120000), AUC 0.9977993965148926
ep45_train_time 60.71849247099999
Test Epoch45 layer0 Acc 0.8857894736842106, AUC 0.9671895503997803, avg_entr 0.047079335898160934, f1 0.8857894539833069
ep45_l0_test_time 0.18963297999971473
Test Epoch45 layer1 Acc 0.8794736842105263, AUC 0.9526048302650452, avg_entr 0.016028067097067833, f1 0.8794736862182617
ep45_l1_test_time 0.23645102299997234
Test Epoch45 layer2 Acc 0.8794736842105263, AUC 0.9588954448699951, avg_entr 0.011562868021428585, f1 0.8794736862182617
ep45_l2_test_time 0.308454329000142
Test Epoch45 layer3 Acc 0.88, AUC 0.9592903256416321, avg_entr 0.010595669969916344, f1 0.8799999952316284
ep45_l3_test_time 0.4097588539998469
Test Epoch45 layer4 Acc 0.8794736842105263, AUC 0.9563405513763428, avg_entr 0.00993990246206522, f1 0.8794736862182617
ep45_l4_test_time 0.5369228340000518
gc 0
Train Epoch46 Acc 0.9801416666666667 (117617/120000), AUC 0.9978922605514526
ep46_train_time 60.762492116999965
Test Epoch46 layer0 Acc 0.8860526315789473, AUC 0.9671997427940369, avg_entr 0.046963926404714584, f1 0.886052668094635
ep46_l0_test_time 0.19010073399977045
Test Epoch46 layer1 Acc 0.8789473684210526, AUC 0.9527578949928284, avg_entr 0.01597357541322708, f1 0.878947377204895
ep46_l1_test_time 0.23815616199999567
Test Epoch46 layer2 Acc 0.8789473684210526, AUC 0.9591391086578369, avg_entr 0.011895542033016682, f1 0.878947377204895
ep46_l2_test_time 0.30854242199984583
Test Epoch46 layer3 Acc 0.8789473684210526, AUC 0.9597402811050415, avg_entr 0.010956468991935253, f1 0.878947377204895
ep46_l3_test_time 0.4093144180001218
Test Epoch46 layer4 Acc 0.8786842105263157, AUC 0.9561347961425781, avg_entr 0.010301806963980198, f1 0.8786842226982117
ep46_l4_test_time 0.5362471749999713
gc 0
Train Epoch47 Acc 0.9796416666666666 (117557/120000), AUC 0.9977867007255554
ep47_train_time 60.9307990479997
Test Epoch47 layer0 Acc 0.8860526315789473, AUC 0.9671977162361145, avg_entr 0.046847157180309296, f1 0.886052668094635
ep47_l0_test_time 0.19082689300012134
Test Epoch47 layer1 Acc 0.8794736842105263, AUC 0.9526604413986206, avg_entr 0.01592615246772766, f1 0.8794736862182617
ep47_l1_test_time 0.23847423699999126
Test Epoch47 layer2 Acc 0.8794736842105263, AUC 0.9589746594429016, avg_entr 0.011580871418118477, f1 0.8794736862182617
ep47_l2_test_time 0.3093262610000238
Test Epoch47 layer3 Acc 0.8794736842105263, AUC 0.9600028991699219, avg_entr 0.010664657689630985, f1 0.8794736862182617
ep47_l3_test_time 0.41021749999981694
Test Epoch47 layer4 Acc 0.8792105263157894, AUC 0.9570863842964172, avg_entr 0.010027558542788029, f1 0.8792105317115784
ep47_l4_test_time 0.5358981489998769
gc 0
Train Epoch48 Acc 0.9800583333333334 (117607/120000), AUC 0.9979662299156189
ep48_train_time 60.76148093199981
Test Epoch48 layer0 Acc 0.8860526315789473, AUC 0.9671806693077087, avg_entr 0.046685539186000824, f1 0.886052668094635
ep48_l0_test_time 0.18952619499987122
Test Epoch48 layer1 Acc 0.8797368421052632, AUC 0.9527311325073242, avg_entr 0.015891533344984055, f1 0.8797368407249451
ep48_l1_test_time 0.23651087799999004
Test Epoch48 layer2 Acc 0.8794736842105263, AUC 0.9592398405075073, avg_entr 0.011632585898041725, f1 0.8794736862182617
ep48_l2_test_time 0.3079400820001865
Test Epoch48 layer3 Acc 0.8797368421052632, AUC 0.9598093032836914, avg_entr 0.010697663761675358, f1 0.8797368407249451
ep48_l3_test_time 0.40959153099993273
Test Epoch48 layer4 Acc 0.8792105263157894, AUC 0.9562842845916748, avg_entr 0.010060500353574753, f1 0.8792105317115784
ep48_l4_test_time 0.5358249129999422
gc 0
Train Epoch49 Acc 0.9805666666666667 (117668/120000), AUC 0.997816264629364
ep49_train_time 60.913224972999615
Test Epoch49 layer0 Acc 0.8857894736842106, AUC 0.9671810865402222, avg_entr 0.04671119526028633, f1 0.8857894539833069
ep49_l0_test_time 0.1908269460000156
Test Epoch49 layer1 Acc 0.8797368421052632, AUC 0.9528019428253174, avg_entr 0.015891728922724724, f1 0.8797368407249451
ep49_l1_test_time 0.23891320800021276
Test Epoch49 layer2 Acc 0.8794736842105263, AUC 0.9591789245605469, avg_entr 0.011614613234996796, f1 0.8794736862182617
ep49_l2_test_time 0.3085034569999152
Test Epoch49 layer3 Acc 0.8797368421052632, AUC 0.9601050019264221, avg_entr 0.010691934265196323, f1 0.8797368407249451
ep49_l3_test_time 0.40947911700004624
Test Epoch49 layer4 Acc 0.8794736842105263, AUC 0.956926167011261, avg_entr 0.010078737512230873, f1 0.8794736862182617
ep49_l4_test_time 0.5359758169997804
Best AUC tensor(0.8884) 28 0
train_as_loss [[4.54102225e+02 3.56751525e+02 3.51138170e+02 3.49870763e+02
  3.49387742e+02 3.49154639e+02 3.49025865e+02 3.48948261e+02
  3.48898607e+02 3.48865438e+02 3.48842563e+02 3.48826398e+02
  3.48814761e+02 3.48808004e+02 3.48804017e+02 3.48800328e+02
  3.48797001e+02 3.48794730e+02 3.48793212e+02 3.48791702e+02
  3.48790233e+02 3.48789137e+02 3.48788384e+02 3.48787594e+02
  3.48786800e+02 3.48786247e+02 3.48785732e+02 3.48785358e+02
  3.48784793e+02 3.48784523e+02 3.48784246e+02 3.48783850e+02
  3.48783656e+02 3.48783461e+02 3.48783216e+02 3.48782981e+02
  3.48782820e+02 3.48782768e+02 3.48782727e+02 3.48782588e+02
  3.48782453e+02 3.48782207e+02 3.48782175e+02 3.48782159e+02
  3.48782105e+02 3.48782051e+02 3.48781978e+02 3.48781959e+02
  3.48781950e+02 3.48781943e+02]
 [1.32188256e+00 1.06190674e-05 7.28924648e-07 1.40791405e-07
  4.20445121e-08 1.66031833e-08 1.23812267e-07 2.70580417e-05
  1.05678248e-04 1.81027677e-04 3.67198245e-04 2.96428872e-04
  4.54705881e-04 3.09627023e-08 3.37163590e-10 1.47341485e-07
  2.23346340e-04 6.75896763e-08 1.93676483e-10 2.24314228e-10
  1.94397920e-05 2.02203659e-08 1.27433702e-10 8.59278955e-10
  1.06052627e-05 2.54357969e-07 9.90306580e-11 2.17480967e-10
  2.85951125e-06 7.49841382e-11 8.37736877e-11 1.52496402e-10
  5.89684226e-07 8.58608766e-09 6.89562563e-11 1.17322758e-10
  1.02375219e-07 1.15544684e-10 5.70946185e-11 8.39181027e-11
  2.50095514e-08 1.21560674e-09 3.97352165e-11 5.16194499e-11
  8.52855402e-11 2.62338733e-11 2.91397538e-11 3.43181124e-11
  4.12238571e-11 1.78036539e-11]
 [1.65602503e+00 1.20992452e-05 8.78847549e-07 1.75637276e-07
  5.76415338e-08 2.77931821e-08 7.66396986e-08 4.11668581e-05
  2.03275936e-04 3.81418580e-04 1.83613477e-04 1.76200075e-04
  2.92549286e-04 2.12172377e-07 1.04791371e-09 7.79050668e-08
  1.63334086e-04 5.78366972e-07 5.36238542e-10 5.79909803e-10
  1.26860679e-05 1.71844995e-07 3.71601225e-10 9.58571062e-10
  7.67063624e-06 1.84073720e-06 2.50889817e-10 4.11883254e-10
  2.22323547e-06 3.89525011e-10 2.11251969e-10 3.11089709e-10
  4.13061178e-07 5.90944156e-08 1.68093697e-10 2.46791827e-10
  7.16551299e-08 7.94184554e-10 1.45076445e-10 1.88801539e-10
  1.76848768e-08 8.01608961e-09 9.18597514e-11 1.11518050e-10
  1.52703846e-10 1.54180742e-10 6.29716153e-11 7.08331421e-11
  8.23925717e-11 8.50747772e-11]
 [1.51110430e+00 1.04705403e-05 9.16034157e-07 2.06336697e-07
  8.49351527e-08 6.14511592e-08 1.30396431e-07 9.96748032e-05
  3.93006426e-04 7.00543623e-04 5.17501157e-04 1.64954205e-04
  2.85411657e-04 1.00059654e-06 5.11412675e-09 6.41175912e-08
  1.75614408e-04 4.07571230e-06 2.78652741e-09 2.24342696e-09
  1.42021617e-05 1.45827541e-06 2.89787885e-09 2.58061814e-09
  8.40169096e-06 1.05581978e-05 1.53288253e-09 9.06152407e-10
  2.11722309e-06 2.39346594e-09 1.47202071e-09 9.59241755e-10
  4.21163322e-07 3.00645381e-07 1.13012731e-09 6.79517824e-10
  6.77029145e-08 4.51471358e-09 1.08820383e-09 5.90812141e-10
  1.64909639e-08 3.61475841e-08 5.33261173e-10 2.76928818e-10
  3.64502187e-10 6.99378685e-10 3.57229445e-10 1.67131966e-10
  2.05347733e-10 3.22898423e-10]
 [1.53140918e+00 1.14423134e-05 1.30016319e-06 3.73303741e-07
  2.13325057e-07 2.21753249e-07 3.86530691e-07 2.30389160e-04
  8.13175869e-04 1.23664234e-03 3.46221386e-04 1.69131355e-04
  3.14635345e-04 3.90651131e-06 1.19050143e-08 7.50524613e-08
  2.23662201e-04 1.86371397e-05 5.11645023e-09 9.08330291e-09
  1.45902201e-05 6.04938494e-06 5.40748687e-09 8.52569465e-09
  9.47386673e-06 2.98739268e-05 1.38755799e-09 1.54756691e-09
  1.90286195e-06 8.04679348e-09 1.62075328e-09 2.38888921e-09
  3.77946430e-07 7.82544141e-07 1.11087465e-09 1.42435958e-09
  6.30572717e-08 1.37730566e-08 1.26543433e-09 1.53075274e-09
  1.68667949e-08 8.63202503e-08 5.10771187e-10 5.72081694e-10
  8.19948681e-10 2.12454839e-09 3.96614344e-10 3.55317099e-10
  4.97012183e-10 9.69723130e-10]]
train_ae_loss [[12.72275581 11.68463897 11.77337342 11.30858375 10.91830334 10.68362053
  10.3956804  10.16685345 10.04121847  9.81964188  9.74284255  9.61458206
   9.43656992  8.65315731  8.42566565  8.30671612  8.19586496  7.77185779
   7.64006602  7.60022807  7.59244965  7.34538478  7.32765464  7.31765995
   7.32705507  7.206836    7.19736958  7.20972429  7.23067273  7.21711904
   7.22242045  7.22554962  7.27383328  7.24052094  7.25916913  7.26990104
   7.29092413  7.32004324  7.31357942  7.35526624  7.37896651  7.36401673
   7.36985715  7.39022399  7.41115601  7.41248117  7.43928895  7.4327985
   7.433034    7.43524577]
 [13.67940018 11.2989052  10.19967141  9.56020966  9.05406989  8.68562858
   8.15728329  7.67008937  7.34676178  6.87993215  6.65670737  6.42138087
   5.99077245  5.01514498  4.700449    4.47516411  4.2906419   3.74913832
   3.60220188  3.50806128  3.44309437  3.12738113  3.09053596  3.0287391
   3.02182678  2.86221181  2.80251256  2.79273379  2.79063581  2.7415368
   2.74370801  2.70803754  2.7209152   2.71065593  2.67828161  2.63504719
   2.66744397  2.67363901  2.64713586  2.65005312  2.66753782  2.63090129
   2.6440869   2.63950229  2.64958487  2.64515848  2.6723399   2.66399507
   2.66774215  2.62397244]
 [14.23763373 10.69012858  9.2500766   8.55938382  7.9627021   7.47820709
   6.9423712   6.5174725   6.25235639  5.83117117  5.56065392  5.37056791
   4.9987537   4.19982174  3.88731222  3.69160411  3.52894871  3.08580068
   2.93905381  2.86150189  2.80442636  2.53447101  2.49431224  2.44111043
   2.43018541  2.29866409  2.24021329  2.2372007   2.22926546  2.1821537
   2.19011469  2.15678373  2.16109315  2.15745339  2.12992155  2.08344554
   2.11877761  2.12113418  2.09297084  2.09522673  2.10832132  2.07403243
   2.09124356  2.07663768  2.08534034  2.0845903   2.1170899   2.09729724
   2.10340494  2.06666368]
 [16.54774046 13.17415363 10.6803607   9.84455208  9.20520817  8.6777106
   7.99446046  7.46612056  7.14880927  6.62700787  6.34284146  6.00693355
   5.56556709  4.67955599  4.33546118  4.07624617  3.89388477  3.40049511
   3.24357715  3.13364827  3.06859402  2.76811321  2.72544515  2.65525673
   2.64201227  2.49661571  2.43084614  2.42325394  2.41130488  2.35756247
   2.3704091   2.32439265  2.32921951  2.32482967  2.29634917  2.23890292
   2.27662548  2.28102808  2.24788785  2.2452821   2.26065671  2.2261395
   2.242921    2.2252527   2.23502462  2.23303294  2.26688383  2.24649555
   2.25119357  2.21162765]
 [16.32490203 13.58326376  9.69385394  8.79495701  8.1821581   7.70297713
   7.08029632  6.58555456  6.26731411  5.76445617  5.39387071  5.17534704
   4.77014729  4.01536661  3.67300699  3.47425335  3.32029103  2.89344464
   2.73842725  2.65617116  2.5982542   2.3428739   2.2968381   2.24129699
   2.22768497  2.10175026  2.04445904  2.03979039  2.02814852  1.9809677
   1.98779658  1.95243282  1.95343419  1.94903531  1.9232678   1.87471942
   1.9073358   1.9098364   1.88084534  1.88034156  1.89160183  1.86075668
   1.87510698  1.85924722  1.86754911  1.86678704  1.89497037  1.87527643
   1.88061498  1.84719053]]
valid_acc (5, 50)
valid_AUC (5, 50)
train_acc (50,)
total_train+valid_time 3129.343958363
Start Testing
Load ckpt at ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt
gc 9
Test layer0 Acc 0.8789473684210526, AUC 0.9665971398353577, avg_entr 0.05174867808818817, f1 0.878947377204895
l0_test_time 0.18926680300000953
gc 0
Test layer1 Acc 0.8763157894736842, AUC 0.9540196061134338, avg_entr 0.01811210997402668, f1 0.8763157725334167
l1_test_time 0.23719302099971173
gc 0
Test layer2 Acc 0.8760526315789474, AUC 0.9592211246490479, avg_entr 0.013844617642462254, f1 0.8760526180267334
l2_test_time 0.3078697670002839
gc 0
Test layer3 Acc 0.8760526315789474, AUC 0.9608573317527771, avg_entr 0.012442677281796932, f1 0.8760526180267334
l3_test_time 0.4075005159998
gc 0
Test layer4 Acc 0.8752631578947369, AUC 0.9572266936302185, avg_entr 0.011489124968647957, f1 0.8752631545066833
l4_test_time 0.5343517420001263
gc 0
Test threshold 0.1 Acc 0.8755263157894737, AUC 0.9608814120292664, avg_entr 0.013100843876600266, f1 0.8755263090133667
t0.1_test_time 0.24689050200004203
gc 0
Test threshold 0.2 Acc 0.8773684210526316, AUC 0.9626529216766357, avg_entr 0.018047958612442017, f1 0.8773684501647949
t0.2_test_time 0.24209236300021075
gc 0
Test threshold 0.3 Acc 0.8792105263157894, AUC 0.9653109312057495, avg_entr 0.026392605155706406, f1 0.8792105317115784
t0.3_test_time 0.23300962800021807
gc 0
Test threshold 0.4 Acc 0.8826315789473684, AUC 0.966505765914917, avg_entr 0.032461050897836685, f1 0.8826315999031067
t0.4_test_time 0.21920111399958842
gc 0
Test threshold 0.5 Acc 0.8792105263157894, AUC 0.966579794883728, avg_entr 0.03654863312840462, f1 0.8792105317115784
t0.5_test_time 0.20728421200010416
gc 0
Test threshold 0.6 Acc 0.8789473684210526, AUC 0.9665971398353577, avg_entr 0.03732877969741821, f1 0.878947377204895
t0.6_test_time 0.21402859199997692
gc 0
Test threshold 0.7 Acc 0.8789473684210526, AUC 0.9665971398353577, avg_entr 0.03732877969741821, f1 0.878947377204895
t0.7_test_time 0.20150371499994435
gc 0
Test threshold 0.8 Acc 0.8789473684210526, AUC 0.9665971398353577, avg_entr 0.03732877969741821, f1 0.878947377204895
t0.8_test_time 0.20151374500028396
gc 0
Test threshold 0.9 Acc 0.8789473684210526, AUC 0.9665971398353577, avg_entr 0.03732877969741821, f1 0.878947377204895
t0.9_test_time 0.20184086300014314
