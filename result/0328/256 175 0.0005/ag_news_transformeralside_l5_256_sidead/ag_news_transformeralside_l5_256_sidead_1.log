total count words 102019
vocab size 30000
train size 120000, valid size 3800, test size 3800
found 26754 words in glove
model: TransformerALsideText(
  (layers): ModuleList(
    (0): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=4, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=4, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
  (emb_layers): ModuleList(
    (0): Embedding(30000, 300)
    (1): Embedding(30000, 300)
    (2): Embedding(30000, 300)
    (3): Embedding(30000, 300)
    (4): Embedding(30000, 300)
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.multi_headed_attention.linears.0.weight 90000
layers.0.enc.f.multi_headed_attention.linears.0.bias 300
layers.0.enc.f.multi_headed_attention.linears.1.weight 90000
layers.0.enc.f.multi_headed_attention.linears.1.bias 300
layers.0.enc.f.multi_headed_attention.linears.2.weight 90000
layers.0.enc.f.multi_headed_attention.linears.2.bias 300
layers.0.enc.f.multi_headed_attention.linears.3.weight 90000
layers.0.enc.f.multi_headed_attention.linears.3.bias 300
layers.0.enc.f.feed_forward.w_1.weight 90000
layers.0.enc.f.feed_forward.w_1.bias 300
layers.0.enc.f.feed_forward.w_2.weight 90000
layers.0.enc.f.feed_forward.w_2.bias 300
layers.0.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.0.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.0.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.0.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.0.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.0.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.0.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.0.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.0.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.0.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.0.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.0.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.0.enc.f.encoder.norm.a 300
layers.0.enc.f.encoder.norm.b 300
layers.0.ae.g.0.weight 512
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 512
layers.0.ae.h.0.bias 4
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
emb_layers.0.weight 9000000
emb_layers.1.weight 9000000
emb_layers.2.weight 9000000
emb_layers.3.weight 9000000
emb_layers.4.weight 9000000
Total Trainable Params: 50758892
init_time 15.879625891999998
Start Training
gc 0
Train Epoch0 Acc 0.27726666666666666 (33272/120000), AUC 0.5216280221939087
ep0_train_time 60.96755184599999
Test Epoch0 layer0 Acc 0.781578947368421, AUC 0.9362346529960632, avg_entr 0.7485517859458923, f1 0.7815789580345154
ep0_l0_test_time 0.18513593199999434
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 0
Test Epoch0 layer1 Acc 0.7394736842105263, AUC 0.9368698596954346, avg_entr 0.7510054707527161, f1 0.7394737005233765
ep0_l1_test_time 0.2325862929999971
Test Epoch0 layer2 Acc 0.6907894736842105, AUC 0.9348969459533691, avg_entr 0.8125825524330139, f1 0.6907894611358643
ep0_l2_test_time 0.3033669609999947
Test Epoch0 layer3 Acc 0.3357894736842105, AUC 0.9227290153503418, avg_entr 0.8380793333053589, f1 0.3357894718647003
ep0_l3_test_time 0.4036622179999938
Test Epoch0 layer4 Acc 0.5639473684210526, AUC 0.8948746919631958, avg_entr 1.0992803573608398, f1 0.5639473795890808
ep0_l4_test_time 0.5299832820000034
gc 0
Train Epoch1 Acc 0.74805 (89766/120000), AUC 0.9117248058319092
ep1_train_time 60.56080342000001
Test Epoch1 layer0 Acc 0.8386842105263158, AUC 0.9563493728637695, avg_entr 0.37726208567619324, f1 0.8386842012405396
ep1_l0_test_time 0.1953443449999952
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 1
Test Epoch1 layer1 Acc 0.838421052631579, AUC 0.9592398405075073, avg_entr 0.32207247614860535, f1 0.8384209871292114
ep1_l1_test_time 0.23182324299997958
Test Epoch1 layer2 Acc 0.8365789473684211, AUC 0.9593772888183594, avg_entr 0.31462588906288147, f1 0.8365789651870728
ep1_l2_test_time 0.30455005099997834
Test Epoch1 layer3 Acc 0.8323684210526315, AUC 0.9597134590148926, avg_entr 0.31725674867630005, f1 0.8323684930801392
ep1_l3_test_time 0.40472272799999587
Test Epoch1 layer4 Acc 0.8276315789473684, AUC 0.9599818587303162, avg_entr 0.33367058634757996, f1 0.8276315927505493
ep1_l4_test_time 0.5321678260000056
gc 0
Train Epoch2 Acc 0.8624083333333333 (103489/120000), AUC 0.962007999420166
ep2_train_time 60.69058873
Test Epoch2 layer0 Acc 0.8594736842105263, AUC 0.9637709259986877, avg_entr 0.2348272055387497, f1 0.859473705291748
ep2_l0_test_time 0.18354662400000166
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 2
Test Epoch2 layer1 Acc 0.8597368421052631, AUC 0.9643696546554565, avg_entr 0.15378335118293762, f1 0.8597368597984314
ep2_l1_test_time 0.23167133699999454
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 2
Test Epoch2 layer2 Acc 0.8597368421052631, AUC 0.9654219746589661, avg_entr 0.1476832926273346, f1 0.8597368597984314
ep2_l2_test_time 0.30368865199997686
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 2
Test Epoch2 layer3 Acc 0.8589473684210527, AUC 0.9658564329147339, avg_entr 0.14527183771133423, f1 0.8589473962783813
ep2_l3_test_time 0.4054446880000171
Test Epoch2 layer4 Acc 0.8589473684210527, AUC 0.9653124809265137, avg_entr 0.1450977623462677, f1 0.8589473962783813
ep2_l4_test_time 0.5306809970000188
gc 0
Train Epoch3 Acc 0.8818333333333334 (105820/120000), AUC 0.9693827033042908
ep3_train_time 60.50459783100001
Test Epoch3 layer0 Acc 0.8655263157894737, AUC 0.9661960005760193, avg_entr 0.17550237476825714, f1 0.8655263185501099
ep3_l0_test_time 0.18431410100004086
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 3
Test Epoch3 layer1 Acc 0.865, AUC 0.9666948914527893, avg_entr 0.11838498711585999, f1 0.8650000095367432
ep3_l1_test_time 0.23232372299997905
Test Epoch3 layer2 Acc 0.8647368421052631, AUC 0.967372477054596, avg_entr 0.11340579390525818, f1 0.8647368550300598
ep3_l2_test_time 0.3032281029999808
Test Epoch3 layer3 Acc 0.8626315789473684, AUC 0.9680413603782654, avg_entr 0.11195705085992813, f1 0.862631618976593
ep3_l3_test_time 0.4046627489999537
Test Epoch3 layer4 Acc 0.8605263157894737, AUC 0.9656397104263306, avg_entr 0.11584711819887161, f1 0.8605263233184814
ep3_l4_test_time 0.5308929689999786
gc 0
Train Epoch4 Acc 0.8955166666666666 (107462/120000), AUC 0.973171591758728
ep4_train_time 60.52975880400004
Test Epoch4 layer0 Acc 0.8755263157894737, AUC 0.9687974452972412, avg_entr 0.13952834904193878, f1 0.8755263090133667
ep4_l0_test_time 0.18519990699996924
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 4
Test Epoch4 layer1 Acc 0.8721052631578947, AUC 0.9693167209625244, avg_entr 0.08673406392335892, f1 0.8721052408218384
ep4_l1_test_time 0.23121382599998697
Test Epoch4 layer2 Acc 0.8726315789473684, AUC 0.9712206721305847, avg_entr 0.07978452742099762, f1 0.8726315498352051
ep4_l2_test_time 0.30284824799997523
Test Epoch4 layer3 Acc 0.8721052631578947, AUC 0.9706975221633911, avg_entr 0.0786975547671318, f1 0.8721052408218384
ep4_l3_test_time 0.4042077020000079
Test Epoch4 layer4 Acc 0.8718421052631579, AUC 0.9700165390968323, avg_entr 0.07693006843328476, f1 0.8718421459197998
ep4_l4_test_time 0.5302508049999801
gc 0
Train Epoch5 Acc 0.90595 (108714/120000), AUC 0.9769177436828613
ep5_train_time 60.69752591700001
Test Epoch5 layer0 Acc 0.8736842105263158, AUC 0.9688035845756531, avg_entr 0.12469970434904099, f1 0.8736842274665833
ep5_l0_test_time 0.184909824999977
Test Epoch5 layer1 Acc 0.873421052631579, AUC 0.9688596725463867, avg_entr 0.07454896718263626, f1 0.8734210729598999
ep5_l1_test_time 0.23170373600004268
Test Epoch5 layer2 Acc 0.8723684210526316, AUC 0.9699519276618958, avg_entr 0.06843023002147675, f1 0.8723683953285217
ep5_l2_test_time 0.30259811000001946
Test Epoch5 layer3 Acc 0.871578947368421, AUC 0.9702008962631226, avg_entr 0.06538011133670807, f1 0.8715789318084717
ep5_l3_test_time 0.4042089750000173
Test Epoch5 layer4 Acc 0.871578947368421, AUC 0.969296932220459, avg_entr 0.062419209629297256, f1 0.8715789318084717
ep5_l4_test_time 0.5305754540000294
gc 0
Train Epoch6 Acc 0.9128833333333334 (109546/120000), AUC 0.9792887568473816
ep6_train_time 60.429246062999994
Test Epoch6 layer0 Acc 0.8739473684210526, AUC 0.9676486849784851, avg_entr 0.10897426307201385, f1 0.8739473819732666
ep6_l0_test_time 0.18739006200001995
Test Epoch6 layer1 Acc 0.87, AUC 0.9659489393234253, avg_entr 0.06313320249319077, f1 0.8700000047683716
ep6_l1_test_time 0.23197711200003823
Test Epoch6 layer2 Acc 0.8705263157894737, AUC 0.9694488644599915, avg_entr 0.05582811310887337, f1 0.8705263137817383
ep6_l2_test_time 0.3043630469999812
Test Epoch6 layer3 Acc 0.8702631578947368, AUC 0.9678623676300049, avg_entr 0.05064839869737625, f1 0.8702631592750549
ep6_l3_test_time 0.40726821999999174
Test Epoch6 layer4 Acc 0.8697368421052631, AUC 0.9682970643043518, avg_entr 0.048582326620817184, f1 0.8697368502616882
ep6_l4_test_time 0.5316034820000368
gc 0
Train Epoch7 Acc 0.9205416666666667 (110465/120000), AUC 0.9819819927215576
ep7_train_time 60.477842235000026
Test Epoch7 layer0 Acc 0.876578947368421, AUC 0.968913197517395, avg_entr 0.10248841345310211, f1 0.8765789270401001
ep7_l0_test_time 0.18489172699992196
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 7
Test Epoch7 layer1 Acc 0.8718421052631579, AUC 0.9675931930541992, avg_entr 0.05703546851873398, f1 0.8718421459197998
ep7_l1_test_time 0.23103873900004146
Test Epoch7 layer2 Acc 0.8723684210526316, AUC 0.9687325358390808, avg_entr 0.04905177652835846, f1 0.8723683953285217
ep7_l2_test_time 0.3020133180000357
Test Epoch7 layer3 Acc 0.8723684210526316, AUC 0.9680250883102417, avg_entr 0.04382559284567833, f1 0.8723683953285217
ep7_l3_test_time 0.40405885300003774
Test Epoch7 layer4 Acc 0.8721052631578947, AUC 0.9654425978660583, avg_entr 0.04016067460179329, f1 0.8721052408218384
ep7_l4_test_time 0.5299305450000702
gc 0
Train Epoch8 Acc 0.92695 (111234/120000), AUC 0.9834703207015991
ep8_train_time 60.48339748000001
Test Epoch8 layer0 Acc 0.8789473684210526, AUC 0.9688780307769775, avg_entr 0.09547726809978485, f1 0.878947377204895
ep8_l0_test_time 0.18534405399998377
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 8
Test Epoch8 layer1 Acc 0.8776315789473684, AUC 0.9684030413627625, avg_entr 0.0485934279859066, f1 0.8776316046714783
ep8_l1_test_time 0.23081204700008584
Test Epoch8 layer2 Acc 0.8778947368421053, AUC 0.9688602089881897, avg_entr 0.04107913747429848, f1 0.8778947591781616
ep8_l2_test_time 0.3021843209999133
Test Epoch8 layer3 Acc 0.8778947368421053, AUC 0.969036877155304, avg_entr 0.03782417252659798, f1 0.8778947591781616
ep8_l3_test_time 0.4035639490000449
Test Epoch8 layer4 Acc 0.8778947368421053, AUC 0.9669166207313538, avg_entr 0.03462417051196098, f1 0.8778947591781616
ep8_l4_test_time 0.5303608579999946
gc 0
Train Epoch9 Acc 0.933675 (112041/120000), AUC 0.9870814085006714
ep9_train_time 60.63534240800004
Test Epoch9 layer0 Acc 0.8805263157894737, AUC 0.969085693359375, avg_entr 0.08245924115180969, f1 0.8805263042449951
ep9_l0_test_time 0.18313376899993727
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 9
Test Epoch9 layer1 Acc 0.8802631578947369, AUC 0.9692888259887695, avg_entr 0.03960976377129555, f1 0.880263090133667
ep9_l1_test_time 0.23098540700004833
Test Epoch9 layer2 Acc 0.8797368421052632, AUC 0.9694306254386902, avg_entr 0.03394597768783569, f1 0.8797368407249451
ep9_l2_test_time 0.30201330700003837
Test Epoch9 layer3 Acc 0.8813157894736842, AUC 0.969561755657196, avg_entr 0.03096695803105831, f1 0.8813157677650452
ep9_l3_test_time 0.40358277399991493
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 9
Test Epoch9 layer4 Acc 0.8807894736842106, AUC 0.965950071811676, avg_entr 0.02801862172782421, f1 0.8807894587516785
ep9_l4_test_time 0.533203632999971
gc 0
Train Epoch10 Acc 0.938325 (112599/120000), AUC 0.9882947206497192
ep10_train_time 60.55584134800006
Test Epoch10 layer0 Acc 0.8786842105263157, AUC 0.968436598777771, avg_entr 0.07917090505361557, f1 0.8786842226982117
ep10_l0_test_time 0.18498000199997477
Test Epoch10 layer1 Acc 0.8786842105263157, AUC 0.967668890953064, avg_entr 0.038078125566244125, f1 0.8786842226982117
ep10_l1_test_time 0.23237289000007877
Test Epoch10 layer2 Acc 0.8792105263157894, AUC 0.967103123664856, avg_entr 0.032421380281448364, f1 0.8792105317115784
ep10_l2_test_time 0.3026779389999774
Test Epoch10 layer3 Acc 0.8797368421052632, AUC 0.9676475524902344, avg_entr 0.030117793008685112, f1 0.8797368407249451
ep10_l3_test_time 0.4052251069999784
Test Epoch10 layer4 Acc 0.8789473684210526, AUC 0.9661856293678284, avg_entr 0.02723241224884987, f1 0.878947377204895
ep10_l4_test_time 0.5303235370000721
gc 0
Train Epoch11 Acc 0.9440166666666666 (113282/120000), AUC 0.9896453619003296
ep11_train_time 60.539306963999934
Test Epoch11 layer0 Acc 0.8831578947368421, AUC 0.9692004919052124, avg_entr 0.0724688395857811, f1 0.8831579089164734
ep11_l0_test_time 0.18355211000005056
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 11
Test Epoch11 layer1 Acc 0.8860526315789473, AUC 0.9684239625930786, avg_entr 0.03577675670385361, f1 0.886052668094635
ep11_l1_test_time 0.23239730300008432
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 11
Test Epoch11 layer2 Acc 0.8852631578947369, AUC 0.9703978300094604, avg_entr 0.030165230855345726, f1 0.8852631449699402
ep11_l2_test_time 0.30258767400005127
Test Epoch11 layer3 Acc 0.8852631578947369, AUC 0.970417857170105, avg_entr 0.027921361848711967, f1 0.8852631449699402
ep11_l3_test_time 0.4031781979999778
Test Epoch11 layer4 Acc 0.8855263157894737, AUC 0.964801549911499, avg_entr 0.02491806074976921, f1 0.8855262398719788
ep11_l4_test_time 0.5316106709999531
gc 0
Train Epoch12 Acc 0.9465333333333333 (113584/120000), AUC 0.9897785186767578
ep12_train_time 60.587326437
Test Epoch12 layer0 Acc 0.8778947368421053, AUC 0.9696568846702576, avg_entr 0.07240017503499985, f1 0.8778947591781616
ep12_l0_test_time 0.18461454199996297
Test Epoch12 layer1 Acc 0.8744736842105263, AUC 0.9675175547599792, avg_entr 0.0345141738653183, f1 0.8744736909866333
ep12_l1_test_time 0.23332641300009982
Test Epoch12 layer2 Acc 0.8744736842105263, AUC 0.9686152338981628, avg_entr 0.02943747118115425, f1 0.8744736909866333
ep12_l2_test_time 0.30185878600002525
Test Epoch12 layer3 Acc 0.8742105263157894, AUC 0.966907799243927, avg_entr 0.02701260708272457, f1 0.87421053647995
ep12_l3_test_time 0.40371868199997607
Test Epoch12 layer4 Acc 0.8747368421052631, AUC 0.9640618562698364, avg_entr 0.024335769936442375, f1 0.8747368454933167
ep12_l4_test_time 0.5313025719999587
gc 0
Train Epoch13 Acc 0.9515333333333333 (114184/120000), AUC 0.9921122789382935
ep13_train_time 60.565549792999946
Test Epoch13 layer0 Acc 0.8818421052631579, AUC 0.9695681929588318, avg_entr 0.0671863853931427, f1 0.8818420767784119
ep13_l0_test_time 0.18673394000006738
Test Epoch13 layer1 Acc 0.8794736842105263, AUC 0.9680060744285583, avg_entr 0.030297502875328064, f1 0.8794736862182617
ep13_l1_test_time 0.23139315299999907
Test Epoch13 layer2 Acc 0.8778947368421053, AUC 0.9696345925331116, avg_entr 0.026934891939163208, f1 0.8778947591781616
ep13_l2_test_time 0.3029552919999787
Test Epoch13 layer3 Acc 0.8776315789473684, AUC 0.9687457084655762, avg_entr 0.0250331349670887, f1 0.8776316046714783
ep13_l3_test_time 0.4045400929999232
Test Epoch13 layer4 Acc 0.878421052631579, AUC 0.964820146560669, avg_entr 0.022863123565912247, f1 0.8784210681915283
ep13_l4_test_time 0.5333693939999193
gc 0
Train Epoch14 Acc 0.9554416666666666 (114653/120000), AUC 0.9929798245429993
ep14_train_time 60.766180158
Test Epoch14 layer0 Acc 0.8802631578947369, AUC 0.9704143404960632, avg_entr 0.06944616138935089, f1 0.880263090133667
ep14_l0_test_time 0.18477615200004038
Test Epoch14 layer1 Acc 0.8789473684210526, AUC 0.9659112691879272, avg_entr 0.02776881493628025, f1 0.878947377204895
ep14_l1_test_time 0.231733462999955
Test Epoch14 layer2 Acc 0.88, AUC 0.9683898687362671, avg_entr 0.022869152948260307, f1 0.8799999952316284
ep14_l2_test_time 0.3032501750000165
Test Epoch14 layer3 Acc 0.8810526315789474, AUC 0.9683185815811157, avg_entr 0.021247245371341705, f1 0.8810526132583618
ep14_l3_test_time 0.40388505700002497
Test Epoch14 layer4 Acc 0.8797368421052632, AUC 0.967292308807373, avg_entr 0.019366227090358734, f1 0.8797368407249451
ep14_l4_test_time 0.529382260000034
gc 0
Train Epoch15 Acc 0.9566333333333333 (114796/120000), AUC 0.993065357208252
ep15_train_time 60.612582257999975
Test Epoch15 layer0 Acc 0.88, AUC 0.969811737537384, avg_entr 0.06456437706947327, f1 0.8799999952316284
ep15_l0_test_time 0.18466613400005372
Test Epoch15 layer1 Acc 0.8778947368421053, AUC 0.9652440547943115, avg_entr 0.02462763898074627, f1 0.8778947591781616
ep15_l1_test_time 0.23166796800001066
Test Epoch15 layer2 Acc 0.8773684210526316, AUC 0.968517541885376, avg_entr 0.0206138975918293, f1 0.8773684501647949
ep15_l2_test_time 0.3026911360000213
Test Epoch15 layer3 Acc 0.8778947368421053, AUC 0.9684979319572449, avg_entr 0.019090518355369568, f1 0.8778947591781616
ep15_l3_test_time 0.4029201030000422
Test Epoch15 layer4 Acc 0.8773684210526316, AUC 0.9671386480331421, avg_entr 0.017139211297035217, f1 0.8773684501647949
ep15_l4_test_time 0.5311958490000279
gc 0
Train Epoch16 Acc 0.9604833333333334 (115258/120000), AUC 0.993868350982666
ep16_train_time 60.62555131800002
Test Epoch16 layer0 Acc 0.8821052631578947, AUC 0.9694032073020935, avg_entr 0.062009215354919434, f1 0.88210529088974
ep16_l0_test_time 0.1839145300000382
Test Epoch16 layer1 Acc 0.8805263157894737, AUC 0.9635180234909058, avg_entr 0.025597719475626945, f1 0.8805263042449951
ep16_l1_test_time 0.2313896539999405
Test Epoch16 layer2 Acc 0.8794736842105263, AUC 0.9672765135765076, avg_entr 0.021470965817570686, f1 0.8794736862182617
ep16_l2_test_time 0.3024749100000008
Test Epoch16 layer3 Acc 0.8781578947368421, AUC 0.9660908579826355, avg_entr 0.020286016166210175, f1 0.878157913684845
ep16_l3_test_time 0.40422838399990724
Test Epoch16 layer4 Acc 0.878421052631579, AUC 0.9626772403717041, avg_entr 0.018463624641299248, f1 0.8784210681915283
ep16_l4_test_time 0.530338195000013
gc 0
Train Epoch17 Acc 0.9624416666666666 (115493/120000), AUC 0.9949283599853516
ep17_train_time 60.523846589000186
Test Epoch17 layer0 Acc 0.8807894736842106, AUC 0.968611478805542, avg_entr 0.05825519934296608, f1 0.8807894587516785
ep17_l0_test_time 0.18398709100006272
Test Epoch17 layer1 Acc 0.878421052631579, AUC 0.9636846780776978, avg_entr 0.024284329265356064, f1 0.8784210681915283
ep17_l1_test_time 0.2315540649999548
Test Epoch17 layer2 Acc 0.878421052631579, AUC 0.9675012826919556, avg_entr 0.02035844884812832, f1 0.8784210681915283
ep17_l2_test_time 0.30467288399995596
Test Epoch17 layer3 Acc 0.8786842105263157, AUC 0.9661805629730225, avg_entr 0.019129553809762, f1 0.8786842226982117
ep17_l3_test_time 0.40404247300011775
Test Epoch17 layer4 Acc 0.8789473684210526, AUC 0.9633187651634216, avg_entr 0.01748705469071865, f1 0.878947377204895
ep17_l4_test_time 0.5299127210000734
gc 0
Train Epoch18 Acc 0.9644166666666667 (115730/120000), AUC 0.9951741099357605
ep18_train_time 60.67364015400017
Test Epoch18 layer0 Acc 0.8807894736842106, AUC 0.9690262675285339, avg_entr 0.05634167045354843, f1 0.8807894587516785
ep18_l0_test_time 0.18395017700004246
Test Epoch18 layer1 Acc 0.8773684210526316, AUC 0.963068962097168, avg_entr 0.023658107966184616, f1 0.8773684501647949
ep18_l1_test_time 0.2313927769998827
Test Epoch18 layer2 Acc 0.8778947368421053, AUC 0.967319667339325, avg_entr 0.019579734653234482, f1 0.8778947591781616
ep18_l2_test_time 0.3029040480000731
Test Epoch18 layer3 Acc 0.8778947368421053, AUC 0.9663882255554199, avg_entr 0.018513372167944908, f1 0.8778947591781616
ep18_l3_test_time 0.4047408890000952
Test Epoch18 layer4 Acc 0.8778947368421053, AUC 0.9649965763092041, avg_entr 0.017088931053876877, f1 0.8778947591781616
ep18_l4_test_time 0.5311618939999789
gc 0
Train Epoch19 Acc 0.9706166666666667 (116474/120000), AUC 0.9961942434310913
ep19_train_time 60.531251247
Test Epoch19 layer0 Acc 0.8839473684210526, AUC 0.9680032730102539, avg_entr 0.05389896407723427, f1 0.8839473724365234
ep19_l0_test_time 0.1837494209999022
Test Epoch19 layer1 Acc 0.8755263157894737, AUC 0.9574950933456421, avg_entr 0.02203894965350628, f1 0.8755263090133667
ep19_l1_test_time 0.23193810799989478
Test Epoch19 layer2 Acc 0.8763157894736842, AUC 0.963912844657898, avg_entr 0.018546167761087418, f1 0.8763157725334167
ep19_l2_test_time 0.3024576000000252
Test Epoch19 layer3 Acc 0.8755263157894737, AUC 0.9598105549812317, avg_entr 0.017535824328660965, f1 0.8755263090133667
ep19_l3_test_time 0.4044078019999233
Test Epoch19 layer4 Acc 0.8760526315789474, AUC 0.9545637369155884, avg_entr 0.01601187139749527, f1 0.8760526180267334
ep19_l4_test_time 0.5308882109998194
gc 0
Train Epoch20 Acc 0.9730083333333334 (116761/120000), AUC 0.996528685092926
ep20_train_time 60.491857239999945
Test Epoch20 layer0 Acc 0.8802631578947369, AUC 0.9683066606521606, avg_entr 0.051464542746543884, f1 0.880263090133667
ep20_l0_test_time 0.18469862799997827
Test Epoch20 layer1 Acc 0.8768421052631579, AUC 0.953915536403656, avg_entr 0.020854167640209198, f1 0.8768420815467834
ep20_l1_test_time 0.23255668400020113
Test Epoch20 layer2 Acc 0.8768421052631579, AUC 0.9615179300308228, avg_entr 0.01606433466076851, f1 0.8768420815467834
ep20_l2_test_time 0.30292770699998073
Test Epoch20 layer3 Acc 0.8768421052631579, AUC 0.9607195854187012, avg_entr 0.014913080260157585, f1 0.8768420815467834
ep20_l3_test_time 0.4041580210000575
Test Epoch20 layer4 Acc 0.8771052631578947, AUC 0.9568007588386536, avg_entr 0.013561388477683067, f1 0.8771052360534668
ep20_l4_test_time 0.530305252000062
gc 0
Train Epoch21 Acc 0.9745916666666666 (116951/120000), AUC 0.9969824552536011
ep21_train_time 60.563194329
Test Epoch21 layer0 Acc 0.8823684210526316, AUC 0.9693239331245422, avg_entr 0.05227324739098549, f1 0.8823684453964233
ep21_l0_test_time 0.18469057399988742
Test Epoch21 layer1 Acc 0.8736842105263158, AUC 0.9578232765197754, avg_entr 0.021181399002671242, f1 0.8736842274665833
ep21_l1_test_time 0.23233720400003222
Test Epoch21 layer2 Acc 0.8744736842105263, AUC 0.9637464284896851, avg_entr 0.016632262617349625, f1 0.8744736909866333
ep21_l2_test_time 0.3033811429997968
Test Epoch21 layer3 Acc 0.8744736842105263, AUC 0.9636960029602051, avg_entr 0.015319016762077808, f1 0.8744736909866333
ep21_l3_test_time 0.40385306899997886
Test Epoch21 layer4 Acc 0.8739473684210526, AUC 0.9592741131782532, avg_entr 0.014016333967447281, f1 0.8739473819732666
ep21_l4_test_time 0.5298481160000392
gc 0
Train Epoch22 Acc 0.9757833333333333 (117094/120000), AUC 0.9970428943634033
ep22_train_time 60.589805468999884
Test Epoch22 layer0 Acc 0.8792105263157894, AUC 0.9678110480308533, avg_entr 0.05123939737677574, f1 0.8792105317115784
ep22_l0_test_time 0.18453960199985886
Test Epoch22 layer1 Acc 0.8747368421052631, AUC 0.9551821947097778, avg_entr 0.018175033852458, f1 0.8747368454933167
ep22_l1_test_time 0.23205750500005706
Test Epoch22 layer2 Acc 0.875, AUC 0.9629037380218506, avg_entr 0.014132781885564327, f1 0.875
ep22_l2_test_time 0.3034827750000204
Test Epoch22 layer3 Acc 0.875, AUC 0.9621729850769043, avg_entr 0.013224155642092228, f1 0.875
ep22_l3_test_time 0.4036487240000497
Test Epoch22 layer4 Acc 0.875, AUC 0.9586600661277771, avg_entr 0.011858212761580944, f1 0.875
ep22_l4_test_time 0.5299816700000974
gc 0
Train Epoch23 Acc 0.9786916666666666 (117443/120000), AUC 0.997536838054657
ep23_train_time 60.75178258300002
Test Epoch23 layer0 Acc 0.8786842105263157, AUC 0.9673634171485901, avg_entr 0.04988964647054672, f1 0.8786842226982117
ep23_l0_test_time 0.18468851699981315
Test Epoch23 layer1 Acc 0.8713157894736843, AUC 0.9533913135528564, avg_entr 0.020332500338554382, f1 0.8713157176971436
ep23_l1_test_time 0.23277791399982561
Test Epoch23 layer2 Acc 0.8728947368421053, AUC 0.9591841697692871, avg_entr 0.014796745963394642, f1 0.8728947639465332
ep23_l2_test_time 0.30474891100016066
Test Epoch23 layer3 Acc 0.8728947368421053, AUC 0.9587671756744385, avg_entr 0.013721766881644726, f1 0.8728947639465332
ep23_l3_test_time 0.40479032799999004
Test Epoch23 layer4 Acc 0.8728947368421053, AUC 0.9555412530899048, avg_entr 0.01242017187178135, f1 0.8728947639465332
ep23_l4_test_time 0.530037386999993
gc 0
Train Epoch24 Acc 0.9795 (117540/120000), AUC 0.9977086186408997
ep24_train_time 60.559695637000004
Test Epoch24 layer0 Acc 0.8805263157894737, AUC 0.9672824144363403, avg_entr 0.048101380467414856, f1 0.8805263042449951
ep24_l0_test_time 0.18327761299997292
Test Epoch24 layer1 Acc 0.8768421052631579, AUC 0.9526383280754089, avg_entr 0.016205649822950363, f1 0.8768420815467834
ep24_l1_test_time 0.23100135200002114
Test Epoch24 layer2 Acc 0.8773684210526316, AUC 0.9598821401596069, avg_entr 0.011594926007091999, f1 0.8773684501647949
ep24_l2_test_time 0.30615051199993104
Test Epoch24 layer3 Acc 0.8763157894736842, AUC 0.9587846994400024, avg_entr 0.01099417358636856, f1 0.8763157725334167
ep24_l3_test_time 0.40618148900011875
Test Epoch24 layer4 Acc 0.8773684210526316, AUC 0.9567005634307861, avg_entr 0.010009411722421646, f1 0.8773684501647949
ep24_l4_test_time 0.530811513999879
gc 0
Train Epoch25 Acc 0.9809666666666667 (117716/120000), AUC 0.9979450702667236
ep25_train_time 60.49516514300012
Test Epoch25 layer0 Acc 0.883421052631579, AUC 0.9676031470298767, avg_entr 0.04732082411646843, f1 0.8834210634231567
ep25_l0_test_time 0.1842926560000251
Test Epoch25 layer1 Acc 0.876578947368421, AUC 0.9515291452407837, avg_entr 0.01859068125486374, f1 0.8765789270401001
ep25_l1_test_time 0.2319018059999962
Test Epoch25 layer2 Acc 0.8768421052631579, AUC 0.9586919546127319, avg_entr 0.014597687870264053, f1 0.8768420815467834
ep25_l2_test_time 0.3026042229998893
Test Epoch25 layer3 Acc 0.8757894736842106, AUC 0.9584305882453918, avg_entr 0.01377253420650959, f1 0.87578946352005
ep25_l3_test_time 0.403772330000038
Test Epoch25 layer4 Acc 0.8757894736842106, AUC 0.9540843963623047, avg_entr 0.012397215701639652, f1 0.87578946352005
ep25_l4_test_time 0.5298460379999597
gc 0
Train Epoch26 Acc 0.9810416666666667 (117725/120000), AUC 0.9980747699737549
ep26_train_time 60.59849022499998
Test Epoch26 layer0 Acc 0.8805263157894737, AUC 0.9674680233001709, avg_entr 0.046297963708639145, f1 0.8805263042449951
ep26_l0_test_time 0.18465691200003675
Test Epoch26 layer1 Acc 0.8739473684210526, AUC 0.9528708457946777, avg_entr 0.016386721283197403, f1 0.8739473819732666
ep26_l1_test_time 0.23139340200009428
Test Epoch26 layer2 Acc 0.8744736842105263, AUC 0.9602234363555908, avg_entr 0.012456866912543774, f1 0.8744736909866333
ep26_l2_test_time 0.30224094699997295
Test Epoch26 layer3 Acc 0.8747368421052631, AUC 0.958765983581543, avg_entr 0.011449826881289482, f1 0.8747368454933167
ep26_l3_test_time 0.414773594000053
Test Epoch26 layer4 Acc 0.8747368421052631, AUC 0.9560090899467468, avg_entr 0.010468292981386185, f1 0.8747368454933167
ep26_l4_test_time 0.5332692669999233
gc 0
Train Epoch27 Acc 0.9825916666666666 (117911/120000), AUC 0.9980316758155823
ep27_train_time 60.73651296000003
Test Epoch27 layer0 Acc 0.8805263157894737, AUC 0.9675250053405762, avg_entr 0.046705227345228195, f1 0.8805263042449951
ep27_l0_test_time 0.1846122419999574
Test Epoch27 layer1 Acc 0.8731578947368421, AUC 0.9524717330932617, avg_entr 0.01702689751982689, f1 0.8731579184532166
ep27_l1_test_time 0.2315881920001175
Test Epoch27 layer2 Acc 0.8731578947368421, AUC 0.959058403968811, avg_entr 0.012053772807121277, f1 0.8731579184532166
ep27_l2_test_time 0.3022149180001179
Test Epoch27 layer3 Acc 0.8731578947368421, AUC 0.9584826827049255, avg_entr 0.011256445199251175, f1 0.8731579184532166
ep27_l3_test_time 0.4033540319999247
Test Epoch27 layer4 Acc 0.8728947368421053, AUC 0.9544376134872437, avg_entr 0.010208104737102985, f1 0.8728947639465332
ep27_l4_test_time 0.5296807469999294
gc 0
Train Epoch28 Acc 0.98325 (117990/120000), AUC 0.9982490539550781
ep28_train_time 60.614820137999914
Test Epoch28 layer0 Acc 0.8813157894736842, AUC 0.9671370983123779, avg_entr 0.04544620215892792, f1 0.8813157677650452
ep28_l0_test_time 0.1855407620000733
Test Epoch28 layer1 Acc 0.8747368421052631, AUC 0.9520000219345093, avg_entr 0.0165102556347847, f1 0.8747368454933167
ep28_l1_test_time 0.2314376760000414
Test Epoch28 layer2 Acc 0.8739473684210526, AUC 0.9593698978424072, avg_entr 0.011906740255653858, f1 0.8739473819732666
ep28_l2_test_time 0.302232168999808
Test Epoch28 layer3 Acc 0.8744736842105263, AUC 0.9577951431274414, avg_entr 0.011103780008852482, f1 0.8744736909866333
ep28_l3_test_time 0.4032214979999935
Test Epoch28 layer4 Acc 0.8739473684210526, AUC 0.9544844627380371, avg_entr 0.01026307512074709, f1 0.8739473819732666
ep28_l4_test_time 0.5314623540000412
gc 0
Train Epoch29 Acc 0.9832333333333333 (117988/120000), AUC 0.9982722997665405
ep29_train_time 60.73872702200015
Test Epoch29 layer0 Acc 0.881578947368421, AUC 0.9672487378120422, avg_entr 0.04372108727693558, f1 0.8815789222717285
ep29_l0_test_time 0.18441167800006042
Test Epoch29 layer1 Acc 0.8742105263157894, AUC 0.9518604874610901, avg_entr 0.01539346482604742, f1 0.87421053647995
ep29_l1_test_time 0.23125222999988182
Test Epoch29 layer2 Acc 0.8739473684210526, AUC 0.9597523212432861, avg_entr 0.011435281485319138, f1 0.8739473819732666
ep29_l2_test_time 0.3020403640000495
Test Epoch29 layer3 Acc 0.873421052631579, AUC 0.9577907919883728, avg_entr 0.010738924145698547, f1 0.8734210729598999
ep29_l3_test_time 0.4037761820000014
Test Epoch29 layer4 Acc 0.8739473684210526, AUC 0.9551156759262085, avg_entr 0.009832760319113731, f1 0.8739473819732666
ep29_l4_test_time 0.5327277389999381
gc 0
Train Epoch30 Acc 0.9828916666666667 (117947/120000), AUC 0.9983189702033997
ep30_train_time 60.59014066200007
Test Epoch30 layer0 Acc 0.8794736842105263, AUC 0.9667551517486572, avg_entr 0.04487863928079605, f1 0.8794736862182617
ep30_l0_test_time 0.18369795800003885
Test Epoch30 layer1 Acc 0.8728947368421053, AUC 0.949806809425354, avg_entr 0.016089223325252533, f1 0.8728947639465332
ep30_l1_test_time 0.2314718099999027
Test Epoch30 layer2 Acc 0.8726315789473684, AUC 0.9567920565605164, avg_entr 0.01178132463246584, f1 0.8726315498352051
ep30_l2_test_time 0.30248455500009186
Test Epoch30 layer3 Acc 0.8726315789473684, AUC 0.9567962884902954, avg_entr 0.011119208298623562, f1 0.8726315498352051
ep30_l3_test_time 0.4041868080000768
Test Epoch30 layer4 Acc 0.8726315789473684, AUC 0.9527993202209473, avg_entr 0.01022313255816698, f1 0.8726315498352051
ep30_l4_test_time 0.5300765120000506
gc 0
Train Epoch31 Acc 0.9844833333333334 (118138/120000), AUC 0.998310923576355
ep31_train_time 60.58596307500011
Test Epoch31 layer0 Acc 0.8810526315789474, AUC 0.9668036103248596, avg_entr 0.04380613565444946, f1 0.8810526132583618
ep31_l0_test_time 0.18432174500003384
Test Epoch31 layer1 Acc 0.873421052631579, AUC 0.9517285823822021, avg_entr 0.015440762042999268, f1 0.8734210729598999
ep31_l1_test_time 0.23111052100011875
Test Epoch31 layer2 Acc 0.8728947368421053, AUC 0.9587819576263428, avg_entr 0.010649657808244228, f1 0.8728947639465332
ep31_l2_test_time 0.30215147200010506
Test Epoch31 layer3 Acc 0.873421052631579, AUC 0.958602786064148, avg_entr 0.009716718457639217, f1 0.8734210729598999
ep31_l3_test_time 0.402740976999894
Test Epoch31 layer4 Acc 0.8731578947368421, AUC 0.9543023109436035, avg_entr 0.008803184144198895, f1 0.8731579184532166
ep31_l4_test_time 0.5286763399999472
gc 0
Train Epoch32 Acc 0.9843333333333333 (118120/120000), AUC 0.9984539151191711
ep32_train_time 60.637140843000225
Test Epoch32 layer0 Acc 0.8802631578947369, AUC 0.9667232036590576, avg_entr 0.0434296540915966, f1 0.880263090133667
ep32_l0_test_time 0.18627792999996018
Test Epoch32 layer1 Acc 0.8736842105263158, AUC 0.9519855976104736, avg_entr 0.014653363265097141, f1 0.8736842274665833
ep32_l1_test_time 0.23217502400029844
Test Epoch32 layer2 Acc 0.8736842105263158, AUC 0.9587825536727905, avg_entr 0.00974266231060028, f1 0.8736842274665833
ep32_l2_test_time 0.3021955650001473
Test Epoch32 layer3 Acc 0.875, AUC 0.9579962491989136, avg_entr 0.008829490281641483, f1 0.875
ep32_l3_test_time 0.4036226279999937
Test Epoch32 layer4 Acc 0.8742105263157894, AUC 0.9543895125389099, avg_entr 0.007838720455765724, f1 0.87421053647995
ep32_l4_test_time 0.5312660399999913
gc 0
Train Epoch33 Acc 0.9849333333333333 (118192/120000), AUC 0.9985389709472656
ep33_train_time 60.54794469999979
Test Epoch33 layer0 Acc 0.8810526315789474, AUC 0.9669516086578369, avg_entr 0.04267025738954544, f1 0.8810526132583618
ep33_l0_test_time 0.18390724800019598
Test Epoch33 layer1 Acc 0.8752631578947369, AUC 0.9508312940597534, avg_entr 0.014647865667939186, f1 0.8752631545066833
ep33_l1_test_time 0.23148932300000524
Test Epoch33 layer2 Acc 0.875, AUC 0.9585071802139282, avg_entr 0.01017465814948082, f1 0.875
ep33_l2_test_time 0.3019452199996522
Test Epoch33 layer3 Acc 0.8747368421052631, AUC 0.9575961828231812, avg_entr 0.009415135718882084, f1 0.8747368454933167
ep33_l3_test_time 0.4026375840003311
Test Epoch33 layer4 Acc 0.875, AUC 0.9552565813064575, avg_entr 0.008466974832117558, f1 0.875
ep33_l4_test_time 0.5299530129996128
gc 0
Train Epoch34 Acc 0.9846166666666667 (118154/120000), AUC 0.998651921749115
ep34_train_time 60.56636675899972
Test Epoch34 layer0 Acc 0.881578947368421, AUC 0.9664530754089355, avg_entr 0.04218559339642525, f1 0.8815789222717285
ep34_l0_test_time 0.18910333500025445
Test Epoch34 layer1 Acc 0.8739473684210526, AUC 0.9513108730316162, avg_entr 0.014653582125902176, f1 0.8739473819732666
ep34_l1_test_time 0.23346150399993348
Test Epoch34 layer2 Acc 0.8739473684210526, AUC 0.9573895931243896, avg_entr 0.01042686216533184, f1 0.8739473819732666
ep34_l2_test_time 0.3024259929998152
Test Epoch34 layer3 Acc 0.873421052631579, AUC 0.9559638500213623, avg_entr 0.009638611227273941, f1 0.8734210729598999
ep34_l3_test_time 0.40441406599984475
Test Epoch34 layer4 Acc 0.873421052631579, AUC 0.9525792598724365, avg_entr 0.008937256410717964, f1 0.8734210729598999
ep34_l4_test_time 0.5304822819998662
gc 0
Train Epoch35 Acc 0.98535 (118242/120000), AUC 0.9986244440078735
ep35_train_time 60.614546595999855
Test Epoch35 layer0 Acc 0.881578947368421, AUC 0.9665922522544861, avg_entr 0.041864629834890366, f1 0.8815789222717285
ep35_l0_test_time 0.18388437100020383
Test Epoch35 layer1 Acc 0.8736842105263158, AUC 0.9502158164978027, avg_entr 0.014236357994377613, f1 0.8736842274665833
ep35_l1_test_time 0.2317394649999187
Test Epoch35 layer2 Acc 0.8739473684210526, AUC 0.9573736190795898, avg_entr 0.010081981308758259, f1 0.8739473819732666
ep35_l2_test_time 0.30226018600023963
Test Epoch35 layer3 Acc 0.8731578947368421, AUC 0.9550513625144958, avg_entr 0.009417419321835041, f1 0.8731579184532166
ep35_l3_test_time 0.403529071000321
Test Epoch35 layer4 Acc 0.8739473684210526, AUC 0.9524365663528442, avg_entr 0.00861825980246067, f1 0.8739473819732666
ep35_l4_test_time 0.5297162589999971
gc 0
Train Epoch36 Acc 0.9849833333333333 (118198/120000), AUC 0.9985432028770447
ep36_train_time 60.49544331800007
Test Epoch36 layer0 Acc 0.8813157894736842, AUC 0.9662705659866333, avg_entr 0.04159026965498924, f1 0.8813157677650452
ep36_l0_test_time 0.1861287030001222
Test Epoch36 layer1 Acc 0.8739473684210526, AUC 0.9502288103103638, avg_entr 0.015012238174676895, f1 0.8739473819732666
ep36_l1_test_time 0.2313070370000787
Test Epoch36 layer2 Acc 0.8742105263157894, AUC 0.9570369720458984, avg_entr 0.010245447047054768, f1 0.87421053647995
ep36_l2_test_time 0.3021960380001474
Test Epoch36 layer3 Acc 0.8728947368421053, AUC 0.9557198286056519, avg_entr 0.009247574023902416, f1 0.8728947639465332
ep36_l3_test_time 0.4030922320002901
Test Epoch36 layer4 Acc 0.873421052631579, AUC 0.9526631236076355, avg_entr 0.008457053452730179, f1 0.8734210729598999
ep36_l4_test_time 0.5305027919998793
gc 0
Train Epoch37 Acc 0.98555 (118266/120000), AUC 0.9985501766204834
ep37_train_time 60.54818950100025
Test Epoch37 layer0 Acc 0.8818421052631579, AUC 0.9663197994232178, avg_entr 0.04121623933315277, f1 0.8818420767784119
ep37_l0_test_time 0.18436690099997577
Test Epoch37 layer1 Acc 0.8731578947368421, AUC 0.949694037437439, avg_entr 0.014523935504257679, f1 0.8731579184532166
ep37_l1_test_time 0.23076759800005675
Test Epoch37 layer2 Acc 0.8728947368421053, AUC 0.957800030708313, avg_entr 0.010239308699965477, f1 0.8728947639465332
ep37_l2_test_time 0.30201546499984033
Test Epoch37 layer3 Acc 0.873421052631579, AUC 0.9561599493026733, avg_entr 0.009448682889342308, f1 0.8734210729598999
ep37_l3_test_time 0.4024445119998745
Test Epoch37 layer4 Acc 0.873421052631579, AUC 0.9524495601654053, avg_entr 0.00859005842357874, f1 0.8734210729598999
ep37_l4_test_time 0.5291692339997098
gc 0
Train Epoch38 Acc 0.9855083333333333 (118261/120000), AUC 0.9985401630401611
ep38_train_time 60.484148930000174
Test Epoch38 layer0 Acc 0.8818421052631579, AUC 0.9663260579109192, avg_entr 0.04083877429366112, f1 0.8818420767784119
ep38_l0_test_time 0.1849505219997809
Test Epoch38 layer1 Acc 0.8742105263157894, AUC 0.9496455788612366, avg_entr 0.013784611597657204, f1 0.87421053647995
ep38_l1_test_time 0.23148577000029036
Test Epoch38 layer2 Acc 0.8747368421052631, AUC 0.957737922668457, avg_entr 0.009834114462137222, f1 0.8747368454933167
ep38_l2_test_time 0.3022787449999669
Test Epoch38 layer3 Acc 0.8744736842105263, AUC 0.9549659490585327, avg_entr 0.009240305051207542, f1 0.8744736909866333
ep38_l3_test_time 0.4028029470000547
Test Epoch38 layer4 Acc 0.8747368421052631, AUC 0.9514973163604736, avg_entr 0.008427727967500687, f1 0.8747368454933167
ep38_l4_test_time 0.5294952029998967
gc 0
Train Epoch39 Acc 0.9855416666666666 (118265/120000), AUC 0.9985967874526978
ep39_train_time 60.679748820999976
Test Epoch39 layer0 Acc 0.881578947368421, AUC 0.9663039445877075, avg_entr 0.0409504733979702, f1 0.8815789222717285
ep39_l0_test_time 0.18532927999967796
Test Epoch39 layer1 Acc 0.8728947368421053, AUC 0.9497343301773071, avg_entr 0.013966293074190617, f1 0.8728947639465332
ep39_l1_test_time 0.23198114100023304
Test Epoch39 layer2 Acc 0.8726315789473684, AUC 0.9574434161186218, avg_entr 0.00983493123203516, f1 0.8726315498352051
ep39_l2_test_time 0.3027969469999334
Test Epoch39 layer3 Acc 0.8731578947368421, AUC 0.9553075432777405, avg_entr 0.009344615042209625, f1 0.8731579184532166
ep39_l3_test_time 0.4031065590002072
Test Epoch39 layer4 Acc 0.873421052631579, AUC 0.952583909034729, avg_entr 0.00853883195668459, f1 0.8734210729598999
ep39_l4_test_time 0.5298135279999769
gc 0
Train Epoch40 Acc 0.98575 (118290/120000), AUC 0.9986003637313843
ep40_train_time 60.57151435199967
Test Epoch40 layer0 Acc 0.881578947368421, AUC 0.9662521481513977, avg_entr 0.04054512828588486, f1 0.8815789222717285
ep40_l0_test_time 0.1852459890001228
Test Epoch40 layer1 Acc 0.873421052631579, AUC 0.9502177834510803, avg_entr 0.013943365775048733, f1 0.8734210729598999
ep40_l1_test_time 0.23152690100005202
Test Epoch40 layer2 Acc 0.8744736842105263, AUC 0.9571855664253235, avg_entr 0.009370701387524605, f1 0.8744736909866333
ep40_l2_test_time 0.30243176700014374
Test Epoch40 layer3 Acc 0.8736842105263158, AUC 0.9560399055480957, avg_entr 0.008826639503240585, f1 0.8736842274665833
ep40_l3_test_time 0.40298983599996063
Test Epoch40 layer4 Acc 0.8736842105263158, AUC 0.9524068236351013, avg_entr 0.007895143702626228, f1 0.8736842274665833
ep40_l4_test_time 0.5299952090003899
gc 0
Train Epoch41 Acc 0.9857833333333333 (118294/120000), AUC 0.9986082315444946
ep41_train_time 60.661562544999924
Test Epoch41 layer0 Acc 0.8818421052631579, AUC 0.9661561250686646, avg_entr 0.0403921864926815, f1 0.8818420767784119
ep41_l0_test_time 0.18394436799962932
Test Epoch41 layer1 Acc 0.8736842105263158, AUC 0.9503740072250366, avg_entr 0.014483379200100899, f1 0.8736842274665833
ep41_l1_test_time 0.2305710519999593
Test Epoch41 layer2 Acc 0.8731578947368421, AUC 0.9570808410644531, avg_entr 0.010191014967858791, f1 0.8731579184532166
ep41_l2_test_time 0.30203451999977915
Test Epoch41 layer3 Acc 0.8728947368421053, AUC 0.9557986259460449, avg_entr 0.009692206047475338, f1 0.8728947639465332
ep41_l3_test_time 0.4027131449997796
Test Epoch41 layer4 Acc 0.8728947368421053, AUC 0.951291561126709, avg_entr 0.008862209506332874, f1 0.8728947639465332
ep41_l4_test_time 0.528623128000163
gc 0
Train Epoch42 Acc 0.9857416666666666 (118289/120000), AUC 0.998640239238739
ep42_train_time 60.45201854299967
Test Epoch42 layer0 Acc 0.8810526315789474, AUC 0.9662315249443054, avg_entr 0.039933159947395325, f1 0.8810526132583618
ep42_l0_test_time 0.18391653799972119
Test Epoch42 layer1 Acc 0.8731578947368421, AUC 0.9500910639762878, avg_entr 0.014306477271020412, f1 0.8731579184532166
ep42_l1_test_time 0.23145802899989576
Test Epoch42 layer2 Acc 0.8736842105263158, AUC 0.9578521251678467, avg_entr 0.009977332316339016, f1 0.8736842274665833
ep42_l2_test_time 0.30238573899987387
Test Epoch42 layer3 Acc 0.8726315789473684, AUC 0.9560161232948303, avg_entr 0.009448024444282055, f1 0.8726315498352051
ep42_l3_test_time 0.4036728569999468
Test Epoch42 layer4 Acc 0.8731578947368421, AUC 0.9516310095787048, avg_entr 0.008550100028514862, f1 0.8731579184532166
ep42_l4_test_time 0.5298154500001147
gc 0
Train Epoch43 Acc 0.9859583333333334 (118315/120000), AUC 0.9986231327056885
ep43_train_time 60.52225450199967
Test Epoch43 layer0 Acc 0.8810526315789474, AUC 0.9661933183670044, avg_entr 0.0397883877158165, f1 0.8810526132583618
ep43_l0_test_time 0.18476799400013988
Test Epoch43 layer1 Acc 0.8742105263157894, AUC 0.9502695798873901, avg_entr 0.014059212990105152, f1 0.87421053647995
ep43_l1_test_time 0.23126066199984052
Test Epoch43 layer2 Acc 0.875, AUC 0.9576212763786316, avg_entr 0.009510909207165241, f1 0.875
ep43_l2_test_time 0.30158839299974716
Test Epoch43 layer3 Acc 0.8747368421052631, AUC 0.9561692476272583, avg_entr 0.009016897529363632, f1 0.8747368454933167
ep43_l3_test_time 0.4024234290000095
Test Epoch43 layer4 Acc 0.8744736842105263, AUC 0.9514026045799255, avg_entr 0.008190151304006577, f1 0.8744736909866333
ep43_l4_test_time 0.529290093000327
gc 0
Train Epoch44 Acc 0.9861166666666666 (118334/120000), AUC 0.9986323118209839
ep44_train_time 60.60392376699974
Test Epoch44 layer0 Acc 0.8813157894736842, AUC 0.9660986661911011, avg_entr 0.039456047117710114, f1 0.8813157677650452
ep44_l0_test_time 0.18437889599999835
Test Epoch44 layer1 Acc 0.8739473684210526, AUC 0.9504134058952332, avg_entr 0.014024605043232441, f1 0.8739473819732666
ep44_l1_test_time 0.23186036500010232
Test Epoch44 layer2 Acc 0.8739473684210526, AUC 0.9573655128479004, avg_entr 0.009477971121668816, f1 0.8739473819732666
ep44_l2_test_time 0.3024182590002056
Test Epoch44 layer3 Acc 0.8736842105263158, AUC 0.9554319381713867, avg_entr 0.008984616957604885, f1 0.8736842274665833
ep44_l3_test_time 0.40408986600004937
Test Epoch44 layer4 Acc 0.8739473684210526, AUC 0.9509560465812683, avg_entr 0.008178558200597763, f1 0.8739473819732666
ep44_l4_test_time 0.5295238620001328
gc 0
Train Epoch45 Acc 0.9857833333333333 (118294/120000), AUC 0.9986651539802551
ep45_train_time 60.42533189000005
Test Epoch45 layer0 Acc 0.8813157894736842, AUC 0.9661602973937988, avg_entr 0.03953877091407776, f1 0.8813157677650452
ep45_l0_test_time 0.18721174600023005
Test Epoch45 layer1 Acc 0.8736842105263158, AUC 0.9500843286514282, avg_entr 0.01374478917568922, f1 0.8736842274665833
ep45_l1_test_time 0.23180295800011663
Test Epoch45 layer2 Acc 0.8744736842105263, AUC 0.9572285413742065, avg_entr 0.0090605104342103, f1 0.8744736909866333
ep45_l2_test_time 0.30166307400031656
Test Epoch45 layer3 Acc 0.8739473684210526, AUC 0.9557654857635498, avg_entr 0.00859494786709547, f1 0.8739473819732666
ep45_l3_test_time 0.40254039499995997
Test Epoch45 layer4 Acc 0.875, AUC 0.9512352347373962, avg_entr 0.007793811149895191, f1 0.875
ep45_l4_test_time 0.5303937389999192
gc 0
Train Epoch46 Acc 0.9861166666666666 (118334/120000), AUC 0.998619556427002
ep46_train_time 60.59769530600033
Test Epoch46 layer0 Acc 0.881578947368421, AUC 0.9661036133766174, avg_entr 0.03934865817427635, f1 0.8815789222717285
ep46_l0_test_time 0.1839004409998779
Test Epoch46 layer1 Acc 0.8742105263157894, AUC 0.949948787689209, avg_entr 0.013662915676832199, f1 0.87421053647995
ep46_l1_test_time 0.23190606000025582
Test Epoch46 layer2 Acc 0.8739473684210526, AUC 0.9569313526153564, avg_entr 0.009091700427234173, f1 0.8739473819732666
ep46_l2_test_time 0.30337350499985405
Test Epoch46 layer3 Acc 0.8739473684210526, AUC 0.9552196860313416, avg_entr 0.008614963851869106, f1 0.8739473819732666
ep46_l3_test_time 0.40519315599976835
Test Epoch46 layer4 Acc 0.8744736842105263, AUC 0.9513083696365356, avg_entr 0.00781610980629921, f1 0.8744736909866333
ep46_l4_test_time 0.5290144310001779
gc 0
Train Epoch47 Acc 0.9859166666666667 (118310/120000), AUC 0.9987055063247681
ep47_train_time 60.48802614199985
Test Epoch47 layer0 Acc 0.8813157894736842, AUC 0.9661183953285217, avg_entr 0.03907611966133118, f1 0.8813157677650452
ep47_l0_test_time 0.18457050300003175
Test Epoch47 layer1 Acc 0.8736842105263158, AUC 0.9502570629119873, avg_entr 0.013714258559048176, f1 0.8736842274665833
ep47_l1_test_time 0.23143755299997792
Test Epoch47 layer2 Acc 0.8744736842105263, AUC 0.9571435451507568, avg_entr 0.009253380820155144, f1 0.8744736909866333
ep47_l2_test_time 0.30172946399989087
Test Epoch47 layer3 Acc 0.8742105263157894, AUC 0.9551819562911987, avg_entr 0.00869794562458992, f1 0.87421053647995
ep47_l3_test_time 0.40273282299995117
Test Epoch47 layer4 Acc 0.8744736842105263, AUC 0.9520019292831421, avg_entr 0.007912161760032177, f1 0.8744736909866333
ep47_l4_test_time 0.529586210999696
gc 0
Train Epoch48 Acc 0.9860166666666667 (118322/120000), AUC 0.9987169504165649
ep48_train_time 60.58781996200014
Test Epoch48 layer0 Acc 0.8813157894736842, AUC 0.9661935567855835, avg_entr 0.03922693431377411, f1 0.8813157677650452
ep48_l0_test_time 0.19111329599991222
Test Epoch48 layer1 Acc 0.8739473684210526, AUC 0.949860692024231, avg_entr 0.01375561486929655, f1 0.8739473819732666
ep48_l1_test_time 0.23221810999984882
Test Epoch48 layer2 Acc 0.8742105263157894, AUC 0.9572920799255371, avg_entr 0.009192156605422497, f1 0.87421053647995
ep48_l2_test_time 0.30296158999999534
Test Epoch48 layer3 Acc 0.8747368421052631, AUC 0.9556375741958618, avg_entr 0.008732777088880539, f1 0.8747368454933167
ep48_l3_test_time 0.403325870000117
Test Epoch48 layer4 Acc 0.8747368421052631, AUC 0.9524237513542175, avg_entr 0.007894453592598438, f1 0.8747368454933167
ep48_l4_test_time 0.5303665870001169
gc 0
Train Epoch49 Acc 0.98585 (118302/120000), AUC 0.9986701607704163
ep49_train_time 60.47467245100006
Test Epoch49 layer0 Acc 0.8813157894736842, AUC 0.9661507606506348, avg_entr 0.03899357467889786, f1 0.8813157677650452
ep49_l0_test_time 0.18398804400021618
Test Epoch49 layer1 Acc 0.8739473684210526, AUC 0.9500935673713684, avg_entr 0.013670065440237522, f1 0.8739473819732666
ep49_l1_test_time 0.23127479999993739
Test Epoch49 layer2 Acc 0.8744736842105263, AUC 0.9572362899780273, avg_entr 0.009166266769170761, f1 0.8744736909866333
ep49_l2_test_time 0.3017889840002681
Test Epoch49 layer3 Acc 0.8744736842105263, AUC 0.955220639705658, avg_entr 0.008709694258868694, f1 0.8744736909866333
ep49_l3_test_time 0.403479556000093
Test Epoch49 layer4 Acc 0.875, AUC 0.952720046043396, avg_entr 0.007881179451942444, f1 0.875
ep49_l4_test_time 0.5291151080000418
Best AUC tensor(0.8861) 11 1
train_as_loss [[4.55760142e+02 3.56652763e+02 3.51075112e+02 3.49837156e+02
  3.49367861e+02 3.49141956e+02 3.49017348e+02 3.48942325e+02
  3.48894366e+02 3.48862345e+02 3.48840271e+02 3.48824677e+02
  3.48813466e+02 3.48805279e+02 3.48799233e+02 3.48794726e+02
  3.48791342e+02 3.48788803e+02 3.48786858e+02 3.48785677e+02
  3.48784989e+02 3.48784351e+02 3.48783716e+02 3.48783312e+02
  3.48782928e+02 3.48782763e+02 3.48782461e+02 3.48782152e+02
  3.48781991e+02 3.48781904e+02 3.48781841e+02 3.48781764e+02
  3.48781646e+02 3.48781552e+02 3.48781298e+02 3.48781235e+02
  3.48781136e+02 3.48781084e+02 3.48781064e+02 3.48781029e+02
  3.48781027e+02 3.48781002e+02 3.48780982e+02 3.48780972e+02
  3.48780972e+02 3.48780972e+02 3.48780969e+02 3.48780929e+02
  3.48780916e+02 3.48780916e+02]
 [2.06137670e+00 1.83933715e-05 1.38599910e-06 2.80564420e-07
  8.52102814e-08 3.21251571e-08 1.43954055e-08 1.13597244e-05
  5.35980997e-05 7.02225730e-06 4.09810999e-09 2.65622333e-09
  6.64806047e-05 1.95256052e-07 2.03677037e-09 1.43409887e-09
  9.10679115e-06 1.21146432e-07 1.20484148e-09 1.08666856e-07
  9.79713393e-06 4.77008893e-08 4.87978986e-10 2.64152424e-10
  9.97673172e-07 4.71451667e-08 2.69706976e-10 2.18374651e-10
  4.47352410e-07 9.75742697e-10 1.96894257e-10 1.68277739e-10
  1.02021708e-07 8.55485287e-10 1.55404971e-10 1.27687197e-10
  1.88384431e-08 2.78300261e-10 1.11864028e-10 8.91733397e-11
  1.41920157e-10 9.21032676e-11 8.59890986e-11 6.09188356e-11
  6.25162373e-11 6.44801954e-11 6.16297575e-11 3.79629347e-11
  3.35735984e-11 5.00069817e-11]
 [1.99810055e+00 1.56419750e-05 1.25356865e-06 2.69045531e-07
  8.66748184e-08 3.59308168e-08 1.90715566e-08 1.55673459e-05
  1.21838394e-04 1.86331021e-05 6.86608307e-09 6.63228050e-09
  1.97778065e-04 2.35552241e-06 3.61747294e-09 3.65020354e-09
  2.48793602e-05 3.95317054e-07 2.78894384e-09 1.33608794e-07
  2.55246959e-05 2.82443069e-07 1.07768398e-09 9.87990889e-10
  1.71453421e-06 6.34103608e-08 7.40061758e-10 9.41474491e-10
  9.39905541e-07 2.92848072e-09 5.75246119e-10 7.49467623e-10
  2.15702096e-07 1.19677020e-09 4.95818594e-10 5.93400286e-10
  4.00886113e-08 6.72222280e-10 3.55837828e-10 4.23127188e-10
  7.25833208e-10 2.80174039e-10 3.01194806e-10 3.07642796e-10
  3.39272989e-10 1.70412283e-10 1.74646669e-10 1.50992321e-10
  1.32148774e-10 9.25185350e-11]
 [2.06696287e+00 1.44284895e-05 1.23115703e-06 3.12367991e-07
  1.24259101e-07 6.18601329e-08 4.32852760e-08 1.92550515e-05
  1.66055492e-04 2.18431186e-05 2.02707398e-08 2.16766705e-08
  2.89812841e-04 6.12987311e-06 1.11766915e-08 1.42640605e-08
  3.98970095e-05 5.34573548e-07 1.05288278e-08 1.74476560e-07
  4.05114091e-05 1.09035640e-06 3.32931744e-09 4.08262050e-09
  2.12700095e-06 8.77777575e-08 2.76028319e-09 4.27948690e-09
  1.24999426e-06 6.96247313e-09 2.46385464e-09 3.62721809e-09
  3.35375296e-07 2.39741535e-09 2.00103283e-09 2.72303106e-09
  6.05093452e-08 1.73381525e-09 1.40610010e-09 2.01416008e-09
  3.35227826e-09 9.74905270e-10 1.22446861e-09 1.29208213e-09
  1.35022171e-09 5.13377390e-10 5.43032945e-10 5.30757074e-10
  4.64457698e-10 2.07540981e-10]
 [2.04132049e+00 1.45622194e-05 1.40516173e-06 4.93551294e-07
  2.72104141e-07 1.52548281e-07 1.40857900e-07 2.36319725e-05
  2.13306245e-04 2.83699827e-05 7.82249492e-08 9.67825340e-08
  3.97727897e-04 1.56304714e-05 4.39879698e-08 5.86970786e-08
  5.26934869e-05 7.39668963e-07 4.36750178e-08 1.91968876e-07
  5.75576044e-05 1.93851764e-06 1.07983052e-08 1.36784138e-08
  2.53093438e-06 1.02922908e-07 9.06749371e-09 1.41108819e-08
  1.60602708e-06 1.35928108e-08 8.01930050e-09 1.28004414e-08
  4.64093417e-07 5.34944881e-09 6.40561547e-09 9.16440426e-09
  8.82287765e-08 3.88991927e-09 4.17437951e-09 5.82484055e-09
  8.78364553e-09 2.33993341e-09 3.36615143e-09 3.36643282e-09
  3.35630948e-09 1.12525881e-09 1.23596532e-09 1.31488539e-09
  1.08037049e-09 4.07016172e-10]]
train_ae_loss [[13.20551912 12.05967292 12.06140293 11.60039601 11.28770246 10.97674855
  10.7710894  10.49694956 10.32281436 10.08019126 10.00674097  9.82218995
   9.72966322  9.57644841  9.43647262  9.37575755  9.2233721   9.19070403
   9.09754977  8.23997537  7.95004411  7.84110749  7.76876593  7.33275004
   7.20543668  7.15937342  7.13201991  6.87814601  6.86394104  6.80077965
   6.8108536   6.74539835  6.6899963   6.71035907  6.69370696  6.69238169
   6.67321598  6.67885757  6.68216302  6.69691546  6.69975116  6.70602598
   6.73194172  6.72039272  6.73190543  6.76709124  6.76297646  6.76960019
   6.76903536  6.79533178]
 [14.96599205 12.77824331 11.13739616 10.36711393  9.85105482  9.29805416
   8.84773934  8.34507604  7.93805493  7.32279696  7.07807855  6.73406588
   6.49346499  6.12012345  5.81014953  5.63717058  5.36801201  5.19577828
   5.02870063  4.11538317  3.76353257  3.64987241  3.45968884  3.0724659
   2.9682859   2.84435508  2.80486017  2.59824006  2.53375831  2.49976744
   2.44441052  2.40481881  2.3383767   2.35595887  2.28633541  2.29047221
   2.25762026  2.22776066  2.22123878  2.22936974  2.23906697  2.2044441
   2.25016911  2.21674038  2.19463887  2.21597716  2.20892444  2.21994256
   2.20342307  2.20517467]
 [16.21342797 12.44992943 10.12211966  9.30914503  8.72068456  8.10211551
   7.62735696  7.13324782  6.74971491  6.12687015  5.8919543   5.56716221
   5.34492505  4.98430841  4.69383244  4.529427    4.29218077  4.12439536
   3.97090212  3.21091895  2.91558827  2.82003708  2.65285354  2.33062377
   2.25289239  2.14466825  2.10606031  1.93826914  1.88150965  1.8554849
   1.80868279  1.77985072  1.71295171  1.73440452  1.66929668  1.67693758
   1.64636531  1.62431362  1.61247336  1.61624111  1.62267454  1.59746782
   1.62764705  1.59982347  1.58258812  1.60115014  1.59691546  1.60658405
   1.58661235  1.58437692]
 [16.47983733 12.537151    9.49989857  8.70090979  8.15350271  7.53885836
   7.06271612  6.56770737  6.206964    5.57514173  5.35817264  5.0561783
   4.86371496  4.49395202  4.22716832  4.07943246  3.85542306  3.69115128
   3.54599613  2.86474903  2.60032301  2.51012924  2.35800367  2.06860751
   2.00095923  1.89944088  1.86312327  1.71420298  1.66307128  1.63754676
   1.59699277  1.56998912  1.51027365  1.52842965  1.46971746  1.47559557
   1.44933452  1.42925635  1.41643968  1.41815054  1.42609903  1.40276893
   1.42961648  1.403315    1.38812964  1.40499024  1.40031829  1.40862576
   1.39169756  1.38916528]
 [16.50845673 13.50558622  8.96825286  8.11698384  7.57750214  6.97639321
   6.52117832  6.0234402   5.68246874  5.02502403  4.8238037   4.53880962
   4.37103171  3.98806649  3.74444876  3.61338735  3.40735184  3.24967257
   3.11454441  2.51376314  2.28314677  2.19836911  2.06312641  1.8122244
   1.75070763  1.6602256   1.62656955  1.49736563  1.45224029  1.42781446
   1.39043254  1.36725849  1.31461656  1.32998126  1.27855305  1.28406351
   1.26045688  1.24336481  1.23052254  1.2327485   1.23770821  1.21779982
   1.24010775  1.21736893  1.20341912  1.21715483  1.21368012  1.2203418
   1.20606376  1.2036779 ]]
valid_acc (5, 50)
valid_AUC (5, 50)
train_acc (50,)
total_train+valid_time 3117.723031174
Start Testing
Load ckpt at ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt
gc 9
Test layer0 Acc 0.873421052631579, AUC 0.9658725261688232, avg_entr 0.07639506459236145, f1 0.8734210729598999
l0_test_time 0.18391826700008096
gc 0
Test layer1 Acc 0.8771052631578947, AUC 0.9644123911857605, avg_entr 0.034732457250356674, f1 0.8771052360534668
l1_test_time 0.2325261450000653
gc 0
Test layer2 Acc 0.8771052631578947, AUC 0.9663949608802795, avg_entr 0.02923012152314186, f1 0.8771052360534668
l2_test_time 0.30382639400022526
gc 0
Test layer3 Acc 0.8763157894736842, AUC 0.9661084413528442, avg_entr 0.026636295020580292, f1 0.8763157725334167
l3_test_time 0.40471758500007127
gc 0
Test layer4 Acc 0.876578947368421, AUC 0.9597955942153931, avg_entr 0.023839320987462997, f1 0.8765789270401001
l4_test_time 0.5308064619998731
gc 0
Test threshold 0.1 Acc 0.876578947368421, AUC 0.9638580083847046, avg_entr 0.025261159986257553, f1 0.8765789270401001
t0.1_test_time 0.25012605300025825
gc 0
Test threshold 0.2 Acc 0.8763157894736842, AUC 0.9644867181777954, avg_entr 0.03156682476401329, f1 0.8763157725334167
t0.2_test_time 0.24183181299986245
gc 0
Test threshold 0.3 Acc 0.8744736842105263, AUC 0.9649551510810852, avg_entr 0.039659351110458374, f1 0.8744736909866333
t0.3_test_time 0.23790278499973283
gc 0
Test threshold 0.4 Acc 0.875, AUC 0.9655849933624268, avg_entr 0.04789988324046135, f1 0.875
t0.4_test_time 0.22934142799977053
gc 0
Test threshold 0.5 Acc 0.8742105263157894, AUC 0.9658167362213135, avg_entr 0.0535832941532135, f1 0.87421053647995
t0.5_test_time 0.20879166199983956
gc 0
Test threshold 0.6 Acc 0.873421052631579, AUC 0.9658725261688232, avg_entr 0.05510738492012024, f1 0.8734210729598999
t0.6_test_time 0.19750906500030396
gc 0
Test threshold 0.7 Acc 0.873421052631579, AUC 0.9658725261688232, avg_entr 0.05510738492012024, f1 0.8734210729598999
t0.7_test_time 0.2066812459997891
gc 0
Test threshold 0.8 Acc 0.873421052631579, AUC 0.9658725261688232, avg_entr 0.05510738492012024, f1 0.8734210729598999
t0.8_test_time 0.19782999199969709
gc 0
Test threshold 0.9 Acc 0.873421052631579, AUC 0.9658725261688232, avg_entr 0.05510738492012024, f1 0.8734210729598999
t0.9_test_time 0.19748570299998391
