total count words 102019
vocab size 30000
train size 120000, valid size 3800, test size 3800
found 26754 words in glove
model: TransformerALsideText(
  (layers): ModuleList(
    (0): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=4, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=4, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
  (emb_layers): ModuleList(
    (0): Embedding(30000, 300)
    (1): Embedding(30000, 300)
    (2): Embedding(30000, 300)
    (3): Embedding(30000, 300)
    (4): Embedding(30000, 300)
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.multi_headed_attention.linears.0.weight 90000
layers.0.enc.f.multi_headed_attention.linears.0.bias 300
layers.0.enc.f.multi_headed_attention.linears.1.weight 90000
layers.0.enc.f.multi_headed_attention.linears.1.bias 300
layers.0.enc.f.multi_headed_attention.linears.2.weight 90000
layers.0.enc.f.multi_headed_attention.linears.2.bias 300
layers.0.enc.f.multi_headed_attention.linears.3.weight 90000
layers.0.enc.f.multi_headed_attention.linears.3.bias 300
layers.0.enc.f.feed_forward.w_1.weight 90000
layers.0.enc.f.feed_forward.w_1.bias 300
layers.0.enc.f.feed_forward.w_2.weight 90000
layers.0.enc.f.feed_forward.w_2.bias 300
layers.0.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.0.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.0.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.0.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.0.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.0.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.0.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.0.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.0.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.0.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.0.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.0.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.0.enc.f.encoder.norm.a 300
layers.0.enc.f.encoder.norm.b 300
layers.0.ae.g.0.weight 512
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 512
layers.0.ae.h.0.bias 4
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
emb_layers.0.weight 9000000
emb_layers.1.weight 9000000
emb_layers.2.weight 9000000
emb_layers.3.weight 9000000
emb_layers.4.weight 9000000
Total Trainable Params: 50758892
init_time 15.971154551
Start Training
gc 0
Train Epoch0 Acc 0.2710916666666667 (32531/120000), AUC 0.5160468816757202
ep0_train_time 61.671018542
Test Epoch0 layer0 Acc 0.8010526315789473, AUC 0.9356001615524292, avg_entr 0.7428988218307495, f1 0.8010526299476624
ep0_l0_test_time 0.1940523079999963
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 0
Test Epoch0 layer1 Acc 0.7936842105263158, AUC 0.9356663227081299, avg_entr 0.7759177684783936, f1 0.7936842441558838
ep0_l1_test_time 0.2417470919999971
Test Epoch0 layer2 Acc 0.7457894736842106, AUC 0.9347023963928223, avg_entr 0.850223183631897, f1 0.7457894682884216
ep0_l2_test_time 0.3153980200000035
Test Epoch0 layer3 Acc 0.5821052631578948, AUC 0.9238528609275818, avg_entr 1.0250345468521118, f1 0.582105278968811
ep0_l3_test_time 0.4161619529999996
Test Epoch0 layer4 Acc 0.45657894736842103, AUC 0.8683129549026489, avg_entr 1.1961369514465332, f1 0.45657894015312195
ep0_l4_test_time 0.5422500509999963
gc 0
Train Epoch1 Acc 0.7217416666666666 (86609/120000), AUC 0.8946850299835205
ep1_train_time 61.42032488599999
Test Epoch1 layer0 Acc 0.838421052631579, AUC 0.9558988809585571, avg_entr 0.3943786025047302, f1 0.8384209871292114
ep1_l0_test_time 0.19367522400000325
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 1
Test Epoch1 layer1 Acc 0.8360526315789474, AUC 0.9579005241394043, avg_entr 0.35894596576690674, f1 0.836052656173706
ep1_l1_test_time 0.2415045920000125
Test Epoch1 layer2 Acc 0.8363157894736842, AUC 0.9588343501091003, avg_entr 0.326820969581604, f1 0.8363158106803894
ep1_l2_test_time 0.314491500999992
Test Epoch1 layer3 Acc 0.8368421052631579, AUC 0.9589028358459473, avg_entr 0.33465105295181274, f1 0.8368421196937561
ep1_l3_test_time 0.4179312429999982
Test Epoch1 layer4 Acc 0.8318421052631579, AUC 0.9592435359954834, avg_entr 0.3473629057407379, f1 0.8318421840667725
ep1_l4_test_time 0.5414698190000138
gc 0
Train Epoch2 Acc 0.8586833333333334 (103042/120000), AUC 0.9609881639480591
ep2_train_time 61.50050260399999
Test Epoch2 layer0 Acc 0.8555263157894737, AUC 0.9629726409912109, avg_entr 0.2551957666873932, f1 0.855526328086853
ep2_l0_test_time 0.19423985000000243
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 2
Test Epoch2 layer1 Acc 0.8542105263157894, AUC 0.9651365280151367, avg_entr 0.1792164146900177, f1 0.8542105555534363
ep2_l1_test_time 0.24389622300000724
Test Epoch2 layer2 Acc 0.8521052631578947, AUC 0.9654359221458435, avg_entr 0.1621653437614441, f1 0.8521052598953247
ep2_l2_test_time 0.31636523899999247
Test Epoch2 layer3 Acc 0.8523684210526316, AUC 0.9661242961883545, avg_entr 0.16250494122505188, f1 0.8523684144020081
ep2_l3_test_time 0.4168391199999917
Test Epoch2 layer4 Acc 0.8531578947368421, AUC 0.9665165543556213, avg_entr 0.1635037660598755, f1 0.8531579375267029
ep2_l4_test_time 0.5439535929999977
gc 0
Train Epoch3 Acc 0.8798666666666667 (105584/120000), AUC 0.9672938585281372
ep3_train_time 61.32521688800003
Test Epoch3 layer0 Acc 0.8652631578947368, AUC 0.9667633175849915, avg_entr 0.19189727306365967, f1 0.8652631640434265
ep3_l0_test_time 0.19449163500001987
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 3
Test Epoch3 layer1 Acc 0.8678947368421053, AUC 0.966779351234436, avg_entr 0.12699516117572784, f1 0.86789470911026
ep3_l1_test_time 0.24217460900001697
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 3
Test Epoch3 layer2 Acc 0.8673684210526316, AUC 0.9673740863800049, avg_entr 0.12322644889354706, f1 0.8673684000968933
ep3_l2_test_time 0.31450345299998617
Test Epoch3 layer3 Acc 0.8663157894736843, AUC 0.9671487808227539, avg_entr 0.1288784295320511, f1 0.8663158416748047
ep3_l3_test_time 0.4135638070000027
Test Epoch3 layer4 Acc 0.8660526315789474, AUC 0.9682803153991699, avg_entr 0.1303098201751709, f1 0.8660526275634766
ep3_l4_test_time 0.5421442840000168
gc 0
Train Epoch4 Acc 0.89375 (107250/120000), AUC 0.9723073244094849
ep4_train_time 61.399468462000016
Test Epoch4 layer0 Acc 0.8768421052631579, AUC 0.9690043926239014, avg_entr 0.15181559324264526, f1 0.8768420815467834
ep4_l0_test_time 0.19454741699996703
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 4
Test Epoch4 layer1 Acc 0.8739473684210526, AUC 0.9673588275909424, avg_entr 0.09447833150625229, f1 0.8739473819732666
ep4_l1_test_time 0.24428360600001042
Test Epoch4 layer2 Acc 0.8757894736842106, AUC 0.96673583984375, avg_entr 0.08695216476917267, f1 0.87578946352005
ep4_l2_test_time 0.3162872529999845
Test Epoch4 layer3 Acc 0.873421052631579, AUC 0.9668493270874023, avg_entr 0.08506496250629425, f1 0.8734210729598999
ep4_l3_test_time 0.414209225000036
Test Epoch4 layer4 Acc 0.8752631578947369, AUC 0.9669334292411804, avg_entr 0.08416999876499176, f1 0.8752631545066833
ep4_l4_test_time 0.5424182959999939
gc 0
Train Epoch5 Acc 0.9039666666666667 (108476/120000), AUC 0.9757689237594604
ep5_train_time 61.34854416100001
Test Epoch5 layer0 Acc 0.8752631578947369, AUC 0.9685766696929932, avg_entr 0.13249610364437103, f1 0.8752631545066833
ep5_l0_test_time 0.19522524299998167
Test Epoch5 layer1 Acc 0.878421052631579, AUC 0.9690553545951843, avg_entr 0.07936015725135803, f1 0.8784210681915283
ep5_l1_test_time 0.24283479199999647
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 5
Test Epoch5 layer2 Acc 0.8786842105263157, AUC 0.9679521322250366, avg_entr 0.07159344851970673, f1 0.8786842226982117
ep5_l2_test_time 0.31247908500000676
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 5
Test Epoch5 layer3 Acc 0.878421052631579, AUC 0.9674032926559448, avg_entr 0.06991696357727051, f1 0.8784210681915283
ep5_l3_test_time 0.41490863699999636
Test Epoch5 layer4 Acc 0.8773684210526316, AUC 0.9666987657546997, avg_entr 0.06770982593297958, f1 0.8773684501647949
ep5_l4_test_time 0.5438200730000062
gc 0
Train Epoch6 Acc 0.9125666666666666 (109508/120000), AUC 0.978554904460907
ep6_train_time 61.32148792999999
Test Epoch6 layer0 Acc 0.8739473684210526, AUC 0.969018816947937, avg_entr 0.12270676344633102, f1 0.8739473819732666
ep6_l0_test_time 0.1969014100000095
Test Epoch6 layer1 Acc 0.8731578947368421, AUC 0.9705104827880859, avg_entr 0.07369956374168396, f1 0.8731579184532166
ep6_l1_test_time 0.24707796900003132
Test Epoch6 layer2 Acc 0.8742105263157894, AUC 0.9707200527191162, avg_entr 0.06594907492399216, f1 0.87421053647995
ep6_l2_test_time 0.3147224100000017
Test Epoch6 layer3 Acc 0.875, AUC 0.9711463451385498, avg_entr 0.0634598433971405, f1 0.875
ep6_l3_test_time 0.4144329410000296
Test Epoch6 layer4 Acc 0.8744736842105263, AUC 0.9703633785247803, avg_entr 0.06047588959336281, f1 0.8744736909866333
ep6_l4_test_time 0.5417728300000135
gc 0
Train Epoch7 Acc 0.9199833333333334 (110398/120000), AUC 0.9818660020828247
ep7_train_time 61.44444064800001
Test Epoch7 layer0 Acc 0.876578947368421, AUC 0.9694293737411499, avg_entr 0.10643848776817322, f1 0.8765789270401001
ep7_l0_test_time 0.19380738300003486
Test Epoch7 layer1 Acc 0.8781578947368421, AUC 0.9694632291793823, avg_entr 0.05713190510869026, f1 0.878157913684845
ep7_l1_test_time 0.2425267349999558
Test Epoch7 layer2 Acc 0.8778947368421053, AUC 0.9708865880966187, avg_entr 0.048159509897232056, f1 0.8778947591781616
ep7_l2_test_time 0.31568450999998277
Test Epoch7 layer3 Acc 0.8781578947368421, AUC 0.9701345562934875, avg_entr 0.04412304237484932, f1 0.878157913684845
ep7_l3_test_time 0.41612010599999394
Test Epoch7 layer4 Acc 0.8776315789473684, AUC 0.9682701826095581, avg_entr 0.042217276990413666, f1 0.8776316046714783
ep7_l4_test_time 0.5428516399999808
gc 0
Train Epoch8 Acc 0.9249083333333333 (110989/120000), AUC 0.9831527471542358
ep8_train_time 61.37691552499996
Test Epoch8 layer0 Acc 0.8855263157894737, AUC 0.9701219797134399, avg_entr 0.0944933071732521, f1 0.8855262398719788
ep8_l0_test_time 0.19366507199993066
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 8
Test Epoch8 layer1 Acc 0.885, AUC 0.969693124294281, avg_entr 0.04971112683415413, f1 0.8849999904632568
ep8_l1_test_time 0.24205894399995032
Test Epoch8 layer2 Acc 0.8860526315789473, AUC 0.969356119632721, avg_entr 0.04206828400492668, f1 0.886052668094635
ep8_l2_test_time 0.3145878849999235
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 8
Test Epoch8 layer3 Acc 0.8860526315789473, AUC 0.9700628519058228, avg_entr 0.03849658742547035, f1 0.886052668094635
ep8_l3_test_time 0.41532268899993596
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 8
Test Epoch8 layer4 Acc 0.8863157894736842, AUC 0.9679920673370361, avg_entr 0.035414863377809525, f1 0.8863158226013184
ep8_l4_test_time 0.5416864879999821
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 8
gc 0
Train Epoch9 Acc 0.9316416666666667 (111797/120000), AUC 0.984891414642334
ep9_train_time 61.239010259999986
Test Epoch9 layer0 Acc 0.88, AUC 0.9703103303909302, avg_entr 0.09730830788612366, f1 0.8799999952316284
ep9_l0_test_time 0.1947495299999673
Test Epoch9 layer1 Acc 0.8739473684210526, AUC 0.9667742252349854, avg_entr 0.054019633680582047, f1 0.8739473819732666
ep9_l1_test_time 0.24297776099990642
Test Epoch9 layer2 Acc 0.8736842105263158, AUC 0.968389630317688, avg_entr 0.04624375328421593, f1 0.8736842274665833
ep9_l2_test_time 0.31540977999998177
Test Epoch9 layer3 Acc 0.8731578947368421, AUC 0.9675995111465454, avg_entr 0.04224550351500511, f1 0.8731579184532166
ep9_l3_test_time 0.415085564999913
Test Epoch9 layer4 Acc 0.8718421052631579, AUC 0.9638999104499817, avg_entr 0.03798393905162811, f1 0.8718421459197998
ep9_l4_test_time 0.542281029000037
gc 0
Train Epoch10 Acc 0.9354583333333333 (112255/120000), AUC 0.9859135746955872
ep10_train_time 61.338791429000025
Test Epoch10 layer0 Acc 0.8831578947368421, AUC 0.970272958278656, avg_entr 0.08481822907924652, f1 0.8831579089164734
ep10_l0_test_time 0.19192516600003273
Test Epoch10 layer1 Acc 0.8763157894736842, AUC 0.9708551168441772, avg_entr 0.04413343220949173, f1 0.8763157725334167
ep10_l1_test_time 0.23800760899996476
Test Epoch10 layer2 Acc 0.8742105263157894, AUC 0.9719173908233643, avg_entr 0.03765368089079857, f1 0.87421053647995
ep10_l2_test_time 0.30913010299991583
Test Epoch10 layer3 Acc 0.8739473684210526, AUC 0.9710615873336792, avg_entr 0.033883966505527496, f1 0.8739473819732666
ep10_l3_test_time 0.4111611260000245
Test Epoch10 layer4 Acc 0.8736842105263158, AUC 0.9676020741462708, avg_entr 0.030695287510752678, f1 0.8736842274665833
ep10_l4_test_time 0.5368523850000884
gc 0
Train Epoch11 Acc 0.9407916666666667 (112895/120000), AUC 0.9890394806861877
ep11_train_time 61.21628875399995
Test Epoch11 layer0 Acc 0.8802631578947369, AUC 0.969950258731842, avg_entr 0.08064597100019455, f1 0.880263090133667
ep11_l0_test_time 0.1874894029999723
Test Epoch11 layer1 Acc 0.8760526315789474, AUC 0.9654818773269653, avg_entr 0.040810491889715195, f1 0.8760526180267334
ep11_l1_test_time 0.23482830300008573
Test Epoch11 layer2 Acc 0.8742105263157894, AUC 0.9649198055267334, avg_entr 0.03585338220000267, f1 0.87421053647995
ep11_l2_test_time 0.3058972129999802
Test Epoch11 layer3 Acc 0.873421052631579, AUC 0.9617167115211487, avg_entr 0.03232305496931076, f1 0.8734210729598999
ep11_l3_test_time 0.406559887999947
Test Epoch11 layer4 Acc 0.8731578947368421, AUC 0.9586553573608398, avg_entr 0.028929008170962334, f1 0.8731579184532166
ep11_l4_test_time 0.5329630380000481
gc 0
Train Epoch12 Acc 0.9449166666666666 (113390/120000), AUC 0.9898626804351807
ep12_train_time 60.54566404100001
Test Epoch12 layer0 Acc 0.883421052631579, AUC 0.9699755907058716, avg_entr 0.073288694024086, f1 0.8834210634231567
ep12_l0_test_time 0.18813715399994635
Test Epoch12 layer1 Acc 0.8797368421052632, AUC 0.9688007235527039, avg_entr 0.03387957811355591, f1 0.8797368407249451
ep12_l1_test_time 0.2347910169999068
Test Epoch12 layer2 Acc 0.8797368421052632, AUC 0.9698534607887268, avg_entr 0.02978079579770565, f1 0.8797368407249451
ep12_l2_test_time 0.3068392559999893
Test Epoch12 layer3 Acc 0.8797368421052632, AUC 0.9705645442008972, avg_entr 0.027064012363553047, f1 0.8797368407249451
ep12_l3_test_time 0.406662654999991
Test Epoch12 layer4 Acc 0.8781578947368421, AUC 0.9656978845596313, avg_entr 0.024952439591288567, f1 0.878157913684845
ep12_l4_test_time 0.533755401999997
gc 0
Train Epoch13 Acc 0.949775 (113973/120000), AUC 0.9907345771789551
ep13_train_time 60.48303754899996
Test Epoch13 layer0 Acc 0.8826315789473684, AUC 0.9696013927459717, avg_entr 0.06800254434347153, f1 0.8826315999031067
ep13_l0_test_time 0.18871588499996506
Test Epoch13 layer1 Acc 0.881578947368421, AUC 0.9657763242721558, avg_entr 0.030271422117948532, f1 0.8815789222717285
ep13_l1_test_time 0.23564978700005668
Test Epoch13 layer2 Acc 0.8821052631578947, AUC 0.9643023014068604, avg_entr 0.0247174222022295, f1 0.88210529088974
ep13_l2_test_time 0.30805766200001017
Test Epoch13 layer3 Acc 0.881578947368421, AUC 0.9627023935317993, avg_entr 0.022123347967863083, f1 0.8815789222717285
ep13_l3_test_time 0.409967603000041
Test Epoch13 layer4 Acc 0.8813157894736842, AUC 0.9569932222366333, avg_entr 0.02000439167022705, f1 0.8813157677650452
ep13_l4_test_time 0.5349399890000086
gc 0
Train Epoch14 Acc 0.9561666666666667 (114740/120000), AUC 0.9915910959243774
ep14_train_time 60.753831189999914
Test Epoch14 layer0 Acc 0.8828947368421053, AUC 0.9695979952812195, avg_entr 0.06669624149799347, f1 0.88289475440979
ep14_l0_test_time 0.18749665200004983
Test Epoch14 layer1 Acc 0.8776315789473684, AUC 0.9625914096832275, avg_entr 0.02894020825624466, f1 0.8776316046714783
ep14_l1_test_time 0.23374383100008345
Test Epoch14 layer2 Acc 0.8768421052631579, AUC 0.9672814607620239, avg_entr 0.024152684956789017, f1 0.8768420815467834
ep14_l2_test_time 0.30611256499992123
Test Epoch14 layer3 Acc 0.8757894736842106, AUC 0.9667919278144836, avg_entr 0.0214264914393425, f1 0.87578946352005
ep14_l3_test_time 0.4068115800000669
Test Epoch14 layer4 Acc 0.875, AUC 0.9627135992050171, avg_entr 0.019090695306658745, f1 0.875
ep14_l4_test_time 0.5329826409999896
gc 0
Train Epoch15 Acc 0.9614416666666666 (115373/120000), AUC 0.9942073822021484
ep15_train_time 60.715554655999995
Test Epoch15 layer0 Acc 0.8844736842105263, AUC 0.9682356119155884, avg_entr 0.06849594414234161, f1 0.8844736814498901
ep15_l0_test_time 0.189999041999954
Test Epoch15 layer1 Acc 0.876578947368421, AUC 0.962795615196228, avg_entr 0.029912352561950684, f1 0.8765789270401001
ep15_l1_test_time 0.23799681799982864
Test Epoch15 layer2 Acc 0.8755263157894737, AUC 0.9653703570365906, avg_entr 0.02406950481235981, f1 0.8755263090133667
ep15_l2_test_time 0.30719079300001795
Test Epoch15 layer3 Acc 0.8757894736842106, AUC 0.9648320078849792, avg_entr 0.021915966644883156, f1 0.87578946352005
ep15_l3_test_time 0.40710175200001686
Test Epoch15 layer4 Acc 0.8760526315789474, AUC 0.9616615176200867, avg_entr 0.0201368760317564, f1 0.8760526180267334
ep15_l4_test_time 0.5341446390000328
gc 0
Train Epoch16 Acc 0.963275 (115593/120000), AUC 0.9945625066757202
ep16_train_time 60.48161023400007
Test Epoch16 layer0 Acc 0.8839473684210526, AUC 0.9675319790840149, avg_entr 0.06452974677085876, f1 0.8839473724365234
ep16_l0_test_time 0.18837560499991923
Test Epoch16 layer1 Acc 0.8760526315789474, AUC 0.9581030607223511, avg_entr 0.029073184356093407, f1 0.8760526180267334
ep16_l1_test_time 0.23783037499993043
Test Epoch16 layer2 Acc 0.8757894736842106, AUC 0.962814450263977, avg_entr 0.023767676204442978, f1 0.87578946352005
ep16_l2_test_time 0.3074655719999555
Test Epoch16 layer3 Acc 0.8760526315789474, AUC 0.9636489152908325, avg_entr 0.021303467452526093, f1 0.8760526180267334
ep16_l3_test_time 0.4069724149999274
Test Epoch16 layer4 Acc 0.8760526315789474, AUC 0.957359790802002, avg_entr 0.01887964829802513, f1 0.8760526180267334
ep16_l4_test_time 0.5334720099999686
gc 0
Train Epoch17 Acc 0.9661916666666667 (115943/120000), AUC 0.9947524070739746
ep17_train_time 60.87483258399993
Test Epoch17 layer0 Acc 0.8810526315789474, AUC 0.9674949645996094, avg_entr 0.06206521391868591, f1 0.8810526132583618
ep17_l0_test_time 0.18759793499998523
Test Epoch17 layer1 Acc 0.8786842105263157, AUC 0.9598340392112732, avg_entr 0.02364703081548214, f1 0.8786842226982117
ep17_l1_test_time 0.23543156799996723
Test Epoch17 layer2 Acc 0.8781578947368421, AUC 0.9645141363143921, avg_entr 0.019403552636504173, f1 0.878157913684845
ep17_l2_test_time 0.3076493160001519
Test Epoch17 layer3 Acc 0.8778947368421053, AUC 0.9631920456886292, avg_entr 0.017663132399320602, f1 0.8778947591781616
ep17_l3_test_time 0.4065039590000197
Test Epoch17 layer4 Acc 0.878421052631579, AUC 0.9592245817184448, avg_entr 0.016036802902817726, f1 0.8784210681915283
ep17_l4_test_time 0.5327455230001306
gc 0
Train Epoch18 Acc 0.969525 (116343/120000), AUC 0.9954670667648315
ep18_train_time 60.623053024
Test Epoch18 layer0 Acc 0.8852631578947369, AUC 0.9679750800132751, avg_entr 0.060431208461523056, f1 0.8852631449699402
ep18_l0_test_time 0.18796804999988126
Test Epoch18 layer1 Acc 0.876578947368421, AUC 0.959845781326294, avg_entr 0.024340664967894554, f1 0.8765789270401001
ep18_l1_test_time 0.23605864099999962
Test Epoch18 layer2 Acc 0.8773684210526316, AUC 0.9615001082420349, avg_entr 0.01904432289302349, f1 0.8773684501647949
ep18_l2_test_time 0.3074387729998307
Test Epoch18 layer3 Acc 0.8771052631578947, AUC 0.9626588821411133, avg_entr 0.017037339508533478, f1 0.8771052360534668
ep18_l3_test_time 0.4067937229999643
Test Epoch18 layer4 Acc 0.8771052631578947, AUC 0.9568701982498169, avg_entr 0.015341733582317829, f1 0.8771052360534668
ep18_l4_test_time 0.5339934319999884
gc 0
Train Epoch19 Acc 0.9713583333333333 (116563/120000), AUC 0.9962133169174194
ep19_train_time 60.83090965200017
Test Epoch19 layer0 Acc 0.8831578947368421, AUC 0.9678634405136108, avg_entr 0.05991365760564804, f1 0.8831579089164734
ep19_l0_test_time 0.18766317300014634
Test Epoch19 layer1 Acc 0.8728947368421053, AUC 0.9578930139541626, avg_entr 0.02507804147899151, f1 0.8728947639465332
ep19_l1_test_time 0.2347769169998628
Test Epoch19 layer2 Acc 0.8726315789473684, AUC 0.9619963765144348, avg_entr 0.020965661853551865, f1 0.8726315498352051
ep19_l2_test_time 0.30521905399996285
Test Epoch19 layer3 Acc 0.8726315789473684, AUC 0.9612834453582764, avg_entr 0.019069043919444084, f1 0.8726315498352051
ep19_l3_test_time 0.40612462500007496
Test Epoch19 layer4 Acc 0.8726315789473684, AUC 0.9576293230056763, avg_entr 0.01735858805477619, f1 0.8726315498352051
ep19_l4_test_time 0.533295679000048
gc 0
Train Epoch20 Acc 0.9727833333333333 (116734/120000), AUC 0.996417224407196
ep20_train_time 60.56388780600014
Test Epoch20 layer0 Acc 0.8847368421052632, AUC 0.967309296131134, avg_entr 0.057724740356206894, f1 0.8847368359565735
ep20_l0_test_time 0.18725124399998094
Test Epoch20 layer1 Acc 0.8792105263157894, AUC 0.9583473205566406, avg_entr 0.019917353987693787, f1 0.8792105317115784
ep20_l1_test_time 0.23430901200003973
Test Epoch20 layer2 Acc 0.8786842105263157, AUC 0.9629071354866028, avg_entr 0.015385406091809273, f1 0.8786842226982117
ep20_l2_test_time 0.3069053189999522
Test Epoch20 layer3 Acc 0.8789473684210526, AUC 0.9632906317710876, avg_entr 0.01370355673134327, f1 0.878947377204895
ep20_l3_test_time 0.40725410999993983
Test Epoch20 layer4 Acc 0.8792105263157894, AUC 0.9613417387008667, avg_entr 0.01216006651520729, f1 0.8792105317115784
ep20_l4_test_time 0.5329887410000538
gc 0
Train Epoch21 Acc 0.974125 (116895/120000), AUC 0.9965651035308838
ep21_train_time 60.65707472700001
Test Epoch21 layer0 Acc 0.8836842105263157, AUC 0.9676850438117981, avg_entr 0.057499103248119354, f1 0.8836842179298401
ep21_l0_test_time 0.18726783000010983
Test Epoch21 layer1 Acc 0.8747368421052631, AUC 0.9559811353683472, avg_entr 0.020520489662885666, f1 0.8747368454933167
ep21_l1_test_time 0.23438765700007025
Test Epoch21 layer2 Acc 0.8742105263157894, AUC 0.9610828161239624, avg_entr 0.016385408118367195, f1 0.87421053647995
ep21_l2_test_time 0.3062581159999809
Test Epoch21 layer3 Acc 0.8742105263157894, AUC 0.9582464694976807, avg_entr 0.01490779872983694, f1 0.87421053647995
ep21_l3_test_time 0.40642065100018954
Test Epoch21 layer4 Acc 0.8739473684210526, AUC 0.9559463858604431, avg_entr 0.013771806843578815, f1 0.8739473819732666
ep21_l4_test_time 0.5332209959999545
gc 0
Train Epoch22 Acc 0.9757666666666667 (117092/120000), AUC 0.9969010353088379
ep22_train_time 60.45717249500012
Test Epoch22 layer0 Acc 0.8844736842105263, AUC 0.9675551056861877, avg_entr 0.0564335398375988, f1 0.8844736814498901
ep22_l0_test_time 0.18704609699989305
Test Epoch22 layer1 Acc 0.8736842105263158, AUC 0.9581209421157837, avg_entr 0.021528055891394615, f1 0.8736842274665833
ep22_l1_test_time 0.2344846260000395
Test Epoch22 layer2 Acc 0.8747368421052631, AUC 0.9643727540969849, avg_entr 0.01762506179511547, f1 0.8747368454933167
ep22_l2_test_time 0.3070853919998626
Test Epoch22 layer3 Acc 0.8742105263157894, AUC 0.9642664194107056, avg_entr 0.01607714779675007, f1 0.87421053647995
ep22_l3_test_time 0.4062609589998374
Test Epoch22 layer4 Acc 0.875, AUC 0.9625056982040405, avg_entr 0.014659770764410496, f1 0.875
ep22_l4_test_time 0.5335918299999776
gc 0
Train Epoch23 Acc 0.9764416666666667 (117173/120000), AUC 0.9972366690635681
ep23_train_time 60.61941976200001
Test Epoch23 layer0 Acc 0.885, AUC 0.96761155128479, avg_entr 0.05626290291547775, f1 0.8849999904632568
ep23_l0_test_time 0.18734639400008746
Test Epoch23 layer1 Acc 0.8739473684210526, AUC 0.9516769051551819, avg_entr 0.020193789154291153, f1 0.8739473819732666
ep23_l1_test_time 0.23404135600003428
Test Epoch23 layer2 Acc 0.8744736842105263, AUC 0.9591934680938721, avg_entr 0.015857145190238953, f1 0.8744736909866333
ep23_l2_test_time 0.3061451629998828
Test Epoch23 layer3 Acc 0.8742105263157894, AUC 0.9592797756195068, avg_entr 0.014550122432410717, f1 0.87421053647995
ep23_l3_test_time 0.40558285499992053
Test Epoch23 layer4 Acc 0.8744736842105263, AUC 0.9541968703269958, avg_entr 0.013439814560115337, f1 0.8744736909866333
ep23_l4_test_time 0.5320107769998685
gc 0
Train Epoch24 Acc 0.9771166666666666 (117254/120000), AUC 0.9973198175430298
ep24_train_time 60.50953692000007
Test Epoch24 layer0 Acc 0.886578947368421, AUC 0.9672630429267883, avg_entr 0.05594231188297272, f1 0.8865789771080017
ep24_l0_test_time 0.18705094799997823
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 24
Test Epoch24 layer1 Acc 0.8728947368421053, AUC 0.9545389413833618, avg_entr 0.020301874727010727, f1 0.8728947639465332
ep24_l1_test_time 0.2374744159999409
Test Epoch24 layer2 Acc 0.8731578947368421, AUC 0.9604325294494629, avg_entr 0.016811225563287735, f1 0.8731579184532166
ep24_l2_test_time 0.30608758899984423
Test Epoch24 layer3 Acc 0.8728947368421053, AUC 0.9591081142425537, avg_entr 0.015691502019762993, f1 0.8728947639465332
ep24_l3_test_time 0.4067442949999531
Test Epoch24 layer4 Acc 0.8721052631578947, AUC 0.9551196694374084, avg_entr 0.014670931734144688, f1 0.8721052408218384
ep24_l4_test_time 0.5330734690001009
gc 0
Train Epoch25 Acc 0.9777833333333333 (117334/120000), AUC 0.9973125457763672
ep25_train_time 60.42853027499996
Test Epoch25 layer0 Acc 0.885, AUC 0.9675009250640869, avg_entr 0.054901015013456345, f1 0.8849999904632568
ep25_l0_test_time 0.18732544800013784
Test Epoch25 layer1 Acc 0.8739473684210526, AUC 0.9540266990661621, avg_entr 0.019207866862416267, f1 0.8739473819732666
ep25_l1_test_time 0.23454123600004095
Test Epoch25 layer2 Acc 0.8736842105263158, AUC 0.9588590264320374, avg_entr 0.01571766287088394, f1 0.8736842274665833
ep25_l2_test_time 0.3065477379998356
Test Epoch25 layer3 Acc 0.873421052631579, AUC 0.9583050012588501, avg_entr 0.014271192252635956, f1 0.8734210729598999
ep25_l3_test_time 0.40578235700013465
Test Epoch25 layer4 Acc 0.8736842105263158, AUC 0.9525692462921143, avg_entr 0.012955119833350182, f1 0.8736842274665833
ep25_l4_test_time 0.532752033999941
gc 0
Train Epoch26 Acc 0.9784416666666667 (117413/120000), AUC 0.9976390600204468
ep26_train_time 60.58728866899992
Test Epoch26 layer0 Acc 0.885, AUC 0.9673380255699158, avg_entr 0.05362632870674133, f1 0.8849999904632568
ep26_l0_test_time 0.18715920599993296
Test Epoch26 layer1 Acc 0.8786842105263157, AUC 0.9539743661880493, avg_entr 0.019961703568696976, f1 0.8786842226982117
ep26_l1_test_time 0.23371444999997948
Test Epoch26 layer2 Acc 0.8786842105263157, AUC 0.9616758227348328, avg_entr 0.016205105930566788, f1 0.8786842226982117
ep26_l2_test_time 0.30553388400016956
Test Epoch26 layer3 Acc 0.8786842105263157, AUC 0.9611965417861938, avg_entr 0.015022815205156803, f1 0.8786842226982117
ep26_l3_test_time 0.40552551599989783
Test Epoch26 layer4 Acc 0.8786842105263157, AUC 0.9552480578422546, avg_entr 0.013882491737604141, f1 0.8786842226982117
ep26_l4_test_time 0.5350296760000219
gc 0
Train Epoch27 Acc 0.979225 (117507/120000), AUC 0.9977520704269409
ep27_train_time 60.735235415000034
Test Epoch27 layer0 Acc 0.8836842105263157, AUC 0.96738201379776, avg_entr 0.053741831332445145, f1 0.8836842179298401
ep27_l0_test_time 0.18738668900004996
Test Epoch27 layer1 Acc 0.8744736842105263, AUC 0.9527451992034912, avg_entr 0.018739260733127594, f1 0.8744736909866333
ep27_l1_test_time 0.2412564310000107
Test Epoch27 layer2 Acc 0.8747368421052631, AUC 0.9601695537567139, avg_entr 0.015791237354278564, f1 0.8747368454933167
ep27_l2_test_time 0.3056444589999501
Test Epoch27 layer3 Acc 0.875, AUC 0.9586603045463562, avg_entr 0.014383411034941673, f1 0.875
ep27_l3_test_time 0.40633873399997356
Test Epoch27 layer4 Acc 0.8747368421052631, AUC 0.9540320634841919, avg_entr 0.013482962734997272, f1 0.8747368454933167
ep27_l4_test_time 0.5330576609999298
gc 0
Train Epoch28 Acc 0.9791583333333334 (117499/120000), AUC 0.997673511505127
ep28_train_time 60.84938596200004
Test Epoch28 layer0 Acc 0.8852631578947369, AUC 0.967352569103241, avg_entr 0.051711585372686386, f1 0.8852631449699402
ep28_l0_test_time 0.1877384990000337
Test Epoch28 layer1 Acc 0.8742105263157894, AUC 0.950982391834259, avg_entr 0.018993636593222618, f1 0.87421053647995
ep28_l1_test_time 0.23578963600016323
Test Epoch28 layer2 Acc 0.8752631578947369, AUC 0.9562713503837585, avg_entr 0.015030165202915668, f1 0.8752631545066833
ep28_l2_test_time 0.3121895360000053
Test Epoch28 layer3 Acc 0.875, AUC 0.9578933715820312, avg_entr 0.013721125200390816, f1 0.875
ep28_l3_test_time 0.40642045799995685
Test Epoch28 layer4 Acc 0.8755263157894737, AUC 0.95196133852005, avg_entr 0.012590629048645496, f1 0.8755263090133667
ep28_l4_test_time 0.5330281729998205
gc 0
Train Epoch29 Acc 0.97915 (117498/120000), AUC 0.9977200031280518
ep29_train_time 60.794323864000035
Test Epoch29 layer0 Acc 0.8844736842105263, AUC 0.9673824310302734, avg_entr 0.052145328372716904, f1 0.8844736814498901
ep29_l0_test_time 0.18694520999997621
Test Epoch29 layer1 Acc 0.8742105263157894, AUC 0.9510571956634521, avg_entr 0.01925228163599968, f1 0.87421053647995
ep29_l1_test_time 0.23405305200003568
Test Epoch29 layer2 Acc 0.875, AUC 0.9579460620880127, avg_entr 0.01575552672147751, f1 0.875
ep29_l2_test_time 0.30620808300000135
Test Epoch29 layer3 Acc 0.875, AUC 0.9585850238800049, avg_entr 0.014486168511211872, f1 0.875
ep29_l3_test_time 0.41480450300014127
Test Epoch29 layer4 Acc 0.875, AUC 0.9536533355712891, avg_entr 0.01330374926328659, f1 0.875
ep29_l4_test_time 0.5373961269999654
gc 0
Train Epoch30 Acc 0.9804666666666667 (117656/120000), AUC 0.9978674650192261
ep30_train_time 60.657441871999936
Test Epoch30 layer0 Acc 0.8847368421052632, AUC 0.9672842025756836, avg_entr 0.05141844227910042, f1 0.8847368359565735
ep30_l0_test_time 0.18714482400014276
Test Epoch30 layer1 Acc 0.8726315789473684, AUC 0.9532440900802612, avg_entr 0.01805642433464527, f1 0.8726315498352051
ep30_l1_test_time 0.23405938299993068
Test Epoch30 layer2 Acc 0.8726315789473684, AUC 0.9599030017852783, avg_entr 0.013658517971634865, f1 0.8726315498352051
ep30_l2_test_time 0.30659599799992066
Test Epoch30 layer3 Acc 0.8728947368421053, AUC 0.9576135277748108, avg_entr 0.012572651728987694, f1 0.8728947639465332
ep30_l3_test_time 0.40605394300018816
Test Epoch30 layer4 Acc 0.8726315789473684, AUC 0.954222559928894, avg_entr 0.0113959526643157, f1 0.8726315498352051
ep30_l4_test_time 0.5329425259999425
gc 0
Train Epoch31 Acc 0.980075 (117609/120000), AUC 0.9978250861167908
ep31_train_time 60.693403907000175
Test Epoch31 layer0 Acc 0.8847368421052632, AUC 0.9670888185501099, avg_entr 0.051015809178352356, f1 0.8847368359565735
ep31_l0_test_time 0.1872498339998856
Test Epoch31 layer1 Acc 0.8726315789473684, AUC 0.9499883651733398, avg_entr 0.018273405730724335, f1 0.8726315498352051
ep31_l1_test_time 0.23472712700004195
Test Epoch31 layer2 Acc 0.873421052631579, AUC 0.9555835723876953, avg_entr 0.01425385382026434, f1 0.8734210729598999
ep31_l2_test_time 0.30698998500020025
Test Epoch31 layer3 Acc 0.8736842105263158, AUC 0.957351565361023, avg_entr 0.01292529795318842, f1 0.8736842274665833
ep31_l3_test_time 0.40603888199984794
Test Epoch31 layer4 Acc 0.8736842105263158, AUC 0.9513347744941711, avg_entr 0.011649220250546932, f1 0.8736842274665833
ep31_l4_test_time 0.5327119129999573
gc 0
Train Epoch32 Acc 0.98005 (117606/120000), AUC 0.9979462623596191
ep32_train_time 60.66221622399985
Test Epoch32 layer0 Acc 0.8847368421052632, AUC 0.9671823382377625, avg_entr 0.051355067640542984, f1 0.8847368359565735
ep32_l0_test_time 0.18687987700013764
Test Epoch32 layer1 Acc 0.873421052631579, AUC 0.9520978331565857, avg_entr 0.017678875476121902, f1 0.8734210729598999
ep32_l1_test_time 0.23412986599987562
Test Epoch32 layer2 Acc 0.8744736842105263, AUC 0.958134114742279, avg_entr 0.01391792856156826, f1 0.8744736909866333
ep32_l2_test_time 0.3064323839998906
Test Epoch32 layer3 Acc 0.8744736842105263, AUC 0.9591068029403687, avg_entr 0.01275948528200388, f1 0.8744736909866333
ep32_l3_test_time 0.4059924029998001
Test Epoch32 layer4 Acc 0.8744736842105263, AUC 0.9538582563400269, avg_entr 0.011666763573884964, f1 0.8744736909866333
ep32_l4_test_time 0.534611256000062
gc 0
Train Epoch33 Acc 0.9810083333333334 (117721/120000), AUC 0.9978659152984619
ep33_train_time 60.671476687999984
Test Epoch33 layer0 Acc 0.8847368421052632, AUC 0.967185378074646, avg_entr 0.05077974870800972, f1 0.8847368359565735
ep33_l0_test_time 0.18975325200017323
Test Epoch33 layer1 Acc 0.871578947368421, AUC 0.9507389068603516, avg_entr 0.017302576452493668, f1 0.8715789318084717
ep33_l1_test_time 0.2341349450002781
Test Epoch33 layer2 Acc 0.871578947368421, AUC 0.9559792876243591, avg_entr 0.013159437105059624, f1 0.8715789318084717
ep33_l2_test_time 0.30816968999988603
Test Epoch33 layer3 Acc 0.8718421052631579, AUC 0.9574692249298096, avg_entr 0.01204400509595871, f1 0.8718421459197998
ep33_l3_test_time 0.4056123559998923
Test Epoch33 layer4 Acc 0.8721052631578947, AUC 0.9506872892379761, avg_entr 0.011011593043804169, f1 0.8721052408218384
ep33_l4_test_time 0.5345643730001939
gc 0
Train Epoch34 Acc 0.9808333333333333 (117700/120000), AUC 0.9978638291358948
ep34_train_time 60.555890455999815
Test Epoch34 layer0 Acc 0.8857894736842106, AUC 0.9671000242233276, avg_entr 0.05028735473752022, f1 0.8857894539833069
ep34_l0_test_time 0.18975117500031047
Test Epoch34 layer1 Acc 0.8726315789473684, AUC 0.9505469799041748, avg_entr 0.016779599711298943, f1 0.8726315498352051
ep34_l1_test_time 0.23600702899966564
Test Epoch34 layer2 Acc 0.873421052631579, AUC 0.956009030342102, avg_entr 0.01266543474048376, f1 0.8734210729598999
ep34_l2_test_time 0.307590003000314
Test Epoch34 layer3 Acc 0.8731578947368421, AUC 0.9579288363456726, avg_entr 0.011450624093413353, f1 0.8731579184532166
ep34_l3_test_time 0.40696712399994794
Test Epoch34 layer4 Acc 0.8736842105263158, AUC 0.9517711400985718, avg_entr 0.010433840565383434, f1 0.8736842274665833
ep34_l4_test_time 0.5332458930001849
gc 0
Train Epoch35 Acc 0.9810166666666666 (117722/120000), AUC 0.998001217842102
ep35_train_time 60.63300249099984
Test Epoch35 layer0 Acc 0.8839473684210526, AUC 0.9671792984008789, avg_entr 0.050096865743398666, f1 0.8839473724365234
ep35_l0_test_time 0.19045556599985503
Test Epoch35 layer1 Acc 0.8731578947368421, AUC 0.9521034955978394, avg_entr 0.016841458156704903, f1 0.8731579184532166
ep35_l1_test_time 0.23485647200004678
Test Epoch35 layer2 Acc 0.8726315789473684, AUC 0.9577324390411377, avg_entr 0.012856646440923214, f1 0.8726315498352051
ep35_l2_test_time 0.3082769450002161
Test Epoch35 layer3 Acc 0.8728947368421053, AUC 0.9579541087150574, avg_entr 0.011668268591165543, f1 0.8728947639465332
ep35_l3_test_time 0.40597707799997806
Test Epoch35 layer4 Acc 0.8728947368421053, AUC 0.95184326171875, avg_entr 0.01068999245762825, f1 0.8728947639465332
ep35_l4_test_time 0.5324182710000969
gc 0
Train Epoch36 Acc 0.981125 (117735/120000), AUC 0.9980599284172058
ep36_train_time 60.63380294700028
Test Epoch36 layer0 Acc 0.8844736842105263, AUC 0.9672284722328186, avg_entr 0.049632422626018524, f1 0.8844736814498901
ep36_l0_test_time 0.18723205400010556
Test Epoch36 layer1 Acc 0.8742105263157894, AUC 0.9528542160987854, avg_entr 0.01752977818250656, f1 0.87421053647995
ep36_l1_test_time 0.23561438799970347
Test Epoch36 layer2 Acc 0.8736842105263158, AUC 0.9568618535995483, avg_entr 0.014326966367661953, f1 0.8736842274665833
ep36_l2_test_time 0.3067856560001019
Test Epoch36 layer3 Acc 0.873421052631579, AUC 0.9572945237159729, avg_entr 0.013102170079946518, f1 0.8734210729598999
ep36_l3_test_time 0.406738986000164
Test Epoch36 layer4 Acc 0.8731578947368421, AUC 0.950477123260498, avg_entr 0.012046914547681808, f1 0.8731579184532166
ep36_l4_test_time 0.5334639809998407
gc 0
Train Epoch37 Acc 0.9806 (117672/120000), AUC 0.9981110692024231
ep37_train_time 60.53790299699995
Test Epoch37 layer0 Acc 0.8836842105263157, AUC 0.9672737121582031, avg_entr 0.0493033193051815, f1 0.8836842179298401
ep37_l0_test_time 0.1887763999998242
Test Epoch37 layer1 Acc 0.8739473684210526, AUC 0.9509128332138062, avg_entr 0.016946028918027878, f1 0.8739473819732666
ep37_l1_test_time 0.23480881100022089
Test Epoch37 layer2 Acc 0.8728947368421053, AUC 0.9549086689949036, avg_entr 0.013117550872266293, f1 0.8728947639465332
ep37_l2_test_time 0.3099412490000759
Test Epoch37 layer3 Acc 0.8731578947368421, AUC 0.9560105204582214, avg_entr 0.011924413964152336, f1 0.8731579184532166
ep37_l3_test_time 0.4048194259999036
Test Epoch37 layer4 Acc 0.873421052631579, AUC 0.9496340155601501, avg_entr 0.011092588305473328, f1 0.8734210729598999
ep37_l4_test_time 0.5331433999999717
gc 0
Train Epoch38 Acc 0.9810583333333334 (117727/120000), AUC 0.998134195804596
ep38_train_time 60.63452114399979
Test Epoch38 layer0 Acc 0.8842105263157894, AUC 0.967211902141571, avg_entr 0.04885561764240265, f1 0.8842105865478516
ep38_l0_test_time 0.18879762100004882
Test Epoch38 layer1 Acc 0.8723684210526316, AUC 0.9525572061538696, avg_entr 0.017131194472312927, f1 0.8723683953285217
ep38_l1_test_time 0.23604964900005143
Test Epoch38 layer2 Acc 0.8731578947368421, AUC 0.956640362739563, avg_entr 0.01358814723789692, f1 0.8731579184532166
ep38_l2_test_time 0.31400561000009475
Test Epoch38 layer3 Acc 0.8726315789473684, AUC 0.9569351673126221, avg_entr 0.01227371022105217, f1 0.8726315498352051
ep38_l3_test_time 0.415587166000023
Test Epoch38 layer4 Acc 0.8728947368421053, AUC 0.9506473541259766, avg_entr 0.01127478014677763, f1 0.8728947639465332
ep38_l4_test_time 0.5337688639997396
gc 0
Train Epoch39 Acc 0.9811416666666667 (117737/120000), AUC 0.9980762600898743
ep39_train_time 60.57502186700003
Test Epoch39 layer0 Acc 0.885, AUC 0.9672194719314575, avg_entr 0.0486416220664978, f1 0.8849999904632568
ep39_l0_test_time 0.18793603100039036
Test Epoch39 layer1 Acc 0.8744736842105263, AUC 0.9517132043838501, avg_entr 0.016557831317186356, f1 0.8744736909866333
ep39_l1_test_time 0.23432001499986654
Test Epoch39 layer2 Acc 0.873421052631579, AUC 0.9557121992111206, avg_entr 0.013046886771917343, f1 0.8734210729598999
ep39_l2_test_time 0.30566857199983133
Test Epoch39 layer3 Acc 0.873421052631579, AUC 0.9568181037902832, avg_entr 0.01183352991938591, f1 0.8734210729598999
ep39_l3_test_time 0.40472684599990316
Test Epoch39 layer4 Acc 0.8731578947368421, AUC 0.9502244591712952, avg_entr 0.01100248098373413, f1 0.8731579184532166
ep39_l4_test_time 0.5330019610000818
gc 0
Train Epoch40 Acc 0.9811 (117732/120000), AUC 0.9980390071868896
ep40_train_time 60.729061868999906
Test Epoch40 layer0 Acc 0.8847368421052632, AUC 0.9672170877456665, avg_entr 0.0485449843108654, f1 0.8847368359565735
ep40_l0_test_time 0.18695440600004076
Test Epoch40 layer1 Acc 0.8728947368421053, AUC 0.9513620734214783, avg_entr 0.01642603985965252, f1 0.8728947639465332
ep40_l1_test_time 0.23372169200001736
Test Epoch40 layer2 Acc 0.8726315789473684, AUC 0.9559739828109741, avg_entr 0.013119752518832684, f1 0.8726315498352051
ep40_l2_test_time 0.3059815809997417
Test Epoch40 layer3 Acc 0.8726315789473684, AUC 0.9575248956680298, avg_entr 0.011921162717044353, f1 0.8726315498352051
ep40_l3_test_time 0.40592834700009917
Test Epoch40 layer4 Acc 0.8726315789473684, AUC 0.9507138729095459, avg_entr 0.011056620627641678, f1 0.8726315498352051
ep40_l4_test_time 0.5325260810000145
gc 0
Train Epoch41 Acc 0.980875 (117705/120000), AUC 0.9980298280715942
ep41_train_time 60.68013788999997
Test Epoch41 layer0 Acc 0.8847368421052632, AUC 0.967253565788269, avg_entr 0.048176806420087814, f1 0.8847368359565735
ep41_l0_test_time 0.18731508700011545
Test Epoch41 layer1 Acc 0.8726315789473684, AUC 0.951028048992157, avg_entr 0.01646270416676998, f1 0.8726315498352051
ep41_l1_test_time 0.23290668900017408
Test Epoch41 layer2 Acc 0.873421052631579, AUC 0.9554219245910645, avg_entr 0.013067623600363731, f1 0.8734210729598999
ep41_l2_test_time 0.30453852300024664
Test Epoch41 layer3 Acc 0.873421052631579, AUC 0.9566317796707153, avg_entr 0.011835499666631222, f1 0.8734210729598999
ep41_l3_test_time 0.40523217000009026
Test Epoch41 layer4 Acc 0.8736842105263158, AUC 0.9501526355743408, avg_entr 0.010891729034483433, f1 0.8736842274665833
ep41_l4_test_time 0.5317599449999761
gc 0
Train Epoch42 Acc 0.9811666666666666 (117740/120000), AUC 0.9980758428573608
ep42_train_time 60.723753319000025
Test Epoch42 layer0 Acc 0.8842105263157894, AUC 0.9671972990036011, avg_entr 0.0481569804251194, f1 0.8842105865478516
ep42_l0_test_time 0.18624095900031534
Test Epoch42 layer1 Acc 0.8726315789473684, AUC 0.9515929818153381, avg_entr 0.016744041815400124, f1 0.8726315498352051
ep42_l1_test_time 0.2335910730002979
Test Epoch42 layer2 Acc 0.8723684210526316, AUC 0.9562642574310303, avg_entr 0.01311419066041708, f1 0.8723683953285217
ep42_l2_test_time 0.3051929999996901
Test Epoch42 layer3 Acc 0.8726315789473684, AUC 0.9568158388137817, avg_entr 0.011885888874530792, f1 0.8726315498352051
ep42_l3_test_time 0.40493372099990665
Test Epoch42 layer4 Acc 0.8723684210526316, AUC 0.9511152505874634, avg_entr 0.01089352648705244, f1 0.8723683953285217
ep42_l4_test_time 0.5315418279997175
gc 0
Train Epoch43 Acc 0.981475 (117777/120000), AUC 0.9981697797775269
ep43_train_time 60.55870641499996
Test Epoch43 layer0 Acc 0.8844736842105263, AUC 0.9671976566314697, avg_entr 0.04798881709575653, f1 0.8844736814498901
ep43_l0_test_time 0.18765428300002895
Test Epoch43 layer1 Acc 0.873421052631579, AUC 0.951414942741394, avg_entr 0.015943774953484535, f1 0.8734210729598999
ep43_l1_test_time 0.23483414100019218
Test Epoch43 layer2 Acc 0.8728947368421053, AUC 0.9558119177818298, avg_entr 0.012293651700019836, f1 0.8728947639465332
ep43_l2_test_time 0.30575846100009585
Test Epoch43 layer3 Acc 0.873421052631579, AUC 0.957464873790741, avg_entr 0.01117880642414093, f1 0.8734210729598999
ep43_l3_test_time 0.4044350439999107
Test Epoch43 layer4 Acc 0.8728947368421053, AUC 0.9506916403770447, avg_entr 0.010267567820847034, f1 0.8728947639465332
ep43_l4_test_time 0.5315447430002678
gc 0
Train Epoch44 Acc 0.98165 (117798/120000), AUC 0.9980583190917969
ep44_train_time 60.541556504000255
Test Epoch44 layer0 Acc 0.8842105263157894, AUC 0.9671915769577026, avg_entr 0.047761037945747375, f1 0.8842105865478516
ep44_l0_test_time 0.18766000300001906
Test Epoch44 layer1 Acc 0.8728947368421053, AUC 0.9516171813011169, avg_entr 0.016243791207671165, f1 0.8728947639465332
ep44_l1_test_time 0.23355896799967013
Test Epoch44 layer2 Acc 0.8728947368421053, AUC 0.9559915661811829, avg_entr 0.012568599544465542, f1 0.8728947639465332
ep44_l2_test_time 0.30494522400022106
Test Epoch44 layer3 Acc 0.8726315789473684, AUC 0.9570925235748291, avg_entr 0.011356617324054241, f1 0.8726315498352051
ep44_l3_test_time 0.40476493400001345
Test Epoch44 layer4 Acc 0.8728947368421053, AUC 0.9506555795669556, avg_entr 0.010391687974333763, f1 0.8728947639465332
ep44_l4_test_time 0.5315701200001968
gc 0
Train Epoch45 Acc 0.9821 (117852/120000), AUC 0.998254120349884
ep45_train_time 60.57443405699996
Test Epoch45 layer0 Acc 0.8852631578947369, AUC 0.9671778678894043, avg_entr 0.04759310185909271, f1 0.8852631449699402
ep45_l0_test_time 0.18680296999991697
Test Epoch45 layer1 Acc 0.8731578947368421, AUC 0.9509505033493042, avg_entr 0.015710677951574326, f1 0.8731579184532166
ep45_l1_test_time 0.2392392080000718
Test Epoch45 layer2 Acc 0.8731578947368421, AUC 0.9552208185195923, avg_entr 0.01213548518717289, f1 0.8731579184532166
ep45_l2_test_time 0.30685003999997207
Test Epoch45 layer3 Acc 0.8736842105263158, AUC 0.9568325281143188, avg_entr 0.011005669832229614, f1 0.8736842274665833
ep45_l3_test_time 0.40578446699964843
Test Epoch45 layer4 Acc 0.8736842105263158, AUC 0.9507232904434204, avg_entr 0.010061911307275295, f1 0.8736842274665833
ep45_l4_test_time 0.5326715360001799
gc 0
Train Epoch46 Acc 0.981125 (117735/120000), AUC 0.9981309175491333
ep46_train_time 60.71780608099971
Test Epoch46 layer0 Acc 0.8842105263157894, AUC 0.9671433568000793, avg_entr 0.047518063336610794, f1 0.8842105865478516
ep46_l0_test_time 0.18758705500022188
Test Epoch46 layer1 Acc 0.8726315789473684, AUC 0.9513466358184814, avg_entr 0.015890661627054214, f1 0.8726315498352051
ep46_l1_test_time 0.23381074000008084
Test Epoch46 layer2 Acc 0.8728947368421053, AUC 0.9555643796920776, avg_entr 0.01242255698889494, f1 0.8728947639465332
ep46_l2_test_time 0.30533410099997127
Test Epoch46 layer3 Acc 0.8728947368421053, AUC 0.957088828086853, avg_entr 0.011217877268791199, f1 0.8728947639465332
ep46_l3_test_time 0.404674046999844
Test Epoch46 layer4 Acc 0.8736842105263158, AUC 0.9510014057159424, avg_entr 0.01024854276329279, f1 0.8736842274665833
ep46_l4_test_time 0.5320009189999837
gc 0
Train Epoch47 Acc 0.9812666666666666 (117752/120000), AUC 0.9980354309082031
ep47_train_time 60.675046278999616
Test Epoch47 layer0 Acc 0.8847368421052632, AUC 0.9671477675437927, avg_entr 0.04746897891163826, f1 0.8847368359565735
ep47_l0_test_time 0.18798852999998417
Test Epoch47 layer1 Acc 0.8728947368421053, AUC 0.9510120749473572, avg_entr 0.01576019823551178, f1 0.8728947639465332
ep47_l1_test_time 0.23492954600033045
Test Epoch47 layer2 Acc 0.873421052631579, AUC 0.9551868438720703, avg_entr 0.012290735729038715, f1 0.8734210729598999
ep47_l2_test_time 0.30654345899984037
Test Epoch47 layer3 Acc 0.873421052631579, AUC 0.9566895961761475, avg_entr 0.011133701540529728, f1 0.8734210729598999
ep47_l3_test_time 0.4065258149998954
Test Epoch47 layer4 Acc 0.873421052631579, AUC 0.9501022100448608, avg_entr 0.010192501358687878, f1 0.8734210729598999
ep47_l4_test_time 0.5331919089999246
gc 0
Train Epoch48 Acc 0.981525 (117783/120000), AUC 0.9980759620666504
ep48_train_time 60.73504702700029
Test Epoch48 layer0 Acc 0.8847368421052632, AUC 0.9671232104301453, avg_entr 0.04742841795086861, f1 0.8847368359565735
ep48_l0_test_time 0.18749955899966153
Test Epoch48 layer1 Acc 0.8731578947368421, AUC 0.9513824582099915, avg_entr 0.015721840783953667, f1 0.8731579184532166
ep48_l1_test_time 0.2351903650001077
Test Epoch48 layer2 Acc 0.8731578947368421, AUC 0.9555125832557678, avg_entr 0.012285307049751282, f1 0.8731579184532166
ep48_l2_test_time 0.3063896830003614
Test Epoch48 layer3 Acc 0.8728947368421053, AUC 0.9570285677909851, avg_entr 0.011146388947963715, f1 0.8728947639465332
ep48_l3_test_time 0.40519511599995894
Test Epoch48 layer4 Acc 0.8731578947368421, AUC 0.9505331516265869, avg_entr 0.01022303570061922, f1 0.8731579184532166
ep48_l4_test_time 0.5326511789999131
gc 0
Train Epoch49 Acc 0.9814083333333333 (117769/120000), AUC 0.9980682730674744
ep49_train_time 60.59660448900013
Test Epoch49 layer0 Acc 0.885, AUC 0.9671565294265747, avg_entr 0.04726286232471466, f1 0.8849999904632568
ep49_l0_test_time 0.18736447900027997
Test Epoch49 layer1 Acc 0.8726315789473684, AUC 0.9513365626335144, avg_entr 0.015710407868027687, f1 0.8726315498352051
ep49_l1_test_time 0.23395417400024598
Test Epoch49 layer2 Acc 0.873421052631579, AUC 0.9553459882736206, avg_entr 0.012165169231593609, f1 0.8734210729598999
ep49_l2_test_time 0.30581896100011363
Test Epoch49 layer3 Acc 0.8731578947368421, AUC 0.9569754004478455, avg_entr 0.01103538740426302, f1 0.8731579184532166
ep49_l3_test_time 0.4059595140001875
Test Epoch49 layer4 Acc 0.8731578947368421, AUC 0.950448215007782, avg_entr 0.010122143663465977, f1 0.8731579184532166
ep49_l4_test_time 0.532539922000069
Best AUC tensor(0.8866) 24 0
train_as_loss [[4.54679491e+02 3.56916520e+02 3.51190313e+02 3.49896078e+02
  3.49402406e+02 3.49163992e+02 3.49032189e+02 3.48952710e+02
  3.48901819e+02 3.48867811e+02 3.48844333e+02 3.48827732e+02
  3.48815778e+02 3.48807042e+02 3.48801921e+02 3.48798870e+02
  3.48796034e+02 3.48793466e+02 3.48791697e+02 3.48790515e+02
  3.48789361e+02 3.48788193e+02 3.48787327e+02 3.48786746e+02
  3.48786164e+02 3.48785495e+02 3.48785046e+02 3.48784652e+02
  3.48784402e+02 3.48783877e+02 3.48783679e+02 3.48783529e+02
  3.48783239e+02 3.48782914e+02 3.48782800e+02 3.48782749e+02
  3.48782643e+02 3.48782380e+02 3.48782172e+02 3.48782136e+02
  3.48782047e+02 3.48781961e+02 3.48781923e+02 3.48781901e+02
  3.48781884e+02 3.48781866e+02 3.48781860e+02 3.48781845e+02
  3.48781825e+02 3.48781811e+02]
 [1.53563859e+00 1.34405649e-05 8.47085602e-07 1.62566911e-07
  4.92990731e-08 1.88865678e-08 8.31762996e-07 2.95371739e-05
  1.40109292e-04 1.69238893e-04 4.22093697e-04 3.36095432e-04
  3.79220081e-04 6.35530306e-04 6.18499971e-04 9.66601227e-06
  3.96039619e-10 4.25215560e-10 1.24277598e-04 4.09694809e-08
  2.29754763e-10 6.25444846e-09 4.54262667e-05 3.95916116e-10
  1.73830606e-10 9.15526004e-08 9.71191119e-06 2.71746867e-07
  1.29022344e-10 4.86775450e-10 2.48365821e-06 1.19809027e-07
  1.08377445e-10 2.83113099e-10 9.06050386e-07 8.61850669e-09
  8.33792418e-11 1.11442166e-10 2.60559163e-08 4.07366741e-09
  6.58044413e-11 1.13387510e-10 4.44054621e-08 4.13218675e-11
  4.31253999e-11 5.30246803e-11 6.13304011e-11 2.92598449e-11
  2.99033140e-11 3.41562953e-11]
 [2.21085070e+00 1.54689161e-05 1.12068341e-06 2.35332488e-07
  8.02903213e-08 3.52053355e-08 4.64941688e-07 2.95279871e-05
  1.99421287e-04 3.05525468e-04 7.36091721e-04 6.49413911e-04
  7.13934325e-04 1.11402024e-03 1.06376814e-03 3.17506219e-05
  1.10388431e-09 1.55889549e-09 2.64851228e-04 3.69057283e-08
  6.94142692e-10 5.58039765e-09 7.66687631e-05 7.68529044e-10
  5.58692559e-10 8.08558911e-08 1.53369601e-05 4.23660910e-07
  4.50029991e-10 1.26558582e-09 4.89788108e-06 1.24899876e-07
  3.65456087e-10 8.39755866e-10 1.63249676e-06 1.95104779e-08
  2.66392020e-10 4.51630218e-10 4.09478319e-08 7.58409734e-09
  2.55538656e-10 5.28223160e-10 9.38748604e-08 1.24991899e-10
  1.53275827e-10 2.17767601e-10 2.90760959e-10 1.02824406e-10
  1.25244362e-10 1.58384410e-10]
 [2.01588176e+00 1.56694248e-05 1.34776902e-06 3.25608492e-07
  1.38720149e-07 7.59579612e-08 5.92741009e-07 3.70637335e-05
  3.11018915e-04 4.95044364e-04 1.24498162e-03 4.01685458e-04
  4.47399127e-04 7.60883799e-04 7.79875359e-04 2.25937628e-05
  2.14526555e-09 2.77536375e-09 2.01025439e-04 2.56686122e-08
  1.21392797e-09 4.69339522e-09 5.74222134e-05 1.02583183e-09
  9.93317158e-10 5.58049774e-08 1.10330520e-05 3.14632258e-07
  7.66080747e-10 1.72471658e-09 3.49793229e-06 8.77077092e-08
  6.12864534e-10 1.19549609e-09 1.22316240e-06 1.47663136e-08
  4.43361824e-10 6.79683169e-10 2.81735830e-08 5.45401180e-09
  4.02006687e-10 7.26482441e-10 6.91432133e-08 1.85291574e-10
  2.24336972e-10 3.04542332e-10 3.85166922e-10 1.36879718e-10
  1.65275087e-10 2.12208755e-10]
 [2.08056267e+00 1.72731307e-05 1.75608356e-06 5.24605112e-07
  2.96259360e-07 2.03587972e-07 8.65027205e-07 5.28895380e-05
  5.26761802e-04 7.82628514e-04 1.77145773e-03 3.61793508e-04
  3.95386172e-04 7.00895997e-04 7.24524109e-04 2.42196202e-05
  5.70699136e-09 8.25332766e-09 2.14903692e-04 2.61483000e-08
  2.90978680e-09 7.30987123e-09 5.54697380e-05 2.17736676e-09
  2.83416668e-09 4.97605298e-08 1.10569686e-05 3.13089601e-07
  2.17775610e-09 4.88311952e-09 3.61420761e-06 8.06005945e-08
  1.69516380e-09 3.43715047e-09 1.21291541e-06 1.77003048e-08
  1.14936459e-09 1.78292275e-09 2.98567637e-08 6.27817488e-09
  9.79053470e-10 1.86913340e-09 7.23518240e-08 3.76041060e-10
  4.61726534e-10 6.58436688e-10 8.28983260e-10 2.54362174e-10
  3.06484442e-10 3.96094063e-10]]
train_ae_loss [[12.80163665 11.53423576 11.63699139 11.23908123 10.84529835 10.55345961
  10.33792135 10.12675624 10.00955101  9.80684364  9.67085031  9.50759968
   9.37854875  9.24628721  8.48561884  8.2364698   8.11351369  8.01545234
   7.59520524  7.4706766   7.40480174  7.37900307  7.17421817  7.12175479
   7.10113334  7.11445471  7.00634377  7.00219778  7.00268038  7.01983418
   6.98456859  6.98260074  7.02387531  7.03976298  7.02499541  7.04884459
   7.08201443  7.08298135  7.10127859  7.1132687   7.10790206  7.15804876
   7.16978068  7.1706106   7.18339502  7.2083954   7.20173445  7.21446046
   7.20951943  7.22791348]
 [13.26727842 11.09877521  9.88028928  9.01688627  8.44059223  7.95131186
   7.5636397   7.16166577  6.93607876  6.56994688  6.29247433  5.99406961
   5.73649582  5.44260616  4.72468456  4.19092348  3.99240172  3.79855419
   3.3793183   3.17473116  3.07872545  2.98499029  2.81744737  2.6709634
   2.61652893  2.61373146  2.52645436  2.45222139  2.41235394  2.4203814
   2.34310163  2.30700707  2.30636683  2.31442246  2.27516581  2.30087222
   2.3254242   2.29899382  2.28830504  2.2916846   2.27142251  2.31669997
   2.31420787  2.29592132  2.30196179  2.29603053  2.31643592  2.2935976
   2.30557346  2.27139875]
 [13.44485348 10.04280534  8.21819159  7.6123528   7.13404126  6.63899916
   6.18539992  5.747998    5.52962159  5.19960893  4.95472986  4.7006951
   4.49021507  4.23190904  3.67378529  3.20154518  3.0454298   2.89102589
   2.5533271   2.38196253  2.3137339   2.23424633  2.10658185  1.97863196
   1.93940877  1.94069254  1.87023772  1.80290444  1.76616997  1.77730282
   1.71335414  1.68130003  1.68005342  1.68984757  1.64890263  1.67613885
   1.68983613  1.66918182  1.65965231  1.66728186  1.64512747  1.68157508
   1.6786194   1.65773462  1.66736325  1.66169288  1.67762047  1.66085328
   1.66681506  1.64166717]
 [13.20256945  9.9862865   7.37843121  6.78456844  6.30868161  5.85127973
   5.45307814  5.03269005  4.86096358  4.54193709  4.32214544  4.00912837
   3.83414312  3.61707282  3.14881995  2.73933722  2.60613128  2.46936599
   2.1827169   2.03447516  1.97396044  1.90226892  1.79555557  1.68235644
   1.6491038   1.64915765  1.59066906  1.53136597  1.49722284  1.5044013
   1.44992484  1.4225494   1.42257696  1.43015358  1.3931387   1.41623585
   1.43071062  1.40873859  1.40069684  1.40643169  1.3884923   1.41657982
   1.4143427   1.39847267  1.40333745  1.40027323  1.41329745  1.39702934
   1.40290872  1.38244115]
 [13.70197587 11.6825441   7.39748562  6.82995212  6.32385037  5.85732408
   5.38628309  4.90786606  4.73735156  4.41613446  4.19785118  3.85835807
   3.68814747  3.47712553  3.04361775  2.630249    2.50260911  2.36741048
   2.09179909  1.94305185  1.88394261  1.81462686  1.71351722  1.60167204
   1.56952265  1.56969685  1.51233777  1.45412571  1.42028852  1.42871482
   1.37384174  1.34793921  1.34720921  1.35267367  1.32136099  1.34056889
   1.35311387  1.33086822  1.32302656  1.32810248  1.31057957  1.33742883
   1.33603594  1.31999624  1.32397954  1.32067923  1.33369846  1.31512108
   1.32622027  1.30342546]]
valid_acc (5, 50)
valid_AUC (5, 50)
train_acc (50,)
total_train+valid_time 3130.3944413179997
Start Testing
Load ckpt at ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt
gc 9
Test layer0 Acc 0.8744736842105263, AUC 0.9664355516433716, avg_entr 0.05912325158715248, f1 0.8744736909866333
l0_test_time 0.18704414300009375
gc 0
Test layer1 Acc 0.8697368421052631, AUC 0.9525283575057983, avg_entr 0.02196490205824375, f1 0.8697368502616882
l1_test_time 0.23431359400001384
gc 0
Test layer2 Acc 0.8705263157894737, AUC 0.9576393961906433, avg_entr 0.017010370269417763, f1 0.8705263137817383
l2_test_time 0.3082337139999254
gc 0
Test layer3 Acc 0.8707894736842106, AUC 0.9576686024665833, avg_entr 0.015679193660616875, f1 0.8707894682884216
l3_test_time 0.4062793640000564
gc 0
Test layer4 Acc 0.8705263157894737, AUC 0.9524074196815491, avg_entr 0.014537373557686806, f1 0.8705263137817383
l4_test_time 0.5320844379998562
gc 0
Test threshold 0.1 Acc 0.8713157894736843, AUC 0.9590445756912231, avg_entr 0.015932483598589897, f1 0.8713157176971436
t0.1_test_time 0.24299227699975745
gc 0
Test threshold 0.2 Acc 0.8710526315789474, AUC 0.9618586301803589, avg_entr 0.021243991330266, f1 0.871052622795105
t0.2_test_time 0.24050966200002222
gc 0
Test threshold 0.3 Acc 0.8721052631578947, AUC 0.9642190933227539, avg_entr 0.030091620981693268, f1 0.8721052408218384
t0.3_test_time 0.23454869499983033
gc 0
Test threshold 0.4 Acc 0.873421052631579, AUC 0.9653931856155396, avg_entr 0.036678947508335114, f1 0.8734210729598999
t0.4_test_time 0.21942165099972044
gc 0
Test threshold 0.5 Acc 0.8739473684210526, AUC 0.96616530418396, avg_entr 0.04130028188228607, f1 0.8739473819732666
t0.5_test_time 0.2058628099998714
gc 0
Test threshold 0.6 Acc 0.8744736842105263, AUC 0.9664355516433716, avg_entr 0.04264840483665466, f1 0.8744736909866333
t0.6_test_time 0.19901283799981684
gc 0
Test threshold 0.7 Acc 0.8744736842105263, AUC 0.9664355516433716, avg_entr 0.04264840483665466, f1 0.8744736909866333
t0.7_test_time 0.199135046000265
gc 0
Test threshold 0.8 Acc 0.8744736842105263, AUC 0.9664355516433716, avg_entr 0.04264840483665466, f1 0.8744736909866333
t0.8_test_time 0.19915189399989686
gc 0
Test threshold 0.9 Acc 0.8744736842105263, AUC 0.9664355516433716, avg_entr 0.04264840483665466, f1 0.8744736909866333
t0.9_test_time 0.19987106999997195
