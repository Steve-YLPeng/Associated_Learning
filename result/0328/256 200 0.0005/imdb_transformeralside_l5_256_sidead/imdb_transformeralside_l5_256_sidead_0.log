total count words 222751
vocab size 30000
train size 40000, valid size 5000, test size 5000
found 27937 words in glove
model: TransformerALsideText(
  (layers): ModuleList(
    (0): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=2, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=2, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
  (emb_layers): ModuleList(
    (0): Embedding(30000, 300)
    (1): Embedding(30000, 300)
    (2): Embedding(30000, 300)
    (3): Embedding(30000, 300)
    (4): Embedding(30000, 300)
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.multi_headed_attention.linears.0.weight 90000
layers.0.enc.f.multi_headed_attention.linears.0.bias 300
layers.0.enc.f.multi_headed_attention.linears.1.weight 90000
layers.0.enc.f.multi_headed_attention.linears.1.bias 300
layers.0.enc.f.multi_headed_attention.linears.2.weight 90000
layers.0.enc.f.multi_headed_attention.linears.2.bias 300
layers.0.enc.f.multi_headed_attention.linears.3.weight 90000
layers.0.enc.f.multi_headed_attention.linears.3.bias 300
layers.0.enc.f.feed_forward.w_1.weight 90000
layers.0.enc.f.feed_forward.w_1.bias 300
layers.0.enc.f.feed_forward.w_2.weight 90000
layers.0.enc.f.feed_forward.w_2.bias 300
layers.0.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.0.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.0.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.0.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.0.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.0.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.0.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.0.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.0.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.0.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.0.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.0.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.0.enc.f.encoder.norm.a 300
layers.0.enc.f.encoder.norm.b 300
layers.0.ae.g.0.weight 256
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 256
layers.0.ae.h.0.bias 2
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
emb_layers.0.weight 9000000
emb_layers.1.weight 9000000
emb_layers.2.weight 9000000
emb_layers.3.weight 9000000
emb_layers.4.weight 9000000
Total Trainable Params: 50758378
init_time 21.53109458
Start Training
gc 0
Train Epoch0 Acc 0.495625 (19825/40000), AUC 0.4970123767852783
ep0_train_time 23.343083657999998
Test Epoch0 layer0 Acc 0.5648, AUC 0.5924551486968994, avg_entr 0.6841526031494141, f1 0.5648000240325928
ep0_l0_test_time 0.28435924600000106
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 0
Test Epoch0 layer1 Acc 0.5168, AUC 0.5592688322067261, avg_entr 0.6934634447097778, f1 0.5167999863624573
ep0_l1_test_time 0.3542667729999991
Test Epoch0 layer2 Acc 0.5328, AUC 0.540775716304779, avg_entr 0.6872057318687439, f1 0.532800018787384
ep0_l2_test_time 0.4622961549999971
Test Epoch0 layer3 Acc 0.4934, AUC 0.47802799940109253, avg_entr 0.6902055144309998, f1 0.4934000074863434
ep0_l3_test_time 0.611440236
Test Epoch0 layer4 Acc 0.4956, AUC 0.49623605608940125, avg_entr 0.6882208585739136, f1 0.49559998512268066
ep0_l4_test_time 0.8074313520000018
gc 0
Train Epoch1 Acc 0.50955 (20382/40000), AUC 0.5149320960044861
ep1_train_time 23.070184552
Test Epoch1 layer0 Acc 0.573, AUC 0.6277874708175659, avg_entr 0.6520224809646606, f1 0.5730000138282776
ep1_l0_test_time 0.28424627399999736
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 1
Test Epoch1 layer1 Acc 0.5378, AUC 0.6487479209899902, avg_entr 0.6675255298614502, f1 0.5378000140190125
ep1_l1_test_time 0.3569797739999956
Test Epoch1 layer2 Acc 0.5062, AUC 0.6355408430099487, avg_entr 0.6676241159439087, f1 0.5062000155448914
ep1_l2_test_time 0.46302414499999145
Test Epoch1 layer3 Acc 0.4956, AUC 0.48943349719047546, avg_entr 0.686188280582428, f1 0.49559998512268066
ep1_l3_test_time 0.6117568190000071
Test Epoch1 layer4 Acc 0.5056, AUC 0.5005214214324951, avg_entr 0.6889856457710266, f1 0.5055999755859375
ep1_l4_test_time 0.8066814729999976
gc 0
Train Epoch2 Acc 0.5123 (20492/40000), AUC 0.5168790817260742
ep2_train_time 23.146786166
Test Epoch2 layer0 Acc 0.6204, AUC 0.6685324907302856, avg_entr 0.575137197971344, f1 0.6204000115394592
ep2_l0_test_time 0.2826986239999911
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 2
Test Epoch2 layer1 Acc 0.5656, AUC 0.6942856907844543, avg_entr 0.4716993570327759, f1 0.5655999779701233
ep2_l1_test_time 0.3545436059999929
Test Epoch2 layer2 Acc 0.5406, AUC 0.6888127326965332, avg_entr 0.47783362865448, f1 0.5406000018119812
ep2_l2_test_time 0.46393126499999937
Test Epoch2 layer3 Acc 0.5216, AUC 0.674780011177063, avg_entr 0.5879533886909485, f1 0.5216000080108643
ep2_l3_test_time 0.6120727889999955
Test Epoch2 layer4 Acc 0.553, AUC 0.5911127328872681, avg_entr 0.6865274906158447, f1 0.5529999732971191
ep2_l4_test_time 0.8068439979999908
gc 0
Train Epoch3 Acc 0.5496 (21984/40000), AUC 0.573371410369873
ep3_train_time 22.969075329000006
Test Epoch3 layer0 Acc 0.5812, AUC 0.7374504208564758, avg_entr 0.27519240975379944, f1 0.5812000036239624
ep3_l0_test_time 0.29666497399999514
Test Epoch3 layer1 Acc 0.5694, AUC 0.7651940584182739, avg_entr 0.2361512929201126, f1 0.5694000124931335
ep3_l1_test_time 0.35450267400000257
Test Epoch3 layer2 Acc 0.5394, AUC 0.7617859840393066, avg_entr 0.201076939702034, f1 0.5393999814987183
ep3_l2_test_time 0.466244961000001
Test Epoch3 layer3 Acc 0.513, AUC 0.7051639556884766, avg_entr 0.21356970071792603, f1 0.5130000114440918
ep3_l3_test_time 0.6164971099999974
Test Epoch3 layer4 Acc 0.5, AUC 0.7072759866714478, avg_entr 0.29319167137145996, f1 0.5
ep3_l4_test_time 0.8129382789999937
gc 0
Train Epoch4 Acc 0.61875 (24750/40000), AUC 0.6629759073257446
ep4_train_time 23.135446295999998
Test Epoch4 layer0 Acc 0.6718, AUC 0.7614218592643738, avg_entr 0.4044942259788513, f1 0.6718000173568726
ep4_l0_test_time 0.2853207660000123
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 4
Test Epoch4 layer1 Acc 0.7084, AUC 0.7929421663284302, avg_entr 0.3886483907699585, f1 0.7084000110626221
ep4_l1_test_time 0.356193281000003
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 4
Test Epoch4 layer2 Acc 0.7136, AUC 0.797540545463562, avg_entr 0.3973764181137085, f1 0.7135999798774719
ep4_l2_test_time 0.4644621809999876
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 4
Test Epoch4 layer3 Acc 0.7174, AUC 0.7958569526672363, avg_entr 0.4258632957935333, f1 0.717400074005127
ep4_l3_test_time 0.6127360140000064
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 4
Test Epoch4 layer4 Acc 0.7166, AUC 0.7933672666549683, avg_entr 0.48111048340797424, f1 0.7165999412536621
ep4_l4_test_time 0.8089549869999928
gc 0
Train Epoch5 Acc 0.680425 (27217/40000), AUC 0.7481964826583862
ep5_train_time 23.052251283999993
Test Epoch5 layer0 Acc 0.6766, AUC 0.7748581171035767, avg_entr 0.3528420031070709, f1 0.6765999794006348
ep5_l0_test_time 0.2830505710000182
Test Epoch5 layer1 Acc 0.6698, AUC 0.8079549074172974, avg_entr 0.28730079531669617, f1 0.6697999835014343
ep5_l1_test_time 0.35307897600000615
Test Epoch5 layer2 Acc 0.644, AUC 0.8156485557556152, avg_entr 0.24664467573165894, f1 0.6439999938011169
ep5_l2_test_time 0.4639964710000015
Test Epoch5 layer3 Acc 0.6192, AUC 0.8164731860160828, avg_entr 0.22734177112579346, f1 0.6191999912261963
ep5_l3_test_time 0.6123704369999814
Test Epoch5 layer4 Acc 0.5962, AUC 0.8151533603668213, avg_entr 0.2097964584827423, f1 0.5961999893188477
ep5_l4_test_time 0.807325560999999
gc 0
Train Epoch6 Acc 0.714175 (28567/40000), AUC 0.7799898386001587
ep6_train_time 23.05334003499999
Test Epoch6 layer0 Acc 0.7032, AUC 0.7815752625465393, avg_entr 0.3518363833427429, f1 0.7031999826431274
ep6_l0_test_time 0.2845238889999848
Test Epoch6 layer1 Acc 0.7254, AUC 0.8188151121139526, avg_entr 0.29245689511299133, f1 0.7253999710083008
ep6_l1_test_time 0.3623512820000201
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 6
Test Epoch6 layer2 Acc 0.7288, AUC 0.8263800740242004, avg_entr 0.2783145010471344, f1 0.7287999987602234
ep6_l2_test_time 0.47496601600002464
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 6
Test Epoch6 layer3 Acc 0.7334, AUC 0.8282384872436523, avg_entr 0.2754009962081909, f1 0.7333999872207642
ep6_l3_test_time 0.613813313999998
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 6
Test Epoch6 layer4 Acc 0.7276, AUC 0.8279656767845154, avg_entr 0.27132144570350647, f1 0.7275999784469604
ep6_l4_test_time 0.8087659429999974
gc 0
Train Epoch7 Acc 0.762825 (30513/40000), AUC 0.8425710797309875
ep7_train_time 23.11745380299999
Test Epoch7 layer0 Acc 0.7062, AUC 0.7882909774780273, avg_entr 0.33037269115448, f1 0.7062000036239624
ep7_l0_test_time 0.28584884800000054
Test Epoch7 layer1 Acc 0.7338, AUC 0.826099157333374, avg_entr 0.28454136848449707, f1 0.7338000535964966
ep7_l1_test_time 0.35684484699999075
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 7
Test Epoch7 layer2 Acc 0.733, AUC 0.836371660232544, avg_entr 0.2651958167552948, f1 0.7329999804496765
ep7_l2_test_time 0.46595205700000975
Test Epoch7 layer3 Acc 0.7222, AUC 0.8394684791564941, avg_entr 0.2671913504600525, f1 0.7222000360488892
ep7_l3_test_time 0.6148518249999881
Test Epoch7 layer4 Acc 0.7152, AUC 0.840389609336853, avg_entr 0.27158477902412415, f1 0.7151999473571777
ep7_l4_test_time 0.8101164630000142
gc 0
Train Epoch8 Acc 0.7692 (30768/40000), AUC 0.8530800342559814
ep8_train_time 23.018674578999992
Test Epoch8 layer0 Acc 0.7184, AUC 0.7933130264282227, avg_entr 0.29511621594429016, f1 0.7184000015258789
ep8_l0_test_time 0.2827539860000172
Test Epoch8 layer1 Acc 0.7508, AUC 0.8328930139541626, avg_entr 0.2569945454597473, f1 0.7508000135421753
ep8_l1_test_time 0.3540461820000189
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 8
Test Epoch8 layer2 Acc 0.7608, AUC 0.8468341827392578, avg_entr 0.2453235387802124, f1 0.7608000040054321
ep8_l2_test_time 0.46330154099999277
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 8
Test Epoch8 layer3 Acc 0.7676, AUC 0.8493314385414124, avg_entr 0.23054736852645874, f1 0.7675999402999878
ep8_l3_test_time 0.6132618389999891
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 8
Test Epoch8 layer4 Acc 0.7678, AUC 0.8505222797393799, avg_entr 0.23845773935317993, f1 0.767799973487854
ep8_l4_test_time 0.8086649510000257
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 8
gc 0
Train Epoch9 Acc 0.798075 (31923/40000), AUC 0.8795800805091858
ep9_train_time 23.059142115000043
Test Epoch9 layer0 Acc 0.6844, AUC 0.7961719036102295, avg_entr 0.28332191705703735, f1 0.6844000220298767
ep9_l0_test_time 0.2833231549999482
Test Epoch9 layer1 Acc 0.7012, AUC 0.8341816663742065, avg_entr 0.22851933538913727, f1 0.701200008392334
ep9_l1_test_time 0.35400211299997864
Test Epoch9 layer2 Acc 0.6964, AUC 0.8496562242507935, avg_entr 0.21047915518283844, f1 0.696399986743927
ep9_l2_test_time 0.465180241999974
Test Epoch9 layer3 Acc 0.689, AUC 0.850456714630127, avg_entr 0.17383356392383575, f1 0.6890000104904175
ep9_l3_test_time 0.6147075860000086
Test Epoch9 layer4 Acc 0.6858, AUC 0.8535112738609314, avg_entr 0.17664074897766113, f1 0.6858000159263611
ep9_l4_test_time 0.8080225819999782
gc 0
Train Epoch10 Acc 0.815075 (32603/40000), AUC 0.8939716219902039
ep10_train_time 23.054156223999996
Test Epoch10 layer0 Acc 0.7116, AUC 0.7968230843544006, avg_entr 0.26233983039855957, f1 0.7116000652313232
ep10_l0_test_time 0.2889013419999742
Test Epoch10 layer1 Acc 0.7176, AUC 0.8366985321044922, avg_entr 0.20512373745441437, f1 0.7175999879837036
ep10_l1_test_time 0.3541488070000014
Test Epoch10 layer2 Acc 0.7146, AUC 0.8531742691993713, avg_entr 0.18854807317256927, f1 0.7146000266075134
ep10_l2_test_time 0.46312241200001836
Test Epoch10 layer3 Acc 0.7116, AUC 0.8571743965148926, avg_entr 0.14123117923736572, f1 0.7116000652313232
ep10_l3_test_time 0.612404435999963
Test Epoch10 layer4 Acc 0.7098, AUC 0.8586403131484985, avg_entr 0.14256209135055542, f1 0.7098000049591064
ep10_l4_test_time 0.8076770499999952
gc 0
Train Epoch11 Acc 0.834825 (33393/40000), AUC 0.9116729497909546
ep11_train_time 23.089584505999994
Test Epoch11 layer0 Acc 0.721, AUC 0.7942069172859192, avg_entr 0.260017991065979, f1 0.7210000157356262
ep11_l0_test_time 0.2831015139999522
Test Epoch11 layer1 Acc 0.7436, AUC 0.8320985436439514, avg_entr 0.21629440784454346, f1 0.7436000108718872
ep11_l1_test_time 0.35351502299999993
Test Epoch11 layer2 Acc 0.763, AUC 0.8477445244789124, avg_entr 0.18662279844284058, f1 0.7630000114440918
ep11_l2_test_time 0.46334808599999633
Test Epoch11 layer3 Acc 0.7632, AUC 0.8514944911003113, avg_entr 0.13724009692668915, f1 0.7631999850273132
ep11_l3_test_time 0.6129498390000094
Test Epoch11 layer4 Acc 0.764, AUC 0.8526172637939453, avg_entr 0.13479666411876678, f1 0.7639999985694885
ep11_l4_test_time 0.8105218479999508
gc 0
Train Epoch12 Acc 0.85965 (34386/40000), AUC 0.9303848147392273
ep12_train_time 23.16095924000001
Test Epoch12 layer0 Acc 0.7038, AUC 0.7920423746109009, avg_entr 0.2222418189048767, f1 0.7038000226020813
ep12_l0_test_time 0.2881380530000115
Test Epoch12 layer1 Acc 0.7178, AUC 0.8294016122817993, avg_entr 0.1802217811346054, f1 0.7178000211715698
ep12_l1_test_time 0.3751380869999821
Test Epoch12 layer2 Acc 0.728, AUC 0.8446117639541626, avg_entr 0.1261880099773407, f1 0.7279999852180481
ep12_l2_test_time 0.4640185149999638
Test Epoch12 layer3 Acc 0.7272, AUC 0.8467975854873657, avg_entr 0.11005809158086777, f1 0.7271999716758728
ep12_l3_test_time 0.6136274640000465
Test Epoch12 layer4 Acc 0.7274, AUC 0.8480185270309448, avg_entr 0.10946718603372574, f1 0.7274000644683838
ep12_l4_test_time 0.8080348819999585
gc 0
Train Epoch13 Acc 0.870075 (34803/40000), AUC 0.9368249177932739
ep13_train_time 23.07372515899999
Test Epoch13 layer0 Acc 0.7044, AUC 0.7874664068222046, avg_entr 0.22064854204654694, f1 0.7044000029563904
ep13_l0_test_time 0.2834272960000135
Test Epoch13 layer1 Acc 0.7148, AUC 0.8223657011985779, avg_entr 0.1809648722410202, f1 0.7148000597953796
ep13_l1_test_time 0.3531237469999837
Test Epoch13 layer2 Acc 0.7214, AUC 0.8364731073379517, avg_entr 0.11352873593568802, f1 0.7214000225067139
ep13_l2_test_time 0.46281474299996717
Test Epoch13 layer3 Acc 0.717, AUC 0.8393073081970215, avg_entr 0.10190707445144653, f1 0.7169999480247498
ep13_l3_test_time 0.6129961170000229
Test Epoch13 layer4 Acc 0.715, AUC 0.8406615853309631, avg_entr 0.09918707609176636, f1 0.7150000333786011
ep13_l4_test_time 0.8076097899999581
gc 0
Train Epoch14 Acc 0.8846 (35384/40000), AUC 0.9473158717155457
ep14_train_time 23.011094814000046
Test Epoch14 layer0 Acc 0.7018, AUC 0.7823091745376587, avg_entr 0.22567617893218994, f1 0.7017999887466431
ep14_l0_test_time 0.28460168099996963
Test Epoch14 layer1 Acc 0.7338, AUC 0.820438027381897, avg_entr 0.18567661941051483, f1 0.7338000535964966
ep14_l1_test_time 0.35567132600004925
Test Epoch14 layer2 Acc 0.747, AUC 0.834607720375061, avg_entr 0.11368434876203537, f1 0.746999979019165
ep14_l2_test_time 0.47166806399997085
Test Epoch14 layer3 Acc 0.7492, AUC 0.8390970230102539, avg_entr 0.1018456444144249, f1 0.7491999864578247
ep14_l3_test_time 0.6125846990000241
Test Epoch14 layer4 Acc 0.7514, AUC 0.8416707515716553, avg_entr 0.10035508126020432, f1 0.7513999938964844
ep14_l4_test_time 0.8083934480000039
gc 0
Train Epoch15 Acc 0.913625 (36545/40000), AUC 0.9676007032394409
ep15_train_time 23.097502658000053
Test Epoch15 layer0 Acc 0.7054, AUC 0.7812155485153198, avg_entr 0.20895031094551086, f1 0.7053999900817871
ep15_l0_test_time 0.2831495580000478
Test Epoch15 layer1 Acc 0.74, AUC 0.8148759603500366, avg_entr 0.17015932500362396, f1 0.7400000095367432
ep15_l1_test_time 0.3531553730000496
Test Epoch15 layer2 Acc 0.7486, AUC 0.827299952507019, avg_entr 0.08719927817583084, f1 0.7486000061035156
ep15_l2_test_time 0.4625143289999869
Test Epoch15 layer3 Acc 0.7498, AUC 0.8320729732513428, avg_entr 0.07829414308071136, f1 0.7498000264167786
ep15_l3_test_time 0.6135217930000181
Test Epoch15 layer4 Acc 0.7496, AUC 0.8346561193466187, avg_entr 0.07580755650997162, f1 0.7495999932289124
ep15_l4_test_time 0.8118920619999699
gc 0
Train Epoch16 Acc 0.92045 (36818/40000), AUC 0.9740462303161621
ep16_train_time 23.104652799000007
Test Epoch16 layer0 Acc 0.7028, AUC 0.7786625623703003, avg_entr 0.19609999656677246, f1 0.7027999758720398
ep16_l0_test_time 0.2841584969999644
Test Epoch16 layer1 Acc 0.7216, AUC 0.81214439868927, avg_entr 0.1535690277814865, f1 0.7215999960899353
ep16_l1_test_time 0.3548805140000013
Test Epoch16 layer2 Acc 0.7354, AUC 0.8240561485290527, avg_entr 0.0856274664402008, f1 0.7354000210762024
ep16_l2_test_time 0.4724051990000362
Test Epoch16 layer3 Acc 0.7366, AUC 0.8319177627563477, avg_entr 0.08074487000703812, f1 0.7365999817848206
ep16_l3_test_time 0.6133493119999684
Test Epoch16 layer4 Acc 0.7378, AUC 0.8338256478309631, avg_entr 0.07846354693174362, f1 0.7378000020980835
ep16_l4_test_time 0.8124422949999826
gc 0
Train Epoch17 Acc 0.9217 (36868/40000), AUC 0.9733048677444458
ep17_train_time 23.08133422100002
Test Epoch17 layer0 Acc 0.7056, AUC 0.7777277827262878, avg_entr 0.20011141896247864, f1 0.7056000232696533
ep17_l0_test_time 0.2858499660000007
Test Epoch17 layer1 Acc 0.736, AUC 0.8103216886520386, avg_entr 0.15452560782432556, f1 0.7360000014305115
ep17_l1_test_time 0.3531884669999954
Test Epoch17 layer2 Acc 0.7478, AUC 0.8220683932304382, avg_entr 0.07741015404462814, f1 0.7477999925613403
ep17_l2_test_time 0.4643070579999744
Test Epoch17 layer3 Acc 0.7482, AUC 0.8283657431602478, avg_entr 0.0720953717827797, f1 0.748199999332428
ep17_l3_test_time 0.6137998030000062
Test Epoch17 layer4 Acc 0.7488, AUC 0.8315154314041138, avg_entr 0.07128982245922089, f1 0.7487999796867371
ep17_l4_test_time 0.8081010340000034
gc 0
Train Epoch18 Acc 0.93085 (37234/40000), AUC 0.9767805337905884
ep18_train_time 22.99415968200003
Test Epoch18 layer0 Acc 0.702, AUC 0.7768842577934265, avg_entr 0.20106516778469086, f1 0.7020000219345093
ep18_l0_test_time 0.2846035869999355
Test Epoch18 layer1 Acc 0.7236, AUC 0.8066189289093018, avg_entr 0.1437857300043106, f1 0.7236000299453735
ep18_l1_test_time 0.3547528199999306
Test Epoch18 layer2 Acc 0.734, AUC 0.8172779679298401, avg_entr 0.07860551029443741, f1 0.7339999675750732
ep18_l2_test_time 0.46483550800007833
Test Epoch18 layer3 Acc 0.7344, AUC 0.8249986171722412, avg_entr 0.0739394873380661, f1 0.7343999147415161
ep18_l3_test_time 0.6148479490000227
Test Epoch18 layer4 Acc 0.7366, AUC 0.8277238607406616, avg_entr 0.07282805442810059, f1 0.7365999817848206
ep18_l4_test_time 0.8088787919999731
gc 0
Train Epoch19 Acc 0.9425 (37700/40000), AUC 0.9844444394111633
ep19_train_time 23.188368714000035
Test Epoch19 layer0 Acc 0.7064, AUC 0.7736682891845703, avg_entr 0.19637072086334229, f1 0.7063999772071838
ep19_l0_test_time 0.291649987000028
Test Epoch19 layer1 Acc 0.719, AUC 0.7978515028953552, avg_entr 0.10845638066530228, f1 0.718999981880188
ep19_l1_test_time 0.35640152900009525
Test Epoch19 layer2 Acc 0.737, AUC 0.8104079961776733, avg_entr 0.06801775842905045, f1 0.7369999885559082
ep19_l2_test_time 0.4674542189999329
Test Epoch19 layer3 Acc 0.7384, AUC 0.818963885307312, avg_entr 0.06414294987916946, f1 0.7384000420570374
ep19_l3_test_time 0.6132723800000122
Test Epoch19 layer4 Acc 0.7392, AUC 0.8219351172447205, avg_entr 0.06217758357524872, f1 0.7391999959945679
ep19_l4_test_time 0.8087513279999712
gc 0
Train Epoch20 Acc 0.946075 (37843/40000), AUC 0.9858554601669312
ep20_train_time 23.072815902000002
Test Epoch20 layer0 Acc 0.7034, AUC 0.7702013850212097, avg_entr 0.1885651797056198, f1 0.7034000158309937
ep20_l0_test_time 0.28685022799993476
Test Epoch20 layer1 Acc 0.716, AUC 0.7950091361999512, avg_entr 0.09003409743309021, f1 0.7160000205039978
ep20_l1_test_time 0.3562043319999475
Test Epoch20 layer2 Acc 0.7336, AUC 0.8078466653823853, avg_entr 0.06166176125407219, f1 0.7335999608039856
ep20_l2_test_time 0.4737174210000603
Test Epoch20 layer3 Acc 0.7362, AUC 0.8156358003616333, avg_entr 0.057001519948244095, f1 0.7361999750137329
ep20_l3_test_time 0.6135322339999902
Test Epoch20 layer4 Acc 0.7376, AUC 0.818597137928009, avg_entr 0.054179199039936066, f1 0.7376000285148621
ep20_l4_test_time 0.8075028780000366
gc 0
Train Epoch21 Acc 0.9482 (37928/40000), AUC 0.9863905310630798
ep21_train_time 22.99391373499998
Test Epoch21 layer0 Acc 0.7008, AUC 0.7702739238739014, avg_entr 0.1874445229768753, f1 0.7008000016212463
ep21_l0_test_time 0.284246924999934
Test Epoch21 layer1 Acc 0.7172, AUC 0.7911847829818726, avg_entr 0.08081533759832382, f1 0.717199981212616
ep21_l1_test_time 0.3546055089999527
Test Epoch21 layer2 Acc 0.7314, AUC 0.8078665733337402, avg_entr 0.05932118371129036, f1 0.7314000129699707
ep21_l2_test_time 0.46403472800000145
Test Epoch21 layer3 Acc 0.7346, AUC 0.8139681220054626, avg_entr 0.05547598749399185, f1 0.7345999479293823
ep21_l3_test_time 0.6137731979999899
Test Epoch21 layer4 Acc 0.7348, AUC 0.8172256946563721, avg_entr 0.05298341065645218, f1 0.7347999811172485
ep21_l4_test_time 0.8105251270000053
gc 0
Train Epoch22 Acc 0.9517 (38068/40000), AUC 0.987606406211853
ep22_train_time 23.05106583199995
Test Epoch22 layer0 Acc 0.6992, AUC 0.7691221833229065, avg_entr 0.18226782977581024, f1 0.6991999745368958
ep22_l0_test_time 0.2889577419999796
Test Epoch22 layer1 Acc 0.7118, AUC 0.7907041311264038, avg_entr 0.08010197430849075, f1 0.7117999792098999
ep22_l1_test_time 0.354439712000044
Test Epoch22 layer2 Acc 0.7314, AUC 0.8096486330032349, avg_entr 0.061759382486343384, f1 0.7314000129699707
ep22_l2_test_time 0.4635733939999227
Test Epoch22 layer3 Acc 0.7328, AUC 0.8163747787475586, avg_entr 0.05795938894152641, f1 0.7327999472618103
ep22_l3_test_time 0.6133465200000501
Test Epoch22 layer4 Acc 0.7344, AUC 0.8195919990539551, avg_entr 0.05544910952448845, f1 0.7343999147415161
ep22_l4_test_time 0.8090382169999657
gc 0
Train Epoch23 Acc 0.955825 (38233/40000), AUC 0.989738404750824
ep23_train_time 23.067921359000024
Test Epoch23 layer0 Acc 0.701, AUC 0.7693902254104614, avg_entr 0.1821853071451187, f1 0.7009999752044678
ep23_l0_test_time 0.28420160999996824
Test Epoch23 layer1 Acc 0.7104, AUC 0.7883280515670776, avg_entr 0.07404792308807373, f1 0.7103999257087708
ep23_l1_test_time 0.35432595199995376
Test Epoch23 layer2 Acc 0.7292, AUC 0.8068504333496094, avg_entr 0.06022774055600166, f1 0.7291999459266663
ep23_l2_test_time 0.4683245349999652
Test Epoch23 layer3 Acc 0.7294, AUC 0.8137329816818237, avg_entr 0.05488043650984764, f1 0.7293999791145325
ep23_l3_test_time 0.6232552989999931
Test Epoch23 layer4 Acc 0.7294, AUC 0.8172144293785095, avg_entr 0.05264727771282196, f1 0.7293999791145325
ep23_l4_test_time 0.820137827999929
gc 0
Train Epoch24 Acc 0.95585 (38234/40000), AUC 0.9895941615104675
ep24_train_time 23.002273022000054
Test Epoch24 layer0 Acc 0.7014, AUC 0.7682775259017944, avg_entr 0.18111391365528107, f1 0.7013999819755554
ep24_l0_test_time 0.28333826800007955
Test Epoch24 layer1 Acc 0.7116, AUC 0.7883678674697876, avg_entr 0.07306606322526932, f1 0.7116000652313232
ep24_l1_test_time 0.35658235400001104
Test Epoch24 layer2 Acc 0.7324, AUC 0.8054351806640625, avg_entr 0.056142255663871765, f1 0.7323999404907227
ep24_l2_test_time 0.4643648219999932
Test Epoch24 layer3 Acc 0.7338, AUC 0.8138745427131653, avg_entr 0.05252305418252945, f1 0.7338000535964966
ep24_l3_test_time 0.6128409399999555
Test Epoch24 layer4 Acc 0.7352, AUC 0.8171607255935669, avg_entr 0.049714140594005585, f1 0.7351999282836914
ep24_l4_test_time 0.8067025750000312
gc 0
Train Epoch25 Acc 0.958075 (38323/40000), AUC 0.9904515743255615
ep25_train_time 23.07705524000005
Test Epoch25 layer0 Acc 0.6994, AUC 0.7677924633026123, avg_entr 0.1792001873254776, f1 0.699400007724762
ep25_l0_test_time 0.28430299800004377
Test Epoch25 layer1 Acc 0.7088, AUC 0.7870008945465088, avg_entr 0.07162942737340927, f1 0.7088000178337097
ep25_l1_test_time 0.3548583049999934
Test Epoch25 layer2 Acc 0.7298, AUC 0.8043851852416992, avg_entr 0.05457828938961029, f1 0.7297999858856201
ep25_l2_test_time 0.462571894000007
Test Epoch25 layer3 Acc 0.7332, AUC 0.8107505440711975, avg_entr 0.050050441175699234, f1 0.7332000136375427
ep25_l3_test_time 0.612535216000083
Test Epoch25 layer4 Acc 0.7328, AUC 0.8150744438171387, avg_entr 0.04764045029878616, f1 0.7327999472618103
ep25_l4_test_time 0.8168071510000345
gc 0
Train Epoch26 Acc 0.959075 (38363/40000), AUC 0.9904052019119263
ep26_train_time 23.01653265799996
Test Epoch26 layer0 Acc 0.6998, AUC 0.7672630548477173, avg_entr 0.17747049033641815, f1 0.6998000144958496
ep26_l0_test_time 0.2850518729999294
Test Epoch26 layer1 Acc 0.7108, AUC 0.7835630178451538, avg_entr 0.06936559081077576, f1 0.7107999920845032
ep26_l1_test_time 0.35467069200001333
Test Epoch26 layer2 Acc 0.7254, AUC 0.8005492687225342, avg_entr 0.052822746336460114, f1 0.7253999710083008
ep26_l2_test_time 0.4636431989999892
Test Epoch26 layer3 Acc 0.7272, AUC 0.8080736994743347, avg_entr 0.04840662702918053, f1 0.7271999716758728
ep26_l3_test_time 0.6133254469999656
Test Epoch26 layer4 Acc 0.7274, AUC 0.8135886192321777, avg_entr 0.04562366008758545, f1 0.7274000644683838
ep26_l4_test_time 0.8082519050000201
gc 0
Train Epoch27 Acc 0.96225 (38490/40000), AUC 0.9917464256286621
ep27_train_time 23.018500402999962
Test Epoch27 layer0 Acc 0.6968, AUC 0.7676427364349365, avg_entr 0.17460666596889496, f1 0.6967999935150146
ep27_l0_test_time 0.28452292099996157
Test Epoch27 layer1 Acc 0.7142, AUC 0.7853928804397583, avg_entr 0.06845207512378693, f1 0.7142000198364258
ep27_l1_test_time 0.35423225400006686
Test Epoch27 layer2 Acc 0.7344, AUC 0.801399827003479, avg_entr 0.05514124035835266, f1 0.7343999147415161
ep27_l2_test_time 0.4633942409999463
Test Epoch27 layer3 Acc 0.7378, AUC 0.81151282787323, avg_entr 0.049852702766656876, f1 0.7378000020980835
ep27_l3_test_time 0.6131760270000086
Test Epoch27 layer4 Acc 0.741, AUC 0.8152485489845276, avg_entr 0.04653159901499748, f1 0.7409999370574951
ep27_l4_test_time 0.8076230010000245
gc 0
Train Epoch28 Acc 0.963075 (38523/40000), AUC 0.9920262098312378
ep28_train_time 23.05417301
Test Epoch28 layer0 Acc 0.7, AUC 0.7669175863265991, avg_entr 0.17604005336761475, f1 0.699999988079071
ep28_l0_test_time 0.2930282399998987
Test Epoch28 layer1 Acc 0.711, AUC 0.7853182554244995, avg_entr 0.07121450453996658, f1 0.7110000848770142
ep28_l1_test_time 0.3547323330000154
Test Epoch28 layer2 Acc 0.7326, AUC 0.8021719455718994, avg_entr 0.05175111070275307, f1 0.7325999140739441
ep28_l2_test_time 0.480693788999929
Test Epoch28 layer3 Acc 0.7346, AUC 0.8101205825805664, avg_entr 0.04761188477277756, f1 0.7345999479293823
ep28_l3_test_time 0.6132793379999839
Test Epoch28 layer4 Acc 0.735, AUC 0.8150693774223328, avg_entr 0.04489828646183014, f1 0.7350000143051147
ep28_l4_test_time 0.8116689280000173
gc 0
Train Epoch29 Acc 0.962925 (38517/40000), AUC 0.9922939538955688
ep29_train_time 23.117549556000085
Test Epoch29 layer0 Acc 0.6972, AUC 0.767266035079956, avg_entr 0.1732543706893921, f1 0.6972000002861023
ep29_l0_test_time 0.2851003260000198
Test Epoch29 layer1 Acc 0.712, AUC 0.7840743064880371, avg_entr 0.0673498809337616, f1 0.7120000123977661
ep29_l1_test_time 0.351615944999935
Test Epoch29 layer2 Acc 0.7334, AUC 0.7992764115333557, avg_entr 0.05355425551533699, f1 0.7333999872207642
ep29_l2_test_time 0.4608729729999368
Test Epoch29 layer3 Acc 0.736, AUC 0.8096316456794739, avg_entr 0.04874641075730324, f1 0.7360000014305115
ep29_l3_test_time 0.6131834290000597
Test Epoch29 layer4 Acc 0.7372, AUC 0.8130127191543579, avg_entr 0.04593164101243019, f1 0.7372000217437744
ep29_l4_test_time 0.8097169110000095
gc 0
Train Epoch30 Acc 0.9637 (38548/40000), AUC 0.992697536945343
ep30_train_time 23.030598155000007
Test Epoch30 layer0 Acc 0.6988, AUC 0.7667837142944336, avg_entr 0.17361067235469818, f1 0.6988000273704529
ep30_l0_test_time 0.2835357540000132
Test Epoch30 layer1 Acc 0.713, AUC 0.782522439956665, avg_entr 0.06459584832191467, f1 0.7130000591278076
ep30_l1_test_time 0.3540686089999099
Test Epoch30 layer2 Acc 0.7346, AUC 0.7981873750686646, avg_entr 0.04936974495649338, f1 0.7345999479293823
ep30_l2_test_time 0.46285053700000844
Test Epoch30 layer3 Acc 0.7342, AUC 0.8075544238090515, avg_entr 0.04492039978504181, f1 0.7342000007629395
ep30_l3_test_time 0.6117769210000006
Test Epoch30 layer4 Acc 0.7366, AUC 0.81251060962677, avg_entr 0.0421454943716526, f1 0.7365999817848206
ep30_l4_test_time 0.8079449460000205
gc 0
Train Epoch31 Acc 0.964775 (38591/40000), AUC 0.9930415153503418
ep31_train_time 23.047214703999998
Test Epoch31 layer0 Acc 0.6986, AUC 0.766798198223114, avg_entr 0.17190060019493103, f1 0.6985999941825867
ep31_l0_test_time 0.28374310599997443
Test Epoch31 layer1 Acc 0.7144, AUC 0.7832608222961426, avg_entr 0.06329840421676636, f1 0.7143999934196472
ep31_l1_test_time 0.35380953599997156
Test Epoch31 layer2 Acc 0.732, AUC 0.7982745170593262, avg_entr 0.04869677871465683, f1 0.7319999933242798
ep31_l2_test_time 0.4647579149999501
Test Epoch31 layer3 Acc 0.736, AUC 0.8082213401794434, avg_entr 0.04420006275177002, f1 0.7360000014305115
ep31_l3_test_time 0.6128708680000727
Test Epoch31 layer4 Acc 0.7374, AUC 0.811876118183136, avg_entr 0.041688740253448486, f1 0.7373999953269958
ep31_l4_test_time 0.8081115749999981
gc 0
Train Epoch32 Acc 0.9647 (38588/40000), AUC 0.9928507804870605
ep32_train_time 23.04210617800004
Test Epoch32 layer0 Acc 0.7012, AUC 0.7668161988258362, avg_entr 0.17330622673034668, f1 0.701200008392334
ep32_l0_test_time 0.2835917549999749
Test Epoch32 layer1 Acc 0.7104, AUC 0.7833817005157471, avg_entr 0.06532621383666992, f1 0.7103999257087708
ep32_l1_test_time 0.35318060099996273
Test Epoch32 layer2 Acc 0.7336, AUC 0.7996828556060791, avg_entr 0.048414845019578934, f1 0.7335999608039856
ep32_l2_test_time 0.46361467099995934
Test Epoch32 layer3 Acc 0.7346, AUC 0.8066117167472839, avg_entr 0.04405682533979416, f1 0.7345999479293823
ep32_l3_test_time 0.6124977239999225
Test Epoch32 layer4 Acc 0.7366, AUC 0.8119065165519714, avg_entr 0.041059449315071106, f1 0.7365999817848206
ep32_l4_test_time 0.8072524799999883
gc 0
Train Epoch33 Acc 0.963975 (38559/40000), AUC 0.992851734161377
ep33_train_time 23.215650502000017
Test Epoch33 layer0 Acc 0.6964, AUC 0.7663030624389648, avg_entr 0.171870157122612, f1 0.696399986743927
ep33_l0_test_time 0.28592784699992535
Test Epoch33 layer1 Acc 0.7128, AUC 0.7821117043495178, avg_entr 0.06370461732149124, f1 0.7128000259399414
ep33_l1_test_time 0.3547563759999548
Test Epoch33 layer2 Acc 0.732, AUC 0.7972718477249146, avg_entr 0.050565220415592194, f1 0.7319999933242798
ep33_l2_test_time 0.46419515200000205
Test Epoch33 layer3 Acc 0.736, AUC 0.8071677088737488, avg_entr 0.04574921727180481, f1 0.7360000014305115
ep33_l3_test_time 0.6144637009999769
Test Epoch33 layer4 Acc 0.7386, AUC 0.8112099766731262, avg_entr 0.042881835252046585, f1 0.7386000156402588
ep33_l4_test_time 0.8085456059999387
gc 0
Train Epoch34 Acc 0.9659 (38636/40000), AUC 0.9930545091629028
ep34_train_time 23.001241454000024
Test Epoch34 layer0 Acc 0.6984, AUC 0.7667958736419678, avg_entr 0.17154082655906677, f1 0.6984000205993652
ep34_l0_test_time 0.28374075400006404
Test Epoch34 layer1 Acc 0.7136, AUC 0.7824885845184326, avg_entr 0.06413839757442474, f1 0.7135999798774719
ep34_l1_test_time 0.3541377699999657
Test Epoch34 layer2 Acc 0.7346, AUC 0.7990924119949341, avg_entr 0.0491836741566658, f1 0.7345999479293823
ep34_l2_test_time 0.46392686900003355
Test Epoch34 layer3 Acc 0.7354, AUC 0.8068687915802002, avg_entr 0.04493303969502449, f1 0.7354000210762024
ep34_l3_test_time 0.6131654170000047
Test Epoch34 layer4 Acc 0.7364, AUC 0.8116845488548279, avg_entr 0.0417853407561779, f1 0.7364000082015991
ep34_l4_test_time 0.8071423650000042
gc 0
Train Epoch35 Acc 0.966925 (38677/40000), AUC 0.9933019876480103
ep35_train_time 23.11896235100005
Test Epoch35 layer0 Acc 0.6958, AUC 0.7666707634925842, avg_entr 0.17063842713832855, f1 0.6958000063896179
ep35_l0_test_time 0.2843038549999619
Test Epoch35 layer1 Acc 0.7126, AUC 0.7824797630310059, avg_entr 0.06316711753606796, f1 0.7125999927520752
ep35_l1_test_time 0.3545504249999567
Test Epoch35 layer2 Acc 0.7334, AUC 0.7978469729423523, avg_entr 0.04794125631451607, f1 0.7333999872207642
ep35_l2_test_time 0.46374875900005463
Test Epoch35 layer3 Acc 0.735, AUC 0.8060232400894165, avg_entr 0.043457984924316406, f1 0.7350000143051147
ep35_l3_test_time 0.6127180779999435
Test Epoch35 layer4 Acc 0.736, AUC 0.8116201162338257, avg_entr 0.03995393216609955, f1 0.7360000014305115
ep35_l4_test_time 0.8099459569999681
gc 0
Train Epoch36 Acc 0.967625 (38705/40000), AUC 0.9934744238853455
ep36_train_time 23.00358900799995
Test Epoch36 layer0 Acc 0.6988, AUC 0.7666066884994507, avg_entr 0.1727430373430252, f1 0.6988000273704529
ep36_l0_test_time 0.2848249170000372
Test Epoch36 layer1 Acc 0.7134, AUC 0.782356858253479, avg_entr 0.06419514119625092, f1 0.7134000062942505
ep36_l1_test_time 0.35465246000001116
Test Epoch36 layer2 Acc 0.7356, AUC 0.7973659038543701, avg_entr 0.04829699546098709, f1 0.7355999946594238
ep36_l2_test_time 0.4638992169999483
Test Epoch36 layer3 Acc 0.7364, AUC 0.8058784008026123, avg_entr 0.043820690363645554, f1 0.7364000082015991
ep36_l3_test_time 0.6261354490000031
Test Epoch36 layer4 Acc 0.7356, AUC 0.811461865901947, avg_entr 0.040777478367090225, f1 0.7355999946594238
ep36_l4_test_time 0.8153854979999551
gc 0
Train Epoch37 Acc 0.96665 (38666/40000), AUC 0.993315577507019
ep37_train_time 23.034013189999996
Test Epoch37 layer0 Acc 0.6978, AUC 0.766627311706543, avg_entr 0.171468123793602, f1 0.6977999806404114
ep37_l0_test_time 0.28256677600006697
Test Epoch37 layer1 Acc 0.713, AUC 0.7827411890029907, avg_entr 0.06285279989242554, f1 0.7130000591278076
ep37_l1_test_time 0.35354475600001933
Test Epoch37 layer2 Acc 0.7346, AUC 0.7980439066886902, avg_entr 0.049107085913419724, f1 0.7345999479293823
ep37_l2_test_time 0.4627880630000618
Test Epoch37 layer3 Acc 0.7354, AUC 0.8068265914916992, avg_entr 0.04420209303498268, f1 0.7354000210762024
ep37_l3_test_time 0.6121238840000842
Test Epoch37 layer4 Acc 0.7372, AUC 0.8120537996292114, avg_entr 0.041011743247509, f1 0.7372000217437744
ep37_l4_test_time 0.8064190180000423
gc 0
Train Epoch38 Acc 0.966825 (38673/40000), AUC 0.9936598539352417
ep38_train_time 23.163286220999908
Test Epoch38 layer0 Acc 0.6982, AUC 0.7666171193122864, avg_entr 0.17106708884239197, f1 0.698199987411499
ep38_l0_test_time 0.2833620730000348
Test Epoch38 layer1 Acc 0.712, AUC 0.7825561761856079, avg_entr 0.0627082884311676, f1 0.7120000123977661
ep38_l1_test_time 0.35420128500004466
Test Epoch38 layer2 Acc 0.7324, AUC 0.7972443103790283, avg_entr 0.0489557720720768, f1 0.7323999404907227
ep38_l2_test_time 0.46574204800003827
Test Epoch38 layer3 Acc 0.7352, AUC 0.806577742099762, avg_entr 0.04412655159831047, f1 0.7351999282836914
ep38_l3_test_time 0.6130971259999569
Test Epoch38 layer4 Acc 0.7374, AUC 0.8116685152053833, avg_entr 0.040921058505773544, f1 0.7373999953269958
ep38_l4_test_time 0.8088072309999461
gc 0
Train Epoch39 Acc 0.966575 (38663/40000), AUC 0.9934202432632446
ep39_train_time 23.093631660000028
Test Epoch39 layer0 Acc 0.6972, AUC 0.7665988802909851, avg_entr 0.17081540822982788, f1 0.6972000002861023
ep39_l0_test_time 0.2848816889998034
Test Epoch39 layer1 Acc 0.7104, AUC 0.7821203470230103, avg_entr 0.06315778940916061, f1 0.7103999257087708
ep39_l1_test_time 0.35517882999988615
Test Epoch39 layer2 Acc 0.7332, AUC 0.7968586683273315, avg_entr 0.04787393659353256, f1 0.7332000136375427
ep39_l2_test_time 0.4649661379999088
Test Epoch39 layer3 Acc 0.734, AUC 0.8056545257568359, avg_entr 0.04307316243648529, f1 0.7339999675750732
ep39_l3_test_time 0.6139039130000583
Test Epoch39 layer4 Acc 0.7358, AUC 0.8113619089126587, avg_entr 0.039660099893808365, f1 0.73580002784729
ep39_l4_test_time 0.808217526000135
gc 0
Train Epoch40 Acc 0.9662 (38648/40000), AUC 0.993545651435852
ep40_train_time 23.06605117999993
Test Epoch40 layer0 Acc 0.6978, AUC 0.7664361000061035, avg_entr 0.17091763019561768, f1 0.6977999806404114
ep40_l0_test_time 0.29335332599998765
Test Epoch40 layer1 Acc 0.7136, AUC 0.7824235558509827, avg_entr 0.0627451092004776, f1 0.7135999798774719
ep40_l1_test_time 0.3579105410001375
Test Epoch40 layer2 Acc 0.7332, AUC 0.796796441078186, avg_entr 0.047788139432668686, f1 0.7332000136375427
ep40_l2_test_time 0.463866414000222
Test Epoch40 layer3 Acc 0.7352, AUC 0.8056344389915466, avg_entr 0.04278428480029106, f1 0.7351999282836914
ep40_l3_test_time 0.6135477450000053
Test Epoch40 layer4 Acc 0.7364, AUC 0.8115041255950928, avg_entr 0.039315324276685715, f1 0.7364000082015991
ep40_l4_test_time 0.808291048000001
gc 0
Train Epoch41 Acc 0.967375 (38695/40000), AUC 0.9935523271560669
ep41_train_time 23.133001699000033
Test Epoch41 layer0 Acc 0.6968, AUC 0.7663679122924805, avg_entr 0.17061644792556763, f1 0.6967999935150146
ep41_l0_test_time 0.28426055900013125
Test Epoch41 layer1 Acc 0.7126, AUC 0.7819647192955017, avg_entr 0.06238383799791336, f1 0.7125999927520752
ep41_l1_test_time 0.35350477699989824
Test Epoch41 layer2 Acc 0.735, AUC 0.7959526777267456, avg_entr 0.04811694473028183, f1 0.7350000143051147
ep41_l2_test_time 0.4660688620001565
Test Epoch41 layer3 Acc 0.7346, AUC 0.8052088022232056, avg_entr 0.043204415589571, f1 0.7345999479293823
ep41_l3_test_time 0.6145651730000736
Test Epoch41 layer4 Acc 0.736, AUC 0.8113540410995483, avg_entr 0.039766717702150345, f1 0.7360000014305115
ep41_l4_test_time 0.8072003409999979
gc 0
Train Epoch42 Acc 0.967375 (38695/40000), AUC 0.993476152420044
ep42_train_time 22.997233553000115
Test Epoch42 layer0 Acc 0.6954, AUC 0.7664405703544617, avg_entr 0.17016054689884186, f1 0.6953999996185303
ep42_l0_test_time 0.2845439949999218
Test Epoch42 layer1 Acc 0.713, AUC 0.7819690704345703, avg_entr 0.06220642104744911, f1 0.7130000591278076
ep42_l1_test_time 0.35454455700005383
Test Epoch42 layer2 Acc 0.7328, AUC 0.7953401803970337, avg_entr 0.04779963940382004, f1 0.7327999472618103
ep42_l2_test_time 0.4637785289999101
Test Epoch42 layer3 Acc 0.735, AUC 0.8050132393836975, avg_entr 0.04273708909749985, f1 0.7350000143051147
ep42_l3_test_time 0.6132877750001171
Test Epoch42 layer4 Acc 0.7366, AUC 0.8109002709388733, avg_entr 0.03921246528625488, f1 0.7365999817848206
ep42_l4_test_time 0.8082435680000799
gc 0
Train Epoch43 Acc 0.966625 (38665/40000), AUC 0.9935030937194824
ep43_train_time 23.147549719999915
Test Epoch43 layer0 Acc 0.696, AUC 0.7664196491241455, avg_entr 0.1704990416765213, f1 0.6959999799728394
ep43_l0_test_time 0.2850937740001882
Test Epoch43 layer1 Acc 0.7126, AUC 0.7819961309432983, avg_entr 0.06241727992892265, f1 0.7125999927520752
ep43_l1_test_time 0.35398088700003427
Test Epoch43 layer2 Acc 0.7338, AUC 0.7957728505134583, avg_entr 0.047492675483226776, f1 0.7338000535964966
ep43_l2_test_time 0.46362780899994505
Test Epoch43 layer3 Acc 0.7342, AUC 0.8049878478050232, avg_entr 0.042561694979667664, f1 0.7342000007629395
ep43_l3_test_time 0.6126778339998964
Test Epoch43 layer4 Acc 0.7354, AUC 0.811235785484314, avg_entr 0.03903236240148544, f1 0.7354000210762024
ep43_l4_test_time 0.809586194999838
gc 0
Train Epoch44 Acc 0.9657 (38628/40000), AUC 0.9933605194091797
ep44_train_time 23.0508232950001
Test Epoch44 layer0 Acc 0.6956, AUC 0.7664519548416138, avg_entr 0.16963417828083038, f1 0.6955999732017517
ep44_l0_test_time 0.2829324290000841
Test Epoch44 layer1 Acc 0.7132, AUC 0.7820663452148438, avg_entr 0.06122290715575218, f1 0.7131999135017395
ep44_l1_test_time 0.3581698860000415
Test Epoch44 layer2 Acc 0.7334, AUC 0.7955853939056396, avg_entr 0.04758632555603981, f1 0.7333999872207642
ep44_l2_test_time 0.4686574679999467
Test Epoch44 layer3 Acc 0.7358, AUC 0.8053462505340576, avg_entr 0.04262950271368027, f1 0.73580002784729
ep44_l3_test_time 0.617032035999955
Test Epoch44 layer4 Acc 0.738, AUC 0.8106836080551147, avg_entr 0.03921313211321831, f1 0.7379999160766602
ep44_l4_test_time 0.8135139779999463
gc 0
Train Epoch45 Acc 0.96845 (38738/40000), AUC 0.9940248727798462
ep45_train_time 23.02625316400008
Test Epoch45 layer0 Acc 0.6988, AUC 0.7663600444793701, avg_entr 0.16990944743156433, f1 0.6988000273704529
ep45_l0_test_time 0.28565455600005407
Test Epoch45 layer1 Acc 0.7118, AUC 0.7820330262184143, avg_entr 0.061849694699048996, f1 0.7117999792098999
ep45_l1_test_time 0.3550852030000442
Test Epoch45 layer2 Acc 0.7342, AUC 0.7961041927337646, avg_entr 0.047333668917417526, f1 0.7342000007629395
ep45_l2_test_time 0.46692448399994646
Test Epoch45 layer3 Acc 0.7346, AUC 0.8048162460327148, avg_entr 0.042474739253520966, f1 0.7345999479293823
ep45_l3_test_time 0.6138422890001038
Test Epoch45 layer4 Acc 0.7352, AUC 0.8111592531204224, avg_entr 0.038962334394454956, f1 0.7351999282836914
ep45_l4_test_time 0.8083911230000922
gc 0
Train Epoch46 Acc 0.96715 (38686/40000), AUC 0.993577241897583
ep46_train_time 23.053301762000046
Test Epoch46 layer0 Acc 0.6986, AUC 0.7663536667823792, avg_entr 0.17022646963596344, f1 0.6985999941825867
ep46_l0_test_time 0.2856672030000027
Test Epoch46 layer1 Acc 0.7106, AUC 0.7817888259887695, avg_entr 0.06141224876046181, f1 0.7106000185012817
ep46_l1_test_time 0.3549368129999948
Test Epoch46 layer2 Acc 0.7336, AUC 0.7959181666374207, avg_entr 0.04727815091609955, f1 0.7335999608039856
ep46_l2_test_time 0.464176352000095
Test Epoch46 layer3 Acc 0.7344, AUC 0.8045952916145325, avg_entr 0.042496662586927414, f1 0.7343999147415161
ep46_l3_test_time 0.6156208930001412
Test Epoch46 layer4 Acc 0.7354, AUC 0.8108358383178711, avg_entr 0.03905249759554863, f1 0.7354000210762024
ep46_l4_test_time 0.8098837350000849
gc 0
Train Epoch47 Acc 0.96805 (38722/40000), AUC 0.9936299324035645
ep47_train_time 23.009447335000004
Test Epoch47 layer0 Acc 0.6972, AUC 0.7663615942001343, avg_entr 0.1699754148721695, f1 0.6972000002861023
ep47_l0_test_time 0.2833203909999611
Test Epoch47 layer1 Acc 0.712, AUC 0.781815230846405, avg_entr 0.061576325446367264, f1 0.7120000123977661
ep47_l1_test_time 0.35342402099990977
Test Epoch47 layer2 Acc 0.734, AUC 0.7959029674530029, avg_entr 0.047453444451093674, f1 0.7339999675750732
ep47_l2_test_time 0.4663112110001748
Test Epoch47 layer3 Acc 0.735, AUC 0.804781436920166, avg_entr 0.04257729649543762, f1 0.7350000143051147
ep47_l3_test_time 0.6144387100000586
Test Epoch47 layer4 Acc 0.7362, AUC 0.8110695481300354, avg_entr 0.039102695882320404, f1 0.7361999750137329
ep47_l4_test_time 0.8134522390000711
gc 0
Train Epoch48 Acc 0.96715 (38686/40000), AUC 0.9935505390167236
ep48_train_time 23.118311204000065
Test Epoch48 layer0 Acc 0.699, AUC 0.7663346529006958, avg_entr 0.17013540863990784, f1 0.6990000009536743
ep48_l0_test_time 0.2850457600000027
Test Epoch48 layer1 Acc 0.7112, AUC 0.7818170785903931, avg_entr 0.06146376207470894, f1 0.7111999988555908
ep48_l1_test_time 0.3555251400000543
Test Epoch48 layer2 Acc 0.7336, AUC 0.7959862947463989, avg_entr 0.04725218936800957, f1 0.7335999608039856
ep48_l2_test_time 0.46377214299991465
Test Epoch48 layer3 Acc 0.734, AUC 0.8046319484710693, avg_entr 0.04254220798611641, f1 0.7339999675750732
ep48_l3_test_time 0.6125179150001259
Test Epoch48 layer4 Acc 0.7354, AUC 0.8109909296035767, avg_entr 0.039135951548814774, f1 0.7354000210762024
ep48_l4_test_time 0.8085969150001802
gc 0
Train Epoch49 Acc 0.968925 (38757/40000), AUC 0.9942659139633179
ep49_train_time 23.039493256000014
Test Epoch49 layer0 Acc 0.6982, AUC 0.7663342952728271, avg_entr 0.16990433633327484, f1 0.698199987411499
ep49_l0_test_time 0.2833351120000316
Test Epoch49 layer1 Acc 0.7124, AUC 0.7818555235862732, avg_entr 0.06149505078792572, f1 0.712399959564209
ep49_l1_test_time 0.35511511300001075
Test Epoch49 layer2 Acc 0.7338, AUC 0.7960395216941833, avg_entr 0.04762674495577812, f1 0.7338000535964966
ep49_l2_test_time 0.4633115670001189
Test Epoch49 layer3 Acc 0.735, AUC 0.8049860000610352, avg_entr 0.0427643284201622, f1 0.7350000143051147
ep49_l3_test_time 0.6149379810001392
Test Epoch49 layer4 Acc 0.737, AUC 0.8110604286193848, avg_entr 0.03922173008322716, f1 0.7369999885559082
ep49_l4_test_time 0.8110834679998788
Best AUC tensor(0.7678) 8 4
train_as_loss [[8.87630275e+01 5.92947980e+01 5.20571098e+01 5.04326232e+01
  4.98697061e+01 4.96135938e+01 4.94763602e+01 4.93944900e+01
  4.93418108e+01 4.93059605e+01 4.92804958e+01 4.92617776e+01
  4.92476329e+01 4.92367012e+01 4.92280894e+01 4.92227114e+01
  4.92195125e+01 4.92165253e+01 4.92137458e+01 4.92117742e+01
  4.92104860e+01 4.92092013e+01 4.92079245e+01 4.92069716e+01
  4.92063212e+01 4.92056511e+01 4.92049634e+01 4.92044358e+01
  4.92040653e+01 4.92036792e+01 4.92032742e+01 4.92029594e+01
  4.92027351e+01 4.92025002e+01 4.92022494e+01 4.92020510e+01
  4.92019103e+01 4.92017615e+01 4.92016019e+01 4.92014715e+01
  4.92013865e+01 4.92012879e+01 4.92011796e+01 4.92011057e+01
  4.92010362e+01 4.92009858e+01 4.92009096e+01 4.92008527e+01
  4.92008305e+01 4.92007822e+01]
 [1.38717381e+00 2.28827012e-04 1.36163650e-05 3.43007701e-06
  1.37353309e-06 6.80905039e-07 3.68066342e-07 2.16210047e-07
  1.39026454e-07 9.19486335e-08 6.29158483e-08 4.36986748e-08
  3.12871197e-08 2.28405732e-08 1.72961142e-08 1.25171403e-08
  1.11342974e-08 1.03569801e-08 8.18762478e-09 7.19079203e-09
  6.59297609e-09 6.04436537e-09 5.77320253e-09 5.01003686e-09
  4.83166384e-09 4.53560033e-09 4.30588348e-09 4.08372031e-09
  4.00741010e-09 3.85624430e-09 3.78832127e-09 3.58942093e-09
  3.49817068e-09 3.41402432e-09 3.32871149e-09 3.20423120e-09
  3.15743422e-09 3.08401821e-09 3.02300240e-09 2.96987218e-09
  2.92950722e-09 2.85516143e-09 2.79049569e-09 2.82738069e-09
  2.72073808e-09 2.66766738e-09 2.56602559e-09 2.73382092e-09
  2.65493734e-09 2.50566510e-09]
 [1.98197324e+00 3.57545925e-04 1.98425499e-05 4.80021512e-06
  1.85142094e-06 9.07519108e-07 4.60614118e-07 2.60693615e-07
  1.67387497e-07 1.12916156e-07 7.18251881e-08 5.13077228e-08
  3.68788357e-08 2.67263137e-08 2.10578283e-08 1.29158333e-08
  1.17899909e-08 1.15548947e-08 8.66694166e-09 7.12347600e-09
  6.54636994e-09 6.00145817e-09 5.81026256e-09 4.84370215e-09
  4.65059773e-09 4.30527139e-09 4.18788521e-09 3.87846428e-09
  3.82765382e-09 3.67062522e-09 3.64870332e-09 3.39919768e-09
  3.26886493e-09 3.27234176e-09 3.13020735e-09 3.01743389e-09
  3.02407192e-09 2.93381835e-09 2.84274502e-09 2.84364645e-09
  2.73302520e-09 2.70783970e-09 2.61288635e-09 2.76746528e-09
  2.72087950e-09 2.58299982e-09 2.50595491e-09 2.81262548e-09
  2.79721376e-09 2.65713262e-09]
 [2.40010073e+00 1.04278747e-03 2.66872132e-05 7.30965230e-06
  2.99051094e-06 1.57897238e-06 7.96405230e-07 4.51879296e-07
  3.01101176e-07 2.04750710e-07 1.29311273e-07 9.72513149e-08
  6.98475980e-08 5.36440163e-08 4.64557542e-08 2.22397308e-08
  2.12905130e-08 2.17027012e-08 1.57685955e-08 1.23207838e-08
  1.11834917e-08 1.02962164e-08 1.01518101e-08 8.13096657e-09
  7.78577953e-09 7.17131106e-09 7.06949617e-09 6.45767504e-09
  6.37077388e-09 6.14848613e-09 6.09141558e-09 5.62035646e-09
  5.42681076e-09 5.44822445e-09 5.21280823e-09 5.06709394e-09
  5.01924011e-09 4.89323033e-09 4.74990257e-09 4.77934311e-09
  4.65826239e-09 4.61444087e-09 4.48643506e-09 4.72107831e-09
  4.70015674e-09 4.42382877e-09 4.27213434e-09 4.99610952e-09
  5.06377276e-09 4.78003959e-09]
 [2.42431875e+00 4.44148188e-03 2.40952008e-05 7.02958481e-06
  3.06880339e-06 1.85931647e-06 9.34347800e-07 5.60574806e-07
  3.91949509e-07 2.90457171e-07 1.80683061e-07 1.42768669e-07
  1.10417603e-07 8.45033573e-08 8.78518390e-08 2.58815973e-08
  2.66411004e-08 3.24586334e-08 2.05971789e-08 1.37081207e-08
  1.24284225e-08 1.12852083e-08 1.22681691e-08 8.48000296e-09
  8.04931245e-09 7.35058856e-09 7.42618722e-09 6.56243753e-09
  6.51122897e-09 6.38462065e-09 6.36012362e-09 5.65646751e-09
  5.50099886e-09 5.49753415e-09 5.28789404e-09 4.99079544e-09
  4.94234548e-09 4.81719313e-09 4.71902060e-09 4.76449528e-09
  4.63237127e-09 4.54487572e-09 4.42514639e-09 4.77587910e-09
  4.70718348e-09 4.48934784e-09 4.31677230e-09 4.95230894e-09
  5.13709443e-09 4.83475635e-09]]
train_ae_loss [[4.31468279 3.01059836 4.28366404 4.83296255 5.09371378 5.29170299
  5.45328705 5.51720541 5.59524959 5.50577291 5.43371206 5.26224032
  5.0057547  4.85620471 4.58161894 4.07963319 3.91786228 3.81648643
  3.6529267  3.4048829  3.32665741 3.26412648 3.20855048 3.1117807
  3.06842839 3.05047866 3.04212715 2.96036025 2.93967183 2.92672568
  2.92147073 2.89791306 2.88363747 2.88766735 2.8805668  2.85268927
  2.87736001 2.86285037 2.87188448 2.85628741 2.85047668 2.84152769
  2.84794686 2.85201788 2.85213112 2.83552373 2.82730771 2.84991242
  2.85132048 2.85579394]
 [3.32427676 2.79449105 3.99179162 4.45368228 4.51753313 4.59936718
  4.73655723 4.64410663 4.69257601 4.53976078 4.47903151 4.31731258
  4.02574646 3.90391558 3.67052154 3.12273351 2.9595538  2.90636041
  2.65846762 2.21195907 2.039993   1.94457272 1.87389678 1.73334654
  1.69800083 1.65410269 1.65918091 1.54313223 1.52686131 1.51254718
  1.49581155 1.47870031 1.45293623 1.4587883  1.4518575  1.41010565
  1.43629408 1.43231118 1.43219547 1.41545581 1.38828968 1.39663556
  1.3874961  1.39682536 1.39776441 1.37268448 1.37657849 1.39531783
  1.39681145 1.40872684]
 [3.42153459 2.70356478 3.90909501 4.42205553 4.32699484 4.31668805
  4.42900726 4.21028028 4.24907014 4.06564892 4.00336772 3.8037118
  3.45650437 3.23608419 2.94447312 2.25163725 2.09267678 2.06505847
  1.88941071 1.57014732 1.49645257 1.44649068 1.39302058 1.27634968
  1.2605058  1.21607331 1.21350393 1.09822731 1.09501748 1.07358346
  1.05530817 1.04901687 1.03197753 1.03003155 1.0220284  0.98085996
  1.00927426 1.00089795 1.00311153 0.98666381 0.95833833 0.97553341
  0.9635997  0.97695865 0.97785268 0.95175503 0.96361635 0.97503095
  0.97996866 0.97378114]
 [3.90954247 2.54821292 3.69827876 4.40266491 4.33790108 4.21134488
  4.3127815  3.98002654 4.00301319 3.76109978 3.64114461 3.41210947
  3.0706071  2.97527971 2.71806875 2.05579651 1.91076204 1.89353039
  1.72114883 1.42011047 1.3490235  1.30503818 1.26135652 1.15197427
  1.14052374 1.09828418 1.09555064 0.98690246 0.98514861 0.96259437
  0.94901709 0.94117059 0.9262402  0.92509213 0.91767703 0.87723784
  0.90581957 0.89756699 0.90128026 0.88232472 0.85834999 0.87510262
  0.86186702 0.87456403 0.8753619  0.85150239 0.86423148 0.87269435
  0.8798602  0.87172233]
 [4.10845269 2.29992621 3.36820018 4.10455243 4.22701705 3.99449936
  4.05235947 3.62239962 3.63864135 3.40305632 3.28160368 3.06239183
  2.74305618 2.6567578  2.43968687 1.81432248 1.6822765  1.67384039
  1.51777635 1.24496577 1.17925369 1.14328768 1.1052139  1.00500131
  0.99672738 0.95959499 0.9590722  0.86100856 0.85915693 0.83942042
  0.82799875 0.82011717 0.808212   0.80607185 0.80024265 0.76480216
  0.79095077 0.78150317 0.78534366 0.76636845 0.74625124 0.7596794
  0.75003647 0.75896429 0.76249239 0.74032509 0.75206508 0.76005394
  0.76647502 0.75798749]]
valid_acc (5, 50)
valid_AUC (5, 50)
train_acc (50,)
total_train+valid_time 1286.1151268209999
Start Testing
Load ckpt at ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt
gc 9
Test layer0 Acc 0.709, AUC 0.7970155477523804, avg_entr 0.296877384185791, f1 0.7090000510215759
l0_test_time 0.2858959010000035
gc 0
Test layer1 Acc 0.7462, AUC 0.8339778184890747, avg_entr 0.2589075565338135, f1 0.7462000250816345
l1_test_time 0.355885495999928
gc 0
Test layer2 Acc 0.7672, AUC 0.8487619161605835, avg_entr 0.2508881688117981, f1 0.7671999931335449
l2_test_time 0.4802823300001364
gc 0
Test layer3 Acc 0.7716, AUC 0.8525897264480591, avg_entr 0.23584149777889252, f1 0.771600067615509
l3_test_time 0.6205110830001104
gc 0
Test layer4 Acc 0.7726, AUC 0.8539676666259766, avg_entr 0.24340195953845978, f1 0.772599995136261
l4_test_time 0.81985394000003
gc 0
Test threshold 0.1 Acc 0.7716, AUC 0.8466690182685852, avg_entr 0.3323039710521698, f1 0.771600067615509
t0.1_test_time 0.7108143059999747
gc 0
Test threshold 0.2 Acc 0.7684, AUC 0.8389711380004883, avg_entr 0.32336533069610596, f1 0.7684000730514526
t0.2_test_time 0.6219067419999647
gc 0
Test threshold 0.3 Acc 0.7656, AUC 0.8329073190689087, avg_entr 0.32003939151763916, f1 0.7655999660491943
t0.3_test_time 0.566911742000002
gc 0
Test threshold 0.4 Acc 0.7598, AUC 0.8272894620895386, avg_entr 0.3207783102989197, f1 0.7598000168800354
t0.4_test_time 0.524457085999984
gc 0
Test threshold 0.5 Acc 0.7572, AUC 0.8230859637260437, avg_entr 0.32492125034332275, f1 0.7572000026702881
t0.5_test_time 0.4929342820000784
gc 0
Test threshold 0.6 Acc 0.752, AUC 0.8176129460334778, avg_entr 0.3314080238342285, f1 0.7519999742507935
t0.6_test_time 0.45923227399998723
gc 0
Test threshold 0.7 Acc 0.7472, AUC 0.8145447373390198, avg_entr 0.34236085414886475, f1 0.747200071811676
t0.7_test_time 0.4754596870000114
gc 0
Test threshold 0.8 Acc 0.7412, AUC 0.8099897503852844, avg_entr 0.35740935802459717, f1 0.7411999702453613
t0.8_test_time 0.4351686749998862
gc 0
Test threshold 0.9 Acc 0.7304, AUC 0.8052912950515747, avg_entr 0.3783194422721863, f1 0.7304000854492188
t0.9_test_time 0.3930307789999006
