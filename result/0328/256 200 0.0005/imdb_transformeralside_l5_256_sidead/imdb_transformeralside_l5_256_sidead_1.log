total count words 222751
vocab size 30000
train size 40000, valid size 5000, test size 5000
found 27937 words in glove
model: TransformerALsideText(
  (layers): ModuleList(
    (0): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=2, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=2, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
  (emb_layers): ModuleList(
    (0): Embedding(30000, 300)
    (1): Embedding(30000, 300)
    (2): Embedding(30000, 300)
    (3): Embedding(30000, 300)
    (4): Embedding(30000, 300)
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.multi_headed_attention.linears.0.weight 90000
layers.0.enc.f.multi_headed_attention.linears.0.bias 300
layers.0.enc.f.multi_headed_attention.linears.1.weight 90000
layers.0.enc.f.multi_headed_attention.linears.1.bias 300
layers.0.enc.f.multi_headed_attention.linears.2.weight 90000
layers.0.enc.f.multi_headed_attention.linears.2.bias 300
layers.0.enc.f.multi_headed_attention.linears.3.weight 90000
layers.0.enc.f.multi_headed_attention.linears.3.bias 300
layers.0.enc.f.feed_forward.w_1.weight 90000
layers.0.enc.f.feed_forward.w_1.bias 300
layers.0.enc.f.feed_forward.w_2.weight 90000
layers.0.enc.f.feed_forward.w_2.bias 300
layers.0.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.0.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.0.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.0.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.0.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.0.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.0.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.0.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.0.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.0.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.0.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.0.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.0.enc.f.encoder.norm.a 300
layers.0.enc.f.encoder.norm.b 300
layers.0.ae.g.0.weight 256
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 256
layers.0.ae.h.0.bias 2
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
emb_layers.0.weight 9000000
emb_layers.1.weight 9000000
emb_layers.2.weight 9000000
emb_layers.3.weight 9000000
emb_layers.4.weight 9000000
Total Trainable Params: 50758378
init_time 21.584099318
Start Training
gc 0
Train Epoch0 Acc 0.497125 (19885/40000), AUC 0.5015045404434204
ep0_train_time 23.37875953
Test Epoch0 layer0 Acc 0.5014, AUC 0.5730699300765991, avg_entr 0.693788468837738, f1 0.5013999938964844
ep0_l0_test_time 0.28731766199999953
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 0
Test Epoch0 layer1 Acc 0.501, AUC 0.5244929790496826, avg_entr 0.6872462630271912, f1 0.5009999871253967
ep0_l1_test_time 0.3592225049999982
Test Epoch0 layer2 Acc 0.5002, AUC 0.5103272199630737, avg_entr 0.6936774253845215, f1 0.5001999735832214
ep0_l2_test_time 0.4674947470000035
Test Epoch0 layer3 Acc 0.499, AUC 0.5269535183906555, avg_entr 0.6892739534378052, f1 0.49900001287460327
ep0_l3_test_time 0.620365421999999
Test Epoch0 layer4 Acc 0.5, AUC 0.4947841167449951, avg_entr 0.6897466778755188, f1 0.5
ep0_l4_test_time 0.8122217690000042
gc 0
Train Epoch1 Acc 0.511425 (20457/40000), AUC 0.5159847736358643
ep1_train_time 23.023628412
Test Epoch1 layer0 Acc 0.5728, AUC 0.6180594563484192, avg_entr 0.6578807830810547, f1 0.5727999806404114
ep1_l0_test_time 0.28851827400001184
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 1
Test Epoch1 layer1 Acc 0.5456, AUC 0.6270506381988525, avg_entr 0.6830350756645203, f1 0.5455999970436096
ep1_l1_test_time 0.3588407800000084
Test Epoch1 layer2 Acc 0.5026, AUC 0.6004682779312134, avg_entr 0.6911193132400513, f1 0.5026000142097473
ep1_l2_test_time 0.4663121729999915
Test Epoch1 layer3 Acc 0.5, AUC 0.5597358345985413, avg_entr 0.6920143365859985, f1 0.5
ep1_l3_test_time 0.6181566700000047
Test Epoch1 layer4 Acc 0.5, AUC 0.4942276179790497, avg_entr 0.6872608661651611, f1 0.5
ep1_l4_test_time 0.8131357620000017
gc 0
Train Epoch2 Acc 0.510575 (20423/40000), AUC 0.5162003636360168
ep2_train_time 23.068296060999998
Test Epoch2 layer0 Acc 0.5242, AUC 0.6703483462333679, avg_entr 0.4897766709327698, f1 0.5242000222206116
ep2_l0_test_time 0.287705552999995
Test Epoch2 layer1 Acc 0.5608, AUC 0.6807706356048584, avg_entr 0.5643147230148315, f1 0.5608000159263611
ep2_l1_test_time 0.3571634740000036
Test Epoch2 layer2 Acc 0.5838, AUC 0.6865240335464478, avg_entr 0.6222056150436401, f1 0.5838000178337097
ep2_l2_test_time 0.4669969929999951
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 2
Test Epoch2 layer3 Acc 0.6178, AUC 0.6877387166023254, avg_entr 0.6746693253517151, f1 0.6177999973297119
ep2_l3_test_time 0.616574350999997
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 2
Test Epoch2 layer4 Acc 0.5, AUC 0.521737813949585, avg_entr 0.6036392450332642, f1 0.5
ep2_l4_test_time 0.8134000649999962
gc 0
Train Epoch3 Acc 0.543675 (21747/40000), AUC 0.5642939805984497
ep3_train_time 23.109185083
Test Epoch3 layer0 Acc 0.6062, AUC 0.7026813626289368, avg_entr 0.49181461334228516, f1 0.6061999797821045
ep3_l0_test_time 0.287756396000006
Test Epoch3 layer1 Acc 0.566, AUC 0.7189490795135498, avg_entr 0.3796432912349701, f1 0.5659999847412109
ep3_l1_test_time 0.3604433450000073
Test Epoch3 layer2 Acc 0.5382, AUC 0.7224364280700684, avg_entr 0.28798291087150574, f1 0.5382000207901001
ep3_l2_test_time 0.4670778290000044
Test Epoch3 layer3 Acc 0.5248, AUC 0.7146737575531006, avg_entr 0.34731414914131165, f1 0.5248000025749207
ep3_l3_test_time 0.6162518329999926
Test Epoch3 layer4 Acc 0.5426, AUC 0.7130497694015503, avg_entr 0.639363169670105, f1 0.5425999760627747
ep3_l4_test_time 0.8124275150000102
gc 0
Train Epoch4 Acc 0.604625 (24185/40000), AUC 0.6470118761062622
ep4_train_time 23.11888224399999
Test Epoch4 layer0 Acc 0.6704, AUC 0.7372573614120483, avg_entr 0.4768252372741699, f1 0.6704000234603882
ep4_l0_test_time 0.2996197719999998
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 4
Test Epoch4 layer1 Acc 0.6812, AUC 0.75634765625, avg_entr 0.452589750289917, f1 0.6812000274658203
ep4_l1_test_time 0.35727944999999295
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 4
Test Epoch4 layer2 Acc 0.6604, AUC 0.7635582089424133, avg_entr 0.4055582582950592, f1 0.6603999733924866
ep4_l2_test_time 0.4677327299999945
Test Epoch4 layer3 Acc 0.6402, AUC 0.7622126340866089, avg_entr 0.4118592143058777, f1 0.6402000188827515
ep4_l3_test_time 0.6154971210000042
Test Epoch4 layer4 Acc 0.6768, AUC 0.7609195709228516, avg_entr 0.4989374876022339, f1 0.676800012588501
ep4_l4_test_time 0.8103363300000126
gc 0
Train Epoch5 Acc 0.612425 (24497/40000), AUC 0.6507611274719238
ep5_train_time 23.21919164099998
Test Epoch5 layer0 Acc 0.6932, AUC 0.7599993944168091, avg_entr 0.43707484006881714, f1 0.6931999921798706
ep5_l0_test_time 0.2863559740000028
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 5
Test Epoch5 layer1 Acc 0.7136, AUC 0.7869541645050049, avg_entr 0.4118819832801819, f1 0.7135999798774719
ep5_l1_test_time 0.362566060000006
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 5
Test Epoch5 layer2 Acc 0.7188, AUC 0.7933043241500854, avg_entr 0.4067186713218689, f1 0.7188000082969666
ep5_l2_test_time 0.46726186499998335
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 5
Test Epoch5 layer3 Acc 0.7186, AUC 0.7910274863243103, avg_entr 0.4182792901992798, f1 0.7185999751091003
ep5_l3_test_time 0.6182386589999851
Test Epoch5 layer4 Acc 0.7128, AUC 0.7868845462799072, avg_entr 0.4491991698741913, f1 0.7128000259399414
ep5_l4_test_time 0.8126458369999909
gc 0
Train Epoch6 Acc 0.705475 (28219/40000), AUC 0.7780675888061523
ep6_train_time 23.041050798000015
Test Epoch6 layer0 Acc 0.6908, AUC 0.7716085910797119, avg_entr 0.36014246940612793, f1 0.6908000111579895
ep6_l0_test_time 0.2912048679999941
Test Epoch6 layer1 Acc 0.721, AUC 0.8059319257736206, avg_entr 0.3219496011734009, f1 0.7210000157356262
ep6_l1_test_time 0.3587841529999878
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 6
Test Epoch6 layer2 Acc 0.727, AUC 0.8138620853424072, avg_entr 0.31770244240760803, f1 0.7269999980926514
ep6_l2_test_time 0.46723832899999707
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 6
Test Epoch6 layer3 Acc 0.7158, AUC 0.8162471055984497, avg_entr 0.32913047075271606, f1 0.7157999873161316
ep6_l3_test_time 0.6170080299999938
Test Epoch6 layer4 Acc 0.6886, AUC 0.8160697817802429, avg_entr 0.33708229660987854, f1 0.6886000037193298
ep6_l4_test_time 0.8139512969999885
gc 0
Train Epoch7 Acc 0.7497 (29988/40000), AUC 0.8310549259185791
ep7_train_time 23.10919057800001
Test Epoch7 layer0 Acc 0.7104, AUC 0.7816339135169983, avg_entr 0.34321513772010803, f1 0.7103999257087708
ep7_l0_test_time 0.28795823799998743
Test Epoch7 layer1 Acc 0.7438, AUC 0.820466160774231, avg_entr 0.29176396131515503, f1 0.7437999248504639
ep7_l1_test_time 0.35679804299999773
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 7
Test Epoch7 layer2 Acc 0.7476, AUC 0.8289211988449097, avg_entr 0.285617858171463, f1 0.7476000189781189
ep7_l2_test_time 0.46714603800000987
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 7
Test Epoch7 layer3 Acc 0.7538, AUC 0.8332293629646301, avg_entr 0.29375767707824707, f1 0.7537999749183655
ep7_l3_test_time 0.6193337029999952
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 7
Test Epoch7 layer4 Acc 0.7436, AUC 0.8337655663490295, avg_entr 0.29635733366012573, f1 0.7436000108718872
ep7_l4_test_time 0.8156245360000014
gc 0
Train Epoch8 Acc 0.768775 (30751/40000), AUC 0.8488764762878418
ep8_train_time 23.084942890999997
Test Epoch8 layer0 Acc 0.7146, AUC 0.786588191986084, avg_entr 0.3136669397354126, f1 0.7146000266075134
ep8_l0_test_time 0.2875553140000022
Test Epoch8 layer1 Acc 0.7404, AUC 0.826454758644104, avg_entr 0.25968384742736816, f1 0.7404000163078308
ep8_l1_test_time 0.356930038999991
Test Epoch8 layer2 Acc 0.7424, AUC 0.8368546962738037, avg_entr 0.2525216341018677, f1 0.742400050163269
ep8_l2_test_time 0.46764720400000215
Test Epoch8 layer3 Acc 0.7496, AUC 0.8417966365814209, avg_entr 0.2574706971645355, f1 0.7495999932289124
ep8_l3_test_time 0.6165141829999925
Test Epoch8 layer4 Acc 0.7408, AUC 0.8413563966751099, avg_entr 0.2463332712650299, f1 0.7408000230789185
ep8_l4_test_time 0.8118340419999868
gc 0
Train Epoch9 Acc 0.7908 (31632/40000), AUC 0.8738080263137817
ep9_train_time 23.04913239199999
Test Epoch9 layer0 Acc 0.708, AUC 0.7877095937728882, avg_entr 0.29759466648101807, f1 0.7080000638961792
ep9_l0_test_time 0.28778525899997476
Test Epoch9 layer1 Acc 0.7376, AUC 0.8288761377334595, avg_entr 0.2620857059955597, f1 0.7376000285148621
ep9_l1_test_time 0.3575181019999718
Test Epoch9 layer2 Acc 0.7476, AUC 0.839479923248291, avg_entr 0.263152152299881, f1 0.7476000189781189
ep9_l2_test_time 0.46733311600002025
Test Epoch9 layer3 Acc 0.7528, AUC 0.8446071147918701, avg_entr 0.25949907302856445, f1 0.7528000473976135
ep9_l3_test_time 0.6173605939999902
Test Epoch9 layer4 Acc 0.749, AUC 0.8446556925773621, avg_entr 0.2443101555109024, f1 0.7490000128746033
ep9_l4_test_time 0.8112785260000237
gc 0
Train Epoch10 Acc 0.8046 (32184/40000), AUC 0.8858764171600342
ep10_train_time 23.097326573999965
Test Epoch10 layer0 Acc 0.6944, AUC 0.7918156385421753, avg_entr 0.2819209396839142, f1 0.6944000124931335
ep10_l0_test_time 0.2878981709999948
Test Epoch10 layer1 Acc 0.7282, AUC 0.8299791216850281, avg_entr 0.2339310497045517, f1 0.7282000184059143
ep10_l1_test_time 0.3576837309999519
Test Epoch10 layer2 Acc 0.7366, AUC 0.8389497995376587, avg_entr 0.22445441782474518, f1 0.7365999817848206
ep10_l2_test_time 0.4719694700000332
Test Epoch10 layer3 Acc 0.7404, AUC 0.8438471555709839, avg_entr 0.20393945276737213, f1 0.7404000163078308
ep10_l3_test_time 0.6173957760000235
Test Epoch10 layer4 Acc 0.742, AUC 0.8445152044296265, avg_entr 0.18666890263557434, f1 0.7419999837875366
ep10_l4_test_time 0.8130993860000331
gc 0
Train Epoch11 Acc 0.828 (33120/40000), AUC 0.9026919603347778
ep11_train_time 23.227784375
Test Epoch11 layer0 Acc 0.701, AUC 0.7895245552062988, avg_entr 0.25577980279922485, f1 0.7009999752044678
ep11_l0_test_time 0.2867244940000546
Test Epoch11 layer1 Acc 0.7268, AUC 0.826832115650177, avg_entr 0.1983717679977417, f1 0.7268000245094299
ep11_l1_test_time 0.372399492999989
Test Epoch11 layer2 Acc 0.726, AUC 0.8369064927101135, avg_entr 0.18012641370296478, f1 0.7260000109672546
ep11_l2_test_time 0.46905877699998655
Test Epoch11 layer3 Acc 0.7182, AUC 0.8405556678771973, avg_entr 0.14295409619808197, f1 0.7182000279426575
ep11_l3_test_time 0.6153673750000053
Test Epoch11 layer4 Acc 0.7128, AUC 0.8416351079940796, avg_entr 0.1248706802725792, f1 0.7128000259399414
ep11_l4_test_time 0.8104371179999816
gc 0
Train Epoch12 Acc 0.843675 (33747/40000), AUC 0.9162908792495728
ep12_train_time 23.153816109000047
Test Epoch12 layer0 Acc 0.6954, AUC 0.7865543365478516, avg_entr 0.24864330887794495, f1 0.6953999996185303
ep12_l0_test_time 0.2879360960000099
Test Epoch12 layer1 Acc 0.7266, AUC 0.8269217014312744, avg_entr 0.19887568056583405, f1 0.7265999913215637
ep12_l1_test_time 0.35684006399998225
Test Epoch12 layer2 Acc 0.7232, AUC 0.8380750417709351, avg_entr 0.18712885677814484, f1 0.7232000827789307
ep12_l2_test_time 0.46846240600001465
Test Epoch12 layer3 Acc 0.7186, AUC 0.8408911228179932, avg_entr 0.15726667642593384, f1 0.7185999751091003
ep12_l3_test_time 0.6170933870000113
Test Epoch12 layer4 Acc 0.7156, AUC 0.8422913551330566, avg_entr 0.14047299325466156, f1 0.7155999541282654
ep12_l4_test_time 0.8126470490000202
gc 0
Train Epoch13 Acc 0.867275 (34691/40000), AUC 0.9375953078269958
ep13_train_time 23.14231551900002
Test Epoch13 layer0 Acc 0.7112, AUC 0.7875804305076599, avg_entr 0.240831196308136, f1 0.7111999988555908
ep13_l0_test_time 0.28797896800000444
Test Epoch13 layer1 Acc 0.7458, AUC 0.8260337710380554, avg_entr 0.18990181386470795, f1 0.7458000183105469
ep13_l1_test_time 0.35804588399997783
Test Epoch13 layer2 Acc 0.7548, AUC 0.8357600569725037, avg_entr 0.15373612940311432, f1 0.754800021648407
ep13_l2_test_time 0.46764616499996237
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 13
Test Epoch13 layer3 Acc 0.7608, AUC 0.8415432572364807, avg_entr 0.1223752424120903, f1 0.7608000040054321
ep13_l3_test_time 0.6183412489999682
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 13
Test Epoch13 layer4 Acc 0.7586, AUC 0.8417390584945679, avg_entr 0.11055969446897507, f1 0.7585999965667725
ep13_l4_test_time 0.8138811829999781
gc 0
Train Epoch14 Acc 0.893825 (35753/40000), AUC 0.9575366377830505
ep14_train_time 23.18821337999998
Test Epoch14 layer0 Acc 0.7046, AUC 0.7862434387207031, avg_entr 0.22543881833553314, f1 0.7045999765396118
ep14_l0_test_time 0.2901385389999973
Test Epoch14 layer1 Acc 0.725, AUC 0.8235973119735718, avg_entr 0.17947356402873993, f1 0.7250000238418579
ep14_l1_test_time 0.35786760000002005
Test Epoch14 layer2 Acc 0.7402, AUC 0.8318153619766235, avg_entr 0.1259259730577469, f1 0.7401999831199646
ep14_l2_test_time 0.4678543439999885
Test Epoch14 layer3 Acc 0.7496, AUC 0.8410520553588867, avg_entr 0.10744920372962952, f1 0.7495999932289124
ep14_l3_test_time 0.6166567079999936
Test Epoch14 layer4 Acc 0.7506, AUC 0.8420037031173706, avg_entr 0.10294273495674133, f1 0.7505999803543091
ep14_l4_test_time 0.8118781140000237
gc 0
Train Epoch15 Acc 0.904225 (36169/40000), AUC 0.9633513689041138
ep15_train_time 23.104437882000013
Test Epoch15 layer0 Acc 0.7074, AUC 0.7828289270401001, avg_entr 0.21740642189979553, f1 0.7074000239372253
ep15_l0_test_time 0.28755704999997533
Test Epoch15 layer1 Acc 0.7408, AUC 0.815890908241272, avg_entr 0.163504496216774, f1 0.7408000230789185
ep15_l1_test_time 0.35788370300002725
Test Epoch15 layer2 Acc 0.7458, AUC 0.8255043029785156, avg_entr 0.09903188049793243, f1 0.7458000183105469
ep15_l2_test_time 0.4683875489999991
Test Epoch15 layer3 Acc 0.7508, AUC 0.8324916958808899, avg_entr 0.090111643075943, f1 0.7508000135421753
ep15_l3_test_time 0.6160947459999875
Test Epoch15 layer4 Acc 0.7508, AUC 0.833217978477478, avg_entr 0.08586595952510834, f1 0.7508000135421753
ep15_l4_test_time 0.8117793210000173
gc 0
Train Epoch16 Acc 0.9172 (36688/40000), AUC 0.972068190574646
ep16_train_time 23.13514944800005
Test Epoch16 layer0 Acc 0.7062, AUC 0.7847123146057129, avg_entr 0.20731493830680847, f1 0.7062000036239624
ep16_l0_test_time 0.29152155599996377
Test Epoch16 layer1 Acc 0.7396, AUC 0.8214434385299683, avg_entr 0.16108019649982452, f1 0.7396000027656555
ep16_l1_test_time 0.3770841799999971
Test Epoch16 layer2 Acc 0.7526, AUC 0.8311845064163208, avg_entr 0.09966067224740982, f1 0.7526000142097473
ep16_l2_test_time 0.47604674100000466
Test Epoch16 layer3 Acc 0.7552, AUC 0.8402411937713623, avg_entr 0.09121419489383698, f1 0.7552000284194946
ep16_l3_test_time 0.6242793319999578
Test Epoch16 layer4 Acc 0.7546, AUC 0.8405808210372925, avg_entr 0.08712407946586609, f1 0.7546000480651855
ep16_l4_test_time 0.8121339269999908
gc 0
Train Epoch17 Acc 0.92285 (36914/40000), AUC 0.97362220287323
ep17_train_time 23.121751710000012
Test Epoch17 layer0 Acc 0.7038, AUC 0.7776217460632324, avg_entr 0.20304323732852936, f1 0.7038000226020813
ep17_l0_test_time 0.28870190799995044
Test Epoch17 layer1 Acc 0.73, AUC 0.8124868869781494, avg_entr 0.15001577138900757, f1 0.7300000190734863
ep17_l1_test_time 0.357769329000007
Test Epoch17 layer2 Acc 0.7398, AUC 0.8232009410858154, avg_entr 0.0908496305346489, f1 0.7398000359535217
ep17_l2_test_time 0.46708518200000526
Test Epoch17 layer3 Acc 0.7458, AUC 0.8315809965133667, avg_entr 0.08669649809598923, f1 0.7458000183105469
ep17_l3_test_time 0.6159311999999773
Test Epoch17 layer4 Acc 0.7436, AUC 0.8323627710342407, avg_entr 0.08369762450456619, f1 0.7436000108718872
ep17_l4_test_time 0.8115829919999555
gc 0
Train Epoch18 Acc 0.931 (37240/40000), AUC 0.9797868728637695
ep18_train_time 23.097834954999996
Test Epoch18 layer0 Acc 0.7044, AUC 0.7767683267593384, avg_entr 0.20652873814105988, f1 0.7044000029563904
ep18_l0_test_time 0.2880200900000318
Test Epoch18 layer1 Acc 0.7246, AUC 0.803583562374115, avg_entr 0.13324064016342163, f1 0.7245999574661255
ep18_l1_test_time 0.35838660999991134
Test Epoch18 layer2 Acc 0.7368, AUC 0.8142881393432617, avg_entr 0.08147374540567398, f1 0.7368000149726868
ep18_l2_test_time 0.4665854379999246
Test Epoch18 layer3 Acc 0.7428, AUC 0.823113203048706, avg_entr 0.07919298112392426, f1 0.7427999973297119
ep18_l3_test_time 0.6170330769999737
Test Epoch18 layer4 Acc 0.7418, AUC 0.8245557546615601, avg_entr 0.07592214643955231, f1 0.74180006980896
ep18_l4_test_time 0.8134348270000373
gc 0
Train Epoch19 Acc 0.9407 (37628/40000), AUC 0.9830960035324097
ep19_train_time 23.19787807399996
Test Epoch19 layer0 Acc 0.7036, AUC 0.7760701179504395, avg_entr 0.1981019228696823, f1 0.7035999894142151
ep19_l0_test_time 0.2869345990000056
Test Epoch19 layer1 Acc 0.7188, AUC 0.8022662997245789, avg_entr 0.11242883652448654, f1 0.7188000082969666
ep19_l1_test_time 0.35834652799997
Test Epoch19 layer2 Acc 0.7328, AUC 0.8109988570213318, avg_entr 0.06901758164167404, f1 0.7327999472618103
ep19_l2_test_time 0.46770982800001093
Test Epoch19 layer3 Acc 0.7368, AUC 0.8215174674987793, avg_entr 0.06326094269752502, f1 0.7368000149726868
ep19_l3_test_time 0.6223877180000272
Test Epoch19 layer4 Acc 0.738, AUC 0.8233504295349121, avg_entr 0.06076355651021004, f1 0.7379999160766602
ep19_l4_test_time 0.8129887459999736
gc 0
Train Epoch20 Acc 0.9414 (37656/40000), AUC 0.9836688041687012
ep20_train_time 23.14043432699998
Test Epoch20 layer0 Acc 0.7036, AUC 0.7755241394042969, avg_entr 0.1951763927936554, f1 0.7035999894142151
ep20_l0_test_time 0.28770954500009793
Test Epoch20 layer1 Acc 0.7266, AUC 0.8050335049629211, avg_entr 0.10993626713752747, f1 0.7265999913215637
ep20_l1_test_time 0.35912010999993527
Test Epoch20 layer2 Acc 0.7356, AUC 0.8153321146965027, avg_entr 0.0709150955080986, f1 0.7355999946594238
ep20_l2_test_time 0.46729483699994034
Test Epoch20 layer3 Acc 0.743, AUC 0.8263429999351501, avg_entr 0.06815122067928314, f1 0.7430000305175781
ep20_l3_test_time 0.6153644729999996
Test Epoch20 layer4 Acc 0.7436, AUC 0.8280182480812073, avg_entr 0.06581787765026093, f1 0.7436000108718872
ep20_l4_test_time 0.8129127150000386
gc 0
Train Epoch21 Acc 0.9463 (37852/40000), AUC 0.9855139851570129
ep21_train_time 23.09014178399991
Test Epoch21 layer0 Acc 0.7032, AUC 0.7741142511367798, avg_entr 0.195985808968544, f1 0.7031999826431274
ep21_l0_test_time 0.2862730869999268
Test Epoch21 layer1 Acc 0.7222, AUC 0.8010224103927612, avg_entr 0.09519059956073761, f1 0.7222000360488892
ep21_l1_test_time 0.3578214169999683
Test Epoch21 layer2 Acc 0.7348, AUC 0.8137789964675903, avg_entr 0.06320387870073318, f1 0.7347999811172485
ep21_l2_test_time 0.47583117700003186
Test Epoch21 layer3 Acc 0.7392, AUC 0.8231223821640015, avg_entr 0.05995985120534897, f1 0.7391999959945679
ep21_l3_test_time 0.632999600000062
Test Epoch21 layer4 Acc 0.7394, AUC 0.8251038789749146, avg_entr 0.057387158274650574, f1 0.7394000291824341
ep21_l4_test_time 0.8183662760000061
gc 0
Train Epoch22 Acc 0.94755 (37902/40000), AUC 0.9870445728302002
ep22_train_time 23.17527498300001
Test Epoch22 layer0 Acc 0.703, AUC 0.772318959236145, avg_entr 0.19093112647533417, f1 0.703000009059906
ep22_l0_test_time 0.2915986900000007
Test Epoch22 layer1 Acc 0.7216, AUC 0.7971463799476624, avg_entr 0.08778656274080276, f1 0.7215999960899353
ep22_l1_test_time 0.36057208400006857
Test Epoch22 layer2 Acc 0.7338, AUC 0.8087202310562134, avg_entr 0.06119158864021301, f1 0.7338000535964966
ep22_l2_test_time 0.4683949479999683
Test Epoch22 layer3 Acc 0.737, AUC 0.8189406394958496, avg_entr 0.057951826602220535, f1 0.7369999885559082
ep22_l3_test_time 0.6165454349999209
Test Epoch22 layer4 Acc 0.7362, AUC 0.821357250213623, avg_entr 0.056373052299022675, f1 0.7361999750137329
ep22_l4_test_time 0.8114962590000232
gc 0
Train Epoch23 Acc 0.952225 (38089/40000), AUC 0.9883941411972046
ep23_train_time 23.082169361999945
Test Epoch23 layer0 Acc 0.7046, AUC 0.7722983360290527, avg_entr 0.18844452500343323, f1 0.7045999765396118
ep23_l0_test_time 0.2878626350000104
Test Epoch23 layer1 Acc 0.7218, AUC 0.7961724996566772, avg_entr 0.0828319787979126, f1 0.7217999696731567
ep23_l1_test_time 0.35881163100009417
Test Epoch23 layer2 Acc 0.7326, AUC 0.80893874168396, avg_entr 0.05802500620484352, f1 0.7325999140739441
ep23_l2_test_time 0.46655567900006645
Test Epoch23 layer3 Acc 0.7376, AUC 0.8194483518600464, avg_entr 0.054381176829338074, f1 0.7376000285148621
ep23_l3_test_time 0.6162693810000519
Test Epoch23 layer4 Acc 0.7378, AUC 0.8217344880104065, avg_entr 0.05237481743097305, f1 0.7378000020980835
ep23_l4_test_time 0.8127874189999602
gc 0
Train Epoch24 Acc 0.954425 (38177/40000), AUC 0.989270031452179
ep24_train_time 23.135953753999956
Test Epoch24 layer0 Acc 0.704, AUC 0.7726311683654785, avg_entr 0.18843913078308105, f1 0.7039999961853027
ep24_l0_test_time 0.28594764600006783
Test Epoch24 layer1 Acc 0.7196, AUC 0.7949186563491821, avg_entr 0.07966303825378418, f1 0.7196000218391418
ep24_l1_test_time 0.3571947210000417
Test Epoch24 layer2 Acc 0.7338, AUC 0.8082600831985474, avg_entr 0.05753323808312416, f1 0.7338000535964966
ep24_l2_test_time 0.4687325330000931
Test Epoch24 layer3 Acc 0.7368, AUC 0.8194887638092041, avg_entr 0.052688512951135635, f1 0.7368000149726868
ep24_l3_test_time 0.6170046349999438
Test Epoch24 layer4 Acc 0.7366, AUC 0.8218035697937012, avg_entr 0.05012980476021767, f1 0.7365999817848206
ep24_l4_test_time 0.8133684139999104
gc 0
Train Epoch25 Acc 0.955075 (38203/40000), AUC 0.9896466732025146
ep25_train_time 23.08419237800001
Test Epoch25 layer0 Acc 0.7046, AUC 0.7710323333740234, avg_entr 0.1912025511264801, f1 0.7045999765396118
ep25_l0_test_time 0.2870488960001012
Test Epoch25 layer1 Acc 0.7234, AUC 0.7943600416183472, avg_entr 0.08014337718486786, f1 0.7233999967575073
ep25_l1_test_time 0.35711002699997607
Test Epoch25 layer2 Acc 0.7344, AUC 0.8074398040771484, avg_entr 0.058724433183670044, f1 0.7343999147415161
ep25_l2_test_time 0.46701805500003957
Test Epoch25 layer3 Acc 0.7388, AUC 0.8194777965545654, avg_entr 0.05321890860795975, f1 0.7387999892234802
ep25_l3_test_time 0.6163980200000196
Test Epoch25 layer4 Acc 0.7394, AUC 0.8215085864067078, avg_entr 0.051012441515922546, f1 0.7394000291824341
ep25_l4_test_time 0.8118192299999691
gc 0
Train Epoch26 Acc 0.956525 (38261/40000), AUC 0.9905154705047607
ep26_train_time 23.081694405999997
Test Epoch26 layer0 Acc 0.7042, AUC 0.7705867886543274, avg_entr 0.18515917658805847, f1 0.704200029373169
ep26_l0_test_time 0.28655392400003166
Test Epoch26 layer1 Acc 0.717, AUC 0.788938581943512, avg_entr 0.07406505197286606, f1 0.7169999480247498
ep26_l1_test_time 0.35754583799996453
Test Epoch26 layer2 Acc 0.7324, AUC 0.8049396872520447, avg_entr 0.05670643970370293, f1 0.7323999404907227
ep26_l2_test_time 0.4691074709999157
Test Epoch26 layer3 Acc 0.7364, AUC 0.8166489601135254, avg_entr 0.051568664610385895, f1 0.7364000082015991
ep26_l3_test_time 0.6167323150000357
Test Epoch26 layer4 Acc 0.7358, AUC 0.8186266422271729, avg_entr 0.04975935444235802, f1 0.73580002784729
ep26_l4_test_time 0.8117037279999977
gc 0
Train Epoch27 Acc 0.9611 (38444/40000), AUC 0.9916497468948364
ep27_train_time 23.045044491999988
Test Epoch27 layer0 Acc 0.705, AUC 0.7710646390914917, avg_entr 0.18536190688610077, f1 0.7049999833106995
ep27_l0_test_time 0.28782545599995046
Test Epoch27 layer1 Acc 0.7204, AUC 0.7908755540847778, avg_entr 0.07465318590402603, f1 0.7204000353813171
ep27_l1_test_time 0.35840967800004364
Test Epoch27 layer2 Acc 0.735, AUC 0.8043891787528992, avg_entr 0.056759901344776154, f1 0.7350000143051147
ep27_l2_test_time 0.4682547349999595
Test Epoch27 layer3 Acc 0.7388, AUC 0.8186159133911133, avg_entr 0.051048643887043, f1 0.7387999892234802
ep27_l3_test_time 0.6172556909999685
Test Epoch27 layer4 Acc 0.7392, AUC 0.8205388784408569, avg_entr 0.04896128922700882, f1 0.7391999959945679
ep27_l4_test_time 0.8124887150000859
gc 0
Train Epoch28 Acc 0.96 (38400/40000), AUC 0.9912053346633911
ep28_train_time 23.142619611999976
Test Epoch28 layer0 Acc 0.7046, AUC 0.7709462642669678, avg_entr 0.18496382236480713, f1 0.7045999765396118
ep28_l0_test_time 0.2881109649999871
Test Epoch28 layer1 Acc 0.7202, AUC 0.7903308868408203, avg_entr 0.07294926047325134, f1 0.7202000021934509
ep28_l1_test_time 0.35696257800009334
Test Epoch28 layer2 Acc 0.734, AUC 0.8053289651870728, avg_entr 0.054232824593782425, f1 0.7339999675750732
ep28_l2_test_time 0.46703014299998813
Test Epoch28 layer3 Acc 0.7366, AUC 0.8181054592132568, avg_entr 0.04805656522512436, f1 0.7365999817848206
ep28_l3_test_time 0.6173157250000259
Test Epoch28 layer4 Acc 0.7372, AUC 0.8203432559967041, avg_entr 0.045889560133218765, f1 0.7372000217437744
ep28_l4_test_time 0.8115610629999992
gc 0
Train Epoch29 Acc 0.959475 (38379/40000), AUC 0.9912583827972412
ep29_train_time 23.06587838300004
Test Epoch29 layer0 Acc 0.7024, AUC 0.7706522941589355, avg_entr 0.1832796037197113, f1 0.7024000287055969
ep29_l0_test_time 0.28627949300005184
Test Epoch29 layer1 Acc 0.715, AUC 0.7893224954605103, avg_entr 0.07463394105434418, f1 0.7150000333786011
ep29_l1_test_time 0.35782956200000626
Test Epoch29 layer2 Acc 0.7314, AUC 0.8039412498474121, avg_entr 0.05432013422250748, f1 0.7314000129699707
ep29_l2_test_time 0.46751969900003587
Test Epoch29 layer3 Acc 0.7346, AUC 0.8172670602798462, avg_entr 0.05134476348757744, f1 0.7345999479293823
ep29_l3_test_time 0.618368187000101
Test Epoch29 layer4 Acc 0.7354, AUC 0.8190993070602417, avg_entr 0.04940735548734665, f1 0.7354000210762024
ep29_l4_test_time 0.8166150920000064
gc 0
Train Epoch30 Acc 0.9611 (38444/40000), AUC 0.9920335412025452
ep30_train_time 23.15576777199999
Test Epoch30 layer0 Acc 0.7048, AUC 0.7700685262680054, avg_entr 0.18465279042720795, f1 0.704800009727478
ep30_l0_test_time 0.28785340799993264
Test Epoch30 layer1 Acc 0.7202, AUC 0.7885231971740723, avg_entr 0.0729898065328598, f1 0.7202000021934509
ep30_l1_test_time 0.358140216000038
Test Epoch30 layer2 Acc 0.734, AUC 0.8031574487686157, avg_entr 0.05618229880928993, f1 0.7339999675750732
ep30_l2_test_time 0.4691332649999822
Test Epoch30 layer3 Acc 0.7382, AUC 0.8170043230056763, avg_entr 0.05023187771439552, f1 0.7382000684738159
ep30_l3_test_time 0.6167961680000644
Test Epoch30 layer4 Acc 0.7376, AUC 0.8191890716552734, avg_entr 0.047985710203647614, f1 0.7376000285148621
ep30_l4_test_time 0.8113316000000168
gc 0
Train Epoch31 Acc 0.961575 (38463/40000), AUC 0.992374062538147
ep31_train_time 23.145456320999983
Test Epoch31 layer0 Acc 0.7048, AUC 0.7699903249740601, avg_entr 0.18273678421974182, f1 0.704800009727478
ep31_l0_test_time 0.2861112339999181
Test Epoch31 layer1 Acc 0.7198, AUC 0.787305474281311, avg_entr 0.07074829190969467, f1 0.7197999358177185
ep31_l1_test_time 0.3666848169999639
Test Epoch31 layer2 Acc 0.7338, AUC 0.8009933829307556, avg_entr 0.05319003760814667, f1 0.7338000535964966
ep31_l2_test_time 0.4812414409999519
Test Epoch31 layer3 Acc 0.737, AUC 0.8154994249343872, avg_entr 0.04746639356017113, f1 0.7369999885559082
ep31_l3_test_time 0.6175222130000293
Test Epoch31 layer4 Acc 0.737, AUC 0.818015456199646, avg_entr 0.04552217572927475, f1 0.7369999885559082
ep31_l4_test_time 0.8125726859999531
gc 0
Train Epoch32 Acc 0.962825 (38513/40000), AUC 0.9925903081893921
ep32_train_time 23.087334205000047
Test Epoch32 layer0 Acc 0.703, AUC 0.7696731090545654, avg_entr 0.18215155601501465, f1 0.703000009059906
ep32_l0_test_time 0.28865674400003627
Test Epoch32 layer1 Acc 0.719, AUC 0.7869425415992737, avg_entr 0.07029277086257935, f1 0.718999981880188
ep32_l1_test_time 0.358565107000004
Test Epoch32 layer2 Acc 0.7336, AUC 0.8017834424972534, avg_entr 0.0540720634162426, f1 0.7335999608039856
ep32_l2_test_time 0.46818733200007046
Test Epoch32 layer3 Acc 0.7378, AUC 0.8161911964416504, avg_entr 0.048229947686195374, f1 0.7378000020980835
ep32_l3_test_time 0.6170508199999176
Test Epoch32 layer4 Acc 0.738, AUC 0.8182951211929321, avg_entr 0.04631412774324417, f1 0.7379999160766602
ep32_l4_test_time 0.8130015660000254
gc 0
Train Epoch33 Acc 0.962 (38480/40000), AUC 0.9923549890518188
ep33_train_time 23.106397608000066
Test Epoch33 layer0 Acc 0.7056, AUC 0.7699469327926636, avg_entr 0.1812949776649475, f1 0.7056000232696533
ep33_l0_test_time 0.28703375299994605
Test Epoch33 layer1 Acc 0.7198, AUC 0.788015604019165, avg_entr 0.07184906303882599, f1 0.7197999358177185
ep33_l1_test_time 0.35888983400002417
Test Epoch33 layer2 Acc 0.7328, AUC 0.8021038174629211, avg_entr 0.05392296612262726, f1 0.7327999472618103
ep33_l2_test_time 0.4711118400000487
Test Epoch33 layer3 Acc 0.7386, AUC 0.8168656826019287, avg_entr 0.05007730424404144, f1 0.7386000156402588
ep33_l3_test_time 0.6169363230000044
Test Epoch33 layer4 Acc 0.739, AUC 0.819192111492157, avg_entr 0.04810256510972977, f1 0.7390000820159912
ep33_l4_test_time 0.8124707860000626
gc 0
Train Epoch34 Acc 0.962725 (38509/40000), AUC 0.992727518081665
ep34_train_time 23.080908787
Test Epoch34 layer0 Acc 0.7032, AUC 0.7698889374732971, avg_entr 0.1803061068058014, f1 0.7031999826431274
ep34_l0_test_time 0.2883081789999551
Test Epoch34 layer1 Acc 0.7174, AUC 0.7871072292327881, avg_entr 0.07053140550851822, f1 0.717400074005127
ep34_l1_test_time 0.35809674099994027
Test Epoch34 layer2 Acc 0.7314, AUC 0.8009927272796631, avg_entr 0.05266406387090683, f1 0.7314000129699707
ep34_l2_test_time 0.4676138259999334
Test Epoch34 layer3 Acc 0.7352, AUC 0.8160439729690552, avg_entr 0.04897015541791916, f1 0.7351999282836914
ep34_l3_test_time 0.617077037999934
Test Epoch34 layer4 Acc 0.7364, AUC 0.8184158802032471, avg_entr 0.04688980057835579, f1 0.7364000082015991
ep34_l4_test_time 0.8116743980000365
gc 0
Train Epoch35 Acc 0.962325 (38493/40000), AUC 0.9929726719856262
ep35_train_time 23.051504294999972
Test Epoch35 layer0 Acc 0.7052, AUC 0.7699233293533325, avg_entr 0.18065021932125092, f1 0.7052000164985657
ep35_l0_test_time 0.2865661570000384
Test Epoch35 layer1 Acc 0.7192, AUC 0.7866429090499878, avg_entr 0.06918477267026901, f1 0.719200074672699
ep35_l1_test_time 0.3578975670000091
Test Epoch35 layer2 Acc 0.7336, AUC 0.7998533248901367, avg_entr 0.05266953259706497, f1 0.7335999608039856
ep35_l2_test_time 0.4673847939999405
Test Epoch35 layer3 Acc 0.7388, AUC 0.8150354623794556, avg_entr 0.046977534890174866, f1 0.7387999892234802
ep35_l3_test_time 0.6166014629999381
Test Epoch35 layer4 Acc 0.7392, AUC 0.8181076049804688, avg_entr 0.04499232769012451, f1 0.7391999959945679
ep35_l4_test_time 0.8117592439999726
gc 0
Train Epoch36 Acc 0.96335 (38534/40000), AUC 0.993121862411499
ep36_train_time 23.233363190999967
Test Epoch36 layer0 Acc 0.704, AUC 0.7698873281478882, avg_entr 0.18108628690242767, f1 0.7039999961853027
ep36_l0_test_time 0.2868902809999554
Test Epoch36 layer1 Acc 0.719, AUC 0.7861943244934082, avg_entr 0.06971453130245209, f1 0.718999981880188
ep36_l1_test_time 0.3566189480000048
Test Epoch36 layer2 Acc 0.7346, AUC 0.800834059715271, avg_entr 0.054719872772693634, f1 0.7345999479293823
ep36_l2_test_time 0.46761775200002376
Test Epoch36 layer3 Acc 0.7396, AUC 0.8156766891479492, avg_entr 0.04909500107169151, f1 0.7396000027656555
ep36_l3_test_time 0.6168246629999885
Test Epoch36 layer4 Acc 0.7392, AUC 0.8183301687240601, avg_entr 0.047096800059080124, f1 0.7391999959945679
ep36_l4_test_time 0.812679499000069
gc 0
Train Epoch37 Acc 0.9644 (38576/40000), AUC 0.9934253692626953
ep37_train_time 23.05046668600005
Test Epoch37 layer0 Acc 0.705, AUC 0.7697998881340027, avg_entr 0.18010902404785156, f1 0.7049999833106995
ep37_l0_test_time 0.2867384940000193
Test Epoch37 layer1 Acc 0.7198, AUC 0.786758303642273, avg_entr 0.06923358887434006, f1 0.7197999358177185
ep37_l1_test_time 0.356227348999937
Test Epoch37 layer2 Acc 0.7338, AUC 0.800271213054657, avg_entr 0.0532744824886322, f1 0.7338000535964966
ep37_l2_test_time 0.46609918100000414
Test Epoch37 layer3 Acc 0.7376, AUC 0.8158280849456787, avg_entr 0.04806435853242874, f1 0.7376000285148621
ep37_l3_test_time 0.6150802610000028
Test Epoch37 layer4 Acc 0.7374, AUC 0.818604052066803, avg_entr 0.04610223323106766, f1 0.7373999953269958
ep37_l4_test_time 0.8110920630000464
gc 0
Train Epoch38 Acc 0.964675 (38587/40000), AUC 0.993268609046936
ep38_train_time 23.122938852999937
Test Epoch38 layer0 Acc 0.7044, AUC 0.7696722745895386, avg_entr 0.1799355149269104, f1 0.7044000029563904
ep38_l0_test_time 0.2865596080000614
Test Epoch38 layer1 Acc 0.718, AUC 0.7860854864120483, avg_entr 0.06833217293024063, f1 0.7179999351501465
ep38_l1_test_time 0.35693089500000497
Test Epoch38 layer2 Acc 0.7316, AUC 0.8001111149787903, avg_entr 0.05224117264151573, f1 0.7316000461578369
ep38_l2_test_time 0.4672093900001073
Test Epoch38 layer3 Acc 0.7372, AUC 0.8152526617050171, avg_entr 0.047519274055957794, f1 0.7372000217437744
ep38_l3_test_time 0.6156872599999588
Test Epoch38 layer4 Acc 0.7376, AUC 0.818028450012207, avg_entr 0.04555576294660568, f1 0.7376000285148621
ep38_l4_test_time 0.8119288049999795
gc 0
Train Epoch39 Acc 0.9633 (38532/40000), AUC 0.9930859804153442
ep39_train_time 23.276097001999915
Test Epoch39 layer0 Acc 0.7046, AUC 0.7695896029472351, avg_entr 0.17997634410858154, f1 0.7045999765396118
ep39_l0_test_time 0.2876991860000544
Test Epoch39 layer1 Acc 0.7176, AUC 0.7858665585517883, avg_entr 0.06756311655044556, f1 0.7175999879837036
ep39_l1_test_time 0.358335870000019
Test Epoch39 layer2 Acc 0.7308, AUC 0.79998379945755, avg_entr 0.05149190500378609, f1 0.7307999730110168
ep39_l2_test_time 0.46773915399990074
Test Epoch39 layer3 Acc 0.7368, AUC 0.8149787187576294, avg_entr 0.046937212347984314, f1 0.7368000149726868
ep39_l3_test_time 0.6187119779999648
Test Epoch39 layer4 Acc 0.7374, AUC 0.8177152276039124, avg_entr 0.04498614743351936, f1 0.7373999953269958
ep39_l4_test_time 0.8129886809999789
gc 0
Train Epoch40 Acc 0.96445 (38578/40000), AUC 0.9933779239654541
ep40_train_time 23.052014561000078
Test Epoch40 layer0 Acc 0.7052, AUC 0.7696805596351624, avg_entr 0.1802360862493515, f1 0.7052000164985657
ep40_l0_test_time 0.28678853400015214
Test Epoch40 layer1 Acc 0.7188, AUC 0.7863932847976685, avg_entr 0.06853797286748886, f1 0.7188000082969666
ep40_l1_test_time 0.3592410440001004
Test Epoch40 layer2 Acc 0.732, AUC 0.8006925582885742, avg_entr 0.052171796560287476, f1 0.7319999933242798
ep40_l2_test_time 0.4663294380000025
Test Epoch40 layer3 Acc 0.7366, AUC 0.815628707408905, avg_entr 0.047360870987176895, f1 0.7365999817848206
ep40_l3_test_time 0.6152169239999239
Test Epoch40 layer4 Acc 0.736, AUC 0.8182826638221741, avg_entr 0.04539426788687706, f1 0.7360000014305115
ep40_l4_test_time 0.8107649559999572
gc 0
Train Epoch41 Acc 0.964225 (38569/40000), AUC 0.9930888414382935
ep41_train_time 23.052390334999927
Test Epoch41 layer0 Acc 0.7056, AUC 0.7696138620376587, avg_entr 0.18041947484016418, f1 0.7056000232696533
ep41_l0_test_time 0.28851097500000833
Test Epoch41 layer1 Acc 0.7172, AUC 0.785682201385498, avg_entr 0.0679323822259903, f1 0.717199981212616
ep41_l1_test_time 0.3589994830001615
Test Epoch41 layer2 Acc 0.731, AUC 0.800449013710022, avg_entr 0.05210639163851738, f1 0.7310000061988831
ep41_l2_test_time 0.468562712999983
Test Epoch41 layer3 Acc 0.7372, AUC 0.8151324391365051, avg_entr 0.04748941585421562, f1 0.7372000217437744
ep41_l3_test_time 0.6164516389999335
Test Epoch41 layer4 Acc 0.7372, AUC 0.817847490310669, avg_entr 0.04552329704165459, f1 0.7372000217437744
ep41_l4_test_time 0.8111792960000912
gc 0
Train Epoch42 Acc 0.964825 (38593/40000), AUC 0.9933205246925354
ep42_train_time 23.073665337999955
Test Epoch42 layer0 Acc 0.7046, AUC 0.7695986032485962, avg_entr 0.1803763210773468, f1 0.7045999765396118
ep42_l0_test_time 0.28724160000001575
Test Epoch42 layer1 Acc 0.719, AUC 0.7861342430114746, avg_entr 0.06819471716880798, f1 0.718999981880188
ep42_l1_test_time 0.3643988410001384
Test Epoch42 layer2 Acc 0.7332, AUC 0.8004671931266785, avg_entr 0.05285220965743065, f1 0.7332000136375427
ep42_l2_test_time 0.46597142099994926
Test Epoch42 layer3 Acc 0.7374, AUC 0.8154416084289551, avg_entr 0.047776948660612106, f1 0.7373999953269958
ep42_l3_test_time 0.6175425520000317
Test Epoch42 layer4 Acc 0.7376, AUC 0.8182591199874878, avg_entr 0.045846063643693924, f1 0.7376000285148621
ep42_l4_test_time 0.8153727259998504
gc 0
Train Epoch43 Acc 0.965025 (38601/40000), AUC 0.9933615922927856
ep43_train_time 23.120618164000007
Test Epoch43 layer0 Acc 0.7052, AUC 0.769611656665802, avg_entr 0.18000641465187073, f1 0.7052000164985657
ep43_l0_test_time 0.28591870099990047
Test Epoch43 layer1 Acc 0.7188, AUC 0.7859413623809814, avg_entr 0.06814617663621902, f1 0.7188000082969666
ep43_l1_test_time 0.3570711160000428
Test Epoch43 layer2 Acc 0.7306, AUC 0.8001216650009155, avg_entr 0.05185984447598457, f1 0.7305999994277954
ep43_l2_test_time 0.47147361100019225
Test Epoch43 layer3 Acc 0.7384, AUC 0.8152786493301392, avg_entr 0.04725291579961777, f1 0.7384000420570374
ep43_l3_test_time 0.6166241390001233
Test Epoch43 layer4 Acc 0.7378, AUC 0.8180930614471436, avg_entr 0.04522986337542534, f1 0.7378000020980835
ep43_l4_test_time 0.8109972380000272
gc 0
Train Epoch44 Acc 0.964575 (38583/40000), AUC 0.9932029247283936
ep44_train_time 23.14331623200019
Test Epoch44 layer0 Acc 0.7052, AUC 0.7695067524909973, avg_entr 0.17970667779445648, f1 0.7052000164985657
ep44_l0_test_time 0.2871199670000806
Test Epoch44 layer1 Acc 0.7194, AUC 0.7857604622840881, avg_entr 0.06782278418540955, f1 0.7193999886512756
ep44_l1_test_time 0.35792566300006
Test Epoch44 layer2 Acc 0.734, AUC 0.7997406721115112, avg_entr 0.05255341902375221, f1 0.7339999675750732
ep44_l2_test_time 0.46834917700016376
Test Epoch44 layer3 Acc 0.7384, AUC 0.8149516582489014, avg_entr 0.04715830087661743, f1 0.7384000420570374
ep44_l3_test_time 0.6164104150000185
Test Epoch44 layer4 Acc 0.7386, AUC 0.8178943395614624, avg_entr 0.0451212152838707, f1 0.7386000156402588
ep44_l4_test_time 0.8119072609999876
gc 0
Train Epoch45 Acc 0.965625 (38625/40000), AUC 0.9933266043663025
ep45_train_time 23.079552320999937
Test Epoch45 layer0 Acc 0.704, AUC 0.7695037126541138, avg_entr 0.1795644462108612, f1 0.7039999961853027
ep45_l0_test_time 0.2876216750000822
Test Epoch45 layer1 Acc 0.7186, AUC 0.7857275605201721, avg_entr 0.06790446490049362, f1 0.7185999751091003
ep45_l1_test_time 0.3578024920000189
Test Epoch45 layer2 Acc 0.7318, AUC 0.7999242544174194, avg_entr 0.05230722576379776, f1 0.7318000197410583
ep45_l2_test_time 0.46667430600018633
Test Epoch45 layer3 Acc 0.7376, AUC 0.8150066137313843, avg_entr 0.047463204711675644, f1 0.7376000285148621
ep45_l3_test_time 0.616216613000006
Test Epoch45 layer4 Acc 0.738, AUC 0.8179153203964233, avg_entr 0.04560822993516922, f1 0.7379999160766602
ep45_l4_test_time 0.8134956680000869
gc 0
Train Epoch46 Acc 0.9648 (38592/40000), AUC 0.9933615922927856
ep46_train_time 23.091731783999876
Test Epoch46 layer0 Acc 0.7048, AUC 0.7694248557090759, avg_entr 0.17944037914276123, f1 0.704800009727478
ep46_l0_test_time 0.2878997249999884
Test Epoch46 layer1 Acc 0.7188, AUC 0.7856321930885315, avg_entr 0.06793633848428726, f1 0.7188000082969666
ep46_l1_test_time 0.3579116360001535
Test Epoch46 layer2 Acc 0.7318, AUC 0.7999996542930603, avg_entr 0.052215903997421265, f1 0.7318000197410583
ep46_l2_test_time 0.46730475799995475
Test Epoch46 layer3 Acc 0.7378, AUC 0.8149765133857727, avg_entr 0.047581013292074203, f1 0.7378000020980835
ep46_l3_test_time 0.6169956530000036
Test Epoch46 layer4 Acc 0.7372, AUC 0.8178356885910034, avg_entr 0.04570543393492699, f1 0.7372000217437744
ep46_l4_test_time 0.811677151999902
gc 0
Train Epoch47 Acc 0.966025 (38641/40000), AUC 0.9935377836227417
ep47_train_time 23.055381504000025
Test Epoch47 layer0 Acc 0.7042, AUC 0.7694659233093262, avg_entr 0.17926457524299622, f1 0.704200029373169
ep47_l0_test_time 0.28685389799989025
Test Epoch47 layer1 Acc 0.7188, AUC 0.7856332063674927, avg_entr 0.06783285737037659, f1 0.7188000082969666
ep47_l1_test_time 0.3586774829998376
Test Epoch47 layer2 Acc 0.7312, AUC 0.7999340891838074, avg_entr 0.05199454352259636, f1 0.7311999797821045
ep47_l2_test_time 0.4715157260000069
Test Epoch47 layer3 Acc 0.7372, AUC 0.8149698972702026, avg_entr 0.04734733700752258, f1 0.7372000217437744
ep47_l3_test_time 0.6169169120000788
Test Epoch47 layer4 Acc 0.737, AUC 0.817821204662323, avg_entr 0.045460473746061325, f1 0.7369999885559082
ep47_l4_test_time 0.8124375239999608
gc 0
Train Epoch48 Acc 0.96645 (38658/40000), AUC 0.9935460686683655
ep48_train_time 23.16432143599991
Test Epoch48 layer0 Acc 0.7046, AUC 0.7694497108459473, avg_entr 0.17934736609458923, f1 0.7045999765396118
ep48_l0_test_time 0.29081272300004457
Test Epoch48 layer1 Acc 0.7192, AUC 0.7853981852531433, avg_entr 0.06784851104021072, f1 0.719200074672699
ep48_l1_test_time 0.3575261349999437
Test Epoch48 layer2 Acc 0.7332, AUC 0.7999144196510315, avg_entr 0.05257803946733475, f1 0.7332000136375427
ep48_l2_test_time 0.46829929299997275
Test Epoch48 layer3 Acc 0.7372, AUC 0.8148150444030762, avg_entr 0.04767480865120888, f1 0.7372000217437744
ep48_l3_test_time 0.6158899550000569
Test Epoch48 layer4 Acc 0.7384, AUC 0.8177865743637085, avg_entr 0.04577414691448212, f1 0.7384000420570374
ep48_l4_test_time 0.8105138789999273
gc 0
Train Epoch49 Acc 0.966225 (38649/40000), AUC 0.9934868812561035
ep49_train_time 23.060533190999877
Test Epoch49 layer0 Acc 0.7044, AUC 0.7694525718688965, avg_entr 0.17919880151748657, f1 0.7044000029563904
ep49_l0_test_time 0.28696095800000876
Test Epoch49 layer1 Acc 0.7186, AUC 0.7854546308517456, avg_entr 0.06781145930290222, f1 0.7185999751091003
ep49_l1_test_time 0.3575754670000606
Test Epoch49 layer2 Acc 0.733, AUC 0.8000239729881287, avg_entr 0.052549976855516434, f1 0.7329999804496765
ep49_l2_test_time 0.46638117099996634
Test Epoch49 layer3 Acc 0.7366, AUC 0.8149121403694153, avg_entr 0.04775536432862282, f1 0.7365999817848206
ep49_l3_test_time 0.6183175720000236
Test Epoch49 layer4 Acc 0.7374, AUC 0.8178136944770813, avg_entr 0.045904338359832764, f1 0.7373999953269958
ep49_l4_test_time 0.8162307509999209
Best AUC tensor(0.7608) 13 3
train_as_loss [[8.70024426e+01 5.94256417e+01 5.22892176e+01 5.05587265e+01
  4.99419021e+01 4.96583942e+01 4.95059606e+01 4.94149776e+01
  4.93565072e+01 4.93168024e+01 4.92886709e+01 4.92680544e+01
  4.92525252e+01 4.92405610e+01 4.92311665e+01 4.92253180e+01
  4.92218471e+01 4.92186098e+01 4.92156039e+01 4.92134754e+01
  4.92120856e+01 4.92107017e+01 4.92093288e+01 4.92083023e+01
  4.92076029e+01 4.92068835e+01 4.92061460e+01 4.92055794e+01
  4.92051836e+01 4.92047693e+01 4.92043374e+01 4.92039989e+01
  4.92037600e+01 4.92035072e+01 4.92032385e+01 4.92030281e+01
  4.92028782e+01 4.92027193e+01 4.92025482e+01 4.92024114e+01
  4.92023175e+01 4.92022108e+01 4.92021012e+01 4.92020172e+01
  4.92019483e+01 4.92018860e+01 4.92018109e+01 4.92017538e+01
  4.92017201e+01 4.92016713e+01]
 [2.08596945e+00 2.21131098e-04 1.22142034e-05 3.53169735e-06
  1.50706351e-06 7.75201147e-07 4.33308952e-07 2.66921269e-07
  1.72048321e-07 1.13967799e-07 7.92181926e-08 5.55442395e-08
  4.02823894e-08 2.81565812e-08 1.05538663e-06 9.80464226e-08
  1.35110704e-08 1.17478507e-08 9.84734117e-09 8.69523627e-09
  7.84827632e-09 7.27547167e-09 6.67556666e-09 5.93740321e-09
  5.72711634e-09 5.36215736e-09 5.06063939e-09 4.78434021e-09
  4.61320029e-09 4.43590266e-09 4.22423097e-09 4.09144661e-09
  3.94018999e-09 3.83099060e-09 3.67515508e-09 3.60296919e-09
  3.46563020e-09 3.38766287e-09 3.25865570e-09 3.19675954e-09
  3.15034020e-09 3.02145431e-09 2.91130704e-09 2.95644896e-09
  2.85456934e-09 2.74689618e-09 2.59830836e-09 2.77417356e-09
  2.67179589e-09 2.49548870e-09]
 [2.19512873e+00 1.67641867e-03 1.75663047e-05 4.36873260e-06
  1.76681444e-06 9.03992750e-07 5.07927395e-07 3.23691726e-07
  2.14067312e-07 1.45237323e-07 1.04271202e-07 7.54400941e-08
  5.64409688e-08 3.94375006e-08 1.14730513e-06 1.37042849e-07
  1.78417795e-08 1.60652175e-08 5.25713173e-08 1.18900250e-08
  1.02755091e-08 9.58252882e-09 3.48227279e-08 7.81661917e-09
  7.39943008e-09 6.93601927e-09 2.62676295e-08 6.27610298e-09
  5.93205809e-09 5.70025641e-09 2.16958950e-08 5.41447627e-09
  5.01230024e-09 4.86810723e-09 1.83289787e-08 4.92251381e-09
  4.39542865e-09 4.29660124e-09 1.57254248e-08 4.66579645e-09
  3.97338431e-09 3.82826879e-09 1.33631119e-08 4.69462210e-09
  3.62646361e-09 3.50868120e-09 1.12932114e-08 4.91374319e-09
  3.50995825e-09 3.25015529e-09]
 [2.23445210e+00 4.82510523e-03 1.77687515e-05 4.38337237e-06
  1.77260346e-06 9.45960934e-07 5.34631580e-07 3.58690791e-07
  2.44808335e-07 1.71744780e-07 1.28324419e-07 1.00853411e-07
  7.78881021e-08 5.35504949e-08 7.98293948e-07 9.73512096e-08
  1.97602524e-08 1.97591235e-08 5.57042967e-08 1.44304920e-08
  1.09786455e-08 1.03987726e-08 3.57546764e-08 8.78293154e-09
  7.58346929e-09 7.12499899e-09 2.63536155e-08 6.92631854e-09
  5.98078164e-09 5.73939704e-09 2.15551996e-08 5.88202428e-09
  5.03091089e-09 4.89379537e-09 1.80709283e-08 5.32777257e-09
  4.42598128e-09 4.30089550e-09 1.52662951e-08 5.19905730e-09
  4.05653642e-09 3.85644768e-09 1.29157433e-08 5.34850716e-09
  3.76110777e-09 3.59815074e-09 1.10185963e-08 5.82570078e-09
  3.95155018e-09 3.62184970e-09]
 [2.72723935e+00 2.05675610e-02 2.05560233e-05 5.67372625e-06
  2.43193902e-06 1.37875720e-06 7.94855286e-07 5.53425061e-07
  3.91164091e-07 2.91128957e-07 2.28080063e-07 2.01047318e-07
  1.66783727e-07 1.04878323e-07 7.06479077e-07 1.17868493e-07
  3.23478639e-08 3.56785228e-08 8.79833887e-08 2.52257190e-08
  1.76058737e-08 1.69357849e-08 5.40189703e-08 1.47426738e-08
  1.17547623e-08 1.10121451e-08 3.94430374e-08 1.13377914e-08
  9.05005993e-09 8.76179071e-09 3.18980201e-08 9.51699090e-09
  7.59344619e-09 7.44268519e-09 2.61957171e-08 8.84942211e-09
  6.65308527e-09 6.53217620e-09 2.17130800e-08 8.84888414e-09
  6.03176905e-09 5.75825241e-09 1.76153198e-08 9.01255253e-09
  5.67611221e-09 5.35120927e-09 1.45930990e-08 9.44820929e-09
  5.89331882e-09 5.32672352e-09]]
train_ae_loss [[4.40196015 3.00360241 4.19528996 4.93437554 5.20486808 5.46721666
  5.47555261 5.53203091 5.58075545 5.50670939 5.42935136 5.26500886
  5.15450142 4.83954991 4.60914705 4.21540296 4.01107262 3.90888513
  3.79594506 3.57434012 3.49591176 3.45160407 3.37902223 3.2968269
  3.23848429 3.21225503 3.19877565 3.16016579 3.12870281 3.12756376
  3.10949571 3.07493144 3.07887822 3.07011308 3.07574469 3.05517378
  3.05066929 3.05061586 3.0654777  3.04710112 3.04515537 3.05241439
  3.03840325 3.05322872 3.03690951 3.03584301 3.04555803 3.05297649
  3.03921688 3.04246195]
 [3.51422296 2.72683742 3.91449194 4.58705819 4.7384661  4.97067726
  4.7425834  4.65659448 4.68095382 4.5516722  4.48744139 4.29299703
  4.18052361 3.81405621 3.54525118 3.16758882 2.93627713 2.84965163
  2.65813432 2.32971728 2.22938099 2.12194515 2.00270663 1.86210151
  1.79147062 1.7392408  1.74573298 1.6761711  1.6532027  1.63400088
  1.61129167 1.55624092 1.57001825 1.55840317 1.54529618 1.53096038
  1.52837439 1.51896563 1.53015185 1.51469954 1.51388546 1.50861798
  1.50614291 1.50888684 1.49404412 1.50342962 1.50601071 1.50515124
  1.48884088 1.50074205]
 [3.56934714 2.69947078 4.07878814 4.78602296 4.93109746 5.25609447
  4.78511196 4.65541906 4.66803259 4.53329727 4.46447334 4.23570549
  4.09269087 3.62247456 3.12933226 2.73951989 2.41348329 2.32739459
  2.09506585 1.82868327 1.78917722 1.7311078  1.6166139  1.50877579
  1.44391358 1.39801553 1.39653908 1.32787956 1.31840378 1.31118899
  1.26875373 1.2125447  1.24080863 1.23133045 1.20859438 1.19038124
  1.19435512 1.17967037 1.18540734 1.18216068 1.17936744 1.17072232
  1.17156509 1.15794422 1.14815937 1.18034613 1.1694352  1.16907366
  1.15463256 1.16691257]
 [3.74211289 2.3893738  3.59914586 4.39116739 4.5088169  4.94387186
  4.29609186 4.11058387 4.10695707 3.9651282  3.88267606 3.63568073
  3.45797974 3.02097769 2.57044896 2.28917958 2.0222095  1.95555298
  1.75011969 1.52661135 1.49687827 1.4477306  1.340466   1.25111949
  1.19603887 1.16055219 1.1569079  1.09624052 1.08398816 1.08498017
  1.04643788 0.99732452 1.02025339 1.01277543 0.99431358 0.97616131
  0.98220025 0.96925747 0.97064718 0.96858626 0.96875812 0.95913242
  0.95535849 0.94963196 0.94124719 0.97032417 0.95675268 0.95625994
  0.94777931 0.95442012]
 [4.26791081 2.21814582 3.35658603 4.12221786 4.50853967 4.99511638
  4.16766271 3.902512   3.85773592 3.68199042 3.58989662 3.36989471
  3.18674461 2.77901099 2.33624497 2.09076784 1.84016708 1.78258217
  1.58710651 1.38347005 1.3589637  1.31458774 1.21369514 1.13183325
  1.08203408 1.05074048 1.04608937 0.99013031 0.97850864 0.98050281
  0.94389706 0.89991269 0.92083299 0.91374736 0.89692451 0.87931011
  0.88551041 0.8731578  0.87452805 0.87238482 0.87302267 0.86438828
  0.86012563 0.85496904 0.84795205 0.8751458  0.8615928  0.8614936
  0.85414664 0.86020342]]
valid_acc (5, 50)
valid_AUC (5, 50)
train_acc (50,)
total_train+valid_time 1289.424402674
Start Testing
Load ckpt at ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt
gc 9
Test layer0 Acc 0.7196, AUC 0.7958537936210632, avg_entr 0.2324468493461609, f1 0.7196000218391418
l0_test_time 0.3040799909999805
gc 0
Test layer1 Acc 0.7482, AUC 0.827545702457428, avg_entr 0.18105463683605194, f1 0.748199999332428
l1_test_time 0.3596950809999271
gc 0
Test layer2 Acc 0.7558, AUC 0.8395100831985474, avg_entr 0.15003244578838348, f1 0.7557999491691589
l2_test_time 0.46861498499993104
gc 0
Test layer3 Acc 0.759, AUC 0.8443667888641357, avg_entr 0.12006276845932007, f1 0.7589999437332153
l3_test_time 0.6195100360000652
gc 0
Test layer4 Acc 0.7568, AUC 0.8447322845458984, avg_entr 0.10779228806495667, f1 0.7567999958992004
l4_test_time 0.8150064980000025
gc 0
Test threshold 0.1 Acc 0.7576, AUC 0.8297233581542969, avg_entr 0.16024260222911835, f1 0.7576000690460205
t0.1_test_time 0.5681138099998861
gc 0
Test threshold 0.2 Acc 0.7568, AUC 0.8233153820037842, avg_entr 0.17130351066589355, f1 0.7567999958992004
t0.2_test_time 0.5149032510000779
gc 0
Test threshold 0.3 Acc 0.7548, AUC 0.8199456930160522, avg_entr 0.18181337416172028, f1 0.754800021648407
t0.3_test_time 0.4820976949999931
gc 0
Test threshold 0.4 Acc 0.7542, AUC 0.8174351453781128, avg_entr 0.19417041540145874, f1 0.7541999816894531
t0.4_test_time 0.4515295599999263
gc 0
Test threshold 0.5 Acc 0.751, AUC 0.8120100498199463, avg_entr 0.20802150666713715, f1 0.7509999871253967
t0.5_test_time 0.4283503589999782
gc 0
Test threshold 0.6 Acc 0.7466, AUC 0.8105826377868652, avg_entr 0.22370921075344086, f1 0.7465999126434326
t0.6_test_time 0.40624742999989394
gc 0
Test threshold 0.7 Acc 0.7426, AUC 0.8072187304496765, avg_entr 0.24041691422462463, f1 0.7426000237464905
t0.7_test_time 0.3864653000000544
gc 0
Test threshold 0.8 Acc 0.7384, AUC 0.8047938942909241, avg_entr 0.25882795453071594, f1 0.7384000420570374
t0.8_test_time 0.3719378549999419
gc 0
Test threshold 0.9 Acc 0.7324, AUC 0.8012645244598389, avg_entr 0.28267425298690796, f1 0.7323999404907227
t0.9_test_time 0.357696666000038
