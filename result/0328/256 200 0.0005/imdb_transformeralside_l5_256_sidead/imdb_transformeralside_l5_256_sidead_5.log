total count words 222751
vocab size 30000
train size 40000, valid size 5000, test size 5000
found 27937 words in glove
model: TransformerALsideText(
  (layers): ModuleList(
    (0): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=2, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=2, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
  (emb_layers): ModuleList(
    (0): Embedding(30000, 300)
    (1): Embedding(30000, 300)
    (2): Embedding(30000, 300)
    (3): Embedding(30000, 300)
    (4): Embedding(30000, 300)
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.multi_headed_attention.linears.0.weight 90000
layers.0.enc.f.multi_headed_attention.linears.0.bias 300
layers.0.enc.f.multi_headed_attention.linears.1.weight 90000
layers.0.enc.f.multi_headed_attention.linears.1.bias 300
layers.0.enc.f.multi_headed_attention.linears.2.weight 90000
layers.0.enc.f.multi_headed_attention.linears.2.bias 300
layers.0.enc.f.multi_headed_attention.linears.3.weight 90000
layers.0.enc.f.multi_headed_attention.linears.3.bias 300
layers.0.enc.f.feed_forward.w_1.weight 90000
layers.0.enc.f.feed_forward.w_1.bias 300
layers.0.enc.f.feed_forward.w_2.weight 90000
layers.0.enc.f.feed_forward.w_2.bias 300
layers.0.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.0.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.0.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.0.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.0.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.0.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.0.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.0.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.0.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.0.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.0.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.0.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.0.enc.f.encoder.norm.a 300
layers.0.enc.f.encoder.norm.b 300
layers.0.ae.g.0.weight 256
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 256
layers.0.ae.h.0.bias 2
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
emb_layers.0.weight 9000000
emb_layers.1.weight 9000000
emb_layers.2.weight 9000000
emb_layers.3.weight 9000000
emb_layers.4.weight 9000000
Total Trainable Params: 50758378
init_time 21.596528128
Start Training
gc 0
Train Epoch0 Acc 0.501825 (20073/40000), AUC 0.5009479522705078
ep0_train_time 23.530933238000003
Test Epoch0 layer0 Acc 0.5124, AUC 0.5726487636566162, avg_entr 0.6955497860908508, f1 0.5123999714851379
ep0_l0_test_time 0.2870220140000015
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 0
Test Epoch0 layer1 Acc 0.511, AUC 0.5615236759185791, avg_entr 0.6926913261413574, f1 0.5109999775886536
ep0_l1_test_time 0.3569987879999985
Test Epoch0 layer2 Acc 0.5006, AUC 0.5426592826843262, avg_entr 0.6886933445930481, f1 0.5005999803543091
ep0_l2_test_time 0.4657173450000016
Test Epoch0 layer3 Acc 0.5, AUC 0.5279157161712646, avg_entr 0.6858727335929871, f1 0.5
ep0_l3_test_time 0.6152921739999968
Test Epoch0 layer4 Acc 0.5, AUC 0.5185447931289673, avg_entr 0.6949020624160767, f1 0.5
ep0_l4_test_time 0.8098441460000032
gc 0
Train Epoch1 Acc 0.5093 (20372/40000), AUC 0.5152311325073242
ep1_train_time 22.987235865000002
Test Epoch1 layer0 Acc 0.5846, AUC 0.6311432123184204, avg_entr 0.6675423383712769, f1 0.5845999717712402
ep1_l0_test_time 0.28539550400000735
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 1
Test Epoch1 layer1 Acc 0.585, AUC 0.6271483302116394, avg_entr 0.6812238097190857, f1 0.5849999785423279
ep1_l1_test_time 0.3599256140000051
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 1
Test Epoch1 layer2 Acc 0.571, AUC 0.6078317165374756, avg_entr 0.6907509565353394, f1 0.5709999799728394
ep1_l2_test_time 0.4738393199999962
Test Epoch1 layer3 Acc 0.5024, AUC 0.5512739419937134, avg_entr 0.6930739283561707, f1 0.5023999810218811
ep1_l3_test_time 0.6142797800000039
Test Epoch1 layer4 Acc 0.513, AUC 0.5270450115203857, avg_entr 0.6913374662399292, f1 0.5130000114440918
ep1_l4_test_time 0.811071265999999
gc 0
Train Epoch2 Acc 0.5204 (20816/40000), AUC 0.5292326211929321
ep2_train_time 23.018332060999995
Test Epoch2 layer0 Acc 0.5614, AUC 0.6553239822387695, avg_entr 0.5188661813735962, f1 0.5613999962806702
ep2_l0_test_time 0.29043340000001194
Test Epoch2 layer1 Acc 0.5488, AUC 0.6692212224006653, avg_entr 0.5335478782653809, f1 0.548799991607666
ep2_l1_test_time 0.35559634499999504
Test Epoch2 layer2 Acc 0.5112, AUC 0.6677556037902832, avg_entr 0.4907371997833252, f1 0.5112000107765198
ep2_l2_test_time 0.4646578410000046
Test Epoch2 layer3 Acc 0.5108, AUC 0.6524538993835449, avg_entr 0.6243463754653931, f1 0.5108000040054321
ep2_l3_test_time 0.615858020999994
Test Epoch2 layer4 Acc 0.5, AUC 0.6311309337615967, avg_entr 0.6634588837623596, f1 0.5
ep2_l4_test_time 0.8085197949999952
gc 0
Train Epoch3 Acc 0.563 (22520/40000), AUC 0.5826316475868225
ep3_train_time 23.059296922
Test Epoch3 layer0 Acc 0.6528, AUC 0.7209712266921997, avg_entr 0.5084503889083862, f1 0.6528000235557556
ep3_l0_test_time 0.28616105400000436
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 3
Test Epoch3 layer1 Acc 0.668, AUC 0.7474809288978577, avg_entr 0.48153525590896606, f1 0.6679999828338623
ep3_l1_test_time 0.3565330490000065
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 3
Test Epoch3 layer2 Acc 0.6744, AUC 0.7549408674240112, avg_entr 0.49331727623939514, f1 0.6743999719619751
ep3_l2_test_time 0.4686499050000066
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 3
Test Epoch3 layer3 Acc 0.668, AUC 0.7459354400634766, avg_entr 0.5476726293563843, f1 0.6679999828338623
ep3_l3_test_time 0.6173511109999907
Test Epoch3 layer4 Acc 0.6212, AUC 0.7388187646865845, avg_entr 0.580786406993866, f1 0.6212000250816345
ep3_l4_test_time 0.8093852669999961
gc 0
Train Epoch4 Acc 0.625575 (25023/40000), AUC 0.672229528427124
ep4_train_time 23.053269825000015
Test Epoch4 layer0 Acc 0.584, AUC 0.747852087020874, avg_entr 0.2532714903354645, f1 0.5839999914169312
ep4_l0_test_time 0.2857410580000135
Test Epoch4 layer1 Acc 0.5836, AUC 0.7787312269210815, avg_entr 0.26238444447517395, f1 0.5835999846458435
ep4_l1_test_time 0.35535372600000414
Test Epoch4 layer2 Acc 0.5754, AUC 0.7901872396469116, avg_entr 0.26870355010032654, f1 0.5753999948501587
ep4_l2_test_time 0.46566792000001556
Test Epoch4 layer3 Acc 0.5458, AUC 0.7783517837524414, avg_entr 0.35072723031044006, f1 0.545799970626831
ep4_l3_test_time 0.6162593889999926
Test Epoch4 layer4 Acc 0.5056, AUC 0.776893138885498, avg_entr 0.40488919615745544, f1 0.5055999755859375
ep4_l4_test_time 0.8111241179999809
gc 0
Train Epoch5 Acc 0.6668 (26672/40000), AUC 0.7327951192855835
ep5_train_time 23.077195473000018
Test Epoch5 layer0 Acc 0.6928, AUC 0.7664005756378174, avg_entr 0.39544570446014404, f1 0.692799985408783
ep5_l0_test_time 0.28670511899997564
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 5
Test Epoch5 layer1 Acc 0.7294, AUC 0.8030668497085571, avg_entr 0.37319356203079224, f1 0.7293999791145325
ep5_l1_test_time 0.3563633800000048
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 5
Test Epoch5 layer2 Acc 0.7316, AUC 0.8167799115180969, avg_entr 0.36327219009399414, f1 0.7316000461578369
ep5_l2_test_time 0.4646056139999928
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 5
Test Epoch5 layer3 Acc 0.714, AUC 0.8178809285163879, avg_entr 0.3518408536911011, f1 0.7139999866485596
ep5_l3_test_time 0.6165108900000007
Test Epoch5 layer4 Acc 0.6892, AUC 0.8144901990890503, avg_entr 0.3503696620464325, f1 0.6891999840736389
ep5_l4_test_time 0.809307686000011
gc 0
Train Epoch6 Acc 0.711725 (28469/40000), AUC 0.7855461239814758
ep6_train_time 23.13963138600002
Test Epoch6 layer0 Acc 0.6804, AUC 0.7764978408813477, avg_entr 0.38116487860679626, f1 0.680400013923645
ep6_l0_test_time 0.2871221089999949
Test Epoch6 layer1 Acc 0.702, AUC 0.8173956274986267, avg_entr 0.352759450674057, f1 0.7020000219345093
ep6_l1_test_time 0.3558702639999751
Test Epoch6 layer2 Acc 0.7104, AUC 0.833405613899231, avg_entr 0.3404064178466797, f1 0.7103999257087708
ep6_l2_test_time 0.465066443000012
Test Epoch6 layer3 Acc 0.6634, AUC 0.8322396278381348, avg_entr 0.3177390694618225, f1 0.6633999943733215
ep6_l3_test_time 0.6206880940000019
Test Epoch6 layer4 Acc 0.618, AUC 0.8307178020477295, avg_entr 0.29663676023483276, f1 0.6179999709129333
ep6_l4_test_time 0.8093822959999954
gc 0
Train Epoch7 Acc 0.762825 (30513/40000), AUC 0.8433206081390381
ep7_train_time 23.062900589999998
Test Epoch7 layer0 Acc 0.6942, AUC 0.7819463610649109, avg_entr 0.34877002239227295, f1 0.6941999793052673
ep7_l0_test_time 0.2850651589999984
Test Epoch7 layer1 Acc 0.7342, AUC 0.8259237408638, avg_entr 0.29720059037208557, f1 0.7342000007629395
ep7_l1_test_time 0.35481201400000373
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 7
Test Epoch7 layer2 Acc 0.7516, AUC 0.8433891534805298, avg_entr 0.27363476157188416, f1 0.7516000270843506
ep7_l2_test_time 0.46933064800001034
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 7
Test Epoch7 layer3 Acc 0.7438, AUC 0.8450227975845337, avg_entr 0.2529122829437256, f1 0.7437999248504639
ep7_l3_test_time 0.6152836720000039
Test Epoch7 layer4 Acc 0.7282, AUC 0.8451036214828491, avg_entr 0.24660104513168335, f1 0.7282000184059143
ep7_l4_test_time 0.8088847129999976
gc 0
Train Epoch8 Acc 0.7759 (31036/40000), AUC 0.8585533499717712
ep8_train_time 23.037682081000014
Test Epoch8 layer0 Acc 0.711, AUC 0.7903287410736084, avg_entr 0.3371189832687378, f1 0.7110000848770142
ep8_l0_test_time 0.28736028299999816
Test Epoch8 layer1 Acc 0.7486, AUC 0.8309570550918579, avg_entr 0.27995172142982483, f1 0.7486000061035156
ep8_l1_test_time 0.38055552800000214
Test Epoch8 layer2 Acc 0.7668, AUC 0.8482988476753235, avg_entr 0.2494538277387619, f1 0.7667999863624573
ep8_l2_test_time 0.46563268100001665
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 8
Test Epoch8 layer3 Acc 0.7656, AUC 0.8508298397064209, avg_entr 0.24538128077983856, f1 0.7655999660491943
ep8_l3_test_time 0.6170598289999703
Test Epoch8 layer4 Acc 0.7648, AUC 0.8518557548522949, avg_entr 0.24067749083042145, f1 0.764799952507019
ep8_l4_test_time 0.8136513899999613
gc 0
Train Epoch9 Acc 0.794425 (31777/40000), AUC 0.8736878633499146
ep9_train_time 23.112537514000053
Test Epoch9 layer0 Acc 0.7026, AUC 0.791664719581604, avg_entr 0.2757291793823242, f1 0.7026000022888184
ep9_l0_test_time 0.2860303040000076
Test Epoch9 layer1 Acc 0.7326, AUC 0.8315796852111816, avg_entr 0.24195873737335205, f1 0.7325999140739441
ep9_l1_test_time 0.365982272999986
Test Epoch9 layer2 Acc 0.7326, AUC 0.8494188189506531, avg_entr 0.22167378664016724, f1 0.7325999140739441
ep9_l2_test_time 0.4645966470000076
Test Epoch9 layer3 Acc 0.727, AUC 0.8512839078903198, avg_entr 0.20223447680473328, f1 0.7269999980926514
ep9_l3_test_time 0.6161004229999776
Test Epoch9 layer4 Acc 0.7224, AUC 0.8522214889526367, avg_entr 0.18853968381881714, f1 0.7224000096321106
ep9_l4_test_time 0.8143954669999971
gc 0
Train Epoch10 Acc 0.8227 (32908/40000), AUC 0.9029138088226318
ep10_train_time 23.142656654999996
Test Epoch10 layer0 Acc 0.7164, AUC 0.7935421466827393, avg_entr 0.3001372814178467, f1 0.7164000272750854
ep10_l0_test_time 0.28647912399998177
Test Epoch10 layer1 Acc 0.7488, AUC 0.8334568738937378, avg_entr 0.2523731291294098, f1 0.7487999796867371
ep10_l1_test_time 0.3541394220000029
Test Epoch10 layer2 Acc 0.77, AUC 0.8532296419143677, avg_entr 0.230680912733078, f1 0.7699999809265137
ep10_l2_test_time 0.46451694800003906
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 10
Test Epoch10 layer3 Acc 0.7726, AUC 0.8555666208267212, avg_entr 0.19921106100082397, f1 0.772599995136261
ep10_l3_test_time 0.6154175059999716
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 10
Test Epoch10 layer4 Acc 0.776, AUC 0.8559719920158386, avg_entr 0.17124158143997192, f1 0.7759999632835388
ep10_l4_test_time 0.8126939569999649
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 10
gc 0
Train Epoch11 Acc 0.8375 (33500/40000), AUC 0.9152472019195557
ep11_train_time 23.057875077000006
Test Epoch11 layer0 Acc 0.6966, AUC 0.7922999858856201, avg_entr 0.28195399045944214, f1 0.6966000199317932
ep11_l0_test_time 0.28783954400000766
Test Epoch11 layer1 Acc 0.711, AUC 0.8345464468002319, avg_entr 0.23947040736675262, f1 0.7110000848770142
ep11_l1_test_time 0.3551752949999809
Test Epoch11 layer2 Acc 0.74, AUC 0.8550947308540344, avg_entr 0.21563325822353363, f1 0.7400000095367432
ep11_l2_test_time 0.464330718000042
Test Epoch11 layer3 Acc 0.7316, AUC 0.8561804294586182, avg_entr 0.1785857081413269, f1 0.7316000461578369
ep11_l3_test_time 0.6252279970000245
Test Epoch11 layer4 Acc 0.7326, AUC 0.8566040992736816, avg_entr 0.15946698188781738, f1 0.7325999140739441
ep11_l4_test_time 0.8186272280000253
gc 0
Train Epoch12 Acc 0.8534 (34136/40000), AUC 0.9265278577804565
ep12_train_time 23.200690908000013
Test Epoch12 layer0 Acc 0.679, AUC 0.7935092449188232, avg_entr 0.2450825721025467, f1 0.6790000200271606
ep12_l0_test_time 0.2873725979999904
Test Epoch12 layer1 Acc 0.7202, AUC 0.8343993425369263, avg_entr 0.21188798546791077, f1 0.7202000021934509
ep12_l1_test_time 0.36010931900000287
Test Epoch12 layer2 Acc 0.7418, AUC 0.8537977933883667, avg_entr 0.17145885527133942, f1 0.74180006980896
ep12_l2_test_time 0.4666913400000112
Test Epoch12 layer3 Acc 0.735, AUC 0.8548721075057983, avg_entr 0.12211772799491882, f1 0.7350000143051147
ep12_l3_test_time 0.616009092000013
Test Epoch12 layer4 Acc 0.7318, AUC 0.856447696685791, avg_entr 0.10879719257354736, f1 0.7318000197410583
ep12_l4_test_time 0.8109897769999748
gc 0
Train Epoch13 Acc 0.864125 (34565/40000), AUC 0.9329205751419067
ep13_train_time 23.055706248999968
Test Epoch13 layer0 Acc 0.6808, AUC 0.7883071899414062, avg_entr 0.19631367921829224, f1 0.6808000206947327
ep13_l0_test_time 0.2861231269999962
Test Epoch13 layer1 Acc 0.7008, AUC 0.8353639245033264, avg_entr 0.16967619955539703, f1 0.7008000016212463
ep13_l1_test_time 0.35368067100000644
Test Epoch13 layer2 Acc 0.686, AUC 0.8507575988769531, avg_entr 0.12215328216552734, f1 0.6859999895095825
ep13_l2_test_time 0.46414646300002005
Test Epoch13 layer3 Acc 0.6786, AUC 0.8527801036834717, avg_entr 0.09174888581037521, f1 0.678600013256073
ep13_l3_test_time 0.6141470499999855
Test Epoch13 layer4 Acc 0.6708, AUC 0.8493881225585938, avg_entr 0.0855545625090599, f1 0.670799970626831
ep13_l4_test_time 0.8095456740000486
gc 0
Train Epoch14 Acc 0.878475 (35139/40000), AUC 0.9451167583465576
ep14_train_time 23.12683840699998
Test Epoch14 layer0 Acc 0.715, AUC 0.7916196584701538, avg_entr 0.23289799690246582, f1 0.7150000333786011
ep14_l0_test_time 0.2860130220000201
Test Epoch14 layer1 Acc 0.7564, AUC 0.8341734409332275, avg_entr 0.19194243848323822, f1 0.7563999891281128
ep14_l1_test_time 0.3560230969999907
Test Epoch14 layer2 Acc 0.7652, AUC 0.848021388053894, avg_entr 0.1418270319700241, f1 0.7652000188827515
ep14_l2_test_time 0.46574572999998054
Test Epoch14 layer3 Acc 0.7668, AUC 0.8489253520965576, avg_entr 0.11394087970256805, f1 0.7667999863624573
ep14_l3_test_time 0.6179168009999785
Test Epoch14 layer4 Acc 0.7684, AUC 0.8520604372024536, avg_entr 0.10621413588523865, f1 0.7684000730514526
ep14_l4_test_time 0.8096392589999937
gc 0
Train Epoch15 Acc 0.877575 (35103/40000), AUC 0.9406783580780029
ep15_train_time 23.209853285999998
Test Epoch15 layer0 Acc 0.7076, AUC 0.7886402010917664, avg_entr 0.22290575504302979, f1 0.707599937915802
ep15_l0_test_time 0.28590703899999426
Test Epoch15 layer1 Acc 0.7246, AUC 0.8321270942687988, avg_entr 0.19342471659183502, f1 0.7245999574661255
ep15_l1_test_time 0.3554115960000104
Test Epoch15 layer2 Acc 0.7256, AUC 0.8513521552085876, avg_entr 0.11532000452280045, f1 0.7255999445915222
ep15_l2_test_time 0.46421466000003875
Test Epoch15 layer3 Acc 0.7178, AUC 0.8527314066886902, avg_entr 0.09385724365711212, f1 0.7178000211715698
ep15_l3_test_time 0.6142222649999667
Test Epoch15 layer4 Acc 0.711, AUC 0.8535823822021484, avg_entr 0.08871723711490631, f1 0.7110000848770142
ep15_l4_test_time 0.8101143580000212
gc 0
Train Epoch16 Acc 0.908325 (36333/40000), AUC 0.9668525457382202
ep16_train_time 23.19404628899997
Test Epoch16 layer0 Acc 0.7054, AUC 0.7831698656082153, avg_entr 0.2138826698064804, f1 0.7053999900817871
ep16_l0_test_time 0.29182214700000486
Test Epoch16 layer1 Acc 0.7388, AUC 0.8225547075271606, avg_entr 0.18795229494571686, f1 0.7387999892234802
ep16_l1_test_time 0.36198783399999
Test Epoch16 layer2 Acc 0.757, AUC 0.8404281139373779, avg_entr 0.10392291098833084, f1 0.7570000886917114
ep16_l2_test_time 0.47131742199997007
Test Epoch16 layer3 Acc 0.7596, AUC 0.842625617980957, avg_entr 0.09126481413841248, f1 0.7595999836921692
ep16_l3_test_time 0.6264149900000007
Test Epoch16 layer4 Acc 0.7614, AUC 0.8444337844848633, avg_entr 0.08946405351161957, f1 0.7613999843597412
ep16_l4_test_time 0.8180813390000026
gc 0
Train Epoch17 Acc 0.91675 (36670/40000), AUC 0.9720003604888916
ep17_train_time 23.156968344999996
Test Epoch17 layer0 Acc 0.7046, AUC 0.7808129191398621, avg_entr 0.2086147516965866, f1 0.7045999765396118
ep17_l0_test_time 0.293159060999983
Test Epoch17 layer1 Acc 0.7426, AUC 0.823739230632782, avg_entr 0.18416468799114227, f1 0.7426000237464905
ep17_l1_test_time 0.369347619999985
Test Epoch17 layer2 Acc 0.7536, AUC 0.8409438133239746, avg_entr 0.09438973665237427, f1 0.7535999417304993
ep17_l2_test_time 0.47240872000003264
Test Epoch17 layer3 Acc 0.7572, AUC 0.8437436819076538, avg_entr 0.0810602679848671, f1 0.7572000026702881
ep17_l3_test_time 0.622803174000012
Test Epoch17 layer4 Acc 0.7582, AUC 0.8460643291473389, avg_entr 0.08069866895675659, f1 0.7582000494003296
ep17_l4_test_time 0.8170537690000401
gc 0
Train Epoch18 Acc 0.930225 (37209/40000), AUC 0.9790335893630981
ep18_train_time 23.150136852000003
Test Epoch18 layer0 Acc 0.7034, AUC 0.7770323753356934, avg_entr 0.19594843685626984, f1 0.7034000158309937
ep18_l0_test_time 0.29452486200000294
Test Epoch18 layer1 Acc 0.7384, AUC 0.8226544857025146, avg_entr 0.16873475909233093, f1 0.7384000420570374
ep18_l1_test_time 0.36182881299998826
Test Epoch18 layer2 Acc 0.7486, AUC 0.8321961164474487, avg_entr 0.08280434459447861, f1 0.7486000061035156
ep18_l2_test_time 0.4717312520000405
Test Epoch18 layer3 Acc 0.7574, AUC 0.8392128348350525, avg_entr 0.07003522664308548, f1 0.7573999762535095
ep18_l3_test_time 0.6242967989999215
Test Epoch18 layer4 Acc 0.758, AUC 0.8419591784477234, avg_entr 0.06995423883199692, f1 0.7580000162124634
ep18_l4_test_time 0.818743844000096
gc 0
Train Epoch19 Acc 0.93115 (37246/40000), AUC 0.9771299362182617
ep19_train_time 23.152724163000016
Test Epoch19 layer0 Acc 0.7082, AUC 0.7784496545791626, avg_entr 0.19468246400356293, f1 0.7081999778747559
ep19_l0_test_time 0.30002386500007105
Test Epoch19 layer1 Acc 0.7452, AUC 0.8220161199569702, avg_entr 0.16669611632823944, f1 0.745199978351593
ep19_l1_test_time 0.3605087739999817
Test Epoch19 layer2 Acc 0.7544, AUC 0.8316874504089355, avg_entr 0.07998006045818329, f1 0.7544000148773193
ep19_l2_test_time 0.4700528570000415
Test Epoch19 layer3 Acc 0.755, AUC 0.8375674486160278, avg_entr 0.0696917325258255, f1 0.7549999356269836
ep19_l3_test_time 0.6249698289999515
Test Epoch19 layer4 Acc 0.7568, AUC 0.83954918384552, avg_entr 0.06675536185503006, f1 0.7567999958992004
ep19_l4_test_time 0.816672644999926
gc 0
Train Epoch20 Acc 0.9393 (37572/40000), AUC 0.9833924174308777
ep20_train_time 23.152835793999998
Test Epoch20 layer0 Acc 0.7036, AUC 0.777251660823822, avg_entr 0.19308598339557648, f1 0.7035999894142151
ep20_l0_test_time 0.2903006750000259
Test Epoch20 layer1 Acc 0.7442, AUC 0.8205589056015015, avg_entr 0.17263263463974, f1 0.7441999316215515
ep20_l1_test_time 0.3664726360000259
Test Epoch20 layer2 Acc 0.7548, AUC 0.8313654661178589, avg_entr 0.08258577436208725, f1 0.754800021648407
ep20_l2_test_time 0.47195357699990836
Test Epoch20 layer3 Acc 0.7616, AUC 0.8381649255752563, avg_entr 0.07607311755418777, f1 0.7616000771522522
ep20_l3_test_time 0.622788263000075
Test Epoch20 layer4 Acc 0.763, AUC 0.8402307033538818, avg_entr 0.07507972419261932, f1 0.7630000114440918
ep20_l4_test_time 0.818488425000055
gc 0
Train Epoch21 Acc 0.943175 (37727/40000), AUC 0.9842910766601562
ep21_train_time 23.17248985899994
Test Epoch21 layer0 Acc 0.709, AUC 0.7757614850997925, avg_entr 0.19481462240219116, f1 0.7090000510215759
ep21_l0_test_time 0.29691283299996485
Test Epoch21 layer1 Acc 0.7454, AUC 0.8193027973175049, avg_entr 0.17083407938480377, f1 0.745400071144104
ep21_l1_test_time 0.3614646180000136
Test Epoch21 layer2 Acc 0.7582, AUC 0.8299399018287659, avg_entr 0.07602880895137787, f1 0.7582000494003296
ep21_l2_test_time 0.4713595540000597
Test Epoch21 layer3 Acc 0.7642, AUC 0.8371474742889404, avg_entr 0.06791699677705765, f1 0.76419997215271
ep21_l3_test_time 0.6253471439999885
Test Epoch21 layer4 Acc 0.7638, AUC 0.8395514488220215, avg_entr 0.06688421219587326, f1 0.7638000249862671
ep21_l4_test_time 0.8172885440000073
gc 0
Train Epoch22 Acc 0.953125 (38125/40000), AUC 0.98900306224823
ep22_train_time 23.13675762699995
Test Epoch22 layer0 Acc 0.7072, AUC 0.775160551071167, avg_entr 0.18884237110614777, f1 0.7071999907493591
ep22_l0_test_time 0.2898988320000626
Test Epoch22 layer1 Acc 0.7446, AUC 0.8190642595291138, avg_entr 0.16735418140888214, f1 0.7445999979972839
ep22_l1_test_time 0.35997634700004255
Test Epoch22 layer2 Acc 0.754, AUC 0.8267205953598022, avg_entr 0.06793608516454697, f1 0.7540000081062317
ep22_l2_test_time 0.47008786100002453
Test Epoch22 layer3 Acc 0.7608, AUC 0.8340941667556763, avg_entr 0.05418635159730911, f1 0.7608000040054321
ep22_l3_test_time 0.627540477000025
Test Epoch22 layer4 Acc 0.762, AUC 0.8370629549026489, avg_entr 0.051965247839689255, f1 0.7619999647140503
ep22_l4_test_time 0.81934722099993
gc 0
Train Epoch23 Acc 0.953175 (38127/40000), AUC 0.9883400797843933
ep23_train_time 23.158197556999994
Test Epoch23 layer0 Acc 0.7054, AUC 0.7741270065307617, avg_entr 0.18461593985557556, f1 0.7053999900817871
ep23_l0_test_time 0.29265398000006826
Test Epoch23 layer1 Acc 0.7402, AUC 0.8167406320571899, avg_entr 0.15656325221061707, f1 0.7401999831199646
ep23_l1_test_time 0.3607980790000056
Test Epoch23 layer2 Acc 0.7502, AUC 0.8252570629119873, avg_entr 0.06383593380451202, f1 0.7501999735832214
ep23_l2_test_time 0.47009127100000114
Test Epoch23 layer3 Acc 0.7542, AUC 0.8317050337791443, avg_entr 0.055659178644418716, f1 0.7541999816894531
ep23_l3_test_time 0.6247135330000901
Test Epoch23 layer4 Acc 0.7572, AUC 0.8345120549201965, avg_entr 0.05335398390889168, f1 0.7572000026702881
ep23_l4_test_time 0.8164458909999439
gc 0
Train Epoch24 Acc 0.957225 (38289/40000), AUC 0.9899826049804688
ep24_train_time 23.14646431899996
Test Epoch24 layer0 Acc 0.7018, AUC 0.7737572193145752, avg_entr 0.18521668016910553, f1 0.7017999887466431
ep24_l0_test_time 0.2905588959999932
Test Epoch24 layer1 Acc 0.7424, AUC 0.8143314123153687, avg_entr 0.16073057055473328, f1 0.742400050163269
ep24_l1_test_time 0.36237832900008016
Test Epoch24 layer2 Acc 0.7546, AUC 0.8234279155731201, avg_entr 0.0666152685880661, f1 0.7546000480651855
ep24_l2_test_time 0.4705234720000817
Test Epoch24 layer3 Acc 0.7568, AUC 0.8295749425888062, avg_entr 0.0530962236225605, f1 0.7567999958992004
ep24_l3_test_time 0.6262885349999578
Test Epoch24 layer4 Acc 0.7586, AUC 0.8336122632026672, avg_entr 0.05108249559998512, f1 0.7585999965667725
ep24_l4_test_time 0.817064321000089
gc 0
Train Epoch25 Acc 0.957625 (38305/40000), AUC 0.9902241826057434
ep25_train_time 23.14197475700007
Test Epoch25 layer0 Acc 0.701, AUC 0.7728612422943115, avg_entr 0.18345531821250916, f1 0.7009999752044678
ep25_l0_test_time 0.29213147999996636
Test Epoch25 layer1 Acc 0.7412, AUC 0.8159962892532349, avg_entr 0.1614261120557785, f1 0.7411999702453613
ep25_l1_test_time 0.3620164220000106
Test Epoch25 layer2 Acc 0.7536, AUC 0.8216626644134521, avg_entr 0.06415873020887375, f1 0.7535999417304993
ep25_l2_test_time 0.4718404789999795
Test Epoch25 layer3 Acc 0.7558, AUC 0.829587459564209, avg_entr 0.05310506746172905, f1 0.7557999491691589
ep25_l3_test_time 0.6214452469999969
Test Epoch25 layer4 Acc 0.7562, AUC 0.8343480825424194, avg_entr 0.05120677500963211, f1 0.7561999559402466
ep25_l4_test_time 0.8215426499999694
gc 0
Train Epoch26 Acc 0.96125 (38450/40000), AUC 0.9918183088302612
ep26_train_time 23.23606705499992
Test Epoch26 layer0 Acc 0.7032, AUC 0.7732359766960144, avg_entr 0.18129956722259521, f1 0.7031999826431274
ep26_l0_test_time 0.29043335500000467
Test Epoch26 layer1 Acc 0.7442, AUC 0.8154661059379578, avg_entr 0.15252888202667236, f1 0.7441999316215515
ep26_l1_test_time 0.36088089199995466
Test Epoch26 layer2 Acc 0.7552, AUC 0.8221689462661743, avg_entr 0.058882251381874084, f1 0.7552000284194946
ep26_l2_test_time 0.4784254230000897
Test Epoch26 layer3 Acc 0.7598, AUC 0.8297025561332703, avg_entr 0.04867559298872948, f1 0.7598000168800354
ep26_l3_test_time 0.6228299489999927
Test Epoch26 layer4 Acc 0.7604, AUC 0.8346526622772217, avg_entr 0.04550681263208389, f1 0.7603999972343445
ep26_l4_test_time 0.8177387029999181
gc 0
Train Epoch27 Acc 0.96245 (38498/40000), AUC 0.9918136596679688
ep27_train_time 23.17520922199992
Test Epoch27 layer0 Acc 0.7012, AUC 0.772020697593689, avg_entr 0.18160651624202728, f1 0.701200008392334
ep27_l0_test_time 0.29125635500008684
Test Epoch27 layer1 Acc 0.7438, AUC 0.8152408599853516, avg_entr 0.15431712567806244, f1 0.7437999248504639
ep27_l1_test_time 0.3625690049999548
Test Epoch27 layer2 Acc 0.7552, AUC 0.8234968185424805, avg_entr 0.060742150992155075, f1 0.7552000284194946
ep27_l2_test_time 0.478665527999965
Test Epoch27 layer3 Acc 0.763, AUC 0.8300835490226746, avg_entr 0.05058414861559868, f1 0.7630000114440918
ep27_l3_test_time 0.6222410850000415
Test Epoch27 layer4 Acc 0.7626, AUC 0.8352959752082825, avg_entr 0.04833770915865898, f1 0.7626000046730042
ep27_l4_test_time 0.8174580779999587
gc 0
Train Epoch28 Acc 0.962975 (38519/40000), AUC 0.9924240112304688
ep28_train_time 23.23901764599998
Test Epoch28 layer0 Acc 0.7, AUC 0.771816611289978, avg_entr 0.18148259818553925, f1 0.699999988079071
ep28_l0_test_time 0.28980304200001683
Test Epoch28 layer1 Acc 0.7396, AUC 0.8156791925430298, avg_entr 0.15351325273513794, f1 0.7396000027656555
ep28_l1_test_time 0.36169276500004344
Test Epoch28 layer2 Acc 0.7532, AUC 0.8212605714797974, avg_entr 0.05998648703098297, f1 0.7531999945640564
ep28_l2_test_time 0.4715736080000852
Test Epoch28 layer3 Acc 0.7596, AUC 0.8299961090087891, avg_entr 0.05320996046066284, f1 0.7595999836921692
ep28_l3_test_time 0.6216763260000562
Test Epoch28 layer4 Acc 0.7612, AUC 0.8343453407287598, avg_entr 0.05122828856110573, f1 0.7612000107765198
ep28_l4_test_time 0.8211203890000434
gc 0
Train Epoch29 Acc 0.964075 (38563/40000), AUC 0.9926292896270752
ep29_train_time 23.160796925
Test Epoch29 layer0 Acc 0.7018, AUC 0.7708468437194824, avg_entr 0.17926865816116333, f1 0.7017999887466431
ep29_l0_test_time 0.29068627499998456
Test Epoch29 layer1 Acc 0.7388, AUC 0.8141684532165527, avg_entr 0.14875681698322296, f1 0.7387999892234802
ep29_l1_test_time 0.360647467999911
Test Epoch29 layer2 Acc 0.748, AUC 0.8203250765800476, avg_entr 0.05814953148365021, f1 0.7480000257492065
ep29_l2_test_time 0.4705269049999288
Test Epoch29 layer3 Acc 0.7578, AUC 0.8292818069458008, avg_entr 0.050443463027477264, f1 0.7577999830245972
ep29_l3_test_time 0.6294421710000506
Test Epoch29 layer4 Acc 0.7592, AUC 0.8329777717590332, avg_entr 0.047904886305332184, f1 0.7591999769210815
ep29_l4_test_time 0.8182562599999983
gc 0
Train Epoch30 Acc 0.966825 (38673/40000), AUC 0.9932512044906616
ep30_train_time 23.211767622000025
Test Epoch30 layer0 Acc 0.7, AUC 0.7712065577507019, avg_entr 0.18056277930736542, f1 0.699999988079071
ep30_l0_test_time 0.29102441800000634
Test Epoch30 layer1 Acc 0.7426, AUC 0.8146100640296936, avg_entr 0.15285435318946838, f1 0.7426000237464905
ep30_l1_test_time 0.3621084040000824
Test Epoch30 layer2 Acc 0.7542, AUC 0.8196167945861816, avg_entr 0.05759109929203987, f1 0.7541999816894531
ep30_l2_test_time 0.4716420969999717
Test Epoch30 layer3 Acc 0.7556, AUC 0.8277406096458435, avg_entr 0.04424557834863663, f1 0.7555999755859375
ep30_l3_test_time 0.6297141200000169
Test Epoch30 layer4 Acc 0.7552, AUC 0.8330097198486328, avg_entr 0.04227396473288536, f1 0.7552000284194946
ep30_l4_test_time 0.818092211000021
gc 0
Train Epoch31 Acc 0.967 (38680/40000), AUC 0.993571400642395
ep31_train_time 23.172615070000006
Test Epoch31 layer0 Acc 0.6996, AUC 0.7708395719528198, avg_entr 0.17819370329380035, f1 0.6995999813079834
ep31_l0_test_time 0.2968181919999324
Test Epoch31 layer1 Acc 0.7384, AUC 0.814164400100708, avg_entr 0.14857466518878937, f1 0.7384000420570374
ep31_l1_test_time 0.36767808199999763
Test Epoch31 layer2 Acc 0.7522, AUC 0.8183627128601074, avg_entr 0.056790437549352646, f1 0.7522000074386597
ep31_l2_test_time 0.4716688290000093
Test Epoch31 layer3 Acc 0.7574, AUC 0.8262564539909363, avg_entr 0.04905427619814873, f1 0.7573999762535095
ep31_l3_test_time 0.6243402899999637
Test Epoch31 layer4 Acc 0.7574, AUC 0.8314986228942871, avg_entr 0.04691262170672417, f1 0.7573999762535095
ep31_l4_test_time 0.8184124120000433
gc 0
Train Epoch32 Acc 0.967725 (38709/40000), AUC 0.9938700199127197
ep32_train_time 23.1520256309999
Test Epoch32 layer0 Acc 0.7008, AUC 0.7709343433380127, avg_entr 0.17786236107349396, f1 0.7008000016212463
ep32_l0_test_time 0.2898162650000131
Test Epoch32 layer1 Acc 0.7386, AUC 0.8128547072410583, avg_entr 0.14899294078350067, f1 0.7386000156402588
ep32_l1_test_time 0.3626662660000193
Test Epoch32 layer2 Acc 0.7538, AUC 0.8167245388031006, avg_entr 0.05685264989733696, f1 0.7537999749183655
ep32_l2_test_time 0.4711939540000003
Test Epoch32 layer3 Acc 0.7558, AUC 0.8249766826629639, avg_entr 0.04358741641044617, f1 0.7557999491691589
ep32_l3_test_time 0.6210558000000219
Test Epoch32 layer4 Acc 0.7568, AUC 0.8311073780059814, avg_entr 0.042168568819761276, f1 0.7567999958992004
ep32_l4_test_time 0.821268305999979
gc 0
Train Epoch33 Acc 0.966675 (38667/40000), AUC 0.9939990043640137
ep33_train_time 23.171617445000038
Test Epoch33 layer0 Acc 0.7034, AUC 0.7708336114883423, avg_entr 0.17834176123142242, f1 0.7034000158309937
ep33_l0_test_time 0.2916711870000199
Test Epoch33 layer1 Acc 0.7354, AUC 0.8123250603675842, avg_entr 0.15068255364894867, f1 0.7354000210762024
ep33_l1_test_time 0.3621870620000891
Test Epoch33 layer2 Acc 0.7508, AUC 0.8163971304893494, avg_entr 0.057214513421058655, f1 0.7508000135421753
ep33_l2_test_time 0.4714995770000314
Test Epoch33 layer3 Acc 0.7522, AUC 0.8267500400543213, avg_entr 0.04407103732228279, f1 0.7522000074386597
ep33_l3_test_time 0.6219601870000133
Test Epoch33 layer4 Acc 0.7522, AUC 0.8306467533111572, avg_entr 0.041894301772117615, f1 0.7522000074386597
ep33_l4_test_time 0.8226175789999388
gc 0
Train Epoch34 Acc 0.9682 (38728/40000), AUC 0.9938106536865234
ep34_train_time 23.177394619000097
Test Epoch34 layer0 Acc 0.7016, AUC 0.770691990852356, avg_entr 0.17706117033958435, f1 0.7016000151634216
ep34_l0_test_time 0.29752316099995824
Test Epoch34 layer1 Acc 0.7362, AUC 0.8124227523803711, avg_entr 0.1484762728214264, f1 0.7361999750137329
ep34_l1_test_time 0.3674632690000408
Test Epoch34 layer2 Acc 0.7492, AUC 0.8156301975250244, avg_entr 0.055843427777290344, f1 0.7491999864578247
ep34_l2_test_time 0.4710698889999776
Test Epoch34 layer3 Acc 0.7528, AUC 0.8254250884056091, avg_entr 0.04077354446053505, f1 0.7528000473976135
ep34_l3_test_time 0.6228084230000377
Test Epoch34 layer4 Acc 0.751, AUC 0.8300114870071411, avg_entr 0.03864052891731262, f1 0.7509999871253967
ep34_l4_test_time 0.8181783469999573
gc 0
Train Epoch35 Acc 0.9679 (38716/40000), AUC 0.9937210083007812
ep35_train_time 23.157753397999954
Test Epoch35 layer0 Acc 0.7012, AUC 0.7705806493759155, avg_entr 0.17684690654277802, f1 0.701200008392334
ep35_l0_test_time 0.3064323769999646
Test Epoch35 layer1 Acc 0.7384, AUC 0.812312126159668, avg_entr 0.14485929906368256, f1 0.7384000420570374
ep35_l1_test_time 0.36109215299995867
Test Epoch35 layer2 Acc 0.75, AUC 0.8161230087280273, avg_entr 0.0557045117020607, f1 0.75
ep35_l2_test_time 0.47149734399999943
Test Epoch35 layer3 Acc 0.756, AUC 0.824547290802002, avg_entr 0.04546019807457924, f1 0.7559999227523804
ep35_l3_test_time 0.6227768450000895
Test Epoch35 layer4 Acc 0.7576, AUC 0.829502522945404, avg_entr 0.0426817461848259, f1 0.7576000690460205
ep35_l4_test_time 0.8177505090000068
gc 0
Train Epoch36 Acc 0.96725 (38690/40000), AUC 0.9940487146377563
ep36_train_time 23.157133988000055
Test Epoch36 layer0 Acc 0.701, AUC 0.7709119915962219, avg_entr 0.17678113281726837, f1 0.7009999752044678
ep36_l0_test_time 0.29218703499998355
Test Epoch36 layer1 Acc 0.7386, AUC 0.8130252957344055, avg_entr 0.14671282470226288, f1 0.7386000156402588
ep36_l1_test_time 0.37091957000006914
Test Epoch36 layer2 Acc 0.751, AUC 0.8167016506195068, avg_entr 0.05612951144576073, f1 0.7509999871253967
ep36_l2_test_time 0.47167325600003096
Test Epoch36 layer3 Acc 0.7546, AUC 0.826213538646698, avg_entr 0.04252619668841362, f1 0.7546000480651855
ep36_l3_test_time 0.6214107879999347
Test Epoch36 layer4 Acc 0.7556, AUC 0.8303654193878174, avg_entr 0.03989630937576294, f1 0.7555999755859375
ep36_l4_test_time 0.8225479230000019
gc 0
Train Epoch37 Acc 0.96855 (38742/40000), AUC 0.9942590594291687
ep37_train_time 23.17523336199997
Test Epoch37 layer0 Acc 0.702, AUC 0.7706148028373718, avg_entr 0.17648714780807495, f1 0.7020000219345093
ep37_l0_test_time 0.29350211399992077
Test Epoch37 layer1 Acc 0.737, AUC 0.8124521970748901, avg_entr 0.14592678844928741, f1 0.7369999885559082
ep37_l1_test_time 0.3613825010000937
Test Epoch37 layer2 Acc 0.751, AUC 0.8155245184898376, avg_entr 0.05545486509799957, f1 0.7509999871253967
ep37_l2_test_time 0.47158385199998065
Test Epoch37 layer3 Acc 0.754, AUC 0.8238691091537476, avg_entr 0.040904246270656586, f1 0.7540000081062317
ep37_l3_test_time 0.6228859020000073
Test Epoch37 layer4 Acc 0.7548, AUC 0.8291885852813721, avg_entr 0.03888363763689995, f1 0.754800021648407
ep37_l4_test_time 0.8164129250000087
gc 0
Train Epoch38 Acc 0.969125 (38765/40000), AUC 0.9943284392356873
ep38_train_time 23.153517223999984
Test Epoch38 layer0 Acc 0.702, AUC 0.770491361618042, avg_entr 0.1760566681623459, f1 0.7020000219345093
ep38_l0_test_time 0.3090928439999061
Test Epoch38 layer1 Acc 0.7378, AUC 0.8123347759246826, avg_entr 0.14598502218723297, f1 0.7378000020980835
ep38_l1_test_time 0.3691619090000131
Test Epoch38 layer2 Acc 0.7496, AUC 0.8149999380111694, avg_entr 0.0544777475297451, f1 0.7495999932289124
ep38_l2_test_time 0.47184761199991954
Test Epoch38 layer3 Acc 0.7542, AUC 0.8253946304321289, avg_entr 0.04056481644511223, f1 0.7541999816894531
ep38_l3_test_time 0.6220007849999547
Test Epoch38 layer4 Acc 0.7526, AUC 0.8298565745353699, avg_entr 0.03825574368238449, f1 0.7526000142097473
ep38_l4_test_time 0.8170789430000696
gc 0
Train Epoch39 Acc 0.96915 (38766/40000), AUC 0.9941370487213135
ep39_train_time 23.17038561200002
Test Epoch39 layer0 Acc 0.701, AUC 0.7704569101333618, avg_entr 0.17612773180007935, f1 0.7009999752044678
ep39_l0_test_time 0.29185553899992556
Test Epoch39 layer1 Acc 0.7382, AUC 0.8126490116119385, avg_entr 0.14345212280750275, f1 0.7382000684738159
ep39_l1_test_time 0.36041402000000744
Test Epoch39 layer2 Acc 0.753, AUC 0.8164238333702087, avg_entr 0.054493263363838196, f1 0.753000020980835
ep39_l2_test_time 0.4715285539998604
Test Epoch39 layer3 Acc 0.7566, AUC 0.8256571292877197, avg_entr 0.041518379002809525, f1 0.756600022315979
ep39_l3_test_time 0.6215274840001257
Test Epoch39 layer4 Acc 0.7574, AUC 0.8298335075378418, avg_entr 0.03957274183630943, f1 0.7573999762535095
ep39_l4_test_time 0.8240328629999567
gc 0
Train Epoch40 Acc 0.969225 (38769/40000), AUC 0.9942827820777893
ep40_train_time 23.1890687099999
Test Epoch40 layer0 Acc 0.7014, AUC 0.7705082893371582, avg_entr 0.17681396007537842, f1 0.7013999819755554
ep40_l0_test_time 0.2919287009999607
Test Epoch40 layer1 Acc 0.738, AUC 0.8126028776168823, avg_entr 0.1436513364315033, f1 0.7379999160766602
ep40_l1_test_time 0.3616188940000029
Test Epoch40 layer2 Acc 0.752, AUC 0.8164322972297668, avg_entr 0.054129213094711304, f1 0.7519999742507935
ep40_l2_test_time 0.4707851440000468
Test Epoch40 layer3 Acc 0.757, AUC 0.8251751661300659, avg_entr 0.04064491018652916, f1 0.7570000886917114
ep40_l3_test_time 0.6286393830000634
Test Epoch40 layer4 Acc 0.7574, AUC 0.8297291398048401, avg_entr 0.038444213569164276, f1 0.7573999762535095
ep40_l4_test_time 0.8222934140001144
gc 0
Train Epoch41 Acc 0.96935 (38774/40000), AUC 0.9940921068191528
ep41_train_time 23.10718259800001
Test Epoch41 layer0 Acc 0.6994, AUC 0.770480215549469, avg_entr 0.17606782913208008, f1 0.699400007724762
ep41_l0_test_time 0.2880598210001608
Test Epoch41 layer1 Acc 0.7378, AUC 0.8123512268066406, avg_entr 0.14310002326965332, f1 0.7378000020980835
ep41_l1_test_time 0.35797875999992357
Test Epoch41 layer2 Acc 0.7508, AUC 0.8162611722946167, avg_entr 0.0540827214717865, f1 0.7508000135421753
ep41_l2_test_time 0.46781440199993085
Test Epoch41 layer3 Acc 0.7554, AUC 0.8254275321960449, avg_entr 0.040078140795230865, f1 0.7554000020027161
ep41_l3_test_time 0.6209496930000569
Test Epoch41 layer4 Acc 0.7568, AUC 0.8296787738800049, avg_entr 0.037952084094285965, f1 0.7567999958992004
ep41_l4_test_time 0.8155590759999995
gc 0
Train Epoch42 Acc 0.96945 (38778/40000), AUC 0.9941391944885254
ep42_train_time 23.203610691999984
Test Epoch42 layer0 Acc 0.6998, AUC 0.7706073522567749, avg_entr 0.17596465349197388, f1 0.6998000144958496
ep42_l0_test_time 0.29210231100000783
Test Epoch42 layer1 Acc 0.7376, AUC 0.812622606754303, avg_entr 0.14335109293460846, f1 0.7376000285148621
ep42_l1_test_time 0.3613343550000536
Test Epoch42 layer2 Acc 0.7524, AUC 0.8165608644485474, avg_entr 0.05424016714096069, f1 0.7523999810218811
ep42_l2_test_time 0.4770576449998316
Test Epoch42 layer3 Acc 0.7556, AUC 0.8253700733184814, avg_entr 0.040532976388931274, f1 0.7555999755859375
ep42_l3_test_time 0.6278699399999823
Test Epoch42 layer4 Acc 0.7568, AUC 0.8297552466392517, avg_entr 0.03841155022382736, f1 0.7567999958992004
ep42_l4_test_time 0.8211057930000152
gc 0
Train Epoch43 Acc 0.97015 (38806/40000), AUC 0.9942494630813599
ep43_train_time 23.187219156999845
Test Epoch43 layer0 Acc 0.7012, AUC 0.7705357074737549, avg_entr 0.17597614228725433, f1 0.701200008392334
ep43_l0_test_time 0.2910368479999761
Test Epoch43 layer1 Acc 0.7378, AUC 0.8124337196350098, avg_entr 0.1429809331893921, f1 0.7378000020980835
ep43_l1_test_time 0.35655525400011356
Test Epoch43 layer2 Acc 0.7524, AUC 0.8163307905197144, avg_entr 0.0545666441321373, f1 0.7523999810218811
ep43_l2_test_time 0.47015143800012993
Test Epoch43 layer3 Acc 0.7556, AUC 0.8249915242195129, avg_entr 0.04084917530417442, f1 0.7555999755859375
ep43_l3_test_time 0.6270197999999709
Test Epoch43 layer4 Acc 0.7554, AUC 0.8294916152954102, avg_entr 0.038858793675899506, f1 0.7554000020027161
ep43_l4_test_time 0.8157061270001122
gc 0
Train Epoch44 Acc 0.9699 (38796/40000), AUC 0.9944629669189453
ep44_train_time 23.146192366999912
Test Epoch44 layer0 Acc 0.7004, AUC 0.770492672920227, avg_entr 0.17591844499111176, f1 0.7003999948501587
ep44_l0_test_time 0.2871292929999072
Test Epoch44 layer1 Acc 0.7368, AUC 0.8123412728309631, avg_entr 0.14228466153144836, f1 0.7368000149726868
ep44_l1_test_time 0.35468239199985874
Test Epoch44 layer2 Acc 0.7536, AUC 0.8161345720291138, avg_entr 0.05438455939292908, f1 0.7535999417304993
ep44_l2_test_time 0.4646562020000147
Test Epoch44 layer3 Acc 0.7568, AUC 0.824723482131958, avg_entr 0.04077615961432457, f1 0.7567999958992004
ep44_l3_test_time 0.6147521929999584
Test Epoch44 layer4 Acc 0.7578, AUC 0.829265832901001, avg_entr 0.03869975730776787, f1 0.7577999830245972
ep44_l4_test_time 0.810129771999982
gc 0
Train Epoch45 Acc 0.9696 (38784/40000), AUC 0.9941756725311279
ep45_train_time 23.037844777999908
Test Epoch45 layer0 Acc 0.6992, AUC 0.7705380916595459, avg_entr 0.17606140673160553, f1 0.6991999745368958
ep45_l0_test_time 0.2844619120000971
Test Epoch45 layer1 Acc 0.737, AUC 0.8120479583740234, avg_entr 0.14244434237480164, f1 0.7369999885559082
ep45_l1_test_time 0.35911538699997436
Test Epoch45 layer2 Acc 0.7518, AUC 0.8159518241882324, avg_entr 0.05433887988328934, f1 0.751800000667572
ep45_l2_test_time 0.47123071000009986
Test Epoch45 layer3 Acc 0.7558, AUC 0.8246287703514099, avg_entr 0.04030771926045418, f1 0.7557999491691589
ep45_l3_test_time 0.6223883449999903
Test Epoch45 layer4 Acc 0.7556, AUC 0.8291795253753662, avg_entr 0.03827464208006859, f1 0.7555999755859375
ep45_l4_test_time 0.8121849069998461
gc 0
Train Epoch46 Acc 0.969075 (38763/40000), AUC 0.994374692440033
ep46_train_time 23.05306039800007
Test Epoch46 layer0 Acc 0.6998, AUC 0.7705297470092773, avg_entr 0.17577655613422394, f1 0.6998000144958496
ep46_l0_test_time 0.2855245880000439
Test Epoch46 layer1 Acc 0.737, AUC 0.8121009469032288, avg_entr 0.1429630070924759, f1 0.7369999885559082
ep46_l1_test_time 0.3542757990001064
Test Epoch46 layer2 Acc 0.75, AUC 0.8158759474754333, avg_entr 0.0539838932454586, f1 0.75
ep46_l2_test_time 0.46406702400008726
Test Epoch46 layer3 Acc 0.7554, AUC 0.8247708678245544, avg_entr 0.03958356752991676, f1 0.7554000020027161
ep46_l3_test_time 0.6182742859998598
Test Epoch46 layer4 Acc 0.7556, AUC 0.8293781280517578, avg_entr 0.03751612454652786, f1 0.7555999755859375
ep46_l4_test_time 0.8128091700000368
gc 0
Train Epoch47 Acc 0.970525 (38821/40000), AUC 0.9944051504135132
ep47_train_time 23.18025241700002
Test Epoch47 layer0 Acc 0.6994, AUC 0.7704702615737915, avg_entr 0.1755686104297638, f1 0.699400007724762
ep47_l0_test_time 0.2864304889999403
Test Epoch47 layer1 Acc 0.737, AUC 0.8120864629745483, avg_entr 0.14249445497989655, f1 0.7369999885559082
ep47_l1_test_time 0.356425156999876
Test Epoch47 layer2 Acc 0.7512, AUC 0.8156003952026367, avg_entr 0.054304614663124084, f1 0.7512000203132629
ep47_l2_test_time 0.464426364000019
Test Epoch47 layer3 Acc 0.7548, AUC 0.8245289325714111, avg_entr 0.039987217634916306, f1 0.754800021648407
ep47_l3_test_time 0.614214727999979
Test Epoch47 layer4 Acc 0.756, AUC 0.8291628360748291, avg_entr 0.03793696314096451, f1 0.7559999227523804
ep47_l4_test_time 0.8091486160001296
gc 0
Train Epoch48 Acc 0.96885 (38754/40000), AUC 0.9943399429321289
ep48_train_time 23.09776289399997
Test Epoch48 layer0 Acc 0.6994, AUC 0.7704550623893738, avg_entr 0.17551547288894653, f1 0.699400007724762
ep48_l0_test_time 0.2855609010000535
Test Epoch48 layer1 Acc 0.7366, AUC 0.8119797706604004, avg_entr 0.14206628501415253, f1 0.7365999817848206
ep48_l1_test_time 0.35703158699993764
Test Epoch48 layer2 Acc 0.7502, AUC 0.8153438568115234, avg_entr 0.05424240604043007, f1 0.7501999735832214
ep48_l2_test_time 0.4694352940000499
Test Epoch48 layer3 Acc 0.7552, AUC 0.8246579170227051, avg_entr 0.04004288092255592, f1 0.7552000284194946
ep48_l3_test_time 0.6162266189999173
Test Epoch48 layer4 Acc 0.7556, AUC 0.8289336562156677, avg_entr 0.03801049664616585, f1 0.7555999755859375
ep48_l4_test_time 0.8097348860001148
gc 0
Train Epoch49 Acc 0.970325 (38813/40000), AUC 0.9945502877235413
ep49_train_time 23.061817636999876
Test Epoch49 layer0 Acc 0.699, AUC 0.7704492807388306, avg_entr 0.17541171610355377, f1 0.6990000009536743
ep49_l0_test_time 0.28615800399984437
Test Epoch49 layer1 Acc 0.7368, AUC 0.8118920922279358, avg_entr 0.1426265388727188, f1 0.7368000149726868
ep49_l1_test_time 0.3554904959999021
Test Epoch49 layer2 Acc 0.75, AUC 0.8152862191200256, avg_entr 0.05388673022389412, f1 0.75
ep49_l2_test_time 0.4643488609999622
Test Epoch49 layer3 Acc 0.755, AUC 0.8246172666549683, avg_entr 0.039590541273355484, f1 0.7549999356269836
ep49_l3_test_time 0.6142969429999994
Test Epoch49 layer4 Acc 0.7556, AUC 0.8291051387786865, avg_entr 0.03755151852965355, f1 0.7555999755859375
ep49_l4_test_time 0.8080315170000176
Best AUC tensor(0.7760) 10 4
train_as_loss [[8.49359750e+01 5.85497727e+01 5.20685667e+01 5.04697481e+01
  4.98941859e+01 4.96287492e+01 4.94859161e+01 4.94006976e+01
  4.93459564e+01 4.93087957e+01 4.92824666e+01 4.92631673e+01
  4.92486265e+01 4.92374179e+01 4.92286063e+01 4.92231168e+01
  4.92198559e+01 4.92168134e+01 4.92139864e+01 4.92119824e+01
  4.92106735e+01 4.92093702e+01 4.92080754e+01 4.92071091e+01
  4.92064493e+01 4.92057711e+01 4.92050750e+01 4.92045393e+01
  4.92041664e+01 4.92037753e+01 4.92033669e+01 4.92030471e+01
  4.92028221e+01 4.92025833e+01 4.92023301e+01 4.92021315e+01
  4.92019882e+01 4.92018381e+01 4.92016761e+01 4.92015506e+01
  4.92014580e+01 4.92013606e+01 4.92012574e+01 4.92011736e+01
  4.92011150e+01 4.92010537e+01 4.92009828e+01 4.92009295e+01
  4.92008919e+01 4.92008505e+01]
 [2.29728667e+00 4.02328002e-04 2.26909929e-05 6.35918366e-06
  2.63150358e-06 1.30704456e-06 6.85941380e-07 3.98581295e-07
  6.74998180e-07 1.56373318e-07 2.67052764e-07 6.17792256e-07
  4.82889991e-08 8.83384579e-07 1.88273432e-07 1.13219083e-06
  5.30365904e-07 2.48947322e-07 1.32977150e-08 2.98404010e-09
  2.25220748e-09 2.00495542e-09 7.55784033e-09 1.79958637e-09
  1.55672229e-09 1.45654996e-09 5.64881162e-09 1.36268377e-09
  1.22509624e-09 1.18225195e-09 4.71008048e-09 1.09095348e-09
  1.07115742e-09 1.00700515e-09 4.04422231e-09 9.64469103e-10
  9.28772992e-10 8.98959623e-10 3.50660227e-09 8.91725399e-10
  8.35845595e-10 8.12881568e-10 3.09681530e-09 8.64096221e-10
  7.72983046e-10 7.44534730e-10 2.72749904e-09 8.85561005e-10
  7.47448566e-10 7.12477414e-10]
 [2.55819416e+00 4.22785759e-04 2.37646480e-05 7.09759926e-06
  3.03201962e-06 1.54155450e-06 8.03582274e-07 4.65309452e-07
  2.74095426e-07 1.81048510e-07 1.15449649e-07 8.87767225e-08
  5.63202803e-08 6.21414124e-08 3.26408562e-08 6.02580836e-08
  3.68799492e-08 3.13162663e-08 1.35913076e-08 4.69916877e-09
  9.45924416e-09 7.99373724e-09 7.76053255e-09 2.36283483e-09
  6.42921009e-09 5.82688933e-09 5.55618500e-09 1.66032147e-09
  5.02492650e-09 4.65212434e-09 4.65508616e-09 1.20955250e-09
  4.28288201e-09 3.97375995e-09 3.95928201e-09 1.04518633e-09
  3.58660190e-09 3.53176829e-09 3.41016619e-09 9.62857399e-10
  3.18724280e-09 3.16800037e-09 3.06407927e-09 9.54323925e-10
  2.87456925e-09 3.01249646e-09 2.82781387e-09 1.01495844e-09
  2.70681005e-09 2.90930311e-09]
 [2.14452486e+00 5.18599980e-04 2.58605543e-05 8.34771323e-06
  3.67154832e-06 1.91795056e-06 9.49108705e-07 5.70653093e-07
  3.25195982e-07 2.34252525e-07 1.47054444e-07 1.17667366e-07
  8.18746661e-08 7.95536931e-08 4.61769806e-08 8.33930552e-08
  4.36881716e-08 3.34214493e-08 2.10971687e-08 9.80904766e-09
  1.20955990e-08 1.00156095e-08 1.04792823e-08 4.04941648e-09
  8.00745468e-09 7.15964909e-09 7.02250133e-09 2.52460826e-09
  6.12656743e-09 5.72301763e-09 5.82566814e-09 1.66700197e-09
  5.22962571e-09 4.89331912e-09 4.92162580e-09 1.39653355e-09
  4.41782244e-09 4.39407366e-09 4.26417936e-09 1.24903618e-09
  3.89416304e-09 4.02306210e-09 3.87989952e-09 1.23867432e-09
  3.48093494e-09 3.74358213e-09 3.50461344e-09 1.29992028e-09
  3.17989458e-09 3.67326893e-09]
 [2.31962118e+00 1.02930169e-03 2.05932528e-05 5.94221520e-06
  2.82979439e-06 1.65079032e-06 7.06772108e-07 5.12179767e-07
  2.55249031e-07 2.63897282e-07 1.57253970e-07 1.60421440e-07
  1.18742594e-07 1.13526926e-07 6.85608356e-08 1.86951106e-07
  4.39315966e-08 3.66423953e-08 3.29833109e-08 2.47429392e-08
  1.04678786e-08 8.26639744e-09 1.16064271e-08 6.98756860e-09
  6.12906932e-09 5.18906386e-09 5.61890934e-09 3.33338765e-09
  4.15085722e-09 4.02769027e-09 4.42792377e-09 1.60573439e-09
  3.46654902e-09 3.18839440e-09 3.41175743e-09 1.14078121e-09
  2.86058412e-09 2.85687455e-09 2.78195204e-09 9.41182842e-10
  2.54865795e-09 2.65203296e-09 2.54966950e-09 9.37361499e-10
  2.38839814e-09 2.64653942e-09 2.44774273e-09 1.04970772e-09
  2.34777633e-09 2.76518820e-09]]
train_ae_loss [[4.54884466 3.29081993 4.53096214 5.23074035 5.55198079 5.86098774
  5.99400818 6.01938812 6.19719302 6.12340473 6.04009044 5.94125855
  5.76545506 5.67228445 5.48418752 4.98710853 4.75802676 4.62303067
  4.4517321  4.24592528 4.13278325 4.0887071  4.0050138  3.88508457
  3.86506591 3.81136674 3.77274432 3.73451934 3.70601243 3.69667765
  3.70935207 3.67655803 3.64856801 3.65849226 3.65522976 3.63506098
  3.61306381 3.63767667 3.61473186 3.63029322 3.63401363 3.61768797
  3.62598844 3.64100073 3.62400469 3.62002811 3.61446603 3.62367838
  3.61107386 3.61972189]
 [3.57178309 3.27043881 4.57772144 5.2010011  5.39387091 5.6022987
  5.57608049 5.45123327 5.53396013 5.43001392 5.24649062 5.08572907
  4.91738326 4.78976828 4.57426677 4.28462953 3.9909955  3.78856155
  3.43639288 3.35360407 3.22957779 3.14043337 2.93158893 2.83012822
  2.80730641 2.75296474 2.62651748 2.58978206 2.57153424 2.55212296
  2.48585931 2.47000811 2.44138777 2.43434239 2.41331276 2.38918334
  2.35578167 2.37587048 2.35200878 2.37258533 2.35854587 2.34152775
  2.34703311 2.34991371 2.3332774  2.3190488  2.33052016 2.33624627
  2.31539858 2.32075338]
 [3.73429837 3.07971071 4.36349597 4.96339679 5.00673984 5.1260722
  5.01784552 4.80630714 4.83985638 4.72596494 4.49430328 4.31176655
  4.13708224 3.98963096 3.65786334 3.52709859 2.86132776 2.63006824
  2.24744464 2.22033376 1.98823439 1.88317365 1.66176675 1.59815357
  1.54434099 1.49788108 1.3748406  1.3624326  1.31778946 1.31193558
  1.25294726 1.25055646 1.21554195 1.21894078 1.18891812 1.18063241
  1.16213781 1.16844888 1.15479024 1.17068951 1.14646146 1.1593195
  1.17157544 1.15811246 1.12945625 1.13662361 1.13172973 1.14531612
  1.13412929 1.13747893]
 [4.04114567 2.99713519 4.28338074 5.02831876 5.09820296 5.15222635
  5.00421768 4.65195638 4.66270563 4.54778387 4.19177064 3.97280487
  3.75606314 3.60621703 3.34419439 3.37710927 2.58517791 2.34723638
  1.98646457 2.00871723 1.75273259 1.65082423 1.42662378 1.38407828
  1.32512906 1.28190059 1.16011214 1.15408325 1.09924845 1.09717579
  1.04352896 1.04757871 1.01428905 1.00304776 0.98080484 0.96512731
  0.96108329 0.96831841 0.94322052 0.96570053 0.93877282 0.94647365
  0.95749258 0.95429274 0.91462943 0.92347541 0.92322681 0.94300701
  0.92271944 0.94337729]
 [4.12926392 2.96453671 4.23466139 5.13919839 5.35737475 5.31052021
  5.13717739 4.65296965 4.60980149 4.50907102 4.04666015 3.83217419
  3.63431296 3.49845842 3.24274701 3.32924572 2.4766694  2.24410747
  1.8981234  1.93150454 1.67123446 1.57377923 1.35290534 1.31909353
  1.2605158  1.21715208 1.10081924 1.09450161 1.04187633 1.03836026
  0.98834831 0.99222033 0.9585851  0.94938145 0.92659515 0.91216124
  0.90794402 0.91587255 0.89240257 0.9123407  0.88849198 0.89484236
  0.90528761 0.90287162 0.86346835 0.87346335 0.87269616 0.89132391
  0.87253689 0.89183166]]
valid_acc (5, 50)
valid_AUC (5, 50)
train_acc (50,)
total_train+valid_time 1291.0652379439998
Start Testing
Load ckpt at ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt
gc 9
Test layer0 Acc 0.722, AUC 0.802943766117096, avg_entr 0.2992493212223053, f1 0.722000002861023
l0_test_time 0.2840882339999098
gc 0
Test layer1 Acc 0.7532, AUC 0.8369748592376709, avg_entr 0.2522442042827606, f1 0.7531999945640564
l1_test_time 0.35682808699993984
gc 0
Test layer2 Acc 0.7692, AUC 0.8581076860427856, avg_entr 0.22829970717430115, f1 0.7692000269889832
l2_test_time 0.46560722699996404
gc 0
Test layer3 Acc 0.7774, AUC 0.8621807098388672, avg_entr 0.19962629675865173, f1 0.777400016784668
l3_test_time 0.6146925620000729
gc 0
Test layer4 Acc 0.7796, AUC 0.8629270195960999, avg_entr 0.1731812208890915, f1 0.7796000838279724
l4_test_time 0.811600320000025
gc 0
Test threshold 0.1 Acc 0.78, AUC 0.857401430606842, avg_entr 0.24142616987228394, f1 0.7799999117851257
t0.1_test_time 0.6638265870001305
gc 0
Test threshold 0.2 Acc 0.7766, AUC 0.8480443954467773, avg_entr 0.2448737621307373, f1 0.7766000032424927
t0.2_test_time 0.589555990000008
gc 0
Test threshold 0.3 Acc 0.7742, AUC 0.8422945737838745, avg_entr 0.2541597783565521, f1 0.7742000222206116
t0.3_test_time 0.5412414670001908
gc 0
Test threshold 0.4 Acc 0.7708, AUC 0.8382967710494995, avg_entr 0.26505246758461, f1 0.7707999348640442
t0.4_test_time 0.5029419840000173
gc 0
Test threshold 0.5 Acc 0.7692, AUC 0.8323243260383606, avg_entr 0.2784714698791504, f1 0.7692000269889832
t0.5_test_time 0.46844454200004293
gc 0
Test threshold 0.6 Acc 0.7654, AUC 0.8275302052497864, avg_entr 0.29367300868034363, f1 0.7653999924659729
t0.6_test_time 0.4356406590000006
gc 0
Test threshold 0.7 Acc 0.7628, AUC 0.8216724991798401, avg_entr 0.3124189078807831, f1 0.7627999782562256
t0.7_test_time 0.41356415000018387
gc 0
Test threshold 0.8 Acc 0.7552, AUC 0.8191301822662354, avg_entr 0.33791297674179077, f1 0.7552000284194946
t0.8_test_time 0.38342818199998874
gc 0
Test threshold 0.9 Acc 0.746, AUC 0.8126333355903625, avg_entr 0.36786144971847534, f1 0.7459999918937683
t0.9_test_time 0.3643829459999779
