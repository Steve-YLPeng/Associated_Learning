total count words 222751
vocab size 30000
train size 40000, valid size 5000, test size 5000
found 27937 words in glove
model: TransformerALsideText(
  (layers): ModuleList(
    (0): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=2, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=2, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
  (emb_layers): ModuleList(
    (0): Embedding(30000, 300)
    (1): Embedding(30000, 300)
    (2): Embedding(30000, 300)
    (3): Embedding(30000, 300)
    (4): Embedding(30000, 300)
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.multi_headed_attention.linears.0.weight 90000
layers.0.enc.f.multi_headed_attention.linears.0.bias 300
layers.0.enc.f.multi_headed_attention.linears.1.weight 90000
layers.0.enc.f.multi_headed_attention.linears.1.bias 300
layers.0.enc.f.multi_headed_attention.linears.2.weight 90000
layers.0.enc.f.multi_headed_attention.linears.2.bias 300
layers.0.enc.f.multi_headed_attention.linears.3.weight 90000
layers.0.enc.f.multi_headed_attention.linears.3.bias 300
layers.0.enc.f.feed_forward.w_1.weight 90000
layers.0.enc.f.feed_forward.w_1.bias 300
layers.0.enc.f.feed_forward.w_2.weight 90000
layers.0.enc.f.feed_forward.w_2.bias 300
layers.0.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.0.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.0.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.0.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.0.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.0.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.0.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.0.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.0.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.0.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.0.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.0.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.0.enc.f.encoder.norm.a 300
layers.0.enc.f.encoder.norm.b 300
layers.0.ae.g.0.weight 256
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 256
layers.0.ae.h.0.bias 2
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
emb_layers.0.weight 9000000
emb_layers.1.weight 9000000
emb_layers.2.weight 9000000
emb_layers.3.weight 9000000
emb_layers.4.weight 9000000
Total Trainable Params: 50758378
init_time 21.979560319
Start Training
gc 0
Train Epoch0 Acc 0.502675 (20107/40000), AUC 0.5018227696418762
ep0_train_time 23.367247646000003
Test Epoch0 layer0 Acc 0.5052, AUC 0.586448609828949, avg_entr 0.6875755190849304, f1 0.5052000284194946
ep0_l0_test_time 0.28614634299999864
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 0
Test Epoch0 layer1 Acc 0.5016, AUC 0.5702624320983887, avg_entr 0.6844995021820068, f1 0.5016000270843506
ep0_l1_test_time 0.3561649179999975
Test Epoch0 layer2 Acc 0.5028, AUC 0.5436061024665833, avg_entr 0.6912087202072144, f1 0.5027999877929688
ep0_l2_test_time 0.46542178500000375
Test Epoch0 layer3 Acc 0.5, AUC 0.5016902089118958, avg_entr 0.6878650188446045, f1 0.5
ep0_l3_test_time 0.6170973700000033
Test Epoch0 layer4 Acc 0.5, AUC 0.5125143527984619, avg_entr 0.6868530511856079, f1 0.5
ep0_l4_test_time 0.8112064970000006
gc 0
Train Epoch1 Acc 0.5131 (20524/40000), AUC 0.519991397857666
ep1_train_time 23.165526943999993
Test Epoch1 layer0 Acc 0.6148, AUC 0.6710492968559265, avg_entr 0.6321378350257874, f1 0.614799976348877
ep1_l0_test_time 0.28732572099998777
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 1
Test Epoch1 layer1 Acc 0.5692, AUC 0.683475136756897, avg_entr 0.5961287617683411, f1 0.5691999793052673
ep1_l1_test_time 0.3573285899999945
Test Epoch1 layer2 Acc 0.624, AUC 0.6821295022964478, avg_entr 0.6515913605690002, f1 0.6240000128746033
ep1_l2_test_time 0.4650117909999949
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 1
Test Epoch1 layer3 Acc 0.612, AUC 0.6588551998138428, avg_entr 0.6940047740936279, f1 0.6119999885559082
ep1_l3_test_time 0.6165471179999997
Test Epoch1 layer4 Acc 0.5258, AUC 0.6099302172660828, avg_entr 0.6866751909255981, f1 0.5257999897003174
ep1_l4_test_time 0.8113785320000062
gc 0
Train Epoch2 Acc 0.578525 (23141/40000), AUC 0.6125904321670532
ep2_train_time 23.047117899
Test Epoch2 layer0 Acc 0.5956, AUC 0.7142136096954346, avg_entr 0.4867653250694275, f1 0.5956000089645386
ep2_l0_test_time 0.2890774499999935
Test Epoch2 layer1 Acc 0.5742, AUC 0.7365081310272217, avg_entr 0.41128847002983093, f1 0.5741999745368958
ep2_l1_test_time 0.3576329989999891
Test Epoch2 layer2 Acc 0.5652, AUC 0.7225514054298401, avg_entr 0.36962783336639404, f1 0.5651999711990356
ep2_l2_test_time 0.46759108899999546
Test Epoch2 layer3 Acc 0.5474, AUC 0.7219773530960083, avg_entr 0.38774073123931885, f1 0.5473999977111816
ep2_l3_test_time 0.6184891590000063
Test Epoch2 layer4 Acc 0.5182, AUC 0.71992027759552, avg_entr 0.3808339834213257, f1 0.5181999802589417
ep2_l4_test_time 0.8131430170000016
gc 0
Train Epoch3 Acc 0.626 (25040/40000), AUC 0.6755260229110718
ep3_train_time 23.02551018300001
Test Epoch3 layer0 Acc 0.6692, AUC 0.738622784614563, avg_entr 0.48822709918022156, f1 0.6692000031471252
ep3_l0_test_time 0.289492641999999
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 3
Test Epoch3 layer1 Acc 0.6924, AUC 0.7637519240379333, avg_entr 0.4769895076751709, f1 0.6923999786376953
ep3_l1_test_time 0.35735767500000293
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 3
Test Epoch3 layer2 Acc 0.6786, AUC 0.7575379014015198, avg_entr 0.4845052659511566, f1 0.678600013256073
ep3_l2_test_time 0.4694717790000027
Test Epoch3 layer3 Acc 0.6726, AUC 0.7555153965950012, avg_entr 0.5147469639778137, f1 0.6725999712944031
ep3_l3_test_time 0.6187155789999963
Test Epoch3 layer4 Acc 0.6562, AUC 0.743288516998291, avg_entr 0.5252801775932312, f1 0.6561999917030334
ep3_l4_test_time 0.815679633000002
gc 0
Train Epoch4 Acc 0.6693 (26772/40000), AUC 0.7340558767318726
ep4_train_time 23.236738658999982
Test Epoch4 layer0 Acc 0.6826, AUC 0.7544914484024048, avg_entr 0.4331090450286865, f1 0.6826000213623047
ep4_l0_test_time 0.29110213600000634
Test Epoch4 layer1 Acc 0.7004, AUC 0.7842261791229248, avg_entr 0.403810054063797, f1 0.7003999948501587
ep4_l1_test_time 0.35733657899999116
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 4
Test Epoch4 layer2 Acc 0.7064, AUC 0.7845008373260498, avg_entr 0.3979191184043884, f1 0.7063999772071838
ep4_l2_test_time 0.47041494399999806
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 4
Test Epoch4 layer3 Acc 0.709, AUC 0.7858095765113831, avg_entr 0.39154064655303955, f1 0.7090000510215759
ep4_l3_test_time 0.617085354000011
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 4
Test Epoch4 layer4 Acc 0.7064, AUC 0.7829773426055908, avg_entr 0.4093388617038727, f1 0.7063999772071838
ep4_l4_test_time 0.8139334939999969
gc 0
Train Epoch5 Acc 0.670925 (26837/40000), AUC 0.7328786849975586
ep5_train_time 23.042508423000015
Test Epoch5 layer0 Acc 0.695, AUC 0.7657387256622314, avg_entr 0.3954307734966278, f1 0.6949999928474426
ep5_l0_test_time 0.28819853699999953
Test Epoch5 layer1 Acc 0.7228, AUC 0.8021600246429443, avg_entr 0.3730289340019226, f1 0.7227999567985535
ep5_l1_test_time 0.3623304879999978
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 5
Test Epoch5 layer2 Acc 0.7296, AUC 0.8078978657722473, avg_entr 0.38318032026290894, f1 0.7295999526977539
ep5_l2_test_time 0.46773057199999357
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 5
Test Epoch5 layer3 Acc 0.7352, AUC 0.8108144998550415, avg_entr 0.4086044728755951, f1 0.7351999282836914
ep5_l3_test_time 0.6180972159999953
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 5
Test Epoch5 layer4 Acc 0.732, AUC 0.811238169670105, avg_entr 0.437640517950058, f1 0.7319999933242798
ep5_l4_test_time 0.8122956969999962
gc 0
Train Epoch6 Acc 0.72165 (28866/40000), AUC 0.7940787076950073
ep6_train_time 23.091804923000012
Test Epoch6 layer0 Acc 0.7036, AUC 0.7768799066543579, avg_entr 0.369245320558548, f1 0.7035999894142151
ep6_l0_test_time 0.2875433389999955
Test Epoch6 layer1 Acc 0.7296, AUC 0.8170773386955261, avg_entr 0.320637047290802, f1 0.7295999526977539
ep6_l1_test_time 0.3587128319999806
Test Epoch6 layer2 Acc 0.7356, AUC 0.828281044960022, avg_entr 0.31228360533714294, f1 0.7355999946594238
ep6_l2_test_time 0.4671341529999893
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 6
Test Epoch6 layer3 Acc 0.7214, AUC 0.8310065269470215, avg_entr 0.30467984080314636, f1 0.7214000225067139
ep6_l3_test_time 0.6200774200000012
Test Epoch6 layer4 Acc 0.6968, AUC 0.831016480922699, avg_entr 0.29201528429985046, f1 0.6967999935150146
ep6_l4_test_time 0.8254486440000051
gc 0
Train Epoch7 Acc 0.752975 (30119/40000), AUC 0.8302096128463745
ep7_train_time 23.11737315099998
Test Epoch7 layer0 Acc 0.7006, AUC 0.7814856767654419, avg_entr 0.31313109397888184, f1 0.7006000280380249
ep7_l0_test_time 0.28817235700000765
Test Epoch7 layer1 Acc 0.737, AUC 0.8236134648323059, avg_entr 0.27186697721481323, f1 0.7369999885559082
ep7_l1_test_time 0.35562233600001036
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 7
Test Epoch7 layer2 Acc 0.7396, AUC 0.8329988718032837, avg_entr 0.25793129205703735, f1 0.7396000027656555
ep7_l2_test_time 0.4662391310000089
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 7
Test Epoch7 layer3 Acc 0.7448, AUC 0.8341920375823975, avg_entr 0.27184513211250305, f1 0.7447999715805054
ep7_l3_test_time 0.6177809440000033
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 7
Test Epoch7 layer4 Acc 0.742, AUC 0.8336122035980225, avg_entr 0.29593685269355774, f1 0.7419999837875366
ep7_l4_test_time 0.8131154890000118
gc 0
Train Epoch8 Acc 0.78335 (31334/40000), AUC 0.862073540687561
ep8_train_time 23.13418925000002
Test Epoch8 layer0 Acc 0.667, AUC 0.7908626794815063, avg_entr 0.2916935682296753, f1 0.6669999957084656
ep8_l0_test_time 0.28665916200003494
Test Epoch8 layer1 Acc 0.696, AUC 0.8277249336242676, avg_entr 0.25693559646606445, f1 0.6959999799728394
ep8_l1_test_time 0.35708633999996664
Test Epoch8 layer2 Acc 0.6978, AUC 0.8421007990837097, avg_entr 0.24116604030132294, f1 0.6977999806404114
ep8_l2_test_time 0.46594152400001576
Test Epoch8 layer3 Acc 0.6984, AUC 0.8435908555984497, avg_entr 0.24035021662712097, f1 0.6984000205993652
ep8_l3_test_time 0.6174134620000018
Test Epoch8 layer4 Acc 0.7164, AUC 0.8452955484390259, avg_entr 0.26699310541152954, f1 0.7164000272750854
ep8_l4_test_time 0.8112399359999927
gc 0
Train Epoch9 Acc 0.80425 (32170/40000), AUC 0.886205792427063
ep9_train_time 23.032118948000004
Test Epoch9 layer0 Acc 0.7196, AUC 0.7933339476585388, avg_entr 0.300074964761734, f1 0.7196000218391418
ep9_l0_test_time 0.2870935090000444
Test Epoch9 layer1 Acc 0.7518, AUC 0.8334630727767944, avg_entr 0.27321380376815796, f1 0.751800000667572
ep9_l1_test_time 0.35598174599999766
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 9
Test Epoch9 layer2 Acc 0.762, AUC 0.8495434522628784, avg_entr 0.2609110474586487, f1 0.7619999647140503
ep9_l2_test_time 0.46968963699998767
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 9
Test Epoch9 layer3 Acc 0.7688, AUC 0.8543526530265808, avg_entr 0.26738691329956055, f1 0.7688000202178955
ep9_l3_test_time 0.6154637189999903
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 9
Test Epoch9 layer4 Acc 0.769, AUC 0.8564387559890747, avg_entr 0.2257881611585617, f1 0.7689999938011169
ep9_l4_test_time 0.8110829649999687
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 9
gc 0
Train Epoch10 Acc 0.825675 (33027/40000), AUC 0.9017921686172485
ep10_train_time 23.058624434000023
Test Epoch10 layer0 Acc 0.7186, AUC 0.7927291393280029, avg_entr 0.28197115659713745, f1 0.7185999751091003
ep10_l0_test_time 0.2860789240000372
Test Epoch10 layer1 Acc 0.7498, AUC 0.8307468891143799, avg_entr 0.22921565175056458, f1 0.7498000264167786
ep10_l1_test_time 0.35557471499998883
Test Epoch10 layer2 Acc 0.7638, AUC 0.8479441404342651, avg_entr 0.2007061094045639, f1 0.7638000249862671
ep10_l2_test_time 0.4678688479999664
Test Epoch10 layer3 Acc 0.7594, AUC 0.8511308431625366, avg_entr 0.18592344224452972, f1 0.759399950504303
ep10_l3_test_time 0.6169089979999853
Test Epoch10 layer4 Acc 0.7538, AUC 0.8544420003890991, avg_entr 0.14720429480075836, f1 0.7537999749183655
ep10_l4_test_time 0.8129118950000134
gc 0
Train Epoch11 Acc 0.852375 (34095/40000), AUC 0.9259823560714722
ep11_train_time 23.033640317999982
Test Epoch11 layer0 Acc 0.7128, AUC 0.7867751121520996, avg_entr 0.2638317048549652, f1 0.7128000259399414
ep11_l0_test_time 0.2877523349999933
Test Epoch11 layer1 Acc 0.7508, AUC 0.8255841135978699, avg_entr 0.22373135387897491, f1 0.7508000135421753
ep11_l1_test_time 0.3571144929999832
Test Epoch11 layer2 Acc 0.7658, AUC 0.8422642946243286, avg_entr 0.18129490315914154, f1 0.7657999992370605
ep11_l2_test_time 0.4677798829999915
Test Epoch11 layer3 Acc 0.7674, AUC 0.84495609998703, avg_entr 0.16022534668445587, f1 0.7673999667167664
ep11_l3_test_time 0.6179338709999911
Test Epoch11 layer4 Acc 0.768, AUC 0.8490157723426819, avg_entr 0.13290686905384064, f1 0.7680000066757202
ep11_l4_test_time 0.812916548999965
gc 0
Train Epoch12 Acc 0.876 (35040/40000), AUC 0.9439589381217957
ep12_train_time 23.063448807999976
Test Epoch12 layer0 Acc 0.7134, AUC 0.7831441164016724, avg_entr 0.23614118993282318, f1 0.7134000062942505
ep12_l0_test_time 0.29338811499997064
Test Epoch12 layer1 Acc 0.7518, AUC 0.827058732509613, avg_entr 0.20302827656269073, f1 0.751800000667572
ep12_l1_test_time 0.35624200799998107
Test Epoch12 layer2 Acc 0.7588, AUC 0.8444032073020935, avg_entr 0.15731456875801086, f1 0.7588000297546387
ep12_l2_test_time 0.4654104029999644
Test Epoch12 layer3 Acc 0.7626, AUC 0.8510768413543701, avg_entr 0.14249876141548157, f1 0.7626000046730042
ep12_l3_test_time 0.6157499239999993
Test Epoch12 layer4 Acc 0.7606, AUC 0.8540462851524353, avg_entr 0.12011848390102386, f1 0.7605999708175659
ep12_l4_test_time 0.810367929999984
gc 0
Train Epoch13 Acc 0.88425 (35370/40000), AUC 0.948415994644165
ep13_train_time 23.156036991000008
Test Epoch13 layer0 Acc 0.704, AUC 0.7791249752044678, avg_entr 0.23669013381004333, f1 0.7039999961853027
ep13_l0_test_time 0.2880889459999594
Test Epoch13 layer1 Acc 0.7522, AUC 0.8237436413764954, avg_entr 0.1963573843240738, f1 0.7522000074386597
ep13_l1_test_time 0.36140552900002376
Test Epoch13 layer2 Acc 0.764, AUC 0.8416719436645508, avg_entr 0.13085417449474335, f1 0.7639999985694885
ep13_l2_test_time 0.47052992399994764
Test Epoch13 layer3 Acc 0.764, AUC 0.8484823107719421, avg_entr 0.1202550083398819, f1 0.7639999985694885
ep13_l3_test_time 0.6287936360000117
Test Epoch13 layer4 Acc 0.769, AUC 0.8519701957702637, avg_entr 0.1125258281826973, f1 0.7689999938011169
ep13_l4_test_time 0.8178166800000213
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 13
gc 0
Train Epoch14 Acc 0.911275 (36451/40000), AUC 0.9683716297149658
ep14_train_time 23.127234482000006
Test Epoch14 layer0 Acc 0.6956, AUC 0.7779281139373779, avg_entr 0.2224942445755005, f1 0.6955999732017517
ep14_l0_test_time 0.29392028999996
Test Epoch14 layer1 Acc 0.73, AUC 0.815937340259552, avg_entr 0.17705047130584717, f1 0.7300000190734863
ep14_l1_test_time 0.35824722900002826
Test Epoch14 layer2 Acc 0.7404, AUC 0.8320878744125366, avg_entr 0.0980946496129036, f1 0.7404000163078308
ep14_l2_test_time 0.46707477000001063
Test Epoch14 layer3 Acc 0.7462, AUC 0.8406488299369812, avg_entr 0.08364257961511612, f1 0.7462000250816345
ep14_l3_test_time 0.6174048540000285
Test Epoch14 layer4 Acc 0.7508, AUC 0.842880368232727, avg_entr 0.08316091448068619, f1 0.7508000135421753
ep14_l4_test_time 0.8131975139999668
gc 0
Train Epoch15 Acc 0.921125 (36845/40000), AUC 0.9742263555526733
ep15_train_time 23.050041495000016
Test Epoch15 layer0 Acc 0.6992, AUC 0.774066686630249, avg_entr 0.21169926226139069, f1 0.6991999745368958
ep15_l0_test_time 0.2861707929999966
Test Epoch15 layer1 Acc 0.7198, AUC 0.8133593797683716, avg_entr 0.1603451371192932, f1 0.7197999358177185
ep15_l1_test_time 0.35633990000002314
Test Epoch15 layer2 Acc 0.7244, AUC 0.8334246873855591, avg_entr 0.0893230065703392, f1 0.7244000434875488
ep15_l2_test_time 0.4649469280000176
Test Epoch15 layer3 Acc 0.7274, AUC 0.8406141996383667, avg_entr 0.07976292073726654, f1 0.7274000644683838
ep15_l3_test_time 0.6161133699999937
Test Epoch15 layer4 Acc 0.7284, AUC 0.8410129547119141, avg_entr 0.0809030830860138, f1 0.7284000515937805
ep15_l4_test_time 0.8133107609999684
gc 0
Train Epoch16 Acc 0.9237 (36948/40000), AUC 0.9757657647132874
ep16_train_time 23.046234712
Test Epoch16 layer0 Acc 0.7, AUC 0.7716302871704102, avg_entr 0.20734915137290955, f1 0.699999988079071
ep16_l0_test_time 0.28568517900004053
Test Epoch16 layer1 Acc 0.7238, AUC 0.8054194450378418, avg_entr 0.13574256002902985, f1 0.723800003528595
ep16_l1_test_time 0.3575280740000153
Test Epoch16 layer2 Acc 0.7494, AUC 0.8275442719459534, avg_entr 0.08925311267375946, f1 0.7494000196456909
ep16_l2_test_time 0.4659495740000352
Test Epoch16 layer3 Acc 0.7526, AUC 0.8368352651596069, avg_entr 0.07943616807460785, f1 0.7526000142097473
ep16_l3_test_time 0.6169445269999869
Test Epoch16 layer4 Acc 0.7552, AUC 0.8381502628326416, avg_entr 0.07782997936010361, f1 0.7552000284194946
ep16_l4_test_time 0.8119265000000269
gc 0
Train Epoch17 Acc 0.930575 (37223/40000), AUC 0.9788504838943481
ep17_train_time 23.100891093999962
Test Epoch17 layer0 Acc 0.704, AUC 0.7731720209121704, avg_entr 0.20987947285175323, f1 0.7039999961853027
ep17_l0_test_time 0.2893275419999668
Test Epoch17 layer1 Acc 0.7274, AUC 0.807401180267334, avg_entr 0.1140286847949028, f1 0.7274000644683838
ep17_l1_test_time 0.35964269599998033
Test Epoch17 layer2 Acc 0.7522, AUC 0.8279359340667725, avg_entr 0.08091896772384644, f1 0.7522000074386597
ep17_l2_test_time 0.46629310600002327
Test Epoch17 layer3 Acc 0.757, AUC 0.8361194133758545, avg_entr 0.07343724370002747, f1 0.7570000886917114
ep17_l3_test_time 0.6171019639999713
Test Epoch17 layer4 Acc 0.7574, AUC 0.8369367122650146, avg_entr 0.07257938385009766, f1 0.7573999762535095
ep17_l4_test_time 0.8099745970000072
gc 0
Train Epoch18 Acc 0.942875 (37715/40000), AUC 0.9849823713302612
ep18_train_time 23.08711790600006
Test Epoch18 layer0 Acc 0.7014, AUC 0.7711658477783203, avg_entr 0.19990409910678864, f1 0.7013999819755554
ep18_l0_test_time 0.28585466700008055
Test Epoch18 layer1 Acc 0.721, AUC 0.8029379844665527, avg_entr 0.09964588284492493, f1 0.7210000157356262
ep18_l1_test_time 0.35598831300001166
Test Epoch18 layer2 Acc 0.7424, AUC 0.823114275932312, avg_entr 0.072174072265625, f1 0.742400050163269
ep18_l2_test_time 0.466173896999976
Test Epoch18 layer3 Acc 0.7494, AUC 0.8333075046539307, avg_entr 0.06479112058877945, f1 0.7494000196456909
ep18_l3_test_time 0.6195057700000461
Test Epoch18 layer4 Acc 0.7532, AUC 0.833253026008606, avg_entr 0.06326273083686829, f1 0.7531999945640564
ep18_l4_test_time 0.8097355620000144
gc 0
Train Epoch19 Acc 0.947 (37880/40000), AUC 0.9858272075653076
ep19_train_time 23.09214931100007
Test Epoch19 layer0 Acc 0.6994, AUC 0.7692974209785461, avg_entr 0.19511914253234863, f1 0.699400007724762
ep19_l0_test_time 0.28707302099996923
Test Epoch19 layer1 Acc 0.7142, AUC 0.7992057800292969, avg_entr 0.09933067858219147, f1 0.7142000198364258
ep19_l1_test_time 0.35780034599997634
Test Epoch19 layer2 Acc 0.744, AUC 0.8225538730621338, avg_entr 0.07409542053937912, f1 0.7439999580383301
ep19_l2_test_time 0.4665490760000921
Test Epoch19 layer3 Acc 0.7512, AUC 0.833847165107727, avg_entr 0.06319472193717957, f1 0.7512000203132629
ep19_l3_test_time 0.6169130880000466
Test Epoch19 layer4 Acc 0.7522, AUC 0.8334833383560181, avg_entr 0.0609251968562603, f1 0.7522000074386597
ep19_l4_test_time 0.8121222349999471
gc 0
Train Epoch20 Acc 0.9491 (37964/40000), AUC 0.9870003461837769
ep20_train_time 23.040218660999926
Test Epoch20 layer0 Acc 0.6984, AUC 0.7695299386978149, avg_entr 0.19411525130271912, f1 0.6984000205993652
ep20_l0_test_time 0.2903700139999046
Test Epoch20 layer1 Acc 0.7166, AUC 0.7985391616821289, avg_entr 0.08922569453716278, f1 0.7165999412536621
ep20_l1_test_time 0.3570261780000692
Test Epoch20 layer2 Acc 0.7414, AUC 0.8169399499893188, avg_entr 0.06571657210588455, f1 0.7414000034332275
ep20_l2_test_time 0.46691132099999777
Test Epoch20 layer3 Acc 0.751, AUC 0.8288412094116211, avg_entr 0.057156018912792206, f1 0.7509999871253967
ep20_l3_test_time 0.6163569100000359
Test Epoch20 layer4 Acc 0.7528, AUC 0.8288079500198364, avg_entr 0.05586252361536026, f1 0.7528000473976135
ep20_l4_test_time 0.8118463529999644
gc 0
Train Epoch21 Acc 0.951125 (38045/40000), AUC 0.9885081052780151
ep21_train_time 23.067927467999993
Test Epoch21 layer0 Acc 0.6946, AUC 0.7661656737327576, avg_entr 0.19115334749221802, f1 0.694599986076355
ep21_l0_test_time 0.2875577270000349
Test Epoch21 layer1 Acc 0.7012, AUC 0.7930435538291931, avg_entr 0.08776695281267166, f1 0.701200008392334
ep21_l1_test_time 0.3569603220000772
Test Epoch21 layer2 Acc 0.734, AUC 0.8168540000915527, avg_entr 0.07278349250555038, f1 0.7339999675750732
ep21_l2_test_time 0.47108364899997923
Test Epoch21 layer3 Acc 0.7444, AUC 0.8287906050682068, avg_entr 0.0637194961309433, f1 0.7444000244140625
ep21_l3_test_time 0.6151327970000011
Test Epoch21 layer4 Acc 0.744, AUC 0.8287049531936646, avg_entr 0.06355267018079758, f1 0.7439999580383301
ep21_l4_test_time 0.8114187639999955
gc 0
Train Epoch22 Acc 0.955525 (38221/40000), AUC 0.989587664604187
ep22_train_time 23.061307127000077
Test Epoch22 layer0 Acc 0.6974, AUC 0.7690443992614746, avg_entr 0.19135335087776184, f1 0.6973999738693237
ep22_l0_test_time 0.2878559089999726
Test Epoch22 layer1 Acc 0.7086, AUC 0.793298065662384, avg_entr 0.07957568019628525, f1 0.7085999846458435
ep22_l1_test_time 0.3566474699999844
Test Epoch22 layer2 Acc 0.7362, AUC 0.8115067481994629, avg_entr 0.062776580452919, f1 0.7361999750137329
ep22_l2_test_time 0.46700139899996884
Test Epoch22 layer3 Acc 0.7434, AUC 0.8254632949829102, avg_entr 0.05342581868171692, f1 0.743399977684021
ep22_l3_test_time 0.6177819590000126
Test Epoch22 layer4 Acc 0.745, AUC 0.8252381086349487, avg_entr 0.052666857838630676, f1 0.7449999451637268
ep22_l4_test_time 0.8144882290000623
gc 0
Train Epoch23 Acc 0.95805 (38322/40000), AUC 0.9908149242401123
ep23_train_time 23.068181502000016
Test Epoch23 layer0 Acc 0.7002, AUC 0.7672829031944275, avg_entr 0.18734319508075714, f1 0.7002000212669373
ep23_l0_test_time 0.28623158200002763
Test Epoch23 layer1 Acc 0.7136, AUC 0.7957611083984375, avg_entr 0.08162003010511398, f1 0.7135999798774719
ep23_l1_test_time 0.35680426199996873
Test Epoch23 layer2 Acc 0.7404, AUC 0.8148515820503235, avg_entr 0.061747387051582336, f1 0.7404000163078308
ep23_l2_test_time 0.4659027010000045
Test Epoch23 layer3 Acc 0.7488, AUC 0.8280073404312134, avg_entr 0.05405884236097336, f1 0.7487999796867371
ep23_l3_test_time 0.6176189969999086
Test Epoch23 layer4 Acc 0.7492, AUC 0.8276509642601013, avg_entr 0.0543619766831398, f1 0.7491999864578247
ep23_l4_test_time 0.8118340519999947
gc 0
Train Epoch24 Acc 0.9593 (38372/40000), AUC 0.9907716512680054
ep24_train_time 23.069748946999994
Test Epoch24 layer0 Acc 0.6966, AUC 0.7667815685272217, avg_entr 0.18755215406417847, f1 0.6966000199317932
ep24_l0_test_time 0.2870964810000487
Test Epoch24 layer1 Acc 0.7136, AUC 0.7923502922058105, avg_entr 0.07584865391254425, f1 0.7135999798774719
ep24_l1_test_time 0.36304663299995354
Test Epoch24 layer2 Acc 0.7376, AUC 0.8121381998062134, avg_entr 0.05959019809961319, f1 0.7376000285148621
ep24_l2_test_time 0.46596157800001947
Test Epoch24 layer3 Acc 0.7484, AUC 0.8229328989982605, avg_entr 0.05291052162647247, f1 0.7483999729156494
ep24_l3_test_time 0.6158210170000302
Test Epoch24 layer4 Acc 0.7496, AUC 0.8235064148902893, avg_entr 0.052582256495952606, f1 0.7495999932289124
ep24_l4_test_time 0.8101963020000085
gc 0
Train Epoch25 Acc 0.95905 (38362/40000), AUC 0.9910659790039062
ep25_train_time 23.02654143799998
Test Epoch25 layer0 Acc 0.697, AUC 0.7668667435646057, avg_entr 0.18354454636573792, f1 0.6970000267028809
ep25_l0_test_time 0.2871764630000371
Test Epoch25 layer1 Acc 0.7068, AUC 0.7909932136535645, avg_entr 0.07478176057338715, f1 0.7067999839782715
ep25_l1_test_time 0.3564874339999733
Test Epoch25 layer2 Acc 0.7354, AUC 0.8111967444419861, avg_entr 0.05679880827665329, f1 0.7354000210762024
ep25_l2_test_time 0.4662478249999822
Test Epoch25 layer3 Acc 0.7436, AUC 0.8262426257133484, avg_entr 0.050876133143901825, f1 0.7436000108718872
ep25_l3_test_time 0.6147176300000865
Test Epoch25 layer4 Acc 0.7442, AUC 0.8259298801422119, avg_entr 0.05036824196577072, f1 0.7441999316215515
ep25_l4_test_time 0.8104856080000218
gc 0
Train Epoch26 Acc 0.96305 (38522/40000), AUC 0.9921159744262695
ep26_train_time 23.104592136000065
Test Epoch26 layer0 Acc 0.698, AUC 0.7673114538192749, avg_entr 0.1836223155260086, f1 0.6980000138282776
ep26_l0_test_time 0.28841703699993104
Test Epoch26 layer1 Acc 0.7134, AUC 0.7932553291320801, avg_entr 0.0755474716424942, f1 0.7134000062942505
ep26_l1_test_time 0.35525824499995906
Test Epoch26 layer2 Acc 0.74, AUC 0.8131706714630127, avg_entr 0.060571666806936264, f1 0.7400000095367432
ep26_l2_test_time 0.46600042999989455
Test Epoch26 layer3 Acc 0.7488, AUC 0.8258864879608154, avg_entr 0.052933044731616974, f1 0.7487999796867371
ep26_l3_test_time 0.616161095999928
Test Epoch26 layer4 Acc 0.7498, AUC 0.8253887891769409, avg_entr 0.05244435369968414, f1 0.7498000264167786
ep26_l4_test_time 0.8153791240000601
gc 0
Train Epoch27 Acc 0.96295 (38518/40000), AUC 0.992277979850769
ep27_train_time 23.10693943399997
Test Epoch27 layer0 Acc 0.6972, AUC 0.7665669918060303, avg_entr 0.18444277346134186, f1 0.6972000002861023
ep27_l0_test_time 0.29098182200004885
Test Epoch27 layer1 Acc 0.7108, AUC 0.7917702198028564, avg_entr 0.07540474086999893, f1 0.7107999920845032
ep27_l1_test_time 0.3589564830000427
Test Epoch27 layer2 Acc 0.7382, AUC 0.8118159770965576, avg_entr 0.05748532712459564, f1 0.7382000684738159
ep27_l2_test_time 0.4677333859999635
Test Epoch27 layer3 Acc 0.7476, AUC 0.8264729976654053, avg_entr 0.050449274480342865, f1 0.7476000189781189
ep27_l3_test_time 0.6223368139999366
Test Epoch27 layer4 Acc 0.7478, AUC 0.8254927396774292, avg_entr 0.04897774010896683, f1 0.7477999925613403
ep27_l4_test_time 0.8113069339999583
gc 0
Train Epoch28 Acc 0.9628 (38512/40000), AUC 0.992453932762146
ep28_train_time 23.03678476999994
Test Epoch28 layer0 Acc 0.6972, AUC 0.7659958600997925, avg_entr 0.18265961110591888, f1 0.6972000002861023
ep28_l0_test_time 0.28714073499997994
Test Epoch28 layer1 Acc 0.7106, AUC 0.7909291386604309, avg_entr 0.07572723180055618, f1 0.7106000185012817
ep28_l1_test_time 0.35744683899997653
Test Epoch28 layer2 Acc 0.7378, AUC 0.8096981048583984, avg_entr 0.05899663642048836, f1 0.7378000020980835
ep28_l2_test_time 0.4638874109999733
Test Epoch28 layer3 Acc 0.748, AUC 0.8258312940597534, avg_entr 0.05122292414307594, f1 0.7480000257492065
ep28_l3_test_time 0.6171001789999764
Test Epoch28 layer4 Acc 0.7484, AUC 0.8253593444824219, avg_entr 0.049217864871025085, f1 0.7483999729156494
ep28_l4_test_time 0.8173171270000239
gc 0
Train Epoch29 Acc 0.96475 (38590/40000), AUC 0.9928367733955383
ep29_train_time 23.06325252099998
Test Epoch29 layer0 Acc 0.6944, AUC 0.7653725147247314, avg_entr 0.18364734947681427, f1 0.6944000124931335
ep29_l0_test_time 0.3040627459999996
Test Epoch29 layer1 Acc 0.712, AUC 0.789905309677124, avg_entr 0.07575248181819916, f1 0.7120000123977661
ep29_l1_test_time 0.36543965700002445
Test Epoch29 layer2 Acc 0.7374, AUC 0.8105770349502563, avg_entr 0.06174334138631821, f1 0.7373999953269958
ep29_l2_test_time 0.46815533200003756
Test Epoch29 layer3 Acc 0.7466, AUC 0.8248088359832764, avg_entr 0.05403686687350273, f1 0.7465999126434326
ep29_l3_test_time 0.617173825000009
Test Epoch29 layer4 Acc 0.7488, AUC 0.8245711326599121, avg_entr 0.053016260266304016, f1 0.7487999796867371
ep29_l4_test_time 0.8175059550000014
gc 0
Train Epoch30 Acc 0.964125 (38565/40000), AUC 0.9931645393371582
ep30_train_time 23.054444620000027
Test Epoch30 layer0 Acc 0.6966, AUC 0.7656919956207275, avg_entr 0.18061339855194092, f1 0.6966000199317932
ep30_l0_test_time 0.28664133400002356
Test Epoch30 layer1 Acc 0.7102, AUC 0.79072105884552, avg_entr 0.07224327325820923, f1 0.7102000117301941
ep30_l1_test_time 0.35555290600007083
Test Epoch30 layer2 Acc 0.7392, AUC 0.809851348400116, avg_entr 0.05833190307021141, f1 0.7391999959945679
ep30_l2_test_time 0.4684874340000533
Test Epoch30 layer3 Acc 0.7452, AUC 0.8232682943344116, avg_entr 0.04989197850227356, f1 0.745199978351593
ep30_l3_test_time 0.617252181999902
Test Epoch30 layer4 Acc 0.7488, AUC 0.8233917951583862, avg_entr 0.048792265355587006, f1 0.7487999796867371
ep30_l4_test_time 0.8112396359999821
gc 0
Train Epoch31 Acc 0.965925 (38637/40000), AUC 0.9935153722763062
ep31_train_time 23.239493560999904
Test Epoch31 layer0 Acc 0.6966, AUC 0.7650141716003418, avg_entr 0.18045082688331604, f1 0.6966000199317932
ep31_l0_test_time 0.288994300000013
Test Epoch31 layer1 Acc 0.712, AUC 0.79047691822052, avg_entr 0.07415373623371124, f1 0.7120000123977661
ep31_l1_test_time 0.3593205700000226
Test Epoch31 layer2 Acc 0.7378, AUC 0.8092396259307861, avg_entr 0.059418048709630966, f1 0.7378000020980835
ep31_l2_test_time 0.46668426899998394
Test Epoch31 layer3 Acc 0.747, AUC 0.8252381086349487, avg_entr 0.05257849767804146, f1 0.746999979019165
ep31_l3_test_time 0.6164541350000263
Test Epoch31 layer4 Acc 0.7492, AUC 0.8250170946121216, avg_entr 0.05063323304057121, f1 0.7491999864578247
ep31_l4_test_time 0.8105460509999602
gc 0
Train Epoch32 Acc 0.965575 (38623/40000), AUC 0.9933398962020874
ep32_train_time 23.124937814999953
Test Epoch32 layer0 Acc 0.6974, AUC 0.7653781771659851, avg_entr 0.18014396727085114, f1 0.6973999738693237
ep32_l0_test_time 0.28657676000000265
Test Epoch32 layer1 Acc 0.7102, AUC 0.7912752628326416, avg_entr 0.07250375300645828, f1 0.7102000117301941
ep32_l1_test_time 0.35620828699995855
Test Epoch32 layer2 Acc 0.7418, AUC 0.8116129636764526, avg_entr 0.05967618525028229, f1 0.74180006980896
ep32_l2_test_time 0.46607551099998545
Test Epoch32 layer3 Acc 0.7512, AUC 0.8252692222595215, avg_entr 0.05215809866786003, f1 0.7512000203132629
ep32_l3_test_time 0.6174029100000098
Test Epoch32 layer4 Acc 0.7538, AUC 0.8246370553970337, avg_entr 0.05092797800898552, f1 0.7537999749183655
ep32_l4_test_time 0.8146061549999786
gc 0
Train Epoch33 Acc 0.9653 (38612/40000), AUC 0.9933536052703857
ep33_train_time 23.142574379999928
Test Epoch33 layer0 Acc 0.6984, AUC 0.7652080059051514, avg_entr 0.18023909628391266, f1 0.6984000205993652
ep33_l0_test_time 0.2863951699999916
Test Epoch33 layer1 Acc 0.7098, AUC 0.7901877164840698, avg_entr 0.07273101061582565, f1 0.7098000049591064
ep33_l1_test_time 0.35793064899996807
Test Epoch33 layer2 Acc 0.738, AUC 0.8087777495384216, avg_entr 0.05696757882833481, f1 0.7379999160766602
ep33_l2_test_time 0.46525493599995116
Test Epoch33 layer3 Acc 0.7478, AUC 0.8240414261817932, avg_entr 0.04875675216317177, f1 0.7477999925613403
ep33_l3_test_time 0.6167485629999874
Test Epoch33 layer4 Acc 0.7496, AUC 0.8244401216506958, avg_entr 0.04581354185938835, f1 0.7495999932289124
ep33_l4_test_time 0.8122094220000236
gc 0
Train Epoch34 Acc 0.967175 (38687/40000), AUC 0.9939472079277039
ep34_train_time 23.111947740999994
Test Epoch34 layer0 Acc 0.6976, AUC 0.7650240659713745, avg_entr 0.17910832166671753, f1 0.6976000070571899
ep34_l0_test_time 0.28606254599992553
Test Epoch34 layer1 Acc 0.7122, AUC 0.790519118309021, avg_entr 0.07123944163322449, f1 0.7121999263763428
ep34_l1_test_time 0.35853481599997394
Test Epoch34 layer2 Acc 0.7414, AUC 0.8091849088668823, avg_entr 0.05762321874499321, f1 0.7414000034332275
ep34_l2_test_time 0.46559548800007633
Test Epoch34 layer3 Acc 0.75, AUC 0.8237714171409607, avg_entr 0.049656204879283905, f1 0.75
ep34_l3_test_time 0.6162895050000543
Test Epoch34 layer4 Acc 0.7504, AUC 0.8237118124961853, avg_entr 0.047841716557741165, f1 0.7504000067710876
ep34_l4_test_time 0.8112947849999728
gc 0
Train Epoch35 Acc 0.96645 (38658/40000), AUC 0.9936486482620239
ep35_train_time 23.05109833000006
Test Epoch35 layer0 Acc 0.6956, AUC 0.7649462223052979, avg_entr 0.17868159711360931, f1 0.6955999732017517
ep35_l0_test_time 0.28650937499992324
Test Epoch35 layer1 Acc 0.7114, AUC 0.7900036573410034, avg_entr 0.07194991409778595, f1 0.7113999724388123
ep35_l1_test_time 0.3576428810000607
Test Epoch35 layer2 Acc 0.7384, AUC 0.8090635538101196, avg_entr 0.057714153081178665, f1 0.7384000420570374
ep35_l2_test_time 0.46571491200006676
Test Epoch35 layer3 Acc 0.75, AUC 0.8242053985595703, avg_entr 0.05105164274573326, f1 0.75
ep35_l3_test_time 0.6171057630000405
Test Epoch35 layer4 Acc 0.7516, AUC 0.824012279510498, avg_entr 0.04884888604283333, f1 0.7516000270843506
ep35_l4_test_time 0.8128906499999857
gc 0
Train Epoch36 Acc 0.967925 (38717/40000), AUC 0.9937582015991211
ep36_train_time 23.15314571800002
Test Epoch36 layer0 Acc 0.697, AUC 0.7651907205581665, avg_entr 0.17901234328746796, f1 0.6970000267028809
ep36_l0_test_time 0.2860905239999738
Test Epoch36 layer1 Acc 0.711, AUC 0.7897925972938538, avg_entr 0.07081855833530426, f1 0.7110000848770142
ep36_l1_test_time 0.371014086999935
Test Epoch36 layer2 Acc 0.7372, AUC 0.8074591755867004, avg_entr 0.05561283603310585, f1 0.7372000217437744
ep36_l2_test_time 0.46790906299997914
Test Epoch36 layer3 Acc 0.747, AUC 0.8234891891479492, avg_entr 0.04803129658102989, f1 0.746999979019165
ep36_l3_test_time 0.6183274640000036
Test Epoch36 layer4 Acc 0.7476, AUC 0.8238022327423096, avg_entr 0.045788731426000595, f1 0.7476000189781189
ep36_l4_test_time 0.8322970880000184
gc 0
Train Epoch37 Acc 0.96745 (38698/40000), AUC 0.9939253926277161
ep37_train_time 23.214727868000068
Test Epoch37 layer0 Acc 0.6978, AUC 0.7646682262420654, avg_entr 0.17920750379562378, f1 0.6977999806404114
ep37_l0_test_time 0.286387287000025
Test Epoch37 layer1 Acc 0.7098, AUC 0.789277195930481, avg_entr 0.071787528693676, f1 0.7098000049591064
ep37_l1_test_time 0.35637397400000737
Test Epoch37 layer2 Acc 0.7378, AUC 0.8080079555511475, avg_entr 0.05753372237086296, f1 0.7378000020980835
ep37_l2_test_time 0.46527223399994
Test Epoch37 layer3 Acc 0.7486, AUC 0.8223372101783752, avg_entr 0.05060391128063202, f1 0.7486000061035156
ep37_l3_test_time 0.6153043819999766
Test Epoch37 layer4 Acc 0.7506, AUC 0.822789192199707, avg_entr 0.04878682270646095, f1 0.7505999803543091
ep37_l4_test_time 0.8130837920000431
gc 0
Train Epoch38 Acc 0.9674 (38696/40000), AUC 0.9940245151519775
ep38_train_time 23.060181643999954
Test Epoch38 layer0 Acc 0.6976, AUC 0.764860987663269, avg_entr 0.17854879796504974, f1 0.6976000070571899
ep38_l0_test_time 0.2949421309999707
Test Epoch38 layer1 Acc 0.71, AUC 0.7891572713851929, avg_entr 0.07080601900815964, f1 0.7099999785423279
ep38_l1_test_time 0.3597098929999447
Test Epoch38 layer2 Acc 0.7378, AUC 0.8068000078201294, avg_entr 0.05533206835389137, f1 0.7378000020980835
ep38_l2_test_time 0.4683242460000656
Test Epoch38 layer3 Acc 0.7478, AUC 0.8217921257019043, avg_entr 0.048284634947776794, f1 0.7477999925613403
ep38_l3_test_time 0.6176090950000344
Test Epoch38 layer4 Acc 0.7494, AUC 0.8224511742591858, avg_entr 0.04630402475595474, f1 0.7494000196456909
ep38_l4_test_time 0.8134921349999331
gc 0
Train Epoch39 Acc 0.9679 (38716/40000), AUC 0.9943003058433533
ep39_train_time 23.16123576999985
Test Epoch39 layer0 Acc 0.6974, AUC 0.7648648023605347, avg_entr 0.1782265603542328, f1 0.6973999738693237
ep39_l0_test_time 0.2955144739999014
Test Epoch39 layer1 Acc 0.7094, AUC 0.7894991636276245, avg_entr 0.0703209936618805, f1 0.709399938583374
ep39_l1_test_time 0.3588516129998425
Test Epoch39 layer2 Acc 0.737, AUC 0.8071581721305847, avg_entr 0.05553906783461571, f1 0.7369999885559082
ep39_l2_test_time 0.4664653779998389
Test Epoch39 layer3 Acc 0.7478, AUC 0.821963906288147, avg_entr 0.048535943031311035, f1 0.7477999925613403
ep39_l3_test_time 0.6210064570000213
Test Epoch39 layer4 Acc 0.7496, AUC 0.8225904703140259, avg_entr 0.046658359467983246, f1 0.7495999932289124
ep39_l4_test_time 0.8161291059998348
gc 0
Train Epoch40 Acc 0.96755 (38702/40000), AUC 0.994198739528656
ep40_train_time 23.12296944500008
Test Epoch40 layer0 Acc 0.6968, AUC 0.7648649215698242, avg_entr 0.17842546105384827, f1 0.6967999935150146
ep40_l0_test_time 0.28604812999992646
Test Epoch40 layer1 Acc 0.7102, AUC 0.7888529300689697, avg_entr 0.07044043391942978, f1 0.7102000117301941
ep40_l1_test_time 0.35814539900002273
Test Epoch40 layer2 Acc 0.7358, AUC 0.8056750297546387, avg_entr 0.05496084690093994, f1 0.73580002784729
ep40_l2_test_time 0.4648672210000768
Test Epoch40 layer3 Acc 0.7478, AUC 0.8219996690750122, avg_entr 0.047546666115522385, f1 0.7477999925613403
ep40_l3_test_time 0.6158368819999396
Test Epoch40 layer4 Acc 0.7476, AUC 0.8226324319839478, avg_entr 0.0456991009414196, f1 0.7476000189781189
ep40_l4_test_time 0.812767053000016
gc 0
Train Epoch41 Acc 0.967575 (38703/40000), AUC 0.9942504167556763
ep41_train_time 23.10329778099981
Test Epoch41 layer0 Acc 0.6972, AUC 0.7647098302841187, avg_entr 0.17820324003696442, f1 0.6972000002861023
ep41_l0_test_time 0.2948677360000147
Test Epoch41 layer1 Acc 0.7092, AUC 0.78932785987854, avg_entr 0.07077930867671967, f1 0.7092000246047974
ep41_l1_test_time 0.3619042450000052
Test Epoch41 layer2 Acc 0.7376, AUC 0.806943416595459, avg_entr 0.05576818063855171, f1 0.7376000285148621
ep41_l2_test_time 0.4658142229998248
Test Epoch41 layer3 Acc 0.7482, AUC 0.8218706846237183, avg_entr 0.048997651785612106, f1 0.748199999332428
ep41_l3_test_time 0.6185587369998302
Test Epoch41 layer4 Acc 0.7502, AUC 0.8224883079528809, avg_entr 0.046997230499982834, f1 0.7501999735832214
ep41_l4_test_time 0.8133118450000438
gc 0
Train Epoch42 Acc 0.9682 (38728/40000), AUC 0.9941017031669617
ep42_train_time 23.223436196999955
Test Epoch42 layer0 Acc 0.6978, AUC 0.764692485332489, avg_entr 0.17795173823833466, f1 0.6977999806404114
ep42_l0_test_time 0.28847861999997804
Test Epoch42 layer1 Acc 0.71, AUC 0.7894167304039001, avg_entr 0.07078298926353455, f1 0.7099999785423279
ep42_l1_test_time 0.36610266899992894
Test Epoch42 layer2 Acc 0.738, AUC 0.8073451519012451, avg_entr 0.05633993074297905, f1 0.7379999160766602
ep42_l2_test_time 0.4684267909999562
Test Epoch42 layer3 Acc 0.7494, AUC 0.8223226070404053, avg_entr 0.04969986155629158, f1 0.7494000196456909
ep42_l3_test_time 0.6240949959999398
Test Epoch42 layer4 Acc 0.7506, AUC 0.8227442502975464, avg_entr 0.0477861613035202, f1 0.7505999803543091
ep42_l4_test_time 0.8181924619998426
gc 0
Train Epoch43 Acc 0.96765 (38706/40000), AUC 0.9943480491638184
ep43_train_time 23.125318934000006
Test Epoch43 layer0 Acc 0.6964, AUC 0.7647151947021484, avg_entr 0.17820675671100616, f1 0.696399986743927
ep43_l0_test_time 0.28588545999991766
Test Epoch43 layer1 Acc 0.7096, AUC 0.7891674041748047, avg_entr 0.07088620960712433, f1 0.7095999717712402
ep43_l1_test_time 0.3558234249999259
Test Epoch43 layer2 Acc 0.7372, AUC 0.806743323802948, avg_entr 0.05556707829236984, f1 0.7372000217437744
ep43_l2_test_time 0.4649264150000363
Test Epoch43 layer3 Acc 0.7482, AUC 0.822468101978302, avg_entr 0.04893018305301666, f1 0.748199999332428
ep43_l3_test_time 0.6244764660000328
Test Epoch43 layer4 Acc 0.7508, AUC 0.8229972124099731, avg_entr 0.046713948249816895, f1 0.7508000135421753
ep43_l4_test_time 0.8123479890000453
gc 0
Train Epoch44 Acc 0.96755 (38702/40000), AUC 0.9944695234298706
ep44_train_time 23.04819763899991
Test Epoch44 layer0 Acc 0.697, AUC 0.7646622657775879, avg_entr 0.17817726731300354, f1 0.6970000267028809
ep44_l0_test_time 0.2897360120000485
Test Epoch44 layer1 Acc 0.7094, AUC 0.7890280485153198, avg_entr 0.07107449322938919, f1 0.709399938583374
ep44_l1_test_time 0.35727913400000944
Test Epoch44 layer2 Acc 0.7364, AUC 0.8069513440132141, avg_entr 0.055871546268463135, f1 0.7364000082015991
ep44_l2_test_time 0.4658176220000314
Test Epoch44 layer3 Acc 0.7492, AUC 0.8226217031478882, avg_entr 0.04908562824130058, f1 0.7491999864578247
ep44_l3_test_time 0.6180231720002212
Test Epoch44 layer4 Acc 0.751, AUC 0.8230577111244202, avg_entr 0.046814896166324615, f1 0.7509999871253967
ep44_l4_test_time 0.8125832069999888
gc 0
Train Epoch45 Acc 0.967225 (38689/40000), AUC 0.9941235780715942
ep45_train_time 23.138150147000033
Test Epoch45 layer0 Acc 0.6968, AUC 0.7647272944450378, avg_entr 0.1779695749282837, f1 0.6967999935150146
ep45_l0_test_time 0.28855910800007223
Test Epoch45 layer1 Acc 0.7096, AUC 0.7889678478240967, avg_entr 0.07058751583099365, f1 0.7095999717712402
ep45_l1_test_time 0.35776967800006787
Test Epoch45 layer2 Acc 0.7368, AUC 0.8064476847648621, avg_entr 0.055363334715366364, f1 0.7368000149726868
ep45_l2_test_time 0.4704902159999165
Test Epoch45 layer3 Acc 0.748, AUC 0.8219247460365295, avg_entr 0.048408329486846924, f1 0.7480000257492065
ep45_l3_test_time 0.6394887440001185
Test Epoch45 layer4 Acc 0.7498, AUC 0.8225775957107544, avg_entr 0.04632590711116791, f1 0.7498000264167786
ep45_l4_test_time 0.8311900769999738
gc 0
Train Epoch46 Acc 0.967975 (38719/40000), AUC 0.9945605397224426
ep46_train_time 23.111653123999986
Test Epoch46 layer0 Acc 0.6972, AUC 0.7647123336791992, avg_entr 0.17768193781375885, f1 0.6972000002861023
ep46_l0_test_time 0.2923605739999857
Test Epoch46 layer1 Acc 0.7098, AUC 0.7891604900360107, avg_entr 0.07036589086055756, f1 0.7098000049591064
ep46_l1_test_time 0.3611230430001342
Test Epoch46 layer2 Acc 0.7358, AUC 0.8067290186882019, avg_entr 0.0550798662006855, f1 0.73580002784729
ep46_l2_test_time 0.47187073600002805
Test Epoch46 layer3 Acc 0.7488, AUC 0.8219435214996338, avg_entr 0.048308391124010086, f1 0.7487999796867371
ep46_l3_test_time 0.6229999849999786
Test Epoch46 layer4 Acc 0.7508, AUC 0.8225095272064209, avg_entr 0.04624726250767708, f1 0.7508000135421753
ep46_l4_test_time 0.8170939169999656
gc 0
Train Epoch47 Acc 0.9678 (38712/40000), AUC 0.9940627217292786
ep47_train_time 23.137697521999826
Test Epoch47 layer0 Acc 0.6984, AUC 0.7646909952163696, avg_entr 0.17768262326717377, f1 0.6984000205993652
ep47_l0_test_time 0.29223079999997026
Test Epoch47 layer1 Acc 0.7092, AUC 0.78905189037323, avg_entr 0.07034831494092941, f1 0.7092000246047974
ep47_l1_test_time 0.36341175899997324
Test Epoch47 layer2 Acc 0.7384, AUC 0.8068556785583496, avg_entr 0.055526770651340485, f1 0.7384000420570374
ep47_l2_test_time 0.4708725210000466
Test Epoch47 layer3 Acc 0.7482, AUC 0.8217225074768066, avg_entr 0.048558685928583145, f1 0.748199999332428
ep47_l3_test_time 0.6244205089999468
Test Epoch47 layer4 Acc 0.7502, AUC 0.8223833441734314, avg_entr 0.04656160995364189, f1 0.7501999735832214
ep47_l4_test_time 0.8197396210000534
gc 0
Train Epoch48 Acc 0.967575 (38703/40000), AUC 0.9942025542259216
ep48_train_time 23.14227189600001
Test Epoch48 layer0 Acc 0.6984, AUC 0.76468825340271, avg_entr 0.17736373841762543, f1 0.6984000205993652
ep48_l0_test_time 0.2908069689999593
Test Epoch48 layer1 Acc 0.7098, AUC 0.7892676591873169, avg_entr 0.0703430026769638, f1 0.7098000049591064
ep48_l1_test_time 0.36214370099992266
Test Epoch48 layer2 Acc 0.736, AUC 0.8068100810050964, avg_entr 0.05497691035270691, f1 0.7360000014305115
ep48_l2_test_time 0.4736764240001321
Test Epoch48 layer3 Acc 0.7488, AUC 0.8219842314720154, avg_entr 0.04809034243226051, f1 0.7487999796867371
ep48_l3_test_time 0.62883168999997
Test Epoch48 layer4 Acc 0.7516, AUC 0.8225972652435303, avg_entr 0.04600242152810097, f1 0.7516000270843506
ep48_l4_test_time 0.8188797450000038
gc 0
Train Epoch49 Acc 0.966975 (38679/40000), AUC 0.9940201044082642
ep49_train_time 23.22604962399987
Test Epoch49 layer0 Acc 0.6978, AUC 0.7646728754043579, avg_entr 0.17708192765712738, f1 0.6977999806404114
ep49_l0_test_time 0.295410671999889
Test Epoch49 layer1 Acc 0.7092, AUC 0.7893432378768921, avg_entr 0.07009895890951157, f1 0.7092000246047974
ep49_l1_test_time 0.36629219699989335
Test Epoch49 layer2 Acc 0.7384, AUC 0.8067677021026611, avg_entr 0.055228691548109055, f1 0.7384000420570374
ep49_l2_test_time 0.4701618490000783
Test Epoch49 layer3 Acc 0.7476, AUC 0.8215528726577759, avg_entr 0.04822568967938423, f1 0.7476000189781189
ep49_l3_test_time 0.6280297509999855
Test Epoch49 layer4 Acc 0.7496, AUC 0.8222408890724182, avg_entr 0.04624791070818901, f1 0.7495999932289124
ep49_l4_test_time 0.8179968709998775
Best AUC tensor(0.7690) 13 4
train_as_loss [[8.76551234e+01 5.84383859e+01 5.18788931e+01 5.03849119e+01
  4.98537200e+01 4.96076518e+01 4.94741453e+01 4.93937911e+01
  4.93417494e+01 4.93061621e+01 4.92807837e+01 4.92620783e+01
  4.92479123e+01 4.92369463e+01 4.92301869e+01 4.92262025e+01
  4.92225033e+01 4.92190815e+01 4.92166680e+01 4.92150974e+01
  4.92135360e+01 4.92119893e+01 4.92108358e+01 4.92100510e+01
  4.92092430e+01 4.92084193e+01 4.92077843e+01 4.92073421e+01
  4.92068796e+01 4.92063965e+01 4.92060210e+01 4.92057545e+01
  4.92054708e+01 4.92051753e+01 4.92049409e+01 4.92047731e+01
  4.92045954e+01 4.92044061e+01 4.92042557e+01 4.92041489e+01
  4.92040344e+01 4.92039123e+01 4.92038155e+01 4.92037452e+01
  4.92036697e+01 4.92035920e+01 4.92035324e+01 4.92034777e+01
  4.92034366e+01 4.92033853e+01]
 [2.59251855e+00 5.67663831e-04 2.10910015e-05 4.91830452e-06
  2.03529334e-06 9.84252595e-07 5.38116451e-07 3.24792121e-07
  2.10257232e-07 2.98247725e-07 3.05063884e-07 7.26480542e-08
  7.20833629e-07 3.99234525e-08 3.16412801e-08 2.70879576e-08
  2.36245017e-08 2.06950353e-08 1.82378504e-08 1.68339443e-08
  1.53695781e-08 1.42323702e-08 1.32390338e-08 1.25084650e-08
  1.18782587e-08 1.13803279e-08 1.06332963e-08 1.03553166e-08
  1.00658325e-08 9.55006590e-09 9.28705465e-09 8.92622183e-09
  8.68315266e-09 8.42508525e-09 8.19096466e-09 7.98225833e-09
  7.71695451e-09 7.44303294e-09 7.37477004e-09 7.08878641e-09
  6.89240212e-09 6.59485321e-09 6.64990240e-09 6.54430345e-09
  6.15088738e-09 5.81765110e-09 6.18751087e-09 6.01756466e-09
  5.65187737e-09 5.34479947e-09]
 [2.36594785e+00 5.24764990e-04 1.64179669e-05 3.76740146e-06
  1.60014051e-06 7.64572174e-07 4.15720357e-07 2.40319827e-07
  1.59958518e-07 1.12662065e-07 7.70767165e-08 5.68351094e-08
  6.28145697e-08 3.03734703e-08 2.25169145e-08 1.87953625e-08
  1.65839707e-08 1.49164409e-08 1.20681046e-08 1.10712429e-08
  1.01479195e-08 9.42103640e-09 8.51195299e-09 7.97360890e-09
  7.64729701e-09 7.29544923e-09 6.73638949e-09 6.53565967e-09
  6.38185501e-09 6.05889364e-09 5.83597894e-09 5.61284909e-09
  5.49113002e-09 5.33978138e-09 5.16876904e-09 5.06839868e-09
  4.90962599e-09 4.72843126e-09 4.74526464e-09 4.53070450e-09
  4.41265640e-09 4.21779082e-09 4.32237937e-09 4.20969698e-09
  3.95501470e-09 3.79464037e-09 4.15833185e-09 4.08164353e-09
  3.83729925e-09 3.63493429e-09]
 [2.08065327e+00 2.49489294e-04 1.18061605e-05 3.00712424e-06
  1.43663453e-06 6.81863294e-07 3.84663792e-07 2.03529735e-07
  1.48093409e-07 1.07559502e-07 7.59100224e-08 6.16960439e-08
  9.76234102e-08 3.38540991e-08 2.00153037e-08 1.60616138e-08
  1.45268146e-08 1.47920533e-08 9.32330274e-09 8.58466253e-09
  8.00326503e-09 7.48053838e-09 6.36049826e-09 5.85734397e-09
  5.74897331e-09 5.48486997e-09 4.91342061e-09 4.76427701e-09
  4.66203422e-09 4.50235067e-09 4.24318165e-09 4.08051746e-09
  4.03152318e-09 3.88772897e-09 3.84924665e-09 3.75623853e-09
  3.62113448e-09 3.51874931e-09 3.58578239e-09 3.47046392e-09
  3.35227592e-09 3.24608289e-09 3.42496785e-09 3.38076211e-09
  3.19893367e-09 3.07792720e-09 3.50249634e-09 3.52428371e-09
  3.28366347e-09 3.13147747e-09]
 [2.39536805e+00 8.35242510e-04 1.81020222e-05 4.52129834e-06
  2.33038742e-06 1.09543072e-06 6.08249894e-07 3.16284054e-07
  2.46617206e-07 1.77795491e-07 1.30905211e-07 1.18851350e-07
  1.21644481e-07 7.07819571e-08 3.36534858e-08 2.56862190e-08
  2.35150854e-08 2.63645771e-08 1.33467764e-08 1.22611869e-08
  1.14209848e-08 1.05983794e-08 8.78617196e-09 7.88250527e-09
  7.80445028e-09 7.46547891e-09 6.55645704e-09 6.34314453e-09
  6.24360159e-09 5.99812269e-09 5.65567421e-09 5.38654884e-09
  5.33029827e-09 5.14072371e-09 5.06519904e-09 4.93125147e-09
  4.76648692e-09 4.61919093e-09 4.70959360e-09 4.58947263e-09
  4.43694963e-09 4.27756605e-09 4.42527196e-09 4.44985045e-09
  4.17666581e-09 3.95776277e-09 4.46072741e-09 4.62014477e-09
  4.30016738e-09 4.08075797e-09]]
train_ae_loss [[4.62488349 3.51432406 4.6231535  5.283358   5.60662026 6.04517195
  6.06352573 6.13248966 6.05680098 5.99404377 5.8580842  5.60734639
  5.29212146 5.14523722 4.55907071 4.32695212 4.22185421 4.13693047
  3.81239269 3.72558355 3.63964433 3.57625913 3.49565617 3.41934151
  3.40179677 3.39056288 3.32925039 3.28593987 3.29227679 3.26637164
  3.24357655 3.229805   3.24151219 3.21749531 3.22246392 3.20776653
  3.20193582 3.18768909 3.1923374  3.18086463 3.21149965 3.19455806
  3.18764693 3.1891327  3.18499858 3.18624472 3.19936372 3.19037103
  3.20737505 3.16618055]
 [3.56624069 3.29728864 4.3167485  4.82725663 4.99010647 5.32016863
  5.23277193 5.15563613 5.02917512 4.91643082 4.73068514 4.46174963
  4.14999248 4.03333259 3.40751702 3.13300145 3.02617418 2.79788515
  2.3782898  2.2680025  2.13850276 2.06597795 1.96560476 1.88211272
  1.86768327 1.83258192 1.76733735 1.70204744 1.71469133 1.67847129
  1.65778636 1.63577397 1.64904355 1.62434412 1.61358277 1.62510477
  1.60288092 1.59090627 1.58460767 1.57647558 1.5953399  1.58925585
  1.57431694 1.58708189 1.58195377 1.56484642 1.5815492  1.586835
  1.57773698 1.55009296]
 [3.77570096 3.3988297  4.55697643 5.08231043 5.23209191 5.58587556
  5.3971286  5.23782501 5.01572535 4.82015346 4.61741415 4.27326411
  3.82486283 3.69173576 2.8085868  2.51956618 2.49965617 2.34807845
  1.93470426 1.82490566 1.71630347 1.66601951 1.54069264 1.463678
  1.46218215 1.41502218 1.35863282 1.29793597 1.29639697 1.28702571
  1.24014819 1.23648714 1.24131467 1.21145824 1.19890163 1.21791418
  1.19094743 1.19464681 1.16920618 1.17636459 1.18471593 1.18418113
  1.17439268 1.17454056 1.17438245 1.15186888 1.16494721 1.17397163
  1.17002635 1.13027986]
 [4.46672669 3.39656593 4.72235081 5.19308236 5.30480392 5.68911354
  5.43462878 5.22620576 4.97173709 4.73532637 4.52551099 4.10312235
  3.60192596 3.49558319 2.60397677 2.34331497 2.33353818 2.19407398
  1.77844761 1.67305451 1.5646381  1.52991833 1.40210356 1.32815307
  1.33188269 1.29036751 1.22342845 1.17076664 1.17075562 1.15710448
  1.12002515 1.11704376 1.1161103  1.08315007 1.07312291 1.09734266
  1.06925107 1.07379371 1.04921189 1.05276783 1.05979856 1.05887472
  1.0516022  1.05613513 1.05433589 1.02988308 1.04451529 1.05579026
  1.04256359 1.00780057]
 [4.56728028 3.01705183 4.38010501 4.78009611 4.89314516 5.35297224
  5.04510865 4.80476666 4.54266633 4.20278331 3.98683426 3.5294326
  3.10049523 3.06377269 2.26391412 2.0383903  2.03815795 1.91605188
  1.55014065 1.45797075 1.36444627 1.33445897 1.22450148 1.15771179
  1.16351227 1.1277064  1.06767092 1.02281473 1.02257978 1.00970326
  0.97950056 0.97503091 0.97525805 0.94668234 0.93621064 0.95851453
  0.93435289 0.93810001 0.91783075 0.92108166 0.92492563 0.9234673
  0.92026063 0.92519406 0.9225627  0.89812648 0.91171735 0.92215834
  0.91063945 0.87965366]]
valid_acc (5, 50)
valid_AUC (5, 50)
train_acc (50,)
total_train+valid_time 1290.0992371480002
Start Testing
Load ckpt at ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt
gc 9
Test layer0 Acc 0.7204, AUC 0.7956465482711792, avg_entr 0.22756387293338776, f1 0.7204000353813171
l0_test_time 0.28979607900009796
gc 0
Test layer1 Acc 0.7534, AUC 0.8343535661697388, avg_entr 0.1966581791639328, f1 0.7533999681472778
l1_test_time 0.36878625600002124
gc 0
Test layer2 Acc 0.7678, AUC 0.8488637208938599, avg_entr 0.13369743525981903, f1 0.767799973487854
l2_test_time 0.47628792099999373
gc 0
Test layer3 Acc 0.77, AUC 0.854433536529541, avg_entr 0.12405835837125778, f1 0.7699999809265137
l3_test_time 0.6248319460000857
gc 0
Test layer4 Acc 0.7782, AUC 0.8592367172241211, avg_entr 0.11543601006269455, f1 0.7781999707221985
l4_test_time 0.8205786789999365
gc 0
Test threshold 0.1 Acc 0.774, AUC 0.8318347930908203, avg_entr 0.15756842494010925, f1 0.7739999890327454
t0.1_test_time 0.5705840790001275
gc 0
Test threshold 0.2 Acc 0.7706, AUC 0.8257153034210205, avg_entr 0.16402015089988708, f1 0.7706000208854675
t0.2_test_time 0.5159940500000175
gc 0
Test threshold 0.3 Acc 0.7692, AUC 0.8231995105743408, avg_entr 0.17325802147388458, f1 0.7692000269889832
t0.3_test_time 0.47910116399998515
gc 0
Test threshold 0.4 Acc 0.7668, AUC 0.8177332878112793, avg_entr 0.18277476727962494, f1 0.7667999863624573
t0.4_test_time 0.4518319609999253
gc 0
Test threshold 0.5 Acc 0.7636, AUC 0.8152273297309875, avg_entr 0.1973043829202652, f1 0.7635999917984009
t0.5_test_time 0.42677198800015503
gc 0
Test threshold 0.6 Acc 0.7598, AUC 0.8126487731933594, avg_entr 0.21548472344875336, f1 0.7598000168800354
t0.6_test_time 0.407126991000041
gc 0
Test threshold 0.7 Acc 0.7534, AUC 0.8108246922492981, avg_entr 0.23380392789840698, f1 0.7533999681472778
t0.7_test_time 0.386626071000137
gc 0
Test threshold 0.8 Acc 0.748, AUC 0.8083502054214478, avg_entr 0.2537536323070526, f1 0.7480000257492065
t0.8_test_time 0.3738111069999377
gc 0
Test threshold 0.9 Acc 0.7376, AUC 0.8033708333969116, avg_entr 0.281799852848053, f1 0.7376000285148621
t0.9_test_time 0.3612508499998057
