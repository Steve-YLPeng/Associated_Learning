total count words 222751
vocab size 30000
train size 40000, valid size 5000, test size 5000
found 27937 words in glove
model: TransformerALsideText(
  (layers): ModuleList(
    (0): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=2, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=2, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
  (emb_layers): ModuleList(
    (0): Embedding(30000, 300)
    (1): Embedding(30000, 300)
    (2): Embedding(30000, 300)
    (3): Embedding(30000, 300)
    (4): Embedding(30000, 300)
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.multi_headed_attention.linears.0.weight 90000
layers.0.enc.f.multi_headed_attention.linears.0.bias 300
layers.0.enc.f.multi_headed_attention.linears.1.weight 90000
layers.0.enc.f.multi_headed_attention.linears.1.bias 300
layers.0.enc.f.multi_headed_attention.linears.2.weight 90000
layers.0.enc.f.multi_headed_attention.linears.2.bias 300
layers.0.enc.f.multi_headed_attention.linears.3.weight 90000
layers.0.enc.f.multi_headed_attention.linears.3.bias 300
layers.0.enc.f.feed_forward.w_1.weight 90000
layers.0.enc.f.feed_forward.w_1.bias 300
layers.0.enc.f.feed_forward.w_2.weight 90000
layers.0.enc.f.feed_forward.w_2.bias 300
layers.0.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.0.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.0.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.0.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.0.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.0.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.0.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.0.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.0.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.0.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.0.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.0.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.0.enc.f.encoder.norm.a 300
layers.0.enc.f.encoder.norm.b 300
layers.0.ae.g.0.weight 256
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 256
layers.0.ae.h.0.bias 2
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
emb_layers.0.weight 9000000
emb_layers.1.weight 9000000
emb_layers.2.weight 9000000
emb_layers.3.weight 9000000
emb_layers.4.weight 9000000
Total Trainable Params: 50758378
init_time 21.741067856
Start Training
gc 0
Train Epoch0 Acc 0.500275 (20011/40000), AUC 0.5031194686889648
ep0_train_time 23.385113593999996
Test Epoch0 layer0 Acc 0.5562, AUC 0.5789894461631775, avg_entr 0.7018811702728271, f1 0.5562000274658203
ep0_l0_test_time 0.2944093210000034
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 0
Test Epoch0 layer1 Acc 0.5032, AUC 0.5347280502319336, avg_entr 0.6920182704925537, f1 0.5031999945640564
ep0_l1_test_time 0.3611270740000023
Test Epoch0 layer2 Acc 0.508, AUC 0.5468602180480957, avg_entr 0.696969211101532, f1 0.5080000162124634
ep0_l2_test_time 0.4628262199999966
Test Epoch0 layer3 Acc 0.5154, AUC 0.5348552465438843, avg_entr 0.7006832957267761, f1 0.5153999924659729
ep0_l3_test_time 0.6138214839999989
Test Epoch0 layer4 Acc 0.5124, AUC 0.5046075582504272, avg_entr 0.7028956413269043, f1 0.5123999714851379
ep0_l4_test_time 0.8072008660000023
gc 0
Train Epoch1 Acc 0.511875 (20475/40000), AUC 0.518103837966919
ep1_train_time 23.093156542000003
Test Epoch1 layer0 Acc 0.5592, AUC 0.6261686682701111, avg_entr 0.6459490060806274, f1 0.5591999888420105
ep1_l0_test_time 0.2855619399999938
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 1
Test Epoch1 layer1 Acc 0.5952, AUC 0.6447783708572388, avg_entr 0.6775302886962891, f1 0.5952000021934509
ep1_l1_test_time 0.35501477400001136
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 1
Test Epoch1 layer2 Acc 0.5498, AUC 0.6482224464416504, avg_entr 0.6797398924827576, f1 0.5497999787330627
ep1_l2_test_time 0.46343659500000456
Test Epoch1 layer3 Acc 0.5452, AUC 0.6123218536376953, avg_entr 0.6918660402297974, f1 0.545199990272522
ep1_l3_test_time 0.6137883900000105
Test Epoch1 layer4 Acc 0.5004, AUC 0.5374506711959839, avg_entr 0.6920744180679321, f1 0.5004000067710876
ep1_l4_test_time 0.8083895729999995
gc 0
Train Epoch2 Acc 0.5407 (21628/40000), AUC 0.5588458776473999
ep2_train_time 22.998720274999997
Test Epoch2 layer0 Acc 0.6342, AUC 0.7057783007621765, avg_entr 0.5593518018722534, f1 0.6341999769210815
ep2_l0_test_time 0.2852345579999991
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 2
Test Epoch2 layer1 Acc 0.6726, AUC 0.7331396341323853, avg_entr 0.5558764934539795, f1 0.6725999712944031
ep2_l1_test_time 0.356109656000001
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 2
Test Epoch2 layer2 Acc 0.6636, AUC 0.7332771420478821, avg_entr 0.5798129439353943, f1 0.6636000275611877
ep2_l2_test_time 0.46341246800000135
Test Epoch2 layer3 Acc 0.65, AUC 0.7194465398788452, avg_entr 0.6230219602584839, f1 0.6499999761581421
ep2_l3_test_time 0.6120187339999887
Test Epoch2 layer4 Acc 0.5944, AUC 0.7135146856307983, avg_entr 0.6768041849136353, f1 0.5943999886512756
ep2_l4_test_time 0.8082480700000048
gc 0
Train Epoch3 Acc 0.617 (24680/40000), AUC 0.6608641147613525
ep3_train_time 23.123844032999997
Test Epoch3 layer0 Acc 0.6696, AUC 0.7461339235305786, avg_entr 0.49985620379447937, f1 0.6696000099182129
ep3_l0_test_time 0.2854152199999902
Test Epoch3 layer1 Acc 0.6912, AUC 0.7744720578193665, avg_entr 0.4401797950267792, f1 0.6912000179290771
ep3_l1_test_time 0.35375559899999587
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 3
Test Epoch3 layer2 Acc 0.682, AUC 0.7767699956893921, avg_entr 0.4317626357078552, f1 0.6819999814033508
ep3_l2_test_time 0.46391026499999555
Test Epoch3 layer3 Acc 0.669, AUC 0.777167797088623, avg_entr 0.4285256266593933, f1 0.6690000295639038
ep3_l3_test_time 0.6132636150000081
Test Epoch3 layer4 Acc 0.6556, AUC 0.7743944525718689, avg_entr 0.46635839343070984, f1 0.6556000113487244
ep3_l4_test_time 0.8079481070000014
gc 0
Train Epoch4 Acc 0.67745 (27098/40000), AUC 0.7428559064865112
ep4_train_time 23.007985613000017
Test Epoch4 layer0 Acc 0.6628, AUC 0.7638441920280457, avg_entr 0.42802363634109497, f1 0.6628000140190125
ep4_l0_test_time 0.28443288100001496
Test Epoch4 layer1 Acc 0.6912, AUC 0.7957171201705933, avg_entr 0.3927119970321655, f1 0.6912000179290771
ep4_l1_test_time 0.35370842500000776
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 4
Test Epoch4 layer2 Acc 0.688, AUC 0.8017383813858032, avg_entr 0.40313097834587097, f1 0.6880000233650208
ep4_l2_test_time 0.46471082099998284
Test Epoch4 layer3 Acc 0.688, AUC 0.8058416843414307, avg_entr 0.41846659779548645, f1 0.6880000233650208
ep4_l3_test_time 0.6135224999999878
Test Epoch4 layer4 Acc 0.6828, AUC 0.8043306469917297, avg_entr 0.45108428597450256, f1 0.6827999949455261
ep4_l4_test_time 0.8078380949999939
gc 0
Train Epoch5 Acc 0.71675 (28670/40000), AUC 0.7899715900421143
ep5_train_time 23.022159920000007
Test Epoch5 layer0 Acc 0.6916, AUC 0.7763996124267578, avg_entr 0.36281582713127136, f1 0.6916000247001648
ep5_l0_test_time 0.2836060110000176
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 5
Test Epoch5 layer1 Acc 0.7084, AUC 0.8103907704353333, avg_entr 0.3040228486061096, f1 0.7084000110626221
ep5_l1_test_time 0.3540434100000027
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 5
Test Epoch5 layer2 Acc 0.7084, AUC 0.8205611705780029, avg_entr 0.298786997795105, f1 0.7084000110626221
ep5_l2_test_time 0.46385922999999707
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 5
Test Epoch5 layer3 Acc 0.7072, AUC 0.8259540796279907, avg_entr 0.29505419731140137, f1 0.7071999907493591
ep5_l3_test_time 0.6151718320000157
Test Epoch5 layer4 Acc 0.6858, AUC 0.8243257999420166, avg_entr 0.2878124415874481, f1 0.6858000159263611
ep5_l4_test_time 0.8086446880000153
gc 0
Train Epoch6 Acc 0.74605 (29842/40000), AUC 0.8242194056510925
ep6_train_time 22.998984954999997
Test Epoch6 layer0 Acc 0.705, AUC 0.7862918376922607, avg_entr 0.3579190671443939, f1 0.7049999833106995
ep6_l0_test_time 0.2849922820000188
Test Epoch6 layer1 Acc 0.7294, AUC 0.8215052485466003, avg_entr 0.2962416708469391, f1 0.7293999791145325
ep6_l1_test_time 0.35407207899999094
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 6
Test Epoch6 layer2 Acc 0.7348, AUC 0.8352349400520325, avg_entr 0.27709296345710754, f1 0.7347999811172485
ep6_l2_test_time 0.4649929670000006
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 6
Test Epoch6 layer3 Acc 0.7324, AUC 0.8411350250244141, avg_entr 0.26985323429107666, f1 0.7323999404907227
ep6_l3_test_time 0.6179477009999914
Test Epoch6 layer4 Acc 0.7304, AUC 0.8404546976089478, avg_entr 0.27736765146255493, f1 0.7304000854492188
ep6_l4_test_time 0.8093582399999946
gc 0
Train Epoch7 Acc 0.762075 (30483/40000), AUC 0.8431386351585388
ep7_train_time 23.12928465599998
Test Epoch7 layer0 Acc 0.7062, AUC 0.7899873852729797, avg_entr 0.33653122186660767, f1 0.7062000036239624
ep7_l0_test_time 0.2855592779999938
Test Epoch7 layer1 Acc 0.7392, AUC 0.8288297653198242, avg_entr 0.3099483251571655, f1 0.7391999959945679
ep7_l1_test_time 0.3554732430000058
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 7
Test Epoch7 layer2 Acc 0.7514, AUC 0.8429679870605469, avg_entr 0.29668134450912476, f1 0.7513999938964844
ep7_l2_test_time 0.46547155600001133
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 7
Test Epoch7 layer3 Acc 0.7526, AUC 0.8494696021080017, avg_entr 0.2854519486427307, f1 0.7526000142097473
ep7_l3_test_time 0.6125170239999989
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 7
Test Epoch7 layer4 Acc 0.7418, AUC 0.85107421875, avg_entr 0.2859443426132202, f1 0.74180006980896
ep7_l4_test_time 0.8084417070000143
gc 0
Train Epoch8 Acc 0.783 (31320/40000), AUC 0.8635292053222656
ep8_train_time 23.02959813999999
Test Epoch8 layer0 Acc 0.6984, AUC 0.7937415838241577, avg_entr 0.29994726181030273, f1 0.6984000205993652
ep8_l0_test_time 0.2841281839999965
Test Epoch8 layer1 Acc 0.6968, AUC 0.8297361135482788, avg_entr 0.2191995531320572, f1 0.6967999935150146
ep8_l1_test_time 0.35410370999997554
Test Epoch8 layer2 Acc 0.6978, AUC 0.8433951139450073, avg_entr 0.21191652119159698, f1 0.6977999806404114
ep8_l2_test_time 0.46318846100001565
Test Epoch8 layer3 Acc 0.6916, AUC 0.8523915410041809, avg_entr 0.21612970530986786, f1 0.6916000247001648
ep8_l3_test_time 0.6127151860000026
Test Epoch8 layer4 Acc 0.6684, AUC 0.8529470562934875, avg_entr 0.2248767614364624, f1 0.66839998960495
ep8_l4_test_time 0.8098296590000018
gc 0
Train Epoch9 Acc 0.79275 (31710/40000), AUC 0.8742813467979431
ep9_train_time 23.15840708500002
Test Epoch9 layer0 Acc 0.71, AUC 0.7959302663803101, avg_entr 0.30024290084838867, f1 0.7099999785423279
ep9_l0_test_time 0.2841637889999902
Test Epoch9 layer1 Acc 0.7324, AUC 0.8297725915908813, avg_entr 0.2561609745025635, f1 0.7323999404907227
ep9_l1_test_time 0.35354130400003214
Test Epoch9 layer2 Acc 0.7394, AUC 0.8446918725967407, avg_entr 0.24631665647029877, f1 0.7394000291824341
ep9_l2_test_time 0.4635619929999848
Test Epoch9 layer3 Acc 0.7408, AUC 0.8549618721008301, avg_entr 0.22992292046546936, f1 0.7408000230789185
ep9_l3_test_time 0.6135324620000233
Test Epoch9 layer4 Acc 0.7352, AUC 0.8560758829116821, avg_entr 0.22536194324493408, f1 0.7351999282836914
ep9_l4_test_time 0.8074166770000488
gc 0
Train Epoch10 Acc 0.8186 (32744/40000), AUC 0.8984602689743042
ep10_train_time 23.00309522599997
Test Epoch10 layer0 Acc 0.6968, AUC 0.7973943948745728, avg_entr 0.272475004196167, f1 0.6967999935150146
ep10_l0_test_time 0.28415343899996515
Test Epoch10 layer1 Acc 0.705, AUC 0.8293496370315552, avg_entr 0.22582808136940002, f1 0.7049999833106995
ep10_l1_test_time 0.35588653899998235
Test Epoch10 layer2 Acc 0.6784, AUC 0.8423745632171631, avg_entr 0.1798793077468872, f1 0.6783999800682068
ep10_l2_test_time 0.46391181800004233
Test Epoch10 layer3 Acc 0.6496, AUC 0.8518756628036499, avg_entr 0.13581818342208862, f1 0.6496000289916992
ep10_l3_test_time 0.6131126340000037
Test Epoch10 layer4 Acc 0.6268, AUC 0.8516268730163574, avg_entr 0.10694892704486847, f1 0.626800000667572
ep10_l4_test_time 0.8089487480000344
gc 0
Train Epoch11 Acc 0.832825 (33313/40000), AUC 0.9105685949325562
ep11_train_time 23.06251970699998
Test Epoch11 layer0 Acc 0.7108, AUC 0.7963753938674927, avg_entr 0.2610132694244385, f1 0.7107999920845032
ep11_l0_test_time 0.2965111219999699
Test Epoch11 layer1 Acc 0.7292, AUC 0.8365658521652222, avg_entr 0.22537221014499664, f1 0.7291999459266663
ep11_l1_test_time 0.36379281699998955
Test Epoch11 layer2 Acc 0.7308, AUC 0.8515771627426147, avg_entr 0.18982520699501038, f1 0.7307999730110168
ep11_l2_test_time 0.47962808199997653
Test Epoch11 layer3 Acc 0.7242, AUC 0.8583780527114868, avg_entr 0.15838605165481567, f1 0.7242000699043274
ep11_l3_test_time 0.6142072779999808
Test Epoch11 layer4 Acc 0.7126, AUC 0.8631064891815186, avg_entr 0.13994084298610687, f1 0.7125999927520752
ep11_l4_test_time 0.8071269110000117
gc 0
Train Epoch12 Acc 0.86215 (34486/40000), AUC 0.935417652130127
ep12_train_time 23.093636462000006
Test Epoch12 layer0 Acc 0.7164, AUC 0.7981194853782654, avg_entr 0.2513846158981323, f1 0.7164000272750854
ep12_l0_test_time 0.2851179070000285
Test Epoch12 layer1 Acc 0.7464, AUC 0.836428165435791, avg_entr 0.19612851738929749, f1 0.7463999390602112
ep12_l1_test_time 0.35629097399998955
Test Epoch12 layer2 Acc 0.7566, AUC 0.8470677137374878, avg_entr 0.1440960317850113, f1 0.756600022315979
ep12_l2_test_time 0.4662391740000089
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 12
Test Epoch12 layer3 Acc 0.7578, AUC 0.8577134609222412, avg_entr 0.13527144491672516, f1 0.7577999830245972
ep12_l3_test_time 0.6149814120000201
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 12
Test Epoch12 layer4 Acc 0.7568, AUC 0.8589344024658203, avg_entr 0.12838196754455566, f1 0.7567999958992004
ep12_l4_test_time 0.8077997440000217
gc 0
Train Epoch13 Acc 0.8672 (34688/40000), AUC 0.9380573630332947
ep13_train_time 23.01105055200003
Test Epoch13 layer0 Acc 0.7128, AUC 0.7900633811950684, avg_entr 0.2343701720237732, f1 0.7128000259399414
ep13_l0_test_time 0.2837832640000215
Test Epoch13 layer1 Acc 0.7418, AUC 0.8288822174072266, avg_entr 0.1897018998861313, f1 0.74180006980896
ep13_l1_test_time 0.3540711669999723
Test Epoch13 layer2 Acc 0.7556, AUC 0.8436846733093262, avg_entr 0.12500406801700592, f1 0.7555999755859375
ep13_l2_test_time 0.4624239780000039
Test Epoch13 layer3 Acc 0.761, AUC 0.8540756702423096, avg_entr 0.1183483675122261, f1 0.7610000371932983
ep13_l3_test_time 0.6123552430000245
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 13
Test Epoch13 layer4 Acc 0.7614, AUC 0.8560454249382019, avg_entr 0.11445047706365585, f1 0.7613999843597412
ep13_l4_test_time 0.8096305509999979
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 13
gc 0
Train Epoch14 Acc 0.888175 (35527/40000), AUC 0.9529644250869751
ep14_train_time 23.07741833800003
Test Epoch14 layer0 Acc 0.7126, AUC 0.7908724546432495, avg_entr 0.22250881791114807, f1 0.7125999927520752
ep14_l0_test_time 0.2849392420000072
Test Epoch14 layer1 Acc 0.7438, AUC 0.8329369425773621, avg_entr 0.19417910277843475, f1 0.7437999248504639
ep14_l1_test_time 0.35476779399999714
Test Epoch14 layer2 Acc 0.7586, AUC 0.8475803136825562, avg_entr 0.12789811193943024, f1 0.7585999965667725
ep14_l2_test_time 0.46482438999998976
Test Epoch14 layer3 Acc 0.7672, AUC 0.8576085567474365, avg_entr 0.12550196051597595, f1 0.7671999931335449
ep14_l3_test_time 0.6152975019999758
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 14
Test Epoch14 layer4 Acc 0.7678, AUC 0.8601150512695312, avg_entr 0.11845600605010986, f1 0.767799973487854
ep14_l4_test_time 0.8100112820000049
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 14
gc 0
Train Epoch15 Acc 0.903375 (36135/40000), AUC 0.9636845588684082
ep15_train_time 23.075906851000013
Test Epoch15 layer0 Acc 0.713, AUC 0.7879410982131958, avg_entr 0.2085471749305725, f1 0.7130000591278076
ep15_l0_test_time 0.28522973600001933
Test Epoch15 layer1 Acc 0.7458, AUC 0.8255500793457031, avg_entr 0.16759273409843445, f1 0.7458000183105469
ep15_l1_test_time 0.35363362999999026
Test Epoch15 layer2 Acc 0.7516, AUC 0.8391871452331543, avg_entr 0.10340683907270432, f1 0.7516000270843506
ep15_l2_test_time 0.4638046769999846
Test Epoch15 layer3 Acc 0.7584, AUC 0.8483977317810059, avg_entr 0.09979156404733658, f1 0.758400022983551
ep15_l3_test_time 0.6130129679999641
Test Epoch15 layer4 Acc 0.76, AUC 0.8513545393943787, avg_entr 0.09364631026983261, f1 0.7599999904632568
ep15_l4_test_time 0.8078804449999666
gc 0
Train Epoch16 Acc 0.9229 (36916/40000), AUC 0.9757273197174072
ep16_train_time 23.053351700000007
Test Epoch16 layer0 Acc 0.7076, AUC 0.7826303839683533, avg_entr 0.20014332234859467, f1 0.707599937915802
ep16_l0_test_time 0.2844582720000517
Test Epoch16 layer1 Acc 0.7336, AUC 0.8192281723022461, avg_entr 0.1546960175037384, f1 0.7335999608039856
ep16_l1_test_time 0.35405858600000784
Test Epoch16 layer2 Acc 0.74, AUC 0.8351655006408691, avg_entr 0.08790222555398941, f1 0.7400000095367432
ep16_l2_test_time 0.46277350799999795
Test Epoch16 layer3 Acc 0.7486, AUC 0.8446429967880249, avg_entr 0.08694185316562653, f1 0.7486000061035156
ep16_l3_test_time 0.6153938049999965
Test Epoch16 layer4 Acc 0.7534, AUC 0.8500292301177979, avg_entr 0.08089884370565414, f1 0.7533999681472778
ep16_l4_test_time 0.8076763419999793
gc 0
Train Epoch17 Acc 0.927225 (37089/40000), AUC 0.9767293334007263
ep17_train_time 23.00660012200001
Test Epoch17 layer0 Acc 0.7136, AUC 0.7825820446014404, avg_entr 0.1855718195438385, f1 0.7135999798774719
ep17_l0_test_time 0.2841932290000386
Test Epoch17 layer1 Acc 0.7316, AUC 0.8128218650817871, avg_entr 0.12954695522785187, f1 0.7316000461578369
ep17_l1_test_time 0.3540949220000016
Test Epoch17 layer2 Acc 0.7438, AUC 0.8268082141876221, avg_entr 0.0728568509221077, f1 0.7437999248504639
ep17_l2_test_time 0.4634339540000383
Test Epoch17 layer3 Acc 0.752, AUC 0.8365989923477173, avg_entr 0.07178919017314911, f1 0.7519999742507935
ep17_l3_test_time 0.6172525819999919
Test Epoch17 layer4 Acc 0.756, AUC 0.8411617875099182, avg_entr 0.06568086892366409, f1 0.7559999227523804
ep17_l4_test_time 0.8169308459999911
gc 0
Train Epoch18 Acc 0.938725 (37549/40000), AUC 0.982418417930603
ep18_train_time 23.01570729200006
Test Epoch18 layer0 Acc 0.708, AUC 0.7802262306213379, avg_entr 0.18400037288665771, f1 0.7080000638961792
ep18_l0_test_time 0.2836143389999961
Test Epoch18 layer1 Acc 0.726, AUC 0.8105969429016113, avg_entr 0.10384422540664673, f1 0.7260000109672546
ep18_l1_test_time 0.3555300799999941
Test Epoch18 layer2 Acc 0.7396, AUC 0.8242413997650146, avg_entr 0.07051193714141846, f1 0.7396000027656555
ep18_l2_test_time 0.46357232699995166
Test Epoch18 layer3 Acc 0.7506, AUC 0.8333163261413574, avg_entr 0.0730481669306755, f1 0.7505999803543091
ep18_l3_test_time 0.6134065050000572
Test Epoch18 layer4 Acc 0.7536, AUC 0.8379956483840942, avg_entr 0.06578036397695541, f1 0.7535999417304993
ep18_l4_test_time 0.8083583380000618
gc 0
Train Epoch19 Acc 0.944225 (37769/40000), AUC 0.9854917526245117
ep19_train_time 23.012791279999988
Test Epoch19 layer0 Acc 0.7098, AUC 0.7807437181472778, avg_entr 0.1805044263601303, f1 0.7098000049591064
ep19_l0_test_time 0.28494264299990846
Test Epoch19 layer1 Acc 0.727, AUC 0.8083207607269287, avg_entr 0.08795066177845001, f1 0.7269999980926514
ep19_l1_test_time 0.355734042999984
Test Epoch19 layer2 Acc 0.7344, AUC 0.8212375640869141, avg_entr 0.06556686758995056, f1 0.7343999147415161
ep19_l2_test_time 0.4642165110000178
Test Epoch19 layer3 Acc 0.7446, AUC 0.8308389186859131, avg_entr 0.06438982486724854, f1 0.7445999979972839
ep19_l3_test_time 0.614528048000011
Test Epoch19 layer4 Acc 0.7498, AUC 0.8357648253440857, avg_entr 0.05811219662427902, f1 0.7498000264167786
ep19_l4_test_time 0.8096299649999992
gc 0
Train Epoch20 Acc 0.950675 (38027/40000), AUC 0.9885164499282837
ep20_train_time 23.06952951400001
Test Epoch20 layer0 Acc 0.7052, AUC 0.778038501739502, avg_entr 0.18118880689144135, f1 0.7052000164985657
ep20_l0_test_time 0.2841811109999526
Test Epoch20 layer1 Acc 0.7262, AUC 0.8076887130737305, avg_entr 0.07887361943721771, f1 0.7261999845504761
ep20_l1_test_time 0.3535181779999448
Test Epoch20 layer2 Acc 0.7352, AUC 0.8228289484977722, avg_entr 0.06328689306974411, f1 0.7351999282836914
ep20_l2_test_time 0.4638643120000552
Test Epoch20 layer3 Acc 0.7392, AUC 0.831208348274231, avg_entr 0.06268767267465591, f1 0.7391999959945679
ep20_l3_test_time 0.6137461270000131
Test Epoch20 layer4 Acc 0.7468, AUC 0.8385690450668335, avg_entr 0.055736541748046875, f1 0.7468000054359436
ep20_l4_test_time 0.810848913999962
gc 0
Train Epoch21 Acc 0.956525 (38261/40000), AUC 0.989517331123352
ep21_train_time 23.13394265699992
Test Epoch21 layer0 Acc 0.7082, AUC 0.777781069278717, avg_entr 0.17482326924800873, f1 0.7081999778747559
ep21_l0_test_time 0.2854007029999366
Test Epoch21 layer1 Acc 0.7262, AUC 0.8030373454093933, avg_entr 0.07118431478738785, f1 0.7261999845504761
ep21_l1_test_time 0.356559749999974
Test Epoch21 layer2 Acc 0.732, AUC 0.8186604976654053, avg_entr 0.05287704989314079, f1 0.7319999933242798
ep21_l2_test_time 0.46286593399997855
Test Epoch21 layer3 Acc 0.74, AUC 0.8230938911437988, avg_entr 0.0515112541615963, f1 0.7400000095367432
ep21_l3_test_time 0.6131562029999031
Test Epoch21 layer4 Acc 0.7436, AUC 0.8287550210952759, avg_entr 0.04629966616630554, f1 0.7436000108718872
ep21_l4_test_time 0.807711157999961
gc 0
Train Epoch22 Acc 0.959625 (38385/40000), AUC 0.9912116527557373
ep22_train_time 23.287102549999986
Test Epoch22 layer0 Acc 0.7068, AUC 0.7764021158218384, avg_entr 0.17398621141910553, f1 0.7067999839782715
ep22_l0_test_time 0.2874041519999082
Test Epoch22 layer1 Acc 0.7182, AUC 0.7957704067230225, avg_entr 0.06517110019922256, f1 0.7182000279426575
ep22_l1_test_time 0.36040321400002995
Test Epoch22 layer2 Acc 0.73, AUC 0.814858078956604, avg_entr 0.049588363617658615, f1 0.7300000190734863
ep22_l2_test_time 0.4652895809999791
Test Epoch22 layer3 Acc 0.7376, AUC 0.8194146156311035, avg_entr 0.04855542629957199, f1 0.7376000285148621
ep22_l3_test_time 0.6143915170000582
Test Epoch22 layer4 Acc 0.7418, AUC 0.8261301517486572, avg_entr 0.04302124306559563, f1 0.74180006980896
ep22_l4_test_time 0.81119104600009
gc 0
Train Epoch23 Acc 0.960375 (38415/40000), AUC 0.991740345954895
ep23_train_time 23.100050668999984
Test Epoch23 layer0 Acc 0.7046, AUC 0.7744277715682983, avg_entr 0.16996869444847107, f1 0.7045999765396118
ep23_l0_test_time 0.28924465900001906
Test Epoch23 layer1 Acc 0.7234, AUC 0.7973142862319946, avg_entr 0.06668484210968018, f1 0.7233999967575073
ep23_l1_test_time 0.35413244899996243
Test Epoch23 layer2 Acc 0.7404, AUC 0.8192701935768127, avg_entr 0.05837108939886093, f1 0.7404000163078308
ep23_l2_test_time 0.46483683500002826
Test Epoch23 layer3 Acc 0.7474, AUC 0.8237029314041138, avg_entr 0.056903865188360214, f1 0.7473999857902527
ep23_l3_test_time 0.6133825859999433
Test Epoch23 layer4 Acc 0.754, AUC 0.8305312991142273, avg_entr 0.05124339461326599, f1 0.7540000081062317
ep23_l4_test_time 0.8098792330000606
gc 0
Train Epoch24 Acc 0.965125 (38605/40000), AUC 0.9931193590164185
ep24_train_time 23.120647385999973
Test Epoch24 layer0 Acc 0.7084, AUC 0.7751497030258179, avg_entr 0.1678391695022583, f1 0.7084000110626221
ep24_l0_test_time 0.28295509700001276
Test Epoch24 layer1 Acc 0.7218, AUC 0.7945489883422852, avg_entr 0.06083812937140465, f1 0.7217999696731567
ep24_l1_test_time 0.35312581999994563
Test Epoch24 layer2 Acc 0.7332, AUC 0.8170374631881714, avg_entr 0.046172816306352615, f1 0.7332000136375427
ep24_l2_test_time 0.4625566949999893
Test Epoch24 layer3 Acc 0.739, AUC 0.8195575475692749, avg_entr 0.044473323971033096, f1 0.7390000820159912
ep24_l3_test_time 0.6149503200000481
Test Epoch24 layer4 Acc 0.7462, AUC 0.8272715210914612, avg_entr 0.04126384109258652, f1 0.7462000250816345
ep24_l4_test_time 0.8080332399999861
gc 0
Train Epoch25 Acc 0.966075 (38643/40000), AUC 0.9937233924865723
ep25_train_time 23.074009608000097
Test Epoch25 layer0 Acc 0.7062, AUC 0.7732876539230347, avg_entr 0.16677990555763245, f1 0.7062000036239624
ep25_l0_test_time 0.2844997060000196
Test Epoch25 layer1 Acc 0.7218, AUC 0.7905112504959106, avg_entr 0.05788808688521385, f1 0.7217999696731567
ep25_l1_test_time 0.3542734909999581
Test Epoch25 layer2 Acc 0.7346, AUC 0.8125973343849182, avg_entr 0.04853548854589462, f1 0.7345999479293823
ep25_l2_test_time 0.46322619700004
Test Epoch25 layer3 Acc 0.7396, AUC 0.8184351921081543, avg_entr 0.04543669894337654, f1 0.7396000027656555
ep25_l3_test_time 0.6147368629999619
Test Epoch25 layer4 Acc 0.7434, AUC 0.8237260580062866, avg_entr 0.041747014969587326, f1 0.743399977684021
ep25_l4_test_time 0.8082220779999716
gc 0
Train Epoch26 Acc 0.967675 (38707/40000), AUC 0.9940828084945679
ep26_train_time 23.012717468999995
Test Epoch26 layer0 Acc 0.7048, AUC 0.772532045841217, avg_entr 0.16557037830352783, f1 0.704800009727478
ep26_l0_test_time 0.28516892799996185
Test Epoch26 layer1 Acc 0.7162, AUC 0.7900663614273071, avg_entr 0.055895399302244186, f1 0.7161999940872192
ep26_l1_test_time 0.35516531600001144
Test Epoch26 layer2 Acc 0.7342, AUC 0.8121774196624756, avg_entr 0.045464348047971725, f1 0.7342000007629395
ep26_l2_test_time 0.4654543860000331
Test Epoch26 layer3 Acc 0.74, AUC 0.8166826963424683, avg_entr 0.043146174401044846, f1 0.7400000095367432
ep26_l3_test_time 0.6161138950000122
Test Epoch26 layer4 Acc 0.7444, AUC 0.8225960731506348, avg_entr 0.039393775165081024, f1 0.7444000244140625
ep26_l4_test_time 0.8082959250000386
gc 0
Train Epoch27 Acc 0.967875 (38715/40000), AUC 0.9942114353179932
ep27_train_time 23.04225859600001
Test Epoch27 layer0 Acc 0.7044, AUC 0.7723733186721802, avg_entr 0.16457705199718475, f1 0.7044000029563904
ep27_l0_test_time 0.28430888799994136
Test Epoch27 layer1 Acc 0.7174, AUC 0.7893413305282593, avg_entr 0.055939268320798874, f1 0.717400074005127
ep27_l1_test_time 0.36215067900002396
Test Epoch27 layer2 Acc 0.733, AUC 0.8116947412490845, avg_entr 0.04664965346455574, f1 0.7329999804496765
ep27_l2_test_time 0.4655984690000423
Test Epoch27 layer3 Acc 0.7366, AUC 0.8156023621559143, avg_entr 0.04458053782582283, f1 0.7365999817848206
ep27_l3_test_time 0.6147635500000206
Test Epoch27 layer4 Acc 0.7398, AUC 0.8208504915237427, avg_entr 0.03815391659736633, f1 0.7398000359535217
ep27_l4_test_time 0.8085604629999352
gc 0
Train Epoch28 Acc 0.97075 (38830/40000), AUC 0.9951469898223877
ep28_train_time 23.046081535999974
Test Epoch28 layer0 Acc 0.7042, AUC 0.771872341632843, avg_entr 0.1663838028907776, f1 0.704200029373169
ep28_l0_test_time 0.2850598640000044
Test Epoch28 layer1 Acc 0.716, AUC 0.786459743976593, avg_entr 0.052204858511686325, f1 0.7160000205039978
ep28_l1_test_time 0.35548177399994074
Test Epoch28 layer2 Acc 0.7304, AUC 0.8104009628295898, avg_entr 0.04428168013691902, f1 0.7304000854492188
ep28_l2_test_time 0.46565356799999336
Test Epoch28 layer3 Acc 0.7352, AUC 0.8142076730728149, avg_entr 0.042395539581775665, f1 0.7351999282836914
ep28_l3_test_time 0.6141593280000279
Test Epoch28 layer4 Acc 0.7434, AUC 0.8207951784133911, avg_entr 0.04017869755625725, f1 0.743399977684021
ep28_l4_test_time 0.8087570690000803
gc 0
Train Epoch29 Acc 0.971825 (38873/40000), AUC 0.9950580596923828
ep29_train_time 23.011171813000033
Test Epoch29 layer0 Acc 0.7016, AUC 0.7716774940490723, avg_entr 0.16220508515834808, f1 0.7016000151634216
ep29_l0_test_time 0.2856770220000726
Test Epoch29 layer1 Acc 0.7192, AUC 0.787247359752655, avg_entr 0.05258917808532715, f1 0.719200074672699
ep29_l1_test_time 0.3558075879999478
Test Epoch29 layer2 Acc 0.7352, AUC 0.810128927230835, avg_entr 0.04411386325955391, f1 0.7351999282836914
ep29_l2_test_time 0.4686709540000038
Test Epoch29 layer3 Acc 0.7408, AUC 0.8148128986358643, avg_entr 0.04041403532028198, f1 0.7408000230789185
ep29_l3_test_time 0.6145275909999555
Test Epoch29 layer4 Acc 0.7436, AUC 0.8205420970916748, avg_entr 0.03824508190155029, f1 0.7436000108718872
ep29_l4_test_time 0.8079456309999387
gc 0
Train Epoch30 Acc 0.972325 (38893/40000), AUC 0.9947776794433594
ep30_train_time 23.06653377300006
Test Epoch30 layer0 Acc 0.7022, AUC 0.7713505625724792, avg_entr 0.16107943654060364, f1 0.7021999955177307
ep30_l0_test_time 0.287886565000008
Test Epoch30 layer1 Acc 0.7194, AUC 0.7854229211807251, avg_entr 0.05165977030992508, f1 0.7193999886512756
ep30_l1_test_time 0.35459228399997755
Test Epoch30 layer2 Acc 0.7308, AUC 0.8073693513870239, avg_entr 0.04302982613444328, f1 0.7307999730110168
ep30_l2_test_time 0.4634637130000101
Test Epoch30 layer3 Acc 0.7358, AUC 0.8137627840042114, avg_entr 0.03969183191657066, f1 0.73580002784729
ep30_l3_test_time 0.6128012450000142
Test Epoch30 layer4 Acc 0.7404, AUC 0.8177181482315063, avg_entr 0.03658483177423477, f1 0.7404000163078308
ep30_l4_test_time 0.8088127239999494
gc 0
Train Epoch31 Acc 0.972125 (38885/40000), AUC 0.9952588677406311
ep31_train_time 23.115058808999947
Test Epoch31 layer0 Acc 0.7026, AUC 0.7707741856575012, avg_entr 0.161612406373024, f1 0.7026000022888184
ep31_l0_test_time 0.2858132939999223
Test Epoch31 layer1 Acc 0.716, AUC 0.7827169895172119, avg_entr 0.05098256841301918, f1 0.7160000205039978
ep31_l1_test_time 0.35589011400008985
Test Epoch31 layer2 Acc 0.7324, AUC 0.8063695430755615, avg_entr 0.041341617703437805, f1 0.7323999404907227
ep31_l2_test_time 0.4650510349999877
Test Epoch31 layer3 Acc 0.7372, AUC 0.8119205236434937, avg_entr 0.03831746056675911, f1 0.7372000217437744
ep31_l3_test_time 0.6154904589999433
Test Epoch31 layer4 Acc 0.7434, AUC 0.8179975152015686, avg_entr 0.035535987466573715, f1 0.743399977684021
ep31_l4_test_time 0.8093899099999362
gc 0
Train Epoch32 Acc 0.97485 (38994/40000), AUC 0.9957014322280884
ep32_train_time 23.07409905599991
Test Epoch32 layer0 Acc 0.7012, AUC 0.7706586122512817, avg_entr 0.16200995445251465, f1 0.701200008392334
ep32_l0_test_time 0.2846021350000001
Test Epoch32 layer1 Acc 0.7166, AUC 0.7828706502914429, avg_entr 0.04992368817329407, f1 0.7165999412536621
ep32_l1_test_time 0.355164567000088
Test Epoch32 layer2 Acc 0.7348, AUC 0.8087822198867798, avg_entr 0.04176882654428482, f1 0.7347999811172485
ep32_l2_test_time 0.4666357699999253
Test Epoch32 layer3 Acc 0.7382, AUC 0.8117056488990784, avg_entr 0.03878692165017128, f1 0.7382000684738159
ep32_l3_test_time 0.6141557820000116
Test Epoch32 layer4 Acc 0.7436, AUC 0.8173285722732544, avg_entr 0.03708966076374054, f1 0.7436000108718872
ep32_l4_test_time 0.8093360780000012
gc 0
Train Epoch33 Acc 0.9733 (38932/40000), AUC 0.995518684387207
ep33_train_time 23.25067956999999
Test Epoch33 layer0 Acc 0.7004, AUC 0.7704617977142334, avg_entr 0.1615922600030899, f1 0.7003999948501587
ep33_l0_test_time 0.28413879499998984
Test Epoch33 layer1 Acc 0.717, AUC 0.78232741355896, avg_entr 0.047767333686351776, f1 0.7169999480247498
ep33_l1_test_time 0.35368263200007277
Test Epoch33 layer2 Acc 0.7314, AUC 0.8083990812301636, avg_entr 0.0411396287381649, f1 0.7314000129699707
ep33_l2_test_time 0.464503542999978
Test Epoch33 layer3 Acc 0.7358, AUC 0.8116525411605835, avg_entr 0.037728119641542435, f1 0.73580002784729
ep33_l3_test_time 0.6145214240000314
Test Epoch33 layer4 Acc 0.74, AUC 0.8172710537910461, avg_entr 0.03478525951504707, f1 0.7400000095367432
ep33_l4_test_time 0.8080514529999618
gc 0
Train Epoch34 Acc 0.9737 (38948/40000), AUC 0.9957623481750488
ep34_train_time 23.07233505900001
Test Epoch34 layer0 Acc 0.702, AUC 0.7704954147338867, avg_entr 0.16078542172908783, f1 0.7020000219345093
ep34_l0_test_time 0.28555440499997076
Test Epoch34 layer1 Acc 0.7192, AUC 0.7835155725479126, avg_entr 0.04800005629658699, f1 0.719200074672699
ep34_l1_test_time 0.3560516530000086
Test Epoch34 layer2 Acc 0.7322, AUC 0.8096189498901367, avg_entr 0.040776271373033524, f1 0.732200026512146
ep34_l2_test_time 0.46458552199999303
Test Epoch34 layer3 Acc 0.7364, AUC 0.8115200996398926, avg_entr 0.03769809752702713, f1 0.7364000082015991
ep34_l3_test_time 0.6159024419999923
Test Epoch34 layer4 Acc 0.7424, AUC 0.818196177482605, avg_entr 0.03546908125281334, f1 0.742400050163269
ep34_l4_test_time 0.8132201089999853
gc 0
Train Epoch35 Acc 0.974175 (38967/40000), AUC 0.9956767559051514
ep35_train_time 23.04752293499996
Test Epoch35 layer0 Acc 0.7016, AUC 0.7698971033096313, avg_entr 0.16044899821281433, f1 0.7016000151634216
ep35_l0_test_time 0.28742953099992974
Test Epoch35 layer1 Acc 0.7176, AUC 0.7823611497879028, avg_entr 0.047765105962753296, f1 0.7175999879837036
ep35_l1_test_time 0.3547912680000991
Test Epoch35 layer2 Acc 0.7348, AUC 0.8082004189491272, avg_entr 0.04070177674293518, f1 0.7347999811172485
ep35_l2_test_time 0.46287852099999327
Test Epoch35 layer3 Acc 0.7362, AUC 0.8113887906074524, avg_entr 0.036842845380306244, f1 0.7361999750137329
ep35_l3_test_time 0.613270961000012
Test Epoch35 layer4 Acc 0.7424, AUC 0.8148928284645081, avg_entr 0.034124843776226044, f1 0.742400050163269
ep35_l4_test_time 0.8073037870000235
gc 0
Train Epoch36 Acc 0.973775 (38951/40000), AUC 0.9957958459854126
ep36_train_time 23.047052391999955
Test Epoch36 layer0 Acc 0.701, AUC 0.769892692565918, avg_entr 0.1588950753211975, f1 0.7009999752044678
ep36_l0_test_time 0.28367390400001113
Test Epoch36 layer1 Acc 0.7166, AUC 0.7827985882759094, avg_entr 0.048667725175619125, f1 0.7165999412536621
ep36_l1_test_time 0.3554736570000614
Test Epoch36 layer2 Acc 0.732, AUC 0.8060684204101562, avg_entr 0.03870321065187454, f1 0.7319999933242798
ep36_l2_test_time 0.4641531099999838
Test Epoch36 layer3 Acc 0.7342, AUC 0.8104203939437866, avg_entr 0.03540632128715515, f1 0.7342000007629395
ep36_l3_test_time 0.6130916989999378
Test Epoch36 layer4 Acc 0.7418, AUC 0.8140187859535217, avg_entr 0.032446522265672684, f1 0.74180006980896
ep36_l4_test_time 0.8070250779999242
gc 0
Train Epoch37 Acc 0.9756 (39024/40000), AUC 0.99605393409729
ep37_train_time 23.258280546000037
Test Epoch37 layer0 Acc 0.7004, AUC 0.7698531746864319, avg_entr 0.1592629998922348, f1 0.7003999948501587
ep37_l0_test_time 0.2840288730000111
Test Epoch37 layer1 Acc 0.7174, AUC 0.7831363677978516, avg_entr 0.04836183041334152, f1 0.717400074005127
ep37_l1_test_time 0.3567230280000331
Test Epoch37 layer2 Acc 0.7324, AUC 0.8069148063659668, avg_entr 0.038636162877082825, f1 0.7323999404907227
ep37_l2_test_time 0.46379572000000735
Test Epoch37 layer3 Acc 0.7354, AUC 0.8114495277404785, avg_entr 0.03529076650738716, f1 0.7354000210762024
ep37_l3_test_time 0.6334887780000145
Test Epoch37 layer4 Acc 0.7422, AUC 0.8148740530014038, avg_entr 0.03266451507806778, f1 0.7422000169754028
ep37_l4_test_time 0.8086721539999644
gc 0
Train Epoch38 Acc 0.97475 (38990/40000), AUC 0.9959944486618042
ep38_train_time 23.024917307999885
Test Epoch38 layer0 Acc 0.7022, AUC 0.7698579430580139, avg_entr 0.1599830538034439, f1 0.7021999955177307
ep38_l0_test_time 0.28542514199989455
Test Epoch38 layer1 Acc 0.716, AUC 0.7819551229476929, avg_entr 0.047141436487436295, f1 0.7160000205039978
ep38_l1_test_time 0.3561756779999996
Test Epoch38 layer2 Acc 0.7338, AUC 0.8072028756141663, avg_entr 0.04004674404859543, f1 0.7338000535964966
ep38_l2_test_time 0.4647766609998598
Test Epoch38 layer3 Acc 0.7374, AUC 0.8101509809494019, avg_entr 0.035931043326854706, f1 0.7373999953269958
ep38_l3_test_time 0.6126799599999231
Test Epoch38 layer4 Acc 0.7392, AUC 0.8164283037185669, avg_entr 0.03367767482995987, f1 0.7391999959945679
ep38_l4_test_time 0.8101240940000025
gc 0
Train Epoch39 Acc 0.975075 (39003/40000), AUC 0.9958820939064026
ep39_train_time 23.089483599000005
Test Epoch39 layer0 Acc 0.7012, AUC 0.7697552442550659, avg_entr 0.1597222536802292, f1 0.701200008392334
ep39_l0_test_time 0.2847980290000578
Test Epoch39 layer1 Acc 0.7186, AUC 0.7833386063575745, avg_entr 0.04812437668442726, f1 0.7185999751091003
ep39_l1_test_time 0.3554814419999275
Test Epoch39 layer2 Acc 0.7368, AUC 0.8076242208480835, avg_entr 0.040066178888082504, f1 0.7368000149726868
ep39_l2_test_time 0.4634480430001986
Test Epoch39 layer3 Acc 0.7382, AUC 0.8118497133255005, avg_entr 0.03602869436144829, f1 0.7382000684738159
ep39_l3_test_time 0.6138328779998119
Test Epoch39 layer4 Acc 0.7424, AUC 0.8169800043106079, avg_entr 0.033547282218933105, f1 0.742400050163269
ep39_l4_test_time 0.8090280989999883
gc 0
Train Epoch40 Acc 0.97565 (39026/40000), AUC 0.9960825443267822
ep40_train_time 23.1136279870002
Test Epoch40 layer0 Acc 0.7014, AUC 0.7696045637130737, avg_entr 0.15956883132457733, f1 0.7013999819755554
ep40_l0_test_time 0.2862784039998587
Test Epoch40 layer1 Acc 0.7162, AUC 0.7819833755493164, avg_entr 0.047232069075107574, f1 0.7161999940872192
ep40_l1_test_time 0.35536267599991334
Test Epoch40 layer2 Acc 0.7336, AUC 0.8060944080352783, avg_entr 0.038737703114748, f1 0.7335999608039856
ep40_l2_test_time 0.46795183999984147
Test Epoch40 layer3 Acc 0.7356, AUC 0.8105359673500061, avg_entr 0.03477463498711586, f1 0.7355999946594238
ep40_l3_test_time 0.6158906850000676
Test Epoch40 layer4 Acc 0.7408, AUC 0.8150839805603027, avg_entr 0.031848400831222534, f1 0.7408000230789185
ep40_l4_test_time 0.8086372689999735
gc 0
Train Epoch41 Acc 0.976125 (39045/40000), AUC 0.9961141347885132
ep41_train_time 23.023902199000077
Test Epoch41 layer0 Acc 0.7016, AUC 0.7696899175643921, avg_entr 0.15885721147060394, f1 0.7016000151634216
ep41_l0_test_time 0.28445573699991655
Test Epoch41 layer1 Acc 0.7186, AUC 0.78179931640625, avg_entr 0.04703603312373161, f1 0.7185999751091003
ep41_l1_test_time 0.3537175030000981
Test Epoch41 layer2 Acc 0.7344, AUC 0.8066000938415527, avg_entr 0.03861641138792038, f1 0.7343999147415161
ep41_l2_test_time 0.464195552000092
Test Epoch41 layer3 Acc 0.7372, AUC 0.810972273349762, avg_entr 0.03464856743812561, f1 0.7372000217437744
ep41_l3_test_time 0.6145082559999082
Test Epoch41 layer4 Acc 0.7422, AUC 0.8157830834388733, avg_entr 0.03200003132224083, f1 0.7422000169754028
ep41_l4_test_time 0.8092699460000858
gc 0
Train Epoch42 Acc 0.975575 (39023/40000), AUC 0.9961491823196411
ep42_train_time 23.03679686800001
Test Epoch42 layer0 Acc 0.7006, AUC 0.7697187662124634, avg_entr 0.15902121365070343, f1 0.7006000280380249
ep42_l0_test_time 0.2837253420000252
Test Epoch42 layer1 Acc 0.7184, AUC 0.782679557800293, avg_entr 0.04696283116936684, f1 0.7184000015258789
ep42_l1_test_time 0.35336333600002945
Test Epoch42 layer2 Acc 0.7338, AUC 0.8074777722358704, avg_entr 0.038734763860702515, f1 0.7338000535964966
ep42_l2_test_time 0.4647720959999333
Test Epoch42 layer3 Acc 0.7364, AUC 0.8115187883377075, avg_entr 0.03482730686664581, f1 0.7364000082015991
ep42_l3_test_time 0.6137367939998057
Test Epoch42 layer4 Acc 0.7434, AUC 0.8163140416145325, avg_entr 0.03224111720919609, f1 0.743399977684021
ep42_l4_test_time 0.8114696310001364
gc 0
Train Epoch43 Acc 0.9754 (39016/40000), AUC 0.9960181713104248
ep43_train_time 23.117001183999946
Test Epoch43 layer0 Acc 0.7006, AUC 0.7696263194084167, avg_entr 0.15878382325172424, f1 0.7006000280380249
ep43_l0_test_time 0.2865723719999096
Test Epoch43 layer1 Acc 0.7186, AUC 0.781909704208374, avg_entr 0.04669376462697983, f1 0.7185999751091003
ep43_l1_test_time 0.3556561600000805
Test Epoch43 layer2 Acc 0.7336, AUC 0.8067706227302551, avg_entr 0.03873329609632492, f1 0.7335999608039856
ep43_l2_test_time 0.46412386499991953
Test Epoch43 layer3 Acc 0.7372, AUC 0.8109972476959229, avg_entr 0.034888286143541336, f1 0.7372000217437744
ep43_l3_test_time 0.6124592690000554
Test Epoch43 layer4 Acc 0.7432, AUC 0.8157297968864441, avg_entr 0.03238600119948387, f1 0.7432000041007996
ep43_l4_test_time 0.8085393839999142
gc 0
Train Epoch44 Acc 0.976375 (39055/40000), AUC 0.9960079193115234
ep44_train_time 23.10933160600007
Test Epoch44 layer0 Acc 0.7004, AUC 0.7696449160575867, avg_entr 0.1588023453950882, f1 0.7003999948501587
ep44_l0_test_time 0.29340074900005675
Test Epoch44 layer1 Acc 0.7182, AUC 0.7813834547996521, avg_entr 0.046576231718063354, f1 0.7182000279426575
ep44_l1_test_time 0.35778735899998537
Test Epoch44 layer2 Acc 0.7342, AUC 0.806578516960144, avg_entr 0.03881597891449928, f1 0.7342000007629395
ep44_l2_test_time 0.4688004419999743
Test Epoch44 layer3 Acc 0.7364, AUC 0.8104974031448364, avg_entr 0.03489246964454651, f1 0.7364000082015991
ep44_l3_test_time 0.6145974160001515
Test Epoch44 layer4 Acc 0.742, AUC 0.8151494264602661, avg_entr 0.032437097281217575, f1 0.7419999837875366
ep44_l4_test_time 0.8081742460001351
gc 0
Train Epoch45 Acc 0.97625 (39050/40000), AUC 0.9964460730552673
ep45_train_time 23.092158377000032
Test Epoch45 layer0 Acc 0.7004, AUC 0.7696284055709839, avg_entr 0.15853987634181976, f1 0.7003999948501587
ep45_l0_test_time 0.2833473260000119
Test Epoch45 layer1 Acc 0.718, AUC 0.7813612222671509, avg_entr 0.04625576362013817, f1 0.7179999351501465
ep45_l1_test_time 0.35419399400007023
Test Epoch45 layer2 Acc 0.7332, AUC 0.8065246343612671, avg_entr 0.038446906954050064, f1 0.7332000136375427
ep45_l2_test_time 0.4626961129999927
Test Epoch45 layer3 Acc 0.7364, AUC 0.8103679418563843, avg_entr 0.034566737711429596, f1 0.7364000082015991
ep45_l3_test_time 0.6127837579999778
Test Epoch45 layer4 Acc 0.743, AUC 0.814440131187439, avg_entr 0.031948234885931015, f1 0.7430000305175781
ep45_l4_test_time 0.8071820110001227
gc 0
Train Epoch46 Acc 0.974975 (38999/40000), AUC 0.996351957321167
ep46_train_time 23.036235027999965
Test Epoch46 layer0 Acc 0.7006, AUC 0.7695996165275574, avg_entr 0.15874001383781433, f1 0.7006000280380249
ep46_l0_test_time 0.2840323069999613
Test Epoch46 layer1 Acc 0.7178, AUC 0.7812657356262207, avg_entr 0.046162743121385574, f1 0.7178000211715698
ep46_l1_test_time 0.35577735499987284
Test Epoch46 layer2 Acc 0.734, AUC 0.8066592216491699, avg_entr 0.038935184478759766, f1 0.7339999675750732
ep46_l2_test_time 0.4719772609998927
Test Epoch46 layer3 Acc 0.7362, AUC 0.8105276823043823, avg_entr 0.0349494032561779, f1 0.7361999750137329
ep46_l3_test_time 0.6174954360001266
Test Epoch46 layer4 Acc 0.7418, AUC 0.8150802850723267, avg_entr 0.032672032713890076, f1 0.74180006980896
ep46_l4_test_time 0.809698520999973
gc 0
Train Epoch47 Acc 0.97545 (39018/40000), AUC 0.9961584806442261
ep47_train_time 23.175583799999913
Test Epoch47 layer0 Acc 0.6992, AUC 0.7696192264556885, avg_entr 0.15828338265419006, f1 0.6991999745368958
ep47_l0_test_time 0.28501859899984083
Test Epoch47 layer1 Acc 0.7178, AUC 0.7815020680427551, avg_entr 0.046463530510663986, f1 0.7178000211715698
ep47_l1_test_time 0.3549659560001146
Test Epoch47 layer2 Acc 0.7326, AUC 0.8062169551849365, avg_entr 0.038368504494428635, f1 0.7325999140739441
ep47_l2_test_time 0.4699238260000129
Test Epoch47 layer3 Acc 0.7362, AUC 0.8107408285140991, avg_entr 0.03461863473057747, f1 0.7361999750137329
ep47_l3_test_time 0.6129092799999398
Test Epoch47 layer4 Acc 0.743, AUC 0.8148066997528076, avg_entr 0.032278601080179214, f1 0.7430000305175781
ep47_l4_test_time 0.809638029000098
gc 0
Train Epoch48 Acc 0.9763 (39052/40000), AUC 0.9961374402046204
ep48_train_time 23.109874665999996
Test Epoch48 layer0 Acc 0.7008, AUC 0.7695446014404297, avg_entr 0.15848995745182037, f1 0.7008000016212463
ep48_l0_test_time 0.28433298200002355
Test Epoch48 layer1 Acc 0.7178, AUC 0.7811527252197266, avg_entr 0.04605549946427345, f1 0.7178000211715698
ep48_l1_test_time 0.354334063999886
Test Epoch48 layer2 Acc 0.734, AUC 0.8065932989120483, avg_entr 0.038475628942251205, f1 0.7339999675750732
ep48_l2_test_time 0.46426706499983084
Test Epoch48 layer3 Acc 0.7366, AUC 0.810366153717041, avg_entr 0.03463183343410492, f1 0.7365999817848206
ep48_l3_test_time 0.615275433999841
Test Epoch48 layer4 Acc 0.742, AUC 0.8149107694625854, avg_entr 0.032432835549116135, f1 0.7419999837875366
ep48_l4_test_time 0.8112959699999465
gc 0
Train Epoch49 Acc 0.975875 (39035/40000), AUC 0.9962723255157471
ep49_train_time 23.24808151000002
Test Epoch49 layer0 Acc 0.701, AUC 0.7695481181144714, avg_entr 0.15840692818164825, f1 0.7009999752044678
ep49_l0_test_time 0.28404776599995785
Test Epoch49 layer1 Acc 0.7176, AUC 0.7811777591705322, avg_entr 0.04612443223595619, f1 0.7175999879837036
ep49_l1_test_time 0.3548024289998466
Test Epoch49 layer2 Acc 0.7336, AUC 0.8065541982650757, avg_entr 0.03862924501299858, f1 0.7335999608039856
ep49_l2_test_time 0.4636546259998795
Test Epoch49 layer3 Acc 0.7368, AUC 0.8104888200759888, avg_entr 0.03469457849860191, f1 0.7368000149726868
ep49_l3_test_time 0.613671236999835
Test Epoch49 layer4 Acc 0.7424, AUC 0.8146957755088806, avg_entr 0.03221406415104866, f1 0.742400050163269
ep49_l4_test_time 0.8090040629999748
Best AUC tensor(0.7678) 14 4
train_as_loss [[8.75895430e+01 5.87037500e+01 5.18895263e+01 5.03739311e+01
  4.98432161e+01 4.95995256e+01 4.94680229e+01 4.93891557e+01
  4.93381964e+01 4.93034110e+01 4.92786331e+01 4.92603820e+01
  4.92465669e+01 4.92358716e+01 4.92274347e+01 4.92206708e+01
  4.92151742e+01 4.92116619e+01 4.92095373e+01 4.92075317e+01
  4.92056440e+01 4.92042932e+01 4.92034037e+01 4.92025127e+01
  4.92016219e+01 4.92009525e+01 4.92004940e+01 4.92000204e+01
  4.91995344e+01 4.91991570e+01 4.91988949e+01 4.91986195e+01
  4.91983301e+01 4.91981040e+01 4.91979435e+01 4.91977741e+01
  4.91975948e+01 4.91974487e+01 4.91973502e+01 4.91972423e+01
  4.91971257e+01 4.91970319e+01 4.91969661e+01 4.91969024e+01
  4.91968216e+01 4.91967719e+01 4.91967191e+01 4.91966718e+01
  4.91966312e+01 4.91965910e+01]
 [2.73275669e+00 4.03015738e-04 1.73267054e-05 4.37720669e-06
  1.67236188e-06 8.19287141e-07 4.45498515e-07 2.68459191e-07
  1.65524591e-07 1.05350130e-07 1.18346052e-07 1.83686164e-07
  3.37279558e-07 5.46642982e-07 4.22081797e-07 1.13078834e-06
  4.78101738e-08 1.06896955e-08 7.68336610e-09 6.69833700e-09
  2.45586387e-08 5.50937738e-09 4.90957572e-09 4.34086438e-09
  1.70282634e-08 3.86617230e-09 3.57100555e-09 3.41903900e-09
  1.34513333e-08 3.16524686e-09 2.99506980e-09 2.92647701e-09
  1.15214078e-08 2.77188210e-09 2.64638091e-09 2.63337524e-09
  1.01257812e-08 2.65353041e-09 2.48815589e-09 2.41694341e-09
  9.03659419e-09 2.64239380e-09 2.32784693e-09 2.26662234e-09
  8.15708899e-09 2.83779359e-09 2.30956396e-09 2.21699469e-09
  7.37781319e-09 3.11278072e-09]
 [2.86735318e+00 2.38093406e-04 1.17605663e-05 3.25538103e-06
  1.29673358e-06 6.63396497e-07 3.62121665e-07 2.23056119e-07
  1.39647008e-07 8.74820257e-08 9.58226167e-08 1.28211304e-07
  2.61658692e-07 4.43755394e-07 4.50707601e-07 1.22836724e-06
  3.98944541e-08 1.20953510e-08 6.07168789e-09 5.08799491e-09
  1.86193361e-08 5.12303063e-09 3.82579194e-09 3.10588688e-09
  1.24109203e-08 3.15553544e-09 2.53730038e-09 2.42727380e-09
  9.48811039e-09 2.42235054e-09 2.08611137e-09 2.02048884e-09
  7.97275259e-09 2.02526573e-09 1.81851302e-09 1.81954735e-09
  6.98676124e-09 1.91889614e-09 1.71136029e-09 1.66424376e-09
  6.22026529e-09 1.89793505e-09 1.61746105e-09 1.57841252e-09
  5.65240047e-09 2.07222894e-09 1.62244159e-09 1.57588662e-09
  5.25474413e-09 2.37005226e-09]
 [2.19638604e+00 2.51941746e-04 8.70660385e-06 2.48246745e-06
  9.81828715e-07 5.63056497e-07 2.92918046e-07 1.90323107e-07
  1.37062836e-07 8.41142895e-08 8.88504353e-08 1.11189998e-07
  1.91708888e-07 3.12777778e-07 3.26190435e-07 8.94730842e-07
  4.44055040e-08 2.11403324e-08 6.23633621e-09 4.90693772e-09
  1.74471214e-08 6.75811117e-09 3.89702818e-09 2.68468253e-09
  1.06586818e-08 3.35455069e-09 2.13117094e-09 2.06005881e-09
  7.83072480e-09 2.27036320e-09 1.73351949e-09 1.61108929e-09
  6.38994963e-09 1.78598085e-09 1.46048434e-09 1.45862357e-09
  5.58645807e-09 1.64922383e-09 1.37985781e-09 1.34862668e-09
  4.98794024e-09 1.65195789e-09 1.35331035e-09 1.31233799e-09
  4.57265790e-09 1.80446892e-09 1.39382412e-09 1.34132221e-09
  4.34321585e-09 2.13166338e-09]
 [2.41205418e+00 1.05913289e-03 9.42122731e-06 2.67591371e-06
  1.09491079e-06 7.54155925e-07 3.73114877e-07 2.56359809e-07
  2.12536993e-07 1.39534337e-07 1.33559399e-07 1.46898138e-07
  1.72168605e-07 2.36454585e-07 2.56147403e-07 6.39210145e-07
  7.78629281e-08 4.74676146e-08 1.01467068e-08 7.46157785e-09
  2.29734588e-08 1.23865623e-08 5.46022770e-09 3.50406179e-09
  1.23095315e-08 4.81756236e-09 2.42580137e-09 2.38551914e-09
  8.49474171e-09 2.88748344e-09 1.91488259e-09 1.72627827e-09
  6.76646618e-09 2.08482606e-09 1.54452848e-09 1.55250234e-09
  5.76949449e-09 1.90179513e-09 1.45331602e-09 1.41961168e-09
  5.02080618e-09 1.87757425e-09 1.40154393e-09 1.36655589e-09
  4.51658497e-09 2.07395478e-09 1.46812457e-09 1.39294727e-09
  4.15431796e-09 2.49836462e-09]]
train_ae_loss [[4.93824508 3.55123322 4.86978818 5.47510433 5.83070554 6.10621832
  6.22842864 6.34509894 6.31622085 6.32359241 6.13384688 5.96851352
  5.68656457 5.59461265 5.2584124  4.96899608 4.72481892 4.34369071
  4.05527605 3.9193616  3.81410928 3.55150788 3.48585549 3.42746828
  3.3590429  3.24543208 3.22431999 3.1940644  3.18917836 3.12264323
  3.0789025  3.07672327 3.08434599 3.03727491 3.04145369 3.01289789
  3.03396927 2.98169609 3.00290228 3.0026066  2.99852118 3.00607678
  3.01197261 2.98483898 2.99294457 2.99633753 2.99797393 3.00319559
  2.98832083 2.97537969]
 [3.9269446  4.13707359 5.67491955 6.0732865  6.27740795 6.46258837
  6.49885294 6.53543061 6.41672777 6.53084285 6.1864256  5.92945782
  5.52666906 5.46608663 5.04421449 4.68820231 4.19793599 3.87720029
  3.31156231 2.92515332 2.64996567 2.37887438 2.24637379 2.1807173
  2.05625086 1.91160835 1.87169319 1.85730359 1.82669884 1.73985877
  1.68104861 1.67378997 1.6711029  1.60737186 1.63892412 1.59991378
  1.60103148 1.56499463 1.59309945 1.59782682 1.5596893  1.57528085
  1.59674112 1.56405149 1.56151737 1.5580031  1.5707595  1.5784094
  1.55788158 1.54458696]
 [3.98647423 3.88634371 5.49032912 5.77156337 5.89081867 5.99446309
  5.94832541 5.94047433 5.80052852 5.88270774 5.51936837 5.29603052
  4.69225488 4.5947135  4.078516   3.65656934 3.10321925 2.84949104
  2.45512143 2.26835974 2.05339143 1.83994685 1.72997772 1.69093864
  1.56262868 1.43993965 1.40492106 1.39812821 1.37254885 1.29601228
  1.24553428 1.24279116 1.23579877 1.17316962 1.20420226 1.1705375
  1.15472613 1.13819058 1.15629786 1.16617765 1.13502927 1.14088308
  1.17622709 1.15160747 1.13388966 1.13468428 1.14298692 1.15756138
  1.13165716 1.11690093]
 [4.06259957 3.50951829 5.04680902 5.32473147 5.3459063  5.37375508
  5.26211065 5.22119281 5.05569698 5.07922326 4.68288835 4.48303499
  3.89506272 3.83800479 3.38881736 3.04111442 2.54759427 2.35062826
  2.0065687  1.85330195 1.65982101 1.48837089 1.39976363 1.35899084
  1.2546578  1.1560476  1.1335531  1.12607048 1.09883001 1.03345589
  0.99361924 0.99125591 0.9795399  0.92887752 0.9589649  0.92638269
  0.91315791 0.90049688 0.91473559 0.92236592 0.89363518 0.90163323
  0.925553   0.9088937  0.89811086 0.89124026 0.90016952 0.91492624
  0.8975555  0.87961114]
 [3.99534373 2.80746713 4.11135382 4.56549823 4.40465208 4.35291212
  4.2306293  4.20102468 4.06575599 4.05326886 3.69161128 3.56502317
  3.02345936 2.9960613  2.61876982 2.3487599  1.92814948 1.79789066
  1.51502869 1.39822915 1.2376254  1.10936194 1.04143744 1.00907801
  0.92448536 0.84870656 0.83983182 0.83129178 0.81043093 0.75317912
  0.72761089 0.72474286 0.7150961  0.67566773 0.69625216 0.6704827
  0.66383595 0.65348511 0.66196342 0.67059432 0.64876325 0.65387994
  0.67183175 0.65876939 0.65187284 0.64593581 0.65197119 0.66373943
  0.65455632 0.63923044]]
valid_acc (5, 50)
valid_AUC (5, 50)
train_acc (50,)
total_train+valid_time 1288.3657481110001
Start Testing
Load ckpt at ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt
gc 9
Test layer0 Acc 0.723, AUC 0.7988144755363464, avg_entr 0.22322775423526764, f1 0.7229999899864197
l0_test_time 0.2844058279999899
gc 0
Test layer1 Acc 0.749, AUC 0.8326901197433472, avg_entr 0.19122864305973053, f1 0.7490000128746033
l1_test_time 0.36874666400012757
gc 0
Test layer2 Acc 0.7608, AUC 0.8466901779174805, avg_entr 0.12704239785671234, f1 0.7608000040054321
l2_test_time 0.4695969550000427
gc 0
Test layer3 Acc 0.7694, AUC 0.8569617867469788, avg_entr 0.1285005658864975, f1 0.7694000005722046
l3_test_time 0.6173661609998362
gc 0
Test layer4 Acc 0.7726, AUC 0.8609944581985474, avg_entr 0.12344461679458618, f1 0.772599995136261
l4_test_time 0.8093538140001328
gc 0
Test threshold 0.1 Acc 0.769, AUC 0.8375325202941895, avg_entr 0.16647399961948395, f1 0.7689999938011169
t0.1_test_time 0.5610701579998931
gc 0
Test threshold 0.2 Acc 0.7656, AUC 0.8306194543838501, avg_entr 0.1688254326581955, f1 0.7655999660491943
t0.2_test_time 0.5052359929998147
gc 0
Test threshold 0.3 Acc 0.7628, AUC 0.8235186338424683, avg_entr 0.17663615942001343, f1 0.7627999782562256
t0.3_test_time 0.4663949590001266
gc 0
Test threshold 0.4 Acc 0.7608, AUC 0.8199377059936523, avg_entr 0.1851404309272766, f1 0.7608000040054321
t0.4_test_time 0.4406965380001111
gc 0
Test threshold 0.5 Acc 0.756, AUC 0.8164753913879395, avg_entr 0.1993318647146225, f1 0.7559999227523804
t0.5_test_time 0.4178828130000056
gc 0
Test threshold 0.6 Acc 0.7508, AUC 0.8137325048446655, avg_entr 0.21429798007011414, f1 0.7508000135421753
t0.6_test_time 0.4048237939998671
gc 0
Test threshold 0.7 Acc 0.7458, AUC 0.811715841293335, avg_entr 0.23144707083702087, f1 0.7458000183105469
t0.7_test_time 0.37944100000004255
gc 0
Test threshold 0.8 Acc 0.7412, AUC 0.8070768713951111, avg_entr 0.2517561912536621, f1 0.7411999702453613
t0.8_test_time 0.36775551099981385
gc 0
Test threshold 0.9 Acc 0.7322, AUC 0.8024942874908447, avg_entr 0.27602216601371765, f1 0.732200026512146
t0.9_test_time 0.3531401249999817
