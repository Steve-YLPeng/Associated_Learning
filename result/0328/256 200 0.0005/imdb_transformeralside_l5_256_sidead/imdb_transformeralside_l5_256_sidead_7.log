total count words 222751
vocab size 30000
train size 40000, valid size 5000, test size 5000
found 27937 words in glove
model: TransformerALsideText(
  (layers): ModuleList(
    (0): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=2, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=2, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
  (emb_layers): ModuleList(
    (0): Embedding(30000, 300)
    (1): Embedding(30000, 300)
    (2): Embedding(30000, 300)
    (3): Embedding(30000, 300)
    (4): Embedding(30000, 300)
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.multi_headed_attention.linears.0.weight 90000
layers.0.enc.f.multi_headed_attention.linears.0.bias 300
layers.0.enc.f.multi_headed_attention.linears.1.weight 90000
layers.0.enc.f.multi_headed_attention.linears.1.bias 300
layers.0.enc.f.multi_headed_attention.linears.2.weight 90000
layers.0.enc.f.multi_headed_attention.linears.2.bias 300
layers.0.enc.f.multi_headed_attention.linears.3.weight 90000
layers.0.enc.f.multi_headed_attention.linears.3.bias 300
layers.0.enc.f.feed_forward.w_1.weight 90000
layers.0.enc.f.feed_forward.w_1.bias 300
layers.0.enc.f.feed_forward.w_2.weight 90000
layers.0.enc.f.feed_forward.w_2.bias 300
layers.0.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.0.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.0.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.0.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.0.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.0.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.0.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.0.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.0.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.0.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.0.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.0.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.0.enc.f.encoder.norm.a 300
layers.0.enc.f.encoder.norm.b 300
layers.0.ae.g.0.weight 256
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 256
layers.0.ae.h.0.bias 2
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
emb_layers.0.weight 9000000
emb_layers.1.weight 9000000
emb_layers.2.weight 9000000
emb_layers.3.weight 9000000
emb_layers.4.weight 9000000
Total Trainable Params: 50758378
init_time 21.650975726
Start Training
gc 0
Train Epoch0 Acc 0.505525 (20221/40000), AUC 0.5016295909881592
ep0_train_time 23.462305126999997
Test Epoch0 layer0 Acc 0.5178, AUC 0.5858396291732788, avg_entr 0.6922737956047058, f1 0.517799973487854
ep0_l0_test_time 0.288011025000003
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 0
Test Epoch0 layer1 Acc 0.5022, AUC 0.5359717607498169, avg_entr 0.692206621170044, f1 0.5022000074386597
ep0_l1_test_time 0.3577789799999991
Test Epoch0 layer2 Acc 0.4972, AUC 0.5075033903121948, avg_entr 0.688225507736206, f1 0.49720001220703125
ep0_l2_test_time 0.4637749979999981
Test Epoch0 layer3 Acc 0.5, AUC 0.4964802861213684, avg_entr 0.6805300712585449, f1 0.5
ep0_l3_test_time 0.613195827999995
Test Epoch0 layer4 Acc 0.4984, AUC 0.4985119104385376, avg_entr 0.6946260929107666, f1 0.4984000027179718
ep0_l4_test_time 0.8063286820000002
gc 0
Train Epoch1 Acc 0.505375 (20215/40000), AUC 0.5094003677368164
ep1_train_time 22.984684775999995
Test Epoch1 layer0 Acc 0.5454, AUC 0.6139223575592041, avg_entr 0.6778636574745178, f1 0.5454000234603882
ep1_l0_test_time 0.29312285000000315
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 1
Test Epoch1 layer1 Acc 0.5044, AUC 0.5817334651947021, avg_entr 0.6846151351928711, f1 0.5044000148773193
ep1_l1_test_time 0.36171017299999164
Test Epoch1 layer2 Acc 0.5, AUC 0.5699775218963623, avg_entr 0.6631314754486084, f1 0.5
ep1_l2_test_time 0.47795954699999754
Test Epoch1 layer3 Acc 0.5, AUC 0.5261566042900085, avg_entr 0.6425361633300781, f1 0.5
ep1_l3_test_time 0.6208787799999982
Test Epoch1 layer4 Acc 0.5, AUC 0.5194572806358337, avg_entr 0.6895034909248352, f1 0.5
ep1_l4_test_time 0.8128584300000057
gc 0
Train Epoch2 Acc 0.5101 (20404/40000), AUC 0.513635516166687
ep2_train_time 23.221132283000003
Test Epoch2 layer0 Acc 0.6286, AUC 0.6859656572341919, avg_entr 0.6016108393669128, f1 0.628600001335144
ep2_l0_test_time 0.2914558829999976
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 2
Test Epoch2 layer1 Acc 0.6328, AUC 0.6850868463516235, avg_entr 0.6408195495605469, f1 0.6327999830245972
ep2_l1_test_time 0.36037885499999334
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 2
Test Epoch2 layer2 Acc 0.5316, AUC 0.6698294878005981, avg_entr 0.6411464214324951, f1 0.5315999984741211
ep2_l2_test_time 0.47228746499999374
Test Epoch2 layer3 Acc 0.5946, AUC 0.6241734027862549, avg_entr 0.6930423378944397, f1 0.5946000218391418
ep2_l3_test_time 0.6194727320000055
Test Epoch2 layer4 Acc 0.5114, AUC 0.5205515623092651, avg_entr 0.6895977258682251, f1 0.5113999843597412
ep2_l4_test_time 0.8170966380000095
gc 0
Train Epoch3 Acc 0.53405 (21362/40000), AUC 0.5517094135284424
ep3_train_time 23.134514658
Test Epoch3 layer0 Acc 0.6686, AUC 0.7371069192886353, avg_entr 0.5053039789199829, f1 0.6686000227928162
ep3_l0_test_time 0.28449335799999176
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 3
Test Epoch3 layer1 Acc 0.6556, AUC 0.75384521484375, avg_entr 0.4342167377471924, f1 0.6556000113487244
ep3_l1_test_time 0.35865129399999773
Test Epoch3 layer2 Acc 0.6314, AUC 0.7605007886886597, avg_entr 0.39869225025177, f1 0.6313999891281128
ep3_l2_test_time 0.4674687480000017
Test Epoch3 layer3 Acc 0.6022, AUC 0.7526301145553589, avg_entr 0.4395075738430023, f1 0.6021999716758728
ep3_l3_test_time 0.6170417480000054
Test Epoch3 layer4 Acc 0.5778, AUC 0.752224326133728, avg_entr 0.6443302035331726, f1 0.5777999758720398
ep3_l4_test_time 0.8099942590000069
gc 0
Train Epoch4 Acc 0.58915 (23566/40000), AUC 0.627648651599884
ep4_train_time 23.043147110000007
Test Epoch4 layer0 Acc 0.684, AUC 0.7578260898590088, avg_entr 0.4501592814922333, f1 0.6840000152587891
ep4_l0_test_time 0.2870184400000255
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 4
Test Epoch4 layer1 Acc 0.6906, AUC 0.7777237892150879, avg_entr 0.39993202686309814, f1 0.6905999779701233
ep4_l1_test_time 0.3554590340000061
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 4
Test Epoch4 layer2 Acc 0.686, AUC 0.7853755354881287, avg_entr 0.37942105531692505, f1 0.6859999895095825
ep4_l2_test_time 0.4647271230000172
Test Epoch4 layer3 Acc 0.6872, AUC 0.7855785489082336, avg_entr 0.39122703671455383, f1 0.6872000098228455
ep4_l3_test_time 0.6185711770000069
Test Epoch4 layer4 Acc 0.6902, AUC 0.7871115803718567, avg_entr 0.5264714956283569, f1 0.6901999711990356
ep4_l4_test_time 0.809922623999995
gc 0
Train Epoch5 Acc 0.686575 (27463/40000), AUC 0.7531552314758301
ep5_train_time 23.077923138999978
Test Epoch5 layer0 Acc 0.6854, AUC 0.774040699005127, avg_entr 0.37438586354255676, f1 0.6854000091552734
ep5_l0_test_time 0.29804594099999804
Test Epoch5 layer1 Acc 0.667, AUC 0.7994978427886963, avg_entr 0.3085748553276062, f1 0.6669999957084656
ep5_l1_test_time 0.3607537050000076
Test Epoch5 layer2 Acc 0.6398, AUC 0.81020188331604, avg_entr 0.28521260619163513, f1 0.6398000121116638
ep5_l2_test_time 0.46682045200000744
Test Epoch5 layer3 Acc 0.59, AUC 0.8165251612663269, avg_entr 0.26903754472732544, f1 0.5899999737739563
ep5_l3_test_time 0.619408663999991
Test Epoch5 layer4 Acc 0.5352, AUC 0.8146462440490723, avg_entr 0.23435693979263306, f1 0.5351999998092651
ep5_l4_test_time 0.8153961129999914
gc 0
Train Epoch6 Acc 0.718075 (28723/40000), AUC 0.7899108529090881
ep6_train_time 23.007458393999997
Test Epoch6 layer0 Acc 0.7002, AUC 0.7816331386566162, avg_entr 0.38816413283348083, f1 0.7002000212669373
ep6_l0_test_time 0.285045193000002
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 6
Test Epoch6 layer1 Acc 0.7248, AUC 0.8113357424736023, avg_entr 0.3349243998527527, f1 0.7247999906539917
ep6_l1_test_time 0.35665972600000373
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 6
Test Epoch6 layer2 Acc 0.7284, AUC 0.8208377361297607, avg_entr 0.3132510483264923, f1 0.7284000515937805
ep6_l2_test_time 0.46462148899999534
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 6
Test Epoch6 layer3 Acc 0.7294, AUC 0.8248178958892822, avg_entr 0.30201074481010437, f1 0.7293999791145325
ep6_l3_test_time 0.6160939869999993
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 6
Test Epoch6 layer4 Acc 0.7254, AUC 0.8253880739212036, avg_entr 0.31137514114379883, f1 0.7253999710083008
ep6_l4_test_time 0.8096283040000003
gc 0
Train Epoch7 Acc 0.756075 (30243/40000), AUC 0.837482213973999
ep7_train_time 23.05395393500001
Test Epoch7 layer0 Acc 0.7068, AUC 0.788356602191925, avg_entr 0.3360309898853302, f1 0.7067999839782715
ep7_l0_test_time 0.28428549900002054
Test Epoch7 layer1 Acc 0.7328, AUC 0.8205724954605103, avg_entr 0.29373615980148315, f1 0.7327999472618103
ep7_l1_test_time 0.36134897699997737
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 7
Test Epoch7 layer2 Acc 0.7372, AUC 0.8290339708328247, avg_entr 0.2768479883670807, f1 0.7372000217437744
ep7_l2_test_time 0.4650003520000041
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 7
Test Epoch7 layer3 Acc 0.737, AUC 0.8333114385604858, avg_entr 0.25140100717544556, f1 0.7369999885559082
ep7_l3_test_time 0.6146195059999968
Test Epoch7 layer4 Acc 0.7334, AUC 0.8348921537399292, avg_entr 0.24937912821769714, f1 0.7333999872207642
ep7_l4_test_time 0.8133115079999982
gc 0
Train Epoch8 Acc 0.7725 (30900/40000), AUC 0.8541097044944763
ep8_train_time 23.00047981899999
Test Epoch8 layer0 Acc 0.7098, AUC 0.7959320545196533, avg_entr 0.3171660304069519, f1 0.7098000049591064
ep8_l0_test_time 0.2873591400000066
Test Epoch8 layer1 Acc 0.7446, AUC 0.8283427953720093, avg_entr 0.2824946641921997, f1 0.7445999979972839
ep8_l1_test_time 0.3542194779999761
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 8
Test Epoch8 layer2 Acc 0.7528, AUC 0.8411810398101807, avg_entr 0.2823333442211151, f1 0.7528000473976135
ep8_l2_test_time 0.46731580299996267
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 8
Test Epoch8 layer3 Acc 0.7624, AUC 0.8476647138595581, avg_entr 0.2870059907436371, f1 0.7623999714851379
ep8_l3_test_time 0.6224675550000143
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 8
Test Epoch8 layer4 Acc 0.7642, AUC 0.8485977649688721, avg_entr 0.3084748387336731, f1 0.76419997215271
ep8_l4_test_time 0.8120496220000177
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 8
gc 0
Train Epoch9 Acc 0.790025 (31601/40000), AUC 0.8709525465965271
ep9_train_time 23.163675366000007
Test Epoch9 layer0 Acc 0.702, AUC 0.7953670620918274, avg_entr 0.3026067018508911, f1 0.7020000219345093
ep9_l0_test_time 0.2901608119999537
Test Epoch9 layer1 Acc 0.7268, AUC 0.8290802240371704, avg_entr 0.2575363516807556, f1 0.7268000245094299
ep9_l1_test_time 0.35988682199996447
Test Epoch9 layer2 Acc 0.738, AUC 0.8416171669960022, avg_entr 0.23671995103359222, f1 0.7379999160766602
ep9_l2_test_time 0.46984838299999865
Test Epoch9 layer3 Acc 0.7402, AUC 0.8487519025802612, avg_entr 0.21837827563285828, f1 0.7401999831199646
ep9_l3_test_time 0.6244056259999979
Test Epoch9 layer4 Acc 0.7302, AUC 0.8500577211380005, avg_entr 0.20171938836574554, f1 0.7301999926567078
ep9_l4_test_time 0.8150380640000208
gc 0
Train Epoch10 Acc 0.8022 (32088/40000), AUC 0.8837183713912964
ep10_train_time 23.116129778000015
Test Epoch10 layer0 Acc 0.7102, AUC 0.7975121736526489, avg_entr 0.2826661169528961, f1 0.7102000117301941
ep10_l0_test_time 0.2925862660000007
Test Epoch10 layer1 Acc 0.7312, AUC 0.8329945802688599, avg_entr 0.2323223352432251, f1 0.7311999797821045
ep10_l1_test_time 0.359628559999976
Test Epoch10 layer2 Acc 0.727, AUC 0.8503604531288147, avg_entr 0.20562511682510376, f1 0.7269999980926514
ep10_l2_test_time 0.47274485700000923
Test Epoch10 layer3 Acc 0.7292, AUC 0.8575782179832458, avg_entr 0.1893981248140335, f1 0.7291999459266663
ep10_l3_test_time 0.6239729469999702
Test Epoch10 layer4 Acc 0.7292, AUC 0.8589897155761719, avg_entr 0.1740271896123886, f1 0.7291999459266663
ep10_l4_test_time 0.8169871599999965
gc 0
Train Epoch11 Acc 0.8283 (33132/40000), AUC 0.905930757522583
ep11_train_time 23.113268059999996
Test Epoch11 layer0 Acc 0.6726, AUC 0.7942977547645569, avg_entr 0.22394277155399323, f1 0.6725999712944031
ep11_l0_test_time 0.3000159759999974
Test Epoch11 layer1 Acc 0.6734, AUC 0.8303360939025879, avg_entr 0.17704033851623535, f1 0.6733999848365784
ep11_l1_test_time 0.3636680030000434
Test Epoch11 layer2 Acc 0.6532, AUC 0.8487420082092285, avg_entr 0.145263671875, f1 0.6531999707221985
ep11_l2_test_time 0.47166621499997063
Test Epoch11 layer3 Acc 0.6452, AUC 0.8556509017944336, avg_entr 0.11678746342658997, f1 0.6452000141143799
ep11_l3_test_time 0.6205428310000229
Test Epoch11 layer4 Acc 0.6338, AUC 0.8546810150146484, avg_entr 0.1005408838391304, f1 0.6338000297546387
ep11_l4_test_time 0.8153655079999567
gc 0
Train Epoch12 Acc 0.836475 (33459/40000), AUC 0.9075490236282349
ep12_train_time 23.08591726999998
Test Epoch12 layer0 Acc 0.7172, AUC 0.7906501293182373, avg_entr 0.25587013363838196, f1 0.717199981212616
ep12_l0_test_time 0.29505384999998796
Test Epoch12 layer1 Acc 0.7424, AUC 0.8264983892440796, avg_entr 0.21315740048885345, f1 0.742400050163269
ep12_l1_test_time 0.3604060769999933
Test Epoch12 layer2 Acc 0.7552, AUC 0.8437501192092896, avg_entr 0.18421366810798645, f1 0.7552000284194946
ep12_l2_test_time 0.47350558899995576
Test Epoch12 layer3 Acc 0.7588, AUC 0.8512171506881714, avg_entr 0.14271636307239532, f1 0.7588000297546387
ep12_l3_test_time 0.6193971569999803
Test Epoch12 layer4 Acc 0.7568, AUC 0.8533793687820435, avg_entr 0.13120117783546448, f1 0.7567999958992004
ep12_l4_test_time 0.8157203239999831
gc 0
Train Epoch13 Acc 0.85475 (34190/40000), AUC 0.9265186786651611
ep13_train_time 23.122580744000004
Test Epoch13 layer0 Acc 0.7144, AUC 0.7910761833190918, avg_entr 0.2612925171852112, f1 0.7143999934196472
ep13_l0_test_time 0.2907577259999812
Test Epoch13 layer1 Acc 0.7548, AUC 0.8288912177085876, avg_entr 0.22620971500873566, f1 0.754800021648407
ep13_l1_test_time 0.35934426199997915
Test Epoch13 layer2 Acc 0.7654, AUC 0.8478188514709473, avg_entr 0.18115338683128357, f1 0.7653999924659729
ep13_l2_test_time 0.4725851000000034
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 13
Test Epoch13 layer3 Acc 0.775, AUC 0.8557842373847961, avg_entr 0.14716628193855286, f1 0.7749999761581421
ep13_l3_test_time 0.6202329559999953
Save ckpt to ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt  ,ep 13
Test Epoch13 layer4 Acc 0.7748, AUC 0.8587039113044739, avg_entr 0.14429767429828644, f1 0.7748000025749207
ep13_l4_test_time 0.8186845589999621
gc 0
Train Epoch14 Acc 0.865425 (34617/40000), AUC 0.9351564645767212
ep14_train_time 23.12732538399996
Test Epoch14 layer0 Acc 0.7102, AUC 0.784817099571228, avg_entr 0.2320183366537094, f1 0.7102000117301941
ep14_l0_test_time 0.293978630999959
Test Epoch14 layer1 Acc 0.738, AUC 0.8206160068511963, avg_entr 0.18468843400478363, f1 0.7379999160766602
ep14_l1_test_time 0.3658809450000149
Test Epoch14 layer2 Acc 0.756, AUC 0.8395110964775085, avg_entr 0.12917473912239075, f1 0.7559999227523804
ep14_l2_test_time 0.4705210090000378
Test Epoch14 layer3 Acc 0.7666, AUC 0.8472684621810913, avg_entr 0.11167358607053757, f1 0.7666000127792358
ep14_l3_test_time 0.6211572600000181
Test Epoch14 layer4 Acc 0.7648, AUC 0.8499025106430054, avg_entr 0.10686548799276352, f1 0.764799952507019
ep14_l4_test_time 0.8187589810000304
gc 0
Train Epoch15 Acc 0.8971 (35884/40000), AUC 0.9589265584945679
ep15_train_time 23.131280308999976
Test Epoch15 layer0 Acc 0.7148, AUC 0.7847369909286499, avg_entr 0.2158460021018982, f1 0.7148000597953796
ep15_l0_test_time 0.2904262860000131
Test Epoch15 layer1 Acc 0.7424, AUC 0.8223550319671631, avg_entr 0.17034853994846344, f1 0.742400050163269
ep15_l1_test_time 0.3613913809999758
Test Epoch15 layer2 Acc 0.755, AUC 0.8409835696220398, avg_entr 0.10938294976949692, f1 0.7549999356269836
ep15_l2_test_time 0.47274520000001985
Test Epoch15 layer3 Acc 0.763, AUC 0.8488255143165588, avg_entr 0.10523227602243423, f1 0.7630000114440918
ep15_l3_test_time 0.6250940040000046
Test Epoch15 layer4 Acc 0.7646, AUC 0.8513338565826416, avg_entr 0.10305345803499222, f1 0.7645999789237976
ep15_l4_test_time 0.8161166080000157
gc 0
Train Epoch16 Acc 0.9002 (36008/40000), AUC 0.9625802040100098
ep16_train_time 23.131472885999983
Test Epoch16 layer0 Acc 0.7142, AUC 0.7827987670898438, avg_entr 0.21535742282867432, f1 0.7142000198364258
ep16_l0_test_time 0.29176574000001665
Test Epoch16 layer1 Acc 0.7322, AUC 0.8187370300292969, avg_entr 0.14739982783794403, f1 0.732200026512146
ep16_l1_test_time 0.361907398000028
Test Epoch16 layer2 Acc 0.749, AUC 0.8376681208610535, avg_entr 0.09768249839544296, f1 0.7490000128746033
ep16_l2_test_time 0.47578789099998176
Test Epoch16 layer3 Acc 0.7564, AUC 0.8455619215965271, avg_entr 0.09081155061721802, f1 0.7563999891281128
ep16_l3_test_time 0.6211787679999929
Test Epoch16 layer4 Acc 0.7594, AUC 0.8488045334815979, avg_entr 0.08854960650205612, f1 0.759399950504303
ep16_l4_test_time 0.8154624090000198
gc 0
Train Epoch17 Acc 0.908625 (36345/40000), AUC 0.9662834405899048
ep17_train_time 23.118432134999978
Test Epoch17 layer0 Acc 0.7094, AUC 0.7794978618621826, avg_entr 0.20720762014389038, f1 0.709399938583374
ep17_l0_test_time 0.29812271399998735
Test Epoch17 layer1 Acc 0.7396, AUC 0.8148986101150513, avg_entr 0.13241684436798096, f1 0.7396000027656555
ep17_l1_test_time 0.363208238000027
Test Epoch17 layer2 Acc 0.7524, AUC 0.8347879648208618, avg_entr 0.0894545167684555, f1 0.7523999810218811
ep17_l2_test_time 0.47120180500002107
Test Epoch17 layer3 Acc 0.7638, AUC 0.8447228670120239, avg_entr 0.08345336467027664, f1 0.7638000249862671
ep17_l3_test_time 0.6220592800000304
Test Epoch17 layer4 Acc 0.7656, AUC 0.8474847674369812, avg_entr 0.08104048669338226, f1 0.7655999660491943
ep17_l4_test_time 0.820097230999977
gc 0
Train Epoch18 Acc 0.91865 (36746/40000), AUC 0.9718451499938965
ep18_train_time 23.122061719000044
Test Epoch18 layer0 Acc 0.7052, AUC 0.777489423751831, avg_entr 0.2049505114555359, f1 0.7052000164985657
ep18_l0_test_time 0.29034433599997556
Test Epoch18 layer1 Acc 0.7274, AUC 0.8090399503707886, avg_entr 0.11854001134634018, f1 0.7274000644683838
ep18_l1_test_time 0.36221443600004477
Test Epoch18 layer2 Acc 0.748, AUC 0.831540048122406, avg_entr 0.0829320028424263, f1 0.7480000257492065
ep18_l2_test_time 0.47177335399999265
Test Epoch18 layer3 Acc 0.7536, AUC 0.8399825692176819, avg_entr 0.08238937705755234, f1 0.7535999417304993
ep18_l3_test_time 0.6260768370000278
Test Epoch18 layer4 Acc 0.7536, AUC 0.8427842855453491, avg_entr 0.08079664409160614, f1 0.7535999417304993
ep18_l4_test_time 0.8169722809999485
gc 0
Train Epoch19 Acc 0.929575 (37183/40000), AUC 0.9780797958374023
ep19_train_time 23.108330421999995
Test Epoch19 layer0 Acc 0.7054, AUC 0.774263858795166, avg_entr 0.1963747888803482, f1 0.7053999900817871
ep19_l0_test_time 0.2916665220000141
Test Epoch19 layer1 Acc 0.7254, AUC 0.8014368414878845, avg_entr 0.10569803416728973, f1 0.7253999710083008
ep19_l1_test_time 0.3688293520000343
Test Epoch19 layer2 Acc 0.7454, AUC 0.8208869099617004, avg_entr 0.07347613573074341, f1 0.745400071144104
ep19_l2_test_time 0.47144431000003806
Test Epoch19 layer3 Acc 0.7502, AUC 0.8305765986442566, avg_entr 0.07155811786651611, f1 0.7501999735832214
ep19_l3_test_time 0.6214471619999813
Test Epoch19 layer4 Acc 0.7516, AUC 0.8337738513946533, avg_entr 0.06917030364274979, f1 0.7516000270843506
ep19_l4_test_time 0.8213429279999218
gc 0
Train Epoch20 Acc 0.93065 (37226/40000), AUC 0.9785040616989136
ep20_train_time 23.102690373999963
Test Epoch20 layer0 Acc 0.7082, AUC 0.7755913734436035, avg_entr 0.19223552942276, f1 0.7081999778747559
ep20_l0_test_time 0.3013720579999699
Test Epoch20 layer1 Acc 0.726, AUC 0.8063135147094727, avg_entr 0.10179810971021652, f1 0.7260000109672546
ep20_l1_test_time 0.3634215020000511
Test Epoch20 layer2 Acc 0.749, AUC 0.8263819217681885, avg_entr 0.07669826596975327, f1 0.7490000128746033
ep20_l2_test_time 0.4721864360000154
Test Epoch20 layer3 Acc 0.752, AUC 0.8369742631912231, avg_entr 0.07392285764217377, f1 0.7519999742507935
ep20_l3_test_time 0.6212414670000044
Test Epoch20 layer4 Acc 0.7558, AUC 0.8389558792114258, avg_entr 0.07242631912231445, f1 0.7557999491691589
ep20_l4_test_time 0.8199555490000421
gc 0
Train Epoch21 Acc 0.933125 (37325/40000), AUC 0.9804083108901978
ep21_train_time 23.121458150999956
Test Epoch21 layer0 Acc 0.7082, AUC 0.7730923295021057, avg_entr 0.19389265775680542, f1 0.7081999778747559
ep21_l0_test_time 0.2943854119999969
Test Epoch21 layer1 Acc 0.731, AUC 0.8042700886726379, avg_entr 0.09997066110372543, f1 0.7310000061988831
ep21_l1_test_time 0.36463837999997395
Test Epoch21 layer2 Acc 0.748, AUC 0.8234293460845947, avg_entr 0.07560218870639801, f1 0.7480000257492065
ep21_l2_test_time 0.47161085899995214
Test Epoch21 layer3 Acc 0.756, AUC 0.8346593379974365, avg_entr 0.07420393824577332, f1 0.7559999227523804
ep21_l3_test_time 0.6206619859999591
Test Epoch21 layer4 Acc 0.7572, AUC 0.837352991104126, avg_entr 0.07051463425159454, f1 0.7572000026702881
ep21_l4_test_time 0.8194021519999524
gc 0
Train Epoch22 Acc 0.936725 (37469/40000), AUC 0.9816834926605225
ep22_train_time 23.128963951999935
Test Epoch22 layer0 Acc 0.7066, AUC 0.7743611335754395, avg_entr 0.1902204155921936, f1 0.70660001039505
ep22_l0_test_time 0.29518927500009795
Test Epoch22 layer1 Acc 0.7302, AUC 0.8015499114990234, avg_entr 0.09268051385879517, f1 0.7301999926567078
ep22_l1_test_time 0.36233644499998263
Test Epoch22 layer2 Acc 0.7492, AUC 0.8217136859893799, avg_entr 0.0701133981347084, f1 0.7491999864578247
ep22_l2_test_time 0.47680208799999946
Test Epoch22 layer3 Acc 0.7562, AUC 0.8341357707977295, avg_entr 0.06849341094493866, f1 0.7561999559402466
ep22_l3_test_time 0.6245192080000379
Test Epoch22 layer4 Acc 0.7598, AUC 0.8369264602661133, avg_entr 0.06735950708389282, f1 0.7598000168800354
ep22_l4_test_time 0.8167296100000385
gc 0
Train Epoch23 Acc 0.941175 (37647/40000), AUC 0.984501302242279
ep23_train_time 23.167878696999992
Test Epoch23 layer0 Acc 0.7146, AUC 0.7726884484291077, avg_entr 0.1897663027048111, f1 0.7146000266075134
ep23_l0_test_time 0.28953381500002706
Test Epoch23 layer1 Acc 0.7214, AUC 0.7988049983978271, avg_entr 0.08811657130718231, f1 0.7214000225067139
ep23_l1_test_time 0.3599183500000436
Test Epoch23 layer2 Acc 0.743, AUC 0.8174279928207397, avg_entr 0.06778239458799362, f1 0.7430000305175781
ep23_l2_test_time 0.4703443229999493
Test Epoch23 layer3 Acc 0.7524, AUC 0.8296406865119934, avg_entr 0.06338049471378326, f1 0.7523999810218811
ep23_l3_test_time 0.62833879599998
Test Epoch23 layer4 Acc 0.7508, AUC 0.833391010761261, avg_entr 0.06080207601189613, f1 0.7508000135421753
ep23_l4_test_time 0.8157782570000336
gc 0
Train Epoch24 Acc 0.9446 (37784/40000), AUC 0.9855613708496094
ep24_train_time 23.202539547000015
Test Epoch24 layer0 Acc 0.7092, AUC 0.7710772752761841, avg_entr 0.18561506271362305, f1 0.7092000246047974
ep24_l0_test_time 0.2918755340000416
Test Epoch24 layer1 Acc 0.727, AUC 0.7956368923187256, avg_entr 0.08511462062597275, f1 0.7269999980926514
ep24_l1_test_time 0.3613005790000443
Test Epoch24 layer2 Acc 0.744, AUC 0.8144674301147461, avg_entr 0.06450649350881577, f1 0.7439999580383301
ep24_l2_test_time 0.47153439000010167
Test Epoch24 layer3 Acc 0.7488, AUC 0.8268263339996338, avg_entr 0.06292631477117538, f1 0.7487999796867371
ep24_l3_test_time 0.6229337669999495
Test Epoch24 layer4 Acc 0.749, AUC 0.8297135829925537, avg_entr 0.061690520495176315, f1 0.7490000128746033
ep24_l4_test_time 0.8156913770000074
gc 0
Train Epoch25 Acc 0.9464 (37856/40000), AUC 0.9857348799705505
ep25_train_time 23.130642531000035
Test Epoch25 layer0 Acc 0.712, AUC 0.771519660949707, avg_entr 0.18517792224884033, f1 0.7120000123977661
ep25_l0_test_time 0.2910285399999566
Test Epoch25 layer1 Acc 0.7258, AUC 0.7964820861816406, avg_entr 0.08295783400535583, f1 0.7257999777793884
ep25_l1_test_time 0.3607093680000162
Test Epoch25 layer2 Acc 0.7438, AUC 0.8159404993057251, avg_entr 0.06381117552518845, f1 0.7437999248504639
ep25_l2_test_time 0.4709147060000305
Test Epoch25 layer3 Acc 0.7516, AUC 0.828230082988739, avg_entr 0.06177650764584541, f1 0.7516000270843506
ep25_l3_test_time 0.6288560489999782
Test Epoch25 layer4 Acc 0.7532, AUC 0.8322471976280212, avg_entr 0.05972400680184364, f1 0.7531999945640564
ep25_l4_test_time 0.8176580159999958
gc 0
Train Epoch26 Acc 0.9464 (37856/40000), AUC 0.9861952662467957
ep26_train_time 23.142883496999957
Test Epoch26 layer0 Acc 0.708, AUC 0.769665002822876, avg_entr 0.18588712811470032, f1 0.7080000638961792
ep26_l0_test_time 0.29195344699996895
Test Epoch26 layer1 Acc 0.7272, AUC 0.792719841003418, avg_entr 0.08311233669519424, f1 0.7271999716758728
ep26_l1_test_time 0.3667254849999608
Test Epoch26 layer2 Acc 0.7412, AUC 0.8110791444778442, avg_entr 0.06355633586645126, f1 0.7411999702453613
ep26_l2_test_time 0.4718924790000756
Test Epoch26 layer3 Acc 0.7512, AUC 0.8242645859718323, avg_entr 0.06059940904378891, f1 0.7512000203132629
ep26_l3_test_time 0.6255868599999985
Test Epoch26 layer4 Acc 0.7512, AUC 0.8285918235778809, avg_entr 0.05883128196001053, f1 0.7512000203132629
ep26_l4_test_time 0.819243308999944
gc 0
Train Epoch27 Acc 0.94845 (37938/40000), AUC 0.9874848127365112
ep27_train_time 23.148374712999953
Test Epoch27 layer0 Acc 0.7116, AUC 0.7695906162261963, avg_entr 0.18274155259132385, f1 0.7116000652313232
ep27_l0_test_time 0.29831141599993316
Test Epoch27 layer1 Acc 0.7228, AUC 0.7938451766967773, avg_entr 0.08055078983306885, f1 0.7227999567985535
ep27_l1_test_time 0.36093465900000865
Test Epoch27 layer2 Acc 0.746, AUC 0.8119918704032898, avg_entr 0.06174982339143753, f1 0.7459999918937683
ep27_l2_test_time 0.47148946100003286
Test Epoch27 layer3 Acc 0.751, AUC 0.8257784843444824, avg_entr 0.05663023516535759, f1 0.7509999871253967
ep27_l3_test_time 0.6313440589999573
Test Epoch27 layer4 Acc 0.749, AUC 0.8294476270675659, avg_entr 0.055019982159137726, f1 0.7490000128746033
ep27_l4_test_time 0.8167928990000064
gc 0
Train Epoch28 Acc 0.9501 (38004/40000), AUC 0.9879021644592285
ep28_train_time 23.118353703000025
Test Epoch28 layer0 Acc 0.7064, AUC 0.7688665986061096, avg_entr 0.18275941908359528, f1 0.7063999772071838
ep28_l0_test_time 0.29111324799998783
Test Epoch28 layer1 Acc 0.7224, AUC 0.7912673950195312, avg_entr 0.07764041423797607, f1 0.7224000096321106
ep28_l1_test_time 0.3615170750000516
Test Epoch28 layer2 Acc 0.741, AUC 0.8103227615356445, avg_entr 0.06002954766154289, f1 0.7409999370574951
ep28_l2_test_time 0.4760608779999984
Test Epoch28 layer3 Acc 0.7452, AUC 0.8247354030609131, avg_entr 0.05717160552740097, f1 0.745199978351593
ep28_l3_test_time 0.6219182879999607
Test Epoch28 layer4 Acc 0.7466, AUC 0.8285281658172607, avg_entr 0.05586490407586098, f1 0.7465999126434326
ep28_l4_test_time 0.8172954210000398
gc 0
Train Epoch29 Acc 0.95065 (38026/40000), AUC 0.9880739450454712
ep29_train_time 23.111264737000056
Test Epoch29 layer0 Acc 0.706, AUC 0.7684314250946045, avg_entr 0.1835622787475586, f1 0.7059999704360962
ep29_l0_test_time 0.2897147160000486
Test Epoch29 layer1 Acc 0.7192, AUC 0.7925789952278137, avg_entr 0.07491322606801987, f1 0.719200074672699
ep29_l1_test_time 0.3614592559999892
Test Epoch29 layer2 Acc 0.74, AUC 0.811686635017395, avg_entr 0.059797339141368866, f1 0.7400000095367432
ep29_l2_test_time 0.4722072009999465
Test Epoch29 layer3 Acc 0.7452, AUC 0.8246247172355652, avg_entr 0.05456975847482681, f1 0.745199978351593
ep29_l3_test_time 0.6299133100000063
Test Epoch29 layer4 Acc 0.7466, AUC 0.8284764289855957, avg_entr 0.05287092179059982, f1 0.7465999126434326
ep29_l4_test_time 0.8153523849999829
gc 0
Train Epoch30 Acc 0.950625 (38025/40000), AUC 0.988427996635437
ep30_train_time 23.14093470800003
Test Epoch30 layer0 Acc 0.7084, AUC 0.768439531326294, avg_entr 0.18217991292476654, f1 0.7084000110626221
ep30_l0_test_time 0.2910688319999508
Test Epoch30 layer1 Acc 0.7216, AUC 0.7890913486480713, avg_entr 0.07863450795412064, f1 0.7215999960899353
ep30_l1_test_time 0.3615826620000462
Test Epoch30 layer2 Acc 0.7422, AUC 0.8098732829093933, avg_entr 0.060489408671855927, f1 0.7422000169754028
ep30_l2_test_time 0.472064320999948
Test Epoch30 layer3 Acc 0.7488, AUC 0.8233530521392822, avg_entr 0.05604812130331993, f1 0.7487999796867371
ep30_l3_test_time 0.6234922819999156
Test Epoch30 layer4 Acc 0.7494, AUC 0.8273577690124512, avg_entr 0.05379592254757881, f1 0.7494000196456909
ep30_l4_test_time 0.8216052309999213
gc 0
Train Epoch31 Acc 0.952175 (38087/40000), AUC 0.9889897108078003
ep31_train_time 23.178838803000076
Test Epoch31 layer0 Acc 0.71, AUC 0.7679867744445801, avg_entr 0.1804739087820053, f1 0.7099999785423279
ep31_l0_test_time 0.29932728599999336
Test Epoch31 layer1 Acc 0.7212, AUC 0.789839506149292, avg_entr 0.07587652653455734, f1 0.7211999893188477
ep31_l1_test_time 0.36208035599997856
Test Epoch31 layer2 Acc 0.744, AUC 0.8097890019416809, avg_entr 0.05899392068386078, f1 0.7439999580383301
ep31_l2_test_time 0.47423019300003943
Test Epoch31 layer3 Acc 0.7488, AUC 0.8235851526260376, avg_entr 0.05458993464708328, f1 0.7487999796867371
ep31_l3_test_time 0.6246333189999405
Test Epoch31 layer4 Acc 0.751, AUC 0.827383279800415, avg_entr 0.053056757897138596, f1 0.7509999871253967
ep31_l4_test_time 0.8175378839999894
gc 0
Train Epoch32 Acc 0.952675 (38107/40000), AUC 0.9894939064979553
ep32_train_time 23.177350120999904
Test Epoch32 layer0 Acc 0.708, AUC 0.7675617933273315, avg_entr 0.18105776607990265, f1 0.7080000638961792
ep32_l0_test_time 0.295163701999968
Test Epoch32 layer1 Acc 0.7208, AUC 0.7890459299087524, avg_entr 0.0771695002913475, f1 0.7207999229431152
ep32_l1_test_time 0.36259362300006615
Test Epoch32 layer2 Acc 0.7432, AUC 0.8093906044960022, avg_entr 0.059021253138780594, f1 0.7432000041007996
ep32_l2_test_time 0.4731064369999558
Test Epoch32 layer3 Acc 0.7488, AUC 0.8235036134719849, avg_entr 0.053688403218984604, f1 0.7487999796867371
ep32_l3_test_time 0.6251774590000423
Test Epoch32 layer4 Acc 0.7488, AUC 0.8272291421890259, avg_entr 0.05216442793607712, f1 0.7487999796867371
ep32_l4_test_time 0.8254786840000179
gc 0
Train Epoch33 Acc 0.954775 (38191/40000), AUC 0.9896107316017151
ep33_train_time 23.197509148999984
Test Epoch33 layer0 Acc 0.7078, AUC 0.7675034403800964, avg_entr 0.18100087344646454, f1 0.7077999711036682
ep33_l0_test_time 0.28863585299995975
Test Epoch33 layer1 Acc 0.7242, AUC 0.7887213230133057, avg_entr 0.07669545710086823, f1 0.7242000699043274
ep33_l1_test_time 0.361263696999913
Test Epoch33 layer2 Acc 0.7412, AUC 0.8091079592704773, avg_entr 0.05784790590405464, f1 0.7411999702453613
ep33_l2_test_time 0.4721232560000317
Test Epoch33 layer3 Acc 0.7498, AUC 0.8232115507125854, avg_entr 0.05268217250704765, f1 0.7498000264167786
ep33_l3_test_time 0.6218805529999827
Test Epoch33 layer4 Acc 0.7494, AUC 0.8271903395652771, avg_entr 0.05044884607195854, f1 0.7494000196456909
ep33_l4_test_time 0.814576777999946
gc 0
Train Epoch34 Acc 0.953625 (38145/40000), AUC 0.9893174171447754
ep34_train_time 23.097476325999992
Test Epoch34 layer0 Acc 0.7062, AUC 0.7669779062271118, avg_entr 0.18111056089401245, f1 0.7062000036239624
ep34_l0_test_time 0.2923482310000054
Test Epoch34 layer1 Acc 0.7214, AUC 0.7883468866348267, avg_entr 0.07628149539232254, f1 0.7214000225067139
ep34_l1_test_time 0.3640403819999847
Test Epoch34 layer2 Acc 0.7434, AUC 0.8087205290794373, avg_entr 0.06046517938375473, f1 0.743399977684021
ep34_l2_test_time 0.4764958089999709
Test Epoch34 layer3 Acc 0.7472, AUC 0.8224053382873535, avg_entr 0.05421696603298187, f1 0.747200071811676
ep34_l3_test_time 0.6266110420000359
Test Epoch34 layer4 Acc 0.7478, AUC 0.8264566659927368, avg_entr 0.05273905023932457, f1 0.7477999925613403
ep34_l4_test_time 0.8216460469999447
gc 0
Train Epoch35 Acc 0.953925 (38157/40000), AUC 0.989780068397522
ep35_train_time 23.191575518000036
Test Epoch35 layer0 Acc 0.7068, AUC 0.7668043375015259, avg_entr 0.17968662083148956, f1 0.7067999839782715
ep35_l0_test_time 0.29438389500000994
Test Epoch35 layer1 Acc 0.7218, AUC 0.7874656319618225, avg_entr 0.0754447802901268, f1 0.7217999696731567
ep35_l1_test_time 0.36531187499997486
Test Epoch35 layer2 Acc 0.7412, AUC 0.80846107006073, avg_entr 0.05853143334388733, f1 0.7411999702453613
ep35_l2_test_time 0.47236777199998414
Test Epoch35 layer3 Acc 0.746, AUC 0.8224914073944092, avg_entr 0.05274589732289314, f1 0.7459999918937683
ep35_l3_test_time 0.6309221199999229
Test Epoch35 layer4 Acc 0.7456, AUC 0.8262277841567993, avg_entr 0.05127578228712082, f1 0.7455999851226807
ep35_l4_test_time 0.8190187430000151
gc 0
Train Epoch36 Acc 0.954575 (38183/40000), AUC 0.9899091720581055
ep36_train_time 23.14526847100001
Test Epoch36 layer0 Acc 0.7068, AUC 0.7668193578720093, avg_entr 0.17998705804347992, f1 0.7067999839782715
ep36_l0_test_time 0.290946071999997
Test Epoch36 layer1 Acc 0.7232, AUC 0.7874716520309448, avg_entr 0.07572875171899796, f1 0.7232000827789307
ep36_l1_test_time 0.36637921799990636
Test Epoch36 layer2 Acc 0.7428, AUC 0.8085646629333496, avg_entr 0.058575842529535294, f1 0.7427999973297119
ep36_l2_test_time 0.47648452999999336
Test Epoch36 layer3 Acc 0.748, AUC 0.8225746750831604, avg_entr 0.052780259400606155, f1 0.7480000257492065
ep36_l3_test_time 0.6246574629999486
Test Epoch36 layer4 Acc 0.7478, AUC 0.8264339566230774, avg_entr 0.05107593163847923, f1 0.7477999925613403
ep36_l4_test_time 0.8188653770000656
gc 0
Train Epoch37 Acc 0.954475 (38179/40000), AUC 0.9899488687515259
ep37_train_time 23.144392573999994
Test Epoch37 layer0 Acc 0.7064, AUC 0.7664777040481567, avg_entr 0.1797565221786499, f1 0.7063999772071838
ep37_l0_test_time 0.29114722899998924
Test Epoch37 layer1 Acc 0.7232, AUC 0.7872538566589355, avg_entr 0.0753345862030983, f1 0.7232000827789307
ep37_l1_test_time 0.3623155960000304
Test Epoch37 layer2 Acc 0.7422, AUC 0.8080424666404724, avg_entr 0.05797722190618515, f1 0.7422000169754028
ep37_l2_test_time 0.4754988060000187
Test Epoch37 layer3 Acc 0.7486, AUC 0.822250485420227, avg_entr 0.051477618515491486, f1 0.7486000061035156
ep37_l3_test_time 0.6292163269999946
Test Epoch37 layer4 Acc 0.7492, AUC 0.8261799812316895, avg_entr 0.04985838010907173, f1 0.7491999864578247
ep37_l4_test_time 0.8184764650000034
gc 0
Train Epoch38 Acc 0.9549 (38196/40000), AUC 0.989888608455658
ep38_train_time 23.150688527000057
Test Epoch38 layer0 Acc 0.7076, AUC 0.766308605670929, avg_entr 0.18030354380607605, f1 0.707599937915802
ep38_l0_test_time 0.292721136000182
Test Epoch38 layer1 Acc 0.7222, AUC 0.7865826487541199, avg_entr 0.07526081055402756, f1 0.7222000360488892
ep38_l1_test_time 0.3624650090000614
Test Epoch38 layer2 Acc 0.7404, AUC 0.8071902990341187, avg_entr 0.057661768049001694, f1 0.7404000163078308
ep38_l2_test_time 0.4758973689999948
Test Epoch38 layer3 Acc 0.7458, AUC 0.8208118677139282, avg_entr 0.05142057314515114, f1 0.7458000183105469
ep38_l3_test_time 0.623328684000171
Test Epoch38 layer4 Acc 0.7476, AUC 0.8251634836196899, avg_entr 0.04929665848612785, f1 0.7476000189781189
ep38_l4_test_time 0.820635411000012
gc 0
Train Epoch39 Acc 0.956 (38240/40000), AUC 0.9899227023124695
ep39_train_time 23.144324116999996
Test Epoch39 layer0 Acc 0.7072, AUC 0.7663549184799194, avg_entr 0.17922070622444153, f1 0.7071999907493591
ep39_l0_test_time 0.292110232999903
Test Epoch39 layer1 Acc 0.7226, AUC 0.7868236303329468, avg_entr 0.0744360014796257, f1 0.722599983215332
ep39_l1_test_time 0.3626088109999728
Test Epoch39 layer2 Acc 0.7422, AUC 0.807353138923645, avg_entr 0.05789656937122345, f1 0.7422000169754028
ep39_l2_test_time 0.4763377770000261
Test Epoch39 layer3 Acc 0.7486, AUC 0.8212757110595703, avg_entr 0.05195356905460358, f1 0.7486000061035156
ep39_l3_test_time 0.623254488999919
Test Epoch39 layer4 Acc 0.7482, AUC 0.8254905939102173, avg_entr 0.05036117509007454, f1 0.748199999332428
ep39_l4_test_time 0.8181802400001743
gc 0
Train Epoch40 Acc 0.9558 (38232/40000), AUC 0.9900484085083008
ep40_train_time 23.161101608999843
Test Epoch40 layer0 Acc 0.7068, AUC 0.7662298679351807, avg_entr 0.1788026988506317, f1 0.7067999839782715
ep40_l0_test_time 0.2931951640000534
Test Epoch40 layer1 Acc 0.7228, AUC 0.7867366075515747, avg_entr 0.07293051481246948, f1 0.7227999567985535
ep40_l1_test_time 0.3612430610000956
Test Epoch40 layer2 Acc 0.7418, AUC 0.8070374727249146, avg_entr 0.057048652321100235, f1 0.74180006980896
ep40_l2_test_time 0.4766007549999358
Test Epoch40 layer3 Acc 0.747, AUC 0.8209188580513, avg_entr 0.05124320089817047, f1 0.746999979019165
ep40_l3_test_time 0.6218406269999832
Test Epoch40 layer4 Acc 0.7462, AUC 0.8250961303710938, avg_entr 0.04967010021209717, f1 0.7462000250816345
ep40_l4_test_time 0.8193661489999613
gc 0
Train Epoch41 Acc 0.955 (38200/40000), AUC 0.9898295402526855
ep41_train_time 23.25336209099987
Test Epoch41 layer0 Acc 0.7082, AUC 0.7661813497543335, avg_entr 0.1792486310005188, f1 0.7081999778747559
ep41_l0_test_time 0.29195776000005935
Test Epoch41 layer1 Acc 0.7218, AUC 0.7864391803741455, avg_entr 0.07403832674026489, f1 0.7217999696731567
ep41_l1_test_time 0.3633697769998889
Test Epoch41 layer2 Acc 0.741, AUC 0.8076827526092529, avg_entr 0.05839778482913971, f1 0.7409999370574951
ep41_l2_test_time 0.4732877480000752
Test Epoch41 layer3 Acc 0.7484, AUC 0.8213508129119873, avg_entr 0.052780259400606155, f1 0.7483999729156494
ep41_l3_test_time 0.6278796699998566
Test Epoch41 layer4 Acc 0.7482, AUC 0.8255593776702881, avg_entr 0.05100470036268234, f1 0.748199999332428
ep41_l4_test_time 0.819054769000104
gc 0
Train Epoch42 Acc 0.956825 (38273/40000), AUC 0.9901772737503052
ep42_train_time 23.179560664000064
Test Epoch42 layer0 Acc 0.7076, AUC 0.7663116455078125, avg_entr 0.1789175122976303, f1 0.707599937915802
ep42_l0_test_time 0.29361227499998677
Test Epoch42 layer1 Acc 0.7222, AUC 0.7863826751708984, avg_entr 0.0743241161108017, f1 0.7222000360488892
ep42_l1_test_time 0.3616969060001338
Test Epoch42 layer2 Acc 0.7412, AUC 0.8067723512649536, avg_entr 0.05748528987169266, f1 0.7411999702453613
ep42_l2_test_time 0.4800280849999581
Test Epoch42 layer3 Acc 0.7474, AUC 0.8205596208572388, avg_entr 0.05179180949926376, f1 0.7473999857902527
ep42_l3_test_time 0.6228055019998919
Test Epoch42 layer4 Acc 0.7478, AUC 0.8248715400695801, avg_entr 0.05004033446311951, f1 0.7477999925613403
ep42_l4_test_time 0.8164614889999484
gc 0
Train Epoch43 Acc 0.9568 (38272/40000), AUC 0.9904011487960815
ep43_train_time 23.15165069999989
Test Epoch43 layer0 Acc 0.709, AUC 0.7663443088531494, avg_entr 0.17892791330814362, f1 0.7090000510215759
ep43_l0_test_time 0.2911219779998646
Test Epoch43 layer1 Acc 0.723, AUC 0.7866129875183105, avg_entr 0.07416056841611862, f1 0.7229999899864197
ep43_l1_test_time 0.36182148900002176
Test Epoch43 layer2 Acc 0.7408, AUC 0.8071156740188599, avg_entr 0.057423267513513565, f1 0.7408000230789185
ep43_l2_test_time 0.47270158800006357
Test Epoch43 layer3 Acc 0.747, AUC 0.8208983540534973, avg_entr 0.051584936678409576, f1 0.746999979019165
ep43_l3_test_time 0.6290569749999122
Test Epoch43 layer4 Acc 0.747, AUC 0.8251676559448242, avg_entr 0.04976705461740494, f1 0.746999979019165
ep43_l4_test_time 0.8197838560001856
gc 0
Train Epoch44 Acc 0.956675 (38267/40000), AUC 0.990392804145813
ep44_train_time 23.142915957000014
Test Epoch44 layer0 Acc 0.708, AUC 0.766321063041687, avg_entr 0.17873062193393707, f1 0.7080000638961792
ep44_l0_test_time 0.2928717140000572
Test Epoch44 layer1 Acc 0.7232, AUC 0.7864739894866943, avg_entr 0.0745682567358017, f1 0.7232000827789307
ep44_l1_test_time 0.3634928100000252
Test Epoch44 layer2 Acc 0.7408, AUC 0.8072623610496521, avg_entr 0.05765792354941368, f1 0.7408000230789185
ep44_l2_test_time 0.4774383450001096
Test Epoch44 layer3 Acc 0.7474, AUC 0.8210079669952393, avg_entr 0.05190810561180115, f1 0.7473999857902527
ep44_l3_test_time 0.6256877300002088
Test Epoch44 layer4 Acc 0.7474, AUC 0.8253132104873657, avg_entr 0.05012212693691254, f1 0.7473999857902527
ep44_l4_test_time 0.8168316300000242
gc 0
Train Epoch45 Acc 0.9555 (38220/40000), AUC 0.9903994798660278
ep45_train_time 23.140423960000135
Test Epoch45 layer0 Acc 0.7086, AUC 0.7662959098815918, avg_entr 0.17858082056045532, f1 0.7085999846458435
ep45_l0_test_time 0.30377984200003993
Test Epoch45 layer1 Acc 0.7224, AUC 0.7862703800201416, avg_entr 0.07439178228378296, f1 0.7224000096321106
ep45_l1_test_time 0.3644822759999897
Test Epoch45 layer2 Acc 0.741, AUC 0.8070403337478638, avg_entr 0.05730597674846649, f1 0.7409999370574951
ep45_l2_test_time 0.4744834750001701
Test Epoch45 layer3 Acc 0.747, AUC 0.8209226727485657, avg_entr 0.05174156650900841, f1 0.746999979019165
ep45_l3_test_time 0.6228686049998942
Test Epoch45 layer4 Acc 0.7472, AUC 0.8251975774765015, avg_entr 0.05001235753297806, f1 0.747200071811676
ep45_l4_test_time 0.823091057000056
gc 0
Train Epoch46 Acc 0.95575 (38230/40000), AUC 0.9904111623764038
ep46_train_time 23.179531489999817
Test Epoch46 layer0 Acc 0.707, AUC 0.7662423849105835, avg_entr 0.17833711206912994, f1 0.7070000171661377
ep46_l0_test_time 0.29160155399995347
Test Epoch46 layer1 Acc 0.7214, AUC 0.7864758968353271, avg_entr 0.07379913330078125, f1 0.7214000225067139
ep46_l1_test_time 0.3623191490000863
Test Epoch46 layer2 Acc 0.7418, AUC 0.8071845769882202, avg_entr 0.05724102258682251, f1 0.74180006980896
ep46_l2_test_time 0.4720748109998567
Test Epoch46 layer3 Acc 0.7476, AUC 0.8210735321044922, avg_entr 0.05122946947813034, f1 0.7476000189781189
ep46_l3_test_time 0.6294958770001813
Test Epoch46 layer4 Acc 0.7478, AUC 0.8252139687538147, avg_entr 0.0495709553360939, f1 0.7477999925613403
ep46_l4_test_time 0.8184091230000377
gc 0
Train Epoch47 Acc 0.957425 (38297/40000), AUC 0.9901567697525024
ep47_train_time 23.144487546999926
Test Epoch47 layer0 Acc 0.7074, AUC 0.7662915587425232, avg_entr 0.17827752232551575, f1 0.7074000239372253
ep47_l0_test_time 0.2935631259999809
Test Epoch47 layer1 Acc 0.7226, AUC 0.786456823348999, avg_entr 0.07388722151517868, f1 0.722599983215332
ep47_l1_test_time 0.3741275800000494
Test Epoch47 layer2 Acc 0.7408, AUC 0.8071375489234924, avg_entr 0.057237617671489716, f1 0.7408000230789185
ep47_l2_test_time 0.4742964730000949
Test Epoch47 layer3 Acc 0.7478, AUC 0.820937991142273, avg_entr 0.051470331847667694, f1 0.7477999925613403
ep47_l3_test_time 0.6239769669998623
Test Epoch47 layer4 Acc 0.748, AUC 0.8251864910125732, avg_entr 0.04979972168803215, f1 0.7480000257492065
ep47_l4_test_time 0.8162680200000523
gc 0
Train Epoch48 Acc 0.95615 (38246/40000), AUC 0.990221381187439
ep48_train_time 23.17908710400002
Test Epoch48 layer0 Acc 0.7074, AUC 0.7662869095802307, avg_entr 0.1781715601682663, f1 0.7074000239372253
ep48_l0_test_time 0.29181870399997933
Test Epoch48 layer1 Acc 0.7228, AUC 0.7864750623703003, avg_entr 0.07395483553409576, f1 0.7227999567985535
ep48_l1_test_time 0.3769708330000867
Test Epoch48 layer2 Acc 0.7402, AUC 0.8073344230651855, avg_entr 0.05730307474732399, f1 0.7401999831199646
ep48_l2_test_time 0.4784882640001342
Test Epoch48 layer3 Acc 0.7478, AUC 0.8210954666137695, avg_entr 0.051516078412532806, f1 0.7477999925613403
ep48_l3_test_time 0.6243063889999121
Test Epoch48 layer4 Acc 0.7488, AUC 0.8252806067466736, avg_entr 0.04979289695620537, f1 0.7487999796867371
ep48_l4_test_time 0.8186502710000241
gc 0
Train Epoch49 Acc 0.955525 (38221/40000), AUC 0.9905224442481995
ep49_train_time 23.164460583999926
Test Epoch49 layer0 Acc 0.7074, AUC 0.7662712335586548, avg_entr 0.17822545766830444, f1 0.7074000239372253
ep49_l0_test_time 0.2952578019999237
Test Epoch49 layer1 Acc 0.7232, AUC 0.7864652872085571, avg_entr 0.07401446253061295, f1 0.7232000827789307
ep49_l1_test_time 0.3635313180000139
Test Epoch49 layer2 Acc 0.7408, AUC 0.8072332739830017, avg_entr 0.05705105513334274, f1 0.7408000230789185
ep49_l2_test_time 0.4730215910001334
Test Epoch49 layer3 Acc 0.7476, AUC 0.8210198879241943, avg_entr 0.051361724734306335, f1 0.7476000189781189
ep49_l3_test_time 0.6234585180000067
Test Epoch49 layer4 Acc 0.7482, AUC 0.8252274990081787, avg_entr 0.04965043067932129, f1 0.748199999332428
ep49_l4_test_time 0.8234444279999025
Best AUC tensor(0.7750) 13 3
train_as_loss [[8.46663871e+01 5.81979598e+01 5.18961675e+01 5.03946764e+01
  4.98571189e+01 4.96085255e+01 4.94740706e+01 4.93933909e+01
  4.93412710e+01 4.93057050e+01 4.92803838e+01 4.92617454e+01
  4.92476449e+01 4.92367373e+01 4.92281348e+01 4.92227619e+01
  4.92195630e+01 4.92165766e+01 4.92137957e+01 4.92118241e+01
  4.92105350e+01 4.92092507e+01 4.92079731e+01 4.92070171e+01
  4.92063660e+01 4.92056950e+01 4.92050087e+01 4.92044787e+01
  4.92041091e+01 4.92037229e+01 4.92033177e+01 4.92030013e+01
  4.92027775e+01 4.92025408e+01 4.92022901e+01 4.92020910e+01
  4.92019523e+01 4.92018009e+01 4.92016423e+01 4.92015163e+01
  4.92014244e+01 4.92013272e+01 4.92012238e+01 4.92011441e+01
  4.92010804e+01 4.92010173e+01 4.92009549e+01 4.92008976e+01
  4.92008590e+01 4.92008208e+01]
 [2.31438113e+00 2.77933109e-04 1.72988700e-05 4.34137264e-06
  1.64936939e-06 7.75680252e-07 4.02908381e-07 2.31176356e-07
  1.40955262e-07 9.13583422e-08 6.06672816e-08 4.29302208e-08
  2.95516682e-08 2.04392081e-08 1.35909687e-07 1.50339983e-08
  9.96019188e-09 8.47469099e-09 7.16603219e-09 6.23397352e-09
  5.67139917e-09 5.29384994e-09 4.93263494e-09 4.33416223e-09
  4.16099822e-09 3.90681175e-09 3.68668291e-09 3.46811209e-09
  3.35756584e-09 3.25780033e-09 3.20466780e-09 3.00802807e-09
  2.93953153e-09 2.87875991e-09 2.75026947e-09 2.72205607e-09
  2.64131600e-09 2.58065944e-09 2.52596255e-09 2.53173827e-09
  2.48581069e-09 2.40851690e-09 2.32327464e-09 2.42966047e-09
  2.37154529e-09 2.26698922e-09 2.18164049e-09 2.43181110e-09
  2.35876558e-09 2.21100101e-09]
 [2.18588270e+00 2.80741556e-04 1.69390861e-05 3.96024166e-06
  1.40474052e-06 6.46171138e-07 3.09693076e-07 1.77819956e-07
  1.04383707e-07 6.82027299e-08 5.01613051e-08 3.55763077e-08
  2.48022223e-08 1.51753285e-08 3.15643415e-08 9.86083024e-09
  6.73155770e-09 6.06341595e-09 4.81697594e-09 3.90401971e-09
  3.55802859e-09 3.44290619e-09 3.25953600e-09 2.53102589e-09
  2.45256708e-09 2.29403779e-09 2.15299610e-09 1.94174767e-09
  1.89634082e-09 1.85507546e-09 1.88651352e-09 1.69888997e-09
  1.64782942e-09 1.64636308e-09 1.52372234e-09 1.51223800e-09
  1.47551359e-09 1.44306174e-09 1.42448058e-09 1.41025393e-09
  1.38914081e-09 1.35493924e-09 1.28129773e-09 1.38167335e-09
  1.33892985e-09 1.27591404e-09 1.22325651e-09 1.39217094e-09
  1.33698588e-09 1.27201868e-09]
 [2.51828206e+00 3.02308803e-04 1.62017651e-05 4.55953378e-06
  1.81814877e-06 9.43957940e-07 4.67047726e-07 2.98950741e-07
  1.80334902e-07 1.20698944e-07 9.60579493e-08 7.15812853e-08
  5.30872998e-08 3.00244858e-08 3.95529399e-08 1.57223708e-08
  1.18838763e-08 1.11793887e-08 8.97696393e-09 6.57630516e-09
  6.15162411e-09 5.99576397e-09 5.83450041e-09 4.13663878e-09
  3.99845962e-09 3.75692383e-09 3.51222713e-09 3.09668459e-09
  3.01164841e-09 2.94385205e-09 3.00263987e-09 2.74837721e-09
  2.62531822e-09 2.59769010e-09 2.40029167e-09 2.43029829e-09
  2.34160348e-09 2.31132790e-09 2.24039124e-09 2.25883610e-09
  2.21268513e-09 2.13140209e-09 2.05151723e-09 2.20944650e-09
  2.16403307e-09 2.04726886e-09 1.97602059e-09 2.30787281e-09
  2.29930191e-09 2.12435894e-09]
 [2.04176856e+00 5.46198068e-04 2.07922235e-05 5.69505428e-06
  2.23411732e-06 1.23316298e-06 5.77737952e-07 4.11280086e-07
  2.51100959e-07 1.88551950e-07 1.61247395e-07 1.35066326e-07
  1.10751572e-07 5.52339495e-08 7.87143629e-08 2.21260348e-08
  1.80833103e-08 1.80253925e-08 1.44122141e-08 8.36037242e-09
  7.77060578e-09 7.73671750e-09 8.19091633e-09 4.52564889e-09
  4.29329267e-09 4.24453492e-09 3.85871564e-09 3.19845464e-09
  3.07207837e-09 3.00000229e-09 3.19840383e-09 2.84592605e-09
  2.67109954e-09 2.64830588e-09 2.40821376e-09 2.46079837e-09
  2.34689823e-09 2.32281815e-09 2.25544477e-09 2.29932411e-09
  2.27153280e-09 2.17213400e-09 2.10029238e-09 2.29635651e-09
  2.30000302e-09 2.16099736e-09 2.08857445e-09 2.42693483e-09
  2.46361164e-09 2.26659546e-09]]
train_ae_loss [[4.67920292 3.49225494 4.78387023 5.29278434 5.5797502  5.82544232
  6.01901771 6.03505712 6.06041992 6.09895229 6.04938595 5.89058927
  5.75588329 5.57822367 5.38691546 4.82221721 4.65841955 4.49575331
  4.32959816 4.08553464 4.03452492 3.93736171 3.87802    3.73401216
  3.7156632  3.6627779  3.64701232 3.59288596 3.56678962 3.54331825
  3.54130417 3.5126895  3.50656328 3.53330543 3.50504515 3.50377914
  3.48892941 3.48523207 3.4790664  3.46621626 3.47407998 3.49398171
  3.49435053 3.46827011 3.46656434 3.4542457  3.47800287 3.45696678
  3.46056513 3.47199556]
 [4.11474836 3.54029341 5.06058812 5.62311096 5.83624496 5.94431348
  6.05764925 5.94745447 5.91750654 5.9136035  5.87186063 5.64488201
  5.54870433 5.31998322 5.09496296 4.35734839 4.13615776 3.88003125
  3.59438531 3.22957999 3.1451764  3.0244508  2.90931192 2.68361898
  2.63112853 2.54901616 2.55546124 2.45586766 2.40787936 2.38742294
  2.37824254 2.31813239 2.31244177 2.3277396  2.30999474 2.29733375
  2.27744948 2.26149553 2.24767682 2.24488519 2.2346518  2.26232392
  2.26340774 2.23787928 2.23823667 2.22249802 2.24994345 2.21320676
  2.23860528 2.2313803 ]
 [3.94245497 3.25456049 4.68944788 5.36491494 5.45313123 5.34496813
  5.40287804 5.20035982 5.14218613 5.10305399 5.02784926 4.74486975
  4.68180009 4.36784646 4.12236998 3.27612745 3.12651453 2.92931407
  2.69008208 2.37798523 2.32855337 2.23332305 2.14678706 1.94120947
  1.91649869 1.84478601 1.85534583 1.75652848 1.72975755 1.69783246
  1.70870169 1.64841254 1.6408131  1.64805412 1.64631941 1.63475223
  1.61217587 1.5916006  1.57952767 1.58689493 1.56562482 1.60100139
  1.58782609 1.57441173 1.58167696 1.57551789 1.58088742 1.56144178
  1.56981076 1.5623115 ]
 [4.90815251 3.72922746 4.70839185 5.41571358 5.54216911 5.21765207
  5.22034566 4.91001111 4.8371561  4.75604827 4.67511357 4.31866165
  4.272424   3.8889864  3.7161816  2.90878749 2.79730833 2.62359805
  2.38943139 2.10006746 2.05775941 1.9646994  1.90003338 1.70269291
  1.68101272 1.6182379  1.62230469 1.53548662 1.50566965 1.47781721
  1.49215107 1.4297971  1.41727127 1.42722446 1.42553981 1.41037905
  1.38984883 1.36899894 1.36271794 1.36892238 1.35443777 1.37611817
  1.37283393 1.3516329  1.37209992 1.36286927 1.36312096 1.34852281
  1.34997758 1.34445492]
 [4.66296207 2.5669123  3.66701937 4.44300131 4.99098501 4.3729735
  4.3044149  3.96876143 3.93053861 3.81740769 3.73484639 3.39843407
  3.41258347 3.07267688 2.95035788 2.27759194 2.19653235 2.06111082
  1.86844461 1.63845229 1.60864452 1.53195168 1.48550702 1.32442983
  1.30900147 1.26009853 1.26395753 1.19415333 1.16998624 1.14888729
  1.1623943  1.11117534 1.10053255 1.10830134 1.1070429  1.09397106
  1.07798381 1.06324246 1.05566694 1.06141012 1.04920671 1.06651196
  1.06448748 1.04970707 1.06385523 1.05718978 1.05712437 1.04556173
  1.04636467 1.04185667]]
valid_acc (5, 50)
valid_AUC (5, 50)
train_acc (50,)
total_train+valid_time 1292.4010783170002
Start Testing
Load ckpt at ckpt/imdb_transformeralside_l5_256_sidead//imdb_transformeralside_l5_side.pt
gc 9
Test layer0 Acc 0.7264, AUC 0.801669716835022, avg_entr 0.25783953070640564, f1 0.7264000177383423
l0_test_time 0.28773675399997956
gc 0
Test layer1 Acc 0.7562, AUC 0.8351861238479614, avg_entr 0.2225380390882492, f1 0.7561999559402466
l1_test_time 0.3622612320000371
gc 0
Test layer2 Acc 0.7738, AUC 0.8565150499343872, avg_entr 0.17848317325115204, f1 0.7738000154495239
l2_test_time 0.4714988670000366
gc 0
Test layer3 Acc 0.7804, AUC 0.8642617464065552, avg_entr 0.14497597515583038, f1 0.7803999781608582
l3_test_time 0.6215015430000221
gc 0
Test layer4 Acc 0.7862, AUC 0.867345929145813, avg_entr 0.14205355942249298, f1 0.7861999869346619
l4_test_time 0.8256295860001046
gc 0
Test threshold 0.1 Acc 0.7828, AUC 0.8498849272727966, avg_entr 0.19742730259895325, f1 0.782800018787384
t0.1_test_time 0.6147340350000832
gc 0
Test threshold 0.2 Acc 0.7798, AUC 0.8417212963104248, avg_entr 0.20162175595760345, f1 0.7797999978065491
t0.2_test_time 0.5458416579999721
gc 0
Test threshold 0.3 Acc 0.7754, AUC 0.8344014883041382, avg_entr 0.20798422396183014, f1 0.775399923324585
t0.3_test_time 0.5069772760000433
gc 0
Test threshold 0.4 Acc 0.7694, AUC 0.828511118888855, avg_entr 0.2193397432565689, f1 0.7694000005722046
t0.4_test_time 0.4835286929999256
gc 0
Test threshold 0.5 Acc 0.767, AUC 0.8235324025154114, avg_entr 0.23304250836372375, f1 0.7670000195503235
t0.5_test_time 0.4495791489998737
gc 0
Test threshold 0.6 Acc 0.7632, AUC 0.8194377422332764, avg_entr 0.24920691549777985, f1 0.7631999850273132
t0.6_test_time 0.4172688129999642
gc 0
Test threshold 0.7 Acc 0.7562, AUC 0.8132213354110718, avg_entr 0.26869508624076843, f1 0.7561999559402466
t0.7_test_time 0.39193693099991833
gc 0
Test threshold 0.8 Acc 0.748, AUC 0.809326171875, avg_entr 0.29174092411994934, f1 0.7480000257492065
t0.8_test_time 0.37582734600005097
gc 0
Test threshold 0.9 Acc 0.7422, AUC 0.8073546886444092, avg_entr 0.31819069385528564, f1 0.7422000169754028
t0.9_test_time 0.3674951999998939
