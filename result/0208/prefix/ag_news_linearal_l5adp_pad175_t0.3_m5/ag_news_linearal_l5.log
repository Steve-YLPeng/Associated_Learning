total count words 102019
vocab size 30000
found 26754 words in glove
Load ckpt from ckpt/ag_news_linearal_l5adp_pad175_t0.3_m4//ag_news_linearal_l5_prefix.pt
model: LinearModelML(
  (layers): ModuleList(
    (0): EMBLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Embedding(30000, 300)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=4, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=4, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): LinearLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Sequential(
          (0): Linear(in_features=300, out_features=300, bias=True)
          (1): ELU(alpha=1.0)
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): LinearLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Sequential(
          (0): Linear(in_features=300, out_features=300, bias=True)
          (1): ELU(alpha=1.0)
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): LinearLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Sequential(
          (0): Linear(in_features=300, out_features=300, bias=True)
          (1): ELU(alpha=1.0)
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): LinearLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Sequential(
          (0): Linear(in_features=300, out_features=300, bias=True)
          (1): ELU(alpha=1.0)
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.weight 9000000
layers.0.ae.g.0.weight 512
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 512
layers.0.ae.h.0.bias 4
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.0.weight 90000
layers.1.enc.f.0.bias 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.0.weight 90000
layers.2.enc.f.0.bias 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.0.weight 90000
layers.3.enc.f.0.bias 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.0.weight 90000
layers.4.enc.f.0.bias 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
Total Trainable Params: 9687092
Start Training
train_mask {0, 1, 2, 3, 4}
gc 9
Train Epoch0 Acc 0.918475 (110217/120000), AUC 0.986550509929657
ep0_train_time 10.512958288192749
Test Epoch0 threshold 0.3 Acc 0.9129934210526316, AUC 0.9807731509208679, avg_entr 0.024920200929045677
ep0_t0.3_test_time 0.27110862731933594
Save ckpt to ckpt/ag_news_linearal_l5adp_pad175_t0.3_m5//ag_news_linearal_l5_prefix.pt  ,ep 0
gc 0
Train Epoch1 Acc 0.9503333333333334 (114040/120000), AUC 0.9934332966804504
ep1_train_time 10.000177383422852
Test Epoch1 threshold 0.3 Acc 0.9133223684210526, AUC 0.9804988503456116, avg_entr 0.024178309366106987
ep1_t0.3_test_time 0.2691195011138916
gc 0
Train Epoch2 Acc 0.9514833333333333 (114178/120000), AUC 0.9934645891189575
ep2_train_time 10.106128215789795
Test Epoch2 threshold 0.3 Acc 0.9136513157894737, AUC 0.9804637432098389, avg_entr 0.02538181096315384
ep2_t0.3_test_time 0.2686901092529297
gc 0
Train Epoch3 Acc 0.9522416666666667 (114269/120000), AUC 0.9935717582702637
ep3_train_time 10.063479900360107
Test Epoch3 threshold 0.3 Acc 0.9115131578947369, AUC 0.9803131222724915, avg_entr 0.025051573291420937
ep3_t0.3_test_time 0.265885591506958
gc 0
Train Epoch4 Acc 0.9532833333333334 (114394/120000), AUC 0.9939076900482178
ep4_train_time 10.03520154953003
Test Epoch4 threshold 0.3 Acc 0.9143092105263158, AUC 0.9801415205001831, avg_entr 0.02506772056221962
ep4_t0.3_test_time 0.263214111328125
gc 0
Train Epoch5 Acc 0.9549083333333334 (114589/120000), AUC 0.9945935606956482
ep5_train_time 10.004637241363525
Test Epoch5 threshold 0.3 Acc 0.9139802631578947, AUC 0.9801279306411743, avg_entr 0.02494141459465027
ep5_t0.3_test_time 0.26546692848205566
gc 0
Train Epoch6 Acc 0.9558583333333334 (114703/120000), AUC 0.9947165846824646
ep6_train_time 9.981292963027954
Test Epoch6 threshold 0.3 Acc 0.9125, AUC 0.9799647927284241, avg_entr 0.02434687130153179
ep6_t0.3_test_time 0.26674509048461914
gc 0
Train Epoch7 Acc 0.9560833333333333 (114730/120000), AUC 0.9947953820228577
ep7_train_time 10.005836248397827
Test Epoch7 threshold 0.3 Acc 0.9116776315789473, AUC 0.9797303676605225, avg_entr 0.0241212360560894
ep7_t0.3_test_time 0.26299238204956055
gc 0
Train Epoch8 Acc 0.95635 (114762/120000), AUC 0.9949272871017456
ep8_train_time 10.178988695144653
Test Epoch8 threshold 0.3 Acc 0.9097039473684211, AUC 0.979575514793396, avg_entr 0.02613789215683937
ep8_t0.3_test_time 0.26255226135253906
gc 0
Train Epoch9 Acc 0.9572083333333333 (114865/120000), AUC 0.9952352046966553
ep9_train_time 10.022670030593872
Test Epoch9 threshold 0.3 Acc 0.9115131578947369, AUC 0.9796446561813354, avg_entr 0.02559635043144226
ep9_t0.3_test_time 0.26353883743286133
Best AUC 0.9807731509208679
train_loss (2, 5, 10)
valid_acc (10, 1)
valid_AUC (10, 1)
train_acc (10,)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Start Testing
Load ckpt at ckpt/ag_news_linearal_l5adp_pad175_t0.3_m5//ag_news_linearal_l5_prefix.pt
[[1372   52   77   31]
 [  11 1480    9    7]
 [  32   14 1345  109]
 [  39   18  127 1357]]
Figure(640x480)
tensor([6.4164e-04, 9.6681e-08, 2.6561e-04,  ..., 1.6092e-08, 6.3114e-06,
        2.4258e-05])
[[1371   52   77   32]
 [  11 1479    9    8]
 [  31   14 1347  108]
 [  38   17  123 1363]]
Figure(640x480)
tensor([5.0065e-04, 1.3208e-06, 3.6931e-04,  ..., 1.9656e-08, 3.5641e-06,
        1.1900e-04])
[[1374   52   73   33]
 [  12 1479    8    8]
 [  34   14 1345  107]
 [  38   16  123 1364]]
Figure(640x480)
tensor([3.0211e-04, 1.7208e-07, 1.0164e-04,  ..., 1.5471e-08, 1.4766e-06,
        8.3466e-06])
[[1372   52   75   33]
 [  12 1475    9   11]
 [  34   10 1350  106]
 [  37   14  126 1364]]
Figure(640x480)
tensor([5.1017e-05, 2.6104e-08, 3.6797e-05,  ..., 1.6056e-08, 1.0474e-06,
        6.2880e-07])
[[1372   52   76   32]
 [  12 1475    9   11]
 [  33   11 1350  106]
 [  37   14  129 1361]]
Figure(640x480)
tensor([4.0227e-05, 1.2566e-08, 5.0916e-05,  ..., 1.4452e-08, 1.3205e-06,
        9.2478e-07])
