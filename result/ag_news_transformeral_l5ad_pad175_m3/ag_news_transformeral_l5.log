total count words 102019
vocab size 30000
found 26754 words in glove
Load ckpt from ckpt/ag_news_transformeral_l5_pad175_m2//ag_news_transformeral_l5_prefix.pt
model: TransformerModelML(
  (layers): ModuleList(
    (0): EMBLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Embedding(30000, 300)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=4, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=4, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.weight 9000000
layers.0.ae.g.0.weight 512
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 512
layers.0.ae.h.0.bias 4
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
Total Trainable Params: 13672292
Start Training
train_mask {0, 1, 2}
gc 9
Train Epoch0 Acc 0.45766666666666667 (54920/120000), AUC 0.4833067059516907
ep0_train_time 60.8238525390625
Test Epoch0 threshold 0.1 Acc 0.9147368421052632, AUC 0.979494035243988, avg_entr 0.009708949364721775
ep0_t0.1_test_time 0.3888206481933594
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m3//ag_news_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.2 Acc 0.9147368421052632, AUC 0.9803886413574219, avg_entr 0.015080486424267292
ep0_t0.2_test_time 0.37024641036987305
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m3//ag_news_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.3 Acc 0.9152631578947369, AUC 0.9816223382949829, avg_entr 0.02399289421737194
ep0_t0.3_test_time 0.33989572525024414
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m3//ag_news_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.4 Acc 0.9146052631578947, AUC 0.981509804725647, avg_entr 0.025471093133091927
ep0_t0.4_test_time 0.328357458114624
Test Epoch0 threshold 0.5 Acc 0.9140789473684211, AUC 0.9815051555633545, avg_entr 0.027038022875785828
ep0_t0.5_test_time 0.3126509189605713
Test Epoch0 threshold 0.6 Acc 0.9138157894736842, AUC 0.9814993143081665, avg_entr 0.027498066425323486
ep0_t0.6_test_time 0.3101959228515625
Test Epoch0 threshold 0.7 Acc 0.9138157894736842, AUC 0.9814993143081665, avg_entr 0.027498066425323486
ep0_t0.7_test_time 0.3094785213470459
Test Epoch0 threshold 0.8 Acc 0.9138157894736842, AUC 0.9814993143081665, avg_entr 0.027498066425323486
ep0_t0.8_test_time 0.3086421489715576
Test Epoch0 threshold 0.9 Acc 0.9138157894736842, AUC 0.9814993143081665, avg_entr 0.027498066425323486
ep0_t0.9_test_time 0.30864405632019043
gc 0
Train Epoch1 Acc 0.470325 (56439/120000), AUC 0.5362982749938965
ep1_train_time 60.57114624977112
Test Epoch1 threshold 0.1 Acc 0.9193421052631578, AUC 0.9800617694854736, avg_entr 0.009186618961393833
ep1_t0.1_test_time 0.3822200298309326
Test Epoch1 threshold 0.2 Acc 0.9192105263157895, AUC 0.9812313318252563, avg_entr 0.01409683097153902
ep1_t0.2_test_time 0.36820411682128906
Test Epoch1 threshold 0.3 Acc 0.9198684210526316, AUC 0.9818390607833862, avg_entr 0.023175116628408432
ep1_t0.3_test_time 0.33988380432128906
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m3//ag_news_transformeral_l5_prefix.pt  ,ep 1
Test Epoch1 threshold 0.4 Acc 0.9196052631578947, AUC 0.9818600416183472, avg_entr 0.024911224842071533
ep1_t0.4_test_time 0.325092077255249
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m3//ag_news_transformeral_l5_prefix.pt  ,ep 1
Test Epoch1 threshold 0.5 Acc 0.9192105263157895, AUC 0.9818604588508606, avg_entr 0.025884617120027542
ep1_t0.5_test_time 0.3141317367553711
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m3//ag_news_transformeral_l5_prefix.pt  ,ep 1
Test Epoch1 threshold 0.6 Acc 0.9189473684210526, AUC 0.9818546175956726, avg_entr 0.026259245350956917
ep1_t0.6_test_time 0.3122408390045166
Test Epoch1 threshold 0.7 Acc 0.9189473684210526, AUC 0.9818546175956726, avg_entr 0.026259245350956917
ep1_t0.7_test_time 0.31133532524108887
Test Epoch1 threshold 0.8 Acc 0.9189473684210526, AUC 0.9818546175956726, avg_entr 0.026259245350956917
ep1_t0.8_test_time 0.3107137680053711
Test Epoch1 threshold 0.9 Acc 0.9189473684210526, AUC 0.9818546175956726, avg_entr 0.026259245350956917
ep1_t0.9_test_time 0.31029653549194336
gc 0
Train Epoch2 Acc 0.47265833333333335 (56719/120000), AUC 0.5565245151519775
ep2_train_time 60.73037600517273
Test Epoch2 threshold 0.1 Acc 0.9178947368421052, AUC 0.9800077080726624, avg_entr 0.008893548510968685
ep2_t0.1_test_time 0.38341712951660156
Test Epoch2 threshold 0.2 Acc 0.9176315789473685, AUC 0.9811764359474182, avg_entr 0.013522101566195488
ep2_t0.2_test_time 0.3606147766113281
Test Epoch2 threshold 0.3 Acc 0.9189473684210526, AUC 0.9819029569625854, avg_entr 0.02281235344707966
ep2_t0.3_test_time 0.33673667907714844
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m3//ag_news_transformeral_l5_prefix.pt  ,ep 2
Test Epoch2 threshold 0.4 Acc 0.9189473684210526, AUC 0.9819190502166748, avg_entr 0.02433287352323532
ep2_t0.4_test_time 0.32606077194213867
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m3//ag_news_transformeral_l5_prefix.pt  ,ep 2
Test Epoch2 threshold 0.5 Acc 0.9184210526315789, AUC 0.9818606972694397, avg_entr 0.025372682139277458
ep2_t0.5_test_time 0.31534695625305176
Test Epoch2 threshold 0.6 Acc 0.9184210526315789, AUC 0.9818882346153259, avg_entr 0.025659438222646713
ep2_t0.6_test_time 0.3101191520690918
Test Epoch2 threshold 0.7 Acc 0.9184210526315789, AUC 0.9818882346153259, avg_entr 0.025659438222646713
ep2_t0.7_test_time 0.30965375900268555
Test Epoch2 threshold 0.8 Acc 0.9184210526315789, AUC 0.9818882346153259, avg_entr 0.025659438222646713
ep2_t0.8_test_time 0.3092536926269531
Test Epoch2 threshold 0.9 Acc 0.9184210526315789, AUC 0.9818882346153259, avg_entr 0.025659438222646713
ep2_t0.9_test_time 0.308612585067749
gc 0
Train Epoch3 Acc 0.47323333333333334 (56788/120000), AUC 0.5611774921417236
ep3_train_time 60.62601089477539
Test Epoch3 threshold 0.1 Acc 0.9173684210526316, AUC 0.9800558686256409, avg_entr 0.008822767995297909
ep3_t0.1_test_time 0.37952399253845215
Test Epoch3 threshold 0.2 Acc 0.9172368421052631, AUC 0.9811950325965881, avg_entr 0.01359486859291792
ep3_t0.2_test_time 0.35986804962158203
Test Epoch3 threshold 0.3 Acc 0.9192105263157895, AUC 0.9818233251571655, avg_entr 0.022694559767842293
ep3_t0.3_test_time 0.33687710762023926
Test Epoch3 threshold 0.4 Acc 0.9186842105263158, AUC 0.9819179773330688, avg_entr 0.024533262476325035
ep3_t0.4_test_time 0.32236695289611816
Test Epoch3 threshold 0.5 Acc 0.9180263157894737, AUC 0.9818713665008545, avg_entr 0.02556513249874115
ep3_t0.5_test_time 0.31116247177124023
Test Epoch3 threshold 0.6 Acc 0.9184210526315789, AUC 0.981905996799469, avg_entr 0.02581287920475006
ep3_t0.6_test_time 0.3083503246307373
Test Epoch3 threshold 0.7 Acc 0.9184210526315789, AUC 0.981905996799469, avg_entr 0.02581287920475006
ep3_t0.7_test_time 0.30913352966308594
Test Epoch3 threshold 0.8 Acc 0.9184210526315789, AUC 0.981905996799469, avg_entr 0.02581287920475006
ep3_t0.8_test_time 0.3082406520843506
Test Epoch3 threshold 0.9 Acc 0.9184210526315789, AUC 0.981905996799469, avg_entr 0.02581287920475006
ep3_t0.9_test_time 0.30878472328186035
gc 0
Train Epoch4 Acc 0.473175 (56781/120000), AUC 0.5629000663757324
ep4_train_time 60.7467200756073
Test Epoch4 threshold 0.1 Acc 0.9178947368421052, AUC 0.98013836145401, avg_entr 0.008990174159407616
ep4_t0.1_test_time 0.38045835494995117
Test Epoch4 threshold 0.2 Acc 0.9175, AUC 0.9810696244239807, avg_entr 0.013496996834874153
ep4_t0.2_test_time 0.3680908679962158
Test Epoch4 threshold 0.3 Acc 0.9190789473684211, AUC 0.9818956851959229, avg_entr 0.022723836824297905
ep4_t0.3_test_time 0.3370490074157715
Test Epoch4 threshold 0.4 Acc 0.9192105263157895, AUC 0.9819169044494629, avg_entr 0.02433883398771286
ep4_t0.4_test_time 0.3202393054962158
Test Epoch4 threshold 0.5 Acc 0.9188157894736843, AUC 0.9818816781044006, avg_entr 0.025332314893603325
ep4_t0.5_test_time 0.3117659091949463
Test Epoch4 threshold 0.6 Acc 0.9186842105263158, AUC 0.9818969368934631, avg_entr 0.025720885023474693
ep4_t0.6_test_time 0.3087897300720215
Test Epoch4 threshold 0.7 Acc 0.9186842105263158, AUC 0.9818969368934631, avg_entr 0.025720885023474693
ep4_t0.7_test_time 0.3094053268432617
Test Epoch4 threshold 0.8 Acc 0.9186842105263158, AUC 0.9818969368934631, avg_entr 0.025720885023474693
ep4_t0.8_test_time 0.31046509742736816
Test Epoch4 threshold 0.9 Acc 0.9186842105263158, AUC 0.9818969368934631, avg_entr 0.025720885023474693
ep4_t0.9_test_time 0.310075044631958
gc 0
Train Epoch5 Acc 0.4732166666666667 (56786/120000), AUC 0.5628001689910889
ep5_train_time 60.6653733253479
Test Epoch5 threshold 0.1 Acc 0.9176315789473685, AUC 0.980067253112793, avg_entr 0.008978885598480701
ep5_t0.1_test_time 0.37868809700012207
Test Epoch5 threshold 0.2 Acc 0.9172368421052631, AUC 0.9811131954193115, avg_entr 0.013511797413229942
ep5_t0.2_test_time 0.35921788215637207
Test Epoch5 threshold 0.3 Acc 0.9192105263157895, AUC 0.9818189144134521, avg_entr 0.022837650030851364
ep5_t0.3_test_time 0.33698582649230957
Test Epoch5 threshold 0.4 Acc 0.9192105263157895, AUC 0.9819339513778687, avg_entr 0.024423658847808838
ep5_t0.4_test_time 0.31948184967041016
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m3//ag_news_transformeral_l5_prefix.pt  ,ep 5
Test Epoch5 threshold 0.5 Acc 0.9185526315789474, AUC 0.9818819761276245, avg_entr 0.025431891903281212
ep5_t0.5_test_time 0.3141763210296631
Test Epoch5 threshold 0.6 Acc 0.9184210526315789, AUC 0.9819023013114929, avg_entr 0.025757495313882828
ep5_t0.6_test_time 0.308330774307251
Test Epoch5 threshold 0.7 Acc 0.9184210526315789, AUC 0.9819023013114929, avg_entr 0.025757495313882828
ep5_t0.7_test_time 0.30873751640319824
Test Epoch5 threshold 0.8 Acc 0.9184210526315789, AUC 0.9819023013114929, avg_entr 0.025757495313882828
ep5_t0.8_test_time 0.31377530097961426
Test Epoch5 threshold 0.9 Acc 0.9184210526315789, AUC 0.9819023013114929, avg_entr 0.025757495313882828
ep5_t0.9_test_time 0.30840468406677246
gc 0
Train Epoch6 Acc 0.47328333333333333 (56794/120000), AUC 0.5627782940864563
ep6_train_time 60.60411524772644
Test Epoch6 threshold 0.1 Acc 0.9175, AUC 0.9800668358802795, avg_entr 0.008970511145889759
ep6_t0.1_test_time 0.3776700496673584
Test Epoch6 threshold 0.2 Acc 0.9171052631578948, AUC 0.9811128377914429, avg_entr 0.013506750576198101
ep6_t0.2_test_time 0.3603367805480957
Test Epoch6 threshold 0.3 Acc 0.9190789473684211, AUC 0.981818437576294, avg_entr 0.022831469774246216
ep6_t0.3_test_time 0.3382413387298584
Test Epoch6 threshold 0.4 Acc 0.9192105263157895, AUC 0.98193359375, avg_entr 0.024417823180556297
ep6_t0.4_test_time 0.31934237480163574
Test Epoch6 threshold 0.5 Acc 0.9185526315789474, AUC 0.9818814992904663, avg_entr 0.02542666532099247
ep6_t0.5_test_time 0.3111717700958252
Test Epoch6 threshold 0.6 Acc 0.9184210526315789, AUC 0.9819018840789795, avg_entr 0.02575240656733513
ep6_t0.6_test_time 0.30945634841918945
Test Epoch6 threshold 0.7 Acc 0.9184210526315789, AUC 0.9819018840789795, avg_entr 0.02575240656733513
ep6_t0.7_test_time 0.30866551399230957
Test Epoch6 threshold 0.8 Acc 0.9184210526315789, AUC 0.9819018840789795, avg_entr 0.02575240656733513
ep6_t0.8_test_time 0.3078935146331787
Test Epoch6 threshold 0.9 Acc 0.9184210526315789, AUC 0.9819018840789795, avg_entr 0.02575240656733513
ep6_t0.9_test_time 0.30995607376098633
gc 0
Train Epoch7 Acc 0.47325 (56790/120000), AUC 0.5630980730056763
ep7_train_time 60.590503215789795
Test Epoch7 threshold 0.1 Acc 0.9175, AUC 0.9800670146942139, avg_entr 0.008970307186245918
ep7_t0.1_test_time 0.3791351318359375
Test Epoch7 threshold 0.2 Acc 0.9171052631578948, AUC 0.9811128377914429, avg_entr 0.013506594114005566
ep7_t0.2_test_time 0.35846829414367676
Test Epoch7 threshold 0.3 Acc 0.9190789473684211, AUC 0.9818183779716492, avg_entr 0.022831378504633904
ep7_t0.3_test_time 0.3379695415496826
Test Epoch7 threshold 0.4 Acc 0.9192105263157895, AUC 0.9819337129592896, avg_entr 0.02441754937171936
ep7_t0.4_test_time 0.32028889656066895
Test Epoch7 threshold 0.5 Acc 0.9185526315789474, AUC 0.9818814992904663, avg_entr 0.02542644366621971
ep7_t0.5_test_time 0.31037354469299316
Test Epoch7 threshold 0.6 Acc 0.9184210526315789, AUC 0.9819018840789795, avg_entr 0.025752197951078415
ep7_t0.6_test_time 0.3086678981781006
Test Epoch7 threshold 0.7 Acc 0.9184210526315789, AUC 0.9819018840789795, avg_entr 0.025752197951078415
ep7_t0.7_test_time 0.3114466667175293
Test Epoch7 threshold 0.8 Acc 0.9184210526315789, AUC 0.9819018840789795, avg_entr 0.025752197951078415
ep7_t0.8_test_time 0.3100011348724365
Test Epoch7 threshold 0.9 Acc 0.9184210526315789, AUC 0.9819018840789795, avg_entr 0.025752197951078415
ep7_t0.9_test_time 0.30866193771362305
gc 0
Train Epoch8 Acc 0.47335 (56802/120000), AUC 0.5627590417861938
ep8_train_time 60.66555953025818
Test Epoch8 threshold 0.1 Acc 0.9175, AUC 0.98006671667099, avg_entr 0.00897009577602148
ep8_t0.1_test_time 0.37826013565063477
Test Epoch8 threshold 0.2 Acc 0.9171052631578948, AUC 0.9811123609542847, avg_entr 0.013506429269909859
ep8_t0.2_test_time 0.35844874382019043
Test Epoch8 threshold 0.3 Acc 0.9190789473684211, AUC 0.9818182587623596, avg_entr 0.022831054404377937
ep8_t0.3_test_time 0.3370184898376465
Test Epoch8 threshold 0.4 Acc 0.9192105263157895, AUC 0.98193359375, avg_entr 0.024417394772171974
ep8_t0.4_test_time 0.3217198848724365
Test Epoch8 threshold 0.5 Acc 0.9185526315789474, AUC 0.9818814992904663, avg_entr 0.02542649582028389
ep8_t0.5_test_time 0.3111128807067871
Test Epoch8 threshold 0.6 Acc 0.9184210526315789, AUC 0.9819018840789795, avg_entr 0.02575228549540043
ep8_t0.6_test_time 0.30907392501831055
Test Epoch8 threshold 0.7 Acc 0.9184210526315789, AUC 0.9819018840789795, avg_entr 0.02575228549540043
ep8_t0.7_test_time 0.3105630874633789
Test Epoch8 threshold 0.8 Acc 0.9184210526315789, AUC 0.9819018840789795, avg_entr 0.02575228549540043
ep8_t0.8_test_time 0.30871033668518066
Test Epoch8 threshold 0.9 Acc 0.9184210526315789, AUC 0.9819018840789795, avg_entr 0.02575228549540043
ep8_t0.9_test_time 0.3105599880218506
gc 0
Train Epoch9 Acc 0.473325 (56799/120000), AUC 0.5625894069671631
ep9_train_time 60.63819932937622
Test Epoch9 threshold 0.1 Acc 0.9175, AUC 0.9800664186477661, avg_entr 0.008970019407570362
ep9_t0.1_test_time 0.3787539005279541
Test Epoch9 threshold 0.2 Acc 0.9171052631578948, AUC 0.9811123609542847, avg_entr 0.013506299816071987
ep9_t0.2_test_time 0.3607053756713867
Test Epoch9 threshold 0.3 Acc 0.9190789473684211, AUC 0.9818182587623596, avg_entr 0.022830868139863014
ep9_t0.3_test_time 0.3377492427825928
Test Epoch9 threshold 0.4 Acc 0.9192105263157895, AUC 0.9819337129592896, avg_entr 0.024417152628302574
ep9_t0.4_test_time 0.31917476654052734
Test Epoch9 threshold 0.5 Acc 0.9185526315789474, AUC 0.9818814992904663, avg_entr 0.025426357984542847
ep9_t0.5_test_time 0.3104729652404785
Test Epoch9 threshold 0.6 Acc 0.9184210526315789, AUC 0.9819018244743347, avg_entr 0.025752171874046326
ep9_t0.6_test_time 0.30838894844055176
Test Epoch9 threshold 0.7 Acc 0.9184210526315789, AUC 0.9819018244743347, avg_entr 0.025752171874046326
ep9_t0.7_test_time 0.3086695671081543
Test Epoch9 threshold 0.8 Acc 0.9184210526315789, AUC 0.9819018244743347, avg_entr 0.025752171874046326
ep9_t0.8_test_time 0.30957865715026855
Test Epoch9 threshold 0.9 Acc 0.9184210526315789, AUC 0.9819018244743347, avg_entr 0.025752171874046326
ep9_t0.9_test_time 0.3101799488067627
Best AUC 0.9819339513778687
train_loss (2, 5, 10)
valid_acc (10, 9)
valid_AUC (10, 9)
train_acc (10,)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Start Testing
Load ckpt at ckpt/ag_news_transformeral_l5_pad175_m3//ag_news_transformeral_l5_prefix.pt
[[1704   62   85   49]
 [  11 1873    6   10]
 [  46   21 1680  153]
 [  43   17  117 1723]]
Figure(640x480)
tensor([1.2393e-03, 1.0019e-07, 1.4062e-03,  ..., 1.1463e-02, 5.4851e-07,
        4.7048e-02])
[[1721   53   65   61]
 [  16 1865    9   10]
 [  54   15 1682  149]
 [  53   14  133 1700]]
Figure(640x480)
tensor([8.5235e-07, 9.4518e-08, 8.3445e-08,  ..., 4.8653e-07, 9.0183e-08,
        9.3609e-08])
[[1727   50   64   59]
 [  19 1861    7   13]
 [  53   15 1684  148]
 [  52   14  135 1699]]
Figure(640x480)
tensor([5.1090e-07, 8.6671e-08, 1.2015e-07,  ..., 5.8223e-07, 9.0403e-08,
        1.0164e-07])
[[   0    2  843 1055]
 [   0   81 1518  301]
 [   0    0 1892    8]
 [   0    1 1892    7]]
Figure(640x480)
tensor([0.6130, 0.7608, 0.7016,  ..., 0.1477, 0.7660, 0.6593])
[[   0    0 1843   57]
 [   0    0 1885   15]
 [   0    0 1767  133]
 [   0    0  219 1681]]
Figure(640x480)
tensor([0.3862, 1.0619, 1.0371,  ..., 0.5673, 0.3560, 0.3776])
