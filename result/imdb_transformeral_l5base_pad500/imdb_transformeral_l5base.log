total count words 222751
vocab size 30000
train size 40000, valid size 5000, test size 5000
found 27937 words in glove
model: TransformerModelML(
  (layers): ModuleList(
    (0): EMBLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Embedding(30000, 300)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=2, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=2, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.weight 9000000
layers.0.ae.g.0.weight 256
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 256
layers.0.ae.h.0.bias 2
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
Total Trainable Params: 13671778
init_time 21.73887062072754
Start Training
gc 0
Train Epoch0 Acc 0.5619 (22476/40000), AUC 0.5822377800941467
ep0_train_time 92.40172457695007
Test Epoch0 layer4 Acc 0.8294, AUC 0.9126152992248535, avg_entr 0.6387845277786255
ep0_l4_test_time 3.0637423992156982
Save ckpt to ckpt/imdb_transformeral_l5base_pad500//imdb_transformeral_l5.pt  ,ep 0
gc 0
Train Epoch1 Acc 0.8611 (34444/40000), AUC 0.9332021474838257
ep1_train_time 91.95262718200684
Test Epoch1 layer4 Acc 0.8916, AUC 0.9549874067306519, avg_entr 0.12440125644207001
ep1_l4_test_time 3.068875551223755
Save ckpt to ckpt/imdb_transformeral_l5base_pad500//imdb_transformeral_l5.pt  ,ep 1
gc 0
Train Epoch2 Acc 0.91695 (36678/40000), AUC 0.970313549041748
ep2_train_time 91.97035312652588
Test Epoch2 layer4 Acc 0.8876, AUC 0.9591717720031738, avg_entr 0.05327226594090462
ep2_l4_test_time 3.062291145324707
Save ckpt to ckpt/imdb_transformeral_l5base_pad500//imdb_transformeral_l5.pt  ,ep 2
gc 0
Train Epoch3 Acc 0.9398 (37592/40000), AUC 0.9807240962982178
ep3_train_time 91.92912650108337
Test Epoch3 layer4 Acc 0.8922, AUC 0.9561941027641296, avg_entr 0.04211176931858063
ep3_l4_test_time 3.058009624481201
gc 0
Train Epoch4 Acc 0.95235 (38094/40000), AUC 0.9853403568267822
ep4_train_time 91.9303457736969
Test Epoch4 layer4 Acc 0.891, AUC 0.9542065262794495, avg_entr 0.03348782658576965
ep4_l4_test_time 3.0580315589904785
gc 0
Train Epoch5 Acc 0.9588 (38352/40000), AUC 0.9881420731544495
ep5_train_time 92.13976263999939
Test Epoch5 layer4 Acc 0.8848, AUC 0.9516737461090088, avg_entr 0.030684927478432655
ep5_l4_test_time 3.0589094161987305
gc 0
Train Epoch6 Acc 0.963775 (38551/40000), AUC 0.9896994829177856
ep6_train_time 92.08657240867615
Test Epoch6 layer4 Acc 0.8774, AUC 0.9483439326286316, avg_entr 0.028286248445510864
ep6_l4_test_time 3.0581095218658447
gc 0
Train Epoch7 Acc 0.9669 (38676/40000), AUC 0.9910426735877991
ep7_train_time 92.0340371131897
Test Epoch7 layer4 Acc 0.8784, AUC 0.9454413652420044, avg_entr 0.023097041994333267
ep7_l4_test_time 3.0640146732330322
gc 0
Train Epoch8 Acc 0.969975 (38799/40000), AUC 0.9922712445259094
ep8_train_time 92.0948874950409
Test Epoch8 layer4 Acc 0.8804, AUC 0.9444088935852051, avg_entr 0.019826065748929977
ep8_l4_test_time 3.059248924255371
gc 0
Train Epoch9 Acc 0.973425 (38937/40000), AUC 0.9929607510566711
ep9_train_time 91.92835855484009
Test Epoch9 layer4 Acc 0.875, AUC 0.9410197734832764, avg_entr 0.017986450344324112
ep9_l4_test_time 3.0621323585510254
gc 0
Train Epoch10 Acc 0.974975 (38999/40000), AUC 0.993946373462677
ep10_train_time 91.96064257621765
Test Epoch10 layer4 Acc 0.874, AUC 0.9389739036560059, avg_entr 0.020478572696447372
ep10_l4_test_time 3.0589983463287354
gc 0
Train Epoch11 Acc 0.9768 (39072/40000), AUC 0.9941596984863281
ep11_train_time 91.9542920589447
Test Epoch11 layer4 Acc 0.876, AUC 0.9379757642745972, avg_entr 0.01778694987297058
ep11_l4_test_time 3.067213535308838
gc 0
Train Epoch12 Acc 0.9793 (39172/40000), AUC 0.9950984716415405
ep12_train_time 91.97023892402649
Test Epoch12 layer4 Acc 0.8734, AUC 0.9367682933807373, avg_entr 0.017799818888306618
ep12_l4_test_time 3.057746171951294
gc 0
Train Epoch13 Acc 0.98065 (39226/40000), AUC 0.9954659342765808
ep13_train_time 91.97350287437439
Test Epoch13 layer4 Acc 0.8714, AUC 0.9328328371047974, avg_entr 0.01280754990875721
ep13_l4_test_time 3.065284252166748
gc 0
Train Epoch14 Acc 0.98175 (39270/40000), AUC 0.995612382888794
ep14_train_time 92.0981810092926
Test Epoch14 layer4 Acc 0.8652, AUC 0.9230723977088928, avg_entr 0.014199933968484402
ep14_l4_test_time 3.068052291870117
gc 0
Train Epoch15 Acc 0.9826 (39304/40000), AUC 0.9959127306938171
ep15_train_time 91.98456835746765
Test Epoch15 layer4 Acc 0.8614, AUC 0.9135839939117432, avg_entr 0.010362906381487846
ep15_l4_test_time 3.059854030609131
gc 0
Train Epoch16 Acc 0.984175 (39367/40000), AUC 0.9968229532241821
ep16_train_time 91.9971616268158
Test Epoch16 layer4 Acc 0.8674, AUC 0.9195870757102966, avg_entr 0.01142570935189724
ep16_l4_test_time 3.0584986209869385
gc 0
Train Epoch17 Acc 0.9849 (39396/40000), AUC 0.9965252876281738
ep17_train_time 92.06404066085815
Test Epoch17 layer4 Acc 0.8644, AUC 0.9149820804595947, avg_entr 0.009564285166561604
ep17_l4_test_time 3.0592660903930664
gc 0
Train Epoch18 Acc 0.98615 (39446/40000), AUC 0.997194766998291
ep18_train_time 91.96008372306824
Test Epoch18 layer4 Acc 0.8546, AUC 0.8940536975860596, avg_entr 0.008421182632446289
ep18_l4_test_time 3.068336248397827
gc 0
Train Epoch19 Acc 0.987175 (39487/40000), AUC 0.9972324371337891
ep19_train_time 91.9930944442749
Test Epoch19 layer4 Acc 0.8606, AUC 0.9010117650032043, avg_entr 0.008954289369285107
ep19_l4_test_time 3.059109926223755
gc 0
Train Epoch20 Acc 0.98755 (39502/40000), AUC 0.9970551133155823
ep20_train_time 91.92737650871277
Test Epoch20 layer4 Acc 0.857, AUC 0.9114805459976196, avg_entr 0.014486209489405155
ep20_l4_test_time 3.0564022064208984
gc 0
Train Epoch21 Acc 0.988175 (39527/40000), AUC 0.9977294206619263
ep21_train_time 91.9852945804596
Test Epoch21 layer4 Acc 0.8608, AUC 0.9070188999176025, avg_entr 0.011799236759543419
ep21_l4_test_time 3.058879852294922
gc 0
Train Epoch22 Acc 0.989075 (39563/40000), AUC 0.998103141784668
ep22_train_time 92.10685300827026
Test Epoch22 layer4 Acc 0.8508, AUC 0.8887428045272827, avg_entr 0.007412339560687542
ep22_l4_test_time 3.063847064971924
gc 0
Train Epoch23 Acc 0.98965 (39586/40000), AUC 0.9981094598770142
ep23_train_time 91.96319627761841
Test Epoch23 layer4 Acc 0.8518, AUC 0.8731415271759033, avg_entr 0.008584574796259403
ep23_l4_test_time 3.067656993865967
gc 0
Train Epoch24 Acc 0.990225 (39609/40000), AUC 0.9980577230453491
ep24_train_time 91.99063467979431
Test Epoch24 layer4 Acc 0.8506, AUC 0.8780376315116882, avg_entr 0.007282118778675795
ep24_l4_test_time 3.063850164413452
gc 0
Train Epoch25 Acc 0.990925 (39637/40000), AUC 0.998091459274292
ep25_train_time 92.02695083618164
Test Epoch25 layer4 Acc 0.8462, AUC 0.8818631172180176, avg_entr 0.0055638584308326244
ep25_l4_test_time 3.0663368701934814
gc 0
Train Epoch26 Acc 0.991275 (39651/40000), AUC 0.998559832572937
ep26_train_time 91.93124413490295
Test Epoch26 layer4 Acc 0.8498, AUC 0.8692368268966675, avg_entr 0.007080473937094212
ep26_l4_test_time 3.05611252784729
gc 0
Train Epoch27 Acc 0.991725 (39669/40000), AUC 0.9985592365264893
ep27_train_time 91.91749334335327
Test Epoch27 layer4 Acc 0.8492, AUC 0.8682497143745422, avg_entr 0.005808360408991575
ep27_l4_test_time 3.0548808574676514
gc 0
Train Epoch28 Acc 0.99175 (39670/40000), AUC 0.9988901019096375
ep28_train_time 91.93417000770569
Test Epoch28 layer4 Acc 0.8486, AUC 0.8684818744659424, avg_entr 0.007056607864797115
ep28_l4_test_time 3.063674211502075
gc 0
Train Epoch29 Acc 0.99295 (39718/40000), AUC 0.9986732006072998
ep29_train_time 91.92378377914429
Test Epoch29 layer4 Acc 0.8418, AUC 0.8540846705436707, avg_entr 0.006562210153788328
ep29_l4_test_time 3.0556721687316895
gc 0
Train Epoch30 Acc 0.9928 (39712/40000), AUC 0.9990023374557495
ep30_train_time 91.92475295066833
Test Epoch30 layer4 Acc 0.8372, AUC 0.8669309616088867, avg_entr 0.0067170485854148865
ep30_l4_test_time 3.0568792819976807
gc 0
Train Epoch31 Acc 0.993025 (39721/40000), AUC 0.9986470937728882
ep31_train_time 91.95212697982788
Test Epoch31 layer4 Acc 0.847, AUC 0.8718827962875366, avg_entr 0.006655138451606035
ep31_l4_test_time 3.0558457374572754
gc 0
Train Epoch32 Acc 0.993475 (39739/40000), AUC 0.9989497065544128
ep32_train_time 92.06541967391968
Test Epoch32 layer4 Acc 0.8442, AUC 0.8457114696502686, avg_entr 0.0042817601934075356
ep32_l4_test_time 3.054560899734497
gc 0
Train Epoch33 Acc 0.99385 (39754/40000), AUC 0.9990584850311279
ep33_train_time 91.91716313362122
Test Epoch33 layer4 Acc 0.8358, AUC 0.8486456871032715, avg_entr 0.005979790352284908
ep33_l4_test_time 3.0571086406707764
gc 0
Train Epoch34 Acc 0.994075 (39763/40000), AUC 0.9991568326950073
ep34_train_time 91.90997529029846
Test Epoch34 layer4 Acc 0.8384, AUC 0.8655905723571777, avg_entr 0.004996044561266899
ep34_l4_test_time 3.058711051940918
gc 0
Train Epoch35 Acc 0.9944 (39776/40000), AUC 0.9990707635879517
ep35_train_time 91.92788624763489
Test Epoch35 layer4 Acc 0.8362, AUC 0.8385737538337708, avg_entr 0.003629571059718728
ep35_l4_test_time 3.0604703426361084
gc 0
Train Epoch36 Acc 0.99485 (39794/40000), AUC 0.9992098808288574
ep36_train_time 91.96672129631042
Test Epoch36 layer4 Acc 0.8324, AUC 0.8305193185806274, avg_entr 0.0036496277898550034
ep36_l4_test_time 3.055856704711914
gc 0
Train Epoch37 Acc 0.995025 (39801/40000), AUC 0.999126672744751
ep37_train_time 91.92244243621826
Test Epoch37 layer4 Acc 0.8334, AUC 0.8401373624801636, avg_entr 0.004066793713718653
ep37_l4_test_time 3.056058406829834
gc 0
Train Epoch38 Acc 0.995325 (39813/40000), AUC 0.9993030428886414
ep38_train_time 91.93048119544983
Test Epoch38 layer4 Acc 0.8372, AUC 0.8322283029556274, avg_entr 0.0035371414851397276
ep38_l4_test_time 3.055040121078491
gc 0
Train Epoch39 Acc 0.995725 (39829/40000), AUC 0.9992253184318542
ep39_train_time 92.0632176399231
Test Epoch39 layer4 Acc 0.8358, AUC 0.8219742178916931, avg_entr 0.003803733503445983
ep39_l4_test_time 3.0561885833740234
gc 0
Train Epoch40 Acc 0.99605 (39842/40000), AUC 0.9995290637016296
ep40_train_time 91.96438980102539
Test Epoch40 layer4 Acc 0.8308, AUC 0.8375554084777832, avg_entr 0.003848262829706073
ep40_l4_test_time 3.06028413772583
gc 0
Train Epoch41 Acc 0.9963 (39852/40000), AUC 0.9995911121368408
ep41_train_time 91.93531274795532
Test Epoch41 layer4 Acc 0.8334, AUC 0.8344574570655823, avg_entr 0.004535188432782888
ep41_l4_test_time 3.05924391746521
gc 0
Train Epoch42 Acc 0.99655 (39862/40000), AUC 0.9995946884155273
ep42_train_time 91.9869236946106
Test Epoch42 layer4 Acc 0.828, AUC 0.830004096031189, avg_entr 0.0038163887802511454
ep42_l4_test_time 3.0639379024505615
gc 0
Train Epoch43 Acc 0.996775 (39871/40000), AUC 0.9995921850204468
ep43_train_time 92.0535979270935
Test Epoch43 layer4 Acc 0.8302, AUC 0.8276442289352417, avg_entr 0.0019608151633292437
ep43_l4_test_time 3.066053867340088
gc 0
Train Epoch44 Acc 0.996575 (39863/40000), AUC 0.9995418190956116
ep44_train_time 92.43139815330505
Test Epoch44 layer4 Acc 0.8304, AUC 0.8286644220352173, avg_entr 0.002570075448602438
ep44_l4_test_time 3.0594594478607178
gc 0
Train Epoch45 Acc 0.996975 (39879/40000), AUC 0.9995213150978088
ep45_train_time 91.92869544029236
Test Epoch45 layer4 Acc 0.8268, AUC 0.8076685667037964, avg_entr 0.0035714523401111364
ep45_l4_test_time 3.0564329624176025
gc 0
Train Epoch46 Acc 0.99715 (39886/40000), AUC 0.9996024966239929
ep46_train_time 92.05995845794678
Test Epoch46 layer4 Acc 0.8278, AUC 0.8239463567733765, avg_entr 0.004073088988661766
ep46_l4_test_time 3.058073043823242
gc 0
Train Epoch47 Acc 0.997675 (39907/40000), AUC 0.9995266199111938
ep47_train_time 91.96076679229736
Test Epoch47 layer4 Acc 0.8234, AUC 0.8169323205947876, avg_entr 0.0029514688067138195
ep47_l4_test_time 3.057133913040161
gc 0
Train Epoch48 Acc 0.997725 (39909/40000), AUC 0.9995228052139282
ep48_train_time 91.99638223648071
Test Epoch48 layer4 Acc 0.829, AUC 0.8172779679298401, avg_entr 0.0025071112904697657
ep48_l4_test_time 3.0621097087860107
gc 0
Train Epoch49 Acc 0.99785 (39914/40000), AUC 0.9996451139450073
ep49_train_time 92.12518739700317
Test Epoch49 layer4 Acc 0.8186, AUC 0.8076921701431274, avg_entr 0.0032137520611286163
ep49_l4_test_time 3.0585615634918213
Best AUC 0.9591717720031738
train_loss (2, 5, 50)
valid_acc (1, 50)
valid_AUC (1, 50)
train_acc (50,)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
total_train+valid_time 4755.735548734665
Start Testing
Load ckpt at ckpt/imdb_transformeral_l5base_pad500//imdb_transformeral_l5.pt
Test layer4 Acc 0.8814, AUC 0.9576228260993958, avg_entr 0.060726843774318695
l4_test_time 3.066020965576172
