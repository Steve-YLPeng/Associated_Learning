total count words 102019
vocab size 30000
train size 120000, valid size 3800, test size 3800
found 26754 words in glove
model: TransformerALsideText(
  (layers): ModuleList(
    (0): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=4, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=4, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
  (emb_layers): ModuleList(
    (0): Embedding(30000, 300)
    (1): Embedding(30000, 300)
    (2): Embedding(30000, 300)
    (3): Embedding(30000, 300)
    (4): Embedding(30000, 300)
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.multi_headed_attention.linears.0.weight 90000
layers.0.enc.f.multi_headed_attention.linears.0.bias 300
layers.0.enc.f.multi_headed_attention.linears.1.weight 90000
layers.0.enc.f.multi_headed_attention.linears.1.bias 300
layers.0.enc.f.multi_headed_attention.linears.2.weight 90000
layers.0.enc.f.multi_headed_attention.linears.2.bias 300
layers.0.enc.f.multi_headed_attention.linears.3.weight 90000
layers.0.enc.f.multi_headed_attention.linears.3.bias 300
layers.0.enc.f.feed_forward.w_1.weight 90000
layers.0.enc.f.feed_forward.w_1.bias 300
layers.0.enc.f.feed_forward.w_2.weight 90000
layers.0.enc.f.feed_forward.w_2.bias 300
layers.0.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.0.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.0.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.0.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.0.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.0.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.0.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.0.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.0.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.0.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.0.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.0.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.0.enc.f.encoder.norm.a 300
layers.0.enc.f.encoder.norm.b 300
layers.0.ae.g.0.weight 512
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 512
layers.0.ae.h.0.bias 4
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
emb_layers.0.weight 9000000
emb_layers.1.weight 9000000
emb_layers.2.weight 9000000
emb_layers.3.weight 9000000
emb_layers.4.weight 9000000
Total Trainable Params: 50758892
init_time 15.167798837000001
Start Training
gc 0
Train Epoch0 Acc 0.26334166666666664 (31601/120000), AUC 0.5094407796859741
ep0_train_time 20.855201652999998
Test Epoch0 layer0 Acc 0.6605263157894737, AUC 0.873682975769043, avg_entr 0.9135409593582153, f1 0.6605263352394104
ep0_l0_test_time 0.06907492700000262
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 0
Test Epoch0 layer1 Acc 0.7221052631578947, AUC 0.9028096199035645, avg_entr 0.8578087687492371, f1 0.7221053242683411
ep0_l1_test_time 0.08976245200000221
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 0
Test Epoch0 layer2 Acc 0.6884210526315789, AUC 0.8896031975746155, avg_entr 0.9605684876441956, f1 0.6884210705757141
ep0_l2_test_time 0.11075776099999501
Test Epoch0 layer3 Acc 0.4978947368421053, AUC 0.8405697345733643, avg_entr 1.098023772239685, f1 0.4978947341442108
ep0_l3_test_time 0.13406922299999735
Test Epoch0 layer4 Acc 0.585, AUC 0.7850677371025085, avg_entr 1.2154889106750488, f1 0.5849999785423279
ep0_l4_test_time 0.1696734660000061
gc 0
Train Epoch1 Acc 0.6429 (77148/120000), AUC 0.8472251296043396
ep1_train_time 20.443164007999997
Test Epoch1 layer0 Acc 0.7547368421052632, AUC 0.9250234961509705, avg_entr 0.49502503871917725, f1 0.7547368407249451
ep1_l0_test_time 0.06743579399999788
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 1
Test Epoch1 layer1 Acc 0.8039473684210526, AUC 0.9433212876319885, avg_entr 0.4186975061893463, f1 0.8039474487304688
ep1_l1_test_time 0.08939779600000008
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 1
Test Epoch1 layer2 Acc 0.8015789473684211, AUC 0.945540726184845, avg_entr 0.39251551032066345, f1 0.801578938961029
ep1_l2_test_time 0.11079568499999937
Test Epoch1 layer3 Acc 0.7921052631578948, AUC 0.9456026554107666, avg_entr 0.39483025670051575, f1 0.7921052575111389
ep1_l3_test_time 0.1333318569999946
Test Epoch1 layer4 Acc 0.7892105263157895, AUC 0.9454244375228882, avg_entr 0.4088609218597412, f1 0.7892104983329773
ep1_l4_test_time 0.16920830800000175
gc 0
Train Epoch2 Acc 0.814575 (97749/120000), AUC 0.943337619304657
ep2_train_time 20.313307566000006
Test Epoch2 layer0 Acc 0.7905263157894736, AUC 0.9404525756835938, avg_entr 0.3324279189109802, f1 0.7905263304710388
ep2_l0_test_time 0.06781805199999269
Test Epoch2 layer1 Acc 0.8155263157894737, AUC 0.9549033045768738, avg_entr 0.2648158073425293, f1 0.8155263066291809
ep2_l1_test_time 0.08212283400000331
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 2
Test Epoch2 layer2 Acc 0.8210526315789474, AUC 0.9572064280509949, avg_entr 0.23928850889205933, f1 0.821052610874176
ep2_l2_test_time 0.11301364399999159
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 2
Test Epoch2 layer3 Acc 0.8218421052631579, AUC 0.9584645628929138, avg_entr 0.2274332493543625, f1 0.8218421339988708
ep2_l3_test_time 0.1406895190000057
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 2
Test Epoch2 layer4 Acc 0.82, AUC 0.9583773612976074, avg_entr 0.21844162046909332, f1 0.8199999928474426
ep2_l4_test_time 0.17819031899999516
gc 0
Train Epoch3 Acc 0.851925 (102231/120000), AUC 0.9578816890716553
ep3_train_time 20.552371515000004
Test Epoch3 layer0 Acc 0.8065789473684211, AUC 0.9480559825897217, avg_entr 0.2608190178871155, f1 0.8065789341926575
ep3_l0_test_time 0.06857149700000775
Test Epoch3 layer1 Acc 0.8347368421052631, AUC 0.9616773128509521, avg_entr 0.1857708990573883, f1 0.8347368240356445
ep3_l1_test_time 0.08234685399999364
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 3
Test Epoch3 layer2 Acc 0.8413157894736842, AUC 0.9635987281799316, avg_entr 0.158210888504982, f1 0.8413158059120178
ep3_l2_test_time 0.11038060599999255
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 3
Test Epoch3 layer3 Acc 0.8444736842105263, AUC 0.9637367129325867, avg_entr 0.1426706612110138, f1 0.844473659992218
ep3_l3_test_time 0.1398273789999962
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 3
Test Epoch3 layer4 Acc 0.8431578947368421, AUC 0.9633057117462158, avg_entr 0.13613052666187286, f1 0.8431578874588013
ep3_l4_test_time 0.17397490400000493
gc 0
Train Epoch4 Acc 0.8737 (104844/120000), AUC 0.9651566743850708
ep4_train_time 20.456386021
Test Epoch4 layer0 Acc 0.8157894736842105, AUC 0.9513225555419922, avg_entr 0.21257925033569336, f1 0.8157894611358643
ep4_l0_test_time 0.06805436800000564
Test Epoch4 layer1 Acc 0.8421052631578947, AUC 0.9630922675132751, avg_entr 0.130695179104805, f1 0.8421053290367126
ep4_l1_test_time 0.08217320499998948
Test Epoch4 layer2 Acc 0.8455263157894737, AUC 0.9654152393341064, avg_entr 0.11302584409713745, f1 0.8455263376235962
ep4_l2_test_time 0.10454726399999004
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 4
Test Epoch4 layer3 Acc 0.8463157894736842, AUC 0.9651786684989929, avg_entr 0.10443217307329178, f1 0.8463158011436462
ep4_l3_test_time 0.13925743500000465
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 4
Test Epoch4 layer4 Acc 0.8444736842105263, AUC 0.9646511077880859, avg_entr 0.10076073557138443, f1 0.844473659992218
ep4_l4_test_time 0.17536288200000172
gc 0
Train Epoch5 Acc 0.8875833333333333 (106510/120000), AUC 0.97023606300354
ep5_train_time 20.369619474999993
Test Epoch5 layer0 Acc 0.8202631578947368, AUC 0.9524635076522827, avg_entr 0.18408742547035217, f1 0.820263147354126
ep5_l0_test_time 0.0672195669999951
Test Epoch5 layer1 Acc 0.8473684210526315, AUC 0.962007999420166, avg_entr 0.09862782061100006, f1 0.8473684191703796
ep5_l1_test_time 0.08194112399999653
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 5
Test Epoch5 layer2 Acc 0.8536842105263158, AUC 0.9640398025512695, avg_entr 0.08309105783700943, f1 0.8536841869354248
ep5_l2_test_time 0.10994669600000861
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 5
Test Epoch5 layer3 Acc 0.853421052631579, AUC 0.9635733366012573, avg_entr 0.07474426180124283, f1 0.8534210324287415
ep5_l3_test_time 0.13970006200000284
Test Epoch5 layer4 Acc 0.8539473684210527, AUC 0.9613901376724243, avg_entr 0.07023317366838455, f1 0.8539473414421082
ep5_l4_test_time 0.16897574600000098
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 5
gc 0
Train Epoch6 Acc 0.9029833333333334 (108358/120000), AUC 0.9762310981750488
ep6_train_time 20.541780348000003
Test Epoch6 layer0 Acc 0.8234210526315789, AUC 0.9537780284881592, avg_entr 0.1693456768989563, f1 0.823421061038971
ep6_l0_test_time 0.06732579299998065
Test Epoch6 layer1 Acc 0.8523684210526316, AUC 0.9645289182662964, avg_entr 0.09053993970155716, f1 0.8523684144020081
ep6_l1_test_time 0.08155182400000172
Test Epoch6 layer2 Acc 0.8586842105263158, AUC 0.9669380187988281, avg_entr 0.07493468374013901, f1 0.8586841821670532
ep6_l2_test_time 0.1044729049999944
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 6
Test Epoch6 layer3 Acc 0.8560526315789474, AUC 0.9665150046348572, avg_entr 0.06650989502668381, f1 0.8560526371002197
ep6_l3_test_time 0.1405005889999984
Test Epoch6 layer4 Acc 0.8563157894736843, AUC 0.9636412262916565, avg_entr 0.06143708899617195, f1 0.8563157916069031
ep6_l4_test_time 0.16904520700001058
gc 0
Train Epoch7 Acc 0.9127916666666667 (109535/120000), AUC 0.9794800877571106
ep7_train_time 20.52530017699999
Test Epoch7 layer0 Acc 0.8213157894736842, AUC 0.9526366591453552, avg_entr 0.15465077757835388, f1 0.8213157653808594
ep7_l0_test_time 0.06777736099999743
Test Epoch7 layer1 Acc 0.8494736842105263, AUC 0.9623997211456299, avg_entr 0.07688547670841217, f1 0.8494736552238464
ep7_l1_test_time 0.08337195500001826
Test Epoch7 layer2 Acc 0.8586842105263158, AUC 0.9656371474266052, avg_entr 0.060007259249687195, f1 0.8586841821670532
ep7_l2_test_time 0.10522790799998916
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 7
Test Epoch7 layer3 Acc 0.8586842105263158, AUC 0.966317892074585, avg_entr 0.0520956888794899, f1 0.8586841821670532
ep7_l3_test_time 0.14000222100000315
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 7
Test Epoch7 layer4 Acc 0.858421052631579, AUC 0.9640598297119141, avg_entr 0.05020235478878021, f1 0.8584210276603699
ep7_l4_test_time 0.17392762900001912
gc 0
Train Epoch8 Acc 0.9209416666666667 (110513/120000), AUC 0.9825342297554016
ep8_train_time 20.477444410000004
Test Epoch8 layer0 Acc 0.8226315789473684, AUC 0.9540472626686096, avg_entr 0.14504171907901764, f1 0.8226315975189209
ep8_l0_test_time 0.06799937499999942
Test Epoch8 layer1 Acc 0.8489473684210527, AUC 0.9623327851295471, avg_entr 0.0740695372223854, f1 0.8489473462104797
ep8_l1_test_time 0.08252991499998075
Test Epoch8 layer2 Acc 0.8547368421052631, AUC 0.9657354354858398, avg_entr 0.05858355388045311, f1 0.854736864566803
ep8_l2_test_time 0.10476946299999668
Test Epoch8 layer3 Acc 0.8555263157894737, AUC 0.9646186232566833, avg_entr 0.04801781103014946, f1 0.855526328086853
ep8_l3_test_time 0.1339692249999871
Test Epoch8 layer4 Acc 0.8544736842105263, AUC 0.9613401293754578, avg_entr 0.04375072941184044, f1 0.8544737100601196
ep8_l4_test_time 0.17043216300001518
gc 0
Train Epoch9 Acc 0.9276166666666666 (111314/120000), AUC 0.9851969480514526
ep9_train_time 20.447180368000005
Test Epoch9 layer0 Acc 0.828421052631579, AUC 0.9542600512504578, avg_entr 0.13733543455600739, f1 0.8284210562705994
ep9_l0_test_time 0.06790682600001219
Test Epoch9 layer1 Acc 0.8571052631578947, AUC 0.9635998010635376, avg_entr 0.0655939131975174, f1 0.8571051955223083
ep9_l1_test_time 0.08228768200001468
Test Epoch9 layer2 Acc 0.8668421052631579, AUC 0.9653826951980591, avg_entr 0.04769822955131531, f1 0.8668420910835266
ep9_l2_test_time 0.10462940100001106
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 9
Test Epoch9 layer3 Acc 0.8668421052631579, AUC 0.9653745293617249, avg_entr 0.038812391459941864, f1 0.8668420910835266
ep9_l3_test_time 0.13754110299998956
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 9
Test Epoch9 layer4 Acc 0.8673684210526316, AUC 0.9652719497680664, avg_entr 0.034402333199977875, f1 0.8673684000968933
ep9_l4_test_time 0.17343502899998953
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 9
gc 0
Train Epoch10 Acc 0.9371833333333334 (112462/120000), AUC 0.9875932931900024
ep10_train_time 20.397399400000012
Test Epoch10 layer0 Acc 0.8344736842105264, AUC 0.9546865224838257, avg_entr 0.12883058190345764, f1 0.8344736695289612
ep10_l0_test_time 0.0676814610000065
Test Epoch10 layer1 Acc 0.8560526315789474, AUC 0.9644895792007446, avg_entr 0.05817146226763725, f1 0.8560526371002197
ep10_l1_test_time 0.08223656500001653
Test Epoch10 layer2 Acc 0.8657894736842106, AUC 0.9644376635551453, avg_entr 0.04398191347718239, f1 0.8657894730567932
ep10_l2_test_time 0.10460151399999518
Test Epoch10 layer3 Acc 0.8668421052631579, AUC 0.9621773958206177, avg_entr 0.03754204884171486, f1 0.8668420910835266
ep10_l3_test_time 0.13283607500000016
Test Epoch10 layer4 Acc 0.8660526315789474, AUC 0.9612798690795898, avg_entr 0.034639909863471985, f1 0.8660526275634766
ep10_l4_test_time 0.16916166300001123
gc 0
Train Epoch11 Acc 0.9463083333333333 (113557/120000), AUC 0.9904435276985168
ep11_train_time 20.351691587000005
Test Epoch11 layer0 Acc 0.8305263157894737, AUC 0.9543408751487732, avg_entr 0.11595991998910904, f1 0.8305262923240662
ep11_l0_test_time 0.06769462400001203
Test Epoch11 layer1 Acc 0.8557894736842105, AUC 0.9612950682640076, avg_entr 0.05249650403857231, f1 0.8557894825935364
ep11_l1_test_time 0.08240700800001832
Test Epoch11 layer2 Acc 0.866578947368421, AUC 0.9602551460266113, avg_entr 0.03721513971686363, f1 0.8665789365768433
ep11_l2_test_time 0.10445696900001167
Test Epoch11 layer3 Acc 0.866578947368421, AUC 0.9587475657463074, avg_entr 0.03080478496849537, f1 0.8665789365768433
ep11_l3_test_time 0.13440813400001161
Test Epoch11 layer4 Acc 0.8663157894736843, AUC 0.9562857151031494, avg_entr 0.027497438713908195, f1 0.8663158416748047
ep11_l4_test_time 0.16922823000004428
gc 0
Train Epoch12 Acc 0.9512083333333333 (114145/120000), AUC 0.991430938243866
ep12_train_time 20.51863737299999
Test Epoch12 layer0 Acc 0.838421052631579, AUC 0.9538595080375671, avg_entr 0.11216919869184494, f1 0.8384209871292114
ep12_l0_test_time 0.06773470600001019
Test Epoch12 layer1 Acc 0.8607894736842105, AUC 0.9596740007400513, avg_entr 0.05036910995841026, f1 0.8607894778251648
ep12_l1_test_time 0.0823287970000024
Test Epoch12 layer2 Acc 0.8660526315789474, AUC 0.9586898684501648, avg_entr 0.035587459802627563, f1 0.8660526275634766
ep12_l2_test_time 0.10429995900000222
Test Epoch12 layer3 Acc 0.868421052631579, AUC 0.9560143351554871, avg_entr 0.030725017189979553, f1 0.8684210777282715
ep12_l3_test_time 0.13296525399999837
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 12
Test Epoch12 layer4 Acc 0.8676315789473684, AUC 0.9532530307769775, avg_entr 0.02794952504336834, f1 0.8676315546035767
ep12_l4_test_time 0.17640042699997593
gc 0
Train Epoch13 Acc 0.95505 (114606/120000), AUC 0.9922831058502197
ep13_train_time 20.452663379
Test Epoch13 layer0 Acc 0.8286842105263158, AUC 0.9550206661224365, avg_entr 0.109327033162117, f1 0.8286842107772827
ep13_l0_test_time 0.0673623589999579
Test Epoch13 layer1 Acc 0.8555263157894737, AUC 0.9620644450187683, avg_entr 0.05095108598470688, f1 0.855526328086853
ep13_l1_test_time 0.08223590199997943
Test Epoch13 layer2 Acc 0.8621052631578947, AUC 0.9606215953826904, avg_entr 0.03333349525928497, f1 0.8621052503585815
ep13_l2_test_time 0.10459893600000214
Test Epoch13 layer3 Acc 0.8621052631578947, AUC 0.9579823017120361, avg_entr 0.02859756350517273, f1 0.8621052503585815
ep13_l3_test_time 0.13321022200000243
Test Epoch13 layer4 Acc 0.8628947368421053, AUC 0.9576870203018188, avg_entr 0.02481534704566002, f1 0.8628947138786316
ep13_l4_test_time 0.16952591799997663
gc 0
Train Epoch14 Acc 0.9586166666666667 (115034/120000), AUC 0.9934964776039124
ep14_train_time 20.471572236999975
Test Epoch14 layer0 Acc 0.8294736842105264, AUC 0.9534565210342407, avg_entr 0.10279643535614014, f1 0.829473614692688
ep14_l0_test_time 0.06820876000000453
Test Epoch14 layer1 Acc 0.8539473684210527, AUC 0.9596145153045654, avg_entr 0.04709460213780403, f1 0.8539473414421082
ep14_l1_test_time 0.0820379820000312
Test Epoch14 layer2 Acc 0.8592105263157894, AUC 0.9578520655632019, avg_entr 0.03169410303235054, f1 0.8592105507850647
ep14_l2_test_time 0.10464271699999017
Test Epoch14 layer3 Acc 0.8623684210526316, AUC 0.9553041458129883, avg_entr 0.025707127526402473, f1 0.8623684048652649
ep14_l3_test_time 0.13296197099998608
Test Epoch14 layer4 Acc 0.8623684210526316, AUC 0.9538125991821289, avg_entr 0.022865336388349533, f1 0.8623684048652649
ep14_l4_test_time 0.16908497400004308
gc 0
Train Epoch15 Acc 0.9636916666666666 (115643/120000), AUC 0.9947937726974487
ep15_train_time 20.520543049000025
Test Epoch15 layer0 Acc 0.828421052631579, AUC 0.9533278942108154, avg_entr 0.09366725385189056, f1 0.8284210562705994
ep15_l0_test_time 0.06802720600001066
Test Epoch15 layer1 Acc 0.855, AUC 0.9580299854278564, avg_entr 0.041712939739227295, f1 0.8550000190734863
ep15_l1_test_time 0.08221910700001445
Test Epoch15 layer2 Acc 0.8631578947368421, AUC 0.9561020731925964, avg_entr 0.02854042500257492, f1 0.8631578683853149
ep15_l2_test_time 0.10459237899999607
Test Epoch15 layer3 Acc 0.8626315789473684, AUC 0.9556233882904053, avg_entr 0.024301128461956978, f1 0.862631618976593
ep15_l3_test_time 0.13300184199999876
Test Epoch15 layer4 Acc 0.8623684210526316, AUC 0.9545720219612122, avg_entr 0.021805597469210625, f1 0.8623684048652649
ep15_l4_test_time 0.1692267119999542
gc 0
Train Epoch16 Acc 0.9657333333333333 (115888/120000), AUC 0.9950149059295654
ep16_train_time 20.559546109999985
Test Epoch16 layer0 Acc 0.8336842105263158, AUC 0.952763557434082, avg_entr 0.0935579389333725, f1 0.8336842060089111
ep16_l0_test_time 0.0680161999999882
Test Epoch16 layer1 Acc 0.8573684210526316, AUC 0.9564687013626099, avg_entr 0.040664784610271454, f1 0.8573684096336365
ep16_l1_test_time 0.08243583600000193
Test Epoch16 layer2 Acc 0.8605263157894737, AUC 0.9548181295394897, avg_entr 0.027059949934482574, f1 0.8605263233184814
ep16_l2_test_time 0.1055734259999781
Test Epoch16 layer3 Acc 0.8607894736842105, AUC 0.9541710615158081, avg_entr 0.022381186485290527, f1 0.8607894778251648
ep16_l3_test_time 0.1342057729999624
Test Epoch16 layer4 Acc 0.8607894736842105, AUC 0.9530555605888367, avg_entr 0.019694199785590172, f1 0.8607894778251648
ep16_l4_test_time 0.16947510099998908
gc 0
Train Epoch17 Acc 0.9681 (116172/120000), AUC 0.9955158233642578
ep17_train_time 20.428241333000017
Test Epoch17 layer0 Acc 0.8305263157894737, AUC 0.9534015655517578, avg_entr 0.09004712104797363, f1 0.8305262923240662
ep17_l0_test_time 0.0679167260000213
Test Epoch17 layer1 Acc 0.8552631578947368, AUC 0.9569884538650513, avg_entr 0.039046816527843475, f1 0.8552631735801697
ep17_l1_test_time 0.08286645800001224
Test Epoch17 layer2 Acc 0.8628947368421053, AUC 0.9561862945556641, avg_entr 0.026157669723033905, f1 0.8628947138786316
ep17_l2_test_time 0.10469651800002566
Test Epoch17 layer3 Acc 0.8652631578947368, AUC 0.9560964107513428, avg_entr 0.021831804886460304, f1 0.8652631640434265
ep17_l3_test_time 0.13335775399997374
Test Epoch17 layer4 Acc 0.8647368421052631, AUC 0.9539031386375427, avg_entr 0.01907416433095932, f1 0.8647368550300598
ep17_l4_test_time 0.16933050000000094
gc 0
Train Epoch18 Acc 0.9728083333333334 (116737/120000), AUC 0.9964835047721863
ep18_train_time 20.370688461999976
Test Epoch18 layer0 Acc 0.8357894736842105, AUC 0.9525395631790161, avg_entr 0.08812803775072098, f1 0.8357895016670227
ep18_l0_test_time 0.06771422799999982
Test Epoch18 layer1 Acc 0.8557894736842105, AUC 0.9555999636650085, avg_entr 0.03549076244235039, f1 0.8557894825935364
ep18_l1_test_time 0.08316204100003688
Test Epoch18 layer2 Acc 0.8626315789473684, AUC 0.9528913497924805, avg_entr 0.02227049507200718, f1 0.862631618976593
ep18_l2_test_time 0.10625126900004034
Test Epoch18 layer3 Acc 0.8621052631578947, AUC 0.953840434551239, avg_entr 0.018086280673742294, f1 0.8621052503585815
ep18_l3_test_time 0.13286087600005203
Test Epoch18 layer4 Acc 0.8621052631578947, AUC 0.9524312615394592, avg_entr 0.015614630654454231, f1 0.8621052503585815
ep18_l4_test_time 0.16917146600002297
gc 0
Train Epoch19 Acc 0.9759583333333334 (117115/120000), AUC 0.9969407320022583
ep19_train_time 20.421029783999984
Test Epoch19 layer0 Acc 0.8310526315789474, AUC 0.9517685174942017, avg_entr 0.08343464136123657, f1 0.8310526013374329
ep19_l0_test_time 0.06753584799997725
Test Epoch19 layer1 Acc 0.8544736842105263, AUC 0.9541222453117371, avg_entr 0.033797431737184525, f1 0.8544737100601196
ep19_l1_test_time 0.0828004449999753
Test Epoch19 layer2 Acc 0.8618421052631579, AUC 0.9506087303161621, avg_entr 0.019264210015535355, f1 0.8618420958518982
ep19_l2_test_time 0.104606423000007
Test Epoch19 layer3 Acc 0.8613157894736843, AUC 0.9515702724456787, avg_entr 0.015619708225131035, f1 0.8613157868385315
ep19_l3_test_time 0.13316221399998085
Test Epoch19 layer4 Acc 0.8605263157894737, AUC 0.9487417936325073, avg_entr 0.013688240200281143, f1 0.8605263233184814
ep19_l4_test_time 0.16962043799998128
Best AUC tensor(0.8684) 12 3
train_as_loss [[4.63134669e+02 3.57201103e+02 3.51225260e+02 3.49905941e+02
  3.49406281e+02 3.49165789e+02 3.49033101e+02 3.48953193e+02
  3.48902081e+02 3.48867942e+02 3.48844396e+02 3.48827759e+02
  3.48815783e+02 3.48807038e+02 3.48800574e+02 3.48795756e+02
  3.48792135e+02 3.48789401e+02 3.48787757e+02 3.48786771e+02]
 [1.47321094e+00 1.35467397e-05 9.19860547e-07 1.79365465e-07
  5.39127662e-08 3.84055172e-08 5.80109559e-06 4.61576644e-05
  9.18394853e-05 2.16127619e-04 2.62838714e-04 4.88249411e-06
  1.47814235e-09 6.45451797e-07 1.42334131e-04 6.14663974e-08
  1.02366157e-09 1.01481068e-09 3.18979174e-05 3.22798103e-07]
 [1.57845134e+00 1.59463779e-05 1.16608455e-06 2.35351404e-07
  7.65972912e-08 5.74250202e-08 8.09770485e-06 8.59330990e-05
  1.68379252e-04 4.24140986e-04 6.18650833e-04 2.76920346e-05
  3.65543761e-09 6.50575105e-07 2.69402792e-04 1.61515216e-07
  2.64228514e-09 3.11844005e-09 6.04473411e-05 5.65539240e-07]
 [1.72492365e+00 1.16050372e-05 9.38636118e-07 2.22082566e-07
  9.84327393e-08 7.51844518e-08 8.63013298e-06 1.39063044e-04
  2.83002368e-04 7.07730473e-04 1.04033192e-03 4.29303509e-05
  1.35862520e-08 6.33389049e-07 4.31288939e-04 5.95640678e-07
  9.80065284e-09 1.21906681e-08 8.95701871e-05 7.87235265e-07]
 [1.65759229e+00 1.16304459e-05 1.18731570e-06 3.71024816e-07
  2.39168848e-07 1.87357569e-07 9.84990441e-06 2.80961732e-04
  5.66792823e-04 4.53686707e-04 7.01445517e-04 1.46727729e-04
  7.65132391e-08 8.12034188e-07 3.00756690e-04 2.62465197e-06
  6.08657555e-08 9.24344134e-08 6.25188644e-05 2.52349531e-06]]
train_ae_loss [[15.95730981 16.26566616 15.64219944 14.72000547 14.22169456 13.90634068
  13.66340825 13.47428307 13.33019552 13.21329043 12.99904975 12.89132316
  12.72363964 12.66488448 12.59397781 12.45856233 12.38694545 12.25893985
  11.23412888 10.94962339]
 [13.81594398 13.66097999 12.76287874 11.96313032 10.98455584 10.08367544
   9.30783358  8.79265892  8.40719412  8.02355913  7.52900364  6.90033012
   6.60095793  6.35613355  6.14073968  5.74631415  5.58597144  5.40943082
   4.67918614  4.38044781]
 [13.81653954 12.94012863 11.3179965  10.33762553  9.32483682  8.53770361
   7.72409281  7.20292643  6.79293585  6.41760715  5.88590004  5.16817413
   4.87156302  4.60916407  4.38939513  3.94857048  3.79339402  3.62828237
   3.05960395  2.78106864]
 [14.78589696 13.55395527 10.90533056  9.58921758  8.56157114  7.77748134
   6.95348575  6.45801035  6.04960175  5.69156633  5.1247941   4.38942652
   4.1026096   3.85174958  3.651346    3.23092553  3.08142245  2.93309251
   2.45642945  2.21211774]
 [15.23665224 15.77939513 11.03767259  9.47359745  8.56641865  7.95746323
   7.08638333  6.50382772  5.98424548  5.450414    4.89577349  4.22003125
   3.9289827   3.68031969  3.43086243  3.04051389  2.90058423  2.75300644
   2.28007642  2.06574111]]
valid_acc (5, 20)
valid_AUC (5, 20)
train_acc (20,)
total_train+valid_time 427.264481084
Start Testing
Load ckpt at ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt
gc 9
Test layer0 Acc 0.8234210526315789, AUC 0.9478387832641602, avg_entr 0.11598488688468933, f1 0.823421061038971
l0_test_time 0.06679733900000429
gc 0
Test layer1 Acc 0.8539473684210527, AUC 0.9580327868461609, avg_entr 0.0530775710940361, f1 0.8539473414421082
l1_test_time 0.08338666800000283
gc 0
Test layer2 Acc 0.8610526315789474, AUC 0.9580565690994263, avg_entr 0.03724566474556923, f1 0.8610526323318481
l2_test_time 0.1058123380000211
gc 0
Test layer3 Acc 0.8628947368421053, AUC 0.9544259905815125, avg_entr 0.030351096764206886, f1 0.8628947138786316
l3_test_time 0.13652235799997925
gc 0
Test layer4 Acc 0.8618421052631579, AUC 0.9517644047737122, avg_entr 0.027003297582268715, f1 0.8618420958518982
l4_test_time 0.17129535800000895
gc 0
Test threshold 0.1 Acc 0.8581578947368421, AUC 0.9534481763839722, avg_entr 0.025910882279276848, f1 0.8581578731536865
t0.1_test_time 0.12108032600002616
gc 0
Test threshold 0.2 Acc 0.8528947368421053, AUC 0.9518079161643982, avg_entr 0.03503262624144554, f1 0.8528947234153748
t0.2_test_time 0.11810228799998868
gc 0
Test threshold 0.3 Acc 0.8442105263157895, AUC 0.9503147006034851, avg_entr 0.04965142905712128, f1 0.8442105054855347
t0.3_test_time 0.11584368199999062
gc 0
Test threshold 0.4 Acc 0.8310526315789474, AUC 0.9490340352058411, avg_entr 0.06805849820375443, f1 0.8310526013374329
t0.4_test_time 0.10045663400001104
gc 0
Test threshold 0.5 Acc 0.8255263157894737, AUC 0.9480533599853516, avg_entr 0.08021800220012665, f1 0.825526237487793
t0.5_test_time 0.08657621300000073
gc 0
Test threshold 0.6 Acc 0.8234210526315789, AUC 0.9478387832641602, avg_entr 0.08366541564464569, f1 0.823421061038971
t0.6_test_time 0.07889297200000556
gc 0
Test threshold 0.7 Acc 0.8234210526315789, AUC 0.9478387832641602, avg_entr 0.08366541564464569, f1 0.823421061038971
t0.7_test_time 0.07887567699998499
gc 0
Test threshold 0.8 Acc 0.8234210526315789, AUC 0.9478387832641602, avg_entr 0.08366541564464569, f1 0.823421061038971
t0.8_test_time 0.07913699200003066
gc 0
Test threshold 0.9 Acc 0.8234210526315789, AUC 0.9478387832641602, avg_entr 0.08366541564464569, f1 0.823421061038971
t0.9_test_time 0.07887684700000364
