total count words 102019
vocab size 30000
train size 120000, valid size 3800, test size 3800
found 26754 words in glove
model: TransformerALsideText(
  (layers): ModuleList(
    (0): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=4, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=4, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
  (emb_layers): ModuleList(
    (0): Embedding(30000, 300)
    (1): Embedding(30000, 300)
    (2): Embedding(30000, 300)
    (3): Embedding(30000, 300)
    (4): Embedding(30000, 300)
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.multi_headed_attention.linears.0.weight 90000
layers.0.enc.f.multi_headed_attention.linears.0.bias 300
layers.0.enc.f.multi_headed_attention.linears.1.weight 90000
layers.0.enc.f.multi_headed_attention.linears.1.bias 300
layers.0.enc.f.multi_headed_attention.linears.2.weight 90000
layers.0.enc.f.multi_headed_attention.linears.2.bias 300
layers.0.enc.f.multi_headed_attention.linears.3.weight 90000
layers.0.enc.f.multi_headed_attention.linears.3.bias 300
layers.0.enc.f.feed_forward.w_1.weight 90000
layers.0.enc.f.feed_forward.w_1.bias 300
layers.0.enc.f.feed_forward.w_2.weight 90000
layers.0.enc.f.feed_forward.w_2.bias 300
layers.0.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.0.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.0.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.0.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.0.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.0.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.0.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.0.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.0.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.0.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.0.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.0.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.0.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.0.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.0.enc.f.encoder.norm.a 300
layers.0.enc.f.encoder.norm.b 300
layers.0.ae.g.0.weight 512
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 512
layers.0.ae.h.0.bias 4
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
emb_layers.0.weight 9000000
emb_layers.1.weight 9000000
emb_layers.2.weight 9000000
emb_layers.3.weight 9000000
emb_layers.4.weight 9000000
Total Trainable Params: 50758892
init_time 15.193410796
Start Training
gc 0
Train Epoch0 Acc 0.26490833333333336 (31789/120000), AUC 0.5074185132980347
ep0_train_time 20.659208288000002
Test Epoch0 layer0 Acc 0.6726315789473685, AUC 0.874714732170105, avg_entr 0.9071996212005615, f1 0.672631561756134
ep0_l0_test_time 0.06938576299999966
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 0
Test Epoch0 layer1 Acc 0.6960526315789474, AUC 0.9057325124740601, avg_entr 0.8172799944877625, f1 0.696052610874176
ep0_l1_test_time 0.09210908600000067
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 0
Test Epoch0 layer2 Acc 0.6344736842105263, AUC 0.9078932404518127, avg_entr 0.8065152168273926, f1 0.6344736814498901
ep0_l2_test_time 0.1107870040000023
Test Epoch0 layer3 Acc 0.5573684210526316, AUC 0.8892624378204346, avg_entr 1.035001516342163, f1 0.5573683977127075
ep0_l3_test_time 0.13234175499999878
Test Epoch0 layer4 Acc 0.4123684210526316, AUC 0.8421623706817627, avg_entr 1.2177339792251587, f1 0.41236841678619385
ep0_l4_test_time 0.1686846779999982
gc 0
Train Epoch1 Acc 0.66105 (79326/120000), AUC 0.8542506098747253
ep1_train_time 20.364151934
Test Epoch1 layer0 Acc 0.7597368421052632, AUC 0.9208313822746277, avg_entr 0.5032588243484497, f1 0.7597368359565735
ep1_l0_test_time 0.06783906000000428
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 1
Test Epoch1 layer1 Acc 0.7826315789473685, AUC 0.9405953884124756, avg_entr 0.3901394307613373, f1 0.7826315760612488
ep1_l1_test_time 0.08936620000000062
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 1
Test Epoch1 layer2 Acc 0.7860526315789473, AUC 0.943555474281311, avg_entr 0.364390105009079, f1 0.7860525846481323
ep1_l2_test_time 0.10957231999999806
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 1
Test Epoch1 layer3 Acc 0.776578947368421, AUC 0.9430172443389893, avg_entr 0.37035468220710754, f1 0.776578962802887
ep1_l3_test_time 0.13637094999999988
Test Epoch1 layer4 Acc 0.7676315789473684, AUC 0.943489670753479, avg_entr 0.384280800819397, f1 0.7676315903663635
ep1_l4_test_time 0.1691152109999976
gc 0
Train Epoch2 Acc 0.809375 (97125/120000), AUC 0.938353419303894
ep2_train_time 20.333476813999994
Test Epoch2 layer0 Acc 0.7918421052631579, AUC 0.9413852691650391, avg_entr 0.3291691839694977, f1 0.7918421030044556
ep2_l0_test_time 0.06742995199999768
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 2
Test Epoch2 layer1 Acc 0.8107894736842105, AUC 0.9558399319648743, avg_entr 0.2598510980606079, f1 0.8107894659042358
ep2_l1_test_time 0.08982855499999687
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 2
Test Epoch2 layer2 Acc 0.8147368421052632, AUC 0.9581906795501709, avg_entr 0.23556992411613464, f1 0.8147368431091309
ep2_l2_test_time 0.10893580500000155
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 2
Test Epoch2 layer3 Acc 0.8152631578947368, AUC 0.9584470987319946, avg_entr 0.23398739099502563, f1 0.8152631521224976
ep2_l3_test_time 0.14113972200000546
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 2
Test Epoch2 layer4 Acc 0.8144736842105263, AUC 0.9584503769874573, avg_entr 0.22870883345603943, f1 0.8144736886024475
ep2_l4_test_time 0.17186633400000062
gc 0
Train Epoch3 Acc 0.8518083333333333 (102217/120000), AUC 0.958611249923706
ep3_train_time 20.309257321999993
Test Epoch3 layer0 Acc 0.8068421052631579, AUC 0.9464249014854431, avg_entr 0.25072264671325684, f1 0.8068421483039856
ep3_l0_test_time 0.06745575600000109
Test Epoch3 layer1 Acc 0.8360526315789474, AUC 0.9596567153930664, avg_entr 0.17981761693954468, f1 0.836052656173706
ep3_l1_test_time 0.08167281100000423
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 3
Test Epoch3 layer2 Acc 0.843421052631579, AUC 0.9620193243026733, avg_entr 0.15315715968608856, f1 0.8434211015701294
ep3_l2_test_time 0.11003187000000025
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 3
Test Epoch3 layer3 Acc 0.8444736842105263, AUC 0.962730884552002, avg_entr 0.14468546211719513, f1 0.844473659992218
ep3_l3_test_time 0.13709082399999772
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 3
Test Epoch3 layer4 Acc 0.8442105263157895, AUC 0.9619795083999634, avg_entr 0.13926929235458374, f1 0.8442105054855347
ep3_l4_test_time 0.1719302590000069
gc 0
Train Epoch4 Acc 0.8737583333333333 (104851/120000), AUC 0.9673778414726257
ep4_train_time 20.429036858000003
Test Epoch4 layer0 Acc 0.8205263157894737, AUC 0.9508867263793945, avg_entr 0.21399196982383728, f1 0.8205263018608093
ep4_l0_test_time 0.0673817169999893
Test Epoch4 layer1 Acc 0.8518421052631578, AUC 0.962890625, avg_entr 0.13512055575847626, f1 0.8518421053886414
ep4_l1_test_time 0.08169875299999774
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 4
Test Epoch4 layer2 Acc 0.8571052631578947, AUC 0.9648842811584473, avg_entr 0.10868794471025467, f1 0.8571051955223083
ep4_l2_test_time 0.11173977999999352
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 4
Test Epoch4 layer3 Acc 0.8563157894736843, AUC 0.9653388857841492, avg_entr 0.10178704559803009, f1 0.8563157916069031
ep4_l3_test_time 0.13758076300000255
Test Epoch4 layer4 Acc 0.8557894736842105, AUC 0.9648117423057556, avg_entr 0.09617958217859268, f1 0.8557894825935364
ep4_l4_test_time 0.16883063400000253
gc 0
Train Epoch5 Acc 0.8895583333333333 (106747/120000), AUC 0.9724868535995483
ep5_train_time 20.40950137099999
Test Epoch5 layer0 Acc 0.8236842105263158, AUC 0.9542767405509949, avg_entr 0.18391236662864685, f1 0.8236842155456543
ep5_l0_test_time 0.06721462100000508
Test Epoch5 layer1 Acc 0.8518421052631578, AUC 0.9644602537155151, avg_entr 0.1050395742058754, f1 0.8518421053886414
ep5_l1_test_time 0.08173659699997415
Test Epoch5 layer2 Acc 0.8547368421052631, AUC 0.9665482044219971, avg_entr 0.08417659997940063, f1 0.854736864566803
ep5_l2_test_time 0.10383093500001905
Test Epoch5 layer3 Acc 0.855, AUC 0.966206431388855, avg_entr 0.07688050717115402, f1 0.8550000190734863
ep5_l3_test_time 0.13247580099999823
Test Epoch5 layer4 Acc 0.8544736842105263, AUC 0.96565842628479, avg_entr 0.0696241706609726, f1 0.8544737100601196
ep5_l4_test_time 0.1686526120000167
gc 0
Train Epoch6 Acc 0.9032916666666667 (108395/120000), AUC 0.9777724742889404
ep6_train_time 20.30777981899999
Test Epoch6 layer0 Acc 0.8213157894736842, AUC 0.9543638229370117, avg_entr 0.17618854343891144, f1 0.8213157653808594
ep6_l0_test_time 0.06716582000001381
Test Epoch6 layer1 Acc 0.8581578947368421, AUC 0.9656018018722534, avg_entr 0.09016083925962448, f1 0.8581578731536865
ep6_l1_test_time 0.0822045970000147
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 6
Test Epoch6 layer2 Acc 0.8647368421052631, AUC 0.9682015180587769, avg_entr 0.07324961572885513, f1 0.8647368550300598
ep6_l2_test_time 0.11002417999998215
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 6
Test Epoch6 layer3 Acc 0.8639473684210527, AUC 0.9681684970855713, avg_entr 0.06721428781747818, f1 0.8639474511146545
ep6_l3_test_time 0.14950389099999484
Test Epoch6 layer4 Acc 0.865, AUC 0.9674404263496399, avg_entr 0.0570346862077713, f1 0.8650000095367432
ep6_l4_test_time 0.16937691100000052
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 6
gc 0
Train Epoch7 Acc 0.9129 (109548/120000), AUC 0.9802278280258179
ep7_train_time 20.341760731999983
Test Epoch7 layer0 Acc 0.8236842105263158, AUC 0.9549846053123474, avg_entr 0.1571839600801468, f1 0.8236842155456543
ep7_l0_test_time 0.0678819469999894
Test Epoch7 layer1 Acc 0.8544736842105263, AUC 0.96468585729599, avg_entr 0.08003486692905426, f1 0.8544737100601196
ep7_l1_test_time 0.08180910900000526
Test Epoch7 layer2 Acc 0.8589473684210527, AUC 0.9666305184364319, avg_entr 0.06356800347566605, f1 0.8589473962783813
ep7_l2_test_time 0.10416922999999656
Test Epoch7 layer3 Acc 0.86, AUC 0.9666202068328857, avg_entr 0.057874105870723724, f1 0.8600000143051147
ep7_l3_test_time 0.13287266800000452
Test Epoch7 layer4 Acc 0.8605263157894737, AUC 0.9662046432495117, avg_entr 0.0526464469730854, f1 0.8605263233184814
ep7_l4_test_time 0.16885228000001007
gc 0
Train Epoch8 Acc 0.9212833333333333 (110554/120000), AUC 0.9829716682434082
ep8_train_time 20.412413891
Test Epoch8 layer0 Acc 0.8344736842105264, AUC 0.9566333293914795, avg_entr 0.1402108371257782, f1 0.8344736695289612
ep8_l0_test_time 0.06735265300000037
Test Epoch8 layer1 Acc 0.8605263157894737, AUC 0.9660642147064209, avg_entr 0.0692848414182663, f1 0.8605263233184814
ep8_l1_test_time 0.08217142099999819
Test Epoch8 layer2 Acc 0.8692105263157894, AUC 0.968923032283783, avg_entr 0.05284929648041725, f1 0.8692105412483215
ep8_l2_test_time 0.10418593599999326
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 8
Test Epoch8 layer3 Acc 0.8694736842105263, AUC 0.9672776460647583, avg_entr 0.04663025587797165, f1 0.8694736957550049
ep8_l3_test_time 0.13915643799998634
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 8
Test Epoch8 layer4 Acc 0.8692105263157894, AUC 0.9651470184326172, avg_entr 0.041448164731264114, f1 0.8692105412483215
ep8_l4_test_time 0.17492694200001324
gc 0
Train Epoch9 Acc 0.9288583333333333 (111463/120000), AUC 0.98459792137146
ep9_train_time 20.407115524999995
Test Epoch9 layer0 Acc 0.8313157894736842, AUC 0.9566755890846252, avg_entr 0.12993842363357544, f1 0.831315815448761
ep9_l0_test_time 0.06752520099999515
Test Epoch9 layer1 Acc 0.8594736842105263, AUC 0.9660062193870544, avg_entr 0.06053284928202629, f1 0.859473705291748
ep9_l1_test_time 0.08187155900000676
Test Epoch9 layer2 Acc 0.868421052631579, AUC 0.9680140614509583, avg_entr 0.04686134681105614, f1 0.8684210777282715
ep9_l2_test_time 0.10405363799998213
Test Epoch9 layer3 Acc 0.866578947368421, AUC 0.9657446146011353, avg_entr 0.04041885957121849, f1 0.8665789365768433
ep9_l3_test_time 0.13256560799999306
Test Epoch9 layer4 Acc 0.8647368421052631, AUC 0.9628243446350098, avg_entr 0.035534169524908066, f1 0.8647368550300598
ep9_l4_test_time 0.1686922279999976
gc 0
Train Epoch10 Acc 0.9364916666666666 (112379/120000), AUC 0.9870814085006714
ep10_train_time 20.357490671000022
Test Epoch10 layer0 Acc 0.8307894736842105, AUC 0.956255316734314, avg_entr 0.123451367020607, f1 0.8307894468307495
ep10_l0_test_time 0.06724084900000094
Test Epoch10 layer1 Acc 0.8613157894736843, AUC 0.9668516516685486, avg_entr 0.05889018997550011, f1 0.8613157868385315
ep10_l1_test_time 0.08199316399998224
Test Epoch10 layer2 Acc 0.8689473684210526, AUC 0.9669115543365479, avg_entr 0.04205607250332832, f1 0.8689473867416382
ep10_l2_test_time 0.10414351700001134
Test Epoch10 layer3 Acc 0.8678947368421053, AUC 0.9650545716285706, avg_entr 0.03597916290163994, f1 0.86789470911026
ep10_l3_test_time 0.13276842299998748
Test Epoch10 layer4 Acc 0.8671052631578947, AUC 0.9614783525466919, avg_entr 0.03166472539305687, f1 0.86710524559021
ep10_l4_test_time 0.1692337460000033
gc 0
Train Epoch11 Acc 0.9439833333333333 (113278/120000), AUC 0.9900669455528259
ep11_train_time 20.513337552999985
Test Epoch11 layer0 Acc 0.828421052631579, AUC 0.9564414620399475, avg_entr 0.11834743618965149, f1 0.8284210562705994
ep11_l0_test_time 0.068655165999985
Test Epoch11 layer1 Acc 0.8613157894736843, AUC 0.9667674899101257, avg_entr 0.051767200231552124, f1 0.8613157868385315
ep11_l1_test_time 0.08197471900001574
Test Epoch11 layer2 Acc 0.8707894736842106, AUC 0.9665372371673584, avg_entr 0.033140987157821655, f1 0.8707894682884216
ep11_l2_test_time 0.10427670199999284
Save ckpt to ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt  ,ep 11
Test Epoch11 layer3 Acc 0.868421052631579, AUC 0.966138482093811, avg_entr 0.027225276455283165, f1 0.8684210777282715
ep11_l3_test_time 0.13759894700001496
Test Epoch11 layer4 Acc 0.8694736842105263, AUC 0.9652378559112549, avg_entr 0.023893598467111588, f1 0.8694736957550049
ep11_l4_test_time 0.17015007800000603
gc 0
Train Epoch12 Acc 0.9490583333333333 (113887/120000), AUC 0.9910480380058289
ep12_train_time 20.429097025000033
Test Epoch12 layer0 Acc 0.8310526315789474, AUC 0.9561136364936829, avg_entr 0.11676163226366043, f1 0.8310526013374329
ep12_l0_test_time 0.06797039799999993
Test Epoch12 layer1 Acc 0.8571052631578947, AUC 0.9658797979354858, avg_entr 0.0521356426179409, f1 0.8571051955223083
ep12_l1_test_time 0.08178681800001186
Test Epoch12 layer2 Acc 0.8626315789473684, AUC 0.9647948145866394, avg_entr 0.034541528671979904, f1 0.862631618976593
ep12_l2_test_time 0.10414023399999905
Test Epoch12 layer3 Acc 0.8621052631578947, AUC 0.9639180898666382, avg_entr 0.029178062453866005, f1 0.8621052503585815
ep12_l3_test_time 0.13291959899999028
Test Epoch12 layer4 Acc 0.8628947368421053, AUC 0.9603561162948608, avg_entr 0.026233961805701256, f1 0.8628947138786316
ep12_l4_test_time 0.1686477740000214
gc 0
Train Epoch13 Acc 0.9562083333333333 (114745/120000), AUC 0.992497444152832
ep13_train_time 20.464307700999996
Test Epoch13 layer0 Acc 0.8307894736842105, AUC 0.9565358757972717, avg_entr 0.10944700241088867, f1 0.8307894468307495
ep13_l0_test_time 0.067397743000015
Test Epoch13 layer1 Acc 0.8557894736842105, AUC 0.9635690450668335, avg_entr 0.049747150391340256, f1 0.8557894825935364
ep13_l1_test_time 0.08177564400000392
Test Epoch13 layer2 Acc 0.865, AUC 0.96338951587677, avg_entr 0.037375349551439285, f1 0.8650000095367432
ep13_l2_test_time 0.10410309200000256
Test Epoch13 layer3 Acc 0.8655263157894737, AUC 0.9618359804153442, avg_entr 0.03169102966785431, f1 0.8655263185501099
ep13_l3_test_time 0.13407367799999292
Test Epoch13 layer4 Acc 0.865, AUC 0.9593193531036377, avg_entr 0.028526604175567627, f1 0.8650000095367432
ep13_l4_test_time 0.16908317499996883
gc 0
Train Epoch14 Acc 0.9621083333333333 (115453/120000), AUC 0.9937323927879333
ep14_train_time 20.39881407599995
Test Epoch14 layer0 Acc 0.8305263157894737, AUC 0.9553591012954712, avg_entr 0.10470705479383469, f1 0.8305262923240662
ep14_l0_test_time 0.06728450699995392
Test Epoch14 layer1 Acc 0.8510526315789474, AUC 0.9619647264480591, avg_entr 0.051315195858478546, f1 0.8510526418685913
ep14_l1_test_time 0.08184708199996749
Test Epoch14 layer2 Acc 0.853421052631579, AUC 0.9601759910583496, avg_entr 0.03812413290143013, f1 0.8534210324287415
ep14_l2_test_time 0.10400959499997953
Test Epoch14 layer3 Acc 0.8528947368421053, AUC 0.9604002237319946, avg_entr 0.03266902267932892, f1 0.8528947234153748
ep14_l3_test_time 0.13263680899996189
Test Epoch14 layer4 Acc 0.8526315789473684, AUC 0.9588443040847778, avg_entr 0.029706833884119987, f1 0.8526315689086914
ep14_l4_test_time 0.1690468140000121
gc 0
Train Epoch15 Acc 0.9681583333333333 (116179/120000), AUC 0.9952881932258606
ep15_train_time 20.349523798999996
Test Epoch15 layer0 Acc 0.8363157894736842, AUC 0.9557598829269409, avg_entr 0.1017516478896141, f1 0.8363158106803894
ep15_l0_test_time 0.06771123300001136
Test Epoch15 layer1 Acc 0.8618421052631579, AUC 0.9624976515769958, avg_entr 0.043470025062561035, f1 0.8618420958518982
ep15_l1_test_time 0.08185724300000174
Test Epoch15 layer2 Acc 0.8673684210526316, AUC 0.9606305956840515, avg_entr 0.027440592646598816, f1 0.8673684000968933
ep15_l2_test_time 0.10567553499998894
Test Epoch15 layer3 Acc 0.8673684210526316, AUC 0.9595333337783813, avg_entr 0.02389579452574253, f1 0.8673684000968933
ep15_l3_test_time 0.13333758799996076
Test Epoch15 layer4 Acc 0.8663157894736843, AUC 0.9589644074440002, avg_entr 0.021495314314961433, f1 0.8663158416748047
ep15_l4_test_time 0.16886622300000909
gc 0
Train Epoch16 Acc 0.970575 (116469/120000), AUC 0.995647668838501
ep16_train_time 20.45222407
Test Epoch16 layer0 Acc 0.8365789473684211, AUC 0.9549702405929565, avg_entr 0.09748002141714096, f1 0.8365789651870728
ep16_l0_test_time 0.0673961429999963
Test Epoch16 layer1 Acc 0.8565789473684211, AUC 0.9584356546401978, avg_entr 0.042079489678144455, f1 0.8565789461135864
ep16_l1_test_time 0.08215759699999126
Test Epoch16 layer2 Acc 0.865, AUC 0.9569621682167053, avg_entr 0.027075938880443573, f1 0.8650000095367432
ep16_l2_test_time 0.1044289470000308
Test Epoch16 layer3 Acc 0.865, AUC 0.9555366039276123, avg_entr 0.02319117821753025, f1 0.8650000095367432
ep16_l3_test_time 0.1329992149999839
Test Epoch16 layer4 Acc 0.8647368421052631, AUC 0.9543226957321167, avg_entr 0.019736463204026222, f1 0.8647368550300598
ep16_l4_test_time 0.16895579700002372
gc 0
Train Epoch17 Acc 0.9742666666666666 (116912/120000), AUC 0.9962452054023743
ep17_train_time 20.363624442999992
Test Epoch17 layer0 Acc 0.8378947368421052, AUC 0.9553232789039612, avg_entr 0.0951913371682167, f1 0.8378947377204895
ep17_l0_test_time 0.06750399999998535
Test Epoch17 layer1 Acc 0.8557894736842105, AUC 0.9599123001098633, avg_entr 0.04096144437789917, f1 0.8557894825935364
ep17_l1_test_time 0.0835174349999761
Test Epoch17 layer2 Acc 0.8626315789473684, AUC 0.9570119380950928, avg_entr 0.02507619932293892, f1 0.862631618976593
ep17_l2_test_time 0.1047841710000057
Test Epoch17 layer3 Acc 0.8657894736842106, AUC 0.9566256999969482, avg_entr 0.0204841960221529, f1 0.8657894730567932
ep17_l3_test_time 0.1328838580000138
Test Epoch17 layer4 Acc 0.8644736842105263, AUC 0.9557429552078247, avg_entr 0.018146369606256485, f1 0.8644736409187317
ep17_l4_test_time 0.16922027600003275
gc 0
Train Epoch18 Acc 0.9765416666666666 (117185/120000), AUC 0.9965774416923523
ep18_train_time 20.451227566
Test Epoch18 layer0 Acc 0.8365789473684211, AUC 0.9544392824172974, avg_entr 0.09427376091480255, f1 0.8365789651870728
ep18_l0_test_time 0.06788063100003683
Test Epoch18 layer1 Acc 0.8571052631578947, AUC 0.9571323394775391, avg_entr 0.04075142368674278, f1 0.8571051955223083
ep18_l1_test_time 0.0823182769999562
Test Epoch18 layer2 Acc 0.8597368421052631, AUC 0.9558112621307373, avg_entr 0.02348039671778679, f1 0.8597368597984314
ep18_l2_test_time 0.10450752500003091
Test Epoch18 layer3 Acc 0.8597368421052631, AUC 0.9535028338432312, avg_entr 0.019273824989795685, f1 0.8597368597984314
ep18_l3_test_time 0.13294088800000736
Test Epoch18 layer4 Acc 0.8607894736842105, AUC 0.9532206058502197, avg_entr 0.01684614270925522, f1 0.8607894778251648
ep18_l4_test_time 0.16860934499999303
gc 0
Train Epoch19 Acc 0.977975 (117357/120000), AUC 0.9971393346786499
ep19_train_time 20.354608865999978
Test Epoch19 layer0 Acc 0.8344736842105264, AUC 0.9540145397186279, avg_entr 0.09113827347755432, f1 0.8344736695289612
ep19_l0_test_time 0.06762957699999106
Test Epoch19 layer1 Acc 0.8592105263157894, AUC 0.9576656222343445, avg_entr 0.03957955539226532, f1 0.8592105507850647
ep19_l1_test_time 0.08144796799996357
Test Epoch19 layer2 Acc 0.8642105263157894, AUC 0.9530854225158691, avg_entr 0.024280980229377747, f1 0.8642105460166931
ep19_l2_test_time 0.10374928999999611
Test Epoch19 layer3 Acc 0.8652631578947368, AUC 0.9562873244285583, avg_entr 0.019705964252352715, f1 0.8652631640434265
ep19_l3_test_time 0.13249072499996828
Test Epoch19 layer4 Acc 0.8652631578947368, AUC 0.9544522166252136, avg_entr 0.017051488161087036, f1 0.8652631640434265
ep19_l4_test_time 0.16862100800000235
Best AUC tensor(0.8708) 11 2
train_as_loss [[4.54753358e+02 3.56530777e+02 3.51054937e+02 3.49828933e+02
  3.49362966e+02 3.49138564e+02 3.49014828e+02 3.48940399e+02
  3.48892867e+02 3.48861177e+02 3.48839353e+02 3.48823961e+02
  3.48812898e+02 3.48806482e+02 3.48802701e+02 3.48799211e+02
  3.48796054e+02 3.48793911e+02 3.48792471e+02 3.48791060e+02]
 [1.62406826e+00 1.64485563e-05 1.17132344e-06 2.39372802e-07
  7.34886551e-08 2.75009975e-08 1.20350622e-08 8.05054593e-06
  7.06044839e-05 1.62081213e-04 2.67502546e-04 3.56549972e-04
  3.66405198e-04 5.16684976e-04 4.38097667e-04 2.57722119e-07
  4.93950667e-10 8.54610049e-09 3.08163364e-04 7.59059247e-07]
 [1.49407174e+00 1.28267952e-05 8.81411921e-07 1.87205239e-07
  6.34070020e-08 2.70557428e-08 1.50494206e-08 5.44693866e-05
  2.46187061e-04 5.06022572e-04 7.99435489e-04 8.36797265e-04
  8.29657463e-04 2.29051248e-04 2.41130585e-04 1.22149069e-06
  2.45948992e-09 4.10981966e-09 2.10242409e-04 6.05055330e-06]
 [1.59916561e+00 1.39775971e-05 1.15120294e-06 2.88652403e-07
  1.25871907e-07 6.72739875e-08 4.97642507e-08 1.17750302e-04
  4.96745504e-04 9.22507967e-04 1.51669460e-03 4.81634779e-04
  5.07290682e-04 2.43211894e-04 2.60021293e-04 8.06353190e-07
  3.58383522e-09 5.97872682e-09 2.62180259e-04 6.20330815e-06]
 [1.66205329e+00 2.22022602e-05 2.07609182e-06 6.18064246e-07
  3.15931584e-07 2.06186020e-07 1.77263407e-07 1.68209879e-04
  7.33823241e-04 1.32315212e-03 2.40355299e-03 4.13116619e-04
  4.49591347e-04 2.55018817e-04 3.07132085e-04 9.19242488e-07
  7.08755842e-09 1.30644977e-08 3.09825512e-04 1.23314958e-05]]
train_ae_loss [[15.95232657 15.90752193 15.32757625 14.42811453 14.02639332 13.70141771
  13.49370536 13.34541093 13.21590984 13.08292651 12.94149716 12.79066624
  12.68064431 11.7774941  11.49785545 11.38507483 11.3309434  10.7866695
  10.6656127  10.65572213]
 [13.45521363 12.66882613 12.06056936 11.15055994 10.17840185  9.27841884
   8.57414931  8.10986784  7.73779153  7.33257941  6.95657108  6.56854002
   6.25966349  5.58020631  5.11731312  4.56385747  4.37083224  3.9521556
   3.74210192  3.51365508]
 [12.91273979 11.44109461 10.52932633  9.4709286   8.5695272   7.7785177
   7.07044914  6.59570168  6.2085086   5.77829486  5.36444952  4.9522048
   4.67458525  3.94547475  3.56109827  3.09146756  2.91336833  2.48132045
   2.32454167  2.15913615]
 [12.46017537 10.5115256   9.0218942   7.82823239  6.92016292  6.21563433
   5.6376229   5.26472288  4.95198341  4.58212491  4.22854948  3.78893636
   3.56400581  3.0405364   2.70160703  2.28520078  2.13991073  1.82340839
   1.69881464  1.54630878]
 [11.65145575 11.69922111  8.70613257  7.25240641  6.35682577  5.76310605
   5.23623577  4.89370153  4.5998949   4.27256719  3.93843699  3.49454797
   3.28901356  2.8105272   2.49192226  2.08962361  1.95925094  1.67251946
   1.55403675  1.41509051]]
valid_acc (5, 20)
valid_AUC (5, 20)
train_acc (20,)
total_train+valid_time 425.215932625
Start Testing
Load ckpt at ckpt/ag_news_transformeralside_l5_256_sidead//ag_news_transformeralside_l5_side.pt
gc 9
Test layer0 Acc 0.8271052631578948, AUC 0.9529147148132324, avg_entr 0.119998998939991, f1 0.8271052837371826
l0_test_time 0.06707254500003046
gc 0
Test layer1 Acc 0.8544736842105263, AUC 0.9594020843505859, avg_entr 0.0506628081202507, f1 0.8544737100601196
l1_test_time 0.08393885200001705
gc 0
Test layer2 Acc 0.8589473684210527, AUC 0.9613869786262512, avg_entr 0.03412574902176857, f1 0.8589473962783813
l2_test_time 0.10591119200000776
gc 0
Test layer3 Acc 0.86, AUC 0.9618126153945923, avg_entr 0.028698062524199486, f1 0.8600000143051147
l3_test_time 0.13504790499996489
gc 0
Test layer4 Acc 0.8626315789473684, AUC 0.9606106281280518, avg_entr 0.025314169004559517, f1 0.862631618976593
l4_test_time 0.17118072800002437
gc 0
Test threshold 0.1 Acc 0.8586842105263158, AUC 0.9542946219444275, avg_entr 0.026662107557058334, f1 0.8586841821670532
t0.1_test_time 0.12018486599998823
gc 0
Test threshold 0.2 Acc 0.8536842105263158, AUC 0.9544732570648193, avg_entr 0.03675089031457901, f1 0.8536841869354248
t0.2_test_time 0.11863221700002669
gc 0
Test threshold 0.3 Acc 0.8457894736842105, AUC 0.9547128081321716, avg_entr 0.05296732857823372, f1 0.8457894921302795
t0.3_test_time 0.11678205699996624
gc 0
Test threshold 0.4 Acc 0.8363157894736842, AUC 0.9542790651321411, avg_entr 0.07300560921430588, f1 0.8363158106803894
t0.4_test_time 0.10194262999999637
gc 0
Test threshold 0.5 Acc 0.83, AUC 0.9532383680343628, avg_entr 0.08277761191129684, f1 0.8299999833106995
t0.5_test_time 0.08787522100004708
gc 0
Test threshold 0.6 Acc 0.8271052631578948, AUC 0.9529147148132324, avg_entr 0.08656096458435059, f1 0.8271052837371826
t0.6_test_time 0.07862348499998006
gc 0
Test threshold 0.7 Acc 0.8271052631578948, AUC 0.9529147148132324, avg_entr 0.08656096458435059, f1 0.8271052837371826
t0.7_test_time 0.07873714799995923
gc 0
Test threshold 0.8 Acc 0.8271052631578948, AUC 0.9529147148132324, avg_entr 0.08656096458435059, f1 0.8271052837371826
t0.8_test_time 0.07939782299996523
gc 0
Test threshold 0.9 Acc 0.8271052631578948, AUC 0.9529147148132324, avg_entr 0.08656096458435059, f1 0.8271052837371826
t0.9_test_time 0.07876006800000823
