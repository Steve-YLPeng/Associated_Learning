total count words 222751
vocab size 30000
train size 40000, valid size 5000, test size 5000
found 27937 words in glove
model: TransformerModelML(
  (layers): ModuleList(
    (0): EMBLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Embedding(30000, 300)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=2, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=2, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.weight 9000000
layers.0.ae.g.0.weight 256
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 256
layers.0.ae.h.0.bias 2
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
Total Trainable Params: 13671778
init_time 21.840424299240112
Start Training
gc 0
Train Epoch0 Acc 0.50995 (20398/40000), AUC 0.511380672454834
ep0_train_time 32.34336614608765
Test Epoch0 layer0 Acc 0.8038, AUC 0.8950046300888062, avg_entr 0.41016361117362976, f1 0.8037999868392944
ep0_l0_test_time 0.3354012966156006
Save ckpt to ckpt/imdb_transformeral_l5_pad100//imdb_transformeral_l5.pt  ,ep 0
Test Epoch0 layer1 Acc 0.7452, AUC 0.8758842945098877, avg_entr 0.5252123475074768, f1 0.745199978351593
ep0_l1_test_time 0.5053837299346924
Test Epoch0 layer2 Acc 0.6618, AUC 0.854736328125, avg_entr 0.6557474136352539, f1 0.6618000268936157
ep0_l2_test_time 0.6824686527252197
Test Epoch0 layer3 Acc 0.5046, AUC 0.6329113245010376, avg_entr 0.6799227595329285, f1 0.5045999884605408
ep0_l3_test_time 0.8617432117462158
Test Epoch0 layer4 Acc 0.5932, AUC 0.651282548904419, avg_entr 0.6903616786003113, f1 0.5932000279426575
ep0_l4_test_time 1.036567211151123
gc 0
Train Epoch1 Acc 0.800225 (32009/40000), AUC 0.8836417198181152
ep1_train_time 31.42996120452881
Test Epoch1 layer0 Acc 0.8394, AUC 0.9191202521324158, avg_entr 0.2656346261501312, f1 0.8393999934196472
ep1_l0_test_time 0.3150196075439453
Save ckpt to ckpt/imdb_transformeral_l5_pad100//imdb_transformeral_l5.pt  ,ep 1
Test Epoch1 layer1 Acc 0.8426, AUC 0.9192296862602234, avg_entr 0.23337186872959137, f1 0.8426000475883484
ep1_l1_test_time 0.491119384765625
Save ckpt to ckpt/imdb_transformeral_l5_pad100//imdb_transformeral_l5.pt  ,ep 1
Test Epoch1 layer2 Acc 0.8432, AUC 0.9211046695709229, avg_entr 0.2052651196718216, f1 0.8432000875473022
ep1_l2_test_time 0.6806936264038086
Save ckpt to ckpt/imdb_transformeral_l5_pad100//imdb_transformeral_l5.pt  ,ep 1
Test Epoch1 layer3 Acc 0.846, AUC 0.9202011227607727, avg_entr 0.17905661463737488, f1 0.8460000157356262
ep1_l3_test_time 0.8604519367218018
Save ckpt to ckpt/imdb_transformeral_l5_pad100//imdb_transformeral_l5.pt  ,ep 1
Test Epoch1 layer4 Acc 0.8444, AUC 0.9198563098907471, avg_entr 0.17523159086704254, f1 0.8443999886512756
ep1_l4_test_time 1.0278732776641846
gc 0
Train Epoch2 Acc 0.8978 (35912/40000), AUC 0.9604934453964233
ep2_train_time 31.433194875717163
Test Epoch2 layer0 Acc 0.843, AUC 0.9199192523956299, avg_entr 0.21783339977264404, f1 0.8429999947547913
ep2_l0_test_time 0.3310129642486572
Test Epoch2 layer1 Acc 0.84, AUC 0.916785478591919, avg_entr 0.18836121261119843, f1 0.8399999737739563
ep2_l1_test_time 0.5105569362640381
Test Epoch2 layer2 Acc 0.8336, AUC 0.9133157134056091, avg_entr 0.15019521117210388, f1 0.8335999846458435
ep2_l2_test_time 0.692209005355835
Test Epoch2 layer3 Acc 0.8338, AUC 0.9120563268661499, avg_entr 0.1051124781370163, f1 0.8338000774383545
ep2_l3_test_time 0.8596484661102295
Test Epoch2 layer4 Acc 0.8334, AUC 0.911412239074707, avg_entr 0.10725969076156616, f1 0.8334000110626221
ep2_l4_test_time 1.033839225769043
gc 0
Train Epoch3 Acc 0.9182 (36728/40000), AUC 0.9716431498527527
ep3_train_time 31.20429754257202
Test Epoch3 layer0 Acc 0.8344, AUC 0.916088879108429, avg_entr 0.19120101630687714, f1 0.8343999981880188
ep3_l0_test_time 0.3299977779388428
Test Epoch3 layer1 Acc 0.8298, AUC 0.9092798829078674, avg_entr 0.1723305732011795, f1 0.829800009727478
ep3_l1_test_time 0.5106608867645264
Test Epoch3 layer2 Acc 0.823, AUC 0.905907392501831, avg_entr 0.10221757739782333, f1 0.8230000138282776
ep3_l2_test_time 0.6931061744689941
Test Epoch3 layer3 Acc 0.8238, AUC 0.9046223163604736, avg_entr 0.07917153090238571, f1 0.8238000273704529
ep3_l3_test_time 0.8581936359405518
Test Epoch3 layer4 Acc 0.8232, AUC 0.9045262336730957, avg_entr 0.08139799535274506, f1 0.823199987411499
ep3_l4_test_time 1.0430972576141357
gc 0
Train Epoch4 Acc 0.931925 (37277/40000), AUC 0.9788148999214172
ep4_train_time 31.48083806037903
Test Epoch4 layer0 Acc 0.833, AUC 0.9110846519470215, avg_entr 0.17721912264823914, f1 0.8330000042915344
ep4_l0_test_time 0.3250875473022461
Test Epoch4 layer1 Acc 0.8244, AUC 0.9010118842124939, avg_entr 0.15058554708957672, f1 0.8243999481201172
ep4_l1_test_time 0.5163707733154297
Test Epoch4 layer2 Acc 0.8202, AUC 0.8979385495185852, avg_entr 0.07469839602708817, f1 0.8202000260353088
ep4_l2_test_time 0.6826128959655762
Test Epoch4 layer3 Acc 0.819, AUC 0.8967911601066589, avg_entr 0.06179482489824295, f1 0.8190000057220459
ep4_l3_test_time 0.8547401428222656
Test Epoch4 layer4 Acc 0.819, AUC 0.8970665335655212, avg_entr 0.05957716703414917, f1 0.8190000057220459
ep4_l4_test_time 1.0149767398834229
gc 0
Train Epoch5 Acc 0.9416 (37664/40000), AUC 0.9832425713539124
ep5_train_time 31.430676698684692
Test Epoch5 layer0 Acc 0.8296, AUC 0.9059694409370422, avg_entr 0.16432629525661469, f1 0.8295999765396118
ep5_l0_test_time 0.32284021377563477
Test Epoch5 layer1 Acc 0.8176, AUC 0.8977380990982056, avg_entr 0.13613899052143097, f1 0.8176000118255615
ep5_l1_test_time 0.4987306594848633
Test Epoch5 layer2 Acc 0.8194, AUC 0.8949719071388245, avg_entr 0.06538961827754974, f1 0.8194000124931335
ep5_l2_test_time 0.6809086799621582
Test Epoch5 layer3 Acc 0.8192, AUC 0.8953779935836792, avg_entr 0.06024825945496559, f1 0.8191999197006226
ep5_l3_test_time 0.8573744297027588
Test Epoch5 layer4 Acc 0.8194, AUC 0.8958328366279602, avg_entr 0.0579729788005352, f1 0.8194000124931335
ep5_l4_test_time 1.0055851936340332
gc 0
Train Epoch6 Acc 0.94995 (37998/40000), AUC 0.9877622723579407
ep6_train_time 31.180731296539307
Test Epoch6 layer0 Acc 0.8208, AUC 0.8998476266860962, avg_entr 0.1546352356672287, f1 0.8208000063896179
ep6_l0_test_time 0.32580041885375977
Test Epoch6 layer1 Acc 0.8116, AUC 0.8920212984085083, avg_entr 0.12581618130207062, f1 0.8116000294685364
ep6_l1_test_time 0.5081782341003418
Test Epoch6 layer2 Acc 0.811, AUC 0.8894535899162292, avg_entr 0.059658702462911606, f1 0.8109999895095825
ep6_l2_test_time 0.6872701644897461
Test Epoch6 layer3 Acc 0.8114, AUC 0.8897877335548401, avg_entr 0.05149209499359131, f1 0.8113999962806702
ep6_l3_test_time 0.8840911388397217
Test Epoch6 layer4 Acc 0.8108, AUC 0.8902032375335693, avg_entr 0.0489623136818409, f1 0.8108000159263611
ep6_l4_test_time 1.045691728591919
gc 0
Train Epoch7 Acc 0.961425 (38457/40000), AUC 0.9919916391372681
ep7_train_time 31.432637691497803
Test Epoch7 layer0 Acc 0.8164, AUC 0.8969011306762695, avg_entr 0.1512196958065033, f1 0.8164000511169434
ep7_l0_test_time 0.31072473526000977
Test Epoch7 layer1 Acc 0.8092, AUC 0.8886693120002747, avg_entr 0.11545062065124512, f1 0.8092000484466553
ep7_l1_test_time 0.48715639114379883
Test Epoch7 layer2 Acc 0.8114, AUC 0.8829149603843689, avg_entr 0.052496373653411865, f1 0.8113999962806702
ep7_l2_test_time 0.6770517826080322
Test Epoch7 layer3 Acc 0.8116, AUC 0.8867260217666626, avg_entr 0.04671568423509598, f1 0.8116000294685364
ep7_l3_test_time 0.8721907138824463
Test Epoch7 layer4 Acc 0.8114, AUC 0.88828444480896, avg_entr 0.04427868127822876, f1 0.8113999962806702
ep7_l4_test_time 1.0053763389587402
gc 0
Train Epoch8 Acc 0.961825 (38473/40000), AUC 0.9920285940170288
ep8_train_time 31.493372201919556
Test Epoch8 layer0 Acc 0.816, AUC 0.8929358720779419, avg_entr 0.147135928273201, f1 0.8160000443458557
ep8_l0_test_time 0.28478407859802246
Test Epoch8 layer1 Acc 0.8046, AUC 0.8840635418891907, avg_entr 0.10685471445322037, f1 0.8046000599861145
ep8_l1_test_time 0.46846795082092285
Test Epoch8 layer2 Acc 0.8038, AUC 0.8779975175857544, avg_entr 0.047670044004917145, f1 0.8037999868392944
ep8_l2_test_time 0.6442501544952393
Test Epoch8 layer3 Acc 0.8042, AUC 0.882314145565033, avg_entr 0.043255627155303955, f1 0.8041999936103821
ep8_l3_test_time 0.797135591506958
Test Epoch8 layer4 Acc 0.803, AUC 0.8838354349136353, avg_entr 0.04104162007570267, f1 0.8029999732971191
ep8_l4_test_time 0.9932336807250977
gc 0
Train Epoch9 Acc 0.96665 (38666/40000), AUC 0.9929327964782715
ep9_train_time 31.265681266784668
Test Epoch9 layer0 Acc 0.813, AUC 0.8904871344566345, avg_entr 0.14273884892463684, f1 0.8130000233650208
ep9_l0_test_time 0.31572699546813965
Test Epoch9 layer1 Acc 0.7996, AUC 0.8815428018569946, avg_entr 0.09190192073583603, f1 0.7996000051498413
ep9_l1_test_time 0.5065734386444092
Test Epoch9 layer2 Acc 0.8014, AUC 0.8732866048812866, avg_entr 0.04158753901720047, f1 0.8014000058174133
ep9_l2_test_time 0.6853158473968506
Test Epoch9 layer3 Acc 0.801, AUC 0.8794437050819397, avg_entr 0.03724701702594757, f1 0.8010000586509705
ep9_l3_test_time 0.8376648426055908
Test Epoch9 layer4 Acc 0.801, AUC 0.8817052841186523, avg_entr 0.03555784374475479, f1 0.8010000586509705
ep9_l4_test_time 1.0441734790802002
gc 0
Train Epoch10 Acc 0.96855 (38742/40000), AUC 0.9941720962524414
ep10_train_time 31.47647762298584
Test Epoch10 layer0 Acc 0.8144, AUC 0.8874205350875854, avg_entr 0.13945388793945312, f1 0.8144000172615051
ep10_l0_test_time 0.31874966621398926
Test Epoch10 layer1 Acc 0.7964, AUC 0.8775510787963867, avg_entr 0.08712352812290192, f1 0.7964000105857849
ep10_l1_test_time 0.5101644992828369
Test Epoch10 layer2 Acc 0.799, AUC 0.8732439279556274, avg_entr 0.04140689596533775, f1 0.7990000247955322
ep10_l2_test_time 0.6859390735626221
Test Epoch10 layer3 Acc 0.799, AUC 0.8778166770935059, avg_entr 0.03762061893939972, f1 0.7990000247955322
ep10_l3_test_time 0.8726520538330078
Test Epoch10 layer4 Acc 0.7988, AUC 0.8792296648025513, avg_entr 0.03580692410469055, f1 0.798799991607666
ep10_l4_test_time 0.9947800636291504
gc 0
Train Epoch11 Acc 0.9716 (38864/40000), AUC 0.9952284097671509
ep11_train_time 31.22394299507141
Test Epoch11 layer0 Acc 0.815, AUC 0.8862844705581665, avg_entr 0.13689543306827545, f1 0.8149999976158142
ep11_l0_test_time 0.3139638900756836
Test Epoch11 layer1 Acc 0.795, AUC 0.8750045895576477, avg_entr 0.07596529275178909, f1 0.7950000166893005
ep11_l1_test_time 0.5116496086120605
Test Epoch11 layer2 Acc 0.798, AUC 0.8665937185287476, avg_entr 0.03733254224061966, f1 0.7979999780654907
ep11_l2_test_time 0.6831178665161133
Test Epoch11 layer3 Acc 0.798, AUC 0.8746845722198486, avg_entr 0.033528294414281845, f1 0.7979999780654907
ep11_l3_test_time 0.8764951229095459
Test Epoch11 layer4 Acc 0.7978, AUC 0.8768600225448608, avg_entr 0.03182961791753769, f1 0.7978000044822693
ep11_l4_test_time 1.024843454360962
gc 0
Train Epoch12 Acc 0.972525 (38901/40000), AUC 0.9952079057693481
ep12_train_time 31.424415349960327
Test Epoch12 layer0 Acc 0.8134, AUC 0.8842799663543701, avg_entr 0.135056734085083, f1 0.8133999109268188
ep12_l0_test_time 0.326824426651001
Test Epoch12 layer1 Acc 0.7952, AUC 0.8735435605049133, avg_entr 0.06941258907318115, f1 0.7952000498771667
ep12_l1_test_time 0.4837679862976074
Test Epoch12 layer2 Acc 0.7986, AUC 0.8617335557937622, avg_entr 0.034861285239458084, f1 0.7986000180244446
ep12_l2_test_time 0.6950030326843262
Test Epoch12 layer3 Acc 0.7988, AUC 0.8723330497741699, avg_entr 0.031242791563272476, f1 0.798799991607666
ep12_l3_test_time 0.8632063865661621
Test Epoch12 layer4 Acc 0.7988, AUC 0.8752687573432922, avg_entr 0.02946825511753559, f1 0.798799991607666
ep12_l4_test_time 1.0422518253326416
gc 0
Train Epoch13 Acc 0.974 (38960/40000), AUC 0.99516361951828
ep13_train_time 31.414209365844727
Test Epoch13 layer0 Acc 0.8114, AUC 0.883002758026123, avg_entr 0.13318794965744019, f1 0.8113999962806702
ep13_l0_test_time 0.31627535820007324
Test Epoch13 layer1 Acc 0.7954, AUC 0.8724421858787537, avg_entr 0.061272747814655304, f1 0.795400083065033
ep13_l1_test_time 0.5096917152404785
Test Epoch13 layer2 Acc 0.7978, AUC 0.861341655254364, avg_entr 0.03198441490530968, f1 0.7978000044822693
ep13_l2_test_time 0.6853516101837158
Test Epoch13 layer3 Acc 0.798, AUC 0.8718892931938171, avg_entr 0.028780780732631683, f1 0.7979999780654907
ep13_l3_test_time 0.8464033603668213
Test Epoch13 layer4 Acc 0.7984, AUC 0.874382734298706, avg_entr 0.026970302686095238, f1 0.7983999848365784
ep13_l4_test_time 1.042471170425415
gc 0
Train Epoch14 Acc 0.975125 (39005/40000), AUC 0.9958429336547852
ep14_train_time 31.24778127670288
Test Epoch14 layer0 Acc 0.8122, AUC 0.8819053173065186, avg_entr 0.1309431940317154, f1 0.8122000098228455
ep14_l0_test_time 0.32085752487182617
Test Epoch14 layer1 Acc 0.7936, AUC 0.8712893128395081, avg_entr 0.0589645616710186, f1 0.7936000227928162
ep14_l1_test_time 0.45789361000061035
Test Epoch14 layer2 Acc 0.7952, AUC 0.8610883951187134, avg_entr 0.031025756150484085, f1 0.7952000498771667
ep14_l2_test_time 0.6834707260131836
Test Epoch14 layer3 Acc 0.7958, AUC 0.8706635236740112, avg_entr 0.027799619361758232, f1 0.7958000302314758
ep14_l3_test_time 0.860938310623169
Test Epoch14 layer4 Acc 0.7956, AUC 0.873195230960846, avg_entr 0.02606140449643135, f1 0.7955999970436096
ep14_l4_test_time 1.0080945491790771
gc 0
Train Epoch15 Acc 0.976 (39040/40000), AUC 0.9960933923721313
ep15_train_time 31.345875024795532
Test Epoch15 layer0 Acc 0.8094, AUC 0.8806415796279907, avg_entr 0.12936808168888092, f1 0.8094000220298767
ep15_l0_test_time 0.3298804759979248
Test Epoch15 layer1 Acc 0.7926, AUC 0.8706159591674805, avg_entr 0.055379368364810944, f1 0.7925999760627747
ep15_l1_test_time 0.5121872425079346
Test Epoch15 layer2 Acc 0.7972, AUC 0.861424446105957, avg_entr 0.030376138165593147, f1 0.7971999645233154
ep15_l2_test_time 0.6822226047515869
Test Epoch15 layer3 Acc 0.7972, AUC 0.8699336647987366, avg_entr 0.027137627825140953, f1 0.7971999645233154
ep15_l3_test_time 0.8488328456878662
Test Epoch15 layer4 Acc 0.7974, AUC 0.8728625774383545, avg_entr 0.025244317948818207, f1 0.7973999977111816
ep15_l4_test_time 1.027123212814331
gc 0
Train Epoch16 Acc 0.976475 (39059/40000), AUC 0.9960898756980896
ep16_train_time 31.423637628555298
Test Epoch16 layer0 Acc 0.8106, AUC 0.8803341388702393, avg_entr 0.1291947066783905, f1 0.8105999231338501
ep16_l0_test_time 0.33280324935913086
Test Epoch16 layer1 Acc 0.7956, AUC 0.8687741756439209, avg_entr 0.05238298326730728, f1 0.7955999970436096
ep16_l1_test_time 0.49703407287597656
Test Epoch16 layer2 Acc 0.7944, AUC 0.863397479057312, avg_entr 0.030558917671442032, f1 0.7943999767303467
ep16_l2_test_time 0.6723027229309082
Test Epoch16 layer3 Acc 0.7942, AUC 0.8692301511764526, avg_entr 0.027395099401474, f1 0.7942000031471252
ep16_l3_test_time 0.8587601184844971
Test Epoch16 layer4 Acc 0.794, AUC 0.8714457750320435, avg_entr 0.025553112849593163, f1 0.7940000295639038
ep16_l4_test_time 1.031780481338501
gc 0
Train Epoch17 Acc 0.977125 (39085/40000), AUC 0.996039867401123
ep17_train_time 31.18976902961731
Test Epoch17 layer0 Acc 0.8108, AUC 0.8797706365585327, avg_entr 0.12770211696624756, f1 0.8108000159263611
ep17_l0_test_time 0.3488612174987793
Test Epoch17 layer1 Acc 0.7922, AUC 0.8685212135314941, avg_entr 0.04957842826843262, f1 0.7922000288963318
ep17_l1_test_time 0.48357319831848145
Test Epoch17 layer2 Acc 0.7948, AUC 0.8635708093643188, avg_entr 0.02834881842136383, f1 0.7947999835014343
ep17_l2_test_time 0.6860394477844238
Test Epoch17 layer3 Acc 0.7942, AUC 0.8690364360809326, avg_entr 0.025757156312465668, f1 0.7942000031471252
ep17_l3_test_time 0.8616769313812256
Test Epoch17 layer4 Acc 0.7946, AUC 0.8710215091705322, avg_entr 0.024262480437755585, f1 0.7946000695228577
ep17_l4_test_time 1.0244135856628418
gc 0
Train Epoch18 Acc 0.9775 (39100/40000), AUC 0.9963716864585876
ep18_train_time 31.43050789833069
Test Epoch18 layer0 Acc 0.8064, AUC 0.8790673017501831, avg_entr 0.1274498850107193, f1 0.8064000010490417
ep18_l0_test_time 0.32862377166748047
Test Epoch18 layer1 Acc 0.7948, AUC 0.8666436672210693, avg_entr 0.04926363378763199, f1 0.7947999835014343
ep18_l1_test_time 0.500098466873169
Test Epoch18 layer2 Acc 0.792, AUC 0.8632603883743286, avg_entr 0.02878757379949093, f1 0.7920000553131104
ep18_l2_test_time 0.6990053653717041
Test Epoch18 layer3 Acc 0.7918, AUC 0.8689887523651123, avg_entr 0.026315152645111084, f1 0.7918000221252441
ep18_l3_test_time 0.8255295753479004
Test Epoch18 layer4 Acc 0.792, AUC 0.8702603578567505, avg_entr 0.024769144132733345, f1 0.7920000553131104
ep18_l4_test_time 1.039806604385376
gc 0
Train Epoch19 Acc 0.978075 (39123/40000), AUC 0.9966012239456177
ep19_train_time 31.361154794692993
Test Epoch19 layer0 Acc 0.8068, AUC 0.8785637617111206, avg_entr 0.12645520269870758, f1 0.8068000078201294
ep19_l0_test_time 0.31894373893737793
Test Epoch19 layer1 Acc 0.7926, AUC 0.8663827776908875, avg_entr 0.04792870581150055, f1 0.7925999760627747
ep19_l1_test_time 0.4590873718261719
Test Epoch19 layer2 Acc 0.7938, AUC 0.8614166975021362, avg_entr 0.027574948966503143, f1 0.7937999963760376
ep19_l2_test_time 0.63057541847229
Test Epoch19 layer3 Acc 0.7932, AUC 0.867739737033844, avg_entr 0.025146521627902985, f1 0.7932000160217285
ep19_l3_test_time 0.8003437519073486
Test Epoch19 layer4 Acc 0.7942, AUC 0.8697032928466797, avg_entr 0.0236669834703207, f1 0.7942000031471252
ep19_l4_test_time 0.9819221496582031
gc 0
Train Epoch20 Acc 0.97855 (39142/40000), AUC 0.9966287016868591
ep20_train_time 31.344492435455322
Test Epoch20 layer0 Acc 0.8076, AUC 0.8782961368560791, avg_entr 0.12562736868858337, f1 0.8076000213623047
ep20_l0_test_time 0.3215067386627197
Test Epoch20 layer1 Acc 0.7914, AUC 0.8661352396011353, avg_entr 0.04602687805891037, f1 0.7914000153541565
ep20_l1_test_time 0.5092105865478516
Test Epoch20 layer2 Acc 0.794, AUC 0.8607728481292725, avg_entr 0.027459023520350456, f1 0.7940000295639038
ep20_l2_test_time 0.6836357116699219
Test Epoch20 layer3 Acc 0.7932, AUC 0.8670200109481812, avg_entr 0.02525397576391697, f1 0.7932000160217285
ep20_l3_test_time 0.8622674942016602
Test Epoch20 layer4 Acc 0.7934, AUC 0.8697238564491272, avg_entr 0.023667963221669197, f1 0.79339998960495
ep20_l4_test_time 1.0353848934173584
gc 0
Train Epoch21 Acc 0.978675 (39147/40000), AUC 0.9966229200363159
ep21_train_time 31.477725982666016
Test Epoch21 layer0 Acc 0.806, AUC 0.8778982162475586, avg_entr 0.12489452958106995, f1 0.8059999942779541
ep21_l0_test_time 0.323427677154541
Test Epoch21 layer1 Acc 0.794, AUC 0.8655418157577515, avg_entr 0.04403003677725792, f1 0.7940000295639038
ep21_l1_test_time 0.5009665489196777
Test Epoch21 layer2 Acc 0.7938, AUC 0.8590288162231445, avg_entr 0.02530520409345627, f1 0.7937999963760376
ep21_l2_test_time 0.6745624542236328
Test Epoch21 layer3 Acc 0.794, AUC 0.8665003776550293, avg_entr 0.023251455277204514, f1 0.7940000295639038
ep21_l3_test_time 0.8747200965881348
Test Epoch21 layer4 Acc 0.794, AUC 0.8692339658737183, avg_entr 0.021853938698768616, f1 0.7940000295639038
ep21_l4_test_time 1.0351805686950684
gc 0
Train Epoch22 Acc 0.978525 (39141/40000), AUC 0.9967765808105469
ep22_train_time 31.192137718200684
Test Epoch22 layer0 Acc 0.8058, AUC 0.877522349357605, avg_entr 0.12357936799526215, f1 0.8058000206947327
ep22_l0_test_time 0.3077423572540283
Test Epoch22 layer1 Acc 0.7928, AUC 0.8656624555587769, avg_entr 0.04325517639517784, f1 0.7928000092506409
ep22_l1_test_time 0.4929227828979492
Test Epoch22 layer2 Acc 0.7938, AUC 0.8580260872840881, avg_entr 0.024964531883597374, f1 0.7937999963760376
ep22_l2_test_time 0.691809892654419
Test Epoch22 layer3 Acc 0.7938, AUC 0.8655151724815369, avg_entr 0.022668365389108658, f1 0.7937999963760376
ep22_l3_test_time 0.8651931285858154
Test Epoch22 layer4 Acc 0.7938, AUC 0.8691442608833313, avg_entr 0.0212264321744442, f1 0.7937999963760376
ep22_l4_test_time 1.0418663024902344
gc 0
Train Epoch23 Acc 0.979125 (39165/40000), AUC 0.9968208074569702
ep23_train_time 31.45330309867859
Test Epoch23 layer0 Acc 0.8048, AUC 0.8773820400238037, avg_entr 0.12346306443214417, f1 0.8047999739646912
ep23_l0_test_time 0.32549452781677246
Test Epoch23 layer1 Acc 0.7932, AUC 0.8650137186050415, avg_entr 0.04198920726776123, f1 0.7932000160217285
ep23_l1_test_time 0.5061099529266357
Test Epoch23 layer2 Acc 0.7934, AUC 0.8582475781440735, avg_entr 0.02495860680937767, f1 0.79339998960495
ep23_l2_test_time 0.6830089092254639
Test Epoch23 layer3 Acc 0.794, AUC 0.8653055429458618, avg_entr 0.022822672501206398, f1 0.7940000295639038
ep23_l3_test_time 0.824500560760498
Test Epoch23 layer4 Acc 0.7938, AUC 0.8686508536338806, avg_entr 0.021475033834576607, f1 0.7937999963760376
ep23_l4_test_time 1.0356035232543945
gc 0
Train Epoch24 Acc 0.9794 (39176/40000), AUC 0.9968822002410889
ep24_train_time 31.42948269844055
Test Epoch24 layer0 Acc 0.8048, AUC 0.8772555589675903, avg_entr 0.12288608402013779, f1 0.8047999739646912
ep24_l0_test_time 0.33385515213012695
Test Epoch24 layer1 Acc 0.7934, AUC 0.8649061322212219, avg_entr 0.04121456295251846, f1 0.79339998960495
ep24_l1_test_time 0.5042798519134521
Test Epoch24 layer2 Acc 0.7936, AUC 0.8585739135742188, avg_entr 0.02446489781141281, f1 0.7936000227928162
ep24_l2_test_time 0.6573173999786377
Test Epoch24 layer3 Acc 0.794, AUC 0.8654868006706238, avg_entr 0.02224288322031498, f1 0.7940000295639038
ep24_l3_test_time 0.8610584735870361
Test Epoch24 layer4 Acc 0.7942, AUC 0.8685498237609863, avg_entr 0.020910412073135376, f1 0.7942000031471252
ep24_l4_test_time 1.040518045425415
gc 0
Train Epoch25 Acc 0.979075 (39163/40000), AUC 0.9968556761741638
ep25_train_time 31.22316813468933
Test Epoch25 layer0 Acc 0.8056, AUC 0.877159059047699, avg_entr 0.12233252078294754, f1 0.8055999875068665
ep25_l0_test_time 0.32869768142700195
Test Epoch25 layer1 Acc 0.7926, AUC 0.8650112748146057, avg_entr 0.04109016805887222, f1 0.7925999760627747
ep25_l1_test_time 0.5021162033081055
Test Epoch25 layer2 Acc 0.793, AUC 0.8560582399368286, avg_entr 0.02320869080722332, f1 0.7929999828338623
ep25_l2_test_time 0.6863954067230225
Test Epoch25 layer3 Acc 0.7932, AUC 0.8641520738601685, avg_entr 0.021031493321061134, f1 0.7932000160217285
ep25_l3_test_time 0.8445510864257812
Test Epoch25 layer4 Acc 0.7932, AUC 0.8685023188591003, avg_entr 0.01966678723692894, f1 0.7932000160217285
ep25_l4_test_time 1.0398857593536377
gc 0
Train Epoch26 Acc 0.979 (39160/40000), AUC 0.9969427585601807
ep26_train_time 31.36377191543579
Test Epoch26 layer0 Acc 0.8054, AUC 0.87700355052948, avg_entr 0.12173672020435333, f1 0.805400013923645
ep26_l0_test_time 0.33124399185180664
Test Epoch26 layer1 Acc 0.7922, AUC 0.8646770715713501, avg_entr 0.040232911705970764, f1 0.7922000288963318
ep26_l1_test_time 0.510657787322998
Test Epoch26 layer2 Acc 0.7928, AUC 0.8573782444000244, avg_entr 0.023097649216651917, f1 0.7928000092506409
ep26_l2_test_time 0.6875448226928711
Test Epoch26 layer3 Acc 0.7934, AUC 0.8647419810295105, avg_entr 0.0210286732763052, f1 0.79339998960495
ep26_l3_test_time 0.8702340126037598
Test Epoch26 layer4 Acc 0.7936, AUC 0.8682229518890381, avg_entr 0.019709156826138496, f1 0.7936000227928162
ep26_l4_test_time 1.0248847007751465
gc 0
Train Epoch27 Acc 0.979375 (39175/40000), AUC 0.9970347881317139
ep27_train_time 31.416802167892456
Test Epoch27 layer0 Acc 0.8042, AUC 0.8769575953483582, avg_entr 0.12158279865980148, f1 0.8041999936103821
ep27_l0_test_time 0.3005959987640381
Test Epoch27 layer1 Acc 0.793, AUC 0.8644770383834839, avg_entr 0.039855923503637314, f1 0.7929999828338623
ep27_l1_test_time 0.5156834125518799
Test Epoch27 layer2 Acc 0.7934, AUC 0.8573816418647766, avg_entr 0.023022418841719627, f1 0.79339998960495
ep27_l2_test_time 0.704749345779419
Test Epoch27 layer3 Acc 0.7938, AUC 0.8647993803024292, avg_entr 0.021030424162745476, f1 0.7937999963760376
ep27_l3_test_time 0.8528873920440674
Test Epoch27 layer4 Acc 0.7936, AUC 0.8681623935699463, avg_entr 0.019703969359397888, f1 0.7936000227928162
ep27_l4_test_time 1.0280916690826416
gc 0
Train Epoch28 Acc 0.979225 (39169/40000), AUC 0.9969700574874878
ep28_train_time 31.244324207305908
Test Epoch28 layer0 Acc 0.8038, AUC 0.8768606185913086, avg_entr 0.12117297202348709, f1 0.8037999868392944
ep28_l0_test_time 0.3242661952972412
Test Epoch28 layer1 Acc 0.7932, AUC 0.8643290996551514, avg_entr 0.03934852033853531, f1 0.7932000160217285
ep28_l1_test_time 0.4676988124847412
Test Epoch28 layer2 Acc 0.7938, AUC 0.8582131266593933, avg_entr 0.023426752537488937, f1 0.7937999963760376
ep28_l2_test_time 0.6831681728363037
Test Epoch28 layer3 Acc 0.7932, AUC 0.8650962114334106, avg_entr 0.021414566785097122, f1 0.7932000160217285
ep28_l3_test_time 0.8457701206207275
Test Epoch28 layer4 Acc 0.7934, AUC 0.8682509064674377, avg_entr 0.020079324021935463, f1 0.79339998960495
ep28_l4_test_time 1.017782211303711
gc 0
Train Epoch29 Acc 0.97995 (39198/40000), AUC 0.9971047639846802
ep29_train_time 31.43523383140564
Test Epoch29 layer0 Acc 0.8036, AUC 0.8768307566642761, avg_entr 0.12085173279047012, f1 0.803600013256073
ep29_l0_test_time 0.33037686347961426
Test Epoch29 layer1 Acc 0.7934, AUC 0.8643049597740173, avg_entr 0.03896758332848549, f1 0.79339998960495
ep29_l1_test_time 0.4984273910522461
Test Epoch29 layer2 Acc 0.7934, AUC 0.8576578497886658, avg_entr 0.023278383538126945, f1 0.79339998960495
ep29_l2_test_time 0.6646394729614258
Test Epoch29 layer3 Acc 0.793, AUC 0.8647460341453552, avg_entr 0.02123757265508175, f1 0.7929999828338623
ep29_l3_test_time 0.86092209815979
Test Epoch29 layer4 Acc 0.793, AUC 0.868223249912262, avg_entr 0.01990482769906521, f1 0.7929999828338623
ep29_l4_test_time 1.0415828227996826
Best AUC tensor(0.8460)
train_loss (2, 5, 30)
valid_acc (5, 30)
valid_AUC (5, 30)
train_acc (30,)
total_train+valid_time 1044.995462179184
Start Testing
Load ckpt at ckpt/imdb_transformeral_l5_pad100//imdb_transformeral_l5.pt
Test layer0 Acc 0.837, AUC 0.9192614555358887, avg_entr 0.2677645981311798, f1 0.8370000123977661
l0_test_time 0.319446325302124
Test layer1 Acc 0.839, AUC 0.9189843535423279, avg_entr 0.23576179146766663, f1 0.8389999866485596
l1_test_time 0.49040961265563965
Test layer2 Acc 0.844, AUC 0.920878529548645, avg_entr 0.20920822024345398, f1 0.8439999222755432
l2_test_time 0.684823751449585
Test layer3 Acc 0.8402, AUC 0.9198462963104248, avg_entr 0.18363980948925018, f1 0.8402000069618225
l3_test_time 0.8732509613037109
Test layer4 Acc 0.842, AUC 0.9192931652069092, avg_entr 0.18048936128616333, f1 0.8420000076293945
l4_test_time 1.0365369319915771
