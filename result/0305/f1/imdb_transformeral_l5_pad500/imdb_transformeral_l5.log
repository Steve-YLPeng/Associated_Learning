total count words 222751
vocab size 30000
train size 40000, valid size 5000, test size 5000
found 27937 words in glove
model: TransformerModelML(
  (layers): ModuleList(
    (0): EMBLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Embedding(30000, 300)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=2, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=2, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.weight 9000000
layers.0.ae.g.0.weight 256
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 256
layers.0.ae.h.0.bias 2
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
Total Trainable Params: 13671778
init_time 21.776434421539307
Start Training
gc 0
Train Epoch0 Acc 0.517325 (20693/40000), AUC 0.5334275960922241
ep0_train_time 182.33422899246216
Test Epoch0 layer0 Acc 0.8062, AUC 0.8922382593154907, avg_entr 0.5644218921661377, f1 0.8062000274658203
ep0_l0_test_time 0.7911579608917236
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 0
Test Epoch0 layer1 Acc 0.8352, AUC 0.9125893115997314, avg_entr 0.3379102349281311, f1 0.8352000117301941
ep0_l1_test_time 2.039691209793091
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 0
Test Epoch0 layer2 Acc 0.8294, AUC 0.9139283895492554, avg_entr 0.41866376996040344, f1 0.8294000029563904
ep0_l2_test_time 3.199172258377075
Test Epoch0 layer3 Acc 0.8184, AUC 0.9132459163665771, avg_entr 0.5280793905258179, f1 0.8184000253677368
ep0_l3_test_time 4.338923692703247
Test Epoch0 layer4 Acc 0.793, AUC 0.9115867614746094, avg_entr 0.6283149719238281, f1 0.7929999828338623
ep0_l4_test_time 5.719938516616821
gc 0
Train Epoch1 Acc 0.8703 (34812/40000), AUC 0.9372228384017944
ep1_train_time 180.99643993377686
Test Epoch1 layer0 Acc 0.8802, AUC 0.9459841251373291, avg_entr 0.2916264533996582, f1 0.8802000284194946
ep1_l0_test_time 0.7790489196777344
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 1
Test Epoch1 layer1 Acc 0.8896, AUC 0.9554853439331055, avg_entr 0.1772342026233673, f1 0.8895999193191528
ep1_l1_test_time 2.044311285018921
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 1
Test Epoch1 layer2 Acc 0.8898, AUC 0.9563573598861694, avg_entr 0.15596172213554382, f1 0.8898000121116638
ep1_l2_test_time 3.246051073074341
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 1
Test Epoch1 layer3 Acc 0.8876, AUC 0.9561947584152222, avg_entr 0.12767693400382996, f1 0.8876000046730042
ep1_l3_test_time 4.522967338562012
Test Epoch1 layer4 Acc 0.8868, AUC 0.9563062191009521, avg_entr 0.11871225386857986, f1 0.8867999911308289
ep1_l4_test_time 5.740855932235718
gc 0
Train Epoch2 Acc 0.9194 (36776/40000), AUC 0.9714668393135071
ep2_train_time 162.770991563797
Test Epoch2 layer0 Acc 0.8924, AUC 0.9557654857635498, avg_entr 0.2284330278635025, f1 0.8923999667167664
ep2_l0_test_time 0.7653436660766602
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 2
Test Epoch2 layer1 Acc 0.8992, AUC 0.9592013955116272, avg_entr 0.11697603762149811, f1 0.8992000222206116
ep2_l1_test_time 2.0488710403442383
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 2
Test Epoch2 layer2 Acc 0.8984, AUC 0.9582946300506592, avg_entr 0.07657086849212646, f1 0.8984000086784363
ep2_l2_test_time 3.2429890632629395
Test Epoch2 layer3 Acc 0.8976, AUC 0.9588555097579956, avg_entr 0.05852053314447403, f1 0.897599995136261
ep2_l3_test_time 4.446253299713135
Test Epoch2 layer4 Acc 0.8978, AUC 0.9590733051300049, avg_entr 0.05410615727305412, f1 0.8978000283241272
ep2_l4_test_time 5.788120985031128
gc 0
Train Epoch3 Acc 0.942425 (37697/40000), AUC 0.981531023979187
ep3_train_time 180.85289525985718
Test Epoch3 layer0 Acc 0.8806, AUC 0.9581077098846436, avg_entr 0.1918584704399109, f1 0.8805999755859375
ep3_l0_test_time 0.7833633422851562
Test Epoch3 layer1 Acc 0.8972, AUC 0.9577626585960388, avg_entr 0.06230529770255089, f1 0.8971999883651733
ep3_l1_test_time 2.0440030097961426
Test Epoch3 layer2 Acc 0.897, AUC 0.9577211141586304, avg_entr 0.04535572975873947, f1 0.8970000147819519
ep3_l2_test_time 3.2503879070281982
Test Epoch3 layer3 Acc 0.8968, AUC 0.9578815698623657, avg_entr 0.04039086773991585, f1 0.8967999815940857
ep3_l3_test_time 4.497544050216675
Test Epoch3 layer4 Acc 0.8972, AUC 0.9574592113494873, avg_entr 0.038570042699575424, f1 0.8971999883651733
ep3_l4_test_time 5.75416898727417
gc 0
Train Epoch4 Acc 0.952 (38080/40000), AUC 0.9847268462181091
ep4_train_time 180.6215751171112
Test Epoch4 layer0 Acc 0.8986, AUC 0.9591935873031616, avg_entr 0.17507779598236084, f1 0.8985999822616577
ep4_l0_test_time 0.7856571674346924
Test Epoch4 layer1 Acc 0.8926, AUC 0.9531022310256958, avg_entr 0.04471863433718681, f1 0.8925999999046326
ep4_l1_test_time 2.0267112255096436
Test Epoch4 layer2 Acc 0.8924, AUC 0.9554965496063232, avg_entr 0.03409557044506073, f1 0.8923999667167664
ep4_l2_test_time 3.2513654232025146
Test Epoch4 layer3 Acc 0.8912, AUC 0.9558292031288147, avg_entr 0.030577437952160835, f1 0.8912000060081482
ep4_l3_test_time 4.485792636871338
Test Epoch4 layer4 Acc 0.892, AUC 0.9553323984146118, avg_entr 0.02914067916572094, f1 0.8920000195503235
ep4_l4_test_time 5.7856996059417725
gc 0
Train Epoch5 Acc 0.959625 (38385/40000), AUC 0.9877879619598389
ep5_train_time 180.88869404792786
Test Epoch5 layer0 Acc 0.8982, AUC 0.9586583971977234, avg_entr 0.16182957589626312, f1 0.8981999754905701
ep5_l0_test_time 0.7694203853607178
Test Epoch5 layer1 Acc 0.8886, AUC 0.9487196207046509, avg_entr 0.04071592167019844, f1 0.8885999917984009
ep5_l1_test_time 2.023827075958252
Test Epoch5 layer2 Acc 0.8886, AUC 0.952776312828064, avg_entr 0.03220439702272415, f1 0.8885999917984009
ep5_l2_test_time 3.2402396202087402
Test Epoch5 layer3 Acc 0.8884, AUC 0.9532955884933472, avg_entr 0.02960588037967682, f1 0.8884000182151794
ep5_l3_test_time 4.531619548797607
Test Epoch5 layer4 Acc 0.8882, AUC 0.9528228640556335, avg_entr 0.028327057138085365, f1 0.888200044631958
ep5_l4_test_time 5.716501235961914
gc 0
Train Epoch6 Acc 0.963325 (38533/40000), AUC 0.9897050857543945
ep6_train_time 180.9517741203308
Test Epoch6 layer0 Acc 0.8968, AUC 0.9573859572410583, avg_entr 0.15609461069107056, f1 0.8967999815940857
ep6_l0_test_time 0.7873806953430176
Test Epoch6 layer1 Acc 0.8888, AUC 0.9449825286865234, avg_entr 0.03282570466399193, f1 0.8888000249862671
ep6_l1_test_time 1.9871766567230225
Test Epoch6 layer2 Acc 0.8876, AUC 0.9503334760665894, avg_entr 0.024731101468205452, f1 0.8876000046730042
ep6_l2_test_time 3.235677480697632
Test Epoch6 layer3 Acc 0.8874, AUC 0.9508888721466064, avg_entr 0.021911263465881348, f1 0.8873999714851379
ep6_l3_test_time 4.5363805294036865
Test Epoch6 layer4 Acc 0.8878, AUC 0.9503419399261475, avg_entr 0.020823178812861443, f1 0.8877999782562256
ep6_l4_test_time 5.7846314907073975
gc 0
Train Epoch7 Acc 0.9666 (38664/40000), AUC 0.9914869666099548
ep7_train_time 181.17442059516907
Test Epoch7 layer0 Acc 0.8946, AUC 0.9554206132888794, avg_entr 0.14696651697158813, f1 0.894599974155426
ep7_l0_test_time 0.782341480255127
Test Epoch7 layer1 Acc 0.8828, AUC 0.9425133466720581, avg_entr 0.03304949775338173, f1 0.8827999830245972
ep7_l1_test_time 2.065645694732666
Test Epoch7 layer2 Acc 0.8818, AUC 0.947510302066803, avg_entr 0.026129700243473053, f1 0.8818000555038452
ep7_l2_test_time 3.1741533279418945
Test Epoch7 layer3 Acc 0.882, AUC 0.9484461545944214, avg_entr 0.023837385699152946, f1 0.8820000290870667
ep7_l3_test_time 4.326426029205322
Test Epoch7 layer4 Acc 0.8818, AUC 0.947860836982727, avg_entr 0.022925561293959618, f1 0.8818000555038452
ep7_l4_test_time 5.699753522872925
gc 0
Train Epoch8 Acc 0.96985 (38794/40000), AUC 0.9922341704368591
ep8_train_time 181.23697519302368
Test Epoch8 layer0 Acc 0.8936, AUC 0.9540889263153076, avg_entr 0.13801930844783783, f1 0.8935999870300293
ep8_l0_test_time 0.7712085247039795
Test Epoch8 layer1 Acc 0.8774, AUC 0.9394190311431885, avg_entr 0.032195307314395905, f1 0.8773999810218811
ep8_l1_test_time 2.035616874694824
Test Epoch8 layer2 Acc 0.8772, AUC 0.9453712701797485, avg_entr 0.024645986035466194, f1 0.8772000074386597
ep8_l2_test_time 3.21178936958313
Test Epoch8 layer3 Acc 0.8772, AUC 0.9472429752349854, avg_entr 0.0220012366771698, f1 0.8772000074386597
ep8_l3_test_time 4.553083896636963
Test Epoch8 layer4 Acc 0.8772, AUC 0.9468377828598022, avg_entr 0.021033380180597305, f1 0.8772000074386597
ep8_l4_test_time 5.637940406799316
gc 0
Train Epoch9 Acc 0.97355 (38942/40000), AUC 0.9928511381149292
ep9_train_time 180.9762363433838
Test Epoch9 layer0 Acc 0.8908, AUC 0.9531539678573608, avg_entr 0.13505485653877258, f1 0.8907999992370605
ep9_l0_test_time 0.7641286849975586
Test Epoch9 layer1 Acc 0.8782, AUC 0.934577465057373, avg_entr 0.02916117198765278, f1 0.8781999945640564
ep9_l1_test_time 2.0590438842773438
Test Epoch9 layer2 Acc 0.8782, AUC 0.9429792761802673, avg_entr 0.022118015214800835, f1 0.8781999945640564
ep9_l2_test_time 3.198169231414795
Test Epoch9 layer3 Acc 0.8782, AUC 0.9454284310340881, avg_entr 0.019579336047172546, f1 0.8781999945640564
ep9_l3_test_time 4.521058082580566
Test Epoch9 layer4 Acc 0.8788, AUC 0.9452517032623291, avg_entr 0.01881180703639984, f1 0.8787999749183655
ep9_l4_test_time 5.7313268184661865
gc 0
Train Epoch10 Acc 0.9746 (38984/40000), AUC 0.993765115737915
ep10_train_time 180.6971435546875
Test Epoch10 layer0 Acc 0.8906, AUC 0.9517194628715515, avg_entr 0.13146726787090302, f1 0.8906000256538391
ep10_l0_test_time 0.7706663608551025
Test Epoch10 layer1 Acc 0.8736, AUC 0.9325357675552368, avg_entr 0.029937991872429848, f1 0.8736000061035156
ep10_l1_test_time 2.02034068107605
Test Epoch10 layer2 Acc 0.8728, AUC 0.9415987730026245, avg_entr 0.023396259173750877, f1 0.8727999925613403
ep10_l2_test_time 3.2404637336730957
Test Epoch10 layer3 Acc 0.8728, AUC 0.9437204599380493, avg_entr 0.021541329100728035, f1 0.8727999925613403
ep10_l3_test_time 4.511741638183594
Test Epoch10 layer4 Acc 0.8726, AUC 0.9434839487075806, avg_entr 0.02080613747239113, f1 0.8726000189781189
ep10_l4_test_time 5.710782289505005
gc 0
Train Epoch11 Acc 0.976075 (39043/40000), AUC 0.9943714141845703
ep11_train_time 180.78902006149292
Test Epoch11 layer0 Acc 0.8896, AUC 0.9510716795921326, avg_entr 0.12877680361270905, f1 0.8895999193191528
ep11_l0_test_time 0.769538164138794
Test Epoch11 layer1 Acc 0.8724, AUC 0.927885890007019, avg_entr 0.02624785155057907, f1 0.8723999857902527
ep11_l1_test_time 2.0350704193115234
Test Epoch11 layer2 Acc 0.8728, AUC 0.9371997714042664, avg_entr 0.020035067573189735, f1 0.8727999925613403
ep11_l2_test_time 3.234905481338501
Test Epoch11 layer3 Acc 0.8722, AUC 0.942447304725647, avg_entr 0.017670491710305214, f1 0.872200071811676
ep11_l3_test_time 4.556771755218506
Test Epoch11 layer4 Acc 0.8726, AUC 0.9427587389945984, avg_entr 0.017032349482178688, f1 0.8726000189781189
ep11_l4_test_time 5.755476951599121
gc 0
Train Epoch12 Acc 0.976575 (39063/40000), AUC 0.994459867477417
ep12_train_time 180.8948473930359
Test Epoch12 layer0 Acc 0.885, AUC 0.9498867988586426, avg_entr 0.1282573789358139, f1 0.8849999904632568
ep12_l0_test_time 0.7770748138427734
Test Epoch12 layer1 Acc 0.874, AUC 0.9272432327270508, avg_entr 0.026990460231900215, f1 0.8740000128746033
ep12_l1_test_time 2.046830177307129
Test Epoch12 layer2 Acc 0.8736, AUC 0.9366803169250488, avg_entr 0.02038385719060898, f1 0.8736000061035156
ep12_l2_test_time 3.232868194580078
Test Epoch12 layer3 Acc 0.873, AUC 0.9414808750152588, avg_entr 0.01822475530207157, f1 0.8730000257492065
ep12_l3_test_time 4.537041187286377
Test Epoch12 layer4 Acc 0.8736, AUC 0.9418870210647583, avg_entr 0.017519574612379074, f1 0.8736000061035156
ep12_l4_test_time 5.766297340393066
gc 0
Train Epoch13 Acc 0.9783 (39132/40000), AUC 0.9949173331260681
ep13_train_time 181.00622034072876
Test Epoch13 layer0 Acc 0.8862, AUC 0.9490333795547485, avg_entr 0.12460041791200638, f1 0.8862000107765198
ep13_l0_test_time 0.7851245403289795
Test Epoch13 layer1 Acc 0.874, AUC 0.9269606471061707, avg_entr 0.026405664160847664, f1 0.8740000128746033
ep13_l1_test_time 2.0309393405914307
Test Epoch13 layer2 Acc 0.8726, AUC 0.9349751472473145, avg_entr 0.019993918016552925, f1 0.8726000189781189
ep13_l2_test_time 3.2327980995178223
Test Epoch13 layer3 Acc 0.8724, AUC 0.940485417842865, avg_entr 0.01823018118739128, f1 0.8723999857902527
ep13_l3_test_time 4.502064943313599
Test Epoch13 layer4 Acc 0.8728, AUC 0.9411129951477051, avg_entr 0.017651064321398735, f1 0.8727999925613403
ep13_l4_test_time 5.781626224517822
gc 0
Train Epoch14 Acc 0.9789 (39156/40000), AUC 0.9951591491699219
ep14_train_time 180.89934086799622
Test Epoch14 layer0 Acc 0.8846, AUC 0.9486834406852722, avg_entr 0.12355343997478485, f1 0.8845999836921692
ep14_l0_test_time 0.7719752788543701
Test Epoch14 layer1 Acc 0.869, AUC 0.922781229019165, avg_entr 0.023335760459303856, f1 0.8690000176429749
ep14_l1_test_time 2.013205051422119
Test Epoch14 layer2 Acc 0.8688, AUC 0.9295833110809326, avg_entr 0.017021628096699715, f1 0.8687999844551086
ep14_l2_test_time 3.2354040145874023
Test Epoch14 layer3 Acc 0.869, AUC 0.9378567934036255, avg_entr 0.015024514868855476, f1 0.8690000176429749
ep14_l3_test_time 4.513282299041748
Test Epoch14 layer4 Acc 0.8698, AUC 0.9394045472145081, avg_entr 0.014467655681073666, f1 0.8697999715805054
ep14_l4_test_time 5.767270088195801
gc 0
Train Epoch15 Acc 0.97935 (39174/40000), AUC 0.9951900839805603
ep15_train_time 180.61886429786682
Test Epoch15 layer0 Acc 0.884, AUC 0.9480448961257935, avg_entr 0.1227831169962883, f1 0.8840000033378601
ep15_l0_test_time 0.7803568840026855
Test Epoch15 layer1 Acc 0.872, AUC 0.9219919443130493, avg_entr 0.026033205911517143, f1 0.871999979019165
ep15_l1_test_time 2.0348000526428223
Test Epoch15 layer2 Acc 0.8696, AUC 0.9314332604408264, avg_entr 0.019710879772901535, f1 0.8695999383926392
ep15_l2_test_time 3.2428033351898193
Test Epoch15 layer3 Acc 0.869, AUC 0.9382700324058533, avg_entr 0.017688974738121033, f1 0.8690000176429749
ep15_l3_test_time 4.530190706253052
Test Epoch15 layer4 Acc 0.8688, AUC 0.9395683407783508, avg_entr 0.017170049250125885, f1 0.8687999844551086
ep15_l4_test_time 5.817937135696411
gc 0
Train Epoch16 Acc 0.980025 (39201/40000), AUC 0.9953881502151489
ep16_train_time 180.67089414596558
Test Epoch16 layer0 Acc 0.8808, AUC 0.9475760459899902, avg_entr 0.12388723343610764, f1 0.8808000087738037
ep16_l0_test_time 0.7722766399383545
Test Epoch16 layer1 Acc 0.8704, AUC 0.919201135635376, avg_entr 0.022992225363850594, f1 0.8704000115394592
ep16_l1_test_time 2.0310850143432617
Test Epoch16 layer2 Acc 0.8688, AUC 0.9271426200866699, avg_entr 0.016944922506809235, f1 0.8687999844551086
ep16_l2_test_time 3.2486395835876465
Test Epoch16 layer3 Acc 0.8692, AUC 0.9359977841377258, avg_entr 0.015164166688919067, f1 0.8691999912261963
ep16_l3_test_time 4.544410943984985
Test Epoch16 layer4 Acc 0.8692, AUC 0.9380955696105957, avg_entr 0.014567773789167404, f1 0.8691999912261963
ep16_l4_test_time 5.807200908660889
gc 0
Train Epoch17 Acc 0.9805 (39220/40000), AUC 0.9959708452224731
ep17_train_time 180.8976001739502
Test Epoch17 layer0 Acc 0.8826, AUC 0.9471635222434998, avg_entr 0.12010366469621658, f1 0.8826000094413757
ep17_l0_test_time 0.7589406967163086
Test Epoch17 layer1 Acc 0.87, AUC 0.9180890321731567, avg_entr 0.024723706766963005, f1 0.8700000047683716
ep17_l1_test_time 2.008728504180908
Test Epoch17 layer2 Acc 0.8694, AUC 0.9276600480079651, avg_entr 0.018616579473018646, f1 0.8694000244140625
ep17_l2_test_time 3.2449333667755127
Test Epoch17 layer3 Acc 0.8694, AUC 0.9358237385749817, avg_entr 0.016850154846906662, f1 0.8694000244140625
ep17_l3_test_time 4.514689922332764
Test Epoch17 layer4 Acc 0.8694, AUC 0.9379110932350159, avg_entr 0.016430741176009178, f1 0.8694000244140625
ep17_l4_test_time 5.730317831039429
gc 0
Train Epoch18 Acc 0.981175 (39247/40000), AUC 0.9957267045974731
ep18_train_time 181.0300052165985
Test Epoch18 layer0 Acc 0.882, AUC 0.9468919634819031, avg_entr 0.12006595730781555, f1 0.8820000290870667
ep18_l0_test_time 0.7715198993682861
Test Epoch18 layer1 Acc 0.868, AUC 0.9167268872261047, avg_entr 0.022404305636882782, f1 0.8679999709129333
ep18_l1_test_time 2.0494513511657715
Test Epoch18 layer2 Acc 0.8664, AUC 0.9236830472946167, avg_entr 0.016154326498508453, f1 0.8664000034332275
ep18_l2_test_time 3.1975114345550537
Test Epoch18 layer3 Acc 0.8674, AUC 0.9336512088775635, avg_entr 0.014393779449164867, f1 0.8673999905586243
ep18_l3_test_time 4.317251682281494
Test Epoch18 layer4 Acc 0.8682, AUC 0.936672031879425, avg_entr 0.013910716399550438, f1 0.8682000041007996
ep18_l4_test_time 5.689834117889404
gc 0
Train Epoch19 Acc 0.981175 (39247/40000), AUC 0.9959143400192261
ep19_train_time 181.132422208786
Test Epoch19 layer0 Acc 0.8816, AUC 0.946521520614624, avg_entr 0.11828233301639557, f1 0.881600022315979
ep19_l0_test_time 0.7671608924865723
Test Epoch19 layer1 Acc 0.8672, AUC 0.9170184135437012, avg_entr 0.021829016506671906, f1 0.8672000169754028
ep19_l1_test_time 2.0462546348571777
Test Epoch19 layer2 Acc 0.8672, AUC 0.9231748580932617, avg_entr 0.015516869723796844, f1 0.8672000169754028
ep19_l2_test_time 3.2339651584625244
Test Epoch19 layer3 Acc 0.8674, AUC 0.9332822561264038, avg_entr 0.013840369880199432, f1 0.8673999905586243
ep19_l3_test_time 4.544417142868042
Test Epoch19 layer4 Acc 0.8674, AUC 0.9364688396453857, avg_entr 0.013483511283993721, f1 0.8673999905586243
ep19_l4_test_time 5.732104539871216
gc 0
Train Epoch20 Acc 0.9812 (39248/40000), AUC 0.9961014986038208
ep20_train_time 180.95620226860046
Test Epoch20 layer0 Acc 0.8818, AUC 0.9462118744850159, avg_entr 0.11738187074661255, f1 0.8818000555038452
ep20_l0_test_time 0.7658185958862305
Test Epoch20 layer1 Acc 0.87, AUC 0.9161525368690491, avg_entr 0.023027649149298668, f1 0.8700000047683716
ep20_l1_test_time 2.047819137573242
Test Epoch20 layer2 Acc 0.8686, AUC 0.9246904850006104, avg_entr 0.017421230673789978, f1 0.8686000108718872
ep20_l2_test_time 3.2420570850372314
Test Epoch20 layer3 Acc 0.8688, AUC 0.9336620569229126, avg_entr 0.015563197433948517, f1 0.8687999844551086
ep20_l3_test_time 4.549584865570068
Test Epoch20 layer4 Acc 0.8686, AUC 0.9365691542625427, avg_entr 0.015206246636807919, f1 0.8686000108718872
ep20_l4_test_time 5.749093294143677
gc 0
Train Epoch21 Acc 0.9816 (39264/40000), AUC 0.9961985349655151
ep21_train_time 180.74923181533813
Test Epoch21 layer0 Acc 0.8806, AUC 0.9461156129837036, avg_entr 0.11762809008359909, f1 0.8805999755859375
ep21_l0_test_time 0.7728786468505859
Test Epoch21 layer1 Acc 0.868, AUC 0.9158620834350586, avg_entr 0.020749425515532494, f1 0.8679999709129333
ep21_l1_test_time 2.0263783931732178
Test Epoch21 layer2 Acc 0.867, AUC 0.9200592041015625, avg_entr 0.014521890319883823, f1 0.8669999837875366
ep21_l2_test_time 3.2401363849639893
Test Epoch21 layer3 Acc 0.8674, AUC 0.9314848184585571, avg_entr 0.012927437201142311, f1 0.8673999905586243
ep21_l3_test_time 4.517235279083252
Test Epoch21 layer4 Acc 0.8672, AUC 0.9355197548866272, avg_entr 0.012671079486608505, f1 0.8672000169754028
ep21_l4_test_time 5.725780010223389
gc 0
Train Epoch22 Acc 0.981875 (39275/40000), AUC 0.9960222244262695
ep22_train_time 180.97241759300232
Test Epoch22 layer0 Acc 0.8816, AUC 0.9459641575813293, avg_entr 0.1163194552063942, f1 0.881600022315979
ep22_l0_test_time 0.7621960639953613
Test Epoch22 layer1 Acc 0.8682, AUC 0.9151535630226135, avg_entr 0.02036900445818901, f1 0.8682000041007996
ep22_l1_test_time 2.0513765811920166
Test Epoch22 layer2 Acc 0.867, AUC 0.9183104038238525, avg_entr 0.014216925017535686, f1 0.8669999837875366
ep22_l2_test_time 3.198591470718384
Test Epoch22 layer3 Acc 0.8672, AUC 0.9303475618362427, avg_entr 0.012386012822389603, f1 0.8672000169754028
ep22_l3_test_time 4.53374719619751
Test Epoch22 layer4 Acc 0.867, AUC 0.9349234104156494, avg_entr 0.012072446756064892, f1 0.8669999837875366
ep22_l4_test_time 5.760681867599487
gc 0
Train Epoch23 Acc 0.98205 (39282/40000), AUC 0.9963381290435791
ep23_train_time 180.73051500320435
Test Epoch23 layer0 Acc 0.881, AUC 0.9458050727844238, avg_entr 0.11559435725212097, f1 0.8809999823570251
ep23_l0_test_time 0.760833740234375
Test Epoch23 layer1 Acc 0.8664, AUC 0.9149245023727417, avg_entr 0.021351320669054985, f1 0.8664000034332275
ep23_l1_test_time 2.0523362159729004
Test Epoch23 layer2 Acc 0.8654, AUC 0.9219635725021362, avg_entr 0.015969188883900642, f1 0.865399956703186
ep23_l2_test_time 3.2337586879730225
Test Epoch23 layer3 Acc 0.8672, AUC 0.9316445589065552, avg_entr 0.014338470064103603, f1 0.8672000169754028
ep23_l3_test_time 4.508252859115601
Test Epoch23 layer4 Acc 0.8668, AUC 0.9356104135513306, avg_entr 0.014061875641345978, f1 0.8668000102043152
ep23_l4_test_time 5.710467576980591
gc 0
Train Epoch24 Acc 0.981975 (39279/40000), AUC 0.9963445663452148
ep24_train_time 180.55813241004944
Test Epoch24 layer0 Acc 0.881, AUC 0.945637583732605, avg_entr 0.11493940651416779, f1 0.8809999823570251
ep24_l0_test_time 0.7671804428100586
Test Epoch24 layer1 Acc 0.8664, AUC 0.9144425392150879, avg_entr 0.020941857248544693, f1 0.8664000034332275
ep24_l1_test_time 2.0337090492248535
Test Epoch24 layer2 Acc 0.8662, AUC 0.920622706413269, avg_entr 0.015486372634768486, f1 0.8661999702453613
ep24_l2_test_time 3.2314188480377197
Test Epoch24 layer3 Acc 0.8678, AUC 0.9310945272445679, avg_entr 0.013865954242646694, f1 0.8677999973297119
ep24_l3_test_time 4.480947971343994
Test Epoch24 layer4 Acc 0.8672, AUC 0.9352055788040161, avg_entr 0.013606707565486431, f1 0.8672000169754028
ep24_l4_test_time 5.7607152462005615
gc 0
Train Epoch25 Acc 0.9825 (39300/40000), AUC 0.996417760848999
ep25_train_time 180.76549172401428
Test Epoch25 layer0 Acc 0.881, AUC 0.9456217288970947, avg_entr 0.11504083126783371, f1 0.8809999823570251
ep25_l0_test_time 0.7727618217468262
Test Epoch25 layer1 Acc 0.8676, AUC 0.9139900803565979, avg_entr 0.019966894760727882, f1 0.8675999641418457
ep25_l1_test_time 2.063537836074829
Test Epoch25 layer2 Acc 0.8668, AUC 0.9165340065956116, avg_entr 0.01393037848174572, f1 0.8668000102043152
ep25_l2_test_time 3.216187000274658
Test Epoch25 layer3 Acc 0.8666, AUC 0.9285604357719421, avg_entr 0.012080341577529907, f1 0.866599977016449
ep25_l3_test_time 4.50473690032959
Test Epoch25 layer4 Acc 0.8668, AUC 0.9339281320571899, avg_entr 0.011790546588599682, f1 0.8668000102043152
ep25_l4_test_time 5.7063868045806885
gc 0
Train Epoch26 Acc 0.9824 (39296/40000), AUC 0.9964278936386108
ep26_train_time 180.95459604263306
Test Epoch26 layer0 Acc 0.8804, AUC 0.9455506801605225, avg_entr 0.11432978510856628, f1 0.8804000020027161
ep26_l0_test_time 0.7640266418457031
Test Epoch26 layer1 Acc 0.8678, AUC 0.9138936400413513, avg_entr 0.021327687427401543, f1 0.8677999973297119
ep26_l1_test_time 2.010035276412964
Test Epoch26 layer2 Acc 0.8664, AUC 0.9197524785995483, avg_entr 0.015852797776460648, f1 0.8664000034332275
ep26_l2_test_time 3.272207736968994
Test Epoch26 layer3 Acc 0.8668, AUC 0.9307149648666382, avg_entr 0.014027669094502926, f1 0.8668000102043152
ep26_l3_test_time 4.532332181930542
Test Epoch26 layer4 Acc 0.8672, AUC 0.9350003004074097, avg_entr 0.013792675919830799, f1 0.8672000169754028
ep26_l4_test_time 5.725128650665283
gc 0
Train Epoch27 Acc 0.982075 (39283/40000), AUC 0.9964370727539062
ep27_train_time 180.97480249404907
Test Epoch27 layer0 Acc 0.8812, AUC 0.9454916715621948, avg_entr 0.11414726823568344, f1 0.8812000155448914
ep27_l0_test_time 0.7682602405548096
Test Epoch27 layer1 Acc 0.8672, AUC 0.9134168028831482, avg_entr 0.01984478160738945, f1 0.8672000169754028
ep27_l1_test_time 2.0519464015960693
Test Epoch27 layer2 Acc 0.8676, AUC 0.9172191619873047, avg_entr 0.014121729880571365, f1 0.8675999641418457
ep27_l2_test_time 3.2019894123077393
Test Epoch27 layer3 Acc 0.8674, AUC 0.928945779800415, avg_entr 0.012548235245049, f1 0.8673999905586243
ep27_l3_test_time 4.461205244064331
Test Epoch27 layer4 Acc 0.8674, AUC 0.9338771104812622, avg_entr 0.012204713188111782, f1 0.8673999905586243
ep27_l4_test_time 5.764225244522095
gc 0
Train Epoch28 Acc 0.9823 (39292/40000), AUC 0.9964755773544312
ep28_train_time 181.11906242370605
Test Epoch28 layer0 Acc 0.8804, AUC 0.9453996419906616, avg_entr 0.1137080192565918, f1 0.8804000020027161
ep28_l0_test_time 0.750680685043335
Test Epoch28 layer1 Acc 0.8672, AUC 0.9136524200439453, avg_entr 0.02095567062497139, f1 0.8672000169754028
ep28_l1_test_time 1.9487287998199463
Test Epoch28 layer2 Acc 0.8666, AUC 0.9188520908355713, avg_entr 0.015803273767232895, f1 0.866599977016449
ep28_l2_test_time 1.8852157592773438
Test Epoch28 layer3 Acc 0.8674, AUC 0.9299895763397217, avg_entr 0.013968436978757381, f1 0.8673999905586243
ep28_l3_test_time 2.4461123943328857
Test Epoch28 layer4 Acc 0.8676, AUC 0.934620201587677, avg_entr 0.01367227640002966, f1 0.8675999641418457
ep28_l4_test_time 3.108684539794922
gc 0
Train Epoch29 Acc 0.9825 (39300/40000), AUC 0.9963724613189697
ep29_train_time 170.50607085227966
Test Epoch29 layer0 Acc 0.8804, AUC 0.9453766345977783, avg_entr 0.11308128386735916, f1 0.8804000020027161
ep29_l0_test_time 0.7626852989196777
Test Epoch29 layer1 Acc 0.8662, AUC 0.9133166074752808, avg_entr 0.019881781190633774, f1 0.8661999702453613
ep29_l1_test_time 2.0284242630004883
Test Epoch29 layer2 Acc 0.8664, AUC 0.9175890684127808, avg_entr 0.014250134117901325, f1 0.8664000034332275
ep29_l2_test_time 3.2044363021850586
Test Epoch29 layer3 Acc 0.8662, AUC 0.929054319858551, avg_entr 0.012694683857262135, f1 0.8661999702453613
ep29_l3_test_time 4.5124359130859375
Test Epoch29 layer4 Acc 0.8666, AUC 0.9338959455490112, avg_entr 0.012466847896575928, f1 0.866599977016449
ep29_l4_test_time 5.775786399841309
Best AUC tensor(0.8992)
train_loss (2, 5, 30)
valid_acc (5, 30)
valid_AUC (5, 30)
train_acc (30,)
total_train+valid_time 5883.901779413223
Start Testing
Load ckpt at ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt
Test layer0 Acc 0.891, AUC 0.9537959694862366, avg_entr 0.22743916511535645, f1 0.890999972820282
l0_test_time 0.7814080715179443
Test layer1 Acc 0.8938, AUC 0.9581062197685242, avg_entr 0.12162131071090698, f1 0.8938000202178955
l1_test_time 2.045072078704834
Test layer2 Acc 0.897, AUC 0.9572268128395081, avg_entr 0.08024957031011581, f1 0.8970000147819519
l2_test_time 3.200786590576172
Test layer3 Acc 0.8972, AUC 0.9579721689224243, avg_entr 0.0613425113260746, f1 0.8971999883651733
l3_test_time 4.523969411849976
Test layer4 Acc 0.8972, AUC 0.958389401435852, avg_entr 0.05656988546252251, f1 0.8971999883651733
l4_test_time 5.70186448097229
