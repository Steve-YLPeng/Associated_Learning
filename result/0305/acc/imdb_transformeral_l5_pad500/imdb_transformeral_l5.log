total count words 222751
vocab size 30000
train size 40000, valid size 5000, test size 5000
found 27937 words in glove
model: TransformerModelML(
  (layers): ModuleList(
    (0): EMBLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Embedding(30000, 300)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=2, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=2, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.weight 9000000
layers.0.ae.g.0.weight 256
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 256
layers.0.ae.h.0.bias 2
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
Total Trainable Params: 13671778
init_time 20.745073556900024
Start Training
gc 0
Train Epoch0 Acc 0.546325 (21853/40000), AUC 0.5678970813751221
ep0_train_time 92.41320514678955
Test Epoch0 layer0 Acc 0.8104, AUC 0.8853374719619751, avg_entr 0.5739356279373169, f1 0.8104000091552734
ep0_l0_test_time 0.5640645027160645
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 0
Test Epoch0 layer1 Acc 0.8184, AUC 0.8991631269454956, avg_entr 0.3676249086856842, f1 0.8184000253677368
ep0_l1_test_time 1.1972620487213135
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 0
Test Epoch0 layer2 Acc 0.8284, AUC 0.9105319976806641, avg_entr 0.3930087387561798, f1 0.8284000158309937
ep0_l2_test_time 1.8236064910888672
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 0
Test Epoch0 layer3 Acc 0.8228, AUC 0.9097824096679688, avg_entr 0.5123642683029175, f1 0.8227999806404114
ep0_l3_test_time 2.454946994781494
Test Epoch0 layer4 Acc 0.8172, AUC 0.9014294743537903, avg_entr 0.6240208745002747, f1 0.8172000050544739
ep0_l4_test_time 3.0850577354431152
gc 0
Train Epoch1 Acc 0.865025 (34601/40000), AUC 0.9346811771392822
ep1_train_time 92.21178722381592
Test Epoch1 layer0 Acc 0.8644, AUC 0.9450335502624512, avg_entr 0.2894601821899414, f1 0.8644000291824341
ep1_l0_test_time 0.5702075958251953
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 1
Test Epoch1 layer1 Acc 0.8788, AUC 0.954806387424469, avg_entr 0.1965765804052353, f1 0.8787999749183655
ep1_l1_test_time 1.2123286724090576
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 1
Test Epoch1 layer2 Acc 0.8734, AUC 0.955636203289032, avg_entr 0.16454948484897614, f1 0.8733999729156494
ep1_l2_test_time 1.8411579132080078
Test Epoch1 layer3 Acc 0.873, AUC 0.9556990265846252, avg_entr 0.1380685567855835, f1 0.8730000257492065
ep1_l3_test_time 2.4684417247772217
Test Epoch1 layer4 Acc 0.8708, AUC 0.9560445547103882, avg_entr 0.12638527154922485, f1 0.8708000183105469
ep1_l4_test_time 3.108733892440796
gc 0
Train Epoch2 Acc 0.913675 (36547/40000), AUC 0.9672471284866333
ep2_train_time 92.37427592277527
Test Epoch2 layer0 Acc 0.8894, AUC 0.9548404812812805, avg_entr 0.22764377295970917, f1 0.8894000053405762
ep2_l0_test_time 0.5774147510528564
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 2
Test Epoch2 layer1 Acc 0.8946, AUC 0.9584184288978577, avg_entr 0.14740730822086334, f1 0.894599974155426
ep2_l1_test_time 1.197744369506836
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 2
Test Epoch2 layer2 Acc 0.89, AUC 0.9551182985305786, avg_entr 0.07119154185056686, f1 0.8899999856948853
ep2_l2_test_time 1.8191158771514893
Test Epoch2 layer3 Acc 0.8894, AUC 0.9568138718605042, avg_entr 0.0553419254720211, f1 0.8894000053405762
ep2_l3_test_time 2.4489288330078125
Test Epoch2 layer4 Acc 0.888, AUC 0.9579772353172302, avg_entr 0.05489743500947952, f1 0.8880000114440918
ep2_l4_test_time 3.075613021850586
gc 0
Train Epoch3 Acc 0.938925 (37557/40000), AUC 0.9806632995605469
ep3_train_time 92.21992516517639
Test Epoch3 layer0 Acc 0.8966, AUC 0.9583511352539062, avg_entr 0.1996375471353531, f1 0.8966000080108643
ep3_l0_test_time 0.5616726875305176
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 3
Test Epoch3 layer1 Acc 0.8926, AUC 0.955983579158783, avg_entr 0.10282295942306519, f1 0.8925999999046326
ep3_l1_test_time 1.2000930309295654
Test Epoch3 layer2 Acc 0.8904, AUC 0.9544247984886169, avg_entr 0.046714309602975845, f1 0.8903999924659729
ep3_l2_test_time 1.8185906410217285
Test Epoch3 layer3 Acc 0.8894, AUC 0.9557816386222839, avg_entr 0.04010550305247307, f1 0.8894000053405762
ep3_l3_test_time 2.4444689750671387
Test Epoch3 layer4 Acc 0.8906, AUC 0.9558634757995605, avg_entr 0.03965917229652405, f1 0.8906000256538391
ep3_l4_test_time 3.0788207054138184
gc 0
Train Epoch4 Acc 0.949825 (37993/40000), AUC 0.9850378036499023
ep4_train_time 92.07635927200317
Test Epoch4 layer0 Acc 0.8994, AUC 0.9591935873031616, avg_entr 0.17709393799304962, f1 0.8993999361991882
ep4_l0_test_time 0.562401533126831
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 4
Test Epoch4 layer1 Acc 0.8892, AUC 0.9533023238182068, avg_entr 0.06502632796764374, f1 0.88919997215271
ep4_l1_test_time 1.206491231918335
Test Epoch4 layer2 Acc 0.8902, AUC 0.9526408910751343, avg_entr 0.03806224837899208, f1 0.8902000188827515
ep4_l2_test_time 1.826004981994629
Test Epoch4 layer3 Acc 0.8904, AUC 0.9536925554275513, avg_entr 0.03657705709338188, f1 0.8903999924659729
ep4_l3_test_time 2.454373836517334
Test Epoch4 layer4 Acc 0.8902, AUC 0.9540133476257324, avg_entr 0.03698832914233208, f1 0.8902000188827515
ep4_l4_test_time 3.0859215259552
gc 0
Train Epoch5 Acc 0.957025 (38281/40000), AUC 0.9875982403755188
ep5_train_time 92.31441688537598
Test Epoch5 layer0 Acc 0.8974, AUC 0.9586185812950134, avg_entr 0.16477526724338531, f1 0.8974000215530396
ep5_l0_test_time 0.5845060348510742
Test Epoch5 layer1 Acc 0.8842, AUC 0.9500652551651001, avg_entr 0.04297948628664017, f1 0.8842000365257263
ep5_l1_test_time 1.2160942554473877
Test Epoch5 layer2 Acc 0.8854, AUC 0.9524935483932495, avg_entr 0.03403174504637718, f1 0.8853999972343445
ep5_l2_test_time 1.829399824142456
Test Epoch5 layer3 Acc 0.885, AUC 0.9527969360351562, avg_entr 0.033259067684412, f1 0.8849999904632568
ep5_l3_test_time 2.4591214656829834
Test Epoch5 layer4 Acc 0.885, AUC 0.9527636170387268, avg_entr 0.03338002413511276, f1 0.8849999904632568
ep5_l4_test_time 3.091628074645996
gc 0
Train Epoch6 Acc 0.962 (38480/40000), AUC 0.989240288734436
ep6_train_time 92.11373114585876
Test Epoch6 layer0 Acc 0.8974, AUC 0.9577486515045166, avg_entr 0.1523367017507553, f1 0.8974000215530396
ep6_l0_test_time 0.5722532272338867
Test Epoch6 layer1 Acc 0.8844, AUC 0.9435561895370483, avg_entr 0.03623025864362717, f1 0.8844000101089478
ep6_l1_test_time 1.2079765796661377
Test Epoch6 layer2 Acc 0.8848, AUC 0.949722945690155, avg_entr 0.027867602184414864, f1 0.8848000168800354
ep6_l2_test_time 1.827272653579712
Test Epoch6 layer3 Acc 0.8852, AUC 0.9496866464614868, avg_entr 0.02691950649023056, f1 0.8852000832557678
ep6_l3_test_time 2.456090211868286
Test Epoch6 layer4 Acc 0.8844, AUC 0.9499241709709167, avg_entr 0.027246437966823578, f1 0.8844000101089478
ep6_l4_test_time 3.083806276321411
gc 0
Train Epoch7 Acc 0.96655 (38662/40000), AUC 0.9911436438560486
ep7_train_time 92.28172731399536
Test Epoch7 layer0 Acc 0.8934, AUC 0.9559109807014465, avg_entr 0.14659760892391205, f1 0.8934000134468079
ep7_l0_test_time 0.5700247287750244
Test Epoch7 layer1 Acc 0.8802, AUC 0.9413309097290039, avg_entr 0.03678463026881218, f1 0.8802000284194946
ep7_l1_test_time 1.2035987377166748
Test Epoch7 layer2 Acc 0.8826, AUC 0.9480044841766357, avg_entr 0.02865915186703205, f1 0.8826000094413757
ep7_l2_test_time 1.8315753936767578
Test Epoch7 layer3 Acc 0.8828, AUC 0.9480670094490051, avg_entr 0.027933580800890923, f1 0.8827999830245972
ep7_l3_test_time 2.464991807937622
Test Epoch7 layer4 Acc 0.883, AUC 0.9482972621917725, avg_entr 0.028045425191521645, f1 0.8830000162124634
ep7_l4_test_time 3.0859861373901367
gc 0
Train Epoch8 Acc 0.96915 (38766/40000), AUC 0.9922794103622437
ep8_train_time 92.18779516220093
Test Epoch8 layer0 Acc 0.8942, AUC 0.9545257091522217, avg_entr 0.1400800347328186, f1 0.8942000269889832
ep8_l0_test_time 0.5725889205932617
Test Epoch8 layer1 Acc 0.8772, AUC 0.9358826875686646, avg_entr 0.032731592655181885, f1 0.8772000074386597
ep8_l1_test_time 1.211057424545288
Test Epoch8 layer2 Acc 0.8778, AUC 0.9445337057113647, avg_entr 0.02294706180691719, f1 0.8778000473976135
ep8_l2_test_time 1.825742244720459
Test Epoch8 layer3 Acc 0.878, AUC 0.9449242353439331, avg_entr 0.02221611700952053, f1 0.878000020980835
ep8_l3_test_time 2.4552035331726074
Test Epoch8 layer4 Acc 0.8784, AUC 0.9450453519821167, avg_entr 0.02234913595020771, f1 0.8784000277519226
ep8_l4_test_time 3.0865228176116943
gc 0
Train Epoch9 Acc 0.972425 (38897/40000), AUC 0.9931696653366089
ep9_train_time 92.12921810150146
Test Epoch9 layer0 Acc 0.889, AUC 0.9534527063369751, avg_entr 0.13344706594944, f1 0.8889999985694885
ep9_l0_test_time 0.5746004581451416
Test Epoch9 layer1 Acc 0.875, AUC 0.9336535930633545, avg_entr 0.02960839867591858, f1 0.875
ep9_l1_test_time 1.2063381671905518
Test Epoch9 layer2 Acc 0.8754, AUC 0.9434459209442139, avg_entr 0.02483983337879181, f1 0.8754000067710876
ep9_l2_test_time 1.828892469406128
Test Epoch9 layer3 Acc 0.8762, AUC 0.9436928033828735, avg_entr 0.02380465902388096, f1 0.8762000203132629
ep9_l3_test_time 2.4590909481048584
Test Epoch9 layer4 Acc 0.876, AUC 0.9435055255889893, avg_entr 0.02403169870376587, f1 0.8760000467300415
ep9_l4_test_time 3.083855152130127
gc 0
Train Epoch10 Acc 0.973825 (38953/40000), AUC 0.9930393695831299
ep10_train_time 92.17783975601196
Test Epoch10 layer0 Acc 0.8908, AUC 0.9524863958358765, avg_entr 0.13252949714660645, f1 0.8907999992370605
ep10_l0_test_time 0.5764691829681396
Test Epoch10 layer1 Acc 0.878, AUC 0.9309559464454651, avg_entr 0.02618519589304924, f1 0.878000020980835
ep10_l1_test_time 1.2091450691223145
Test Epoch10 layer2 Acc 0.8774, AUC 0.9428580403327942, avg_entr 0.022341255098581314, f1 0.8773999810218811
ep10_l2_test_time 1.8282012939453125
Test Epoch10 layer3 Acc 0.8776, AUC 0.9428565502166748, avg_entr 0.021272046491503716, f1 0.8776000142097473
ep10_l3_test_time 2.4557223320007324
Test Epoch10 layer4 Acc 0.878, AUC 0.9434201717376709, avg_entr 0.021204903721809387, f1 0.878000020980835
ep10_l4_test_time 3.0873537063598633
gc 0
Train Epoch11 Acc 0.975675 (39027/40000), AUC 0.9944753646850586
ep11_train_time 92.185795545578
Test Epoch11 layer0 Acc 0.8888, AUC 0.9513970613479614, avg_entr 0.1290445625782013, f1 0.8888000249862671
ep11_l0_test_time 0.5737476348876953
Test Epoch11 layer1 Acc 0.873, AUC 0.9295423030853271, avg_entr 0.02551046386361122, f1 0.8730000257492065
ep11_l1_test_time 1.2071280479431152
Test Epoch11 layer2 Acc 0.8724, AUC 0.9408783912658691, avg_entr 0.019961657002568245, f1 0.8723999857902527
ep11_l2_test_time 1.8321552276611328
Test Epoch11 layer3 Acc 0.8726, AUC 0.9406819343566895, avg_entr 0.019233310595154762, f1 0.8726000189781189
ep11_l3_test_time 2.46187686920166
Test Epoch11 layer4 Acc 0.8726, AUC 0.9412646889686584, avg_entr 0.01941779814660549, f1 0.8726000189781189
ep11_l4_test_time 3.0872042179107666
gc 0
Train Epoch12 Acc 0.97675 (39070/40000), AUC 0.9946669340133667
ep12_train_time 92.16420793533325
Test Epoch12 layer0 Acc 0.8876, AUC 0.9502158164978027, avg_entr 0.1272159218788147, f1 0.8876000046730042
ep12_l0_test_time 0.5972299575805664
Test Epoch12 layer1 Acc 0.8742, AUC 0.9273401498794556, avg_entr 0.026744991540908813, f1 0.8741999864578247
ep12_l1_test_time 1.2090692520141602
Test Epoch12 layer2 Acc 0.8734, AUC 0.9386358261108398, avg_entr 0.020473428070545197, f1 0.8733999729156494
ep12_l2_test_time 1.8295469284057617
Test Epoch12 layer3 Acc 0.8736, AUC 0.9381936192512512, avg_entr 0.01986534334719181, f1 0.8736000061035156
ep12_l3_test_time 2.46374773979187
Test Epoch12 layer4 Acc 0.8736, AUC 0.9398518800735474, avg_entr 0.020190808922052383, f1 0.8736000061035156
ep12_l4_test_time 3.09555983543396
gc 0
Train Epoch13 Acc 0.97775 (39110/40000), AUC 0.9951989650726318
ep13_train_time 92.14240646362305
Test Epoch13 layer0 Acc 0.8854, AUC 0.9494692087173462, avg_entr 0.12450290471315384, f1 0.8853999972343445
ep13_l0_test_time 0.5812466144561768
Test Epoch13 layer1 Acc 0.8724, AUC 0.9258314967155457, avg_entr 0.024893345311284065, f1 0.8723999857902527
ep13_l1_test_time 1.2059519290924072
Test Epoch13 layer2 Acc 0.8724, AUC 0.936206042766571, avg_entr 0.018233945593237877, f1 0.8723999857902527
ep13_l2_test_time 1.8275516033172607
Test Epoch13 layer3 Acc 0.872, AUC 0.9368816614151001, avg_entr 0.01758684031665325, f1 0.871999979019165
ep13_l3_test_time 2.457750082015991
Test Epoch13 layer4 Acc 0.872, AUC 0.9386401772499084, avg_entr 0.017572972923517227, f1 0.871999979019165
ep13_l4_test_time 3.0868687629699707
gc 0
Train Epoch14 Acc 0.978475 (39139/40000), AUC 0.9949071407318115
ep14_train_time 92.1135687828064
Test Epoch14 layer0 Acc 0.8862, AUC 0.9490498304367065, avg_entr 0.12278150767087936, f1 0.8862000107765198
ep14_l0_test_time 0.5781073570251465
Test Epoch14 layer1 Acc 0.871, AUC 0.9240685701370239, avg_entr 0.0236190315335989, f1 0.8709999918937683
ep14_l1_test_time 1.2088966369628906
Test Epoch14 layer2 Acc 0.8718, AUC 0.9351392388343811, avg_entr 0.017070051282644272, f1 0.8718000650405884
ep14_l2_test_time 1.8342399597167969
Test Epoch14 layer3 Acc 0.8718, AUC 0.9361048936843872, avg_entr 0.016480233520269394, f1 0.8718000650405884
ep14_l3_test_time 2.461520195007324
Test Epoch14 layer4 Acc 0.8718, AUC 0.9375669956207275, avg_entr 0.0166133064776659, f1 0.8718000650405884
ep14_l4_test_time 3.0917325019836426
gc 0
Train Epoch15 Acc 0.979325 (39173/40000), AUC 0.9955298900604248
ep15_train_time 92.18107461929321
Test Epoch15 layer0 Acc 0.8846, AUC 0.9484160542488098, avg_entr 0.12164865434169769, f1 0.8845999836921692
ep15_l0_test_time 0.5766851902008057
Test Epoch15 layer1 Acc 0.871, AUC 0.9235218167304993, avg_entr 0.023324063047766685, f1 0.8709999918937683
ep15_l1_test_time 1.2056169509887695
Test Epoch15 layer2 Acc 0.8718, AUC 0.9345016479492188, avg_entr 0.016709553077816963, f1 0.8718000650405884
ep15_l2_test_time 1.836061716079712
Test Epoch15 layer3 Acc 0.8718, AUC 0.935202956199646, avg_entr 0.0160873681306839, f1 0.8718000650405884
ep15_l3_test_time 2.4628026485443115
Test Epoch15 layer4 Acc 0.8718, AUC 0.9372168779373169, avg_entr 0.01625003293156624, f1 0.8718000650405884
ep15_l4_test_time 3.0955097675323486
gc 0
Train Epoch16 Acc 0.97965 (39186/40000), AUC 0.9955440759658813
ep16_train_time 92.18892002105713
Test Epoch16 layer0 Acc 0.883, AUC 0.9479526281356812, avg_entr 0.12107466906309128, f1 0.8830000162124634
ep16_l0_test_time 0.5758075714111328
Test Epoch16 layer1 Acc 0.8704, AUC 0.9213305711746216, avg_entr 0.021532902494072914, f1 0.8704000115394592
ep16_l1_test_time 1.2051684856414795
Test Epoch16 layer2 Acc 0.8706, AUC 0.9328117370605469, avg_entr 0.015787998214364052, f1 0.8705999851226807
ep16_l2_test_time 1.8306760787963867
Test Epoch16 layer3 Acc 0.8706, AUC 0.933688223361969, avg_entr 0.015276806429028511, f1 0.8705999851226807
ep16_l3_test_time 2.4580304622650146
Test Epoch16 layer4 Acc 0.8708, AUC 0.9359463453292847, avg_entr 0.015370577573776245, f1 0.8708000183105469
ep16_l4_test_time 3.088233232498169
gc 0
Train Epoch17 Acc 0.9803 (39212/40000), AUC 0.9956503510475159
ep17_train_time 92.24584197998047
Test Epoch17 layer0 Acc 0.8836, AUC 0.9477019906044006, avg_entr 0.12054020911455154, f1 0.8835999965667725
ep17_l0_test_time 0.5827820301055908
Test Epoch17 layer1 Acc 0.8698, AUC 0.9218771457672119, avg_entr 0.021848605945706367, f1 0.8697999715805054
ep17_l1_test_time 1.2006871700286865
Test Epoch17 layer2 Acc 0.8694, AUC 0.9330978989601135, avg_entr 0.01581631228327751, f1 0.8694000244140625
ep17_l2_test_time 1.830763339996338
Test Epoch17 layer3 Acc 0.87, AUC 0.9335448741912842, avg_entr 0.015304976142942905, f1 0.8700000047683716
ep17_l3_test_time 2.461721658706665
Test Epoch17 layer4 Acc 0.8698, AUC 0.9360374212265015, avg_entr 0.015468246303498745, f1 0.8697999715805054
ep17_l4_test_time 3.0894734859466553
gc 0
Train Epoch18 Acc 0.9805 (39220/40000), AUC 0.9959449768066406
ep18_train_time 92.18924236297607
Test Epoch18 layer0 Acc 0.8832, AUC 0.9473618268966675, avg_entr 0.11925041675567627, f1 0.8831999897956848
ep18_l0_test_time 0.5725553035736084
Test Epoch18 layer1 Acc 0.8706, AUC 0.920650839805603, avg_entr 0.021328289061784744, f1 0.8705999851226807
ep18_l1_test_time 1.2063963413238525
Test Epoch18 layer2 Acc 0.8694, AUC 0.9303568601608276, avg_entr 0.01577518694102764, f1 0.8694000244140625
ep18_l2_test_time 1.8421862125396729
Test Epoch18 layer3 Acc 0.8692, AUC 0.9309909343719482, avg_entr 0.015033215284347534, f1 0.8691999912261963
ep18_l3_test_time 2.4709391593933105
Test Epoch18 layer4 Acc 0.8696, AUC 0.9347535371780396, avg_entr 0.015118460170924664, f1 0.8695999383926392
ep18_l4_test_time 3.0961403846740723
gc 0
Train Epoch19 Acc 0.98095 (39238/40000), AUC 0.9957646131515503
ep19_train_time 92.27095031738281
Test Epoch19 layer0 Acc 0.8838, AUC 0.9471069574356079, avg_entr 0.1178121566772461, f1 0.8838000297546387
ep19_l0_test_time 0.5822100639343262
Test Epoch19 layer1 Acc 0.8696, AUC 0.9197901487350464, avg_entr 0.021730728447437286, f1 0.8695999383926392
ep19_l1_test_time 1.2061395645141602
Test Epoch19 layer2 Acc 0.8698, AUC 0.929721474647522, avg_entr 0.01548045501112938, f1 0.8697999715805054
ep19_l2_test_time 1.8292272090911865
Test Epoch19 layer3 Acc 0.8692, AUC 0.9311935901641846, avg_entr 0.015056481584906578, f1 0.8691999912261963
ep19_l3_test_time 2.4570298194885254
Test Epoch19 layer4 Acc 0.8694, AUC 0.9344865679740906, avg_entr 0.01510810386389494, f1 0.8694000244140625
ep19_l4_test_time 3.091050386428833
gc 0
Train Epoch20 Acc 0.981125 (39245/40000), AUC 0.9960898756980896
ep20_train_time 92.1592652797699
Test Epoch20 layer0 Acc 0.8826, AUC 0.9467846155166626, avg_entr 0.11735236644744873, f1 0.8826000094413757
ep20_l0_test_time 0.585383415222168
Test Epoch20 layer1 Acc 0.8696, AUC 0.9195399284362793, avg_entr 0.021106000989675522, f1 0.8695999383926392
ep20_l1_test_time 1.2058732509613037
Test Epoch20 layer2 Acc 0.8702, AUC 0.9274680614471436, avg_entr 0.015013003721833229, f1 0.870199978351593
ep20_l2_test_time 1.831242561340332
Test Epoch20 layer3 Acc 0.8702, AUC 0.9292946457862854, avg_entr 0.014419950544834137, f1 0.870199978351593
ep20_l3_test_time 2.4588003158569336
Test Epoch20 layer4 Acc 0.8704, AUC 0.9333088397979736, avg_entr 0.01450385432690382, f1 0.8704000115394592
ep20_l4_test_time 3.090477705001831
gc 0
Train Epoch21 Acc 0.9812 (39248/40000), AUC 0.9962506294250488
ep21_train_time 92.1619188785553
Test Epoch21 layer0 Acc 0.8836, AUC 0.9466278553009033, avg_entr 0.11583885550498962, f1 0.8835999965667725
ep21_l0_test_time 0.5733630657196045
Test Epoch21 layer1 Acc 0.87, AUC 0.9185718297958374, avg_entr 0.021276965737342834, f1 0.8700000047683716
ep21_l1_test_time 1.2067170143127441
Test Epoch21 layer2 Acc 0.8694, AUC 0.9268535375595093, avg_entr 0.0153818279504776, f1 0.8694000244140625
ep21_l2_test_time 1.8276276588439941
Test Epoch21 layer3 Acc 0.8694, AUC 0.9299652576446533, avg_entr 0.014874869026243687, f1 0.8694000244140625
ep21_l3_test_time 2.4556350708007812
Test Epoch21 layer4 Acc 0.8692, AUC 0.9331263303756714, avg_entr 0.01489348616451025, f1 0.8691999912261963
ep21_l4_test_time 3.0850062370300293
gc 0
Train Epoch22 Acc 0.98165 (39266/40000), AUC 0.9962075352668762
ep22_train_time 92.12643837928772
Test Epoch22 layer0 Acc 0.8824, AUC 0.9464850425720215, avg_entr 0.11574791371822357, f1 0.8823999762535095
ep22_l0_test_time 0.5757699012756348
Test Epoch22 layer1 Acc 0.8686, AUC 0.9186021089553833, avg_entr 0.020311273634433746, f1 0.8686000108718872
ep22_l1_test_time 1.2107443809509277
Test Epoch22 layer2 Acc 0.868, AUC 0.92684006690979, avg_entr 0.01426485925912857, f1 0.8679999709129333
ep22_l2_test_time 1.8297810554504395
Test Epoch22 layer3 Acc 0.868, AUC 0.9293885231018066, avg_entr 0.013705377466976643, f1 0.8679999709129333
ep22_l3_test_time 2.4583778381347656
Test Epoch22 layer4 Acc 0.868, AUC 0.9327456951141357, avg_entr 0.013792977668344975, f1 0.8679999709129333
ep22_l4_test_time 3.085756778717041
gc 0
Train Epoch23 Acc 0.9818 (39272/40000), AUC 0.9962545037269592
ep23_train_time 92.12020921707153
Test Epoch23 layer0 Acc 0.8824, AUC 0.9463579058647156, avg_entr 0.11512310802936554, f1 0.8823999762535095
ep23_l0_test_time 0.5752625465393066
Test Epoch23 layer1 Acc 0.8698, AUC 0.9183428287506104, avg_entr 0.020651020109653473, f1 0.8697999715805054
ep23_l1_test_time 1.2137508392333984
Test Epoch23 layer2 Acc 0.8696, AUC 0.9248827695846558, avg_entr 0.014489117078483105, f1 0.8695999383926392
ep23_l2_test_time 1.8295364379882812
Test Epoch23 layer3 Acc 0.8696, AUC 0.9271354675292969, avg_entr 0.013872172683477402, f1 0.8695999383926392
ep23_l3_test_time 2.45582914352417
Test Epoch23 layer4 Acc 0.8696, AUC 0.9320350289344788, avg_entr 0.013976478949189186, f1 0.8695999383926392
ep23_l4_test_time 3.091219425201416
gc 0
Train Epoch24 Acc 0.9816 (39264/40000), AUC 0.9962344765663147
ep24_train_time 92.19491457939148
Test Epoch24 layer0 Acc 0.8822, AUC 0.9461987614631653, avg_entr 0.11443459242582321, f1 0.8822000026702881
ep24_l0_test_time 0.5736703872680664
Test Epoch24 layer1 Acc 0.8698, AUC 0.9180237650871277, avg_entr 0.020649829879403114, f1 0.8697999715805054
ep24_l1_test_time 1.2078337669372559
Test Epoch24 layer2 Acc 0.8696, AUC 0.9236887693405151, avg_entr 0.01449995394796133, f1 0.8695999383926392
ep24_l2_test_time 1.8274259567260742
Test Epoch24 layer3 Acc 0.8698, AUC 0.925845205783844, avg_entr 0.014029559679329395, f1 0.8697999715805054
ep24_l3_test_time 2.4607720375061035
Test Epoch24 layer4 Acc 0.8694, AUC 0.9314059019088745, avg_entr 0.01410722080618143, f1 0.8694000244140625
ep24_l4_test_time 3.087669849395752
gc 0
Train Epoch25 Acc 0.981875 (39275/40000), AUC 0.9962676763534546
ep25_train_time 92.15539765357971
Test Epoch25 layer0 Acc 0.8822, AUC 0.9461637735366821, avg_entr 0.11430861800909042, f1 0.8822000026702881
ep25_l0_test_time 0.6084637641906738
Test Epoch25 layer1 Acc 0.8684, AUC 0.9174342751502991, avg_entr 0.021191466599702835, f1 0.868399977684021
ep25_l1_test_time 1.2351222038269043
Test Epoch25 layer2 Acc 0.868, AUC 0.923211395740509, avg_entr 0.014605874195694923, f1 0.8679999709129333
ep25_l2_test_time 1.8365025520324707
Test Epoch25 layer3 Acc 0.868, AUC 0.9261692762374878, avg_entr 0.014207732863724232, f1 0.8679999709129333
ep25_l3_test_time 2.460170269012451
Test Epoch25 layer4 Acc 0.8682, AUC 0.9312439560890198, avg_entr 0.014308279380202293, f1 0.8682000041007996
ep25_l4_test_time 3.0890743732452393
gc 0
Train Epoch26 Acc 0.982125 (39285/40000), AUC 0.9964922666549683
ep26_train_time 92.15091252326965
Test Epoch26 layer0 Acc 0.8824, AUC 0.9460657238960266, avg_entr 0.11323780566453934, f1 0.8823999762535095
ep26_l0_test_time 0.5739750862121582
Test Epoch26 layer1 Acc 0.8688, AUC 0.9178767204284668, avg_entr 0.02079441025853157, f1 0.8687999844551086
ep26_l1_test_time 1.217712640762329
Test Epoch26 layer2 Acc 0.868, AUC 0.9237853288650513, avg_entr 0.014508575201034546, f1 0.8679999709129333
ep26_l2_test_time 1.8320999145507812
Test Epoch26 layer3 Acc 0.868, AUC 0.9265310764312744, avg_entr 0.014074554666876793, f1 0.8679999709129333
ep26_l3_test_time 2.4720993041992188
Test Epoch26 layer4 Acc 0.8682, AUC 0.9314472079277039, avg_entr 0.014144839718937874, f1 0.8682000041007996
ep26_l4_test_time 3.0916268825531006
gc 0
Train Epoch27 Acc 0.982025 (39281/40000), AUC 0.996422529220581
ep27_train_time 92.1960859298706
Test Epoch27 layer0 Acc 0.882, AUC 0.9460370540618896, avg_entr 0.1134495735168457, f1 0.8820000290870667
ep27_l0_test_time 0.5823075771331787
Test Epoch27 layer1 Acc 0.8692, AUC 0.9178910255432129, avg_entr 0.02085459791123867, f1 0.8691999912261963
ep27_l1_test_time 1.2078831195831299
Test Epoch27 layer2 Acc 0.8676, AUC 0.9233191609382629, avg_entr 0.014385194517672062, f1 0.8675999641418457
ep27_l2_test_time 1.8326168060302734
Test Epoch27 layer3 Acc 0.8678, AUC 0.926287055015564, avg_entr 0.013929090462625027, f1 0.8677999973297119
ep27_l3_test_time 2.458583116531372
Test Epoch27 layer4 Acc 0.8676, AUC 0.9311076402664185, avg_entr 0.013992075808346272, f1 0.8675999641418457
ep27_l4_test_time 3.082228899002075
gc 0
Train Epoch28 Acc 0.982125 (39285/40000), AUC 0.9964702129364014
ep28_train_time 92.12086224555969
Test Epoch28 layer0 Acc 0.881, AUC 0.9459750652313232, avg_entr 0.11317282915115356, f1 0.8809999823570251
ep28_l0_test_time 0.5811140537261963
Test Epoch28 layer1 Acc 0.869, AUC 0.917628288269043, avg_entr 0.020135750994086266, f1 0.8690000176429749
ep28_l1_test_time 1.206674575805664
Test Epoch28 layer2 Acc 0.8694, AUC 0.922918438911438, avg_entr 0.014333456754684448, f1 0.8694000244140625
ep28_l2_test_time 1.8289446830749512
Test Epoch28 layer3 Acc 0.869, AUC 0.9245665669441223, avg_entr 0.01375229936093092, f1 0.8690000176429749
ep28_l3_test_time 2.4562761783599854
Test Epoch28 layer4 Acc 0.8688, AUC 0.9306768178939819, avg_entr 0.013872701674699783, f1 0.8687999844551086
ep28_l4_test_time 3.088252305984497
gc 0
Train Epoch29 Acc 0.9823 (39292/40000), AUC 0.9964734315872192
ep29_train_time 92.1186192035675
Test Epoch29 layer0 Acc 0.8822, AUC 0.9459456205368042, avg_entr 0.1124613806605339, f1 0.8822000026702881
ep29_l0_test_time 0.570303201675415
Test Epoch29 layer1 Acc 0.8684, AUC 0.9173808097839355, avg_entr 0.020585153251886368, f1 0.868399977684021
ep29_l1_test_time 1.2056787014007568
Test Epoch29 layer2 Acc 0.8664, AUC 0.9223777055740356, avg_entr 0.013765951618552208, f1 0.8664000034332275
ep29_l2_test_time 1.825364351272583
Test Epoch29 layer3 Acc 0.8668, AUC 0.9250780344009399, avg_entr 0.013293469324707985, f1 0.8668000102043152
ep29_l3_test_time 2.454746961593628
Test Epoch29 layer4 Acc 0.8668, AUC 0.9305057525634766, avg_entr 0.013355551287531853, f1 0.8668000102043152
ep29_l4_test_time 3.0844621658325195
Best AUC 0.8994
train_loss (2, 5, 30)
valid_acc (5, 30)
valid_AUC (5, 30)
train_acc (30,)
total_train+valid_time 3042.514081954956
Start Testing
Load ckpt at ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt
Test layer0 Acc 0.895, AUC 0.9555366039276123, avg_entr 0.17716607451438904, f1 0.8949999809265137
l0_test_time 0.5796186923980713
Test layer1 Acc 0.8844, AUC 0.9512803554534912, avg_entr 0.06692877411842346, f1 0.8844000101089478
l1_test_time 1.1998202800750732
Test layer2 Acc 0.8844, AUC 0.9517354965209961, avg_entr 0.03949353098869324, f1 0.8844000101089478
l2_test_time 1.822350025177002
Test layer3 Acc 0.8842, AUC 0.9512652158737183, avg_entr 0.03808806836605072, f1 0.8842000365257263
l3_test_time 2.4549925327301025
Test layer4 Acc 0.884, AUC 0.9514904022216797, avg_entr 0.03844025731086731, f1 0.8840000033378601
l4_test_time 3.0886948108673096
