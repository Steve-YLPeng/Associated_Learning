total count words 222751
vocab size 30000
train size 40000, valid size 5000, test size 5000
found 27937 words in glove
model: TransformerModelML(
  (layers): ModuleList(
    (0): EMBLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Embedding(30000, 300)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=2, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=2, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.weight 9000000
layers.0.ae.g.0.weight 256
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 256
layers.0.ae.h.0.bias 2
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
Total Trainable Params: 13671778
init_time 20.436095476150513
Start Training
gc 0
Train Epoch0 Acc 0.5198 (20792/40000), AUC 0.5250340700149536
ep0_train_time 16.248637914657593
Test Epoch0 layer0 Acc 0.8074, AUC 0.8901592493057251, avg_entr 0.4144582450389862, f1 0.8073999881744385
ep0_l0_test_time 0.13964033126831055
Save ckpt to ckpt/imdb_transformeral_l5_pad100//imdb_transformeral_l5.pt  ,ep 0
Test Epoch0 layer1 Acc 0.7822, AUC 0.8675330877304077, avg_entr 0.44690191745758057, f1 0.7821999788284302
ep0_l1_test_time 0.23621869087219238
Test Epoch0 layer2 Acc 0.7846, AUC 0.868905246257782, avg_entr 0.602891206741333, f1 0.784600019454956
ep0_l2_test_time 0.3285398483276367
Test Epoch0 layer3 Acc 0.773, AUC 0.8624632358551025, avg_entr 0.6670037508010864, f1 0.7730000019073486
ep0_l3_test_time 0.42116284370422363
Test Epoch0 layer4 Acc 0.761, AUC 0.8489315509796143, avg_entr 0.6943543553352356, f1 0.7610000371932983
ep0_l4_test_time 0.5139377117156982
gc 0
Train Epoch1 Acc 0.810275 (32411/40000), AUC 0.8901451826095581
ep1_train_time 15.901939630508423
Test Epoch1 layer0 Acc 0.837, AUC 0.9187439680099487, avg_entr 0.25901079177856445, f1 0.8370000123977661
ep1_l0_test_time 0.13885068893432617
Save ckpt to ckpt/imdb_transformeral_l5_pad100//imdb_transformeral_l5.pt  ,ep 1
Test Epoch1 layer1 Acc 0.8424, AUC 0.9211406707763672, avg_entr 0.22361509501934052, f1 0.8424000144004822
ep1_l1_test_time 0.2353532314300537
Save ckpt to ckpt/imdb_transformeral_l5_pad100//imdb_transformeral_l5.pt  ,ep 1
Test Epoch1 layer2 Acc 0.8466, AUC 0.9225119352340698, avg_entr 0.21174503862857819, f1 0.8465999960899353
ep1_l2_test_time 0.330106258392334
Save ckpt to ckpt/imdb_transformeral_l5_pad100//imdb_transformeral_l5.pt  ,ep 1
Test Epoch1 layer3 Acc 0.8442, AUC 0.9222935438156128, avg_entr 0.1977315992116928, f1 0.8442000150680542
ep1_l3_test_time 0.42446374893188477
Test Epoch1 layer4 Acc 0.8406, AUC 0.9221646785736084, avg_entr 0.19261838495731354, f1 0.8405999541282654
ep1_l4_test_time 0.5161421298980713
gc 0
Train Epoch2 Acc 0.897725 (35909/40000), AUC 0.9598293304443359
ep2_train_time 15.908249378204346
Test Epoch2 layer0 Acc 0.8422, AUC 0.9210938215255737, avg_entr 0.21510395407676697, f1 0.842199981212616
ep2_l0_test_time 0.13774466514587402
Test Epoch2 layer1 Acc 0.8368, AUC 0.9174491167068481, avg_entr 0.17559203505516052, f1 0.8368000388145447
ep2_l1_test_time 0.23242878913879395
Test Epoch2 layer2 Acc 0.8364, AUC 0.9184175729751587, avg_entr 0.12683993577957153, f1 0.8363999724388123
ep2_l2_test_time 0.32834815979003906
Test Epoch2 layer3 Acc 0.8366, AUC 0.9186839461326599, avg_entr 0.10272849351167679, f1 0.8366000056266785
ep2_l3_test_time 0.4215381145477295
Test Epoch2 layer4 Acc 0.8362, AUC 0.9186486005783081, avg_entr 0.08878317475318909, f1 0.8361999988555908
ep2_l4_test_time 0.5152106285095215
gc 0
Train Epoch3 Acc 0.926 (37040/40000), AUC 0.9768304228782654
ep3_train_time 15.936447858810425
Test Epoch3 layer0 Acc 0.8384, AUC 0.9173679947853088, avg_entr 0.19123722612857819, f1 0.8384000658988953
ep3_l0_test_time 0.1383044719696045
Test Epoch3 layer1 Acc 0.8228, AUC 0.9073173999786377, avg_entr 0.14632485806941986, f1 0.8227999806404114
ep3_l1_test_time 0.23289179801940918
Test Epoch3 layer2 Acc 0.8242, AUC 0.9048269391059875, avg_entr 0.08344496041536331, f1 0.8241999745368958
ep3_l2_test_time 0.3295929431915283
Test Epoch3 layer3 Acc 0.8242, AUC 0.9078990817070007, avg_entr 0.07181835919618607, f1 0.8241999745368958
ep3_l3_test_time 0.42266297340393066
Test Epoch3 layer4 Acc 0.8244, AUC 0.9082138538360596, avg_entr 0.06685227155685425, f1 0.8243999481201172
ep3_l4_test_time 0.5150995254516602
gc 0
Train Epoch4 Acc 0.93815 (37526/40000), AUC 0.9804344773292542
ep4_train_time 15.918707132339478
Test Epoch4 layer0 Acc 0.833, AUC 0.9124599695205688, avg_entr 0.17599688470363617, f1 0.8330000042915344
ep4_l0_test_time 0.1392989158630371
Test Epoch4 layer1 Acc 0.8182, AUC 0.9024533033370972, avg_entr 0.11280220001935959, f1 0.8181999921798706
ep4_l1_test_time 0.2329106330871582
Test Epoch4 layer2 Acc 0.8234, AUC 0.8998034000396729, avg_entr 0.06613878905773163, f1 0.8234000205993652
ep4_l2_test_time 0.3280644416809082
Test Epoch4 layer3 Acc 0.8226, AUC 0.9030243754386902, avg_entr 0.062326543033123016, f1 0.8226000070571899
ep4_l3_test_time 0.4226250648498535
Test Epoch4 layer4 Acc 0.8224, AUC 0.9035024642944336, avg_entr 0.05909145995974541, f1 0.8223999738693237
ep4_l4_test_time 0.5155596733093262
gc 0
Train Epoch5 Acc 0.948675 (37947/40000), AUC 0.98667973279953
ep5_train_time 15.937829494476318
Test Epoch5 layer0 Acc 0.8246, AUC 0.9067738056182861, avg_entr 0.16595199704170227, f1 0.8245999813079834
ep5_l0_test_time 0.13820695877075195
Test Epoch5 layer1 Acc 0.808, AUC 0.8912186622619629, avg_entr 0.07849223166704178, f1 0.8080000281333923
ep5_l1_test_time 0.23265671730041504
Test Epoch5 layer2 Acc 0.8066, AUC 0.8967410326004028, avg_entr 0.05117432028055191, f1 0.8065999746322632
ep5_l2_test_time 0.3286318778991699
Test Epoch5 layer3 Acc 0.8052, AUC 0.8972292542457581, avg_entr 0.04943600296974182, f1 0.8051999807357788
ep5_l3_test_time 0.42137956619262695
Test Epoch5 layer4 Acc 0.8056, AUC 0.8978203535079956, avg_entr 0.04728015884757042, f1 0.8055999875068665
ep5_l4_test_time 0.5148611068725586
gc 0
Train Epoch6 Acc 0.956425 (38257/40000), AUC 0.9897191524505615
ep6_train_time 15.928240060806274
Test Epoch6 layer0 Acc 0.8226, AUC 0.9012964963912964, avg_entr 0.15670964121818542, f1 0.8226000070571899
ep6_l0_test_time 0.13891887664794922
Test Epoch6 layer1 Acc 0.8094, AUC 0.8848477602005005, avg_entr 0.06957263499498367, f1 0.8094000220298767
ep6_l1_test_time 0.234083890914917
Test Epoch6 layer2 Acc 0.8106, AUC 0.8920984864234924, avg_entr 0.0463268905878067, f1 0.8105999231338501
ep6_l2_test_time 0.3283684253692627
Test Epoch6 layer3 Acc 0.8112, AUC 0.8924770951271057, avg_entr 0.0443839430809021, f1 0.8112000226974487
ep6_l3_test_time 0.4217357635498047
Test Epoch6 layer4 Acc 0.8112, AUC 0.8930366039276123, avg_entr 0.042313575744628906, f1 0.8112000226974487
ep6_l4_test_time 0.515244722366333
gc 0
Train Epoch7 Acc 0.96375 (38550/40000), AUC 0.9918134808540344
ep7_train_time 15.923020362854004
Test Epoch7 layer0 Acc 0.8202, AUC 0.8980662226676941, avg_entr 0.15189871191978455, f1 0.8202000260353088
ep7_l0_test_time 0.138413667678833
Test Epoch7 layer1 Acc 0.8092, AUC 0.875741720199585, avg_entr 0.06066436320543289, f1 0.8092000484466553
ep7_l1_test_time 0.23217058181762695
Test Epoch7 layer2 Acc 0.8086, AUC 0.8874189853668213, avg_entr 0.04017218202352524, f1 0.8086000084877014
ep7_l2_test_time 0.32823610305786133
Test Epoch7 layer3 Acc 0.8082, AUC 0.8887150287628174, avg_entr 0.037387665361166, f1 0.808199942111969
ep7_l3_test_time 0.4221482276916504
Test Epoch7 layer4 Acc 0.808, AUC 0.889512836933136, avg_entr 0.03512781485915184, f1 0.8080000281333923
ep7_l4_test_time 0.5153934955596924
gc 0
Train Epoch8 Acc 0.9667 (38668/40000), AUC 0.9925362467765808
ep8_train_time 15.918336868286133
Test Epoch8 layer0 Acc 0.8148, AUC 0.8946533203125, avg_entr 0.1469399482011795, f1 0.8148000240325928
ep8_l0_test_time 0.13769984245300293
Test Epoch8 layer1 Acc 0.8056, AUC 0.8717911243438721, avg_entr 0.054764118045568466, f1 0.8055999875068665
ep8_l1_test_time 0.23197102546691895
Test Epoch8 layer2 Acc 0.8084, AUC 0.8859481811523438, avg_entr 0.038160476833581924, f1 0.8083999752998352
ep8_l2_test_time 0.32766103744506836
Test Epoch8 layer3 Acc 0.8076, AUC 0.8865184783935547, avg_entr 0.03615478426218033, f1 0.8076000213623047
ep8_l3_test_time 0.4212303161621094
Test Epoch8 layer4 Acc 0.8074, AUC 0.887187123298645, avg_entr 0.03391076624393463, f1 0.8073999881744385
ep8_l4_test_time 0.5158355236053467
gc 0
Train Epoch9 Acc 0.9689 (38756/40000), AUC 0.9929734468460083
ep9_train_time 15.935059070587158
Test Epoch9 layer0 Acc 0.818, AUC 0.8916950225830078, avg_entr 0.14449211955070496, f1 0.8180000185966492
ep9_l0_test_time 0.1389012336730957
Test Epoch9 layer1 Acc 0.8042, AUC 0.8676770329475403, avg_entr 0.05103977769613266, f1 0.8041999936103821
ep9_l1_test_time 0.23457074165344238
Test Epoch9 layer2 Acc 0.8056, AUC 0.8841946125030518, avg_entr 0.035791169852018356, f1 0.8055999875068665
ep9_l2_test_time 0.32816052436828613
Test Epoch9 layer3 Acc 0.8064, AUC 0.8846118450164795, avg_entr 0.034478820860385895, f1 0.8064000010490417
ep9_l3_test_time 0.4212164878845215
Test Epoch9 layer4 Acc 0.8064, AUC 0.8851545453071594, avg_entr 0.03282976523041725, f1 0.8064000010490417
ep9_l4_test_time 0.515007734298706
gc 0
Train Epoch10 Acc 0.97145 (38858/40000), AUC 0.9942047595977783
ep10_train_time 15.941562414169312
Test Epoch10 layer0 Acc 0.8132, AUC 0.8886674642562866, avg_entr 0.13971883058547974, f1 0.8131999969482422
ep10_l0_test_time 0.13870716094970703
Test Epoch10 layer1 Acc 0.8024, AUC 0.8613930940628052, avg_entr 0.04651397839188576, f1 0.8023999929428101
ep10_l1_test_time 0.23293161392211914
Test Epoch10 layer2 Acc 0.8018, AUC 0.8805400729179382, avg_entr 0.032143544405698776, f1 0.801800012588501
ep10_l2_test_time 0.3284623622894287
Test Epoch10 layer3 Acc 0.8016, AUC 0.8809881806373596, avg_entr 0.03023602068424225, f1 0.8015999794006348
ep10_l3_test_time 0.42293548583984375
Test Epoch10 layer4 Acc 0.802, AUC 0.8816335201263428, avg_entr 0.028609707951545715, f1 0.8019999861717224
ep10_l4_test_time 0.5155925750732422
gc 0
Train Epoch11 Acc 0.974475 (38979/40000), AUC 0.9950112104415894
ep11_train_time 15.921802520751953
Test Epoch11 layer0 Acc 0.8144, AUC 0.8872653841972351, avg_entr 0.13789355754852295, f1 0.8144000172615051
ep11_l0_test_time 0.13856291770935059
Test Epoch11 layer1 Acc 0.8006, AUC 0.8588796854019165, avg_entr 0.04474756121635437, f1 0.800599992275238
ep11_l1_test_time 0.23209023475646973
Test Epoch11 layer2 Acc 0.8022, AUC 0.8789834976196289, avg_entr 0.030723663046956062, f1 0.8022000193595886
ep11_l2_test_time 0.3276352882385254
Test Epoch11 layer3 Acc 0.8022, AUC 0.8794335722923279, avg_entr 0.028601285070180893, f1 0.8022000193595886
ep11_l3_test_time 0.4210188388824463
Test Epoch11 layer4 Acc 0.8022, AUC 0.8799244165420532, avg_entr 0.027018509805202484, f1 0.8022000193595886
ep11_l4_test_time 0.5160207748413086
gc 0
Train Epoch12 Acc 0.974525 (38981/40000), AUC 0.9952388405799866
ep12_train_time 15.946471452713013
Test Epoch12 layer0 Acc 0.8136, AUC 0.8857423067092896, avg_entr 0.1361028254032135, f1 0.8136000037193298
ep12_l0_test_time 0.14082956314086914
Test Epoch12 layer1 Acc 0.8002, AUC 0.856560468673706, avg_entr 0.04440636187791824, f1 0.8001999855041504
ep12_l1_test_time 0.23306655883789062
Test Epoch12 layer2 Acc 0.8006, AUC 0.8772932291030884, avg_entr 0.03067653812468052, f1 0.800599992275238
ep12_l2_test_time 0.32967591285705566
Test Epoch12 layer3 Acc 0.801, AUC 0.8785611391067505, avg_entr 0.028672248125076294, f1 0.8010000586509705
ep12_l3_test_time 0.4221012592315674
Test Epoch12 layer4 Acc 0.8012, AUC 0.8791801929473877, avg_entr 0.0270271934568882, f1 0.8011999726295471
ep12_l4_test_time 0.5159649848937988
gc 0
Train Epoch13 Acc 0.9763 (39052/40000), AUC 0.995602011680603
ep13_train_time 15.940520763397217
Test Epoch13 layer0 Acc 0.8102, AUC 0.88387531042099, avg_entr 0.13498204946517944, f1 0.8101999759674072
ep13_l0_test_time 0.13837671279907227
Test Epoch13 layer1 Acc 0.7994, AUC 0.8537081480026245, avg_entr 0.04302695766091347, f1 0.7993999123573303
ep13_l1_test_time 0.2323291301727295
Test Epoch13 layer2 Acc 0.7994, AUC 0.8756853342056274, avg_entr 0.02921440452337265, f1 0.7993999123573303
ep13_l2_test_time 0.3281114101409912
Test Epoch13 layer3 Acc 0.799, AUC 0.8775115013122559, avg_entr 0.027010977268218994, f1 0.7990000247955322
ep13_l3_test_time 0.42417025566101074
Test Epoch13 layer4 Acc 0.799, AUC 0.8781247138977051, avg_entr 0.025405334308743477, f1 0.7990000247955322
ep13_l4_test_time 0.5164251327514648
gc 0
Train Epoch14 Acc 0.97695 (39078/40000), AUC 0.9958834648132324
ep14_train_time 15.949281692504883
Test Epoch14 layer0 Acc 0.811, AUC 0.8829213380813599, avg_entr 0.13213783502578735, f1 0.8109999895095825
ep14_l0_test_time 0.13927960395812988
Test Epoch14 layer1 Acc 0.7996, AUC 0.8516289591789246, avg_entr 0.03862966597080231, f1 0.7996000051498413
ep14_l1_test_time 0.23326659202575684
Test Epoch14 layer2 Acc 0.799, AUC 0.8747963309288025, avg_entr 0.026793386787176132, f1 0.7990000247955322
ep14_l2_test_time 0.3285844326019287
Test Epoch14 layer3 Acc 0.799, AUC 0.876159131526947, avg_entr 0.02443256601691246, f1 0.7990000247955322
ep14_l3_test_time 0.421231746673584
Test Epoch14 layer4 Acc 0.7992, AUC 0.8763821125030518, avg_entr 0.022786693647503853, f1 0.7991999983787537
ep14_l4_test_time 0.517695426940918
gc 0
Train Epoch15 Acc 0.978475 (39139/40000), AUC 0.9964969158172607
ep15_train_time 15.964953899383545
Test Epoch15 layer0 Acc 0.8082, AUC 0.8819189071655273, avg_entr 0.1308208554983139, f1 0.808199942111969
ep15_l0_test_time 0.1389918327331543
Test Epoch15 layer1 Acc 0.7982, AUC 0.8519672751426697, avg_entr 0.04056220129132271, f1 0.7982000708580017
ep15_l1_test_time 0.23273444175720215
Test Epoch15 layer2 Acc 0.7992, AUC 0.8737120628356934, avg_entr 0.02826572023332119, f1 0.7991999983787537
ep15_l2_test_time 0.3286569118499756
Test Epoch15 layer3 Acc 0.7984, AUC 0.8756569027900696, avg_entr 0.026297500357031822, f1 0.7983999848365784
ep15_l3_test_time 0.4211771488189697
Test Epoch15 layer4 Acc 0.7986, AUC 0.8760537505149841, avg_entr 0.02500256709754467, f1 0.7986000180244446
ep15_l4_test_time 0.5154473781585693
gc 0
Train Epoch16 Acc 0.978475 (39139/40000), AUC 0.9964368343353271
ep16_train_time 15.9428129196167
Test Epoch16 layer0 Acc 0.809, AUC 0.8811198472976685, avg_entr 0.1300894021987915, f1 0.8090000748634338
ep16_l0_test_time 0.13927268981933594
Test Epoch16 layer1 Acc 0.7972, AUC 0.8500760197639465, avg_entr 0.038448382169008255, f1 0.7971999645233154
ep16_l1_test_time 0.23275184631347656
Test Epoch16 layer2 Acc 0.7982, AUC 0.8727990388870239, avg_entr 0.027177494019269943, f1 0.7982000708580017
ep16_l2_test_time 0.32957029342651367
Test Epoch16 layer3 Acc 0.7982, AUC 0.8748693466186523, avg_entr 0.025283226743340492, f1 0.7982000708580017
ep16_l3_test_time 0.4212684631347656
Test Epoch16 layer4 Acc 0.7978, AUC 0.8750497102737427, avg_entr 0.023765377700328827, f1 0.7978000044822693
ep16_l4_test_time 0.5156917572021484
gc 0
Train Epoch17 Acc 0.9788 (39152/40000), AUC 0.9963229298591614
ep17_train_time 15.959532022476196
Test Epoch17 layer0 Acc 0.807, AUC 0.880325436592102, avg_entr 0.12905552983283997, f1 0.8069999814033508
ep17_l0_test_time 0.13738727569580078
Test Epoch17 layer1 Acc 0.7968, AUC 0.8475456833839417, avg_entr 0.038573719561100006, f1 0.7968000173568726
ep17_l1_test_time 0.23254108428955078
Test Epoch17 layer2 Acc 0.7964, AUC 0.8693566918373108, avg_entr 0.026916686445474625, f1 0.7964000105857849
ep17_l2_test_time 0.32792210578918457
Test Epoch17 layer3 Acc 0.796, AUC 0.8733596801757812, avg_entr 0.024952717125415802, f1 0.796000063419342
ep17_l3_test_time 0.4217865467071533
Test Epoch17 layer4 Acc 0.7964, AUC 0.8737766742706299, avg_entr 0.023636139929294586, f1 0.7964000105857849
ep17_l4_test_time 0.5170540809631348
gc 0
Train Epoch18 Acc 0.9792 (39168/40000), AUC 0.9964726567268372
ep18_train_time 15.951746940612793
Test Epoch18 layer0 Acc 0.8054, AUC 0.8793544769287109, avg_entr 0.12759511172771454, f1 0.805400013923645
ep18_l0_test_time 0.13879013061523438
Test Epoch18 layer1 Acc 0.797, AUC 0.847075879573822, avg_entr 0.03794481232762337, f1 0.796999990940094
ep18_l1_test_time 0.23237252235412598
Test Epoch18 layer2 Acc 0.7976, AUC 0.8693523406982422, avg_entr 0.02639608085155487, f1 0.7976000308990479
ep18_l2_test_time 0.32802510261535645
Test Epoch18 layer3 Acc 0.7972, AUC 0.8730016350746155, avg_entr 0.02431696467101574, f1 0.7971999645233154
ep18_l3_test_time 0.42162108421325684
Test Epoch18 layer4 Acc 0.7978, AUC 0.8733131885528564, avg_entr 0.022973360493779182, f1 0.7978000044822693
ep18_l4_test_time 0.5150465965270996
gc 0
Train Epoch19 Acc 0.98 (39200/40000), AUC 0.9966642260551453
ep19_train_time 15.945805788040161
Test Epoch19 layer0 Acc 0.8058, AUC 0.8792120218276978, avg_entr 0.1267867237329483, f1 0.8058000206947327
ep19_l0_test_time 0.1387312412261963
Test Epoch19 layer1 Acc 0.7968, AUC 0.8466933965682983, avg_entr 0.0381883904337883, f1 0.7968000173568726
ep19_l1_test_time 0.23262929916381836
Test Epoch19 layer2 Acc 0.7962, AUC 0.8681802749633789, avg_entr 0.02645796164870262, f1 0.7961999177932739
ep19_l2_test_time 0.3298032283782959
Test Epoch19 layer3 Acc 0.7972, AUC 0.8726402521133423, avg_entr 0.024296993389725685, f1 0.7971999645233154
ep19_l3_test_time 0.42175745964050293
Test Epoch19 layer4 Acc 0.797, AUC 0.8730641007423401, avg_entr 0.02295846678316593, f1 0.796999990940094
ep19_l4_test_time 0.5158452987670898
gc 0
Train Epoch20 Acc 0.979925 (39197/40000), AUC 0.9968774318695068
ep20_train_time 15.92375659942627
Test Epoch20 layer0 Acc 0.8064, AUC 0.8790568709373474, avg_entr 0.12634821236133575, f1 0.8064000010490417
ep20_l0_test_time 0.13851213455200195
Test Epoch20 layer1 Acc 0.7962, AUC 0.8461782932281494, avg_entr 0.03768693283200264, f1 0.7961999177932739
ep20_l1_test_time 0.23279476165771484
Test Epoch20 layer2 Acc 0.7964, AUC 0.8677185773849487, avg_entr 0.026131946593523026, f1 0.7964000105857849
ep20_l2_test_time 0.32862353324890137
Test Epoch20 layer3 Acc 0.7966, AUC 0.87226402759552, avg_entr 0.02396337501704693, f1 0.7965999841690063
ep20_l3_test_time 0.42316389083862305
Test Epoch20 layer4 Acc 0.7966, AUC 0.8726545572280884, avg_entr 0.022641971707344055, f1 0.7965999841690063
ep20_l4_test_time 0.5158827304840088
gc 0
Train Epoch21 Acc 0.9802 (39208/40000), AUC 0.9967856407165527
ep21_train_time 15.943533182144165
Test Epoch21 layer0 Acc 0.8062, AUC 0.8787348866462708, avg_entr 0.12559926509857178, f1 0.8062000274658203
ep21_l0_test_time 0.13904953002929688
Test Epoch21 layer1 Acc 0.7964, AUC 0.8447052836418152, avg_entr 0.0371859110891819, f1 0.7964000105857849
ep21_l1_test_time 0.23326897621154785
Test Epoch21 layer2 Acc 0.796, AUC 0.8664655685424805, avg_entr 0.025647347792983055, f1 0.796000063419342
ep21_l2_test_time 0.3284332752227783
Test Epoch21 layer3 Acc 0.796, AUC 0.871699333190918, avg_entr 0.023577187210321426, f1 0.796000063419342
ep21_l3_test_time 0.42291831970214844
Test Epoch21 layer4 Acc 0.7956, AUC 0.8721299171447754, avg_entr 0.022324835881590843, f1 0.7955999970436096
ep21_l4_test_time 0.515186071395874
gc 0
Train Epoch22 Acc 0.9807 (39228/40000), AUC 0.9969919919967651
ep22_train_time 15.958775997161865
Test Epoch22 layer0 Acc 0.8052, AUC 0.8784522414207458, avg_entr 0.12509147822856903, f1 0.8051999807357788
ep22_l0_test_time 0.1384449005126953
Test Epoch22 layer1 Acc 0.797, AUC 0.844107985496521, avg_entr 0.036518681794404984, f1 0.796999990940094
ep22_l1_test_time 0.2345600128173828
Test Epoch22 layer2 Acc 0.7964, AUC 0.8656693696975708, avg_entr 0.025217637419700623, f1 0.7964000105857849
ep22_l2_test_time 0.3285048007965088
Test Epoch22 layer3 Acc 0.7958, AUC 0.8713312149047852, avg_entr 0.023117631673812866, f1 0.7958000302314758
ep22_l3_test_time 0.4220399856567383
Test Epoch22 layer4 Acc 0.7964, AUC 0.8718255162239075, avg_entr 0.02182997204363346, f1 0.7964000105857849
ep22_l4_test_time 0.5153775215148926
gc 0
Train Epoch23 Acc 0.980625 (39225/40000), AUC 0.9970270395278931
ep23_train_time 15.954054355621338
Test Epoch23 layer0 Acc 0.805, AUC 0.8782715797424316, avg_entr 0.12422201037406921, f1 0.8050000667572021
ep23_l0_test_time 0.1384410858154297
Test Epoch23 layer1 Acc 0.7968, AUC 0.8437901735305786, avg_entr 0.036298565566539764, f1 0.7968000173568726
ep23_l1_test_time 0.23235130310058594
Test Epoch23 layer2 Acc 0.7962, AUC 0.8651145696640015, avg_entr 0.025043679401278496, f1 0.7961999177932739
ep23_l2_test_time 0.3278024196624756
Test Epoch23 layer3 Acc 0.796, AUC 0.8711197376251221, avg_entr 0.02286704257130623, f1 0.796000063419342
ep23_l3_test_time 0.42269301414489746
Test Epoch23 layer4 Acc 0.796, AUC 0.8716576099395752, avg_entr 0.02156851999461651, f1 0.796000063419342
ep23_l4_test_time 0.5147631168365479
gc 0
Train Epoch24 Acc 0.981 (39240/40000), AUC 0.9969191551208496
ep24_train_time 15.948979377746582
Test Epoch24 layer0 Acc 0.805, AUC 0.8780750036239624, avg_entr 0.12344993650913239, f1 0.8050000667572021
ep24_l0_test_time 0.13811874389648438
Test Epoch24 layer1 Acc 0.7966, AUC 0.843040943145752, avg_entr 0.03482542559504509, f1 0.7965999841690063
ep24_l1_test_time 0.23301172256469727
Test Epoch24 layer2 Acc 0.797, AUC 0.8660871982574463, avg_entr 0.024382255971431732, f1 0.796999990940094
ep24_l2_test_time 0.32814478874206543
Test Epoch24 layer3 Acc 0.7972, AUC 0.8712809085845947, avg_entr 0.022369325160980225, f1 0.7971999645233154
ep24_l3_test_time 0.421842098236084
Test Epoch24 layer4 Acc 0.7972, AUC 0.8715006113052368, avg_entr 0.021008968353271484, f1 0.7971999645233154
ep24_l4_test_time 0.5168538093566895
gc 0
Train Epoch25 Acc 0.980925 (39237/40000), AUC 0.9970468878746033
ep25_train_time 15.944761514663696
Test Epoch25 layer0 Acc 0.8038, AUC 0.8779720664024353, avg_entr 0.1232973113656044, f1 0.8037999868392944
ep25_l0_test_time 0.13978219032287598
Test Epoch25 layer1 Acc 0.795, AUC 0.8427773714065552, avg_entr 0.035645488649606705, f1 0.7950000166893005
ep25_l1_test_time 0.23337054252624512
Test Epoch25 layer2 Acc 0.7952, AUC 0.864301323890686, avg_entr 0.02489321120083332, f1 0.7952000498771667
ep25_l2_test_time 0.32806849479675293
Test Epoch25 layer3 Acc 0.7954, AUC 0.8707314729690552, avg_entr 0.02293773554265499, f1 0.795400083065033
ep25_l3_test_time 0.42137837409973145
Test Epoch25 layer4 Acc 0.7954, AUC 0.8712491989135742, avg_entr 0.02172437496483326, f1 0.795400083065033
ep25_l4_test_time 0.5152838230133057
gc 0
Train Epoch26 Acc 0.98105 (39242/40000), AUC 0.9969276785850525
ep26_train_time 15.954992294311523
Test Epoch26 layer0 Acc 0.8044, AUC 0.8777344822883606, avg_entr 0.12221115827560425, f1 0.8044000267982483
ep26_l0_test_time 0.13824033737182617
Test Epoch26 layer1 Acc 0.7966, AUC 0.8424146175384521, avg_entr 0.03456122428178787, f1 0.7965999841690063
ep26_l1_test_time 0.23306632041931152
Test Epoch26 layer2 Acc 0.7966, AUC 0.8650983572006226, avg_entr 0.024290578439831734, f1 0.7965999841690063
ep26_l2_test_time 0.32810187339782715
Test Epoch26 layer3 Acc 0.7966, AUC 0.8707985877990723, avg_entr 0.02236742340028286, f1 0.7965999841690063
ep26_l3_test_time 0.4231076240539551
Test Epoch26 layer4 Acc 0.7966, AUC 0.8710907697677612, avg_entr 0.02099134586751461, f1 0.7965999841690063
ep26_l4_test_time 0.5145974159240723
gc 0
Train Epoch27 Acc 0.98095 (39238/40000), AUC 0.9971280097961426
ep27_train_time 15.95486044883728
Test Epoch27 layer0 Acc 0.8046, AUC 0.8777799606323242, avg_entr 0.12236540764570236, f1 0.8046000599861145
ep27_l0_test_time 0.13919782638549805
Test Epoch27 layer1 Acc 0.7952, AUC 0.8424961566925049, avg_entr 0.03533925488591194, f1 0.7952000498771667
ep27_l1_test_time 0.23339200019836426
Test Epoch27 layer2 Acc 0.7956, AUC 0.8639513254165649, avg_entr 0.02464776672422886, f1 0.7955999970436096
ep27_l2_test_time 0.3282430171966553
Test Epoch27 layer3 Acc 0.7954, AUC 0.8704546689987183, avg_entr 0.022639034315943718, f1 0.795400083065033
ep27_l3_test_time 0.4214341640472412
Test Epoch27 layer4 Acc 0.796, AUC 0.8709939122200012, avg_entr 0.02141403965651989, f1 0.796000063419342
ep27_l4_test_time 0.5162997245788574
gc 0
Train Epoch28 Acc 0.98115 (39246/40000), AUC 0.9971897602081299
ep28_train_time 15.982454299926758
Test Epoch28 layer0 Acc 0.8052, AUC 0.8777243494987488, avg_entr 0.12181922793388367, f1 0.8051999807357788
ep28_l0_test_time 0.13829469680786133
Test Epoch28 layer1 Acc 0.7944, AUC 0.8422056436538696, avg_entr 0.03467273339629173, f1 0.7943999767303467
ep28_l1_test_time 0.23219060897827148
Test Epoch28 layer2 Acc 0.7964, AUC 0.8643373847007751, avg_entr 0.024356605485081673, f1 0.7964000105857849
ep28_l2_test_time 0.32790517807006836
Test Epoch28 layer3 Acc 0.7962, AUC 0.8705440163612366, avg_entr 0.022420965135097504, f1 0.7961999177932739
ep28_l3_test_time 0.420576810836792
Test Epoch28 layer4 Acc 0.7968, AUC 0.8709584474563599, avg_entr 0.021114429458975792, f1 0.7968000173568726
ep28_l4_test_time 0.5150010585784912
gc 0
Train Epoch29 Acc 0.9812 (39248/40000), AUC 0.9971888661384583
ep29_train_time 15.965716361999512
Test Epoch29 layer0 Acc 0.8038, AUC 0.8776808977127075, avg_entr 0.12166960537433624, f1 0.8037999868392944
ep29_l0_test_time 0.13779115676879883
Test Epoch29 layer1 Acc 0.795, AUC 0.8422191739082336, avg_entr 0.03491757810115814, f1 0.7950000166893005
ep29_l1_test_time 0.23194003105163574
Test Epoch29 layer2 Acc 0.7956, AUC 0.8637348413467407, avg_entr 0.024343930184841156, f1 0.7955999970436096
ep29_l2_test_time 0.32929134368896484
Test Epoch29 layer3 Acc 0.795, AUC 0.8701993227005005, avg_entr 0.02229267545044422, f1 0.7950000166893005
ep29_l3_test_time 0.4217801094055176
Test Epoch29 layer4 Acc 0.795, AUC 0.8708051443099976, avg_entr 0.02108919434249401, f1 0.7950000166893005
ep29_l4_test_time 0.5150542259216309
Best AUC 0.8466
train_loss (2, 5, 30)
valid_acc (5, 30)
valid_AUC (5, 30)
train_acc (30,)
total_train+valid_time 529.1932892799377
Start Testing
Load ckpt at ckpt/imdb_transformeral_l5_pad100//imdb_transformeral_l5.pt
Test layer0 Acc 0.8296, AUC 0.9171388149261475, avg_entr 0.2607314884662628, f1 0.8295999765396118
l0_test_time 0.13573837280273438
Test layer1 Acc 0.835, AUC 0.9190782904624939, avg_entr 0.2246643751859665, f1 0.8349999785423279
l1_test_time 0.23230385780334473
Test layer2 Acc 0.8372, AUC 0.9198756217956543, avg_entr 0.21446208655834198, f1 0.8371999859809875
l2_test_time 0.3277931213378906
Test layer3 Acc 0.8354, AUC 0.9198830723762512, avg_entr 0.20053242146968842, f1 0.8353999853134155
l3_test_time 0.42177438735961914
Test layer4 Acc 0.837, AUC 0.9196427464485168, avg_entr 0.1964302808046341, f1 0.8370000123977661
l4_test_time 0.5148580074310303
