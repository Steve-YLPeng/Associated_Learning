total count words 887881
vocab size 30000
found 28354 words in glove
model: TransformerModelML(
  (layers): ModuleList(
    (0): EMBLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Embedding(30000, 300)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=14, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=14, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.weight 9000000
layers.0.ae.g.0.weight 1792
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 1792
layers.0.ae.h.0.bias 14
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
Total Trainable Params: 13674862
Start Training
gc 0
Train Epoch0 Acc 0.8325839285714286 (466247/560000), AUC 0.9795562028884888
Test Epoch0 threshold 0.2 Acc 0.9740142857142857, AUC 0.9983808398246765, avg_entr 0.024694448336958885
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad20//dbpedia_14_transformeral_l5.pt  ,ep 0
Test Epoch0 threshold 0.4 Acc 0.9741142857142857, AUC 0.9984490275382996, avg_entr 0.026212604716420174
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad20//dbpedia_14_transformeral_l5.pt  ,ep 0
Test Epoch0 threshold 0.6 Acc 0.9741142857142857, AUC 0.9984490275382996, avg_entr 0.026212604716420174
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad20//dbpedia_14_transformeral_l5.pt  ,ep 0
Test Epoch0 threshold 0.8 Acc 0.9741142857142857, AUC 0.9984490275382996, avg_entr 0.026212604716420174
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad20//dbpedia_14_transformeral_l5.pt  ,ep 0
gc 0
Train Epoch1 Acc 0.9787571428571429 (548104/560000), AUC 0.9972661137580872
Test Epoch1 threshold 0.2 Acc 0.9752428571428572, AUC 0.998509407043457, avg_entr 0.013000850565731525
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad20//dbpedia_14_transformeral_l5.pt  ,ep 1
Test Epoch1 threshold 0.4 Acc 0.9744714285714285, AUC 0.9985343217849731, avg_entr 0.014078459702432156
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad20//dbpedia_14_transformeral_l5.pt  ,ep 1
Test Epoch1 threshold 0.6 Acc 0.9744714285714285, AUC 0.9985343217849731, avg_entr 0.014078459702432156
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad20//dbpedia_14_transformeral_l5.pt  ,ep 1
Test Epoch1 threshold 0.8 Acc 0.9744714285714285, AUC 0.9985343217849731, avg_entr 0.014078459702432156
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad20//dbpedia_14_transformeral_l5.pt  ,ep 1
gc 0
Train Epoch2 Acc 0.9828482142857143 (550395/560000), AUC 0.9977795481681824
Test Epoch2 threshold 0.2 Acc 0.9748285714285714, AUC 0.9984630942344666, avg_entr 0.009483125060796738
Test Epoch2 threshold 0.4 Acc 0.9743714285714286, AUC 0.9984773397445679, avg_entr 0.01021906640380621
Test Epoch2 threshold 0.6 Acc 0.9743714285714286, AUC 0.9984773397445679, avg_entr 0.01021906640380621
Test Epoch2 threshold 0.8 Acc 0.9743714285714286, AUC 0.9984773397445679, avg_entr 0.01021906640380621
gc 0
Train Epoch3 Acc 0.9854607142857142 (551858/560000), AUC 0.9982640147209167
Test Epoch3 threshold 0.2 Acc 0.9753714285714286, AUC 0.9985243678092957, avg_entr 0.007903113961219788
Test Epoch3 threshold 0.4 Acc 0.9749428571428571, AUC 0.9985388517379761, avg_entr 0.008513343520462513
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad20//dbpedia_14_transformeral_l5.pt  ,ep 3
Test Epoch3 threshold 0.6 Acc 0.9749428571428571, AUC 0.9985388517379761, avg_entr 0.008513343520462513
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad20//dbpedia_14_transformeral_l5.pt  ,ep 3
Test Epoch3 threshold 0.8 Acc 0.9749428571428571, AUC 0.9985388517379761, avg_entr 0.008513343520462513
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad20//dbpedia_14_transformeral_l5.pt  ,ep 3
gc 0
Train Epoch4 Acc 0.9868839285714286 (552655/560000), AUC 0.9983839392662048
Test Epoch4 threshold 0.2 Acc 0.9752714285714286, AUC 0.9985531568527222, avg_entr 0.007290597073733807
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad20//dbpedia_14_transformeral_l5.pt  ,ep 4
Test Epoch4 threshold 0.4 Acc 0.9749142857142857, AUC 0.9985514283180237, avg_entr 0.007799195125699043
Test Epoch4 threshold 0.6 Acc 0.9749142857142857, AUC 0.9985514283180237, avg_entr 0.007799195125699043
Test Epoch4 threshold 0.8 Acc 0.9749142857142857, AUC 0.9985514283180237, avg_entr 0.007799195125699043
gc 0
Train Epoch5 Acc 0.9876392857142857 (553078/560000), AUC 0.9986410737037659
Test Epoch5 threshold 0.2 Acc 0.9754857142857143, AUC 0.9985499978065491, avg_entr 0.007113071158528328
Test Epoch5 threshold 0.4 Acc 0.975, AUC 0.9985529184341431, avg_entr 0.00757013913244009
Test Epoch5 threshold 0.6 Acc 0.975, AUC 0.9985529184341431, avg_entr 0.00757013913244009
Test Epoch5 threshold 0.8 Acc 0.975, AUC 0.9985529184341431, avg_entr 0.00757013913244009
gc 0
Train Epoch6 Acc 0.9880232142857143 (553293/560000), AUC 0.9987220764160156
Test Epoch6 threshold 0.2 Acc 0.9754, AUC 0.9985564947128296, avg_entr 0.007058020681142807
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad20//dbpedia_14_transformeral_l5.pt  ,ep 6
Test Epoch6 threshold 0.4 Acc 0.9749, AUC 0.9985580444335938, avg_entr 0.007527816109359264
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad20//dbpedia_14_transformeral_l5.pt  ,ep 6
Test Epoch6 threshold 0.6 Acc 0.9749, AUC 0.9985580444335938, avg_entr 0.007527816109359264
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad20//dbpedia_14_transformeral_l5.pt  ,ep 6
Test Epoch6 threshold 0.8 Acc 0.9749, AUC 0.9985580444335938, avg_entr 0.007527816109359264
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad20//dbpedia_14_transformeral_l5.pt  ,ep 6
gc 0
Train Epoch7 Acc 0.9883392857142858 (553470/560000), AUC 0.9987644553184509
Test Epoch7 threshold 0.2 Acc 0.9753571428571428, AUC 0.998555064201355, avg_entr 0.007025042548775673
Test Epoch7 threshold 0.4 Acc 0.9748714285714286, AUC 0.9985586404800415, avg_entr 0.0074960412457585335
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad20//dbpedia_14_transformeral_l5.pt  ,ep 7
Test Epoch7 threshold 0.6 Acc 0.9748714285714286, AUC 0.9985586404800415, avg_entr 0.0074960412457585335
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad20//dbpedia_14_transformeral_l5.pt  ,ep 7
Test Epoch7 threshold 0.8 Acc 0.9748714285714286, AUC 0.9985586404800415, avg_entr 0.0074960412457585335
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad20//dbpedia_14_transformeral_l5.pt  ,ep 7
gc 0
Train Epoch8 Acc 0.988375 (553490/560000), AUC 0.9987625479698181
Test Epoch8 threshold 0.2 Acc 0.9752571428571428, AUC 0.9985560178756714, avg_entr 0.007009194698184729
Test Epoch8 threshold 0.4 Acc 0.9747571428571429, AUC 0.998559296131134, avg_entr 0.007480761501938105
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad20//dbpedia_14_transformeral_l5.pt  ,ep 8
Test Epoch8 threshold 0.6 Acc 0.9747571428571429, AUC 0.998559296131134, avg_entr 0.007480761501938105
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad20//dbpedia_14_transformeral_l5.pt  ,ep 8
Test Epoch8 threshold 0.8 Acc 0.9747571428571429, AUC 0.998559296131134, avg_entr 0.007480761501938105
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad20//dbpedia_14_transformeral_l5.pt  ,ep 8
gc 0
Train Epoch9 Acc 0.9885 (553560/560000), AUC 0.9987735748291016
Test Epoch9 threshold 0.2 Acc 0.9753285714285714, AUC 0.9985564947128296, avg_entr 0.006995923817157745
Test Epoch9 threshold 0.4 Acc 0.9748142857142857, AUC 0.9985589385032654, avg_entr 0.007465733215212822
Test Epoch9 threshold 0.6 Acc 0.9748142857142857, AUC 0.9985589385032654, avg_entr 0.007465733215212822
Test Epoch9 threshold 0.8 Acc 0.9748142857142857, AUC 0.9985589385032654, avg_entr 0.007465733215212822
gc 0
Train Epoch10 Acc 0.9885857142857143 (553608/560000), AUC 0.998789370059967
Test Epoch10 threshold 0.2 Acc 0.9753142857142857, AUC 0.9985572099685669, avg_entr 0.0069974674843251705
Test Epoch10 threshold 0.4 Acc 0.9748285714285714, AUC 0.9985600113868713, avg_entr 0.007466343697160482
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad20//dbpedia_14_transformeral_l5.pt  ,ep 10
Test Epoch10 threshold 0.6 Acc 0.9748285714285714, AUC 0.9985600113868713, avg_entr 0.007466343697160482
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad20//dbpedia_14_transformeral_l5.pt  ,ep 10
Test Epoch10 threshold 0.8 Acc 0.9748285714285714, AUC 0.9985600113868713, avg_entr 0.007466343697160482
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad20//dbpedia_14_transformeral_l5.pt  ,ep 10
gc 0
Train Epoch11 Acc 0.9886196428571429 (553627/560000), AUC 0.998765766620636
Test Epoch11 threshold 0.2 Acc 0.9753, AUC 0.998556911945343, avg_entr 0.0069895535707473755
Test Epoch11 threshold 0.4 Acc 0.9748142857142857, AUC 0.9985594153404236, avg_entr 0.0074636186473071575
Test Epoch11 threshold 0.6 Acc 0.9748142857142857, AUC 0.9985594153404236, avg_entr 0.0074636186473071575
Test Epoch11 threshold 0.8 Acc 0.9748142857142857, AUC 0.9985594153404236, avg_entr 0.0074636186473071575
gc 0
Train Epoch12 Acc 0.9885892857142857 (553610/560000), AUC 0.9987812638282776
Test Epoch12 threshold 0.2 Acc 0.9753142857142857, AUC 0.9985566139221191, avg_entr 0.006993536371737719
Test Epoch12 threshold 0.4 Acc 0.9748142857142857, AUC 0.9985594153404236, avg_entr 0.007462063804268837
Test Epoch12 threshold 0.6 Acc 0.9748142857142857, AUC 0.9985594153404236, avg_entr 0.007462063804268837
Test Epoch12 threshold 0.8 Acc 0.9748142857142857, AUC 0.9985594153404236, avg_entr 0.007462063804268837
gc 0
Train Epoch13 Acc 0.9885053571428571 (553563/560000), AUC 0.9987668991088867
Test Epoch13 threshold 0.2 Acc 0.9753142857142857, AUC 0.9985564947128296, avg_entr 0.006992905400693417
Test Epoch13 threshold 0.4 Acc 0.9748142857142857, AUC 0.9985594749450684, avg_entr 0.0074613140895962715
Test Epoch13 threshold 0.6 Acc 0.9748142857142857, AUC 0.9985594749450684, avg_entr 0.0074613140895962715
Test Epoch13 threshold 0.8 Acc 0.9748142857142857, AUC 0.9985594749450684, avg_entr 0.0074613140895962715
gc 0
Train Epoch14 Acc 0.9885821428571429 (553606/560000), AUC 0.9987675547599792
Test Epoch14 threshold 0.2 Acc 0.9753142857142857, AUC 0.9985566139221191, avg_entr 0.006992624141275883
Test Epoch14 threshold 0.4 Acc 0.9748142857142857, AUC 0.9985594749450684, avg_entr 0.007461063098162413
Test Epoch14 threshold 0.6 Acc 0.9748142857142857, AUC 0.9985594749450684, avg_entr 0.007461063098162413
Test Epoch14 threshold 0.8 Acc 0.9748142857142857, AUC 0.9985594749450684, avg_entr 0.007461063098162413
gc 0
Train Epoch15 Acc 0.9885875 (553609/560000), AUC 0.998771607875824
Test Epoch15 threshold 0.2 Acc 0.9753142857142857, AUC 0.9985566735267639, avg_entr 0.006992674432694912
Test Epoch15 threshold 0.4 Acc 0.9748142857142857, AUC 0.9985595941543579, avg_entr 0.007461082190275192
Test Epoch15 threshold 0.6 Acc 0.9748142857142857, AUC 0.9985595941543579, avg_entr 0.007461082190275192
Test Epoch15 threshold 0.8 Acc 0.9748142857142857, AUC 0.9985595941543579, avg_entr 0.007461082190275192
gc 0
Train Epoch16 Acc 0.9885821428571429 (553606/560000), AUC 0.9987651109695435
Test Epoch16 threshold 0.2 Acc 0.9753142857142857, AUC 0.9985566735267639, avg_entr 0.00699266791343689
Test Epoch16 threshold 0.4 Acc 0.9748142857142857, AUC 0.9985595941543579, avg_entr 0.007461070083081722
Test Epoch16 threshold 0.6 Acc 0.9748142857142857, AUC 0.9985595941543579, avg_entr 0.007461070083081722
Test Epoch16 threshold 0.8 Acc 0.9748142857142857, AUC 0.9985595941543579, avg_entr 0.007461070083081722
gc 0
Train Epoch17 Acc 0.9885285714285714 (553576/560000), AUC 0.9987687468528748
Test Epoch17 threshold 0.2 Acc 0.9753142857142857, AUC 0.9985566735267639, avg_entr 0.00699264369904995
Test Epoch17 threshold 0.4 Acc 0.9748142857142857, AUC 0.9985595941543579, avg_entr 0.007461061235517263
Test Epoch17 threshold 0.6 Acc 0.9748142857142857, AUC 0.9985595941543579, avg_entr 0.007461061235517263
Test Epoch17 threshold 0.8 Acc 0.9748142857142857, AUC 0.9985595941543579, avg_entr 0.007461061235517263
gc 0
Train Epoch18 Acc 0.9885660714285714 (553597/560000), AUC 0.9987696409225464
Test Epoch18 threshold 0.2 Acc 0.9753142857142857, AUC 0.9985566735267639, avg_entr 0.006992635782808065
Test Epoch18 threshold 0.4 Acc 0.9748142857142857, AUC 0.9985596537590027, avg_entr 0.007461063098162413
Test Epoch18 threshold 0.6 Acc 0.9748142857142857, AUC 0.9985596537590027, avg_entr 0.007461063098162413
Test Epoch18 threshold 0.8 Acc 0.9748142857142857, AUC 0.9985596537590027, avg_entr 0.007461063098162413
gc 0
Train Epoch19 Acc 0.9886035714285715 (553618/560000), AUC 0.9987822771072388
Test Epoch19 threshold 0.2 Acc 0.9753142857142857, AUC 0.9985566735267639, avg_entr 0.00699263159185648
Test Epoch19 threshold 0.4 Acc 0.9748142857142857, AUC 0.9985595941543579, avg_entr 0.007461063098162413
Test Epoch19 threshold 0.6 Acc 0.9748142857142857, AUC 0.9985595941543579, avg_entr 0.007461063098162413
Test Epoch19 threshold 0.8 Acc 0.9748142857142857, AUC 0.9985595941543579, avg_entr 0.007461063098162413
gc 0
Train Epoch20 Acc 0.9885857142857143 (553608/560000), AUC 0.9987710118293762
Test Epoch20 threshold 0.2 Acc 0.9753142857142857, AUC 0.9985566735267639, avg_entr 0.006992617156356573
Test Epoch20 threshold 0.4 Acc 0.9748142857142857, AUC 0.9985595941543579, avg_entr 0.007461060304194689
Test Epoch20 threshold 0.6 Acc 0.9748142857142857, AUC 0.9985595941543579, avg_entr 0.007461060304194689
Test Epoch20 threshold 0.8 Acc 0.9748142857142857, AUC 0.9985595941543579, avg_entr 0.007461060304194689
gc 0
Train Epoch21 Acc 0.9885821428571429 (553606/560000), AUC 0.9987903833389282
Test Epoch21 threshold 0.2 Acc 0.9753142857142857, AUC 0.9985566735267639, avg_entr 0.006992618087679148
Test Epoch21 threshold 0.4 Acc 0.9748142857142857, AUC 0.9985595941543579, avg_entr 0.007461059372872114
Test Epoch21 threshold 0.6 Acc 0.9748142857142857, AUC 0.9985595941543579, avg_entr 0.007461059372872114
Test Epoch21 threshold 0.8 Acc 0.9748142857142857, AUC 0.9985595941543579, avg_entr 0.007461059372872114
gc 0
Train Epoch22 Acc 0.9885392857142857 (553582/560000), AUC 0.9987698197364807
Test Epoch22 threshold 0.2 Acc 0.9753142857142857, AUC 0.9985566735267639, avg_entr 0.006992612034082413
Test Epoch22 threshold 0.4 Acc 0.9748142857142857, AUC 0.9985596537590027, avg_entr 0.007461061235517263
Test Epoch22 threshold 0.6 Acc 0.9748142857142857, AUC 0.9985596537590027, avg_entr 0.007461061235517263
Test Epoch22 threshold 0.8 Acc 0.9748142857142857, AUC 0.9985596537590027, avg_entr 0.007461061235517263
gc 0
Train Epoch23 Acc 0.9885696428571429 (553599/560000), AUC 0.9987793564796448
Test Epoch23 threshold 0.2 Acc 0.9753142857142857, AUC 0.9985566735267639, avg_entr 0.006992617156356573
Test Epoch23 threshold 0.4 Acc 0.9748142857142857, AUC 0.9985596537590027, avg_entr 0.007461062166839838
Test Epoch23 threshold 0.6 Acc 0.9748142857142857, AUC 0.9985596537590027, avg_entr 0.007461062166839838
Test Epoch23 threshold 0.8 Acc 0.9748142857142857, AUC 0.9985596537590027, avg_entr 0.007461062166839838
gc 0
Train Epoch24 Acc 0.988525 (553574/560000), AUC 0.9987735152244568
Test Epoch24 threshold 0.2 Acc 0.9753142857142857, AUC 0.9985566735267639, avg_entr 0.006992616690695286
Test Epoch24 threshold 0.4 Acc 0.9748142857142857, AUC 0.9985595941543579, avg_entr 0.007461061235517263
Test Epoch24 threshold 0.6 Acc 0.9748142857142857, AUC 0.9985595941543579, avg_entr 0.007461061235517263
Test Epoch24 threshold 0.8 Acc 0.9748142857142857, AUC 0.9985595941543579, avg_entr 0.007461061235517263
gc 0
Train Epoch25 Acc 0.9885607142857142 (553594/560000), AUC 0.9987792372703552
Test Epoch25 threshold 0.2 Acc 0.9753142857142857, AUC 0.9985566735267639, avg_entr 0.006992611102759838
Test Epoch25 threshold 0.4 Acc 0.9748142857142857, AUC 0.9985595941543579, avg_entr 0.007461061235517263
Test Epoch25 threshold 0.6 Acc 0.9748142857142857, AUC 0.9985595941543579, avg_entr 0.007461061235517263
Test Epoch25 threshold 0.8 Acc 0.9748142857142857, AUC 0.9985595941543579, avg_entr 0.007461061235517263
gc 0
Train Epoch26 Acc 0.9886107142857142 (553622/560000), AUC 0.998787522315979
Test Epoch26 threshold 0.2 Acc 0.9753142857142857, AUC 0.9985566735267639, avg_entr 0.006992616690695286
Test Epoch26 threshold 0.4 Acc 0.9748142857142857, AUC 0.9985596537590027, avg_entr 0.007461059372872114
Test Epoch26 threshold 0.6 Acc 0.9748142857142857, AUC 0.9985596537590027, avg_entr 0.007461059372872114
Test Epoch26 threshold 0.8 Acc 0.9748142857142857, AUC 0.9985596537590027, avg_entr 0.007461059372872114
gc 0
Train Epoch27 Acc 0.9886053571428571 (553619/560000), AUC 0.9987877607345581
Test Epoch27 threshold 0.2 Acc 0.9753142857142857, AUC 0.9985566735267639, avg_entr 0.006992619950324297
Test Epoch27 threshold 0.4 Acc 0.9748142857142857, AUC 0.9985595941543579, avg_entr 0.007461057975888252
Test Epoch27 threshold 0.6 Acc 0.9748142857142857, AUC 0.9985595941543579, avg_entr 0.007461057975888252
Test Epoch27 threshold 0.8 Acc 0.9748142857142857, AUC 0.9985595941543579, avg_entr 0.007461057975888252
gc 0
Train Epoch28 Acc 0.98865 (553644/560000), AUC 0.9987707138061523
Test Epoch28 threshold 0.2 Acc 0.9753142857142857, AUC 0.9985566735267639, avg_entr 0.006992599461227655
Test Epoch28 threshold 0.4 Acc 0.9748142857142857, AUC 0.9985595941543579, avg_entr 0.007461056113243103
Test Epoch28 threshold 0.6 Acc 0.9748142857142857, AUC 0.9985595941543579, avg_entr 0.007461056113243103
Test Epoch28 threshold 0.8 Acc 0.9748142857142857, AUC 0.9985595941543579, avg_entr 0.007461056113243103
gc 0
Train Epoch29 Acc 0.9885767857142858 (553603/560000), AUC 0.9987838864326477
Test Epoch29 threshold 0.2 Acc 0.9753142857142857, AUC 0.9985566735267639, avg_entr 0.006992600858211517
Test Epoch29 threshold 0.4 Acc 0.9748142857142857, AUC 0.9985596537590027, avg_entr 0.007461056113243103
Test Epoch29 threshold 0.6 Acc 0.9748142857142857, AUC 0.9985596537590027, avg_entr 0.007461056113243103
Test Epoch29 threshold 0.8 Acc 0.9748142857142857, AUC 0.9985596537590027, avg_entr 0.007461056113243103
Best AUC 0.9985600113868713
train_loss (2, 5, 30)
valid_acc (30, 4)
valid_AUC (30, 4)
train_acc (30,)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Start Testing
Load ckpt at ckpt/dbpedia_14_transformeral_l5_pad20//dbpedia_14_transformeral_l5.pt
[[4718   40   21   11   15   56   43    6    5    4    7   13   24   37]
 [  34 4911    1    2    5    0   27    5    2    2    2    0    4    5]
 [  31   10 4672   14   85    2    9    5    2    0    3   48   22   97]
 [   2    2   52 4917   17    0    2    4    0    2    0    1    0    1]
 [  10   16  103   12 4837    3    7    5    3    0    0    0    2    2]
 [  41    0    1    2    2 4943    5    1    1    2    0    0    2    0]
 [  68   51    4    2   18   21 4782   35   12    3    0    0    2    2]
 [   1    0    1    0    2    0   13 4969   11    2    0    0    1    0]
 [   1    1    1    0    1    0    9   12 4975    0    0    0    0    0]
 [   0    1    2    2    0    1    0    5    0 4949   39    0    0    1]
 [  11    1    1    0    0    0    2    5    0   34 4946    0    0    0]
 [   6    0   24    1    1    1    2    0    0    0    1 4935   20    9]
 [  11    1   19    5    2    3    1    1    1    2    0   17 4884   53]
 [  36    9   65    8    8    5    6    3    1    4    4    9   42 4800]]
Figure(640x480)
tensor([2.3702e-06, 3.7019e-05, 3.1208e-06,  ..., 8.8144e-06, 5.3637e-05,
        1.1217e-05])
[[4752   38   18    5   10   41   56    1    1    3    6   15   15   39]
 [  41 4910    2    1    7    0   27    2    1    1    1    0    1    6]
 [  24    6 4727   16   95    1    9    1    1    0    3   29   17   71]
 [   1    1   24 4947   22    0    2    1    0    0    1    0    0    1]
 [  11    6   86   13 4863    3    5    2    1    2    0    0    3    5]
 [  37    0    0    1    0 4948    8    1    1    1    0    0    3    0]
 [  64   35    2    1    6   18 4829   28    5    3    1    0    3    5]
 [   2    0    1    0    1    0   15 4967   10    2    0    0    1    1]
 [   2    1    2    0    2    0   13    8 4971    0    0    0    0    1]
 [   0    0    2    1    0    1    0    3    0 4962   30    0    0    1]
 [  12    1    1    0    0    0    3    0    0   29 4954    0    0    0]
 [   5    0   18    1    0    1    0    0    0    0    0 4949   17    9]
 [  12    0   14    1    1    1    0    0    0    1    0   18 4909   43]
 [  30    3   54    1    4    4    3    2    1    0    2   12   40 4844]]
Figure(640x480)
tensor([1.2117e-06, 1.1902e-06, 1.1310e-06,  ..., 1.1025e-06, 1.1658e-06,
        1.3423e-06])
[[4757   41   18    4    8   40   56    1    1    3    4   12   15   40]
 [  40 4911    1    1    7    0   29    2    1    1    1    0    0    6]
 [  24    5 4729   16   99    2   11    1    1    0    3   28   16   65]
 [   1    1   23 4947   22    0    2    1    0    0    1    0    1    1]
 [   9    5   78   12 4875    2    5    2    1    2    0    0    3    6]
 [  37    0    0    1    0 4949    7    1    1    1    0    0    3    0]
 [  60   35    2    1    5   18 4836   28    6    3    1    0    2    3]
 [   2    0    0    0    1    0   15 4968   10    2    0    0    1    1]
 [   2    1    2    0    2    0   13    8 4971    0    0    0    0    1]
 [   0    0    2    1    0    1    0    3    0 4963   29    0    0    1]
 [  12    1    1    0    0    0    3    1    0   29 4953    0    0    0]
 [   6    0   19    1    0    1    0    0    0    0    0 4946   18    9]
 [  13    0   13    1    1    1    0    0    0    1    0   19 4904   47]
 [  31    2   62    1    4    3    5    2    1    0    2   12   39 4836]]
Figure(640x480)
tensor([1.3751e-06, 1.3423e-06, 1.3753e-06,  ..., 1.5393e-06, 1.5355e-06,
        1.4316e-06])
[[4758   41   17    4    8   40   56    1    1    3    4   12   16   39]
 [  40 4911    1    1    7    0   29    2    1    1    1    0    0    6]
 [  23    4 4730   16   99    2   11    1    1    0    3   28   18   64]
 [   1    1   23 4947   22    0    2    1    0    0    1    0    1    1]
 [   9    5   81   12 4872    2    5    2    1    2    0    0    3    6]
 [  37    0    0    1    0 4948    8    1    1    1    0    0    3    0]
 [  60   35    2    1    4   18 4837   28    6    3    1    0    2    3]
 [   2    0    0    0    1    0   15 4968   10    2    0    0    1    1]
 [   2    1    2    0    2    0   13    8 4971    0    0    0    0    1]
 [   0    1    2    1    0    1    0    3    0 4962   29    0    0    1]
 [  12    1    1    0    0    0    3    0    0   29 4954    0    0    0]
 [   6    0   18    1    0    1    0    0    0    0    0 4947   18    9]
 [  13    0   13    1    1    1    0    0    0    1    0   19 4903   48]
 [  31    3   62    0    4    4    5    2    1    0    2   12   37 4837]]
Figure(640x480)
tensor([1.2253e-06, 1.1492e-06, 1.2089e-06,  ..., 1.8050e-06, 1.7458e-06,
        1.6506e-06])
[[4756   42   18    4    8   40   57    1    1    3    4   12   15   39]
 [  38 4913    1    1    7    0   29    2    1    1    1    0    0    6]
 [  22    4 4734   15   99    2   10    1    1    0    3   28   17   64]
 [   1    1   23 4947   22    0    2    1    0    0    1    0    1    1]
 [   9    5   80   12 4873    2    5    2    1    2    0    0    3    6]
 [  37    0    0    1    0 4948    8    1    1    1    0    0    3    0]
 [  62   35    2    1    5   18 4834   28    6    3    1    0    2    3]
 [   2    0    0    0    1    0   15 4968   10    2    0    0    1    1]
 [   2    1    2    0    1    0   13    8 4972    0    0    0    0    1]
 [   0    0    2    1    0    1    0    3    0 4963   29    0    0    1]
 [  12    1    1    0    0    0    3    1    0   29 4953    0    0    0]
 [   6    0   18    1    0    1    0    0    0    0    0 4947   18    9]
 [  13    0   13    1    1    1    0    0    0    1    0   19 4903   48]
 [  31    3   62    0    4    4    5    2    1    0    1   12   37 4838]]
Figure(640x480)
tensor([1.2821e-06, 1.2572e-06, 1.2842e-06,  ..., 1.3836e-06, 1.3884e-06,
        1.3721e-06])
