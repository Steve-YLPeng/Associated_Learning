total count words 102019
vocab size 30000
found 26754 words in glove
model: TransformerModelML(
  (layers): ModuleList(
    (0): EMBLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Embedding(30000, 300)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=4, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=4, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.weight 9000000
layers.0.ae.g.0.weight 512
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 512
layers.0.ae.h.0.bias 4
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
Total Trainable Params: 13672292
Start Training
gc 0
Train Epoch0 Acc 0.63245 (75894/120000), AUC 0.8583886623382568
Test Epoch0 threshold 0.2 Acc 0.9076315789473685, AUC 0.9764171242713928, avg_entr 0.12993228435516357
Save ckpt to ckpt/ag_news_transformeral_l5_pad100//ag_news_transformeral_l5.pt  ,ep 0
Test Epoch0 threshold 0.4 Acc 0.9077631578947368, AUC 0.9766035079956055, avg_entr 0.150177463889122
Save ckpt to ckpt/ag_news_transformeral_l5_pad100//ag_news_transformeral_l5.pt  ,ep 0
Test Epoch0 threshold 0.6 Acc 0.905, AUC 0.9773909449577332, avg_entr 0.1691867560148239
Save ckpt to ckpt/ag_news_transformeral_l5_pad100//ag_news_transformeral_l5.pt  ,ep 0
Test Epoch0 threshold 0.8 Acc 0.9048684210526315, AUC 0.9774264693260193, avg_entr 0.1695089489221573
Save ckpt to ckpt/ag_news_transformeral_l5_pad100//ag_news_transformeral_l5.pt  ,ep 0
gc 0
Train Epoch1 Acc 0.922775 (110733/120000), AUC 0.9821118116378784
Test Epoch1 threshold 0.2 Acc 0.9160526315789473, AUC 0.9793094396591187, avg_entr 0.07189697027206421
Save ckpt to ckpt/ag_news_transformeral_l5_pad100//ag_news_transformeral_l5.pt  ,ep 1
Test Epoch1 threshold 0.4 Acc 0.9160526315789473, AUC 0.9802765846252441, avg_entr 0.09231183677911758
Save ckpt to ckpt/ag_news_transformeral_l5_pad100//ag_news_transformeral_l5.pt  ,ep 1
Test Epoch1 threshold 0.6 Acc 0.9157894736842105, AUC 0.9804915189743042, avg_entr 0.10129480808973312
Save ckpt to ckpt/ag_news_transformeral_l5_pad100//ag_news_transformeral_l5.pt  ,ep 1
Test Epoch1 threshold 0.8 Acc 0.9157894736842105, AUC 0.9804902672767639, avg_entr 0.10131537169218063
gc 0
Train Epoch2 Acc 0.93635 (112362/120000), AUC 0.9877556562423706
Test Epoch2 threshold 0.2 Acc 0.9201315789473684, AUC 0.9798149466514587, avg_entr 0.04895510897040367
Test Epoch2 threshold 0.4 Acc 0.9188157894736843, AUC 0.9812320470809937, avg_entr 0.07022939622402191
Save ckpt to ckpt/ag_news_transformeral_l5_pad100//ag_news_transformeral_l5.pt  ,ep 2
Test Epoch2 threshold 0.6 Acc 0.9164473684210527, AUC 0.981336236000061, avg_entr 0.07745742052793503
Save ckpt to ckpt/ag_news_transformeral_l5_pad100//ag_news_transformeral_l5.pt  ,ep 2
Test Epoch2 threshold 0.8 Acc 0.9164473684210527, AUC 0.981336236000061, avg_entr 0.07745742052793503
Save ckpt to ckpt/ag_news_transformeral_l5_pad100//ag_news_transformeral_l5.pt  ,ep 2
gc 0
Train Epoch3 Acc 0.9448833333333333 (113386/120000), AUC 0.989837646484375
Test Epoch3 threshold 0.2 Acc 0.9203947368421053, AUC 0.9790549874305725, avg_entr 0.03875705227255821
Test Epoch3 threshold 0.4 Acc 0.9193421052631578, AUC 0.9813413023948669, avg_entr 0.05937182158231735
Save ckpt to ckpt/ag_news_transformeral_l5_pad100//ag_news_transformeral_l5.pt  ,ep 3
Test Epoch3 threshold 0.6 Acc 0.9177631578947368, AUC 0.9817726016044617, avg_entr 0.06656482070684433
Save ckpt to ckpt/ag_news_transformeral_l5_pad100//ag_news_transformeral_l5.pt  ,ep 3
Test Epoch3 threshold 0.8 Acc 0.9177631578947368, AUC 0.9817726016044617, avg_entr 0.06656482070684433
Save ckpt to ckpt/ag_news_transformeral_l5_pad100//ag_news_transformeral_l5.pt  ,ep 3
gc 0
Train Epoch4 Acc 0.9502166666666667 (114026/120000), AUC 0.9911463856697083
Test Epoch4 threshold 0.2 Acc 0.9184210526315789, AUC 0.9780025482177734, avg_entr 0.03208146244287491
Test Epoch4 threshold 0.4 Acc 0.9173684210526316, AUC 0.9813024997711182, avg_entr 0.05340809002518654
Test Epoch4 threshold 0.6 Acc 0.9167105263157894, AUC 0.9816619157791138, avg_entr 0.05984252691268921
Test Epoch4 threshold 0.8 Acc 0.9167105263157894, AUC 0.9816619157791138, avg_entr 0.05984252691268921
gc 0
Train Epoch5 Acc 0.9561416666666667 (114737/120000), AUC 0.9931044578552246
Test Epoch5 threshold 0.2 Acc 0.9147368421052632, AUC 0.9776906967163086, avg_entr 0.030478881672024727
Test Epoch5 threshold 0.4 Acc 0.9178947368421052, AUC 0.981029212474823, avg_entr 0.05001230910420418
Test Epoch5 threshold 0.6 Acc 0.9172368421052631, AUC 0.9816851019859314, avg_entr 0.05579783394932747
Test Epoch5 threshold 0.8 Acc 0.9171052631578948, AUC 0.98167884349823, avg_entr 0.055863238871097565
gc 0
Train Epoch6 Acc 0.958925 (115071/120000), AUC 0.9940074682235718
Test Epoch6 threshold 0.2 Acc 0.9168421052631579, AUC 0.9768914580345154, avg_entr 0.028636369854211807
Test Epoch6 threshold 0.4 Acc 0.9190789473684211, AUC 0.9812052249908447, avg_entr 0.047598645091056824
Test Epoch6 threshold 0.6 Acc 0.9178947368421052, AUC 0.9816599488258362, avg_entr 0.05330249294638634
Test Epoch6 threshold 0.8 Acc 0.9177631578947368, AUC 0.9816537499427795, avg_entr 0.053369149565696716
gc 0
Train Epoch7 Acc 0.960125 (115215/120000), AUC 0.9943103790283203
Test Epoch7 threshold 0.2 Acc 0.916578947368421, AUC 0.9770909547805786, avg_entr 0.02845437079668045
Test Epoch7 threshold 0.4 Acc 0.9185526315789474, AUC 0.9812110066413879, avg_entr 0.047428298741579056
Test Epoch7 threshold 0.6 Acc 0.9178947368421052, AUC 0.9816207885742188, avg_entr 0.05298645794391632
Test Epoch7 threshold 0.8 Acc 0.9177631578947368, AUC 0.9816147685050964, avg_entr 0.05306234583258629
gc 0
Train Epoch8 Acc 0.961075 (115329/120000), AUC 0.9945248961448669
Test Epoch8 threshold 0.2 Acc 0.9168421052631579, AUC 0.9770178198814392, avg_entr 0.027874233201146126
Test Epoch8 threshold 0.4 Acc 0.9189473684210526, AUC 0.9811791181564331, avg_entr 0.0464932844042778
Test Epoch8 threshold 0.6 Acc 0.9175, AUC 0.9816085696220398, avg_entr 0.05158548802137375
Test Epoch8 threshold 0.8 Acc 0.9173684210526316, AUC 0.981602668762207, avg_entr 0.05166255310177803
gc 0
Train Epoch9 Acc 0.9614833333333334 (115378/120000), AUC 0.994681715965271
Test Epoch9 threshold 0.2 Acc 0.9157894736842105, AUC 0.9767624735832214, avg_entr 0.027598638087511063
Test Epoch9 threshold 0.4 Acc 0.9188157894736843, AUC 0.9811075329780579, avg_entr 0.04591245576739311
Test Epoch9 threshold 0.6 Acc 0.9181578947368421, AUC 0.9815987348556519, avg_entr 0.05132025107741356
Test Epoch9 threshold 0.8 Acc 0.9180263157894737, AUC 0.9815927743911743, avg_entr 0.05140012875199318
gc 0
Train Epoch10 Acc 0.9616083333333333 (115393/120000), AUC 0.9946893453598022
Test Epoch10 threshold 0.2 Acc 0.9160526315789473, AUC 0.976800799369812, avg_entr 0.027359170839190483
Test Epoch10 threshold 0.4 Acc 0.9180263157894737, AUC 0.9811253547668457, avg_entr 0.04556027054786682
Test Epoch10 threshold 0.6 Acc 0.9173684210526316, AUC 0.9816040396690369, avg_entr 0.05080893635749817
Test Epoch10 threshold 0.8 Acc 0.9172368421052631, AUC 0.9815979599952698, avg_entr 0.05088824778795242
gc 0
Train Epoch11 Acc 0.9618583333333334 (115423/120000), AUC 0.9948009252548218
Test Epoch11 threshold 0.2 Acc 0.9161842105263158, AUC 0.9768308997154236, avg_entr 0.02720194309949875
Test Epoch11 threshold 0.4 Acc 0.9181578947368421, AUC 0.9811240434646606, avg_entr 0.045349493622779846
Test Epoch11 threshold 0.6 Acc 0.9173684210526316, AUC 0.9816030859947205, avg_entr 0.05053510144352913
Test Epoch11 threshold 0.8 Acc 0.9172368421052631, AUC 0.9815970659255981, avg_entr 0.05061417445540428
gc 0
Train Epoch12 Acc 0.9617 (115404/120000), AUC 0.9947282075881958
Test Epoch12 threshold 0.2 Acc 0.9163157894736842, AUC 0.976850152015686, avg_entr 0.027015715837478638
Test Epoch12 threshold 0.4 Acc 0.9181578947368421, AUC 0.9811215996742249, avg_entr 0.04519645869731903
Test Epoch12 threshold 0.6 Acc 0.9173684210526316, AUC 0.98160320520401, avg_entr 0.05042083561420441
Test Epoch12 threshold 0.8 Acc 0.9172368421052631, AUC 0.9815972447395325, avg_entr 0.05049983784556389
gc 0
Train Epoch13 Acc 0.9617333333333333 (115408/120000), AUC 0.9948263168334961
Test Epoch13 threshold 0.2 Acc 0.916578947368421, AUC 0.976854681968689, avg_entr 0.027039535343647003
Test Epoch13 threshold 0.4 Acc 0.9184210526315789, AUC 0.9811811447143555, avg_entr 0.04513638839125633
Test Epoch13 threshold 0.6 Acc 0.9177631578947368, AUC 0.981603741645813, avg_entr 0.05034264549612999
Test Epoch13 threshold 0.8 Acc 0.9176315789473685, AUC 0.9815977215766907, avg_entr 0.050421640276908875
gc 0
Train Epoch14 Acc 0.9617083333333334 (115405/120000), AUC 0.9948903918266296
Test Epoch14 threshold 0.2 Acc 0.916578947368421, AUC 0.9768116474151611, avg_entr 0.02690885402262211
Test Epoch14 threshold 0.4 Acc 0.9184210526315789, AUC 0.9811215996742249, avg_entr 0.045002978295087814
Test Epoch14 threshold 0.6 Acc 0.9176315789473685, AUC 0.9816042184829712, avg_entr 0.0502360463142395
Test Epoch14 threshold 0.8 Acc 0.9175, AUC 0.9815981388092041, avg_entr 0.05031507462263107
gc 0
Train Epoch15 Acc 0.961625 (115395/120000), AUC 0.9948232173919678
Test Epoch15 threshold 0.2 Acc 0.9167105263157894, AUC 0.9769205451011658, avg_entr 0.02691752463579178
Test Epoch15 threshold 0.4 Acc 0.9185526315789474, AUC 0.9811743497848511, avg_entr 0.045030009001493454
Test Epoch15 threshold 0.6 Acc 0.9178947368421052, AUC 0.9816046357154846, avg_entr 0.05019110441207886
Test Epoch15 threshold 0.8 Acc 0.9177631578947368, AUC 0.9815986156463623, avg_entr 0.05027010291814804
gc 0
Train Epoch16 Acc 0.961925 (115431/120000), AUC 0.9947534203529358
Test Epoch16 threshold 0.2 Acc 0.9167105263157894, AUC 0.9769068360328674, avg_entr 0.026913711801171303
Test Epoch16 threshold 0.4 Acc 0.9185526315789474, AUC 0.981173574924469, avg_entr 0.04499194398522377
Test Epoch16 threshold 0.6 Acc 0.9178947368421052, AUC 0.9816041588783264, avg_entr 0.050153233110904694
Test Epoch16 threshold 0.8 Acc 0.9177631578947368, AUC 0.9815981388092041, avg_entr 0.05023222789168358
gc 0
Train Epoch17 Acc 0.9615333333333334 (115384/120000), AUC 0.9949439167976379
Test Epoch17 threshold 0.2 Acc 0.9167105263157894, AUC 0.9769133925437927, avg_entr 0.026869630441069603
Test Epoch17 threshold 0.4 Acc 0.9185526315789474, AUC 0.981173574924469, avg_entr 0.04497184231877327
Test Epoch17 threshold 0.6 Acc 0.9178947368421052, AUC 0.9816040992736816, avg_entr 0.05013345927000046
Test Epoch17 threshold 0.8 Acc 0.9177631578947368, AUC 0.9815980792045593, avg_entr 0.05021244287490845
gc 0
Train Epoch18 Acc 0.9616416666666666 (115397/120000), AUC 0.9948890209197998
Test Epoch18 threshold 0.2 Acc 0.9167105263157894, AUC 0.9769129753112793, avg_entr 0.026861343532800674
Test Epoch18 threshold 0.4 Acc 0.9185526315789474, AUC 0.9811765551567078, avg_entr 0.04492570459842682
Test Epoch18 threshold 0.6 Acc 0.9177631578947368, AUC 0.9816044569015503, avg_entr 0.05012310668826103
Test Epoch18 threshold 0.8 Acc 0.9176315789473685, AUC 0.981598436832428, avg_entr 0.05020209029316902
gc 0
Train Epoch19 Acc 0.9616083333333333 (115393/120000), AUC 0.9948612451553345
Test Epoch19 threshold 0.2 Acc 0.9167105263157894, AUC 0.976912260055542, avg_entr 0.02685578353703022
Test Epoch19 threshold 0.4 Acc 0.9185526315789474, AUC 0.9811762571334839, avg_entr 0.04492023214697838
Test Epoch19 threshold 0.6 Acc 0.9177631578947368, AUC 0.9816043376922607, avg_entr 0.05011781305074692
Test Epoch19 threshold 0.8 Acc 0.9176315789473685, AUC 0.9815982580184937, avg_entr 0.05019679665565491
gc 0
Train Epoch20 Acc 0.96175 (115410/120000), AUC 0.9947776794433594
Test Epoch20 threshold 0.2 Acc 0.9167105263157894, AUC 0.9769119024276733, avg_entr 0.02685500495135784
Test Epoch20 threshold 0.4 Acc 0.9185526315789474, AUC 0.9811761975288391, avg_entr 0.04491857439279556
Test Epoch20 threshold 0.6 Acc 0.9177631578947368, AUC 0.9816043972969055, avg_entr 0.05011636018753052
Test Epoch20 threshold 0.8 Acc 0.9176315789473685, AUC 0.9815983176231384, avg_entr 0.050195347517728806
gc 0
Train Epoch21 Acc 0.961725 (115407/120000), AUC 0.9948499202728271
Test Epoch21 threshold 0.2 Acc 0.9167105263157894, AUC 0.9769112467765808, avg_entr 0.02685265801846981
Test Epoch21 threshold 0.4 Acc 0.9185526315789474, AUC 0.9811761379241943, avg_entr 0.044916506856679916
Test Epoch21 threshold 0.6 Acc 0.9177631578947368, AUC 0.9816043972969055, avg_entr 0.0501144640147686
Test Epoch21 threshold 0.8 Acc 0.9176315789473685, AUC 0.981598436832428, avg_entr 0.050193458795547485
gc 0
Train Epoch22 Acc 0.9616916666666666 (115403/120000), AUC 0.9947253465652466
Test Epoch22 threshold 0.2 Acc 0.9167105263157894, AUC 0.976910412311554, avg_entr 0.026850827038288116
Test Epoch22 threshold 0.4 Acc 0.9185526315789474, AUC 0.9811762571334839, avg_entr 0.04491446167230606
Test Epoch22 threshold 0.6 Acc 0.9177631578947368, AUC 0.9816045761108398, avg_entr 0.050112564116716385
Test Epoch22 threshold 0.8 Acc 0.9176315789473685, AUC 0.9815986156463623, avg_entr 0.05019155517220497
gc 0
Train Epoch23 Acc 0.961775 (115413/120000), AUC 0.9948060512542725
Test Epoch23 threshold 0.2 Acc 0.9167105263157894, AUC 0.9769093990325928, avg_entr 0.026848845183849335
Test Epoch23 threshold 0.4 Acc 0.9185526315789474, AUC 0.9811760187149048, avg_entr 0.044912490993738174
Test Epoch23 threshold 0.6 Acc 0.9177631578947368, AUC 0.9816044569015503, avg_entr 0.050110749900341034
Test Epoch23 threshold 0.8 Acc 0.9176315789473685, AUC 0.9815983772277832, avg_entr 0.05018974468111992
gc 0
Train Epoch24 Acc 0.96155 (115386/120000), AUC 0.994713544845581
Test Epoch24 threshold 0.2 Acc 0.9167105263157894, AUC 0.9769077897071838, avg_entr 0.026850463822484016
Test Epoch24 threshold 0.4 Acc 0.9185526315789474, AUC 0.9811757802963257, avg_entr 0.0449104905128479
Test Epoch24 threshold 0.6 Acc 0.9177631578947368, AUC 0.9816044569015503, avg_entr 0.05010875687003136
Test Epoch24 threshold 0.8 Acc 0.9176315789473685, AUC 0.9815983772277832, avg_entr 0.05018774792551994
gc 0
Train Epoch25 Acc 0.9619666666666666 (115436/120000), AUC 0.9947223663330078
Test Epoch25 threshold 0.2 Acc 0.9167105263157894, AUC 0.9769076108932495, avg_entr 0.026850618422031403
Test Epoch25 threshold 0.4 Acc 0.9185526315789474, AUC 0.9811757206916809, avg_entr 0.04490848258137703
Test Epoch25 threshold 0.6 Acc 0.9177631578947368, AUC 0.9816044569015503, avg_entr 0.050106946378946304
Test Epoch25 threshold 0.8 Acc 0.9176315789473685, AUC 0.9815984964370728, avg_entr 0.05018594115972519
gc 0
Train Epoch26 Acc 0.961875 (115425/120000), AUC 0.9948862195014954
Test Epoch26 threshold 0.2 Acc 0.9167105263157894, AUC 0.9769070148468018, avg_entr 0.026849186047911644
Test Epoch26 threshold 0.4 Acc 0.9185526315789474, AUC 0.9811756610870361, avg_entr 0.044906508177518845
Test Epoch26 threshold 0.6 Acc 0.9177631578947368, AUC 0.9816046953201294, avg_entr 0.050105080008506775
Test Epoch26 threshold 0.8 Acc 0.9176315789473685, AUC 0.9815986156463623, avg_entr 0.05018407478928566
gc 0
Train Epoch27 Acc 0.96185 (115422/120000), AUC 0.9947357177734375
Test Epoch27 threshold 0.2 Acc 0.9167105263157894, AUC 0.9769059419631958, avg_entr 0.026846952736377716
Test Epoch27 threshold 0.4 Acc 0.9185526315789474, AUC 0.981175422668457, avg_entr 0.04490450769662857
Test Epoch27 threshold 0.6 Acc 0.9177631578947368, AUC 0.9816045761108398, avg_entr 0.05010315030813217
Test Epoch27 threshold 0.8 Acc 0.9176315789473685, AUC 0.9815984964370728, avg_entr 0.050182148814201355
gc 0
Train Epoch28 Acc 0.9619 (115428/120000), AUC 0.9947553873062134
Test Epoch28 threshold 0.2 Acc 0.9167105263157894, AUC 0.9769052267074585, avg_entr 0.026844898238778114
Test Epoch28 threshold 0.4 Acc 0.9185526315789474, AUC 0.9811752438545227, avg_entr 0.044902507215738297
Test Epoch28 threshold 0.6 Acc 0.9177631578947368, AUC 0.9816044569015503, avg_entr 0.050101324915885925
Test Epoch28 threshold 0.8 Acc 0.9176315789473685, AUC 0.981598436832428, avg_entr 0.05018031969666481
gc 0
Train Epoch29 Acc 0.9618916666666667 (115427/120000), AUC 0.9946722388267517
Test Epoch29 threshold 0.2 Acc 0.9167105263157894, AUC 0.9769047498703003, avg_entr 0.026843370869755745
Test Epoch29 threshold 0.4 Acc 0.9185526315789474, AUC 0.9811753034591675, avg_entr 0.0449005626142025
Test Epoch29 threshold 0.6 Acc 0.9177631578947368, AUC 0.9816045761108398, avg_entr 0.05009959265589714
Test Epoch29 threshold 0.8 Acc 0.9176315789473685, AUC 0.9815985560417175, avg_entr 0.05017858371138573
Best AUC 0.9817726016044617
train_loss (2, 5, 30)
valid_acc (30, 4)
valid_AUC (30, 4)
train_acc (30,)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Start Testing
Load ckpt at ckpt/ag_news_transformeral_l5_pad100//ag_news_transformeral_l5.pt
[[1703   62   89   46]
 [  14 1872    6    8]
 [  40   19 1700  141]
 [  47   18  135 1700]]
Figure(640x480)
tensor([0.0713, 0.0021, 0.0447,  ..., 0.0596, 0.0048, 0.0798])
[[1710   60   72   58]
 [  11 1872    9    8]
 [  40   17 1702  141]
 [  41   13  133 1713]]
Figure(640x480)
tensor([0.0077, 0.0025, 0.0023,  ..., 0.0043, 0.0020, 0.0025])
[[1714   57   69   60]
 [  12 1871    9    8]
 [  42   15 1703  140]
 [  39   12  135 1714]]
Figure(640x480)
tensor([0.0054, 0.0029, 0.0031,  ..., 0.0031, 0.0018, 0.0025])
[[1714   58   70   58]
 [  11 1872    9    8]
 [  42   16 1705  137]
 [  39   12  139 1710]]
Figure(640x480)
tensor([0.0066, 0.0028, 0.0032,  ..., 0.0029, 0.0021, 0.0025])
[[1717   58   70   55]
 [  11 1872    9    8]
 [  43   16 1705  136]
 [  42   12  139 1707]]
Figure(640x480)
tensor([0.0044, 0.0029, 0.0032,  ..., 0.0029, 0.0020, 0.0024])
