total count words 222751
vocab size 30000
found 27937 words in glove
model: TransformerModelML(
  (layers): ModuleList(
    (0): EMBLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Embedding(30000, 300)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=2, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=2, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.weight 9000000
layers.0.ae.g.0.weight 256
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 256
layers.0.ae.h.0.bias 2
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
Total Trainable Params: 13671778
Start Training
gc 0
Train Epoch0 Acc 0.494475 (19779/40000), AUC 0.48175936937332153
Test Epoch0 threshold 0.2 Acc 0.6343, AUC 0.8554565906524658, avg_entr 0.7759445905685425
Save ckpt to ckpt/imdb_transformeral_l5_pad700//imdb_transformeral_l5.pt  ,ep 0
Test Epoch0 threshold 0.4 Acc 0.709, AUC 0.8665196895599365, avg_entr 0.6411990523338318
Save ckpt to ckpt/imdb_transformeral_l5_pad700//imdb_transformeral_l5.pt  ,ep 0
Test Epoch0 threshold 0.6 Acc 0.766, AUC 0.8762145042419434, avg_entr 0.5787163376808167
Save ckpt to ckpt/imdb_transformeral_l5_pad700//imdb_transformeral_l5.pt  ,ep 0
Test Epoch0 threshold 0.8 Acc 0.8044, AUC 0.8803040385246277, avg_entr 0.5931678414344788
Save ckpt to ckpt/imdb_transformeral_l5_pad700//imdb_transformeral_l5.pt  ,ep 0
gc 0
Train Epoch1 Acc 0.850125 (34005/40000), AUC 0.9200749397277832
Test Epoch1 threshold 0.2 Acc 0.8572, AUC 0.9462218284606934, avg_entr 0.20813727378845215
Save ckpt to ckpt/imdb_transformeral_l5_pad700//imdb_transformeral_l5.pt  ,ep 1
Test Epoch1 threshold 0.4 Acc 0.8635, AUC 0.9420390129089355, avg_entr 0.24094361066818237
Test Epoch1 threshold 0.6 Acc 0.8714, AUC 0.939520001411438, avg_entr 0.29141923785209656
Test Epoch1 threshold 0.8 Acc 0.8791, AUC 0.9398605823516846, avg_entr 0.3559117317199707
gc 0
Train Epoch2 Acc 0.905 (36200/40000), AUC 0.9642877578735352
Test Epoch2 threshold 0.2 Acc 0.8846, AUC 0.9504653215408325, avg_entr 0.12826749682426453
Save ckpt to ckpt/imdb_transformeral_l5_pad700//imdb_transformeral_l5.pt  ,ep 2
Test Epoch2 threshold 0.4 Acc 0.8858, AUC 0.9477376937866211, avg_entr 0.17243686318397522
Test Epoch2 threshold 0.6 Acc 0.8868, AUC 0.9492897987365723, avg_entr 0.2213716357946396
Test Epoch2 threshold 0.8 Acc 0.8899, AUC 0.9516584873199463, avg_entr 0.26724717020988464
Save ckpt to ckpt/imdb_transformeral_l5_pad700//imdb_transformeral_l5.pt  ,ep 2
gc 0
Train Epoch3 Acc 0.933575 (37343/40000), AUC 0.977505087852478
Test Epoch3 threshold 0.2 Acc 0.9006, AUC 0.948798656463623, avg_entr 0.0917641669511795
Test Epoch3 threshold 0.4 Acc 0.9021, AUC 0.9458692073822021, avg_entr 0.13353011012077332
Test Epoch3 threshold 0.6 Acc 0.902, AUC 0.9462939500808716, avg_entr 0.17424902319908142
Test Epoch3 threshold 0.8 Acc 0.9026, AUC 0.9500510692596436, avg_entr 0.21589025855064392
gc 0
Train Epoch4 Acc 0.9488 (37952/40000), AUC 0.9852374792098999
Test Epoch4 threshold 0.2 Acc 0.8984, AUC 0.9434722661972046, avg_entr 0.08096931874752045
Test Epoch4 threshold 0.4 Acc 0.8997, AUC 0.9409985542297363, avg_entr 0.11838330328464508
Test Epoch4 threshold 0.6 Acc 0.9006, AUC 0.9425048828125, avg_entr 0.15464209020137787
Test Epoch4 threshold 0.8 Acc 0.902, AUC 0.9470175504684448, avg_entr 0.19448839128017426
gc 0
Train Epoch5 Acc 0.95215 (38086/40000), AUC 0.9873104095458984
Test Epoch5 threshold 0.2 Acc 0.8977, AUC 0.9421995878219604, avg_entr 0.0778011605143547
Test Epoch5 threshold 0.4 Acc 0.8993, AUC 0.9387369155883789, avg_entr 0.11156120896339417
Test Epoch5 threshold 0.6 Acc 0.9007, AUC 0.9403598308563232, avg_entr 0.147248312830925
Test Epoch5 threshold 0.8 Acc 0.9035, AUC 0.944259762763977, avg_entr 0.18676070868968964
gc 0
Train Epoch6 Acc 0.9548 (38192/40000), AUC 0.9880345463752747
Test Epoch6 threshold 0.2 Acc 0.897, AUC 0.9401980638504028, avg_entr 0.07506703585386276
Test Epoch6 threshold 0.4 Acc 0.8988, AUC 0.9366027116775513, avg_entr 0.10951016843318939
Test Epoch6 threshold 0.6 Acc 0.9006, AUC 0.9399645924568176, avg_entr 0.1437794715166092
Test Epoch6 threshold 0.8 Acc 0.9021, AUC 0.9447765350341797, avg_entr 0.1829051375389099
gc 0
Train Epoch7 Acc 0.95565 (38226/40000), AUC 0.9884492754936218
Test Epoch7 threshold 0.2 Acc 0.8969, AUC 0.938351035118103, avg_entr 0.07251639664173126
Test Epoch7 threshold 0.4 Acc 0.8983, AUC 0.9358541965484619, avg_entr 0.10694238543510437
Test Epoch7 threshold 0.6 Acc 0.9003, AUC 0.9376935958862305, avg_entr 0.14072227478027344
Test Epoch7 threshold 0.8 Acc 0.9004, AUC 0.9440221190452576, avg_entr 0.17982825636863708
gc 0
Train Epoch8 Acc 0.955775 (38231/40000), AUC 0.9884278774261475
Test Epoch8 threshold 0.2 Acc 0.8963, AUC 0.9382097125053406, avg_entr 0.0721263587474823
Test Epoch8 threshold 0.4 Acc 0.8978, AUC 0.9361525177955627, avg_entr 0.10591806471347809
Test Epoch8 threshold 0.6 Acc 0.9001, AUC 0.9378443956375122, avg_entr 0.14020554721355438
Test Epoch8 threshold 0.8 Acc 0.9009, AUC 0.9440076351165771, avg_entr 0.17741188406944275
gc 0
Train Epoch9 Acc 0.9565 (38260/40000), AUC 0.9887720346450806
Test Epoch9 threshold 0.2 Acc 0.8973, AUC 0.9380007982254028, avg_entr 0.07120496034622192
Test Epoch9 threshold 0.4 Acc 0.8986, AUC 0.9356982707977295, avg_entr 0.10544344782829285
Test Epoch9 threshold 0.6 Acc 0.9005, AUC 0.9378257989883423, avg_entr 0.13815636932849884
Test Epoch9 threshold 0.8 Acc 0.901, AUC 0.9434750080108643, avg_entr 0.1777370125055313
gc 0
Train Epoch10 Acc 0.956525 (38261/40000), AUC 0.9887354373931885
Test Epoch10 threshold 0.2 Acc 0.8972, AUC 0.9379539489746094, avg_entr 0.07123736292123795
Test Epoch10 threshold 0.4 Acc 0.8987, AUC 0.9358245134353638, avg_entr 0.10532324016094208
Test Epoch10 threshold 0.6 Acc 0.9007, AUC 0.9379278421401978, avg_entr 0.13801008462905884
Test Epoch10 threshold 0.8 Acc 0.9011, AUC 0.9434327483177185, avg_entr 0.17750048637390137
gc 0
Train Epoch11 Acc 0.95625 (38250/40000), AUC 0.9885921478271484
Test Epoch11 threshold 0.2 Acc 0.8973, AUC 0.937895655632019, avg_entr 0.07121626287698746
Test Epoch11 threshold 0.4 Acc 0.8987, AUC 0.9353993535041809, avg_entr 0.10540725290775299
Test Epoch11 threshold 0.6 Acc 0.9006, AUC 0.9375237226486206, avg_entr 0.13809320330619812
Test Epoch11 threshold 0.8 Acc 0.9013, AUC 0.9435237646102905, avg_entr 0.1776060312986374
gc 0
Train Epoch12 Acc 0.956525 (38261/40000), AUC 0.9888206124305725
Test Epoch12 threshold 0.2 Acc 0.8973, AUC 0.9379377961158752, avg_entr 0.07116173207759857
Test Epoch12 threshold 0.4 Acc 0.8987, AUC 0.935269296169281, avg_entr 0.10526952892541885
Test Epoch12 threshold 0.6 Acc 0.9006, AUC 0.937606692314148, avg_entr 0.1382112056016922
Test Epoch12 threshold 0.8 Acc 0.9011, AUC 0.943450927734375, avg_entr 0.17728197574615479
gc 0
Train Epoch13 Acc 0.95645 (38258/40000), AUC 0.9887822866439819
Test Epoch13 threshold 0.2 Acc 0.8973, AUC 0.9378933906555176, avg_entr 0.07115334272384644
Test Epoch13 threshold 0.4 Acc 0.8987, AUC 0.935352087020874, avg_entr 0.1053231805562973
Test Epoch13 threshold 0.6 Acc 0.9006, AUC 0.9376087784767151, avg_entr 0.13808102905750275
Test Epoch13 threshold 0.8 Acc 0.9011, AUC 0.9434391260147095, avg_entr 0.17722027003765106
gc 0
Train Epoch14 Acc 0.9568 (38272/40000), AUC 0.9888437986373901
Test Epoch14 threshold 0.2 Acc 0.8972, AUC 0.9379187822341919, avg_entr 0.07113785296678543
Test Epoch14 threshold 0.4 Acc 0.8986, AUC 0.9352596402168274, avg_entr 0.10518776625394821
Test Epoch14 threshold 0.6 Acc 0.9005, AUC 0.9375836849212646, avg_entr 0.13817690312862396
Test Epoch14 threshold 0.8 Acc 0.901, AUC 0.9434386491775513, avg_entr 0.17718109488487244
gc 0
Train Epoch15 Acc 0.9569 (38276/40000), AUC 0.9889158010482788
Test Epoch15 threshold 0.2 Acc 0.8972, AUC 0.9379395842552185, avg_entr 0.07110600918531418
Test Epoch15 threshold 0.4 Acc 0.8986, AUC 0.935267448425293, avg_entr 0.10515294224023819
Test Epoch15 threshold 0.6 Acc 0.9006, AUC 0.9376953840255737, avg_entr 0.13813555240631104
Test Epoch15 threshold 0.8 Acc 0.901, AUC 0.943412184715271, avg_entr 0.17715059220790863
gc 0
Train Epoch16 Acc 0.956425 (38257/40000), AUC 0.9886167645454407
Test Epoch16 threshold 0.2 Acc 0.8972, AUC 0.9379366636276245, avg_entr 0.0710982158780098
Test Epoch16 threshold 0.4 Acc 0.8986, AUC 0.935265064239502, avg_entr 0.10514235496520996
Test Epoch16 threshold 0.6 Acc 0.9006, AUC 0.9376794099807739, avg_entr 0.1381828635931015
Test Epoch16 threshold 0.8 Acc 0.901, AUC 0.9434114694595337, avg_entr 0.17713934183120728
gc 0
Train Epoch17 Acc 0.95675 (38270/40000), AUC 0.988885760307312
Test Epoch17 threshold 0.2 Acc 0.8972, AUC 0.9379339218139648, avg_entr 0.07109234482049942
Test Epoch17 threshold 0.4 Acc 0.8986, AUC 0.9352622032165527, avg_entr 0.10513472557067871
Test Epoch17 threshold 0.6 Acc 0.9006, AUC 0.9377143383026123, avg_entr 0.13814185559749603
Test Epoch17 threshold 0.8 Acc 0.901, AUC 0.943425714969635, avg_entr 0.17705146968364716
gc 0
Train Epoch18 Acc 0.95685 (38274/40000), AUC 0.988831639289856
Test Epoch18 threshold 0.2 Acc 0.8972, AUC 0.9379359483718872, avg_entr 0.07109051197767258
Test Epoch18 threshold 0.4 Acc 0.8986, AUC 0.9352630376815796, avg_entr 0.10513732582330704
Test Epoch18 threshold 0.6 Acc 0.9006, AUC 0.9376751184463501, avg_entr 0.13819414377212524
Test Epoch18 threshold 0.8 Acc 0.901, AUC 0.9434102177619934, avg_entr 0.17713020741939545
gc 0
Train Epoch19 Acc 0.9565 (38260/40000), AUC 0.9889864921569824
Test Epoch19 threshold 0.2 Acc 0.8972, AUC 0.9379323720932007, avg_entr 0.07108621299266815
Test Epoch19 threshold 0.4 Acc 0.8986, AUC 0.9352600574493408, avg_entr 0.10512883216142654
Test Epoch19 threshold 0.6 Acc 0.9006, AUC 0.9376721382141113, avg_entr 0.13818947970867157
Test Epoch19 threshold 0.8 Acc 0.901, AUC 0.9434082508087158, avg_entr 0.17712676525115967
gc 0
Train Epoch20 Acc 0.9567 (38268/40000), AUC 0.9886918067932129
Test Epoch20 threshold 0.2 Acc 0.8972, AUC 0.9379311800003052, avg_entr 0.07108466327190399
Test Epoch20 threshold 0.4 Acc 0.8986, AUC 0.9352593421936035, avg_entr 0.10512615740299225
Test Epoch20 threshold 0.6 Acc 0.9006, AUC 0.9376853704452515, avg_entr 0.13812649250030518
Test Epoch20 threshold 0.8 Acc 0.901, AUC 0.9434075951576233, avg_entr 0.17712372541427612
gc 0
Train Epoch21 Acc 0.9562 (38248/40000), AUC 0.9887925386428833
Test Epoch21 threshold 0.2 Acc 0.8972, AUC 0.9379327297210693, avg_entr 0.07108402252197266
Test Epoch21 threshold 0.4 Acc 0.8986, AUC 0.9352573156356812, avg_entr 0.10512827336788177
Test Epoch21 threshold 0.6 Acc 0.9006, AUC 0.9376723766326904, avg_entr 0.1381904035806656
Test Epoch21 threshold 0.8 Acc 0.901, AUC 0.9434073567390442, avg_entr 0.17712324857711792
gc 0
Train Epoch22 Acc 0.9565 (38260/40000), AUC 0.988772988319397
Test Epoch22 threshold 0.2 Acc 0.8972, AUC 0.9379262328147888, avg_entr 0.07110675424337387
Test Epoch22 threshold 0.4 Acc 0.8986, AUC 0.9351787567138672, avg_entr 0.10506263375282288
Test Epoch22 threshold 0.6 Acc 0.9006, AUC 0.9376479387283325, avg_entr 0.13812591135501862
Test Epoch22 threshold 0.8 Acc 0.901, AUC 0.9434047937393188, avg_entr 0.17713816463947296
gc 0
Train Epoch23 Acc 0.956625 (38265/40000), AUC 0.9887117147445679
Test Epoch23 threshold 0.2 Acc 0.8972, AUC 0.9379215240478516, avg_entr 0.07111570239067078
Test Epoch23 threshold 0.4 Acc 0.8986, AUC 0.9351773858070374, avg_entr 0.10506080090999603
Test Epoch23 threshold 0.6 Acc 0.9006, AUC 0.937653660774231, avg_entr 0.1380949467420578
Test Epoch23 threshold 0.8 Acc 0.901, AUC 0.9434001445770264, avg_entr 0.1771302968263626
gc 0
Train Epoch24 Acc 0.95645 (38258/40000), AUC 0.9887261390686035
Test Epoch24 threshold 0.2 Acc 0.8972, AUC 0.937921941280365, avg_entr 0.07109913229942322
Test Epoch24 threshold 0.4 Acc 0.8986, AUC 0.9352174997329712, avg_entr 0.10508602112531662
Test Epoch24 threshold 0.6 Acc 0.9006, AUC 0.9376437067985535, avg_entr 0.13812188804149628
Test Epoch24 threshold 0.8 Acc 0.901, AUC 0.9434058666229248, avg_entr 0.17711487412452698
gc 0
Train Epoch25 Acc 0.956625 (38265/40000), AUC 0.9887897968292236
Test Epoch25 threshold 0.2 Acc 0.8972, AUC 0.9379167556762695, avg_entr 0.07109362632036209
Test Epoch25 threshold 0.4 Acc 0.8986, AUC 0.9352132081985474, avg_entr 0.10507865995168686
Test Epoch25 threshold 0.6 Acc 0.9006, AUC 0.9376325011253357, avg_entr 0.13814528286457062
Test Epoch25 threshold 0.8 Acc 0.901, AUC 0.943402886390686, avg_entr 0.17711034417152405
gc 0
Train Epoch26 Acc 0.956325 (38253/40000), AUC 0.9887479543685913
Test Epoch26 threshold 0.2 Acc 0.8972, AUC 0.9379119873046875, avg_entr 0.07108473032712936
Test Epoch26 threshold 0.4 Acc 0.8986, AUC 0.9352465271949768, avg_entr 0.10510965436697006
Test Epoch26 threshold 0.6 Acc 0.9006, AUC 0.9376434087753296, avg_entr 0.1381138265132904
Test Epoch26 threshold 0.8 Acc 0.901, AUC 0.9434019327163696, avg_entr 0.17710688710212708
gc 0
Train Epoch27 Acc 0.956925 (38277/40000), AUC 0.9887100458145142
Test Epoch27 threshold 0.2 Acc 0.8972, AUC 0.9379131197929382, avg_entr 0.07108651846647263
Test Epoch27 threshold 0.4 Acc 0.8986, AUC 0.9352093935012817, avg_entr 0.10507092624902725
Test Epoch27 threshold 0.6 Acc 0.9006, AUC 0.9376367926597595, avg_entr 0.13810844719409943
Test Epoch27 threshold 0.8 Acc 0.901, AUC 0.9433977007865906, avg_entr 0.17712150514125824
gc 0
Train Epoch28 Acc 0.9564 (38256/40000), AUC 0.9887347221374512
Test Epoch28 threshold 0.2 Acc 0.8972, AUC 0.9379150867462158, avg_entr 0.07108623534440994
Test Epoch28 threshold 0.4 Acc 0.8986, AUC 0.9351670742034912, avg_entr 0.10503901541233063
Test Epoch28 threshold 0.6 Acc 0.9006, AUC 0.9376385807991028, avg_entr 0.13810986280441284
Test Epoch28 threshold 0.8 Acc 0.901, AUC 0.9433940649032593, avg_entr 0.1771133542060852
gc 0
Train Epoch29 Acc 0.9566 (38264/40000), AUC 0.9887553453445435
Test Epoch29 threshold 0.2 Acc 0.8972, AUC 0.9379109144210815, avg_entr 0.07108166068792343
Test Epoch29 threshold 0.4 Acc 0.8986, AUC 0.9352067708969116, avg_entr 0.10506396740674973
Test Epoch29 threshold 0.6 Acc 0.9006, AUC 0.9376339912414551, avg_entr 0.13810065388679504
Test Epoch29 threshold 0.8 Acc 0.901, AUC 0.9433958530426025, avg_entr 0.177114337682724
Best AUC 0.9516584873199463
train_loss (2, 5, 30)
valid_acc (30, 4)
valid_AUC (30, 4)
train_acc (30,)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Start Testing
Load ckpt at ckpt/imdb_transformeral_l5_pad700//imdb_transformeral_l5.pt
[[4675  295]
 [ 862 4168]]
Figure(640x480)
tensor([0.4582, 0.0708, 0.4568,  ..., 0.1486, 0.3592, 0.6449])
[[4728  242]
 [ 864 4166]]
Figure(640x480)
tensor([0.0312, 0.0384, 0.1602,  ..., 0.2765, 0.0682, 0.6661])
[[4734  236]
 [ 870 4160]]
Figure(640x480)
tensor([0.0124, 0.0073, 0.0390,  ..., 0.1674, 0.0154, 0.3265])
[[4750  220]
 [ 923 4107]]
Figure(640x480)
tensor([0.0081, 0.0074, 0.0189,  ..., 0.0524, 0.0106, 0.2024])
[[4754  216]
 [ 951 4079]]
Figure(640x480)
tensor([0.0069, 0.0064, 0.0140,  ..., 0.0264, 0.0092, 0.1432])
