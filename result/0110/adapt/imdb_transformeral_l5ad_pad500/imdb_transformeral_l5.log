total count words 222751
vocab size 30000
found 27937 words in glove
model: TransformerModelML(
  (layers): ModuleList(
    (0): EMBLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Embedding(30000, 300)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=2, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=2, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.weight 9000000
layers.0.ae.g.0.weight 256
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 256
layers.0.ae.h.0.bias 2
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
Total Trainable Params: 13671778
Start Training
gc 0
Train Epoch0 Acc 0.466575 (18663/40000), AUC 0.46307870745658875
Test Epoch0 threshold 0.2 Acc 0.5793, AUC 0.6634165048599243, avg_entr 0.800740659236908
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 0
Test Epoch0 threshold 0.4 Acc 0.6881, AUC 0.8013336658477783, avg_entr 0.6734381318092346
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 0
Test Epoch0 threshold 0.6 Acc 0.7631, AUC 0.8698362112045288, avg_entr 0.6140144467353821
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 0
Test Epoch0 threshold 0.8 Acc 0.8084, AUC 0.8894361853599548, avg_entr 0.6380762457847595
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 0
gc 0
Train Epoch1 Acc 0.822825 (32913/40000), AUC 0.9071053266525269
Test Epoch1 threshold 0.2 Acc 0.8664, AUC 0.9501328468322754, avg_entr 0.20282313227653503
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 1
Test Epoch1 threshold 0.4 Acc 0.8695, AUC 0.9452583193778992, avg_entr 0.24151994287967682
Test Epoch1 threshold 0.6 Acc 0.8719, AUC 0.9412976503372192, avg_entr 0.2897774279117584
Test Epoch1 threshold 0.8 Acc 0.8786, AUC 0.9390603303909302, avg_entr 0.34762483835220337
gc 0
Train Epoch2 Acc 0.913325 (36533/40000), AUC 0.9670535922050476
Test Epoch2 threshold 0.2 Acc 0.9014, AUC 0.9529018402099609, avg_entr 0.11935168504714966
Save ckpt to ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt  ,ep 2
Test Epoch2 threshold 0.4 Acc 0.9015, AUC 0.9501413106918335, avg_entr 0.16586440801620483
Test Epoch2 threshold 0.6 Acc 0.9018, AUC 0.9489821195602417, avg_entr 0.21170102059841156
Test Epoch2 threshold 0.8 Acc 0.9027, AUC 0.9521393775939941, avg_entr 0.2576583921909332
gc 0
Train Epoch3 Acc 0.93955 (37582/40000), AUC 0.9806587100028992
Test Epoch3 threshold 0.2 Acc 0.8977, AUC 0.9453945159912109, avg_entr 0.09057841449975967
Test Epoch3 threshold 0.4 Acc 0.8986, AUC 0.9425212144851685, avg_entr 0.12865637242794037
Test Epoch3 threshold 0.6 Acc 0.8993, AUC 0.9443584084510803, avg_entr 0.16743655502796173
Test Epoch3 threshold 0.8 Acc 0.8996, AUC 0.9470568895339966, avg_entr 0.2070595771074295
gc 0
Train Epoch4 Acc 0.952675 (38107/40000), AUC 0.9855395555496216
Test Epoch4 threshold 0.2 Acc 0.8967, AUC 0.941801905632019, avg_entr 0.08236490190029144
Test Epoch4 threshold 0.4 Acc 0.8973, AUC 0.9385001063346863, avg_entr 0.11563343554735184
Test Epoch4 threshold 0.6 Acc 0.8993, AUC 0.9403015375137329, avg_entr 0.14996826648712158
Test Epoch4 threshold 0.8 Acc 0.9007, AUC 0.9446305632591248, avg_entr 0.1884712427854538
gc 0
Train Epoch5 Acc 0.956925 (38277/40000), AUC 0.9880712032318115
Test Epoch5 threshold 0.2 Acc 0.893, AUC 0.9398097991943359, avg_entr 0.07853075861930847
Test Epoch5 threshold 0.4 Acc 0.8941, AUC 0.9374662637710571, avg_entr 0.11014291644096375
Test Epoch5 threshold 0.6 Acc 0.8966, AUC 0.939704179763794, avg_entr 0.14243154227733612
Test Epoch5 threshold 0.8 Acc 0.8983, AUC 0.9450817108154297, avg_entr 0.1771215796470642
gc 0
Train Epoch6 Acc 0.95945 (38378/40000), AUC 0.9893045425415039
Test Epoch6 threshold 0.2 Acc 0.8939, AUC 0.9381012320518494, avg_entr 0.07755126059055328
Test Epoch6 threshold 0.4 Acc 0.8945, AUC 0.9358540773391724, avg_entr 0.10745956003665924
Test Epoch6 threshold 0.6 Acc 0.8964, AUC 0.9382346272468567, avg_entr 0.1404135674238205
Test Epoch6 threshold 0.8 Acc 0.8995, AUC 0.9443057179450989, avg_entr 0.17660486698150635
gc 0
Train Epoch7 Acc 0.9604 (38416/40000), AUC 0.9898760318756104
Test Epoch7 threshold 0.2 Acc 0.8929, AUC 0.9378030300140381, avg_entr 0.0766608715057373
Test Epoch7 threshold 0.4 Acc 0.8946, AUC 0.9344910383224487, avg_entr 0.10672976076602936
Test Epoch7 threshold 0.6 Acc 0.8965, AUC 0.9374637603759766, avg_entr 0.1390891671180725
Test Epoch7 threshold 0.8 Acc 0.8991, AUC 0.9431605339050293, avg_entr 0.17498423159122467
gc 0
Train Epoch8 Acc 0.96055 (38422/40000), AUC 0.9898149967193604
Test Epoch8 threshold 0.2 Acc 0.8932, AUC 0.9367029666900635, avg_entr 0.07551789283752441
Test Epoch8 threshold 0.4 Acc 0.8937, AUC 0.934822678565979, avg_entr 0.10558325052261353
Test Epoch8 threshold 0.6 Acc 0.8957, AUC 0.9368588328361511, avg_entr 0.1374005824327469
Test Epoch8 threshold 0.8 Acc 0.899, AUC 0.943260908126831, avg_entr 0.1725560873746872
gc 0
Train Epoch9 Acc 0.961325 (38453/40000), AUC 0.9899271726608276
Test Epoch9 threshold 0.2 Acc 0.8932, AUC 0.9366755485534668, avg_entr 0.07531339675188065
Test Epoch9 threshold 0.4 Acc 0.8939, AUC 0.935105562210083, avg_entr 0.10522543638944626
Test Epoch9 threshold 0.6 Acc 0.8959, AUC 0.9370123147964478, avg_entr 0.13716594874858856
Test Epoch9 threshold 0.8 Acc 0.8989, AUC 0.9432812333106995, avg_entr 0.1719081550836563
gc 0
Train Epoch10 Acc 0.961 (38440/40000), AUC 0.9897860288619995
Test Epoch10 threshold 0.2 Acc 0.8927, AUC 0.9370137453079224, avg_entr 0.07524874061346054
Test Epoch10 threshold 0.4 Acc 0.8943, AUC 0.9341843128204346, avg_entr 0.1053263321518898
Test Epoch10 threshold 0.6 Acc 0.8963, AUC 0.9370067119598389, avg_entr 0.13721036911010742
Test Epoch10 threshold 0.8 Acc 0.899, AUC 0.9431654810905457, avg_entr 0.17273573577404022
gc 0
Train Epoch11 Acc 0.9613 (38452/40000), AUC 0.9901509284973145
Test Epoch11 threshold 0.2 Acc 0.8927, AUC 0.9366834163665771, avg_entr 0.07497680932283401
Test Epoch11 threshold 0.4 Acc 0.8938, AUC 0.9345956444740295, avg_entr 0.10467014461755753
Test Epoch11 threshold 0.6 Acc 0.8957, AUC 0.9369379281997681, avg_entr 0.13622283935546875
Test Epoch11 threshold 0.8 Acc 0.899, AUC 0.9431358575820923, avg_entr 0.1719408631324768
gc 0
Train Epoch12 Acc 0.961225 (38449/40000), AUC 0.989896297454834
Test Epoch12 threshold 0.2 Acc 0.8925, AUC 0.9367507696151733, avg_entr 0.07473433017730713
Test Epoch12 threshold 0.4 Acc 0.8938, AUC 0.9345443844795227, avg_entr 0.10464775562286377
Test Epoch12 threshold 0.6 Acc 0.8955, AUC 0.9368429183959961, avg_entr 0.13600021600723267
Test Epoch12 threshold 0.8 Acc 0.8989, AUC 0.9430166482925415, avg_entr 0.17202673852443695
gc 0
Train Epoch13 Acc 0.961125 (38445/40000), AUC 0.9901863932609558
Test Epoch13 threshold 0.2 Acc 0.8925, AUC 0.9370336532592773, avg_entr 0.07470393180847168
Test Epoch13 threshold 0.4 Acc 0.8939, AUC 0.9343822598457336, avg_entr 0.10461215674877167
Test Epoch13 threshold 0.6 Acc 0.8956, AUC 0.9369145631790161, avg_entr 0.13605833053588867
Test Epoch13 threshold 0.8 Acc 0.8989, AUC 0.9429975748062134, avg_entr 0.1720433384180069
gc 0
Train Epoch14 Acc 0.961225 (38449/40000), AUC 0.989983081817627
Test Epoch14 threshold 0.2 Acc 0.8927, AUC 0.9368591904640198, avg_entr 0.07466263324022293
Test Epoch14 threshold 0.4 Acc 0.8941, AUC 0.9342654943466187, avg_entr 0.10482019186019897
Test Epoch14 threshold 0.6 Acc 0.8959, AUC 0.9370257258415222, avg_entr 0.13626761734485626
Test Epoch14 threshold 0.8 Acc 0.899, AUC 0.9429664611816406, avg_entr 0.17209608852863312
gc 0
Train Epoch15 Acc 0.961175 (38447/40000), AUC 0.9897140860557556
Test Epoch15 threshold 0.2 Acc 0.8926, AUC 0.9368515014648438, avg_entr 0.07467341423034668
Test Epoch15 threshold 0.4 Acc 0.894, AUC 0.9342557191848755, avg_entr 0.10477783530950546
Test Epoch15 threshold 0.6 Acc 0.8957, AUC 0.9370251893997192, avg_entr 0.13621827960014343
Test Epoch15 threshold 0.8 Acc 0.8989, AUC 0.9429657459259033, avg_entr 0.1720704585313797
gc 0
Train Epoch16 Acc 0.961175 (38447/40000), AUC 0.9901071190834045
Test Epoch16 threshold 0.2 Acc 0.8927, AUC 0.9368427991867065, avg_entr 0.07466478645801544
Test Epoch16 threshold 0.4 Acc 0.8941, AUC 0.9342721700668335, avg_entr 0.10477818548679352
Test Epoch16 threshold 0.6 Acc 0.8958, AUC 0.9370218515396118, avg_entr 0.13622692227363586
Test Epoch16 threshold 0.8 Acc 0.899, AUC 0.9429644346237183, avg_entr 0.1720620095729828
gc 0
Train Epoch17 Acc 0.961325 (38453/40000), AUC 0.9899343252182007
Test Epoch17 threshold 0.2 Acc 0.8927, AUC 0.9368414878845215, avg_entr 0.0746590793132782
Test Epoch17 threshold 0.4 Acc 0.8941, AUC 0.9342800378799438, avg_entr 0.10474187135696411
Test Epoch17 threshold 0.6 Acc 0.8958, AUC 0.9370216727256775, avg_entr 0.13621945679187775
Test Epoch17 threshold 0.8 Acc 0.899, AUC 0.9429639577865601, avg_entr 0.17205388844013214
gc 0
Train Epoch18 Acc 0.961175 (38447/40000), AUC 0.9902024269104004
Test Epoch18 threshold 0.2 Acc 0.8926, AUC 0.9368475079536438, avg_entr 0.07465624064207077
Test Epoch18 threshold 0.4 Acc 0.894, AUC 0.9342689514160156, avg_entr 0.10477202385663986
Test Epoch18 threshold 0.6 Acc 0.8957, AUC 0.9370192885398865, avg_entr 0.13621923327445984
Test Epoch18 threshold 0.8 Acc 0.8989, AUC 0.9429627656936646, avg_entr 0.17205455899238586
gc 0
Train Epoch19 Acc 0.961075 (38443/40000), AUC 0.9899255633354187
Test Epoch19 threshold 0.2 Acc 0.8927, AUC 0.9368335604667664, avg_entr 0.07464244961738586
Test Epoch19 threshold 0.4 Acc 0.8941, AUC 0.9342739582061768, avg_entr 0.10473968833684921
Test Epoch19 threshold 0.6 Acc 0.8958, AUC 0.9370217323303223, avg_entr 0.13621380925178528
Test Epoch19 threshold 0.8 Acc 0.899, AUC 0.942962646484375, avg_entr 0.1720447689294815
gc 0
Train Epoch20 Acc 0.961225 (38449/40000), AUC 0.9901942014694214
Test Epoch20 threshold 0.2 Acc 0.8927, AUC 0.9368518590927124, avg_entr 0.07462842017412186
Test Epoch20 threshold 0.4 Acc 0.8941, AUC 0.9342758655548096, avg_entr 0.10472998023033142
Test Epoch20 threshold 0.6 Acc 0.8958, AUC 0.9370160102844238, avg_entr 0.136205792427063
Test Epoch20 threshold 0.8 Acc 0.899, AUC 0.9429619312286377, avg_entr 0.17204508185386658
gc 0
Train Epoch21 Acc 0.96125 (38450/40000), AUC 0.9902034997940063
Test Epoch21 threshold 0.2 Acc 0.8926, AUC 0.9368445873260498, avg_entr 0.074649378657341
Test Epoch21 threshold 0.4 Acc 0.894, AUC 0.9342651963233948, avg_entr 0.10476543009281158
Test Epoch21 threshold 0.6 Acc 0.8957, AUC 0.9370165467262268, avg_entr 0.1362133026123047
Test Epoch21 threshold 0.8 Acc 0.8989, AUC 0.9429611563682556, avg_entr 0.17204971611499786
gc 0
Train Epoch22 Acc 0.961275 (38451/40000), AUC 0.9901117086410522
Test Epoch22 threshold 0.2 Acc 0.8926, AUC 0.9368436932563782, avg_entr 0.07464436441659927
Test Epoch22 threshold 0.4 Acc 0.894, AUC 0.9342650175094604, avg_entr 0.10475952178239822
Test Epoch22 threshold 0.6 Acc 0.8957, AUC 0.9370163679122925, avg_entr 0.1362089067697525
Test Epoch22 threshold 0.8 Acc 0.8989, AUC 0.9429608583450317, avg_entr 0.1720457822084427
gc 0
Train Epoch23 Acc 0.96125 (38450/40000), AUC 0.9898867607116699
Test Epoch23 threshold 0.2 Acc 0.8927, AUC 0.9368411302566528, avg_entr 0.07463733851909637
Test Epoch23 threshold 0.4 Acc 0.8941, AUC 0.9342629909515381, avg_entr 0.10475281625986099
Test Epoch23 threshold 0.6 Acc 0.8958, AUC 0.9370146989822388, avg_entr 0.13620391488075256
Test Epoch23 threshold 0.8 Acc 0.899, AUC 0.9429595470428467, avg_entr 0.1720416098833084
gc 0
Train Epoch24 Acc 0.961425 (38457/40000), AUC 0.9899744987487793
Test Epoch24 threshold 0.2 Acc 0.8927, AUC 0.9368269443511963, avg_entr 0.07462957501411438
Test Epoch24 threshold 0.4 Acc 0.8941, AUC 0.9342687129974365, avg_entr 0.1047205701470375
Test Epoch24 threshold 0.6 Acc 0.8958, AUC 0.9370168447494507, avg_entr 0.13620105385780334
Test Epoch24 threshold 0.8 Acc 0.899, AUC 0.942959189414978, avg_entr 0.1720345914363861
gc 0
Train Epoch25 Acc 0.96115 (38446/40000), AUC 0.989976167678833
Test Epoch25 threshold 0.2 Acc 0.8927, AUC 0.9368386268615723, avg_entr 0.07462897896766663
Test Epoch25 threshold 0.4 Acc 0.8941, AUC 0.9342610239982605, avg_entr 0.10473652184009552
Test Epoch25 threshold 0.6 Acc 0.8958, AUC 0.9370127320289612, avg_entr 0.1361960917711258
Test Epoch25 threshold 0.8 Acc 0.899, AUC 0.942958414554596, avg_entr 0.172035351395607
gc 0
Train Epoch26 Acc 0.961225 (38449/40000), AUC 0.9902304410934448
Test Epoch26 threshold 0.2 Acc 0.8927, AUC 0.9368326663970947, avg_entr 0.07461769133806229
Test Epoch26 threshold 0.4 Acc 0.8941, AUC 0.93426913022995, avg_entr 0.10470075905323029
Test Epoch26 threshold 0.6 Acc 0.8958, AUC 0.9370144605636597, avg_entr 0.13619475066661835
Test Epoch26 threshold 0.8 Acc 0.899, AUC 0.9429572820663452, avg_entr 0.17202991247177124
gc 0
Train Epoch27 Acc 0.9614 (38456/40000), AUC 0.990277886390686
Test Epoch27 threshold 0.2 Acc 0.8927, AUC 0.9368364810943604, avg_entr 0.07462049275636673
Test Epoch27 threshold 0.4 Acc 0.8941, AUC 0.9342602491378784, avg_entr 0.10472948104143143
Test Epoch27 threshold 0.6 Acc 0.8958, AUC 0.937011182308197, avg_entr 0.13618925213813782
Test Epoch27 threshold 0.8 Acc 0.899, AUC 0.9429569244384766, avg_entr 0.1720290184020996
gc 0
Train Epoch28 Acc 0.961225 (38449/40000), AUC 0.989886999130249
Test Epoch28 threshold 0.2 Acc 0.8927, AUC 0.9368295669555664, avg_entr 0.07461108267307281
Test Epoch28 threshold 0.4 Acc 0.8941, AUC 0.9342637658119202, avg_entr 0.10470623522996902
Test Epoch28 threshold 0.6 Acc 0.8958, AUC 0.9370126724243164, avg_entr 0.1361873596906662
Test Epoch28 threshold 0.8 Acc 0.899, AUC 0.9429556727409363, avg_entr 0.17202316224575043
gc 0
Train Epoch29 Acc 0.9612 (38448/40000), AUC 0.9898673295974731
Test Epoch29 threshold 0.2 Acc 0.8927, AUC 0.9368276596069336, avg_entr 0.0746072456240654
Test Epoch29 threshold 0.4 Acc 0.8941, AUC 0.9342628717422485, avg_entr 0.104698047041893
Test Epoch29 threshold 0.6 Acc 0.8958, AUC 0.9370099306106567, avg_entr 0.13618893921375275
Test Epoch29 threshold 0.8 Acc 0.899, AUC 0.9429546594619751, avg_entr 0.17201852798461914
Best AUC 0.9529018402099609
train_loss (2, 5, 30)
valid_acc (30, 4)
valid_AUC (30, 4)
train_acc (30,)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Start Testing
Load ckpt at ckpt/imdb_transformeral_l5_pad500//imdb_transformeral_l5.pt
[[4495  460]
 [ 603 4442]]
Figure(640x480)
tensor([0.0019, 0.0088, 0.1085,  ..., 0.0157, 0.0257, 0.0320])
[[4407  548]
 [ 418 4627]]
Figure(640x480)
tensor([0.0002, 0.0104, 0.0231,  ..., 0.0822, 0.1901, 0.0047])
[[4363  592]
 [ 395 4650]]
Figure(640x480)
tensor([0.0037, 0.0052, 0.0134,  ..., 0.0332, 0.0183, 0.0044])
[[4350  605]
 [ 386 4659]]
Figure(640x480)
tensor([0.0076, 0.0047, 0.0128,  ..., 0.0195, 0.0105, 0.0049])
[[4356  599]
 [ 387 4658]]
Figure(640x480)
tensor([0.0049, 0.0058, 0.0110,  ..., 0.0182, 0.0098, 0.0055])
