total count words 102019
vocab size 30000
found 26754 words in glove
model: TransformerModelML(
  (layers): ModuleList(
    (0): EMBLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Embedding(30000, 300)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=4, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=4, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.weight 9000000
layers.0.ae.g.0.weight 512
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 512
layers.0.ae.h.0.bias 4
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
Total Trainable Params: 13672292
Start Training
gc 0
Train Epoch0 Acc 0.6290583333333334 (75487/120000), AUC 0.8571611642837524
Test Epoch0 threshold 0.2 Acc 0.8976315789473684, AUC 0.9733761548995972, avg_entr 0.1342126429080963
Save ckpt to ckpt/ag_news_transformeral_l5_pad200//ag_news_transformeral_l5.pt  ,ep 0
Test Epoch0 threshold 0.4 Acc 0.8994736842105263, AUC 0.9734676480293274, avg_entr 0.1607474982738495
Save ckpt to ckpt/ag_news_transformeral_l5_pad200//ag_news_transformeral_l5.pt  ,ep 0
Test Epoch0 threshold 0.6 Acc 0.8976315789473684, AUC 0.9749321937561035, avg_entr 0.18725931644439697
Save ckpt to ckpt/ag_news_transformeral_l5_pad200//ag_news_transformeral_l5.pt  ,ep 0
Test Epoch0 threshold 0.8 Acc 0.8972368421052631, AUC 0.9748533964157104, avg_entr 0.1878518909215927
gc 0
Train Epoch1 Acc 0.918075 (110169/120000), AUC 0.9810917377471924
Test Epoch1 threshold 0.2 Acc 0.9151315789473684, AUC 0.9784268140792847, avg_entr 0.07402541488409042
Save ckpt to ckpt/ag_news_transformeral_l5_pad200//ag_news_transformeral_l5.pt  ,ep 1
Test Epoch1 threshold 0.4 Acc 0.9130263157894737, AUC 0.9785714149475098, avg_entr 0.096501424908638
Save ckpt to ckpt/ag_news_transformeral_l5_pad200//ag_news_transformeral_l5.pt  ,ep 1
Test Epoch1 threshold 0.6 Acc 0.9110526315789473, AUC 0.9787996411323547, avg_entr 0.10705071687698364
Save ckpt to ckpt/ag_news_transformeral_l5_pad200//ag_news_transformeral_l5.pt  ,ep 1
Test Epoch1 threshold 0.8 Acc 0.9110526315789473, AUC 0.9787996411323547, avg_entr 0.10705071687698364
Save ckpt to ckpt/ag_news_transformeral_l5_pad200//ag_news_transformeral_l5.pt  ,ep 1
gc 0
Train Epoch2 Acc 0.9334083333333333 (112009/120000), AUC 0.9864133596420288
Test Epoch2 threshold 0.2 Acc 0.9193421052631578, AUC 0.9781005382537842, avg_entr 0.04791567102074623
Test Epoch2 threshold 0.4 Acc 0.916578947368421, AUC 0.9798863530158997, avg_entr 0.07333099097013474
Save ckpt to ckpt/ag_news_transformeral_l5_pad200//ag_news_transformeral_l5.pt  ,ep 2
Test Epoch2 threshold 0.6 Acc 0.9153947368421053, AUC 0.9802542924880981, avg_entr 0.08225049823522568
Save ckpt to ckpt/ag_news_transformeral_l5_pad200//ag_news_transformeral_l5.pt  ,ep 2
Test Epoch2 threshold 0.8 Acc 0.9153947368421053, AUC 0.9802542924880981, avg_entr 0.08225049823522568
Save ckpt to ckpt/ag_news_transformeral_l5_pad200//ag_news_transformeral_l5.pt  ,ep 2
gc 0
Train Epoch3 Acc 0.9418833333333333 (113026/120000), AUC 0.9885324239730835
Test Epoch3 threshold 0.2 Acc 0.9185526315789474, AUC 0.9774439334869385, avg_entr 0.03803570568561554
Test Epoch3 threshold 0.4 Acc 0.9163157894736842, AUC 0.9802072048187256, avg_entr 0.06183283403515816
Test Epoch3 threshold 0.6 Acc 0.9156578947368421, AUC 0.9811829924583435, avg_entr 0.06941130012273788
Save ckpt to ckpt/ag_news_transformeral_l5_pad200//ag_news_transformeral_l5.pt  ,ep 3
Test Epoch3 threshold 0.8 Acc 0.9155263157894736, AUC 0.9811695218086243, avg_entr 0.06948870420455933
gc 0
Train Epoch4 Acc 0.9472166666666667 (113666/120000), AUC 0.9897927641868591
Test Epoch4 threshold 0.2 Acc 0.9176315789473685, AUC 0.9785401821136475, avg_entr 0.03328746557235718
Test Epoch4 threshold 0.4 Acc 0.9196052631578947, AUC 0.9812259674072266, avg_entr 0.054760534316301346
Save ckpt to ckpt/ag_news_transformeral_l5_pad200//ag_news_transformeral_l5.pt  ,ep 4
Test Epoch4 threshold 0.6 Acc 0.9192105263157895, AUC 0.9817376136779785, avg_entr 0.06084214150905609
Save ckpt to ckpt/ag_news_transformeral_l5_pad200//ag_news_transformeral_l5.pt  ,ep 4
Test Epoch4 threshold 0.8 Acc 0.9190789473684211, AUC 0.9817259311676025, avg_entr 0.06092631444334984
gc 0
Train Epoch5 Acc 0.9514666666666667 (114176/120000), AUC 0.9911260604858398
Test Epoch5 threshold 0.2 Acc 0.9157894736842105, AUC 0.9786074161529541, avg_entr 0.02930341474711895
Test Epoch5 threshold 0.4 Acc 0.9167105263157894, AUC 0.9811221361160278, avg_entr 0.04739410802721977
Test Epoch5 threshold 0.6 Acc 0.9164473684210527, AUC 0.9818412065505981, avg_entr 0.053921088576316833
Save ckpt to ckpt/ag_news_transformeral_l5_pad200//ag_news_transformeral_l5.pt  ,ep 5
Test Epoch5 threshold 0.8 Acc 0.9164473684210527, AUC 0.9818412065505981, avg_entr 0.053921088576316833
Save ckpt to ckpt/ag_news_transformeral_l5_pad200//ag_news_transformeral_l5.pt  ,ep 5
gc 0
Train Epoch6 Acc 0.9545583333333333 (114547/120000), AUC 0.9918968081474304
Test Epoch6 threshold 0.2 Acc 0.915921052631579, AUC 0.9771898984909058, avg_entr 0.025357218459248543
Test Epoch6 threshold 0.4 Acc 0.9167105263157894, AUC 0.9809986352920532, avg_entr 0.04368526116013527
Test Epoch6 threshold 0.6 Acc 0.9146052631578947, AUC 0.981727123260498, avg_entr 0.0501868762075901
Test Epoch6 threshold 0.8 Acc 0.9146052631578947, AUC 0.981727123260498, avg_entr 0.0501868762075901
gc 0
Train Epoch7 Acc 0.9589 (115068/120000), AUC 0.9932550191879272
Test Epoch7 threshold 0.2 Acc 0.9135526315789474, AUC 0.9773716926574707, avg_entr 0.024420030415058136
Test Epoch7 threshold 0.4 Acc 0.9172368421052631, AUC 0.9810094237327576, avg_entr 0.04162019491195679
Test Epoch7 threshold 0.6 Acc 0.9167105263157894, AUC 0.9818364977836609, avg_entr 0.04736125096678734
Test Epoch7 threshold 0.8 Acc 0.9167105263157894, AUC 0.9818364977836609, avg_entr 0.04736125096678734
gc 0
Train Epoch8 Acc 0.9613916666666666 (115367/120000), AUC 0.9939442276954651
Test Epoch8 threshold 0.2 Acc 0.9134210526315789, AUC 0.9769781231880188, avg_entr 0.023067696020007133
Test Epoch8 threshold 0.4 Acc 0.9171052631578948, AUC 0.981061577796936, avg_entr 0.04012903571128845
Test Epoch8 threshold 0.6 Acc 0.9171052631578948, AUC 0.9818050861358643, avg_entr 0.045605458319187164
Test Epoch8 threshold 0.8 Acc 0.9171052631578948, AUC 0.9818050861358643, avg_entr 0.045605458319187164
gc 0
Train Epoch9 Acc 0.9621916666666667 (115463/120000), AUC 0.9945167303085327
Test Epoch9 threshold 0.2 Acc 0.9144736842105263, AUC 0.9768356084823608, avg_entr 0.022739488631486893
Test Epoch9 threshold 0.4 Acc 0.9180263157894737, AUC 0.9810296297073364, avg_entr 0.04046771675348282
Test Epoch9 threshold 0.6 Acc 0.9175, AUC 0.9818044304847717, avg_entr 0.045468542724847794
Test Epoch9 threshold 0.8 Acc 0.9175, AUC 0.9818044304847717, avg_entr 0.045468542724847794
gc 0
Train Epoch10 Acc 0.9626416666666666 (115517/120000), AUC 0.9945745468139648
Test Epoch10 threshold 0.2 Acc 0.9143421052631578, AUC 0.9767618179321289, avg_entr 0.02224261686205864
Test Epoch10 threshold 0.4 Acc 0.9173684210526316, AUC 0.9810545444488525, avg_entr 0.039460424333810806
Test Epoch10 threshold 0.6 Acc 0.9175, AUC 0.981791079044342, avg_entr 0.044420287013053894
Test Epoch10 threshold 0.8 Acc 0.9175, AUC 0.981791079044342, avg_entr 0.044420287013053894
gc 0
Train Epoch11 Acc 0.9629333333333333 (115552/120000), AUC 0.99476158618927
Test Epoch11 threshold 0.2 Acc 0.9143421052631578, AUC 0.9766937494277954, avg_entr 0.022240545600652695
Test Epoch11 threshold 0.4 Acc 0.9178947368421052, AUC 0.981050968170166, avg_entr 0.039132386445999146
Test Epoch11 threshold 0.6 Acc 0.9177631578947368, AUC 0.9817922115325928, avg_entr 0.04422489181160927
Test Epoch11 threshold 0.8 Acc 0.9177631578947368, AUC 0.9817922115325928, avg_entr 0.04422489181160927
gc 0
Train Epoch12 Acc 0.9631583333333333 (115579/120000), AUC 0.9948055148124695
Test Epoch12 threshold 0.2 Acc 0.9147368421052632, AUC 0.9768308401107788, avg_entr 0.022067014127969742
Test Epoch12 threshold 0.4 Acc 0.9181578947368421, AUC 0.9810695052146912, avg_entr 0.03894339129328728
Test Epoch12 threshold 0.6 Acc 0.9175, AUC 0.9817915558815002, avg_entr 0.04398762434720993
Test Epoch12 threshold 0.8 Acc 0.9175, AUC 0.9817915558815002, avg_entr 0.04398762434720993
gc 0
Train Epoch13 Acc 0.9633583333333333 (115603/120000), AUC 0.9948421716690063
Test Epoch13 threshold 0.2 Acc 0.9143421052631578, AUC 0.9767888784408569, avg_entr 0.021862518042325974
Test Epoch13 threshold 0.4 Acc 0.9178947368421052, AUC 0.9811576008796692, avg_entr 0.03867322579026222
Test Epoch13 threshold 0.6 Acc 0.9175, AUC 0.981786847114563, avg_entr 0.04367510974407196
Test Epoch13 threshold 0.8 Acc 0.9175, AUC 0.981786847114563, avg_entr 0.04367510974407196
gc 0
Train Epoch14 Acc 0.9633 (115596/120000), AUC 0.9947522282600403
Test Epoch14 threshold 0.2 Acc 0.9146052631578947, AUC 0.9770017862319946, avg_entr 0.021722925826907158
Test Epoch14 threshold 0.4 Acc 0.9175, AUC 0.9809868335723877, avg_entr 0.03860839456319809
Test Epoch14 threshold 0.6 Acc 0.9173684210526316, AUC 0.981788694858551, avg_entr 0.043590836226940155
Test Epoch14 threshold 0.8 Acc 0.9173684210526316, AUC 0.981788694858551, avg_entr 0.043590836226940155
gc 0
Train Epoch15 Acc 0.9634666666666667 (115616/120000), AUC 0.9948155879974365
Test Epoch15 threshold 0.2 Acc 0.9139473684210526, AUC 0.9768017530441284, avg_entr 0.021666737273335457
Test Epoch15 threshold 0.4 Acc 0.9177631578947368, AUC 0.9810037612915039, avg_entr 0.03853757679462433
Test Epoch15 threshold 0.6 Acc 0.9176315789473685, AUC 0.9817850589752197, avg_entr 0.04357575997710228
Test Epoch15 threshold 0.8 Acc 0.9176315789473685, AUC 0.9817850589752197, avg_entr 0.04357575997710228
gc 0
Train Epoch16 Acc 0.9633333333333334 (115600/120000), AUC 0.9948545694351196
Test Epoch16 threshold 0.2 Acc 0.9140789473684211, AUC 0.9768528342247009, avg_entr 0.02171209640800953
Test Epoch16 threshold 0.4 Acc 0.9177631578947368, AUC 0.9810054302215576, avg_entr 0.03848129138350487
Test Epoch16 threshold 0.6 Acc 0.9176315789473685, AUC 0.9817867279052734, avg_entr 0.043519310653209686
Test Epoch16 threshold 0.8 Acc 0.9176315789473685, AUC 0.9817867279052734, avg_entr 0.043519310653209686
gc 0
Train Epoch17 Acc 0.9632416666666667 (115589/120000), AUC 0.9948352575302124
Test Epoch17 threshold 0.2 Acc 0.9140789473684211, AUC 0.9767748117446899, avg_entr 0.02164417877793312
Test Epoch17 threshold 0.4 Acc 0.9177631578947368, AUC 0.9810062646865845, avg_entr 0.038412321358919144
Test Epoch17 threshold 0.6 Acc 0.9176315789473685, AUC 0.9817871451377869, avg_entr 0.043451886624097824
Test Epoch17 threshold 0.8 Acc 0.9176315789473685, AUC 0.9817871451377869, avg_entr 0.043451886624097824
gc 0
Train Epoch18 Acc 0.9633416666666667 (115601/120000), AUC 0.9949575662612915
Test Epoch18 threshold 0.2 Acc 0.9140789473684211, AUC 0.976772665977478, avg_entr 0.02163119986653328
Test Epoch18 threshold 0.4 Acc 0.9177631578947368, AUC 0.9810062646865845, avg_entr 0.03838430717587471
Test Epoch18 threshold 0.6 Acc 0.9176315789473685, AUC 0.9817877411842346, avg_entr 0.04342379793524742
Test Epoch18 threshold 0.8 Acc 0.9176315789473685, AUC 0.9817877411842346, avg_entr 0.04342379793524742
gc 0
Train Epoch19 Acc 0.963175 (115581/120000), AUC 0.9947949647903442
Test Epoch19 threshold 0.2 Acc 0.9140789473684211, AUC 0.9767712354660034, avg_entr 0.021609721705317497
Test Epoch19 threshold 0.4 Acc 0.9177631578947368, AUC 0.981006383895874, avg_entr 0.038369469344615936
Test Epoch19 threshold 0.6 Acc 0.9175, AUC 0.981787919998169, avg_entr 0.043408919125795364
Test Epoch19 threshold 0.8 Acc 0.9175, AUC 0.981787919998169, avg_entr 0.043408919125795364
gc 0
Train Epoch20 Acc 0.96345 (115614/120000), AUC 0.9949016571044922
Test Epoch20 threshold 0.2 Acc 0.9140789473684211, AUC 0.9767695665359497, avg_entr 0.02160230651497841
Test Epoch20 threshold 0.4 Acc 0.9177631578947368, AUC 0.9810061454772949, avg_entr 0.038361575454473495
Test Epoch20 threshold 0.6 Acc 0.9175, AUC 0.9817878007888794, avg_entr 0.0434010848402977
Test Epoch20 threshold 0.8 Acc 0.9175, AUC 0.9817878007888794, avg_entr 0.0434010848402977
gc 0
Train Epoch21 Acc 0.9631583333333333 (115579/120000), AUC 0.9948176741600037
Test Epoch21 threshold 0.2 Acc 0.9140789473684211, AUC 0.9767696857452393, avg_entr 0.021607432514429092
Test Epoch21 threshold 0.4 Acc 0.9177631578947368, AUC 0.981006383895874, avg_entr 0.038357920944690704
Test Epoch21 threshold 0.6 Acc 0.9175, AUC 0.9817880988121033, avg_entr 0.043397676199674606
Test Epoch21 threshold 0.8 Acc 0.9175, AUC 0.9817880988121033, avg_entr 0.043397676199674606
gc 0
Train Epoch22 Acc 0.9633166666666667 (115598/120000), AUC 0.9947713613510132
Test Epoch22 threshold 0.2 Acc 0.9140789473684211, AUC 0.9767692685127258, avg_entr 0.021607693284749985
Test Epoch22 threshold 0.4 Acc 0.9177631578947368, AUC 0.9810062646865845, avg_entr 0.03835739940404892
Test Epoch22 threshold 0.6 Acc 0.9175, AUC 0.981788158416748, avg_entr 0.04339715093374252
Test Epoch22 threshold 0.8 Acc 0.9175, AUC 0.981788158416748, avg_entr 0.04339715093374252
gc 0
Train Epoch23 Acc 0.96355 (115626/120000), AUC 0.9949017763137817
Test Epoch23 threshold 0.2 Acc 0.9140789473684211, AUC 0.9767684936523438, avg_entr 0.021606575697660446
Test Epoch23 threshold 0.4 Acc 0.9177631578947368, AUC 0.9810060262680054, avg_entr 0.038356415927410126
Test Epoch23 threshold 0.6 Acc 0.9175, AUC 0.981788158416748, avg_entr 0.04339635744690895
Test Epoch23 threshold 0.8 Acc 0.9175, AUC 0.981788158416748, avg_entr 0.04339635744690895
gc 0
Train Epoch24 Acc 0.9632666666666667 (115592/120000), AUC 0.9947896003723145
Test Epoch24 threshold 0.2 Acc 0.9140789473684211, AUC 0.9767671823501587, avg_entr 0.021605361253023148
Test Epoch24 threshold 0.4 Acc 0.9177631578947368, AUC 0.981005847454071, avg_entr 0.038355372846126556
Test Epoch24 threshold 0.6 Acc 0.9175, AUC 0.981788158416748, avg_entr 0.043395377695560455
Test Epoch24 threshold 0.8 Acc 0.9175, AUC 0.981788158416748, avg_entr 0.043395377695560455
gc 0
Train Epoch25 Acc 0.9630833333333333 (115570/120000), AUC 0.9949926733970642
Test Epoch25 threshold 0.2 Acc 0.9140789473684211, AUC 0.9767663478851318, avg_entr 0.02160448580980301
Test Epoch25 threshold 0.4 Acc 0.9177631578947368, AUC 0.981005847454071, avg_entr 0.03835427016019821
Test Epoch25 threshold 0.6 Acc 0.9175, AUC 0.9817882180213928, avg_entr 0.04339434579014778
Test Epoch25 threshold 0.8 Acc 0.9175, AUC 0.9817882180213928, avg_entr 0.04339434579014778
gc 0
Train Epoch26 Acc 0.9634083333333333 (115609/120000), AUC 0.9948849678039551
Test Epoch26 threshold 0.2 Acc 0.9140789473684211, AUC 0.9767656326293945, avg_entr 0.021602755412459373
Test Epoch26 threshold 0.4 Acc 0.9177631578947368, AUC 0.9810056686401367, avg_entr 0.03835313767194748
Test Epoch26 threshold 0.6 Acc 0.9175, AUC 0.981788158416748, avg_entr 0.04339337348937988
Test Epoch26 threshold 0.8 Acc 0.9175, AUC 0.981788158416748, avg_entr 0.04339337348937988
gc 0
Train Epoch27 Acc 0.9634416666666666 (115613/120000), AUC 0.9947304725646973
Test Epoch27 threshold 0.2 Acc 0.9140789473684211, AUC 0.9767654538154602, avg_entr 0.021601980552077293
Test Epoch27 threshold 0.4 Acc 0.9177631578947368, AUC 0.9810056686401367, avg_entr 0.038352277129888535
Test Epoch27 threshold 0.6 Acc 0.9175, AUC 0.9817882180213928, avg_entr 0.04339255020022392
Test Epoch27 threshold 0.8 Acc 0.9175, AUC 0.9817882180213928, avg_entr 0.04339255020022392
gc 0
Train Epoch28 Acc 0.96335 (115602/120000), AUC 0.9949308037757874
Test Epoch28 threshold 0.2 Acc 0.9140789473684211, AUC 0.9767651557922363, avg_entr 0.021601898595690727
Test Epoch28 threshold 0.4 Acc 0.9177631578947368, AUC 0.9810055494308472, avg_entr 0.03835134953260422
Test Epoch28 threshold 0.6 Acc 0.9175, AUC 0.9817883372306824, avg_entr 0.04339170828461647
Test Epoch28 threshold 0.8 Acc 0.9175, AUC 0.9817883372306824, avg_entr 0.04339170828461647
gc 0
Train Epoch29 Acc 0.9632166666666667 (115586/120000), AUC 0.9947944283485413
Test Epoch29 threshold 0.2 Acc 0.9140789473684211, AUC 0.9767646789550781, avg_entr 0.02160002663731575
Test Epoch29 threshold 0.4 Acc 0.9177631578947368, AUC 0.9810053706169128, avg_entr 0.0383501797914505
Test Epoch29 threshold 0.6 Acc 0.9175, AUC 0.9817882776260376, avg_entr 0.04339069873094559
Test Epoch29 threshold 0.8 Acc 0.9175, AUC 0.9817882776260376, avg_entr 0.04339069873094559
Best AUC 0.9818412065505981
train_loss (2, 5, 30)
valid_acc (30, 4)
valid_AUC (30, 4)
train_acc (30,)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Start Testing
Load ckpt at ckpt/ag_news_transformeral_l5_pad200//ag_news_transformeral_l5.pt
[[1699   61   93   47]
 [  12 1868   10   10]
 [  43   17 1697  143]
 [  46   15  138 1701]]
Figure(640x480)
tensor([0.0295, 0.0013, 0.0204,  ..., 0.0637, 0.0014, 0.0480])
[[1704   56   80   60]
 [  13 1862   11   14]
 [  47   17 1684  152]
 [  46   13  130 1711]]
Figure(640x480)
tensor([0.0058, 0.0009, 0.0008,  ..., 0.0010, 0.0008, 0.0007])
[[1703   55   81   61]
 [  14 1858   13   15]
 [  45   16 1688  151]
 [  44   11  131 1714]]
Figure(640x480)
tensor([0.0022, 0.0008, 0.0009,  ..., 0.0012, 0.0009, 0.0009])
[[1704   54   80   62]
 [  14 1858   13   15]
 [  46   16 1687  151]
 [  44   10  132 1714]]
Figure(640x480)
tensor([0.0021, 0.0009, 0.0010,  ..., 0.0010, 0.0010, 0.0010])
[[1704   54   79   63]
 [  14 1858   13   15]
 [  46   16 1683  155]
 [  44   11  129 1716]]
Figure(640x480)
tensor([0.0017, 0.0008, 0.0009,  ..., 0.0010, 0.0010, 0.0010])
