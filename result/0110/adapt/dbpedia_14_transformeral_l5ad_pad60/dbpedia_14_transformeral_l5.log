total count words 887881
vocab size 30000
found 28354 words in glove
model: TransformerModelML(
  (layers): ModuleList(
    (0): EMBLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Embedding(30000, 300)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=14, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=14, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.weight 9000000
layers.0.ae.g.0.weight 1792
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 1792
layers.0.ae.h.0.bias 14
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
Total Trainable Params: 13674862
Start Training
gc 0
Train Epoch0 Acc 0.8363910714285714 (468379/560000), AUC 0.9808462262153625
Test Epoch0 threshold 0.2 Acc 0.9737142857142858, AUC 0.9981331825256348, avg_entr 0.03072172775864601
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60//dbpedia_14_transformeral_l5.pt  ,ep 0
Test Epoch0 threshold 0.4 Acc 0.9733428571428572, AUC 0.9981981515884399, avg_entr 0.03227570652961731
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60//dbpedia_14_transformeral_l5.pt  ,ep 0
Test Epoch0 threshold 0.6 Acc 0.9733428571428572, AUC 0.9981981515884399, avg_entr 0.03227570652961731
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60//dbpedia_14_transformeral_l5.pt  ,ep 0
Test Epoch0 threshold 0.8 Acc 0.9733428571428572, AUC 0.9981981515884399, avg_entr 0.03227570652961731
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60//dbpedia_14_transformeral_l5.pt  ,ep 0
gc 0
Train Epoch1 Acc 0.9800928571428571 (548852/560000), AUC 0.9976218342781067
Test Epoch1 threshold 0.2 Acc 0.9750571428571428, AUC 0.9983205795288086, avg_entr 0.01654435135424137
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60//dbpedia_14_transformeral_l5.pt  ,ep 1
Test Epoch1 threshold 0.4 Acc 0.9744428571428572, AUC 0.9983453154563904, avg_entr 0.017459703609347343
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60//dbpedia_14_transformeral_l5.pt  ,ep 1
Test Epoch1 threshold 0.6 Acc 0.9744428571428572, AUC 0.9983453154563904, avg_entr 0.017459703609347343
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60//dbpedia_14_transformeral_l5.pt  ,ep 1
Test Epoch1 threshold 0.8 Acc 0.9744428571428572, AUC 0.9983453154563904, avg_entr 0.017459703609347343
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60//dbpedia_14_transformeral_l5.pt  ,ep 1
gc 0
Train Epoch2 Acc 0.9837875 (550921/560000), AUC 0.9980280995368958
Test Epoch2 threshold 0.2 Acc 0.9752, AUC 0.9983435869216919, avg_entr 0.011779331602156162
Test Epoch2 threshold 0.4 Acc 0.9746571428571429, AUC 0.9983471035957336, avg_entr 0.012596847489476204
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60//dbpedia_14_transformeral_l5.pt  ,ep 2
Test Epoch2 threshold 0.6 Acc 0.9746571428571429, AUC 0.9983471035957336, avg_entr 0.01259889081120491
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60//dbpedia_14_transformeral_l5.pt  ,ep 2
Test Epoch2 threshold 0.8 Acc 0.9746571428571429, AUC 0.9983471035957336, avg_entr 0.01259889081120491
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60//dbpedia_14_transformeral_l5.pt  ,ep 2
gc 0
Train Epoch3 Acc 0.9861375 (552237/560000), AUC 0.9984518885612488
Test Epoch3 threshold 0.2 Acc 0.9753857142857143, AUC 0.9983559250831604, avg_entr 0.009792345575988293
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60//dbpedia_14_transformeral_l5.pt  ,ep 3
Test Epoch3 threshold 0.4 Acc 0.9748142857142857, AUC 0.9983776807785034, avg_entr 0.010440658777952194
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60//dbpedia_14_transformeral_l5.pt  ,ep 3
Test Epoch3 threshold 0.6 Acc 0.9748142857142857, AUC 0.9983776807785034, avg_entr 0.010440658777952194
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60//dbpedia_14_transformeral_l5.pt  ,ep 3
Test Epoch3 threshold 0.8 Acc 0.9748142857142857, AUC 0.9983776807785034, avg_entr 0.010440658777952194
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60//dbpedia_14_transformeral_l5.pt  ,ep 3
gc 0
Train Epoch4 Acc 0.9874982142857143 (552999/560000), AUC 0.9985563158988953
Test Epoch4 threshold 0.2 Acc 0.9754285714285714, AUC 0.9983879923820496, avg_entr 0.009233095683157444
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60//dbpedia_14_transformeral_l5.pt  ,ep 4
Test Epoch4 threshold 0.4 Acc 0.9749428571428571, AUC 0.9983895421028137, avg_entr 0.009849755093455315
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60//dbpedia_14_transformeral_l5.pt  ,ep 4
Test Epoch4 threshold 0.6 Acc 0.9749428571428571, AUC 0.9983895421028137, avg_entr 0.009849755093455315
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60//dbpedia_14_transformeral_l5.pt  ,ep 4
Test Epoch4 threshold 0.8 Acc 0.9749428571428571, AUC 0.9983895421028137, avg_entr 0.009849755093455315
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60//dbpedia_14_transformeral_l5.pt  ,ep 4
gc 0
Train Epoch5 Acc 0.9881678571428572 (553374/560000), AUC 0.9988313317298889
Test Epoch5 threshold 0.2 Acc 0.9751428571428571, AUC 0.9983901977539062, avg_entr 0.009102114476263523
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60//dbpedia_14_transformeral_l5.pt  ,ep 5
Test Epoch5 threshold 0.4 Acc 0.9747857142857143, AUC 0.9983946681022644, avg_entr 0.00957686547189951
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60//dbpedia_14_transformeral_l5.pt  ,ep 5
Test Epoch5 threshold 0.6 Acc 0.9747857142857143, AUC 0.9983946681022644, avg_entr 0.00957686547189951
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60//dbpedia_14_transformeral_l5.pt  ,ep 5
Test Epoch5 threshold 0.8 Acc 0.9747857142857143, AUC 0.9983946681022644, avg_entr 0.00957686547189951
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60//dbpedia_14_transformeral_l5.pt  ,ep 5
gc 0
Train Epoch6 Acc 0.9884821428571429 (553550/560000), AUC 0.9989026784896851
Test Epoch6 threshold 0.2 Acc 0.9752714285714286, AUC 0.9983863234519958, avg_entr 0.00910615362226963
Test Epoch6 threshold 0.4 Acc 0.9749571428571429, AUC 0.9983931183815002, avg_entr 0.009562578052282333
Test Epoch6 threshold 0.6 Acc 0.9749571428571429, AUC 0.9983931183815002, avg_entr 0.009562578052282333
Test Epoch6 threshold 0.8 Acc 0.9749571428571429, AUC 0.9983931183815002, avg_entr 0.009562578052282333
gc 0
Train Epoch7 Acc 0.9886160714285714 (553625/560000), AUC 0.9989423155784607
Test Epoch7 threshold 0.2 Acc 0.9751285714285715, AUC 0.9983930587768555, avg_entr 0.009082869626581669
Test Epoch7 threshold 0.4 Acc 0.9748285714285714, AUC 0.9983983635902405, avg_entr 0.009556081146001816
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60//dbpedia_14_transformeral_l5.pt  ,ep 7
Test Epoch7 threshold 0.6 Acc 0.9748285714285714, AUC 0.9983983635902405, avg_entr 0.009556081146001816
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60//dbpedia_14_transformeral_l5.pt  ,ep 7
Test Epoch7 threshold 0.8 Acc 0.9748285714285714, AUC 0.9983983635902405, avg_entr 0.009556081146001816
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad60//dbpedia_14_transformeral_l5.pt  ,ep 7
gc 0
Train Epoch8 Acc 0.9887875 (553721/560000), AUC 0.9989230036735535
Test Epoch8 threshold 0.2 Acc 0.9751285714285715, AUC 0.9983906745910645, avg_entr 0.009068331681191921
Test Epoch8 threshold 0.4 Acc 0.9747857142857143, AUC 0.9983962774276733, avg_entr 0.00951261818408966
Test Epoch8 threshold 0.6 Acc 0.9747857142857143, AUC 0.9983962774276733, avg_entr 0.00951261818408966
Test Epoch8 threshold 0.8 Acc 0.9747857142857143, AUC 0.9983962774276733, avg_entr 0.00951261818408966
gc 0
Train Epoch9 Acc 0.9887625 (553707/560000), AUC 0.9989351630210876
Test Epoch9 threshold 0.2 Acc 0.9750428571428571, AUC 0.9983921647071838, avg_entr 0.009069403633475304
Test Epoch9 threshold 0.4 Acc 0.9747, AUC 0.9983974695205688, avg_entr 0.009502038359642029
Test Epoch9 threshold 0.6 Acc 0.9747, AUC 0.9983974695205688, avg_entr 0.009502038359642029
Test Epoch9 threshold 0.8 Acc 0.9747, AUC 0.9983974695205688, avg_entr 0.009502038359642029
gc 0
Train Epoch10 Acc 0.9888017857142857 (553729/560000), AUC 0.9989516139030457
Test Epoch10 threshold 0.2 Acc 0.9750571428571428, AUC 0.9983919262886047, avg_entr 0.009068158455193043
Test Epoch10 threshold 0.4 Acc 0.9747428571428571, AUC 0.998397707939148, avg_entr 0.009500840678811073
Test Epoch10 threshold 0.6 Acc 0.9747428571428571, AUC 0.998397707939148, avg_entr 0.009500840678811073
Test Epoch10 threshold 0.8 Acc 0.9747428571428571, AUC 0.998397707939148, avg_entr 0.009500840678811073
gc 0
Train Epoch11 Acc 0.9888446428571429 (553753/560000), AUC 0.9989624619483948
Test Epoch11 threshold 0.2 Acc 0.9751, AUC 0.9983914494514465, avg_entr 0.009067604318261147
Test Epoch11 threshold 0.4 Acc 0.9747714285714286, AUC 0.9983969330787659, avg_entr 0.009501272812485695
Test Epoch11 threshold 0.6 Acc 0.9747714285714286, AUC 0.9983969330787659, avg_entr 0.009501272812485695
Test Epoch11 threshold 0.8 Acc 0.9747714285714286, AUC 0.9983969330787659, avg_entr 0.009501272812485695
gc 0
Train Epoch12 Acc 0.9887714285714285 (553712/560000), AUC 0.9989835619926453
Test Epoch12 threshold 0.2 Acc 0.9750428571428571, AUC 0.9983916878700256, avg_entr 0.0090755894780159
Test Epoch12 threshold 0.4 Acc 0.9747571428571429, AUC 0.9983975291252136, avg_entr 0.00950400996953249
Test Epoch12 threshold 0.6 Acc 0.9747571428571429, AUC 0.9983975291252136, avg_entr 0.00950400996953249
Test Epoch12 threshold 0.8 Acc 0.9747571428571429, AUC 0.9983975291252136, avg_entr 0.00950400996953249
gc 0
Train Epoch13 Acc 0.9888375 (553749/560000), AUC 0.9989542365074158
Test Epoch13 threshold 0.2 Acc 0.9750571428571428, AUC 0.9983921051025391, avg_entr 0.009071017615497112
Test Epoch13 threshold 0.4 Acc 0.9747285714285714, AUC 0.998397707939148, avg_entr 0.009501752443611622
Test Epoch13 threshold 0.6 Acc 0.9747285714285714, AUC 0.998397707939148, avg_entr 0.009501752443611622
Test Epoch13 threshold 0.8 Acc 0.9747285714285714, AUC 0.998397707939148, avg_entr 0.009501752443611622
gc 0
Train Epoch14 Acc 0.9888660714285714 (553765/560000), AUC 0.998960018157959
Test Epoch14 threshold 0.2 Acc 0.9750714285714286, AUC 0.9983921051025391, avg_entr 0.009067111648619175
Test Epoch14 threshold 0.4 Acc 0.9747428571428571, AUC 0.9983976483345032, avg_entr 0.00950070284307003
Test Epoch14 threshold 0.6 Acc 0.9747428571428571, AUC 0.9983976483345032, avg_entr 0.00950070284307003
Test Epoch14 threshold 0.8 Acc 0.9747428571428571, AUC 0.9983976483345032, avg_entr 0.00950070284307003
gc 0
Train Epoch15 Acc 0.9888035714285714 (553730/560000), AUC 0.9989604949951172
Test Epoch15 threshold 0.2 Acc 0.9750714285714286, AUC 0.9983921051025391, avg_entr 0.009067332372069359
Test Epoch15 threshold 0.4 Acc 0.9747428571428571, AUC 0.9983976483345032, avg_entr 0.009500906802713871
Test Epoch15 threshold 0.6 Acc 0.9747428571428571, AUC 0.9983976483345032, avg_entr 0.009500906802713871
Test Epoch15 threshold 0.8 Acc 0.9747428571428571, AUC 0.9983976483345032, avg_entr 0.009500906802713871
gc 0
Train Epoch16 Acc 0.9888625 (553763/560000), AUC 0.9989342093467712
Test Epoch16 threshold 0.2 Acc 0.9750571428571428, AUC 0.9983919858932495, avg_entr 0.009069978259503841
Test Epoch16 threshold 0.4 Acc 0.9747428571428571, AUC 0.9983976483345032, avg_entr 0.009500774554908276
Test Epoch16 threshold 0.6 Acc 0.9747428571428571, AUC 0.9983976483345032, avg_entr 0.009500774554908276
Test Epoch16 threshold 0.8 Acc 0.9747428571428571, AUC 0.9983976483345032, avg_entr 0.009500774554908276
gc 0
Train Epoch17 Acc 0.9888196428571429 (553739/560000), AUC 0.9989734292030334
Test Epoch17 threshold 0.2 Acc 0.9750571428571428, AUC 0.9983921051025391, avg_entr 0.009069855324923992
Test Epoch17 threshold 0.4 Acc 0.9747428571428571, AUC 0.9983976483345032, avg_entr 0.009500661864876747
Test Epoch17 threshold 0.6 Acc 0.9747428571428571, AUC 0.9983976483345032, avg_entr 0.009500661864876747
Test Epoch17 threshold 0.8 Acc 0.9747428571428571, AUC 0.9983976483345032, avg_entr 0.009500661864876747
gc 0
Train Epoch18 Acc 0.9888071428571429 (553732/560000), AUC 0.9989681243896484
Test Epoch18 threshold 0.2 Acc 0.9750571428571428, AUC 0.9983921051025391, avg_entr 0.009069799445569515
Test Epoch18 threshold 0.4 Acc 0.9747428571428571, AUC 0.9983976483345032, avg_entr 0.009500627405941486
Test Epoch18 threshold 0.6 Acc 0.9747428571428571, AUC 0.9983976483345032, avg_entr 0.009500627405941486
Test Epoch18 threshold 0.8 Acc 0.9747428571428571, AUC 0.9983976483345032, avg_entr 0.009500627405941486
gc 0
Train Epoch19 Acc 0.9888660714285714 (553765/560000), AUC 0.9989511370658875
Test Epoch19 threshold 0.2 Acc 0.9750714285714286, AUC 0.9983922243118286, avg_entr 0.009066922590136528
Test Epoch19 threshold 0.4 Acc 0.9747428571428571, AUC 0.998397707939148, avg_entr 0.009500603191554546
Test Epoch19 threshold 0.6 Acc 0.9747428571428571, AUC 0.998397707939148, avg_entr 0.009500603191554546
Test Epoch19 threshold 0.8 Acc 0.9747428571428571, AUC 0.998397707939148, avg_entr 0.009500603191554546
gc 0
Train Epoch20 Acc 0.9888160714285714 (553737/560000), AUC 0.9989500641822815
Test Epoch20 threshold 0.2 Acc 0.9750714285714286, AUC 0.9983921051025391, avg_entr 0.009066875092685223
Test Epoch20 threshold 0.4 Acc 0.9747428571428571, AUC 0.998397707939148, avg_entr 0.009500577114522457
Test Epoch20 threshold 0.6 Acc 0.9747428571428571, AUC 0.998397707939148, avg_entr 0.009500577114522457
Test Epoch20 threshold 0.8 Acc 0.9747428571428571, AUC 0.998397707939148, avg_entr 0.009500577114522457
gc 0
Train Epoch21 Acc 0.9888589285714285 (553761/560000), AUC 0.9989723563194275
Test Epoch21 threshold 0.2 Acc 0.9750714285714286, AUC 0.9983922243118286, avg_entr 0.009066853672266006
Test Epoch21 threshold 0.4 Acc 0.9747428571428571, AUC 0.998397707939148, avg_entr 0.00950055941939354
Test Epoch21 threshold 0.6 Acc 0.9747428571428571, AUC 0.998397707939148, avg_entr 0.00950055941939354
Test Epoch21 threshold 0.8 Acc 0.9747428571428571, AUC 0.998397707939148, avg_entr 0.00950055941939354
gc 0
Train Epoch22 Acc 0.9888160714285714 (553737/560000), AUC 0.9989407658576965
Test Epoch22 threshold 0.2 Acc 0.9750714285714286, AUC 0.9983922243118286, avg_entr 0.009066791273653507
Test Epoch22 threshold 0.4 Acc 0.9747428571428571, AUC 0.9983976483345032, avg_entr 0.009500524960458279
Test Epoch22 threshold 0.6 Acc 0.9747428571428571, AUC 0.9983976483345032, avg_entr 0.009500524960458279
Test Epoch22 threshold 0.8 Acc 0.9747428571428571, AUC 0.9983976483345032, avg_entr 0.009500524960458279
gc 0
Train Epoch23 Acc 0.9888 (553728/560000), AUC 0.9989547729492188
Test Epoch23 threshold 0.2 Acc 0.9750714285714286, AUC 0.9983922243118286, avg_entr 0.009066767990589142
Test Epoch23 threshold 0.4 Acc 0.9747428571428571, AUC 0.998397707939148, avg_entr 0.009500507265329361
Test Epoch23 threshold 0.6 Acc 0.9747428571428571, AUC 0.998397707939148, avg_entr 0.009500507265329361
Test Epoch23 threshold 0.8 Acc 0.9747428571428571, AUC 0.998397707939148, avg_entr 0.009500507265329361
gc 0
Train Epoch24 Acc 0.988825 (553742/560000), AUC 0.9989377856254578
Test Epoch24 threshold 0.2 Acc 0.9750714285714286, AUC 0.9983922243118286, avg_entr 0.009066730737686157
Test Epoch24 threshold 0.4 Acc 0.9747428571428571, AUC 0.998397707939148, avg_entr 0.009500483982264996
Test Epoch24 threshold 0.6 Acc 0.9747428571428571, AUC 0.998397707939148, avg_entr 0.009500483982264996
Test Epoch24 threshold 0.8 Acc 0.9747428571428571, AUC 0.998397707939148, avg_entr 0.009500483982264996
gc 0
Train Epoch25 Acc 0.9888017857142857 (553729/560000), AUC 0.9989734292030334
Test Epoch25 threshold 0.2 Acc 0.9750714285714286, AUC 0.9983921647071838, avg_entr 0.009066709317266941
Test Epoch25 threshold 0.4 Acc 0.9747428571428571, AUC 0.998397707939148, avg_entr 0.0095004728063941
Test Epoch25 threshold 0.6 Acc 0.9747428571428571, AUC 0.998397707939148, avg_entr 0.0095004728063941
Test Epoch25 threshold 0.8 Acc 0.9747428571428571, AUC 0.998397707939148, avg_entr 0.0095004728063941
gc 0
Train Epoch26 Acc 0.9888357142857143 (553748/560000), AUC 0.9989761710166931
Test Epoch26 threshold 0.2 Acc 0.9750714285714286, AUC 0.9983921051025391, avg_entr 0.00906669907271862
Test Epoch26 threshold 0.4 Acc 0.9747428571428571, AUC 0.998397707939148, avg_entr 0.009500444866716862
Test Epoch26 threshold 0.6 Acc 0.9747428571428571, AUC 0.998397707939148, avg_entr 0.009500444866716862
Test Epoch26 threshold 0.8 Acc 0.9747428571428571, AUC 0.998397707939148, avg_entr 0.009500444866716862
gc 0
Train Epoch27 Acc 0.9888625 (553763/560000), AUC 0.9989388585090637
Test Epoch27 threshold 0.2 Acc 0.9750714285714286, AUC 0.9983922243118286, avg_entr 0.009066645987331867
Test Epoch27 threshold 0.4 Acc 0.9747428571428571, AUC 0.998397707939148, avg_entr 0.009500425308942795
Test Epoch27 threshold 0.6 Acc 0.9747428571428571, AUC 0.998397707939148, avg_entr 0.009500425308942795
Test Epoch27 threshold 0.8 Acc 0.9747428571428571, AUC 0.998397707939148, avg_entr 0.009500425308942795
gc 0
Train Epoch28 Acc 0.9888107142857143 (553734/560000), AUC 0.9989556074142456
Test Epoch28 threshold 0.2 Acc 0.9750714285714286, AUC 0.9983922243118286, avg_entr 0.009066639468073845
Test Epoch28 threshold 0.4 Acc 0.9747428571428571, AUC 0.998397707939148, avg_entr 0.009500427171587944
Test Epoch28 threshold 0.6 Acc 0.9747428571428571, AUC 0.998397707939148, avg_entr 0.009500427171587944
Test Epoch28 threshold 0.8 Acc 0.9747428571428571, AUC 0.998397707939148, avg_entr 0.009500427171587944
gc 0
Train Epoch29 Acc 0.988825 (553742/560000), AUC 0.9989722371101379
Test Epoch29 threshold 0.2 Acc 0.9750714285714286, AUC 0.9983922243118286, avg_entr 0.009066623635590076
Test Epoch29 threshold 0.4 Acc 0.9747428571428571, AUC 0.998397707939148, avg_entr 0.009500399231910706
Test Epoch29 threshold 0.6 Acc 0.9747428571428571, AUC 0.998397707939148, avg_entr 0.009500399231910706
Test Epoch29 threshold 0.8 Acc 0.9747428571428571, AUC 0.998397707939148, avg_entr 0.009500399231910706
Best AUC 0.9983983635902405
train_loss (2, 5, 30)
valid_acc (30, 4)
valid_AUC (30, 4)
train_acc (30,)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Start Testing
Load ckpt at ckpt/dbpedia_14_transformeral_l5_pad60//dbpedia_14_transformeral_l5.pt
[[4700   42   20   15   12   66   43    7    4    6    5   18   19   43]
 [  33 4897    2    1    7    0   33    9    3    2    1    0    4    8]
 [  35   13 4636   18   50    2    9    1    3    0    1   81   22  129]
 [   3    2   27 4953   11    0    1    0    0    0    0    0    0    3]
 [  12   18   67   12 4860    9    6    1    2    1    0    0    2   10]
 [  35    1    3    3    1 4944    5    3    1    2    0    0    1    1]
 [  60   46    6    1   10   15 4801   38    9    5    1    1    4    3]
 [   0    0    0    0    2    0   12 4963   18    3    1    0    0    1]
 [   1    4    2    0    3    0   10   13 4967    0    0    0    0    0]
 [   1    0    1    2    0    0    0    7    0 4959   29    0    0    1]
 [  13    1    0    0    0    0    2    4    0   34 4946    0    0    0]
 [   7    0   34    2    0    0    1    0    0    0    0 4933   11   12]
 [   7    2   24    4    1    3    1    3    1    2    0   22 4885   45]
 [  31    6   81    7   12    3    3    9    1    5    2    6   40 4794]]
Figure(640x480)
tensor([1.8016e-06, 1.8078e-01, 1.6825e-04,  ..., 1.7679e-06, 1.0730e-03,
        6.6706e-05])
[[4748   38   21    7   13   45   50    3    0    4    2   14   13   42]
 [  35 4909    7    1    7    0   32    0    1    1    1    0    1    5]
 [  28   11 4742   10   61    2    7    0    0    0    2   43   21   73]
 [   5    1   15 4962   10    1    0    0    0    1    0    1    2    2]
 [   7   10   70    9 4878    6    6    0    2    3    0    0    4    5]
 [  44    0    2    0    0 4941    6    2    1    0    0    0    2    2]
 [  54   35    5    1    7   10 4850   24    5    3    0    1    2    3]
 [   1    1    0    0    0    0   16 4963   12    5    1    0    0    1]
 [   2    0    1    0    4    0   10   10 4972    0    0    0    0    1]
 [   1    0    1    1    0    0    0    5    0 4958   32    0    0    2]
 [  12    1    0    0    0    3    1    0    0   31 4952    0    0    0]
 [   6    0   36    2    0    0    0    0    0    0    0 4937   10    9]
 [   8    0   14    0    1    0    0    0    0    0    0   21 4919   37]
 [  27    3   71    3    3    1    4    0    0    1    3    8   42 4834]]
Figure(640x480)
tensor([1.1975e-06, 1.1703e-01, 2.1919e-06,  ..., 1.2021e-06, 1.5438e-06,
        1.4104e-06])
[[4750   38   22    7   13   45   53    2    0    4    2   14   13   37]
 [  34 4914    5    0    8    0   30    0    1    1    1    0    0    6]
 [  28    8 4750   10   61    1    8    1    1    0    2   42   18   70]
 [   5    1   17 4961   11    1    0    0    0    1    0    1    1    1]
 [   6    9   74    7 4878    6    8    0    2    2    0    0    3    5]
 [  43    0    2    0    0 4943    5    2    1    0    0    0    2    2]
 [  54   35    4    1    6   12 4850   24    5    3    0    1    2    3]
 [   1    1    0    0    0    0   15 4964   12    5    1    0    0    1]
 [   2    0    1    0    3    0    9    9 4975    0    0    0    0    1]
 [   1    0    1    0    0    0    0    6    0 4961   29    0    0    2]
 [  13    1    0    0    0    3    1    0    0   28 4954    0    0    0]
 [   8    0   37    2    0    0    0    0    0    0    0 4935    9    9]
 [   7    0   16    0    0    0    0    0    0    0    0   20 4915   42]
 [  29    3   76    3    2    1    4    0    0    1    1    8   41 4831]]
Figure(640x480)
tensor([1.7686e-06, 3.4284e-05, 2.3165e-06,  ..., 1.5000e-06, 1.5016e-06,
        1.4052e-06])
[[4749   37   20    7   13   45   55    3    0    4    2   14   13   38]
 [  35 4912    5    0    8    0   31    0    1    1    1    0    0    6]
 [  28    7 4749   11   63    1    8    1    1    0    2   42   18   69]
 [   5    1   16 4962   11    1    0    0    0    1    0    1    1    1]
 [   6    9   72    7 4880    6    8    0    2    2    0    0    3    5]
 [  44    0    2    0    0 4944    4    2    1    0    0    0    2    1]
 [  52   35    4    1    6   12 4852   24    5    3    0    1    2    3]
 [   1    1    0    0    0    0   16 4963   12    5    0    0    1    1]
 [   2    0    1    0    3    0   10    9 4974    0    0    0    0    1]
 [   1    0    1    0    0    0    0    6    0 4962   29    0    0    1]
 [  12    1    0    0    0    3    1    0    0   28 4954    0    0    1]
 [   8    0   38    2    0    0    0    0    0    0    0 4933   10    9]
 [   7    1   16    0    0    0    0    0    0    0    0   21 4914   41]
 [  29    3   78    2    2    1    4    0    0    1    0    8   41 4831]]
Figure(640x480)
tensor([1.8151e-06, 6.9006e-06, 1.9943e-06,  ..., 1.4416e-06, 1.4253e-06,
        1.3962e-06])
[[4750   37   20    7   13   45   54    3    0    4    2   14   13   38]
 [  35 4913    5    0    8    0   30    0    1    1    1    0    0    6]
 [  28    8 4747   11   64    1    8    1    1    0    2   42   18   69]
 [   5    1   17 4961   11    1    0    0    0    1    0    1    1    1]
 [   6    9   72    7 4880    6    7    0    2    2    0    0    3    6]
 [  45    0    2    0    0 4943    4    2    1    0    0    0    2    1]
 [  54   34    4    1    6   12 4851   24    5    3    0    1    2    3]
 [   1    1    0    0    0    0   16 4963   12    5    0    0    1    1]
 [   2    0    1    0    4    0    9    9 4974    0    0    0    0    1]
 [   1    0    1    0    0    0    0    6    0 4963   28    0    0    1]
 [  12    1    0    0    0    3    1    0    0   28 4954    0    0    1]
 [   8    0   38    2    0    0    0    0    0    0    0 4932   11    9]
 [   7    1   17    0    0    0    0    0    0    0    0   21 4913   41]
 [  29    3   77    2    2    1    4    0    0    1    0    8   41 4832]]
Figure(640x480)
tensor([1.6027e-06, 2.8078e-06, 1.6179e-06,  ..., 1.3561e-06, 1.2605e-06,
        1.3372e-06])
