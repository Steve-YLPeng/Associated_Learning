total count words 102019
vocab size 30000
found 26754 words in glove
model: TransformerModelML(
  (layers): ModuleList(
    (0): EMBLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Embedding(30000, 300)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=4, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=4, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.weight 9000000
layers.0.ae.g.0.weight 512
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 512
layers.0.ae.h.0.bias 4
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
Total Trainable Params: 13672292
Start Training
gc 0
Train Epoch0 Acc 0.6354166666666666 (76250/120000), AUC 0.8608369827270508
Test Epoch0 threshold 0.2 Acc 0.9071052631578947, AUC 0.975147545337677, avg_entr 0.13440565764904022
Save ckpt to ckpt/ag_news_transformeral_l5_pad150//ag_news_transformeral_l5.pt  ,ep 0
Test Epoch0 threshold 0.4 Acc 0.9069736842105263, AUC 0.9751585125923157, avg_entr 0.15722160041332245
Save ckpt to ckpt/ag_news_transformeral_l5_pad150//ag_news_transformeral_l5.pt  ,ep 0
Test Epoch0 threshold 0.6 Acc 0.9021052631578947, AUC 0.9756281971931458, avg_entr 0.1784437596797943
Save ckpt to ckpt/ag_news_transformeral_l5_pad150//ag_news_transformeral_l5.pt  ,ep 0
Test Epoch0 threshold 0.8 Acc 0.9018421052631579, AUC 0.9755967855453491, avg_entr 0.1787908673286438
gc 0
Train Epoch1 Acc 0.9215833333333333 (110590/120000), AUC 0.9818694591522217
Test Epoch1 threshold 0.2 Acc 0.9163157894736842, AUC 0.9788981676101685, avg_entr 0.07154900580644608
Save ckpt to ckpt/ag_news_transformeral_l5_pad150//ag_news_transformeral_l5.pt  ,ep 1
Test Epoch1 threshold 0.4 Acc 0.9161842105263158, AUC 0.9790353775024414, avg_entr 0.0943278968334198
Save ckpt to ckpt/ag_news_transformeral_l5_pad150//ag_news_transformeral_l5.pt  ,ep 1
Test Epoch1 threshold 0.6 Acc 0.9136842105263158, AUC 0.9792541265487671, avg_entr 0.10430900752544403
Save ckpt to ckpt/ag_news_transformeral_l5_pad150//ag_news_transformeral_l5.pt  ,ep 1
Test Epoch1 threshold 0.8 Acc 0.9136842105263158, AUC 0.9792541265487671, avg_entr 0.10430900752544403
Save ckpt to ckpt/ag_news_transformeral_l5_pad150//ag_news_transformeral_l5.pt  ,ep 1
gc 0
Train Epoch2 Acc 0.936025 (112323/120000), AUC 0.9869688749313354
Test Epoch2 threshold 0.2 Acc 0.9185526315789474, AUC 0.9788674116134644, avg_entr 0.047692589461803436
Test Epoch2 threshold 0.4 Acc 0.9180263157894737, AUC 0.9804264903068542, avg_entr 0.0713583454489708
Save ckpt to ckpt/ag_news_transformeral_l5_pad150//ag_news_transformeral_l5.pt  ,ep 2
Test Epoch2 threshold 0.6 Acc 0.9176315789473685, AUC 0.9807506799697876, avg_entr 0.08003469556570053
Save ckpt to ckpt/ag_news_transformeral_l5_pad150//ag_news_transformeral_l5.pt  ,ep 2
Test Epoch2 threshold 0.8 Acc 0.9176315789473685, AUC 0.9807506799697876, avg_entr 0.08003469556570053
Save ckpt to ckpt/ag_news_transformeral_l5_pad150//ag_news_transformeral_l5.pt  ,ep 2
gc 0
Train Epoch3 Acc 0.9436833333333333 (113242/120000), AUC 0.9890707731246948
Test Epoch3 threshold 0.2 Acc 0.9194736842105263, AUC 0.9780707955360413, avg_entr 0.03907858952879906
Test Epoch3 threshold 0.4 Acc 0.9190789473684211, AUC 0.9811147451400757, avg_entr 0.060618072748184204
Save ckpt to ckpt/ag_news_transformeral_l5_pad150//ag_news_transformeral_l5.pt  ,ep 3
Test Epoch3 threshold 0.6 Acc 0.9181578947368421, AUC 0.9814136028289795, avg_entr 0.06719593703746796
Save ckpt to ckpt/ag_news_transformeral_l5_pad150//ag_news_transformeral_l5.pt  ,ep 3
Test Epoch3 threshold 0.8 Acc 0.9181578947368421, AUC 0.9814136028289795, avg_entr 0.06719593703746796
Save ckpt to ckpt/ag_news_transformeral_l5_pad150//ag_news_transformeral_l5.pt  ,ep 3
gc 0
Train Epoch4 Acc 0.9489833333333333 (113878/120000), AUC 0.9906240701675415
Test Epoch4 threshold 0.2 Acc 0.9185526315789474, AUC 0.9788392186164856, avg_entr 0.03327323496341705
Test Epoch4 threshold 0.4 Acc 0.9182894736842105, AUC 0.9811553955078125, avg_entr 0.05280080437660217
Test Epoch4 threshold 0.6 Acc 0.9178947368421052, AUC 0.9815926551818848, avg_entr 0.05870744585990906
Save ckpt to ckpt/ag_news_transformeral_l5_pad150//ag_news_transformeral_l5.pt  ,ep 4
Test Epoch4 threshold 0.8 Acc 0.9178947368421052, AUC 0.9815926551818848, avg_entr 0.05870744585990906
Save ckpt to ckpt/ag_news_transformeral_l5_pad150//ag_news_transformeral_l5.pt  ,ep 4
gc 0
Train Epoch5 Acc 0.9532083333333333 (114385/120000), AUC 0.9915152788162231
Test Epoch5 threshold 0.2 Acc 0.9171052631578948, AUC 0.9774377942085266, avg_entr 0.028514962643384933
Test Epoch5 threshold 0.4 Acc 0.9178947368421052, AUC 0.9813015460968018, avg_entr 0.04851916432380676
Test Epoch5 threshold 0.6 Acc 0.9177631578947368, AUC 0.9818370342254639, avg_entr 0.054145436733961105
Save ckpt to ckpt/ag_news_transformeral_l5_pad150//ag_news_transformeral_l5.pt  ,ep 5
Test Epoch5 threshold 0.8 Acc 0.9177631578947368, AUC 0.9818370342254639, avg_entr 0.054145436733961105
Save ckpt to ckpt/ag_news_transformeral_l5_pad150//ag_news_transformeral_l5.pt  ,ep 5
gc 0
Train Epoch6 Acc 0.956625 (114795/120000), AUC 0.9924906492233276
Test Epoch6 threshold 0.2 Acc 0.9157894736842105, AUC 0.9769937992095947, avg_entr 0.02567499689757824
Test Epoch6 threshold 0.4 Acc 0.9178947368421052, AUC 0.9812070727348328, avg_entr 0.0441027395427227
Test Epoch6 threshold 0.6 Acc 0.9178947368421052, AUC 0.9817285537719727, avg_entr 0.04975814372301102
Test Epoch6 threshold 0.8 Acc 0.9178947368421052, AUC 0.9817285537719727, avg_entr 0.04975814372301102
gc 0
Train Epoch7 Acc 0.961 (115320/120000), AUC 0.993929922580719
Test Epoch7 threshold 0.2 Acc 0.9131578947368421, AUC 0.9766932129859924, avg_entr 0.023531312122941017
Test Epoch7 threshold 0.4 Acc 0.9180263157894737, AUC 0.9811469912528992, avg_entr 0.041021961718797684
Test Epoch7 threshold 0.6 Acc 0.9186842105263158, AUC 0.9816247224807739, avg_entr 0.04678211733698845
Test Epoch7 threshold 0.8 Acc 0.9186842105263158, AUC 0.9816247224807739, avg_entr 0.04678211733698845
gc 0
Train Epoch8 Acc 0.96335 (115602/120000), AUC 0.9945988059043884
Test Epoch8 threshold 0.2 Acc 0.9151315789473684, AUC 0.9770722985267639, avg_entr 0.023102419450879097
Test Epoch8 threshold 0.4 Acc 0.9189473684210526, AUC 0.9809927940368652, avg_entr 0.040671974420547485
Test Epoch8 threshold 0.6 Acc 0.9182894736842105, AUC 0.9815400242805481, avg_entr 0.04544179141521454
Test Epoch8 threshold 0.8 Acc 0.9184210526315789, AUC 0.9815748929977417, avg_entr 0.045481789857149124
gc 0
Train Epoch9 Acc 0.9642 (115704/120000), AUC 0.9950019121170044
Test Epoch9 threshold 0.2 Acc 0.9143421052631578, AUC 0.9766942262649536, avg_entr 0.022595873102545738
Test Epoch9 threshold 0.4 Acc 0.9184210526315789, AUC 0.9810050129890442, avg_entr 0.04008578136563301
Test Epoch9 threshold 0.6 Acc 0.9190789473684211, AUC 0.981569766998291, avg_entr 0.04500788822770119
Test Epoch9 threshold 0.8 Acc 0.9192105263157895, AUC 0.9816123247146606, avg_entr 0.04505973309278488
gc 0
Train Epoch10 Acc 0.9650833333333333 (115810/120000), AUC 0.9951757788658142
Test Epoch10 threshold 0.2 Acc 0.9135526315789474, AUC 0.9768535494804382, avg_entr 0.02244921401143074
Test Epoch10 threshold 0.4 Acc 0.9178947368421052, AUC 0.9809441566467285, avg_entr 0.03959418088197708
Test Epoch10 threshold 0.6 Acc 0.9182894736842105, AUC 0.9815424680709839, avg_entr 0.04477611184120178
Test Epoch10 threshold 0.8 Acc 0.9184210526315789, AUC 0.9815948009490967, avg_entr 0.04482061788439751
gc 0
Train Epoch11 Acc 0.96525 (115830/120000), AUC 0.9953342080116272
Test Epoch11 threshold 0.2 Acc 0.9153947368421053, AUC 0.9768127799034119, avg_entr 0.02206995151937008
Test Epoch11 threshold 0.4 Acc 0.9181578947368421, AUC 0.9809167981147766, avg_entr 0.03932648152112961
Test Epoch11 threshold 0.6 Acc 0.9189473684210526, AUC 0.9815428256988525, avg_entr 0.04398235306143761
Test Epoch11 threshold 0.8 Acc 0.9190789473684211, AUC 0.9815945625305176, avg_entr 0.04403716325759888
gc 0
Train Epoch12 Acc 0.9656333333333333 (115876/120000), AUC 0.9953357577323914
Test Epoch12 threshold 0.2 Acc 0.9140789473684211, AUC 0.9768499732017517, avg_entr 0.021963153034448624
Test Epoch12 threshold 0.4 Acc 0.9182894736842105, AUC 0.9809871912002563, avg_entr 0.03940830007195473
Test Epoch12 threshold 0.6 Acc 0.9181578947368421, AUC 0.9815412163734436, avg_entr 0.043914858251810074
Test Epoch12 threshold 0.8 Acc 0.9182894736842105, AUC 0.9815918803215027, avg_entr 0.04396416246891022
gc 0
Train Epoch13 Acc 0.9655916666666666 (115871/120000), AUC 0.9954524636268616
Test Epoch13 threshold 0.2 Acc 0.9144736842105263, AUC 0.9768701791763306, avg_entr 0.022028520703315735
Test Epoch13 threshold 0.4 Acc 0.9184210526315789, AUC 0.9809510111808777, avg_entr 0.03913910314440727
Test Epoch13 threshold 0.6 Acc 0.9190789473684211, AUC 0.9815401434898376, avg_entr 0.043670181185007095
Test Epoch13 threshold 0.8 Acc 0.9192105263157895, AUC 0.9815904498100281, avg_entr 0.04372096061706543
gc 0
Train Epoch14 Acc 0.9654916666666666 (115859/120000), AUC 0.9953807592391968
Test Epoch14 threshold 0.2 Acc 0.9139473684210526, AUC 0.9768157005310059, avg_entr 0.021786373108625412
Test Epoch14 threshold 0.4 Acc 0.9181578947368421, AUC 0.9809534549713135, avg_entr 0.03892413526773453
Test Epoch14 threshold 0.6 Acc 0.9180263157894737, AUC 0.9815357327461243, avg_entr 0.043553370982408524
Test Epoch14 threshold 0.8 Acc 0.9181578947368421, AUC 0.9815860986709595, avg_entr 0.043602146208286285
gc 0
Train Epoch15 Acc 0.9655583333333333 (115867/120000), AUC 0.9954822063446045
Test Epoch15 threshold 0.2 Acc 0.9142105263157895, AUC 0.9767825603485107, avg_entr 0.021844182163476944
Test Epoch15 threshold 0.4 Acc 0.9184210526315789, AUC 0.9809067845344543, avg_entr 0.03888958320021629
Test Epoch15 threshold 0.6 Acc 0.9182894736842105, AUC 0.9815382957458496, avg_entr 0.04348176345229149
Test Epoch15 threshold 0.8 Acc 0.9184210526315789, AUC 0.98158860206604, avg_entr 0.04353058338165283
gc 0
Train Epoch16 Acc 0.9653583333333333 (115843/120000), AUC 0.9954554438591003
Test Epoch16 threshold 0.2 Acc 0.9142105263157895, AUC 0.9767926931381226, avg_entr 0.021663589403033257
Test Epoch16 threshold 0.4 Acc 0.9185526315789474, AUC 0.9809525012969971, avg_entr 0.038958530873060226
Test Epoch16 threshold 0.6 Acc 0.9188157894736843, AUC 0.9815405607223511, avg_entr 0.04338783770799637
Test Epoch16 threshold 0.8 Acc 0.9189473684210526, AUC 0.9815902709960938, avg_entr 0.043437279760837555
gc 0
Train Epoch17 Acc 0.9656833333333333 (115882/120000), AUC 0.9955432415008545
Test Epoch17 threshold 0.2 Acc 0.9142105263157895, AUC 0.9768239855766296, avg_entr 0.02166728302836418
Test Epoch17 threshold 0.4 Acc 0.9184210526315789, AUC 0.9809762239456177, avg_entr 0.038894183933734894
Test Epoch17 threshold 0.6 Acc 0.9182894736842105, AUC 0.9815418720245361, avg_entr 0.04334703087806702
Test Epoch17 threshold 0.8 Acc 0.9184210526315789, AUC 0.981591522693634, avg_entr 0.043396223336458206
gc 0
Train Epoch18 Acc 0.9656666666666667 (115880/120000), AUC 0.9954394102096558
Test Epoch18 threshold 0.2 Acc 0.9142105263157895, AUC 0.9768226146697998, avg_entr 0.021642275154590607
Test Epoch18 threshold 0.4 Acc 0.9184210526315789, AUC 0.9809757471084595, avg_entr 0.03887057304382324
Test Epoch18 threshold 0.6 Acc 0.9184210526315789, AUC 0.9815420508384705, avg_entr 0.04332324489951134
Test Epoch18 threshold 0.8 Acc 0.9185526315789474, AUC 0.9815917611122131, avg_entr 0.04337240010499954
gc 0
Train Epoch19 Acc 0.9654833333333334 (115858/120000), AUC 0.9954553246498108
Test Epoch19 threshold 0.2 Acc 0.9142105263157895, AUC 0.9768202304840088, avg_entr 0.021635640412569046
Test Epoch19 threshold 0.4 Acc 0.9182894736842105, AUC 0.9809696078300476, avg_entr 0.03888210281729698
Test Epoch19 threshold 0.6 Acc 0.9184210526315789, AUC 0.9815417528152466, avg_entr 0.043312087655067444
Test Epoch19 threshold 0.8 Acc 0.9185526315789474, AUC 0.9815915822982788, avg_entr 0.04336123913526535
gc 0
Train Epoch20 Acc 0.9658583333333334 (115903/120000), AUC 0.9955407977104187
Test Epoch20 threshold 0.2 Acc 0.9142105263157895, AUC 0.9768196940422058, avg_entr 0.021630413830280304
Test Epoch20 threshold 0.4 Acc 0.9182894736842105, AUC 0.9809694290161133, avg_entr 0.03887466341257095
Test Epoch20 threshold 0.6 Acc 0.9184210526315789, AUC 0.9815417528152466, avg_entr 0.043304745107889175
Test Epoch20 threshold 0.8 Acc 0.9185526315789474, AUC 0.9815915822982788, avg_entr 0.043353885412216187
gc 0
Train Epoch21 Acc 0.9654916666666666 (115859/120000), AUC 0.9955229759216309
Test Epoch21 threshold 0.2 Acc 0.9142105263157895, AUC 0.9768187999725342, avg_entr 0.021623868495225906
Test Epoch21 threshold 0.4 Acc 0.9182894736842105, AUC 0.9809693098068237, avg_entr 0.03887200355529785
Test Epoch21 threshold 0.6 Acc 0.9184210526315789, AUC 0.9815417528152466, avg_entr 0.043302010744810104
Test Epoch21 threshold 0.8 Acc 0.9185526315789474, AUC 0.9815916419029236, avg_entr 0.04335111752152443
gc 0
Train Epoch22 Acc 0.9655583333333333 (115867/120000), AUC 0.9955710172653198
Test Epoch22 threshold 0.2 Acc 0.9142105263157895, AUC 0.9768176674842834, avg_entr 0.02162593975663185
Test Epoch22 threshold 0.4 Acc 0.9182894736842105, AUC 0.9809691309928894, avg_entr 0.03887119144201279
Test Epoch22 threshold 0.6 Acc 0.9184210526315789, AUC 0.9815417528152466, avg_entr 0.04330132156610489
Test Epoch22 threshold 0.8 Acc 0.9185526315789474, AUC 0.9815915822982788, avg_entr 0.04335043579339981
gc 0
Train Epoch23 Acc 0.965875 (115905/120000), AUC 0.9954785108566284
Test Epoch23 threshold 0.2 Acc 0.9142105263157895, AUC 0.97681725025177, avg_entr 0.02162480354309082
Test Epoch23 threshold 0.4 Acc 0.9182894736842105, AUC 0.9809693694114685, avg_entr 0.03887029364705086
Test Epoch23 threshold 0.6 Acc 0.9184210526315789, AUC 0.9815418720245361, avg_entr 0.04330051690340042
Test Epoch23 threshold 0.8 Acc 0.9185526315789474, AUC 0.9815917015075684, avg_entr 0.04334964230656624
gc 0
Train Epoch24 Acc 0.965775 (115893/120000), AUC 0.9954317808151245
Test Epoch24 threshold 0.2 Acc 0.9142105263157895, AUC 0.9768170118331909, avg_entr 0.021623246371746063
Test Epoch24 threshold 0.4 Acc 0.9182894736842105, AUC 0.9809694290161133, avg_entr 0.038869090378284454
Test Epoch24 threshold 0.6 Acc 0.9184210526315789, AUC 0.9815419912338257, avg_entr 0.043299444019794464
Test Epoch24 threshold 0.8 Acc 0.9185526315789474, AUC 0.9815917611122131, avg_entr 0.043348584324121475
gc 0
Train Epoch25 Acc 0.9657 (115884/120000), AUC 0.995473325252533
Test Epoch25 threshold 0.2 Acc 0.9142105263157895, AUC 0.9768167734146118, avg_entr 0.02162194438278675
Test Epoch25 threshold 0.4 Acc 0.9182894736842105, AUC 0.9809694290161133, avg_entr 0.03886805474758148
Test Epoch25 threshold 0.6 Acc 0.9184210526315789, AUC 0.9815419316291809, avg_entr 0.043298494070768356
Test Epoch25 threshold 0.8 Acc 0.9185526315789474, AUC 0.9815918207168579, avg_entr 0.04334765672683716
gc 0
Train Epoch26 Acc 0.9657083333333333 (115885/120000), AUC 0.9955581426620483
Test Epoch26 threshold 0.2 Acc 0.9142105263157895, AUC 0.9768128991127014, avg_entr 0.021643487736582756
Test Epoch26 threshold 0.4 Acc 0.9182894736842105, AUC 0.9809694290161133, avg_entr 0.03886706382036209
Test Epoch26 threshold 0.6 Acc 0.9184210526315789, AUC 0.9815419912338257, avg_entr 0.04329754784703255
Test Epoch26 threshold 0.8 Acc 0.9185526315789474, AUC 0.9815918207168579, avg_entr 0.04334671050310135
gc 0
Train Epoch27 Acc 0.9656166666666667 (115874/120000), AUC 0.9953901171684265
Test Epoch27 threshold 0.2 Acc 0.9142105263157895, AUC 0.9768123030662537, avg_entr 0.021642114967107773
Test Epoch27 threshold 0.4 Acc 0.9182894736842105, AUC 0.9809694290161133, avg_entr 0.03886584937572479
Test Epoch27 threshold 0.6 Acc 0.9184210526315789, AUC 0.9815421104431152, avg_entr 0.043296411633491516
Test Epoch27 threshold 0.8 Acc 0.9185526315789474, AUC 0.9815918207168579, avg_entr 0.043345581740140915
gc 0
Train Epoch28 Acc 0.965725 (115887/120000), AUC 0.9955610632896423
Test Epoch28 threshold 0.2 Acc 0.9142105263157895, AUC 0.9768117666244507, avg_entr 0.02164090797305107
Test Epoch28 threshold 0.4 Acc 0.9182894736842105, AUC 0.9809694290161133, avg_entr 0.038864877074956894
Test Epoch28 threshold 0.6 Acc 0.9184210526315789, AUC 0.98154217004776, avg_entr 0.0432954765856266
Test Epoch28 threshold 0.8 Acc 0.9185526315789474, AUC 0.9815919399261475, avg_entr 0.0433446504175663
gc 0
Train Epoch29 Acc 0.9656166666666667 (115874/120000), AUC 0.995436429977417
Test Epoch29 threshold 0.2 Acc 0.9142105263157895, AUC 0.9768111705780029, avg_entr 0.021640079095959663
Test Epoch29 threshold 0.4 Acc 0.9182894736842105, AUC 0.9809695482254028, avg_entr 0.03886391967535019
Test Epoch29 threshold 0.6 Acc 0.9184210526315789, AUC 0.9815422892570496, avg_entr 0.043294508010149
Test Epoch29 threshold 0.8 Acc 0.9185526315789474, AUC 0.9815921187400818, avg_entr 0.043343670666217804
Best AUC 0.9818370342254639
train_loss (2, 5, 30)
valid_acc (30, 4)
valid_AUC (30, 4)
train_acc (30,)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Start Testing
Load ckpt at ckpt/ag_news_transformeral_l5_pad150//ag_news_transformeral_l5.pt
[[1712   60   76   52]
 [  15 1867    5   13]
 [  48   21 1650  181]
 [  48   14   92 1746]]
Figure(640x480)
tensor([0.0767, 0.0007, 0.0131,  ..., 0.0723, 0.0017, 0.0575])
[[1716   58   66   60]
 [  10 1870    9   11]
 [  56   20 1658  166]
 [  46   15  102 1737]]
Figure(640x480)
tensor([0.0177, 0.0007, 0.0007,  ..., 0.0019, 0.0009, 0.0008])
[[1714   57   66   63]
 [  11 1868    9   12]
 [  54   19 1656  171]
 [  43   13  104 1740]]
Figure(640x480)
tensor([0.0045, 0.0008, 0.0009,  ..., 0.0016, 0.0010, 0.0010])
[[1717   56   64   63]
 [  12 1867    9   12]
 [  56   20 1648  176]
 [  43   13  102 1742]]
Figure(640x480)
tensor([0.0042, 0.0008, 0.0009,  ..., 0.0013, 0.0012, 0.0012])
[[1715   56   64   65]
 [  11 1868    9   12]
 [  55   20 1644  181]
 [  43   12   99 1746]]
Figure(640x480)
tensor([0.0046, 0.0007, 0.0008,  ..., 0.0018, 0.0014, 0.0014])
