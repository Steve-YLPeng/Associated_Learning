total count words 102019
vocab size 30000
found 26754 words in glove
model: TransformerModelML(
  (layers): ModuleList(
    (0): EMBLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Embedding(30000, 300)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=4, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=4, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.weight 9000000
layers.0.ae.g.0.weight 512
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 512
layers.0.ae.h.0.bias 4
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
Total Trainable Params: 13672292
Start Training
gc 0
Train Epoch0 Acc 0.61645 (73974/120000), AUC 0.852303147315979
Test Epoch0 layer0 Acc 0.9084210526315789, AUC 0.9775804281234741, avg_entr 0.23737390339374542
Save ckpt to ckpt/ag_news_transformeral_l5_pad100//ag_news_transformeral_l5.pt  ,ep 0
Test Epoch0 layer1 Acc 0.9102631578947369, AUC 0.9790676832199097, avg_entr 0.1638900339603424
Save ckpt to ckpt/ag_news_transformeral_l5_pad100//ag_news_transformeral_l5.pt  ,ep 0
Test Epoch0 layer2 Acc 0.9094736842105263, AUC 0.979224681854248, avg_entr 0.15653793513774872
Save ckpt to ckpt/ag_news_transformeral_l5_pad100//ag_news_transformeral_l5.pt  ,ep 0
Test Epoch0 layer3 Acc 0.9089473684210526, AUC 0.9792459011077881, avg_entr 0.1511496901512146
Save ckpt to ckpt/ag_news_transformeral_l5_pad100//ag_news_transformeral_l5.pt  ,ep 0
Test Epoch0 layer4 Acc 0.9077631578947368, AUC 0.979475736618042, avg_entr 0.1442733258008957
Save ckpt to ckpt/ag_news_transformeral_l5_pad100//ag_news_transformeral_l5.pt  ,ep 0
gc 0
Train Epoch1 Acc 0.9234416666666667 (110813/120000), AUC 0.9824249744415283
Test Epoch1 layer0 Acc 0.9161842105263158, AUC 0.9805854558944702, avg_entr 0.14172343909740448
Save ckpt to ckpt/ag_news_transformeral_l5_pad100//ag_news_transformeral_l5.pt  ,ep 1
Test Epoch1 layer1 Acc 0.9176315789473685, AUC 0.9821211695671082, avg_entr 0.08218137919902802
Save ckpt to ckpt/ag_news_transformeral_l5_pad100//ag_news_transformeral_l5.pt  ,ep 1
Test Epoch1 layer2 Acc 0.9182894736842105, AUC 0.9817999601364136, avg_entr 0.07068213820457458
Test Epoch1 layer3 Acc 0.9186842105263158, AUC 0.9817988872528076, avg_entr 0.060367800295352936
Test Epoch1 layer4 Acc 0.9188157894736843, AUC 0.9820803999900818, avg_entr 0.05429616943001747
gc 0
Train Epoch2 Acc 0.9373 (112476/120000), AUC 0.9875244498252869
Test Epoch2 layer0 Acc 0.9182894736842105, AUC 0.9815213084220886, avg_entr 0.10820572823286057
Test Epoch2 layer1 Acc 0.9190789473684211, AUC 0.9820719361305237, avg_entr 0.043853577226400375
Test Epoch2 layer2 Acc 0.92, AUC 0.9825854301452637, avg_entr 0.03724323958158493
Save ckpt to ckpt/ag_news_transformeral_l5_pad100//ag_news_transformeral_l5.pt  ,ep 2
Test Epoch2 layer3 Acc 0.9198684210526316, AUC 0.9824166297912598, avg_entr 0.03181116282939911
Test Epoch2 layer4 Acc 0.9193421052631578, AUC 0.9828059673309326, avg_entr 0.029269041493535042
Save ckpt to ckpt/ag_news_transformeral_l5_pad100//ag_news_transformeral_l5.pt  ,ep 2
gc 0
Train Epoch3 Acc 0.9449583333333333 (113395/120000), AUC 0.9893274307250977
Test Epoch3 layer0 Acc 0.9177631578947368, AUC 0.9817007184028625, avg_entr 0.09196838736534119
Test Epoch3 layer1 Acc 0.9184210526315789, AUC 0.9809938669204712, avg_entr 0.0336112417280674
Test Epoch3 layer2 Acc 0.9194736842105263, AUC 0.9819286465644836, avg_entr 0.028002800419926643
Test Epoch3 layer3 Acc 0.9189473684210526, AUC 0.9821606874465942, avg_entr 0.025134559720754623
Test Epoch3 layer4 Acc 0.9192105263157895, AUC 0.9820665121078491, avg_entr 0.023436054587364197
gc 0
Train Epoch4 Acc 0.9502583333333333 (114031/120000), AUC 0.9905717968940735
Test Epoch4 layer0 Acc 0.9178947368421052, AUC 0.9816518425941467, avg_entr 0.0814136415719986
Test Epoch4 layer1 Acc 0.9192105263157895, AUC 0.9800331592559814, avg_entr 0.029036736115813255
Test Epoch4 layer2 Acc 0.9185526315789474, AUC 0.981113076210022, avg_entr 0.024458207190036774
Test Epoch4 layer3 Acc 0.9185526315789474, AUC 0.9813377857208252, avg_entr 0.02216244675219059
Test Epoch4 layer4 Acc 0.9190789473684211, AUC 0.9820032119750977, avg_entr 0.020628394559025764
gc 0
Train Epoch5 Acc 0.9545333333333333 (114544/120000), AUC 0.9914895296096802
Test Epoch5 layer0 Acc 0.9172368421052631, AUC 0.9815987348556519, avg_entr 0.07583291083574295
Test Epoch5 layer1 Acc 0.9157894736842105, AUC 0.9790681600570679, avg_entr 0.026242757216095924
Test Epoch5 layer2 Acc 0.9156578947368421, AUC 0.9805476665496826, avg_entr 0.020201846957206726
Test Epoch5 layer3 Acc 0.9147368421052632, AUC 0.9813266396522522, avg_entr 0.017861973494291306
Test Epoch5 layer4 Acc 0.9144736842105263, AUC 0.980900228023529, avg_entr 0.016367627307772636
gc 0
Train Epoch6 Acc 0.957975 (114957/120000), AUC 0.9925333857536316
Test Epoch6 layer0 Acc 0.9160526315789473, AUC 0.9813700914382935, avg_entr 0.06817060708999634
Test Epoch6 layer1 Acc 0.9132894736842105, AUC 0.9785046577453613, avg_entr 0.02418399043381214
Test Epoch6 layer2 Acc 0.9132894736842105, AUC 0.9780142307281494, avg_entr 0.018413927406072617
Test Epoch6 layer3 Acc 0.9122368421052631, AUC 0.9787700176239014, avg_entr 0.01705463044345379
Test Epoch6 layer4 Acc 0.9122368421052631, AUC 0.9802837371826172, avg_entr 0.016050130128860474
gc 0
Train Epoch7 Acc 0.9597833333333333 (115174/120000), AUC 0.9933385848999023
Test Epoch7 layer0 Acc 0.9180263157894737, AUC 0.9812938570976257, avg_entr 0.06348248571157455
Test Epoch7 layer1 Acc 0.9152631578947369, AUC 0.9771672487258911, avg_entr 0.022192295640707016
Test Epoch7 layer2 Acc 0.9151315789473684, AUC 0.9771519303321838, avg_entr 0.01565229520201683
Test Epoch7 layer3 Acc 0.9147368421052632, AUC 0.9784756898880005, avg_entr 0.014091103337705135
Test Epoch7 layer4 Acc 0.9144736842105263, AUC 0.9798155426979065, avg_entr 0.013108953833580017
gc 0
Train Epoch8 Acc 0.9629416666666667 (115553/120000), AUC 0.9943487644195557
Test Epoch8 layer0 Acc 0.9180263157894737, AUC 0.9811379313468933, avg_entr 0.061523690819740295
Test Epoch8 layer1 Acc 0.9134210526315789, AUC 0.9764260649681091, avg_entr 0.021384134888648987
Test Epoch8 layer2 Acc 0.9136842105263158, AUC 0.9772591590881348, avg_entr 0.015395586378872395
Test Epoch8 layer3 Acc 0.9138157894736842, AUC 0.9800360202789307, avg_entr 0.014089792966842651
Test Epoch8 layer4 Acc 0.9135526315789474, AUC 0.9809050559997559, avg_entr 0.013100720010697842
gc 0
Train Epoch9 Acc 0.964325 (115719/120000), AUC 0.994486927986145
Test Epoch9 layer0 Acc 0.9163157894736842, AUC 0.9808578491210938, avg_entr 0.05982853099703789
Test Epoch9 layer1 Acc 0.9125, AUC 0.9746965169906616, avg_entr 0.020598752424120903
Test Epoch9 layer2 Acc 0.9128947368421053, AUC 0.9738778471946716, avg_entr 0.014263599179685116
Test Epoch9 layer3 Acc 0.9128947368421053, AUC 0.9774984121322632, avg_entr 0.012790248729288578
Test Epoch9 layer4 Acc 0.9125, AUC 0.9795702695846558, avg_entr 0.011479954235255718
gc 0
Train Epoch10 Acc 0.9656333333333333 (115876/120000), AUC 0.9948305487632751
Test Epoch10 layer0 Acc 0.915921052631579, AUC 0.9808814525604248, avg_entr 0.057069990783929825
Test Epoch10 layer1 Acc 0.9107894736842105, AUC 0.974370002746582, avg_entr 0.02064722776412964
Test Epoch10 layer2 Acc 0.9092105263157895, AUC 0.9735795855522156, avg_entr 0.01520000584423542
Test Epoch10 layer3 Acc 0.9094736842105263, AUC 0.9774421453475952, avg_entr 0.014028316363692284
Test Epoch10 layer4 Acc 0.9088157894736842, AUC 0.9785749316215515, avg_entr 0.012603878043591976
gc 0
Train Epoch11 Acc 0.9666333333333333 (115996/120000), AUC 0.9951816201210022
Test Epoch11 layer0 Acc 0.9161842105263158, AUC 0.9807707667350769, avg_entr 0.05425935611128807
Test Epoch11 layer1 Acc 0.9128947368421053, AUC 0.9743438959121704, avg_entr 0.019105207175016403
Test Epoch11 layer2 Acc 0.9134210526315789, AUC 0.9727399349212646, avg_entr 0.01351146399974823
Test Epoch11 layer3 Acc 0.9134210526315789, AUC 0.9754876494407654, avg_entr 0.01248815469443798
Test Epoch11 layer4 Acc 0.9132894736842105, AUC 0.9770304560661316, avg_entr 0.01145301852375269
gc 0
Train Epoch12 Acc 0.9681833333333333 (116182/120000), AUC 0.9954087734222412
Test Epoch12 layer0 Acc 0.9151315789473684, AUC 0.980654239654541, avg_entr 0.05293487384915352
Test Epoch12 layer1 Acc 0.9132894736842105, AUC 0.972970724105835, avg_entr 0.018365513533353806
Test Epoch12 layer2 Acc 0.9128947368421053, AUC 0.971470832824707, avg_entr 0.012850644998252392
Test Epoch12 layer3 Acc 0.9126315789473685, AUC 0.9754940867424011, avg_entr 0.011752644553780556
Test Epoch12 layer4 Acc 0.9122368421052631, AUC 0.9776743054389954, avg_entr 0.010530616156756878
gc 0
Train Epoch13 Acc 0.9685833333333334 (116230/120000), AUC 0.9956310987472534
Test Epoch13 layer0 Acc 0.9148684210526316, AUC 0.980634331703186, avg_entr 0.05130424723029137
Test Epoch13 layer1 Acc 0.9119736842105263, AUC 0.973081111907959, avg_entr 0.017998458817601204
Test Epoch13 layer2 Acc 0.9117105263157895, AUC 0.9716885089874268, avg_entr 0.012306022457778454
Test Epoch13 layer3 Acc 0.911578947368421, AUC 0.9749661087989807, avg_entr 0.011026175692677498
Test Epoch13 layer4 Acc 0.9111842105263158, AUC 0.977390468120575, avg_entr 0.010145210660994053
gc 0
Train Epoch14 Acc 0.9694083333333333 (116329/120000), AUC 0.9957606792449951
Test Epoch14 layer0 Acc 0.9155263157894736, AUC 0.9806153774261475, avg_entr 0.04969499632716179
Test Epoch14 layer1 Acc 0.9119736842105263, AUC 0.9730595350265503, avg_entr 0.017648544162511826
Test Epoch14 layer2 Acc 0.9113157894736842, AUC 0.971057116985321, avg_entr 0.01265002228319645
Test Epoch14 layer3 Acc 0.9106578947368421, AUC 0.9732226133346558, avg_entr 0.01136967446655035
Test Epoch14 layer4 Acc 0.9105263157894737, AUC 0.9758946299552917, avg_entr 0.010072135366499424
gc 0
Train Epoch15 Acc 0.9696583333333333 (116359/120000), AUC 0.9958746433258057
Test Epoch15 layer0 Acc 0.9147368421052632, AUC 0.9804599285125732, avg_entr 0.047635268419981
Test Epoch15 layer1 Acc 0.9119736842105263, AUC 0.9726628065109253, avg_entr 0.016879966482520103
Test Epoch15 layer2 Acc 0.9119736842105263, AUC 0.9705187678337097, avg_entr 0.011867300607264042
Test Epoch15 layer3 Acc 0.9122368421052631, AUC 0.9735114574432373, avg_entr 0.010550099425017834
Test Epoch15 layer4 Acc 0.9119736842105263, AUC 0.9737924933433533, avg_entr 0.009345971047878265
gc 0
Train Epoch16 Acc 0.9705333333333334 (116464/120000), AUC 0.9960938096046448
Test Epoch16 layer0 Acc 0.9139473684210526, AUC 0.9804753065109253, avg_entr 0.04718963801860809
Test Epoch16 layer1 Acc 0.9113157894736842, AUC 0.9728319644927979, avg_entr 0.016639383509755135
Test Epoch16 layer2 Acc 0.9102631578947369, AUC 0.9711847901344299, avg_entr 0.011860976926982403
Test Epoch16 layer3 Acc 0.9097368421052632, AUC 0.9725629687309265, avg_entr 0.010553914122283459
Test Epoch16 layer4 Acc 0.9094736842105263, AUC 0.9732959866523743, avg_entr 0.00965609960258007
gc 0
Train Epoch17 Acc 0.970825 (116499/120000), AUC 0.996157169342041
Test Epoch17 layer0 Acc 0.9138157894736842, AUC 0.9804439544677734, avg_entr 0.0461588092148304
Test Epoch17 layer1 Acc 0.9126315789473685, AUC 0.9724760055541992, avg_entr 0.01576521061360836
Test Epoch17 layer2 Acc 0.9130263157894737, AUC 0.9697573184967041, avg_entr 0.010518243536353111
Test Epoch17 layer3 Acc 0.9123684210526316, AUC 0.9716717004776001, avg_entr 0.00920718815177679
Test Epoch17 layer4 Acc 0.9123684210526316, AUC 0.9733872413635254, avg_entr 0.008030657656490803
gc 0
Train Epoch18 Acc 0.9710333333333333 (116524/120000), AUC 0.9962108135223389
Test Epoch18 layer0 Acc 0.9151315789473684, AUC 0.9804674386978149, avg_entr 0.04484229162335396
Test Epoch18 layer1 Acc 0.911578947368421, AUC 0.9721721410751343, avg_entr 0.015856517478823662
Test Epoch18 layer2 Acc 0.9114473684210527, AUC 0.969930112361908, avg_entr 0.010825688019394875
Test Epoch18 layer3 Acc 0.9106578947368421, AUC 0.9725841283798218, avg_entr 0.00939288828521967
Test Epoch18 layer4 Acc 0.9106578947368421, AUC 0.9733409285545349, avg_entr 0.008568832650780678
gc 0
Train Epoch19 Acc 0.9714916666666666 (116579/120000), AUC 0.9961687922477722
Test Epoch19 layer0 Acc 0.9138157894736842, AUC 0.9804403185844421, avg_entr 0.04332447797060013
Test Epoch19 layer1 Acc 0.9114473684210527, AUC 0.9724403023719788, avg_entr 0.015384788624942303
Test Epoch19 layer2 Acc 0.9122368421052631, AUC 0.9702823162078857, avg_entr 0.010508147068321705
Test Epoch19 layer3 Acc 0.911578947368421, AUC 0.9727867245674133, avg_entr 0.009249722585082054
Test Epoch19 layer4 Acc 0.911578947368421, AUC 0.97322678565979, avg_entr 0.008409595116972923
gc 0
Train Epoch20 Acc 0.9719 (116628/120000), AUC 0.9962141513824463
Test Epoch20 layer0 Acc 0.9148684210526316, AUC 0.9804657101631165, avg_entr 0.042512211948633194
Test Epoch20 layer1 Acc 0.9121052631578948, AUC 0.9723425507545471, avg_entr 0.015002508647739887
Test Epoch20 layer2 Acc 0.9114473684210527, AUC 0.9696763157844543, avg_entr 0.01005755178630352
Test Epoch20 layer3 Acc 0.910921052631579, AUC 0.9724028706550598, avg_entr 0.008748363703489304
Test Epoch20 layer4 Acc 0.9110526315789473, AUC 0.9724156260490417, avg_entr 0.008081505075097084
gc 0
Train Epoch21 Acc 0.9720666666666666 (116648/120000), AUC 0.996349573135376
Test Epoch21 layer0 Acc 0.9147368421052632, AUC 0.9804725050926208, avg_entr 0.041561469435691833
Test Epoch21 layer1 Acc 0.9117105263157895, AUC 0.972434401512146, avg_entr 0.0149860680103302
Test Epoch21 layer2 Acc 0.9102631578947369, AUC 0.9700069427490234, avg_entr 0.01029366347938776
Test Epoch21 layer3 Acc 0.9101315789473684, AUC 0.9716997742652893, avg_entr 0.009039635770022869
Test Epoch21 layer4 Acc 0.9106578947368421, AUC 0.9714974164962769, avg_entr 0.00818302296102047
gc 0
Train Epoch22 Acc 0.9721416666666667 (116657/120000), AUC 0.9962812066078186
Test Epoch22 layer0 Acc 0.9142105263157895, AUC 0.9804569482803345, avg_entr 0.04100203514099121
Test Epoch22 layer1 Acc 0.9117105263157895, AUC 0.9723347425460815, avg_entr 0.014573846943676472
Test Epoch22 layer2 Acc 0.9118421052631579, AUC 0.969876766204834, avg_entr 0.010075772181153297
Test Epoch22 layer3 Acc 0.9113157894736842, AUC 0.972099781036377, avg_entr 0.008787652477622032
Test Epoch22 layer4 Acc 0.911578947368421, AUC 0.9718466997146606, avg_entr 0.00800058338791132
gc 0
Train Epoch23 Acc 0.972075 (116649/120000), AUC 0.996259331703186
Test Epoch23 layer0 Acc 0.9146052631578947, AUC 0.980445146560669, avg_entr 0.03998741880059242
Test Epoch23 layer1 Acc 0.9113157894736842, AUC 0.9720674753189087, avg_entr 0.01440230943262577
Test Epoch23 layer2 Acc 0.9105263157894737, AUC 0.9694869518280029, avg_entr 0.009899856522679329
Test Epoch23 layer3 Acc 0.9113157894736842, AUC 0.9713285565376282, avg_entr 0.008540011011064053
Test Epoch23 layer4 Acc 0.9110526315789473, AUC 0.971493124961853, avg_entr 0.007679146248847246
gc 0
Train Epoch24 Acc 0.972625 (116715/120000), AUC 0.9963970184326172
Test Epoch24 layer0 Acc 0.9139473684210526, AUC 0.9804404377937317, avg_entr 0.03977445140480995
Test Epoch24 layer1 Acc 0.9118421052631579, AUC 0.9721200466156006, avg_entr 0.014112058095633984
Test Epoch24 layer2 Acc 0.9107894736842105, AUC 0.9690518975257874, avg_entr 0.009603293612599373
Test Epoch24 layer3 Acc 0.9107894736842105, AUC 0.9704912900924683, avg_entr 0.008247654885053635
Test Epoch24 layer4 Acc 0.9106578947368421, AUC 0.9701744318008423, avg_entr 0.00745438365265727
gc 0
Train Epoch25 Acc 0.9725333333333334 (116704/120000), AUC 0.9962753057479858
Test Epoch25 layer0 Acc 0.9139473684210526, AUC 0.9804368019104004, avg_entr 0.03944176435470581
Test Epoch25 layer1 Acc 0.9113157894736842, AUC 0.9721208214759827, avg_entr 0.014166397973895073
Test Epoch25 layer2 Acc 0.9106578947368421, AUC 0.969670295715332, avg_entr 0.009674850851297379
Test Epoch25 layer3 Acc 0.9105263157894737, AUC 0.971643328666687, avg_entr 0.008304830640554428
Test Epoch25 layer4 Acc 0.9105263157894737, AUC 0.9718316197395325, avg_entr 0.007540758233517408
gc 0
Train Epoch26 Acc 0.9726916666666666 (116723/120000), AUC 0.9963754415512085
Test Epoch26 layer0 Acc 0.9140789473684211, AUC 0.9804307818412781, avg_entr 0.039275072515010834
Test Epoch26 layer1 Acc 0.9121052631578948, AUC 0.9720266461372375, avg_entr 0.013868610374629498
Test Epoch26 layer2 Acc 0.9106578947368421, AUC 0.9692870378494263, avg_entr 0.009632660076022148
Test Epoch26 layer3 Acc 0.9106578947368421, AUC 0.9712504744529724, avg_entr 0.008309007622301579
Test Epoch26 layer4 Acc 0.9106578947368421, AUC 0.9708458781242371, avg_entr 0.007568358443677425
gc 0
Train Epoch27 Acc 0.9727333333333333 (116728/120000), AUC 0.9964529275894165
Test Epoch27 layer0 Acc 0.9143421052631578, AUC 0.9804202318191528, avg_entr 0.038867074996232986
Test Epoch27 layer1 Acc 0.9118421052631579, AUC 0.9721044898033142, avg_entr 0.013746810145676136
Test Epoch27 layer2 Acc 0.910921052631579, AUC 0.9693724513053894, avg_entr 0.009477482177317142
Test Epoch27 layer3 Acc 0.910921052631579, AUC 0.9712347984313965, avg_entr 0.008114678785204887
Test Epoch27 layer4 Acc 0.9105263157894737, AUC 0.9714097380638123, avg_entr 0.007337595336139202
gc 0
Train Epoch28 Acc 0.9728666666666667 (116744/120000), AUC 0.9963597655296326
Test Epoch28 layer0 Acc 0.9139473684210526, AUC 0.9804235696792603, avg_entr 0.03873994201421738
Test Epoch28 layer1 Acc 0.9114473684210527, AUC 0.9721419215202332, avg_entr 0.013819031417369843
Test Epoch28 layer2 Acc 0.9106578947368421, AUC 0.969795286655426, avg_entr 0.009519555605947971
Test Epoch28 layer3 Acc 0.9105263157894737, AUC 0.9718636870384216, avg_entr 0.008129477500915527
Test Epoch28 layer4 Acc 0.9106578947368421, AUC 0.9717351794242859, avg_entr 0.007318705786019564
gc 0
Train Epoch29 Acc 0.972975 (116757/120000), AUC 0.9964697957038879
Test Epoch29 layer0 Acc 0.9139473684210526, AUC 0.9804179668426514, avg_entr 0.03865363448858261
Test Epoch29 layer1 Acc 0.9114473684210527, AUC 0.9720199108123779, avg_entr 0.013796576298773289
Test Epoch29 layer2 Acc 0.9105263157894737, AUC 0.9693305492401123, avg_entr 0.009488992393016815
Test Epoch29 layer3 Acc 0.9105263157894737, AUC 0.9713916778564453, avg_entr 0.008054806850850582
Test Epoch29 layer4 Acc 0.9106578947368421, AUC 0.971432089805603, avg_entr 0.0072280908934772015
Best AUC 0.9828059673309326
train_loss (2, 5, 30)
valid_acc (5, 30)
valid_AUC (5, 30)
train_acc (30,)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Start Testing
Load ckpt at ckpt/ag_news_transformeral_l5_pad100//ag_news_transformeral_l5.pt
[[1693   62   94   51]
 [   7 1874    9   10]
 [  36   21 1689  154]
 [  40   14  123 1723]]
Figure(640x480)
tensor([0.0612, 0.0035, 0.0624,  ..., 0.1009, 0.0125, 0.2965])
[[1730   56   62   52]
 [   9 1872    9   10]
 [  58   19 1660  163]
 [  53   14  110 1723]]
Figure(640x480)
tensor([0.0062, 0.0033, 0.0052,  ..., 0.0071, 0.0044, 0.0099])
[[1734   53   59   54]
 [   8 1871   10   11]
 [  58   19 1658  165]
 [  50   13  108 1729]]
Figure(640x480)
tensor([0.0063, 0.0047, 0.0057,  ..., 0.0066, 0.0038, 0.0053])
[[1735   53   58   54]
 [   9 1870    9   12]
 [  57   20 1656  167]
 [  50   13  107 1730]]
Figure(640x480)
tensor([0.0051, 0.0051, 0.0059,  ..., 0.0066, 0.0044, 0.0047])
[[1732   54   59   55]
 [   8 1870   10   12]
 [  58   19 1655  168]
 [  50   12  108 1730]]
Figure(640x480)
tensor([0.0050, 0.0046, 0.0052,  ..., 0.0062, 0.0044, 0.0050])
