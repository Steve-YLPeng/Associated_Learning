total count words 222751
vocab size 30000
found 27937 words in glove
model: TransformerModelML(
  (layers): ModuleList(
    (0): EMBLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Embedding(30000, 300)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=2, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=2, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.weight 9000000
layers.0.ae.g.0.weight 256
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 256
layers.0.ae.h.0.bias 2
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
Total Trainable Params: 13671778
Start Training
gc 0
Train Epoch0 Acc 0.52125 (20850/40000), AUC 0.5279824733734131
Test Epoch0 layer0 Acc 0.8311, AUC 0.9076739549636841, avg_entr 0.45930638909339905
Save ckpt to ckpt/imdb_transformeral_l5_pad300//imdb_transformeral_l5.pt  ,ep 0
Test Epoch0 layer1 Acc 0.8292, AUC 0.9066687822341919, avg_entr 0.36766937375068665
Test Epoch0 layer2 Acc 0.829, AUC 0.9073806405067444, avg_entr 0.5887608528137207
Test Epoch0 layer3 Acc 0.8097, AUC 0.9033401012420654, avg_entr 0.6640904545783997
Test Epoch0 layer4 Acc 0.7621, AUC 0.8814024925231934, avg_entr 0.6720104813575745
gc 0
Train Epoch1 Acc 0.850775 (34031/40000), AUC 0.925544023513794
Test Epoch1 layer0 Acc 0.8852, AUC 0.9491351842880249, avg_entr 0.24683308601379395
Save ckpt to ckpt/imdb_transformeral_l5_pad300//imdb_transformeral_l5.pt  ,ep 1
Test Epoch1 layer1 Acc 0.8754, AUC 0.9544174671173096, avg_entr 0.18567778170108795
Save ckpt to ckpt/imdb_transformeral_l5_pad300//imdb_transformeral_l5.pt  ,ep 1
Test Epoch1 layer2 Acc 0.8563, AUC 0.9535441398620605, avg_entr 0.1641678363084793
Test Epoch1 layer3 Acc 0.86, AUC 0.9532372951507568, avg_entr 0.14199501276016235
Test Epoch1 layer4 Acc 0.8468, AUC 0.9527921676635742, avg_entr 0.1381729692220688
gc 0
Train Epoch2 Acc 0.916475 (36659/40000), AUC 0.9705197811126709
Test Epoch2 layer0 Acc 0.8907, AUC 0.9550741910934448, avg_entr 0.19686400890350342
Save ckpt to ckpt/imdb_transformeral_l5_pad300//imdb_transformeral_l5.pt  ,ep 2
Test Epoch2 layer1 Acc 0.8884, AUC 0.9568607211112976, avg_entr 0.13419446349143982
Save ckpt to ckpt/imdb_transformeral_l5_pad300//imdb_transformeral_l5.pt  ,ep 2
Test Epoch2 layer2 Acc 0.8875, AUC 0.9579721689224243, avg_entr 0.07866030931472778
Save ckpt to ckpt/imdb_transformeral_l5_pad300//imdb_transformeral_l5.pt  ,ep 2
Test Epoch2 layer3 Acc 0.8865, AUC 0.9578043222427368, avg_entr 0.06100751832127571
Test Epoch2 layer4 Acc 0.8834, AUC 0.9583740234375, avg_entr 0.05660499259829521
Save ckpt to ckpt/imdb_transformeral_l5_pad300//imdb_transformeral_l5.pt  ,ep 2
gc 0
Train Epoch3 Acc 0.943675 (37747/40000), AUC 0.982897162437439
Test Epoch3 layer0 Acc 0.8928, AUC 0.9565590023994446, avg_entr 0.1724923998117447
Test Epoch3 layer1 Acc 0.8901, AUC 0.9536253213882446, avg_entr 0.08821849524974823
Test Epoch3 layer2 Acc 0.8905, AUC 0.9538642168045044, avg_entr 0.05231725424528122
Test Epoch3 layer3 Acc 0.8905, AUC 0.9538787603378296, avg_entr 0.04601231962442398
Test Epoch3 layer4 Acc 0.8905, AUC 0.9541119337081909, avg_entr 0.04376791790127754
gc 0
Train Epoch4 Acc 0.9542 (38168/40000), AUC 0.9865037202835083
Test Epoch4 layer0 Acc 0.8893, AUC 0.9559075832366943, avg_entr 0.15329232811927795
Test Epoch4 layer1 Acc 0.8871, AUC 0.949146032333374, avg_entr 0.05557277053594589
Test Epoch4 layer2 Acc 0.8856, AUC 0.9511911869049072, avg_entr 0.038064535707235336
Test Epoch4 layer3 Acc 0.8864, AUC 0.9515285491943359, avg_entr 0.03593433275818825
Test Epoch4 layer4 Acc 0.8865, AUC 0.951771080493927, avg_entr 0.03496494144201279
gc 0
Train Epoch5 Acc 0.959125 (38365/40000), AUC 0.9884852170944214
Test Epoch5 layer0 Acc 0.8865, AUC 0.9547495245933533, avg_entr 0.14206238090991974
Test Epoch5 layer1 Acc 0.8812, AUC 0.9427000284194946, avg_entr 0.046618904918432236
Test Epoch5 layer2 Acc 0.8832, AUC 0.9492063522338867, avg_entr 0.03654317930340767
Test Epoch5 layer3 Acc 0.8835, AUC 0.9492702484130859, avg_entr 0.03499965742230415
Test Epoch5 layer4 Acc 0.8834, AUC 0.9495419859886169, avg_entr 0.03451269865036011
gc 0
Train Epoch6 Acc 0.965175 (38607/40000), AUC 0.9907073974609375
Test Epoch6 layer0 Acc 0.8867, AUC 0.9524205327033997, avg_entr 0.13824190199375153
Test Epoch6 layer1 Acc 0.8798, AUC 0.9392653107643127, avg_entr 0.039552316069602966
Test Epoch6 layer2 Acc 0.8787, AUC 0.9482272863388062, avg_entr 0.03178821876645088
Test Epoch6 layer3 Acc 0.8792, AUC 0.948077917098999, avg_entr 0.03035714104771614
Test Epoch6 layer4 Acc 0.8787, AUC 0.9483217000961304, avg_entr 0.029387040063738823
gc 0
Train Epoch7 Acc 0.97045 (38818/40000), AUC 0.9927030801773071
Test Epoch7 layer0 Acc 0.8813, AUC 0.9501832723617554, avg_entr 0.12933792173862457
Test Epoch7 layer1 Acc 0.8768, AUC 0.9357267618179321, avg_entr 0.03824007138609886
Test Epoch7 layer2 Acc 0.8768, AUC 0.9458882212638855, avg_entr 0.03094414435327053
Test Epoch7 layer3 Acc 0.8769, AUC 0.9456276893615723, avg_entr 0.02959330379962921
Test Epoch7 layer4 Acc 0.8763, AUC 0.9460119009017944, avg_entr 0.028860002756118774
gc 0
Train Epoch8 Acc 0.975375 (39015/40000), AUC 0.9941154718399048
Test Epoch8 layer0 Acc 0.8829, AUC 0.9493538737297058, avg_entr 0.12645508348941803
Test Epoch8 layer1 Acc 0.8748, AUC 0.9321653842926025, avg_entr 0.03392888605594635
Test Epoch8 layer2 Acc 0.8753, AUC 0.9436155557632446, avg_entr 0.0258193276822567
Test Epoch8 layer3 Acc 0.8751, AUC 0.9435238838195801, avg_entr 0.023948201909661293
Test Epoch8 layer4 Acc 0.8751, AUC 0.944072961807251, avg_entr 0.023193158209323883
gc 0
Train Epoch9 Acc 0.97685 (39074/40000), AUC 0.994714617729187
Test Epoch9 layer0 Acc 0.8829, AUC 0.9481776356697083, avg_entr 0.1223728358745575
Test Epoch9 layer1 Acc 0.8716, AUC 0.9271432161331177, avg_entr 0.029131049290299416
Test Epoch9 layer2 Acc 0.8714, AUC 0.9417624473571777, avg_entr 0.021348005160689354
Test Epoch9 layer3 Acc 0.8716, AUC 0.9417787194252014, avg_entr 0.01967741921544075
Test Epoch9 layer4 Acc 0.8715, AUC 0.9423424005508423, avg_entr 0.01904848963022232
gc 0
Train Epoch10 Acc 0.978375 (39135/40000), AUC 0.9946858286857605
Test Epoch10 layer0 Acc 0.8803, AUC 0.9466843605041504, avg_entr 0.12031077593564987
Test Epoch10 layer1 Acc 0.8718, AUC 0.9241257905960083, avg_entr 0.028551321476697922
Test Epoch10 layer2 Acc 0.8721, AUC 0.9408427476882935, avg_entr 0.02192402072250843
Test Epoch10 layer3 Acc 0.8722, AUC 0.9406540989875793, avg_entr 0.020396482199430466
Test Epoch10 layer4 Acc 0.8718, AUC 0.9414218068122864, avg_entr 0.019936727359890938
gc 0
Train Epoch11 Acc 0.98035 (39214/40000), AUC 0.9955328106880188
Test Epoch11 layer0 Acc 0.8774, AUC 0.9456666707992554, avg_entr 0.1165047213435173
Test Epoch11 layer1 Acc 0.8692, AUC 0.9202002286911011, avg_entr 0.025385897606611252
Test Epoch11 layer2 Acc 0.8699, AUC 0.9386934041976929, avg_entr 0.01887483522295952
Test Epoch11 layer3 Acc 0.8698, AUC 0.9390218257904053, avg_entr 0.01743166893720627
Test Epoch11 layer4 Acc 0.8701, AUC 0.939970850944519, avg_entr 0.017013106495141983
gc 0
Train Epoch12 Acc 0.9816 (39264/40000), AUC 0.9957157373428345
Test Epoch12 layer0 Acc 0.8777, AUC 0.9449825286865234, avg_entr 0.11523476243019104
Test Epoch12 layer1 Acc 0.8692, AUC 0.9199249148368835, avg_entr 0.024693435057997704
Test Epoch12 layer2 Acc 0.8699, AUC 0.9375894665718079, avg_entr 0.018655290827155113
Test Epoch12 layer3 Acc 0.8696, AUC 0.938473105430603, avg_entr 0.017351021990180016
Test Epoch12 layer4 Acc 0.8696, AUC 0.9395542144775391, avg_entr 0.01706903986632824
gc 0
Train Epoch13 Acc 0.98215 (39286/40000), AUC 0.9960013031959534
Test Epoch13 layer0 Acc 0.8775, AUC 0.9442534446716309, avg_entr 0.11451198905706406
Test Epoch13 layer1 Acc 0.8682, AUC 0.9174651503562927, avg_entr 0.023129083216190338
Test Epoch13 layer2 Acc 0.8691, AUC 0.934891939163208, avg_entr 0.01661672256886959
Test Epoch13 layer3 Acc 0.8688, AUC 0.936569094657898, avg_entr 0.01519704144448042
Test Epoch13 layer4 Acc 0.8683, AUC 0.9380398392677307, avg_entr 0.014902540482580662
gc 0
Train Epoch14 Acc 0.9834 (39336/40000), AUC 0.9961390495300293
Test Epoch14 layer0 Acc 0.8763, AUC 0.9435914158821106, avg_entr 0.11263678967952728
Test Epoch14 layer1 Acc 0.867, AUC 0.9146244525909424, avg_entr 0.023090731352567673
Test Epoch14 layer2 Acc 0.8675, AUC 0.9305163025856018, avg_entr 0.0165975634008646
Test Epoch14 layer3 Acc 0.8674, AUC 0.9338796138763428, avg_entr 0.015076498501002789
Test Epoch14 layer4 Acc 0.8676, AUC 0.936042308807373, avg_entr 0.014726229943335056
gc 0
Train Epoch15 Acc 0.9837 (39348/40000), AUC 0.9964762330055237
Test Epoch15 layer0 Acc 0.8721, AUC 0.9428236484527588, avg_entr 0.11050025373697281
Test Epoch15 layer1 Acc 0.8668, AUC 0.9132809042930603, avg_entr 0.0219072625041008
Test Epoch15 layer2 Acc 0.8673, AUC 0.9289945363998413, avg_entr 0.015331500209867954
Test Epoch15 layer3 Acc 0.867, AUC 0.9330108761787415, avg_entr 0.013881980441510677
Test Epoch15 layer4 Acc 0.8672, AUC 0.9354788064956665, avg_entr 0.013540489599108696
gc 0
Train Epoch16 Acc 0.98445 (39378/40000), AUC 0.9967684149742126
Test Epoch16 layer0 Acc 0.8743, AUC 0.9425276517868042, avg_entr 0.11018549650907516
Test Epoch16 layer1 Acc 0.8668, AUC 0.9114017486572266, avg_entr 0.021994758397340775
Test Epoch16 layer2 Acc 0.8668, AUC 0.9272911548614502, avg_entr 0.015758372843265533
Test Epoch16 layer3 Acc 0.8665, AUC 0.9322164058685303, avg_entr 0.014419526793062687
Test Epoch16 layer4 Acc 0.8668, AUC 0.9349119067192078, avg_entr 0.014118597842752934
gc 0
Train Epoch17 Acc 0.9845 (39380/40000), AUC 0.9966017007827759
Test Epoch17 layer0 Acc 0.8757, AUC 0.9422045946121216, avg_entr 0.1099521815776825
Test Epoch17 layer1 Acc 0.8687, AUC 0.9131182432174683, avg_entr 0.02254907414317131
Test Epoch17 layer2 Acc 0.8679, AUC 0.9242838621139526, avg_entr 0.016135621815919876
Test Epoch17 layer3 Acc 0.8681, AUC 0.9304124712944031, avg_entr 0.014838849194347858
Test Epoch17 layer4 Acc 0.8683, AUC 0.93416827917099, avg_entr 0.014489785768091679
gc 0
Train Epoch18 Acc 0.984975 (39399/40000), AUC 0.996773362159729
Test Epoch18 layer0 Acc 0.8741, AUC 0.9418542981147766, avg_entr 0.10883254557847977
Test Epoch18 layer1 Acc 0.8667, AUC 0.9126166105270386, avg_entr 0.022581305354833603
Test Epoch18 layer2 Acc 0.8678, AUC 0.9235567450523376, avg_entr 0.01621093414723873
Test Epoch18 layer3 Acc 0.8677, AUC 0.9297229051589966, avg_entr 0.014597278088331223
Test Epoch18 layer4 Acc 0.8675, AUC 0.9335026741027832, avg_entr 0.014174357987940311
gc 0
Train Epoch19 Acc 0.98495 (39398/40000), AUC 0.9966442584991455
Test Epoch19 layer0 Acc 0.8743, AUC 0.94145667552948, avg_entr 0.10815849900245667
Test Epoch19 layer1 Acc 0.8655, AUC 0.9100973010063171, avg_entr 0.020636340603232384
Test Epoch19 layer2 Acc 0.8664, AUC 0.9227085709571838, avg_entr 0.014300033450126648
Test Epoch19 layer3 Acc 0.8665, AUC 0.9287066459655762, avg_entr 0.013048021122813225
Test Epoch19 layer4 Acc 0.8663, AUC 0.9325298070907593, avg_entr 0.012841296382248402
gc 0
Train Epoch20 Acc 0.98595 (39438/40000), AUC 0.9969122409820557
Test Epoch20 layer0 Acc 0.8729, AUC 0.9413410425186157, avg_entr 0.10728452354669571
Test Epoch20 layer1 Acc 0.8646, AUC 0.9093161821365356, avg_entr 0.02006281167268753
Test Epoch20 layer2 Acc 0.8655, AUC 0.9203099608421326, avg_entr 0.013689497485756874
Test Epoch20 layer3 Acc 0.8655, AUC 0.9275210499763489, avg_entr 0.012363613583147526
Test Epoch20 layer4 Acc 0.8651, AUC 0.9316450357437134, avg_entr 0.012189245782792568
gc 0
Train Epoch21 Acc 0.985975 (39439/40000), AUC 0.9968768954277039
Test Epoch21 layer0 Acc 0.8737, AUC 0.9411848187446594, avg_entr 0.10685758292675018
Test Epoch21 layer1 Acc 0.8647, AUC 0.9082573652267456, avg_entr 0.019717400893568993
Test Epoch21 layer2 Acc 0.8658, AUC 0.9193738698959351, avg_entr 0.01369810663163662
Test Epoch21 layer3 Acc 0.8655, AUC 0.9269323348999023, avg_entr 0.01241052895784378
Test Epoch21 layer4 Acc 0.8655, AUC 0.9312244653701782, avg_entr 0.012226923368871212
gc 0
Train Epoch22 Acc 0.98605 (39442/40000), AUC 0.9971061944961548
Test Epoch22 layer0 Acc 0.8734, AUC 0.9410213232040405, avg_entr 0.1063680425286293
Test Epoch22 layer1 Acc 0.8646, AUC 0.9076368808746338, avg_entr 0.019443167373538017
Test Epoch22 layer2 Acc 0.8656, AUC 0.9170393347740173, avg_entr 0.013292767107486725
Test Epoch22 layer3 Acc 0.8656, AUC 0.9253751635551453, avg_entr 0.012026366777718067
Test Epoch22 layer4 Acc 0.8651, AUC 0.9303487539291382, avg_entr 0.011869971640408039
gc 0
Train Epoch23 Acc 0.98615 (39446/40000), AUC 0.9970207214355469
Test Epoch23 layer0 Acc 0.8735, AUC 0.9408329725265503, avg_entr 0.10622767359018326
Test Epoch23 layer1 Acc 0.8642, AUC 0.9070682525634766, avg_entr 0.019439533352851868
Test Epoch23 layer2 Acc 0.8657, AUC 0.9155769944190979, avg_entr 0.013448106124997139
Test Epoch23 layer3 Acc 0.8653, AUC 0.9241089820861816, avg_entr 0.012159595265984535
Test Epoch23 layer4 Acc 0.865, AUC 0.9294301271438599, avg_entr 0.011998976580798626
gc 0
Train Epoch24 Acc 0.986225 (39449/40000), AUC 0.9972876310348511
Test Epoch24 layer0 Acc 0.8738, AUC 0.9407968521118164, avg_entr 0.10536777228116989
Test Epoch24 layer1 Acc 0.8649, AUC 0.9074969291687012, avg_entr 0.0196071807295084
Test Epoch24 layer2 Acc 0.8658, AUC 0.9147219657897949, avg_entr 0.012570721097290516
Test Epoch24 layer3 Acc 0.8657, AUC 0.9234780073165894, avg_entr 0.011350463144481182
Test Epoch24 layer4 Acc 0.8657, AUC 0.9293509721755981, avg_entr 0.011063653975725174
gc 0
Train Epoch25 Acc 0.98635 (39454/40000), AUC 0.9973703622817993
Test Epoch25 layer0 Acc 0.8727, AUC 0.9407172799110413, avg_entr 0.10472024977207184
Test Epoch25 layer1 Acc 0.8653, AUC 0.9070131778717041, avg_entr 0.01941084675490856
Test Epoch25 layer2 Acc 0.8659, AUC 0.9122210144996643, avg_entr 0.01242716796696186
Test Epoch25 layer3 Acc 0.8657, AUC 0.9219084978103638, avg_entr 0.011146769858896732
Test Epoch25 layer4 Acc 0.866, AUC 0.9285699725151062, avg_entr 0.010855956003069878
gc 0
Train Epoch26 Acc 0.986275 (39451/40000), AUC 0.9973710775375366
Test Epoch26 layer0 Acc 0.8732, AUC 0.9406615495681763, avg_entr 0.10442639142274857
Test Epoch26 layer1 Acc 0.8643, AUC 0.9068137407302856, avg_entr 0.01915030926465988
Test Epoch26 layer2 Acc 0.865, AUC 0.9141165614128113, avg_entr 0.012708310037851334
Test Epoch26 layer3 Acc 0.8652, AUC 0.922752857208252, avg_entr 0.011474993079900742
Test Epoch26 layer4 Acc 0.8649, AUC 0.9286603927612305, avg_entr 0.011323072947561741
gc 0
Train Epoch27 Acc 0.98655 (39462/40000), AUC 0.9971988201141357
Test Epoch27 layer0 Acc 0.8722, AUC 0.9405808448791504, avg_entr 0.10373253375291824
Test Epoch27 layer1 Acc 0.8632, AUC 0.9062124490737915, avg_entr 0.0191117562353611
Test Epoch27 layer2 Acc 0.8649, AUC 0.9142410159111023, avg_entr 0.013341563753783703
Test Epoch27 layer3 Acc 0.8644, AUC 0.9228010177612305, avg_entr 0.012189799919724464
Test Epoch27 layer4 Acc 0.8646, AUC 0.9284864664077759, avg_entr 0.012035435065627098
gc 0
Train Epoch28 Acc 0.986675 (39467/40000), AUC 0.9972831010818481
Test Epoch28 layer0 Acc 0.8728, AUC 0.9405487179756165, avg_entr 0.10359939187765121
Test Epoch28 layer1 Acc 0.8645, AUC 0.9065920114517212, avg_entr 0.01883336901664734
Test Epoch28 layer2 Acc 0.8651, AUC 0.9132248163223267, avg_entr 0.012413819320499897
Test Epoch28 layer3 Acc 0.865, AUC 0.921924889087677, avg_entr 0.011160033755004406
Test Epoch28 layer4 Acc 0.8648, AUC 0.9281129837036133, avg_entr 0.010987929999828339
gc 0
Train Epoch29 Acc 0.986625 (39465/40000), AUC 0.997015655040741
Test Epoch29 layer0 Acc 0.8732, AUC 0.9405348896980286, avg_entr 0.10349738597869873
Test Epoch29 layer1 Acc 0.8647, AUC 0.9066817760467529, avg_entr 0.01894685998558998
Test Epoch29 layer2 Acc 0.8656, AUC 0.9116131067276001, avg_entr 0.012017204426229
Test Epoch29 layer3 Acc 0.8656, AUC 0.9208523035049438, avg_entr 0.010709083639085293
Test Epoch29 layer4 Acc 0.8657, AUC 0.9276285171508789, avg_entr 0.010397871024906635
Best AUC 0.9583740234375
train_loss (2, 5, 30)
valid_acc (5, 30)
valid_AUC (5, 30)
train_acc (30,)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Start Testing
Load ckpt at ckpt/imdb_transformeral_l5_pad300//imdb_transformeral_l5.pt
[[4381  642]
 [ 451 4526]]
Figure(640x480)
tensor([0.6838, 0.6191, 0.6663,  ..., 0.0923, 0.0018, 0.4481])
[[4674  349]
 [ 767 4210]]
Figure(640x480)
tensor([0.0523, 0.1710, 0.0989,  ..., 0.2330, 0.0431, 0.4768])
[[4724  299]
 [ 826 4151]]
Figure(640x480)
tensor([0.0054, 0.0192, 0.0053,  ..., 0.0651, 0.0125, 0.3898])
[[4735  288]
 [ 847 4130]]
Figure(640x480)
tensor([0.0064, 0.0109, 0.0064,  ..., 0.0371, 0.0117, 0.3653])
[[4761  262]
 [ 904 4073]]
Figure(640x480)
tensor([0.0052, 0.0095, 0.0048,  ..., 0.0394, 0.0151, 0.4615])
