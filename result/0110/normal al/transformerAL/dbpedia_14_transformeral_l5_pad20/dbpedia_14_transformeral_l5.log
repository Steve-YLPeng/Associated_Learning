total count words 887881
vocab size 30000
found 28354 words in glove
model: TransformerModelML(
  (layers): ModuleList(
    (0): EMBLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Embedding(30000, 300)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=14, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=14, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.weight 9000000
layers.0.ae.g.0.weight 1792
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 1792
layers.0.ae.h.0.bias 14
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
Total Trainable Params: 13674862
Start Training
gc 0
Train Epoch0 Acc 0.8234035714285715 (461106/560000), AUC 0.9780076742172241
Test Epoch0 layer0 Acc 0.9745857142857143, AUC 0.9984132647514343, avg_entr 0.06945273280143738
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad20//dbpedia_14_transformeral_l5.pt  ,ep 0
Test Epoch0 layer1 Acc 0.9715571428571429, AUC 0.9980589151382446, avg_entr 0.03948252648115158
Test Epoch0 layer2 Acc 0.9708571428571429, AUC 0.997792661190033, avg_entr 0.02902922034263611
Test Epoch0 layer3 Acc 0.9711, AUC 0.99769127368927, avg_entr 0.02694718912243843
Test Epoch0 layer4 Acc 0.9711, AUC 0.9976323843002319, avg_entr 0.02376585826277733
gc 0
Train Epoch1 Acc 0.9782303571428571 (547809/560000), AUC 0.9973850846290588
Test Epoch1 layer0 Acc 0.9745857142857143, AUC 0.9984847903251648, avg_entr 0.03762958571314812
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad20//dbpedia_14_transformeral_l5.pt  ,ep 1
Test Epoch1 layer1 Acc 0.9766285714285714, AUC 0.9979446530342102, avg_entr 0.012018685229122639
Test Epoch1 layer2 Acc 0.9771571428571428, AUC 0.9975888133049011, avg_entr 0.008386827073991299
Test Epoch1 layer3 Acc 0.9770857142857143, AUC 0.9973459839820862, avg_entr 0.007080372888594866
Test Epoch1 layer4 Acc 0.9770571428571428, AUC 0.9972558617591858, avg_entr 0.006117635872215033
gc 0
Train Epoch2 Acc 0.9827232142857143 (550325/560000), AUC 0.9978234171867371
Test Epoch2 layer0 Acc 0.9745285714285714, AUC 0.9984838366508484, avg_entr 0.02689577266573906
Test Epoch2 layer1 Acc 0.9784714285714285, AUC 0.9978640675544739, avg_entr 0.007126188836991787
Test Epoch2 layer2 Acc 0.9787714285714286, AUC 0.99769526720047, avg_entr 0.0047537959180772305
Test Epoch2 layer3 Acc 0.9786571428571429, AUC 0.9974389672279358, avg_entr 0.003935439512133598
Test Epoch2 layer4 Acc 0.9786857142857143, AUC 0.9968202710151672, avg_entr 0.0034118525218218565
gc 0
Train Epoch3 Acc 0.9848214285714286 (551500/560000), AUC 0.9979239702224731
Test Epoch3 layer0 Acc 0.9748, AUC 0.9985312819480896, avg_entr 0.021564746275544167
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad20//dbpedia_14_transformeral_l5.pt  ,ep 3
Test Epoch3 layer1 Acc 0.9787571428571429, AUC 0.9975690245628357, avg_entr 0.005313444882631302
Test Epoch3 layer2 Acc 0.9789428571428571, AUC 0.9974882006645203, avg_entr 0.0034754786174744368
Test Epoch3 layer3 Acc 0.9788714285714286, AUC 0.997128963470459, avg_entr 0.0028546317480504513
Test Epoch3 layer4 Acc 0.9788428571428571, AUC 0.9964903593063354, avg_entr 0.0024782479740679264
gc 0
Train Epoch4 Acc 0.9862464285714285 (552298/560000), AUC 0.9980193376541138
Test Epoch4 layer0 Acc 0.9745428571428572, AUC 0.9985518455505371, avg_entr 0.019721247255802155
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad20//dbpedia_14_transformeral_l5.pt  ,ep 4
Test Epoch4 layer1 Acc 0.9788857142857142, AUC 0.9974085688591003, avg_entr 0.004804867319762707
Test Epoch4 layer2 Acc 0.9791285714285715, AUC 0.9972915649414062, avg_entr 0.0032731113024055958
Test Epoch4 layer3 Acc 0.9791285714285715, AUC 0.9968757033348083, avg_entr 0.0028098837938159704
Test Epoch4 layer4 Acc 0.9791285714285715, AUC 0.9961586594581604, avg_entr 0.002516248030588031
gc 0
Train Epoch5 Acc 0.9875553571428571 (553031/560000), AUC 0.998364269733429
Test Epoch5 layer0 Acc 0.9747428571428571, AUC 0.9985607266426086, avg_entr 0.019145557656884193
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad20//dbpedia_14_transformeral_l5.pt  ,ep 5
Test Epoch5 layer1 Acc 0.9791285714285715, AUC 0.9972816705703735, avg_entr 0.004314012825489044
Test Epoch5 layer2 Acc 0.9793142857142857, AUC 0.9970694780349731, avg_entr 0.0028838985599577427
Test Epoch5 layer3 Acc 0.9792, AUC 0.9966138005256653, avg_entr 0.0024743375834077597
Test Epoch5 layer4 Acc 0.9791857142857143, AUC 0.9958876967430115, avg_entr 0.002227406483143568
gc 0
Train Epoch6 Acc 0.98815 (553364/560000), AUC 0.9985353350639343
Test Epoch6 layer0 Acc 0.9747571428571429, AUC 0.9985373616218567, avg_entr 0.01900133490562439
Test Epoch6 layer1 Acc 0.9791857142857143, AUC 0.9971741437911987, avg_entr 0.004087588284164667
Test Epoch6 layer2 Acc 0.9792285714285714, AUC 0.9972659945487976, avg_entr 0.00276370532810688
Test Epoch6 layer3 Acc 0.9791714285714286, AUC 0.9967871904373169, avg_entr 0.0023262437898665667
Test Epoch6 layer4 Acc 0.9792142857142857, AUC 0.9957923889160156, avg_entr 0.002130824839696288
gc 0
Train Epoch7 Acc 0.9886714285714285 (553656/560000), AUC 0.9985906481742859
Test Epoch7 layer0 Acc 0.9750285714285715, AUC 0.9985652565956116, avg_entr 0.01873653009533882
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad20//dbpedia_14_transformeral_l5.pt  ,ep 7
Test Epoch7 layer1 Acc 0.9798428571428571, AUC 0.9971349835395813, avg_entr 0.004010673612356186
Test Epoch7 layer2 Acc 0.9797285714285714, AUC 0.996947169303894, avg_entr 0.002728103892877698
Test Epoch7 layer3 Acc 0.9796, AUC 0.996522068977356, avg_entr 0.0022757502738386393
Test Epoch7 layer4 Acc 0.9795714285714285, AUC 0.9955862164497375, avg_entr 0.0020526538137346506
gc 0
Train Epoch8 Acc 0.9896821428571428 (554222/560000), AUC 0.998684823513031
Test Epoch8 layer0 Acc 0.9748428571428571, AUC 0.9985880851745605, avg_entr 0.01871594972908497
Save ckpt to ckpt/dbpedia_14_transformeral_l5_pad20//dbpedia_14_transformeral_l5.pt  ,ep 8
Test Epoch8 layer1 Acc 0.9791714285714286, AUC 0.9970448613166809, avg_entr 0.004101532977074385
Test Epoch8 layer2 Acc 0.9793571428571428, AUC 0.996851921081543, avg_entr 0.0026761575136333704
Test Epoch8 layer3 Acc 0.9794, AUC 0.9960688948631287, avg_entr 0.0022441549226641655
Test Epoch8 layer4 Acc 0.9792857142857143, AUC 0.9953470826148987, avg_entr 0.0019752327352762222
gc 0
Train Epoch9 Acc 0.9902857142857143 (554560/560000), AUC 0.9986874461174011
Test Epoch9 layer0 Acc 0.9748857142857142, AUC 0.9985620379447937, avg_entr 0.01869562640786171
Test Epoch9 layer1 Acc 0.9793285714285714, AUC 0.9969117045402527, avg_entr 0.003838140983134508
Test Epoch9 layer2 Acc 0.9794714285714285, AUC 0.9967605471611023, avg_entr 0.0024172328412532806
Test Epoch9 layer3 Acc 0.9793285714285714, AUC 0.9961575269699097, avg_entr 0.0020032089669257402
Test Epoch9 layer4 Acc 0.9793285714285714, AUC 0.9951267242431641, avg_entr 0.001764998072758317
gc 0
Train Epoch10 Acc 0.9905946428571428 (554733/560000), AUC 0.9987109303474426
Test Epoch10 layer0 Acc 0.9752428571428572, AUC 0.9985696077346802, avg_entr 0.018652375787496567
Test Epoch10 layer1 Acc 0.9794285714285714, AUC 0.9968099594116211, avg_entr 0.004089470021426678
Test Epoch10 layer2 Acc 0.9793142857142857, AUC 0.9964576959609985, avg_entr 0.00242345267906785
Test Epoch10 layer3 Acc 0.9792571428571428, AUC 0.9959184527397156, avg_entr 0.002026650356128812
Test Epoch10 layer4 Acc 0.9791857142857143, AUC 0.9952811598777771, avg_entr 0.0017687918152660131
gc 0
Train Epoch11 Acc 0.9907892857142857 (554842/560000), AUC 0.998706042766571
Test Epoch11 layer0 Acc 0.9752, AUC 0.9985834956169128, avg_entr 0.018348706886172295
Test Epoch11 layer1 Acc 0.9794571428571428, AUC 0.9967509508132935, avg_entr 0.003868408966809511
Test Epoch11 layer2 Acc 0.9792428571428572, AUC 0.9963635206222534, avg_entr 0.0024457217659801245
Test Epoch11 layer3 Acc 0.9793, AUC 0.9958418607711792, avg_entr 0.0019768436904996634
Test Epoch11 layer4 Acc 0.9793142857142857, AUC 0.9948591589927673, avg_entr 0.0017186962068080902
gc 0
Train Epoch12 Acc 0.9912517857142857 (555101/560000), AUC 0.998721182346344
Test Epoch12 layer0 Acc 0.9751428571428571, AUC 0.9985600709915161, avg_entr 0.018348071724176407
Test Epoch12 layer1 Acc 0.9791714285714286, AUC 0.996574878692627, avg_entr 0.003867711639031768
Test Epoch12 layer2 Acc 0.9789857142857142, AUC 0.9961528778076172, avg_entr 0.0024447955656796694
Test Epoch12 layer3 Acc 0.9791142857142857, AUC 0.9955951571464539, avg_entr 0.0019678177777677774
Test Epoch12 layer4 Acc 0.9791285714285715, AUC 0.9945829510688782, avg_entr 0.0017039412632584572
gc 0
Train Epoch13 Acc 0.9915857142857143 (555288/560000), AUC 0.9987226128578186
Test Epoch13 layer0 Acc 0.9749285714285715, AUC 0.9985634684562683, avg_entr 0.018520964309573174
Test Epoch13 layer1 Acc 0.9790714285714286, AUC 0.9965940713882446, avg_entr 0.003807400120422244
Test Epoch13 layer2 Acc 0.9789142857142857, AUC 0.9961230158805847, avg_entr 0.0023691700771450996
Test Epoch13 layer3 Acc 0.9789428571428571, AUC 0.995430052280426, avg_entr 0.001960640773177147
Test Epoch13 layer4 Acc 0.9789714285714286, AUC 0.9945055842399597, avg_entr 0.0017256038263440132
gc 0
Train Epoch14 Acc 0.9916875 (555345/560000), AUC 0.9987943768501282
Test Epoch14 layer0 Acc 0.9748142857142857, AUC 0.9985529780387878, avg_entr 0.018385883420705795
Test Epoch14 layer1 Acc 0.9791571428571428, AUC 0.9966177344322205, avg_entr 0.003800666192546487
Test Epoch14 layer2 Acc 0.9789285714285715, AUC 0.9961424469947815, avg_entr 0.002346112160012126
Test Epoch14 layer3 Acc 0.9788857142857142, AUC 0.9953807592391968, avg_entr 0.001898850779980421
Test Epoch14 layer4 Acc 0.9789, AUC 0.9944618344306946, avg_entr 0.0016993850003927946
gc 0
Train Epoch15 Acc 0.9918589285714285 (555441/560000), AUC 0.9987631440162659
Test Epoch15 layer0 Acc 0.9751285714285715, AUC 0.9985538125038147, avg_entr 0.018386343494057655
Test Epoch15 layer1 Acc 0.9791, AUC 0.996498167514801, avg_entr 0.0038605250883847475
Test Epoch15 layer2 Acc 0.979, AUC 0.9960231781005859, avg_entr 0.00229111616499722
Test Epoch15 layer3 Acc 0.9789714285714286, AUC 0.9953479766845703, avg_entr 0.0018831179477274418
Test Epoch15 layer4 Acc 0.979, AUC 0.9943105578422546, avg_entr 0.0017222014721482992
gc 0
Train Epoch16 Acc 0.9920714285714286 (555560/560000), AUC 0.9987839460372925
Test Epoch16 layer0 Acc 0.975, AUC 0.9985608458518982, avg_entr 0.01836227811872959
Test Epoch16 layer1 Acc 0.9789857142857142, AUC 0.9965072870254517, avg_entr 0.0037922747433185577
Test Epoch16 layer2 Acc 0.9786428571428571, AUC 0.995969831943512, avg_entr 0.002330261515453458
Test Epoch16 layer3 Acc 0.9787142857142858, AUC 0.9952934384346008, avg_entr 0.0019140280783176422
Test Epoch16 layer4 Acc 0.9787142857142858, AUC 0.9942302703857422, avg_entr 0.0017281147884204984
gc 0
Train Epoch17 Acc 0.9923053571428572 (555691/560000), AUC 0.9988003373146057
Test Epoch17 layer0 Acc 0.9750714285714286, AUC 0.9985591769218445, avg_entr 0.018393298611044884
Test Epoch17 layer1 Acc 0.9793428571428572, AUC 0.9964852333068848, avg_entr 0.0038095563650131226
Test Epoch17 layer2 Acc 0.9788571428571429, AUC 0.9960254430770874, avg_entr 0.0022523952648043633
Test Epoch17 layer3 Acc 0.9788428571428571, AUC 0.9952560663223267, avg_entr 0.0017729850951582193
Test Epoch17 layer4 Acc 0.9788714285714286, AUC 0.9942549467086792, avg_entr 0.0015259244246408343
gc 0
Train Epoch18 Acc 0.9923303571428571 (555705/560000), AUC 0.9988419413566589
Test Epoch18 layer0 Acc 0.9750428571428571, AUC 0.9985599517822266, avg_entr 0.018367862328886986
Test Epoch18 layer1 Acc 0.9791714285714286, AUC 0.9964802861213684, avg_entr 0.0037798627745360136
Test Epoch18 layer2 Acc 0.9787285714285714, AUC 0.9959985017776489, avg_entr 0.002241157228127122
Test Epoch18 layer3 Acc 0.9789285714285715, AUC 0.9953575134277344, avg_entr 0.0017269799718633294
Test Epoch18 layer4 Acc 0.9788428571428571, AUC 0.9943466186523438, avg_entr 0.0014603101881220937
gc 0
Train Epoch19 Acc 0.9923964285714286 (555742/560000), AUC 0.9987978935241699
Test Epoch19 layer0 Acc 0.9749714285714286, AUC 0.9985599517822266, avg_entr 0.018428442999720573
Test Epoch19 layer1 Acc 0.9792714285714286, AUC 0.996420681476593, avg_entr 0.003731570905074477
Test Epoch19 layer2 Acc 0.9787571428571429, AUC 0.9959549307823181, avg_entr 0.002254033926874399
Test Epoch19 layer3 Acc 0.9787571428571429, AUC 0.9952124953269958, avg_entr 0.001808689907193184
Test Epoch19 layer4 Acc 0.9787571428571429, AUC 0.9942318201065063, avg_entr 0.0016643881099298596
gc 0
Train Epoch20 Acc 0.9926107142857142 (555862/560000), AUC 0.9988095164299011
Test Epoch20 layer0 Acc 0.9751428571428571, AUC 0.9985571503639221, avg_entr 0.01828824356198311
Test Epoch20 layer1 Acc 0.9790571428571428, AUC 0.9964659810066223, avg_entr 0.0037884810008108616
Test Epoch20 layer2 Acc 0.9787857142857143, AUC 0.9958946108818054, avg_entr 0.002245552372187376
Test Epoch20 layer3 Acc 0.9788285714285714, AUC 0.9952365756034851, avg_entr 0.001771532348357141
Test Epoch20 layer4 Acc 0.9788285714285714, AUC 0.9941540956497192, avg_entr 0.001582018448971212
gc 0
Train Epoch21 Acc 0.9926678571428571 (555894/560000), AUC 0.9988022446632385
Test Epoch21 layer0 Acc 0.9749857142857142, AUC 0.9985625147819519, avg_entr 0.018378829583525658
Test Epoch21 layer1 Acc 0.9791142857142857, AUC 0.996394157409668, avg_entr 0.0037372615188360214
Test Epoch21 layer2 Acc 0.9785714285714285, AUC 0.9959648847579956, avg_entr 0.0022304353769868612
Test Epoch21 layer3 Acc 0.9785857142857143, AUC 0.9951661229133606, avg_entr 0.0017692702822387218
Test Epoch21 layer4 Acc 0.9786428571428571, AUC 0.9941869378089905, avg_entr 0.001627939403988421
gc 0
Train Epoch22 Acc 0.9926821428571428 (555902/560000), AUC 0.9988265037536621
Test Epoch22 layer0 Acc 0.9750142857142857, AUC 0.9985538125038147, avg_entr 0.018396971747279167
Test Epoch22 layer1 Acc 0.979, AUC 0.9964041113853455, avg_entr 0.0037487505469471216
Test Epoch22 layer2 Acc 0.9785571428571429, AUC 0.9958716034889221, avg_entr 0.0022320463322103024
Test Epoch22 layer3 Acc 0.9785714285714285, AUC 0.9951079487800598, avg_entr 0.001734095741994679
Test Epoch22 layer4 Acc 0.9786285714285714, AUC 0.9942070245742798, avg_entr 0.0015289988368749619
gc 0
Train Epoch23 Acc 0.9926767857142857 (555899/560000), AUC 0.9988332986831665
Test Epoch23 layer0 Acc 0.9750857142857143, AUC 0.9985605478286743, avg_entr 0.018336432054638863
Test Epoch23 layer1 Acc 0.9790571428571428, AUC 0.9963895678520203, avg_entr 0.003730948781594634
Test Epoch23 layer2 Acc 0.9785714285714285, AUC 0.9958564043045044, avg_entr 0.0022678545210510492
Test Epoch23 layer3 Acc 0.9785714285714285, AUC 0.9951321482658386, avg_entr 0.0017956409137696028
Test Epoch23 layer4 Acc 0.9785571428571429, AUC 0.9941651225090027, avg_entr 0.0016132309101521969
gc 0
Train Epoch24 Acc 0.9927910714285715 (555963/560000), AUC 0.9988312721252441
Test Epoch24 layer0 Acc 0.9750571428571428, AUC 0.9985598921775818, avg_entr 0.01833217591047287
Test Epoch24 layer1 Acc 0.9791142857142857, AUC 0.9963884949684143, avg_entr 0.0037511487025767565
Test Epoch24 layer2 Acc 0.9785428571428572, AUC 0.9957900643348694, avg_entr 0.0022534530144184828
Test Epoch24 layer3 Acc 0.9787, AUC 0.9950189590454102, avg_entr 0.0017919648671522737
Test Epoch24 layer4 Acc 0.9786714285714285, AUC 0.9940995573997498, avg_entr 0.0015822231071069837
gc 0
Train Epoch25 Acc 0.9928642857142858 (556004/560000), AUC 0.9988105893135071
Test Epoch25 layer0 Acc 0.9750571428571428, AUC 0.9985623955726624, avg_entr 0.018327001482248306
Test Epoch25 layer1 Acc 0.979, AUC 0.996396541595459, avg_entr 0.00371350534260273
Test Epoch25 layer2 Acc 0.9785285714285714, AUC 0.9957739114761353, avg_entr 0.0022536630276590586
Test Epoch25 layer3 Acc 0.9786142857142857, AUC 0.9949931502342224, avg_entr 0.001812967355363071
Test Epoch25 layer4 Acc 0.9785571428571429, AUC 0.9940398931503296, avg_entr 0.0016058117616921663
gc 0
Train Epoch26 Acc 0.9928285714285714 (555984/560000), AUC 0.9988494515419006
Test Epoch26 layer0 Acc 0.9750142857142857, AUC 0.9985597729682922, avg_entr 0.018302056938409805
Test Epoch26 layer1 Acc 0.9790285714285715, AUC 0.9963948130607605, avg_entr 0.0036752738524228334
Test Epoch26 layer2 Acc 0.9785, AUC 0.9957415461540222, avg_entr 0.0022023944184184074
Test Epoch26 layer3 Acc 0.9786, AUC 0.994916558265686, avg_entr 0.001773117808625102
Test Epoch26 layer4 Acc 0.9785, AUC 0.9940447807312012, avg_entr 0.0016274119261652231
gc 0
Train Epoch27 Acc 0.9928339285714286 (555987/560000), AUC 0.9988245368003845
Test Epoch27 layer0 Acc 0.975, AUC 0.9985643625259399, avg_entr 0.01830040104687214
Test Epoch27 layer1 Acc 0.9792, AUC 0.9963824152946472, avg_entr 0.003714695107191801
Test Epoch27 layer2 Acc 0.9786714285714285, AUC 0.9957500696182251, avg_entr 0.0022081334609538317
Test Epoch27 layer3 Acc 0.9785857142857143, AUC 0.9950103759765625, avg_entr 0.0018112584948539734
Test Epoch27 layer4 Acc 0.9784857142857143, AUC 0.9940763115882874, avg_entr 0.0016204757848754525
gc 0
Train Epoch28 Acc 0.9928571428571429 (556000/560000), AUC 0.998850405216217
Test Epoch28 layer0 Acc 0.9750428571428571, AUC 0.9985593557357788, avg_entr 0.01832817681133747
Test Epoch28 layer1 Acc 0.9790857142857143, AUC 0.996377170085907, avg_entr 0.0037155291065573692
Test Epoch28 layer2 Acc 0.9786428571428571, AUC 0.9956973791122437, avg_entr 0.002241584938019514
Test Epoch28 layer3 Acc 0.9786, AUC 0.9949021339416504, avg_entr 0.0018342797411605716
Test Epoch28 layer4 Acc 0.9786428571428571, AUC 0.9939661026000977, avg_entr 0.001631491701118648
gc 0
Train Epoch29 Acc 0.992975 (556066/560000), AUC 0.9988474249839783
Test Epoch29 layer0 Acc 0.9750285714285715, AUC 0.9985617399215698, avg_entr 0.018328653648495674
Test Epoch29 layer1 Acc 0.9790428571428571, AUC 0.9963707327842712, avg_entr 0.0037043613847345114
Test Epoch29 layer2 Acc 0.9786, AUC 0.9957447052001953, avg_entr 0.002226281678304076
Test Epoch29 layer3 Acc 0.9785428571428572, AUC 0.9949827194213867, avg_entr 0.00180231558624655
Test Epoch29 layer4 Acc 0.9785428571428572, AUC 0.9940207600593567, avg_entr 0.001597492373548448
Best AUC 0.9985880851745605
train_loss (2, 5, 30)
valid_acc (5, 30)
valid_AUC (5, 30)
train_acc (30,)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Start Testing
Load ckpt at ckpt/dbpedia_14_transformeral_l5_pad20//dbpedia_14_transformeral_l5.pt
[[4713   40   19   12   15   57   43    6    6    4    8   12   26   39]
 [  35 4907    2    3    5    0   27    8    2    2    1    0    4    4]
 [  30   11 4684   15   74    1    8    5    2    0    3   51   20   96]
 [   2    2   56 4915   15    0    2    4    0    2    0    1    0    1]
 [  10   16  112   12 4827    2    8    6    3    0    0    0    2    2]
 [  39    0    1    2    2 4945    5    1    1    2    0    0    2    0]
 [  69   52    6    1   13   20 4786   35   12    3    0    0    1    2]
 [   1    0    1    0    2    0   15 4965   13    2    0    0    1    0]
 [   0    1    1    0    1    0    9    9 4978    0    0    0    0    1]
 [   0    1    2    3    0    1    0    5    0 4940   47    0    0    1]
 [  10    1    1    0    0    1    2    4    0   31 4950    0    0    0]
 [   6    0   24    1    1    1    2    0    0    0    0 4937   18   10]
 [  10    1   19    4    2    3    1    1    0    1    0   17 4887   54]
 [  35    8   64    7    8    5    6    3    1    3    3    9   43 4805]]
Figure(640x480)
tensor([1.9054e-07, 7.4480e-06, 1.9260e-07,  ..., 3.2508e-07, 6.8479e-05,
        1.3764e-06])
[[4757   40   20    6   10   43   52    2    1    3    4    8   15   39]
 [  39 4915    3    1    7    0   24    1    2    1    1    0    1    5]
 [  24    4 4733   18   98    0    7    1    2    0    2   29   17   65]
 [   2    1   29 4944   17    0    2    1    0    1    1    0    0    2]
 [   9    3   90   13 4870    2    7    1    1    1    0    0    1    2]
 [  33    0    1    0    0 4955    4    2    1    1    0    0    3    0]
 [  65   42    5    0    5   19 4824   25    4    2    1    0    1    7]
 [   2    0    0    0    0    0   21 4956   15    4    0    0    1    1]
 [   1    1    2    0    3    0   11    8 4973    0    0    0    0    1]
 [   0    0    3    1    0    2    0    5    0 4958   30    0    0    1]
 [  12    1    0    0    0    2    1    0    0   22 4961    0    0    1]
 [   7    0   19    1    0    1    0    0    0    0    0 4946   18    8]
 [  12    0   13    1    1    1    0    0    1    3    0   17 4902   49]
 [  32    2   56    1    5    5    5    1    1    0    1    8   35 4848]]
Figure(640x480)
tensor([1.0097e-07, 9.5486e-08, 8.9184e-08,  ..., 2.0175e-07, 9.0171e-08,
        8.4521e-08])
[[4761   38   23    3   10   38   57    2    1    2    2    6   15   42]
 [  40 4914    4    0    7    0   26    0    2    1    1    0    0    5]
 [  25    4 4742   17   97    0    9    1    2    0    2   28   16   57]
 [   1    1   29 4944   18    0    2    1    1    1    0    0    0    2]
 [   8    3   90   14 4869    1    9    1    1    1    1    0    1    1]
 [  35    0    1    0    0 4953    4    2    1    0    0    1    2    1]
 [  66   38    4    0    2   18 4834   24    4    2    0    0    1    7]
 [   2    0    0    0    0    0   21 4956   15    4    0    0    1    1]
 [   1    1    2    0    3    0   12    7 4973    0    0    0    0    1]
 [   1    0    3    0    0    2    0    5    0 4960   28    0    0    1]
 [  12    1    1    0    0    1    2    0    0   23 4960    0    0    0]
 [   9    0   23    1    0    1    0    0    0    0    0 4944   14    8]
 [  14    1   15    1    1    1    0    0    0    3    0   17 4895   52]
 [  30    2   61    0    5    5    5    1    0    0    1    8   32 4850]]
Figure(640x480)
tensor([9.6109e-08, 9.0755e-08, 9.4157e-08,  ..., 1.1234e-07, 1.1191e-07,
        1.0983e-07])
[[4768   37   23    3   10   36   56    2    1    2    2    5   15   40]
 [  43 4912    4    0    7    0   26    0    1    1    1    0    0    5]
 [  26    4 4744   15   96    0    9    1    2    0    2   28   16   57]
 [   1    1   29 4943   19    0    2    1    1    1    0    0    0    2]
 [   8    3   91   14 4869    1    8    1    1    1    1    0    1    1]
 [  37    0    1    0    0 4952    4    2    1    0    0    1    1    1]
 [  67   35    4    0    2   19 4834   25    4    2    0    0    1    7]
 [   2    0    0    0    0    0   20 4958   14    4    0    0    1    1]
 [   1    1    2    0    3    0   13    8 4971    0    0    0    0    1]
 [   1    0    3    0    0    2    0    5    0 4960   28    0    0    1]
 [  12    1    2    0    0    1    2    0    0   23 4959    0    0    0]
 [   9    0   23    1    0    1    0    0    0    0    0 4943   15    8]
 [  14    0   16    1    1    1    0    0    0    3    0   17 4892   55]
 [  30    2   60    0    5    5    5    1    0    0    1    8   30 4853]]
Figure(640x480)
tensor([8.9222e-08, 8.4101e-08, 8.6954e-08,  ..., 1.0420e-07, 1.0387e-07,
        1.0317e-07])
[[4766   37   23    3   10   36   56    2    1    2    2    5   15   42]
 [  43 4912    4    0    7    0   26    0    1    1    1    0    0    5]
 [  26    4 4744   15   96    0    9    1    2    0    2   28   15   58]
 [   2    1   29 4942   20    0    2    1    1    0    0    0    0    2]
 [   8    3   92   14 4868    1    8    1    1    1    1    0    1    1]
 [  37    0    1    0    0 4951    4    2    1    0    0    1    2    1]
 [  67   35    4    0    2   18 4835   25    4    2    0    0    1    7]
 [   2    0    0    0    0    0   20 4957   15    4    0    0    1    1]
 [   1    1    2    0    3    0   12    8 4972    0    0    0    0    1]
 [   1    0    3    0    0    2    0    5    0 4960   28    0    0    1]
 [  12    1    2    0    0    1    2    0    0   23 4958    0    0    1]
 [   9    0   23    1    0    1    0    0    0    0    0 4944   14    8]
 [  14    0   17    1    1    1    0    0    0    3    0   18 4887   58]
 [  29    2   60    0    5    5    5    1    0    0    1    8   30 4854]]
Figure(640x480)
tensor([9.7575e-08, 9.5970e-08, 9.6787e-08,  ..., 9.4726e-08, 9.4100e-08,
        9.2964e-08])
