total count words 102019
vocab size 30000
found 26754 words in glove
model: TransformerModelML(
  (layers): ModuleList(
    (0): EMBLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Embedding(30000, 300)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=4, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=4, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.weight 9000000
layers.0.ae.g.0.weight 512
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 512
layers.0.ae.h.0.bias 4
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
Total Trainable Params: 13672292
Start Training
gc 0
Train Epoch0 Acc 0.6534166666666666 (78410/120000), AUC 0.8713070154190063
Test Epoch0 layer0 Acc 0.8953947368421052, AUC 0.9745868444442749, avg_entr 0.25304779410362244
Save ckpt to ckpt/ag_news_transformeral_l5_pad200//ag_news_transformeral_l5.pt  ,ep 0
Test Epoch0 layer1 Acc 0.9064473684210527, AUC 0.9776649475097656, avg_entr 0.15901149809360504
Save ckpt to ckpt/ag_news_transformeral_l5_pad200//ag_news_transformeral_l5.pt  ,ep 0
Test Epoch0 layer2 Acc 0.9040789473684211, AUC 0.9780411720275879, avg_entr 0.14622072875499725
Save ckpt to ckpt/ag_news_transformeral_l5_pad200//ag_news_transformeral_l5.pt  ,ep 0
Test Epoch0 layer3 Acc 0.9027631578947368, AUC 0.9781976938247681, avg_entr 0.14188049733638763
Save ckpt to ckpt/ag_news_transformeral_l5_pad200//ag_news_transformeral_l5.pt  ,ep 0
Test Epoch0 layer4 Acc 0.9025, AUC 0.9783717393875122, avg_entr 0.14030887186527252
Save ckpt to ckpt/ag_news_transformeral_l5_pad200//ag_news_transformeral_l5.pt  ,ep 0
gc 0
Train Epoch1 Acc 0.9201166666666667 (110414/120000), AUC 0.9813364148139954
Test Epoch1 layer0 Acc 0.9121052631578948, AUC 0.9789408445358276, avg_entr 0.14798907935619354
Save ckpt to ckpt/ag_news_transformeral_l5_pad200//ag_news_transformeral_l5.pt  ,ep 1
Test Epoch1 layer1 Acc 0.9186842105263158, AUC 0.9821299910545349, avg_entr 0.07273975014686584
Save ckpt to ckpt/ag_news_transformeral_l5_pad200//ag_news_transformeral_l5.pt  ,ep 1
Test Epoch1 layer2 Acc 0.9184210526315789, AUC 0.9820345044136047, avg_entr 0.06440787017345428
Test Epoch1 layer3 Acc 0.9177631578947368, AUC 0.9819586277008057, avg_entr 0.05933258682489395
Test Epoch1 layer4 Acc 0.9177631578947368, AUC 0.9819173216819763, avg_entr 0.05442144349217415
gc 0
Train Epoch2 Acc 0.93485 (112182/120000), AUC 0.9865451455116272
Test Epoch2 layer0 Acc 0.9160526315789473, AUC 0.9805766940116882, avg_entr 0.1096271350979805
Test Epoch2 layer1 Acc 0.9193421052631578, AUC 0.981823205947876, avg_entr 0.04142541065812111
Test Epoch2 layer2 Acc 0.9192105263157895, AUC 0.982596218585968, avg_entr 0.035109974443912506
Save ckpt to ckpt/ag_news_transformeral_l5_pad200//ag_news_transformeral_l5.pt  ,ep 2
Test Epoch2 layer3 Acc 0.9197368421052632, AUC 0.9823648929595947, avg_entr 0.03152600675821304
Test Epoch2 layer4 Acc 0.9194736842105263, AUC 0.9821519255638123, avg_entr 0.028956472873687744
gc 0
Train Epoch3 Acc 0.9422833333333334 (113074/120000), AUC 0.9889163970947266
Test Epoch3 layer0 Acc 0.9144736842105263, AUC 0.9812909364700317, avg_entr 0.09702429175376892
Test Epoch3 layer1 Acc 0.9173684210526316, AUC 0.9813575148582458, avg_entr 0.03303017467260361
Test Epoch3 layer2 Acc 0.9177631578947368, AUC 0.981918454170227, avg_entr 0.028096629306674004
Test Epoch3 layer3 Acc 0.9180263157894737, AUC 0.9821881651878357, avg_entr 0.026102421805262566
Test Epoch3 layer4 Acc 0.9177631578947368, AUC 0.9822975397109985, avg_entr 0.024797137826681137
gc 0
Train Epoch4 Acc 0.947925 (113751/120000), AUC 0.9901731610298157
Test Epoch4 layer0 Acc 0.9182894736842105, AUC 0.981721818447113, avg_entr 0.08138999342918396
Test Epoch4 layer1 Acc 0.9171052631578948, AUC 0.9802443981170654, avg_entr 0.02928953245282173
Test Epoch4 layer2 Acc 0.9164473684210527, AUC 0.9812982082366943, avg_entr 0.023811686784029007
Test Epoch4 layer3 Acc 0.916578947368421, AUC 0.981423556804657, avg_entr 0.022449452430009842
Test Epoch4 layer4 Acc 0.9161842105263158, AUC 0.9814341068267822, avg_entr 0.021237658336758614
gc 0
Train Epoch5 Acc 0.9521333333333334 (114256/120000), AUC 0.9912523031234741
Test Epoch5 layer0 Acc 0.9178947368421052, AUC 0.9818906784057617, avg_entr 0.07188767194747925
Test Epoch5 layer1 Acc 0.9188157894736843, AUC 0.9792526364326477, avg_entr 0.025474775582551956
Test Epoch5 layer2 Acc 0.9193421052631578, AUC 0.9809796810150146, avg_entr 0.019717982038855553
Test Epoch5 layer3 Acc 0.9196052631578947, AUC 0.9819340109825134, avg_entr 0.01781109720468521
Test Epoch5 layer4 Acc 0.9192105263157895, AUC 0.9823519587516785, avg_entr 0.016767295077443123
gc 0
Train Epoch6 Acc 0.9553833333333334 (114646/120000), AUC 0.9923264980316162
Test Epoch6 layer0 Acc 0.9178947368421052, AUC 0.9819498062133789, avg_entr 0.06686709076166153
Test Epoch6 layer1 Acc 0.9178947368421052, AUC 0.9786131978034973, avg_entr 0.023599646985530853
Test Epoch6 layer2 Acc 0.9169736842105263, AUC 0.9796409606933594, avg_entr 0.01840876042842865
Test Epoch6 layer3 Acc 0.9172368421052631, AUC 0.9795586466789246, avg_entr 0.016597982496023178
Test Epoch6 layer4 Acc 0.9175, AUC 0.9798629283905029, avg_entr 0.015574442222714424
gc 0
Train Epoch7 Acc 0.957375 (114885/120000), AUC 0.9930671453475952
Test Epoch7 layer0 Acc 0.9185526315789474, AUC 0.98177170753479, avg_entr 0.06430802494287491
Test Epoch7 layer1 Acc 0.9176315789473685, AUC 0.9781745672225952, avg_entr 0.021829204633831978
Test Epoch7 layer2 Acc 0.916578947368421, AUC 0.9799368381500244, avg_entr 0.016835834830999374
Test Epoch7 layer3 Acc 0.9163157894736842, AUC 0.9810451865196228, avg_entr 0.015198580920696259
Test Epoch7 layer4 Acc 0.916578947368421, AUC 0.9812142848968506, avg_entr 0.014466337859630585
gc 0
Train Epoch8 Acc 0.9590916666666667 (115091/120000), AUC 0.9937441349029541
Test Epoch8 layer0 Acc 0.9182894736842105, AUC 0.9817888736724854, avg_entr 0.058544058352708817
Test Epoch8 layer1 Acc 0.9167105263157894, AUC 0.9763994812965393, avg_entr 0.02040189690887928
Test Epoch8 layer2 Acc 0.9153947368421053, AUC 0.9772992134094238, avg_entr 0.01589338295161724
Test Epoch8 layer3 Acc 0.9152631578947369, AUC 0.9786570072174072, avg_entr 0.014610635116696358
Test Epoch8 layer4 Acc 0.9153947368421053, AUC 0.979965090751648, avg_entr 0.0138176204636693
gc 0
Train Epoch9 Acc 0.9612333333333334 (115348/120000), AUC 0.994035542011261
Test Epoch9 layer0 Acc 0.9175, AUC 0.9816128015518188, avg_entr 0.05507105961441994
Test Epoch9 layer1 Acc 0.9161842105263158, AUC 0.9763492941856384, avg_entr 0.01997954398393631
Test Epoch9 layer2 Acc 0.9156578947368421, AUC 0.977637767791748, avg_entr 0.015088950283825397
Test Epoch9 layer3 Acc 0.9156578947368421, AUC 0.9778988361358643, avg_entr 0.013438870199024677
Test Epoch9 layer4 Acc 0.9153947368421053, AUC 0.9792339205741882, avg_entr 0.012918650172650814
gc 0
Train Epoch10 Acc 0.96375 (115650/120000), AUC 0.994872510433197
Test Epoch10 layer0 Acc 0.9157894736842105, AUC 0.981623113155365, avg_entr 0.053560543805360794
Test Epoch10 layer1 Acc 0.9153947368421053, AUC 0.9757110476493835, avg_entr 0.018918827176094055
Test Epoch10 layer2 Acc 0.9142105263157895, AUC 0.9770215749740601, avg_entr 0.014506487175822258
Test Epoch10 layer3 Acc 0.9138157894736842, AUC 0.9769382476806641, avg_entr 0.012649578042328358
Test Epoch10 layer4 Acc 0.9135526315789474, AUC 0.9786532521247864, avg_entr 0.012031176127493382
gc 0
Train Epoch11 Acc 0.9644 (115728/120000), AUC 0.9950664639472961
Test Epoch11 layer0 Acc 0.9168421052631579, AUC 0.9815885424613953, avg_entr 0.05153797194361687
Test Epoch11 layer1 Acc 0.9139473684210526, AUC 0.9749724268913269, avg_entr 0.018447687849402428
Test Epoch11 layer2 Acc 0.9118421052631579, AUC 0.9758195281028748, avg_entr 0.014178468845784664
Test Epoch11 layer3 Acc 0.9118421052631579, AUC 0.9755691885948181, avg_entr 0.012592745013535023
Test Epoch11 layer4 Acc 0.9122368421052631, AUC 0.9762025475502014, avg_entr 0.011841975152492523
gc 0
Train Epoch12 Acc 0.9652666666666667 (115832/120000), AUC 0.9952722787857056
Test Epoch12 layer0 Acc 0.9173684210526316, AUC 0.981412410736084, avg_entr 0.048920001834630966
Test Epoch12 layer1 Acc 0.9132894736842105, AUC 0.9737530946731567, avg_entr 0.017961619421839714
Test Epoch12 layer2 Acc 0.9121052631578948, AUC 0.9748585820198059, avg_entr 0.013641554862260818
Test Epoch12 layer3 Acc 0.9123684210526316, AUC 0.9739980697631836, avg_entr 0.012004966847598553
Test Epoch12 layer4 Acc 0.9118421052631579, AUC 0.9767428636550903, avg_entr 0.011386273428797722
gc 0
Train Epoch13 Acc 0.9659833333333333 (115918/120000), AUC 0.9952542185783386
Test Epoch13 layer0 Acc 0.9168421052631579, AUC 0.9814298152923584, avg_entr 0.0483524426817894
Test Epoch13 layer1 Acc 0.9122368421052631, AUC 0.9745402336120605, avg_entr 0.016877049580216408
Test Epoch13 layer2 Acc 0.9121052631578948, AUC 0.9764410257339478, avg_entr 0.012510574422776699
Test Epoch13 layer3 Acc 0.9122368421052631, AUC 0.9747130870819092, avg_entr 0.010841154493391514
Test Epoch13 layer4 Acc 0.9122368421052631, AUC 0.9762543439865112, avg_entr 0.010174228809773922
gc 0
Train Epoch14 Acc 0.9674166666666667 (116090/120000), AUC 0.9957338571548462
Test Epoch14 layer0 Acc 0.9160526315789473, AUC 0.9813333749771118, avg_entr 0.046868667006492615
Test Epoch14 layer1 Acc 0.9123684210526316, AUC 0.9736446142196655, avg_entr 0.016761871054768562
Test Epoch14 layer2 Acc 0.9118421052631579, AUC 0.975236177444458, avg_entr 0.012817197479307652
Test Epoch14 layer3 Acc 0.9122368421052631, AUC 0.9739704728126526, avg_entr 0.011078541167080402
Test Epoch14 layer4 Acc 0.9123684210526316, AUC 0.975095808506012, avg_entr 0.010497928597033024
gc 0
Train Epoch15 Acc 0.9679333333333333 (116152/120000), AUC 0.9957079887390137
Test Epoch15 layer0 Acc 0.9175, AUC 0.981306791305542, avg_entr 0.04496406018733978
Test Epoch15 layer1 Acc 0.9126315789473685, AUC 0.9731183052062988, avg_entr 0.01639958657324314
Test Epoch15 layer2 Acc 0.9114473684210527, AUC 0.9746232032775879, avg_entr 0.012469341047108173
Test Epoch15 layer3 Acc 0.9111842105263158, AUC 0.9726389050483704, avg_entr 0.010744241997599602
Test Epoch15 layer4 Acc 0.9111842105263158, AUC 0.9754895567893982, avg_entr 0.010124024003744125
gc 0
Train Epoch16 Acc 0.968175 (116181/120000), AUC 0.9959399700164795
Test Epoch16 layer0 Acc 0.9160526315789473, AUC 0.9812599420547485, avg_entr 0.04394672438502312
Test Epoch16 layer1 Acc 0.9126315789473685, AUC 0.9728941321372986, avg_entr 0.015976952388882637
Test Epoch16 layer2 Acc 0.9111842105263158, AUC 0.9741806983947754, avg_entr 0.012068075127899647
Test Epoch16 layer3 Acc 0.9107894736842105, AUC 0.9732772707939148, avg_entr 0.010720224119722843
Test Epoch16 layer4 Acc 0.910921052631579, AUC 0.9757518768310547, avg_entr 0.010171394795179367
gc 0
Train Epoch17 Acc 0.968775 (116253/120000), AUC 0.9959052801132202
Test Epoch17 layer0 Acc 0.9163157894736842, AUC 0.9812167882919312, avg_entr 0.0421486422419548
Test Epoch17 layer1 Acc 0.9123684210526316, AUC 0.972538948059082, avg_entr 0.01577838882803917
Test Epoch17 layer2 Acc 0.9125, AUC 0.9737429022789001, avg_entr 0.011953788809478283
Test Epoch17 layer3 Acc 0.9117105263157895, AUC 0.9727751016616821, avg_entr 0.010005267336964607
Test Epoch17 layer4 Acc 0.9118421052631579, AUC 0.9753831028938293, avg_entr 0.009400807321071625
gc 0
Train Epoch18 Acc 0.9692916666666667 (116315/120000), AUC 0.9960464239120483
Test Epoch18 layer0 Acc 0.9171052631578948, AUC 0.9812124967575073, avg_entr 0.041603557765483856
Test Epoch18 layer1 Acc 0.9122368421052631, AUC 0.9725017547607422, avg_entr 0.015297295525670052
Test Epoch18 layer2 Acc 0.9117105263157895, AUC 0.9738588929176331, avg_entr 0.011271733790636063
Test Epoch18 layer3 Acc 0.911578947368421, AUC 0.9724596738815308, avg_entr 0.009632302448153496
Test Epoch18 layer4 Acc 0.9118421052631579, AUC 0.974463701248169, avg_entr 0.009125753305852413
gc 0
Train Epoch19 Acc 0.9695583333333333 (116347/120000), AUC 0.9960217475891113
Test Epoch19 layer0 Acc 0.9172368421052631, AUC 0.9812382459640503, avg_entr 0.04057404026389122
Test Epoch19 layer1 Acc 0.9117105263157895, AUC 0.9726990461349487, avg_entr 0.014987124130129814
Test Epoch19 layer2 Acc 0.9105263157894737, AUC 0.9740005135536194, avg_entr 0.011029478162527084
Test Epoch19 layer3 Acc 0.9103947368421053, AUC 0.9714334011077881, avg_entr 0.009503151290118694
Test Epoch19 layer4 Acc 0.9102631578947369, AUC 0.9745088815689087, avg_entr 0.008988285437226295
gc 0
Train Epoch20 Acc 0.96985 (116382/120000), AUC 0.9960702657699585
Test Epoch20 layer0 Acc 0.9164473684210527, AUC 0.9812291860580444, avg_entr 0.03988730162382126
Test Epoch20 layer1 Acc 0.9119736842105263, AUC 0.972629725933075, avg_entr 0.014824350364506245
Test Epoch20 layer2 Acc 0.9105263157894737, AUC 0.9743818044662476, avg_entr 0.010979879647493362
Test Epoch20 layer3 Acc 0.910921052631579, AUC 0.9721181392669678, avg_entr 0.009104430675506592
Test Epoch20 layer4 Acc 0.9111842105263158, AUC 0.974528968334198, avg_entr 0.008575749583542347
gc 0
Train Epoch21 Acc 0.9698583333333334 (116383/120000), AUC 0.996189296245575
Test Epoch21 layer0 Acc 0.9163157894736842, AUC 0.9812159538269043, avg_entr 0.0391346737742424
Test Epoch21 layer1 Acc 0.910921052631579, AUC 0.9728696346282959, avg_entr 0.014658385887742043
Test Epoch21 layer2 Acc 0.91, AUC 0.9747601747512817, avg_entr 0.010700522921979427
Test Epoch21 layer3 Acc 0.91, AUC 0.9726349115371704, avg_entr 0.009139069356024265
Test Epoch21 layer4 Acc 0.9103947368421053, AUC 0.9745568037033081, avg_entr 0.008655887097120285
gc 0
Train Epoch22 Acc 0.9703916666666667 (116447/120000), AUC 0.9961183071136475
Test Epoch22 layer0 Acc 0.9163157894736842, AUC 0.981196403503418, avg_entr 0.0387558676302433
Test Epoch22 layer1 Acc 0.9111842105263158, AUC 0.9723677635192871, avg_entr 0.014568316750228405
Test Epoch22 layer2 Acc 0.9106578947368421, AUC 0.9740672707557678, avg_entr 0.010718380101025105
Test Epoch22 layer3 Acc 0.9105263157894737, AUC 0.9713741540908813, avg_entr 0.009112225845456123
Test Epoch22 layer4 Acc 0.9107894736842105, AUC 0.9736257791519165, avg_entr 0.008622484281659126
gc 0
Train Epoch23 Acc 0.9702916666666667 (116435/120000), AUC 0.9962048530578613
Test Epoch23 layer0 Acc 0.9168421052631579, AUC 0.9811965227127075, avg_entr 0.038377176970243454
Test Epoch23 layer1 Acc 0.9107894736842105, AUC 0.9723252654075623, avg_entr 0.014393385499715805
Test Epoch23 layer2 Acc 0.9105263157894737, AUC 0.9737675189971924, avg_entr 0.010228338651359081
Test Epoch23 layer3 Acc 0.9103947368421053, AUC 0.9714523553848267, avg_entr 0.008796710520982742
Test Epoch23 layer4 Acc 0.91, AUC 0.9720609188079834, avg_entr 0.008373997174203396
gc 0
Train Epoch24 Acc 0.970475 (116457/120000), AUC 0.9962558150291443
Test Epoch24 layer0 Acc 0.9171052631578948, AUC 0.9812034964561462, avg_entr 0.038551442325115204
Test Epoch24 layer1 Acc 0.9106578947368421, AUC 0.9723728895187378, avg_entr 0.014390571042895317
Test Epoch24 layer2 Acc 0.9105263157894737, AUC 0.9740530848503113, avg_entr 0.010342041961848736
Test Epoch24 layer3 Acc 0.9102631578947369, AUC 0.9709537029266357, avg_entr 0.008846698328852654
Test Epoch24 layer4 Acc 0.9105263157894737, AUC 0.9729447364807129, avg_entr 0.008456725627183914
gc 0
Train Epoch25 Acc 0.970725 (116487/120000), AUC 0.9962303638458252
Test Epoch25 layer0 Acc 0.9160526315789473, AUC 0.9811700582504272, avg_entr 0.03771470487117767
Test Epoch25 layer1 Acc 0.910921052631579, AUC 0.9722814559936523, avg_entr 0.01418470498174429
Test Epoch25 layer2 Acc 0.9103947368421053, AUC 0.9737972021102905, avg_entr 0.009901031851768494
Test Epoch25 layer3 Acc 0.910921052631579, AUC 0.9708934426307678, avg_entr 0.008461021818220615
Test Epoch25 layer4 Acc 0.9110526315789473, AUC 0.972652792930603, avg_entr 0.008078302256762981
gc 0
Train Epoch26 Acc 0.9708 (116496/120000), AUC 0.9962336421012878
Test Epoch26 layer0 Acc 0.9163157894736842, AUC 0.9811723232269287, avg_entr 0.03780021890997887
Test Epoch26 layer1 Acc 0.9106578947368421, AUC 0.9722516536712646, avg_entr 0.014223767444491386
Test Epoch26 layer2 Acc 0.91, AUC 0.9737004041671753, avg_entr 0.009934179484844208
Test Epoch26 layer3 Acc 0.9106578947368421, AUC 0.9702293276786804, avg_entr 0.008517048321664333
Test Epoch26 layer4 Acc 0.9105263157894737, AUC 0.9719423055648804, avg_entr 0.00813098344951868
gc 0
Train Epoch27 Acc 0.9706166666666667 (116474/120000), AUC 0.9962804913520813
Test Epoch27 layer0 Acc 0.9169736842105263, AUC 0.9811760187149048, avg_entr 0.03758905827999115
Test Epoch27 layer1 Acc 0.9106578947368421, AUC 0.9722449779510498, avg_entr 0.014210645109415054
Test Epoch27 layer2 Acc 0.9102631578947369, AUC 0.973782479763031, avg_entr 0.010058884508907795
Test Epoch27 layer3 Acc 0.91, AUC 0.9707611799240112, avg_entr 0.008579418063163757
Test Epoch27 layer4 Acc 0.9103947368421053, AUC 0.9727135896682739, avg_entr 0.008167887106537819
gc 0
Train Epoch28 Acc 0.97065 (116478/120000), AUC 0.9963369965553284
Test Epoch28 layer0 Acc 0.9169736842105263, AUC 0.9811738133430481, avg_entr 0.03749455511569977
Test Epoch28 layer1 Acc 0.9106578947368421, AUC 0.9721781015396118, avg_entr 0.01410150807350874
Test Epoch28 layer2 Acc 0.9107894736842105, AUC 0.9734355211257935, avg_entr 0.009866764768958092
Test Epoch28 layer3 Acc 0.9105263157894737, AUC 0.9703606367111206, avg_entr 0.008392083458602428
Test Epoch28 layer4 Acc 0.9107894736842105, AUC 0.9720415472984314, avg_entr 0.007964801974594593
gc 0
Train Epoch29 Acc 0.9707666666666667 (116492/120000), AUC 0.9962592124938965
Test Epoch29 layer0 Acc 0.9163157894736842, AUC 0.9811704158782959, avg_entr 0.0373808853328228
Test Epoch29 layer1 Acc 0.9102631578947369, AUC 0.972182035446167, avg_entr 0.014062200672924519
Test Epoch29 layer2 Acc 0.910921052631579, AUC 0.9734903573989868, avg_entr 0.009964484721422195
Test Epoch29 layer3 Acc 0.9106578947368421, AUC 0.970768392086029, avg_entr 0.008422227576375008
Test Epoch29 layer4 Acc 0.9106578947368421, AUC 0.9722535610198975, avg_entr 0.007988530211150646
Best AUC 0.982596218585968
train_loss (2, 5, 30)
valid_acc (5, 30)
valid_AUC (5, 30)
train_acc (30,)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Start Testing
Load ckpt at ckpt/ag_news_transformeral_l5_pad200//ag_news_transformeral_l5.pt
[[1697   66   90   47]
 [   8 1877    8    7]
 [  45   21 1675  159]
 [  52   17  118 1713]]
Figure(640x480)
tensor([0.0792, 0.0035, 0.0618,  ..., 0.0569, 0.0147, 0.5990])
[[1722   56   64   58]
 [  12 1866    9   13]
 [  50   18 1664  168]
 [  52   12  101 1735]]
Figure(640x480)
tensor([0.0772, 0.0032, 0.0040,  ..., 0.0066, 0.0048, 0.0120])
[[1722   55   64   59]
 [  12 1863   10   15]
 [  54   15 1661  170]
 [  48   12  100 1740]]
Figure(640x480)
tensor([0.0450, 0.0045, 0.0053,  ..., 0.0062, 0.0050, 0.0059])
[[1724   55   63   58]
 [  13 1863    9   15]
 [  55   15 1659  171]
 [  49   12   95 1744]]
Figure(640x480)
tensor([0.0456, 0.0036, 0.0047,  ..., 0.0055, 0.0047, 0.0063])
[[1724   55   63   58]
 [  13 1861   11   15]
 [  55   15 1659  171]
 [  49   12   95 1744]]
Figure(640x480)
tensor([0.0298, 0.0035, 0.0044,  ..., 0.0054, 0.0048, 0.0062])
