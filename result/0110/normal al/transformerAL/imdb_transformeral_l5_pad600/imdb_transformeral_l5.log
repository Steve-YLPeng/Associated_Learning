total count words 222751
vocab size 30000
found 27937 words in glove
model: TransformerModelML(
  (layers): ModuleList(
    (0): EMBLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Embedding(30000, 300)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=2, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=2, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.weight 9000000
layers.0.ae.g.0.weight 256
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 256
layers.0.ae.h.0.bias 2
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
Total Trainable Params: 13671778
Start Training
gc 0
Train Epoch0 Acc 0.48575 (19430/40000), AUC 0.4742286205291748
Test Epoch0 layer0 Acc 0.7885, AUC 0.8828820586204529, avg_entr 0.6069024801254272
Save ckpt to ckpt/imdb_transformeral_l5_pad600//imdb_transformeral_l5.pt  ,ep 0
Test Epoch0 layer1 Acc 0.816, AUC 0.9013131260871887, avg_entr 0.3703080713748932
Save ckpt to ckpt/imdb_transformeral_l5_pad600//imdb_transformeral_l5.pt  ,ep 0
Test Epoch0 layer2 Acc 0.8159, AUC 0.8986709713935852, avg_entr 0.5814356207847595
Test Epoch0 layer3 Acc 0.8162, AUC 0.8981108665466309, avg_entr 0.641755223274231
Test Epoch0 layer4 Acc 0.4418, AUC 0.18272417783737183, avg_entr 0.6944603323936462
gc 0
Train Epoch1 Acc 0.82475 (32990/40000), AUC 0.9119791984558105
Test Epoch1 layer0 Acc 0.8773, AUC 0.9433829188346863, avg_entr 0.30576181411743164
Save ckpt to ckpt/imdb_transformeral_l5_pad600//imdb_transformeral_l5.pt  ,ep 1
Test Epoch1 layer1 Acc 0.8876, AUC 0.9560947418212891, avg_entr 0.18876658380031586
Save ckpt to ckpt/imdb_transformeral_l5_pad600//imdb_transformeral_l5.pt  ,ep 1
Test Epoch1 layer2 Acc 0.8818, AUC 0.9557991027832031, avg_entr 0.17890778183937073
Test Epoch1 layer3 Acc 0.876, AUC 0.9552825689315796, avg_entr 0.1507643610239029
Test Epoch1 layer4 Acc 0.8748, AUC 0.9546787738800049, avg_entr 0.13536730408668518
gc 0
Train Epoch2 Acc 0.91215 (36486/40000), AUC 0.9670472145080566
Test Epoch2 layer0 Acc 0.8909, AUC 0.9539105296134949, avg_entr 0.22981689870357513
Test Epoch2 layer1 Acc 0.9013, AUC 0.9625623226165771, avg_entr 0.14556069672107697
Save ckpt to ckpt/imdb_transformeral_l5_pad600//imdb_transformeral_l5.pt  ,ep 2
Test Epoch2 layer2 Acc 0.8998, AUC 0.9627832174301147, avg_entr 0.10985518991947174
Save ckpt to ckpt/imdb_transformeral_l5_pad600//imdb_transformeral_l5.pt  ,ep 2
Test Epoch2 layer3 Acc 0.8992, AUC 0.9627450704574585, avg_entr 0.07267691940069199
Test Epoch2 layer4 Acc 0.8998, AUC 0.9628745317459106, avg_entr 0.06768093258142471
Save ckpt to ckpt/imdb_transformeral_l5_pad600//imdb_transformeral_l5.pt  ,ep 2
gc 0
Train Epoch3 Acc 0.93475 (37390/40000), AUC 0.9785380363464355
Test Epoch3 layer0 Acc 0.8932, AUC 0.9582800269126892, avg_entr 0.1985180377960205
Test Epoch3 layer1 Acc 0.8988, AUC 0.9606442451477051, avg_entr 0.08698827028274536
Test Epoch3 layer2 Acc 0.8974, AUC 0.959813117980957, avg_entr 0.05279809609055519
Test Epoch3 layer3 Acc 0.8973, AUC 0.9609613418579102, avg_entr 0.04438326507806778
Test Epoch3 layer4 Acc 0.8979, AUC 0.9616671800613403, avg_entr 0.04324091970920563
gc 0
Train Epoch4 Acc 0.9474 (37896/40000), AUC 0.9835700988769531
Test Epoch4 layer0 Acc 0.8956, AUC 0.9598180055618286, avg_entr 0.17701953649520874
Test Epoch4 layer1 Acc 0.8781, AUC 0.9537744522094727, avg_entr 0.05788407474756241
Test Epoch4 layer2 Acc 0.8798, AUC 0.9569637775421143, avg_entr 0.044669486582279205
Test Epoch4 layer3 Acc 0.8773, AUC 0.9581179022789001, avg_entr 0.042084746062755585
Test Epoch4 layer4 Acc 0.8759, AUC 0.958759069442749, avg_entr 0.04102056473493576
gc 0
Train Epoch5 Acc 0.953325 (38133/40000), AUC 0.9861040115356445
Test Epoch5 layer0 Acc 0.8968, AUC 0.9601448774337769, avg_entr 0.16685597598552704
Test Epoch5 layer1 Acc 0.8932, AUC 0.9507217407226562, avg_entr 0.045292068272829056
Test Epoch5 layer2 Acc 0.8943, AUC 0.9531515836715698, avg_entr 0.03433762490749359
Test Epoch5 layer3 Acc 0.8937, AUC 0.9559732675552368, avg_entr 0.0334244929254055
Test Epoch5 layer4 Acc 0.8937, AUC 0.9565680027008057, avg_entr 0.032410770654678345
gc 0
Train Epoch6 Acc 0.95945 (38378/40000), AUC 0.9883366823196411
Test Epoch6 layer0 Acc 0.8928, AUC 0.9593663811683655, avg_entr 0.15368647873401642
Test Epoch6 layer1 Acc 0.8889, AUC 0.9490975141525269, avg_entr 0.039163365960121155
Test Epoch6 layer2 Acc 0.89, AUC 0.9540227055549622, avg_entr 0.03256925195455551
Test Epoch6 layer3 Acc 0.8901, AUC 0.9554052352905273, avg_entr 0.0317322313785553
Test Epoch6 layer4 Acc 0.8891, AUC 0.9558870792388916, avg_entr 0.030158312991261482
gc 0
Train Epoch7 Acc 0.9637 (38548/40000), AUC 0.9899537563323975
Test Epoch7 layer0 Acc 0.8996, AUC 0.9586300849914551, avg_entr 0.14689049124717712
Test Epoch7 layer1 Acc 0.8875, AUC 0.9422845840454102, avg_entr 0.03450126573443413
Test Epoch7 layer2 Acc 0.8874, AUC 0.9487717151641846, avg_entr 0.027859143912792206
Test Epoch7 layer3 Acc 0.8868, AUC 0.9518144130706787, avg_entr 0.02725204825401306
Test Epoch7 layer4 Acc 0.8868, AUC 0.9524738788604736, avg_entr 0.02613706700503826
gc 0
Train Epoch8 Acc 0.96655 (38662/40000), AUC 0.9907768964767456
Test Epoch8 layer0 Acc 0.8959, AUC 0.9572286605834961, avg_entr 0.14112970232963562
Test Epoch8 layer1 Acc 0.8805, AUC 0.9381187558174133, avg_entr 0.03231128305196762
Test Epoch8 layer2 Acc 0.8809, AUC 0.9454740285873413, avg_entr 0.02591034583747387
Test Epoch8 layer3 Acc 0.8806, AUC 0.9479775428771973, avg_entr 0.02544538304209709
Test Epoch8 layer4 Acc 0.881, AUC 0.9488773345947266, avg_entr 0.024257877841591835
gc 0
Train Epoch9 Acc 0.96825 (38730/40000), AUC 0.991938591003418
Test Epoch9 layer0 Acc 0.8931, AUC 0.9555150270462036, avg_entr 0.1364213079214096
Test Epoch9 layer1 Acc 0.8798, AUC 0.9370892643928528, avg_entr 0.03031434863805771
Test Epoch9 layer2 Acc 0.8797, AUC 0.9440401792526245, avg_entr 0.024626553058624268
Test Epoch9 layer3 Acc 0.8797, AUC 0.9472946524620056, avg_entr 0.02405298687517643
Test Epoch9 layer4 Acc 0.8799, AUC 0.9483057260513306, avg_entr 0.022801971063017845
gc 0
Train Epoch10 Acc 0.971375 (38855/40000), AUC 0.9933291673660278
Test Epoch10 layer0 Acc 0.8937, AUC 0.9551070332527161, avg_entr 0.1319548785686493
Test Epoch10 layer1 Acc 0.8817, AUC 0.9365304708480835, avg_entr 0.027676282450556755
Test Epoch10 layer2 Acc 0.882, AUC 0.9439562559127808, avg_entr 0.023939721286296844
Test Epoch10 layer3 Acc 0.8818, AUC 0.9472731947898865, avg_entr 0.023745611310005188
Test Epoch10 layer4 Acc 0.882, AUC 0.9483327865600586, avg_entr 0.022361092269420624
gc 0
Train Epoch11 Acc 0.9727 (38908/40000), AUC 0.99400794506073
Test Epoch11 layer0 Acc 0.8926, AUC 0.954161524772644, avg_entr 0.12922348082065582
Test Epoch11 layer1 Acc 0.8787, AUC 0.9337129592895508, avg_entr 0.027179120108485222
Test Epoch11 layer2 Acc 0.879, AUC 0.9391692876815796, avg_entr 0.022485483437776566
Test Epoch11 layer3 Acc 0.8788, AUC 0.9446181654930115, avg_entr 0.022338081151247025
Test Epoch11 layer4 Acc 0.8788, AUC 0.946068525314331, avg_entr 0.021321125328540802
gc 0
Train Epoch12 Acc 0.973625 (38945/40000), AUC 0.994330883026123
Test Epoch12 layer0 Acc 0.8877, AUC 0.9531738758087158, avg_entr 0.12583857774734497
Test Epoch12 layer1 Acc 0.8772, AUC 0.9295488595962524, avg_entr 0.024574199691414833
Test Epoch12 layer2 Acc 0.8776, AUC 0.9337834119796753, avg_entr 0.01954926922917366
Test Epoch12 layer3 Acc 0.8775, AUC 0.9422827959060669, avg_entr 0.019204381853342056
Test Epoch12 layer4 Acc 0.8773, AUC 0.9443258047103882, avg_entr 0.018236126750707626
gc 0
Train Epoch13 Acc 0.97415 (38966/40000), AUC 0.994052529335022
Test Epoch13 layer0 Acc 0.8867, AUC 0.9519620537757874, avg_entr 0.12385743111371994
Test Epoch13 layer1 Acc 0.8778, AUC 0.9278821349143982, avg_entr 0.025869445875287056
Test Epoch13 layer2 Acc 0.8775, AUC 0.9296420812606812, avg_entr 0.020436709746718407
Test Epoch13 layer3 Acc 0.8775, AUC 0.9403060674667358, avg_entr 0.01997506245970726
Test Epoch13 layer4 Acc 0.8779, AUC 0.942973256111145, avg_entr 0.018953662365674973
gc 0
Train Epoch14 Acc 0.97615 (39046/40000), AUC 0.9947046041488647
Test Epoch14 layer0 Acc 0.8877, AUC 0.9517236948013306, avg_entr 0.1224062591791153
Test Epoch14 layer1 Acc 0.8767, AUC 0.9258536100387573, avg_entr 0.023712370544672012
Test Epoch14 layer2 Acc 0.8781, AUC 0.9285818338394165, avg_entr 0.018672965466976166
Test Epoch14 layer3 Acc 0.8782, AUC 0.9391727447509766, avg_entr 0.018477752804756165
Test Epoch14 layer4 Acc 0.8782, AUC 0.9420856833457947, avg_entr 0.017669107764959335
gc 0
Train Epoch15 Acc 0.976575 (39063/40000), AUC 0.9950322508811951
Test Epoch15 layer0 Acc 0.8855, AUC 0.9511955976486206, avg_entr 0.12031935155391693
Test Epoch15 layer1 Acc 0.877, AUC 0.9264494180679321, avg_entr 0.022960465401411057
Test Epoch15 layer2 Acc 0.8777, AUC 0.92783522605896, avg_entr 0.018361568450927734
Test Epoch15 layer3 Acc 0.878, AUC 0.9384258389472961, avg_entr 0.018135467544198036
Test Epoch15 layer4 Acc 0.8779, AUC 0.9416921734809875, avg_entr 0.017276719212532043
gc 0
Train Epoch16 Acc 0.976875 (39075/40000), AUC 0.9951696395874023
Test Epoch16 layer0 Acc 0.8869, AUC 0.95064377784729, avg_entr 0.12000833451747894
Test Epoch16 layer1 Acc 0.8761, AUC 0.9243410229682922, avg_entr 0.022747576236724854
Test Epoch16 layer2 Acc 0.8757, AUC 0.9230899214744568, avg_entr 0.017720621079206467
Test Epoch16 layer3 Acc 0.8759, AUC 0.9360666275024414, avg_entr 0.01742819882929325
Test Epoch16 layer4 Acc 0.876, AUC 0.9401310086250305, avg_entr 0.016601242125034332
gc 0
Train Epoch17 Acc 0.9775 (39100/40000), AUC 0.9951639175415039
Test Epoch17 layer0 Acc 0.8867, AUC 0.9502592086791992, avg_entr 0.1183566153049469
Test Epoch17 layer1 Acc 0.8761, AUC 0.9242868423461914, avg_entr 0.022457653656601906
Test Epoch17 layer2 Acc 0.8756, AUC 0.9229289293289185, avg_entr 0.017310690134763718
Test Epoch17 layer3 Acc 0.8755, AUC 0.9355740547180176, avg_entr 0.017113851383328438
Test Epoch17 layer4 Acc 0.8755, AUC 0.9398168325424194, avg_entr 0.01629846729338169
gc 0
Train Epoch18 Acc 0.97765 (39106/40000), AUC 0.9953291416168213
Test Epoch18 layer0 Acc 0.8837, AUC 0.9498279094696045, avg_entr 0.11680247634649277
Test Epoch18 layer1 Acc 0.8741, AUC 0.9215610027313232, avg_entr 0.02137758396565914
Test Epoch18 layer2 Acc 0.874, AUC 0.9170535206794739, avg_entr 0.016054697334766388
Test Epoch18 layer3 Acc 0.8741, AUC 0.932198166847229, avg_entr 0.015803491696715355
Test Epoch18 layer4 Acc 0.8739, AUC 0.9380435347557068, avg_entr 0.014971914701163769
gc 0
Train Epoch19 Acc 0.9786 (39144/40000), AUC 0.9958041906356812
Test Epoch19 layer0 Acc 0.8831, AUC 0.9495792984962463, avg_entr 0.11605358868837357
Test Epoch19 layer1 Acc 0.8747, AUC 0.9212732315063477, avg_entr 0.02118965983390808
Test Epoch19 layer2 Acc 0.8735, AUC 0.9164196848869324, avg_entr 0.016090979799628258
Test Epoch19 layer3 Acc 0.8738, AUC 0.9316120147705078, avg_entr 0.01576465740799904
Test Epoch19 layer4 Acc 0.8738, AUC 0.9379310607910156, avg_entr 0.014966759830713272
gc 0
Train Epoch20 Acc 0.97895 (39158/40000), AUC 0.9956262111663818
Test Epoch20 layer0 Acc 0.8841, AUC 0.9493517279624939, avg_entr 0.11517143249511719
Test Epoch20 layer1 Acc 0.8732, AUC 0.9212229251861572, avg_entr 0.021783174946904182
Test Epoch20 layer2 Acc 0.8735, AUC 0.9167704582214355, avg_entr 0.017079902812838554
Test Epoch20 layer3 Acc 0.8737, AUC 0.931586742401123, avg_entr 0.016848251223564148
Test Epoch20 layer4 Acc 0.8737, AUC 0.9378997087478638, avg_entr 0.016068505123257637
gc 0
Train Epoch21 Acc 0.978825 (39153/40000), AUC 0.9955500364303589
Test Epoch21 layer0 Acc 0.8847, AUC 0.9490572810173035, avg_entr 0.11439856141805649
Test Epoch21 layer1 Acc 0.8745, AUC 0.9204199314117432, avg_entr 0.020910736173391342
Test Epoch21 layer2 Acc 0.8739, AUC 0.9138372540473938, avg_entr 0.015661317855119705
Test Epoch21 layer3 Acc 0.8738, AUC 0.9291521310806274, avg_entr 0.015450197272002697
Test Epoch21 layer4 Acc 0.8737, AUC 0.9367184638977051, avg_entr 0.014739236794412136
gc 0
Train Epoch22 Acc 0.9789 (39156/40000), AUC 0.9958080053329468
Test Epoch22 layer0 Acc 0.8827, AUC 0.948876142501831, avg_entr 0.11366996169090271
Test Epoch22 layer1 Acc 0.8747, AUC 0.9202201962471008, avg_entr 0.02031290903687477
Test Epoch22 layer2 Acc 0.8741, AUC 0.9121673107147217, avg_entr 0.0148540074005723
Test Epoch22 layer3 Acc 0.8739, AUC 0.9283739924430847, avg_entr 0.01452552154660225
Test Epoch22 layer4 Acc 0.8738, AUC 0.936255693435669, avg_entr 0.013713601976633072
gc 0
Train Epoch23 Acc 0.97955 (39182/40000), AUC 0.9960514307022095
Test Epoch23 layer0 Acc 0.8831, AUC 0.9487301707267761, avg_entr 0.11310169845819473
Test Epoch23 layer1 Acc 0.8747, AUC 0.9201063513755798, avg_entr 0.020614588633179665
Test Epoch23 layer2 Acc 0.8731, AUC 0.9131163358688354, avg_entr 0.015654990449547768
Test Epoch23 layer3 Acc 0.8729, AUC 0.9284893870353699, avg_entr 0.015320106409490108
Test Epoch23 layer4 Acc 0.873, AUC 0.9362356662750244, avg_entr 0.014565804973244667
gc 0
Train Epoch24 Acc 0.980325 (39213/40000), AUC 0.995888352394104
Test Epoch24 layer0 Acc 0.8845, AUC 0.9486186504364014, avg_entr 0.11285664141178131
Test Epoch24 layer1 Acc 0.8743, AUC 0.9197648763656616, avg_entr 0.020418435335159302
Test Epoch24 layer2 Acc 0.8732, AUC 0.912445068359375, avg_entr 0.015458869747817516
Test Epoch24 layer3 Acc 0.8734, AUC 0.9278615713119507, avg_entr 0.015292280353605747
Test Epoch24 layer4 Acc 0.8738, AUC 0.9359665513038635, avg_entr 0.014655999839305878
gc 0
Train Epoch25 Acc 0.98 (39200/40000), AUC 0.9959969520568848
Test Epoch25 layer0 Acc 0.8832, AUC 0.9484472274780273, avg_entr 0.11176589131355286
Test Epoch25 layer1 Acc 0.8748, AUC 0.9191954731941223, avg_entr 0.02003653533756733
Test Epoch25 layer2 Acc 0.8738, AUC 0.9110527038574219, avg_entr 0.014671284705400467
Test Epoch25 layer3 Acc 0.8739, AUC 0.926301121711731, avg_entr 0.01443982869386673
Test Epoch25 layer4 Acc 0.8738, AUC 0.9353217482566833, avg_entr 0.013633097521960735
gc 0
Train Epoch26 Acc 0.979725 (39189/40000), AUC 0.9961794018745422
Test Epoch26 layer0 Acc 0.883, AUC 0.9484214782714844, avg_entr 0.11147647351026535
Test Epoch26 layer1 Acc 0.8728, AUC 0.9189640879631042, avg_entr 0.02066917158663273
Test Epoch26 layer2 Acc 0.8726, AUC 0.9110053777694702, avg_entr 0.015923449769616127
Test Epoch26 layer3 Acc 0.8729, AUC 0.9267386198043823, avg_entr 0.01570574939250946
Test Epoch26 layer4 Acc 0.8726, AUC 0.9353721141815186, avg_entr 0.014951763674616814
gc 0
Train Epoch27 Acc 0.98005 (39202/40000), AUC 0.996254563331604
Test Epoch27 layer0 Acc 0.8842, AUC 0.9483897686004639, avg_entr 0.11119972914457321
Test Epoch27 layer1 Acc 0.8749, AUC 0.9187520742416382, avg_entr 0.019785160198807716
Test Epoch27 layer2 Acc 0.8733, AUC 0.9103857278823853, avg_entr 0.014511311426758766
Test Epoch27 layer3 Acc 0.8737, AUC 0.9254821538925171, avg_entr 0.014300905168056488
Test Epoch27 layer4 Acc 0.8736, AUC 0.934932291507721, avg_entr 0.013533293269574642
gc 0
Train Epoch28 Acc 0.98005 (39202/40000), AUC 0.9960309863090515
Test Epoch28 layer0 Acc 0.8817, AUC 0.9482781887054443, avg_entr 0.11052597314119339
Test Epoch28 layer1 Acc 0.8741, AUC 0.9182294607162476, avg_entr 0.01989121176302433
Test Epoch28 layer2 Acc 0.8736, AUC 0.9088504314422607, avg_entr 0.014630922116339207
Test Epoch28 layer3 Acc 0.8738, AUC 0.9245725870132446, avg_entr 0.014390192925930023
Test Epoch28 layer4 Acc 0.8737, AUC 0.9344359636306763, avg_entr 0.013619867153465748
gc 0
Train Epoch29 Acc 0.979775 (39191/40000), AUC 0.9958581924438477
Test Epoch29 layer0 Acc 0.8837, AUC 0.9482816457748413, avg_entr 0.11027618497610092
Test Epoch29 layer1 Acc 0.8748, AUC 0.91827392578125, avg_entr 0.019619321450591087
Test Epoch29 layer2 Acc 0.8736, AUC 0.9094834327697754, avg_entr 0.014511995017528534
Test Epoch29 layer3 Acc 0.8733, AUC 0.9245193004608154, avg_entr 0.014313186518847942
Test Epoch29 layer4 Acc 0.8733, AUC 0.9344929456710815, avg_entr 0.013580291531980038
Best AUC 0.9628745317459106
train_loss (2, 5, 30)
valid_acc (5, 30)
valid_AUC (5, 30)
train_acc (30,)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Start Testing
Load ckpt at ckpt/imdb_transformeral_l5_pad600//imdb_transformeral_l5.pt
[[4433  561]
 [ 530 4476]]
Figure(640x480)
tensor([7.2757e-02, 1.1297e-01, 5.3598e-03,  ..., 2.7247e-02, 2.1798e-04,
        4.8837e-01])
[[4541  453]
 [ 534 4472]]
Figure(640x480)
tensor([0.0070, 0.0053, 0.0007,  ..., 0.0013, 0.0172, 0.1260])
[[4518  476]
 [ 526 4480]]
Figure(640x480)
tensor([0.0046, 0.0042, 0.0028,  ..., 0.0024, 0.0108, 0.1525])
[[4550  444]
 [ 564 4442]]
Figure(640x480)
tensor([0.0062, 0.0051, 0.0046,  ..., 0.0050, 0.0086, 0.0373])
[[4566  428]
 [ 574 4432]]
Figure(640x480)
tensor([0.0055, 0.0049, 0.0033,  ..., 0.0056, 0.0087, 0.0318])
