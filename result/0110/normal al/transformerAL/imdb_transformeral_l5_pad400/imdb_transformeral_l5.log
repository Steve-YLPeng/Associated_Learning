total count words 222751
vocab size 30000
found 27937 words in glove
model: TransformerModelML(
  (layers): ModuleList(
    (0): EMBLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Embedding(30000, 300)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=2, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=2, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.weight 9000000
layers.0.ae.g.0.weight 256
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 256
layers.0.ae.h.0.bias 2
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
Total Trainable Params: 13671778
Start Training
gc 0
Train Epoch0 Acc 0.525225 (21009/40000), AUC 0.5318098664283752
Test Epoch0 layer0 Acc 0.7896, AUC 0.8959090709686279, avg_entr 0.5290884375572205
Save ckpt to ckpt/imdb_transformeral_l5_pad400//imdb_transformeral_l5.pt  ,ep 0
Test Epoch0 layer1 Acc 0.8026, AUC 0.9040263295173645, avg_entr 0.36095914244651794
Save ckpt to ckpt/imdb_transformeral_l5_pad400//imdb_transformeral_l5.pt  ,ep 0
Test Epoch0 layer2 Acc 0.7786, AUC 0.9047654867172241, avg_entr 0.5003634095191956
Save ckpt to ckpt/imdb_transformeral_l5_pad400//imdb_transformeral_l5.pt  ,ep 0
Test Epoch0 layer3 Acc 0.747, AUC 0.8977099657058716, avg_entr 0.6477168798446655
Test Epoch0 layer4 Acc 0.6355, AUC 0.8948479294776917, avg_entr 0.6805513501167297
gc 0
Train Epoch1 Acc 0.853625 (34145/40000), AUC 0.9228336811065674
Test Epoch1 layer0 Acc 0.8794, AUC 0.9461780190467834, avg_entr 0.2719549834728241
Save ckpt to ckpt/imdb_transformeral_l5_pad400//imdb_transformeral_l5.pt  ,ep 1
Test Epoch1 layer1 Acc 0.8824, AUC 0.9522699117660522, avg_entr 0.19900056719779968
Save ckpt to ckpt/imdb_transformeral_l5_pad400//imdb_transformeral_l5.pt  ,ep 1
Test Epoch1 layer2 Acc 0.8721, AUC 0.9533438086509705, avg_entr 0.18232446908950806
Save ckpt to ckpt/imdb_transformeral_l5_pad400//imdb_transformeral_l5.pt  ,ep 1
Test Epoch1 layer3 Acc 0.8612, AUC 0.953266978263855, avg_entr 0.15339036285877228
Test Epoch1 layer4 Acc 0.8552, AUC 0.9533156156539917, avg_entr 0.1463155746459961
gc 0
Train Epoch2 Acc 0.911575 (36463/40000), AUC 0.9671974182128906
Test Epoch2 layer0 Acc 0.8865, AUC 0.9540796279907227, avg_entr 0.21761538088321686
Save ckpt to ckpt/imdb_transformeral_l5_pad400//imdb_transformeral_l5.pt  ,ep 2
Test Epoch2 layer1 Acc 0.8778, AUC 0.9571592807769775, avg_entr 0.16889679431915283
Save ckpt to ckpt/imdb_transformeral_l5_pad400//imdb_transformeral_l5.pt  ,ep 2
Test Epoch2 layer2 Acc 0.8813, AUC 0.9568848609924316, avg_entr 0.11514677852392197
Test Epoch2 layer3 Acc 0.8844, AUC 0.9566770792007446, avg_entr 0.08283770829439163
Test Epoch2 layer4 Acc 0.8832, AUC 0.9568277597427368, avg_entr 0.0811769887804985
gc 0
Train Epoch3 Acc 0.934675 (37387/40000), AUC 0.9786134958267212
Test Epoch3 layer0 Acc 0.8972, AUC 0.9560559988021851, avg_entr 0.1868099868297577
Test Epoch3 layer1 Acc 0.8817, AUC 0.954970121383667, avg_entr 0.13974252343177795
Test Epoch3 layer2 Acc 0.8822, AUC 0.953301191329956, avg_entr 0.06678608059883118
Test Epoch3 layer3 Acc 0.8852, AUC 0.9554077386856079, avg_entr 0.052539464086294174
Test Epoch3 layer4 Acc 0.8832, AUC 0.9557182788848877, avg_entr 0.04928591847419739
gc 0
Train Epoch4 Acc 0.951425 (38057/40000), AUC 0.9851535558700562
Test Epoch4 layer0 Acc 0.8802, AUC 0.9557708501815796, avg_entr 0.16239142417907715
Test Epoch4 layer1 Acc 0.8863, AUC 0.9510143995285034, avg_entr 0.08278697729110718
Test Epoch4 layer2 Acc 0.8862, AUC 0.9486416578292847, avg_entr 0.040293946862220764
Test Epoch4 layer3 Acc 0.8863, AUC 0.9515008926391602, avg_entr 0.04020855575799942
Test Epoch4 layer4 Acc 0.8862, AUC 0.9510332942008972, avg_entr 0.03909295052289963
gc 0
Train Epoch5 Acc 0.95785 (38314/40000), AUC 0.987688422203064
Test Epoch5 layer0 Acc 0.893, AUC 0.9546164274215698, avg_entr 0.15435679256916046
Test Epoch5 layer1 Acc 0.8878, AUC 0.9484674334526062, avg_entr 0.054420337080955505
Test Epoch5 layer2 Acc 0.8876, AUC 0.9510670900344849, avg_entr 0.03375697135925293
Test Epoch5 layer3 Acc 0.8875, AUC 0.9516199827194214, avg_entr 0.0356886088848114
Test Epoch5 layer4 Acc 0.8875, AUC 0.9514025449752808, avg_entr 0.035168539732694626
gc 0
Train Epoch6 Acc 0.963325 (38533/40000), AUC 0.9897931218147278
Test Epoch6 layer0 Acc 0.8925, AUC 0.9530472755432129, avg_entr 0.14859294891357422
Test Epoch6 layer1 Acc 0.8825, AUC 0.94158935546875, avg_entr 0.04488556087017059
Test Epoch6 layer2 Acc 0.8827, AUC 0.9472119212150574, avg_entr 0.031280118972063065
Test Epoch6 layer3 Acc 0.8826, AUC 0.948020339012146, avg_entr 0.0320889987051487
Test Epoch6 layer4 Acc 0.8832, AUC 0.9475760459899902, avg_entr 0.0314742811024189
gc 0
Train Epoch7 Acc 0.96645 (38658/40000), AUC 0.9919544458389282
Test Epoch7 layer0 Acc 0.8905, AUC 0.9512308239936829, avg_entr 0.13948647677898407
Test Epoch7 layer1 Acc 0.8826, AUC 0.9390934705734253, avg_entr 0.03983885794878006
Test Epoch7 layer2 Acc 0.8819, AUC 0.9453413486480713, avg_entr 0.029258044436573982
Test Epoch7 layer3 Acc 0.8818, AUC 0.9468480348587036, avg_entr 0.029887957498431206
Test Epoch7 layer4 Acc 0.8819, AUC 0.9464161396026611, avg_entr 0.029147610068321228
gc 0
Train Epoch8 Acc 0.97135 (38854/40000), AUC 0.9929164052009583
Test Epoch8 layer0 Acc 0.8871, AUC 0.9502143263816833, avg_entr 0.13579584658145905
Test Epoch8 layer1 Acc 0.8801, AUC 0.9345673322677612, avg_entr 0.038643915206193924
Test Epoch8 layer2 Acc 0.8812, AUC 0.9406336545944214, avg_entr 0.02759855054318905
Test Epoch8 layer3 Acc 0.8813, AUC 0.9445185661315918, avg_entr 0.027440877631306648
Test Epoch8 layer4 Acc 0.8807, AUC 0.9445357322692871, avg_entr 0.02671683020889759
gc 0
Train Epoch9 Acc 0.973025 (38921/40000), AUC 0.9936841726303101
Test Epoch9 layer0 Acc 0.8881, AUC 0.9490123987197876, avg_entr 0.1343788504600525
Test Epoch9 layer1 Acc 0.8744, AUC 0.9317232370376587, avg_entr 0.03638651221990585
Test Epoch9 layer2 Acc 0.8758, AUC 0.941001832485199, avg_entr 0.027068182826042175
Test Epoch9 layer3 Acc 0.8754, AUC 0.9430994987487793, avg_entr 0.026989294216036797
Test Epoch9 layer4 Acc 0.8752, AUC 0.9428752660751343, avg_entr 0.02661783993244171
gc 0
Train Epoch10 Acc 0.974525 (38981/40000), AUC 0.9936115741729736
Test Epoch10 layer0 Acc 0.8841, AUC 0.9477400183677673, avg_entr 0.12967294454574585
Test Epoch10 layer1 Acc 0.879, AUC 0.9302352666854858, avg_entr 0.03348534181714058
Test Epoch10 layer2 Acc 0.8793, AUC 0.9400016665458679, avg_entr 0.024104680866003036
Test Epoch10 layer3 Acc 0.8794, AUC 0.9432981014251709, avg_entr 0.02374715358018875
Test Epoch10 layer4 Acc 0.8795, AUC 0.9432892799377441, avg_entr 0.02319972775876522
gc 0
Train Epoch11 Acc 0.975975 (39039/40000), AUC 0.994280219078064
Test Epoch11 layer0 Acc 0.8833, AUC 0.9463012218475342, avg_entr 0.1284528374671936
Test Epoch11 layer1 Acc 0.8726, AUC 0.9263595938682556, avg_entr 0.032376255840063095
Test Epoch11 layer2 Acc 0.8738, AUC 0.936739981174469, avg_entr 0.023554394021630287
Test Epoch11 layer3 Acc 0.8736, AUC 0.9404765963554382, avg_entr 0.023072997108101845
Test Epoch11 layer4 Acc 0.8734, AUC 0.940822958946228, avg_entr 0.022596636787056923
gc 0
Train Epoch12 Acc 0.97715 (39086/40000), AUC 0.9947856068611145
Test Epoch12 layer0 Acc 0.8833, AUC 0.9459129571914673, avg_entr 0.12599702179431915
Test Epoch12 layer1 Acc 0.8762, AUC 0.9267346858978271, avg_entr 0.030265118926763535
Test Epoch12 layer2 Acc 0.8763, AUC 0.9361034631729126, avg_entr 0.021990392357110977
Test Epoch12 layer3 Acc 0.8762, AUC 0.940696656703949, avg_entr 0.021611010655760765
Test Epoch12 layer4 Acc 0.8763, AUC 0.9410408735275269, avg_entr 0.021206440404057503
gc 0
Train Epoch13 Acc 0.977925 (39117/40000), AUC 0.9950172901153564
Test Epoch13 layer0 Acc 0.884, AUC 0.945226788520813, avg_entr 0.12471596896648407
Test Epoch13 layer1 Acc 0.8724, AUC 0.9232900142669678, avg_entr 0.030031653121113777
Test Epoch13 layer2 Acc 0.8726, AUC 0.9347125291824341, avg_entr 0.02198088727891445
Test Epoch13 layer3 Acc 0.8725, AUC 0.939159631729126, avg_entr 0.02133573591709137
Test Epoch13 layer4 Acc 0.8721, AUC 0.9400004744529724, avg_entr 0.021019726991653442
gc 0
Train Epoch14 Acc 0.978525 (39141/40000), AUC 0.9950201511383057
Test Epoch14 layer0 Acc 0.8826, AUC 0.9445481300354004, avg_entr 0.12312451004981995
Test Epoch14 layer1 Acc 0.874, AUC 0.9209659099578857, avg_entr 0.028662292286753654
Test Epoch14 layer2 Acc 0.8749, AUC 0.9305628538131714, avg_entr 0.0204806886613369
Test Epoch14 layer3 Acc 0.8742, AUC 0.9384874105453491, avg_entr 0.019759908318519592
Test Epoch14 layer4 Acc 0.8744, AUC 0.9395286440849304, avg_entr 0.019420241937041283
gc 0
Train Epoch15 Acc 0.9798 (39192/40000), AUC 0.9951835870742798
Test Epoch15 layer0 Acc 0.8809, AUC 0.9439169764518738, avg_entr 0.1215682253241539
Test Epoch15 layer1 Acc 0.8723, AUC 0.9211448431015015, avg_entr 0.0280079897493124
Test Epoch15 layer2 Acc 0.8723, AUC 0.9309561252593994, avg_entr 0.02020256407558918
Test Epoch15 layer3 Acc 0.8723, AUC 0.9374418258666992, avg_entr 0.019681299105286598
Test Epoch15 layer4 Acc 0.8722, AUC 0.9385044574737549, avg_entr 0.0194290429353714
gc 0
Train Epoch16 Acc 0.979925 (39197/40000), AUC 0.9954952001571655
Test Epoch16 layer0 Acc 0.8802, AUC 0.9433901309967041, avg_entr 0.1203974261879921
Test Epoch16 layer1 Acc 0.8693, AUC 0.9193716049194336, avg_entr 0.027621062472462654
Test Epoch16 layer2 Acc 0.869, AUC 0.9303125739097595, avg_entr 0.0196995846927166
Test Epoch16 layer3 Acc 0.8688, AUC 0.9364553689956665, avg_entr 0.0188301932066679
Test Epoch16 layer4 Acc 0.8689, AUC 0.938191831111908, avg_entr 0.01852765865623951
gc 0
Train Epoch17 Acc 0.980175 (39207/40000), AUC 0.9957650899887085
Test Epoch17 layer0 Acc 0.879, AUC 0.9431968927383423, avg_entr 0.11909398436546326
Test Epoch17 layer1 Acc 0.8673, AUC 0.9180283546447754, avg_entr 0.027180852368474007
Test Epoch17 layer2 Acc 0.8678, AUC 0.9306334853172302, avg_entr 0.019708186388015747
Test Epoch17 layer3 Acc 0.8681, AUC 0.9367455244064331, avg_entr 0.01919371448457241
Test Epoch17 layer4 Acc 0.868, AUC 0.9381799697875977, avg_entr 0.019019411876797676
gc 0
Train Epoch18 Acc 0.980475 (39219/40000), AUC 0.9957250356674194
Test Epoch18 layer0 Acc 0.8791, AUC 0.9428634643554688, avg_entr 0.11832068860530853
Test Epoch18 layer1 Acc 0.8709, AUC 0.9170873165130615, avg_entr 0.02639373205602169
Test Epoch18 layer2 Acc 0.8712, AUC 0.9274033308029175, avg_entr 0.01885109394788742
Test Epoch18 layer3 Acc 0.8711, AUC 0.9354645609855652, avg_entr 0.01802903227508068
Test Epoch18 layer4 Acc 0.8709, AUC 0.937396764755249, avg_entr 0.017759395763278008
gc 0
Train Epoch19 Acc 0.9808 (39232/40000), AUC 0.9959749579429626
Test Epoch19 layer0 Acc 0.8769, AUC 0.9424594640731812, avg_entr 0.11701628565788269
Test Epoch19 layer1 Acc 0.8691, AUC 0.9164689183235168, avg_entr 0.02592785283923149
Test Epoch19 layer2 Acc 0.8686, AUC 0.9272633790969849, avg_entr 0.018811458721756935
Test Epoch19 layer3 Acc 0.8684, AUC 0.9348410367965698, avg_entr 0.017939778044819832
Test Epoch19 layer4 Acc 0.8686, AUC 0.937124490737915, avg_entr 0.01768733747303486
gc 0
Train Epoch20 Acc 0.9814 (39256/40000), AUC 0.995948314666748
Test Epoch20 layer0 Acc 0.8789, AUC 0.9423216581344604, avg_entr 0.11726144701242447
Test Epoch20 layer1 Acc 0.8699, AUC 0.9163531064987183, avg_entr 0.025723397731781006
Test Epoch20 layer2 Acc 0.8693, AUC 0.9258453845977783, avg_entr 0.018392493948340416
Test Epoch20 layer3 Acc 0.8693, AUC 0.9341428875923157, avg_entr 0.01747887022793293
Test Epoch20 layer4 Acc 0.8691, AUC 0.9370735287666321, avg_entr 0.01717851497232914
gc 0
Train Epoch21 Acc 0.98155 (39262/40000), AUC 0.9959399700164795
Test Epoch21 layer0 Acc 0.8772, AUC 0.9421629905700684, avg_entr 0.11580636352300644
Test Epoch21 layer1 Acc 0.8701, AUC 0.9156173467636108, avg_entr 0.02536500059068203
Test Epoch21 layer2 Acc 0.8703, AUC 0.9248687028884888, avg_entr 0.01798449084162712
Test Epoch21 layer3 Acc 0.8705, AUC 0.9342011213302612, avg_entr 0.01706627383828163
Test Epoch21 layer4 Acc 0.8704, AUC 0.936739444732666, avg_entr 0.01677246205508709
gc 0
Train Epoch22 Acc 0.9819 (39276/40000), AUC 0.99588942527771
Test Epoch22 layer0 Acc 0.8775, AUC 0.9419984817504883, avg_entr 0.11633434891700745
Test Epoch22 layer1 Acc 0.8697, AUC 0.915573239326477, avg_entr 0.02508733794093132
Test Epoch22 layer2 Acc 0.8697, AUC 0.9247043132781982, avg_entr 0.01763821579515934
Test Epoch22 layer3 Acc 0.8698, AUC 0.9340915679931641, avg_entr 0.016790127381682396
Test Epoch22 layer4 Acc 0.8698, AUC 0.9365767240524292, avg_entr 0.016552604734897614
gc 0
Train Epoch23 Acc 0.9816 (39264/40000), AUC 0.9962708353996277
Test Epoch23 layer0 Acc 0.8773, AUC 0.9418306946754456, avg_entr 0.11499625444412231
Test Epoch23 layer1 Acc 0.8696, AUC 0.9154298305511475, avg_entr 0.0247599296271801
Test Epoch23 layer2 Acc 0.8701, AUC 0.9241772890090942, avg_entr 0.017352603375911713
Test Epoch23 layer3 Acc 0.8697, AUC 0.9338938593864441, avg_entr 0.016565250232815742
Test Epoch23 layer4 Acc 0.8704, AUC 0.9363042116165161, avg_entr 0.01632419228553772
gc 0
Train Epoch24 Acc 0.9818 (39272/40000), AUC 0.9960258603096008
Test Epoch24 layer0 Acc 0.8778, AUC 0.9417694807052612, avg_entr 0.11469200253486633
Test Epoch24 layer1 Acc 0.8687, AUC 0.9150865077972412, avg_entr 0.024528102949261665
Test Epoch24 layer2 Acc 0.8695, AUC 0.9243528842926025, avg_entr 0.01744132861495018
Test Epoch24 layer3 Acc 0.8688, AUC 0.933643639087677, avg_entr 0.016628224402666092
Test Epoch24 layer4 Acc 0.8694, AUC 0.9362109899520874, avg_entr 0.016381144523620605
gc 0
Train Epoch25 Acc 0.981875 (39275/40000), AUC 0.9961262345314026
Test Epoch25 layer0 Acc 0.8772, AUC 0.9417085647583008, avg_entr 0.11405378580093384
Test Epoch25 layer1 Acc 0.8692, AUC 0.9147610664367676, avg_entr 0.024278121069073677
Test Epoch25 layer2 Acc 0.8697, AUC 0.923665463924408, avg_entr 0.017076807096600533
Test Epoch25 layer3 Acc 0.8694, AUC 0.9332542419433594, avg_entr 0.016240844503045082
Test Epoch25 layer4 Acc 0.8697, AUC 0.936069905757904, avg_entr 0.015992607921361923
gc 0
Train Epoch26 Acc 0.981925 (39277/40000), AUC 0.996070146560669
Test Epoch26 layer0 Acc 0.8778, AUC 0.9416289329528809, avg_entr 0.11411216109991074
Test Epoch26 layer1 Acc 0.8691, AUC 0.9146703481674194, avg_entr 0.0241086445748806
Test Epoch26 layer2 Acc 0.8698, AUC 0.9230963587760925, avg_entr 0.017019890248775482
Test Epoch26 layer3 Acc 0.8698, AUC 0.9332284331321716, avg_entr 0.0162054393440485
Test Epoch26 layer4 Acc 0.8698, AUC 0.9358842372894287, avg_entr 0.01597226783633232
gc 0
Train Epoch27 Acc 0.982125 (39285/40000), AUC 0.996270477771759
Test Epoch27 layer0 Acc 0.8775, AUC 0.9415507316589355, avg_entr 0.1134248673915863
Test Epoch27 layer1 Acc 0.8692, AUC 0.914625883102417, avg_entr 0.023889614269137383
Test Epoch27 layer2 Acc 0.8694, AUC 0.9229785799980164, avg_entr 0.016772165894508362
Test Epoch27 layer3 Acc 0.8696, AUC 0.9329478740692139, avg_entr 0.015988336876034737
Test Epoch27 layer4 Acc 0.869, AUC 0.9357641339302063, avg_entr 0.015730582177639008
gc 0
Train Epoch28 Acc 0.982475 (39299/40000), AUC 0.9962468147277832
Test Epoch28 layer0 Acc 0.8765, AUC 0.9415411353111267, avg_entr 0.11256930232048035
Test Epoch28 layer1 Acc 0.8689, AUC 0.9142183661460876, avg_entr 0.023847561329603195
Test Epoch28 layer2 Acc 0.8687, AUC 0.9226698875427246, avg_entr 0.016804561018943787
Test Epoch28 layer3 Acc 0.8686, AUC 0.9326735734939575, avg_entr 0.015949593856930733
Test Epoch28 layer4 Acc 0.8688, AUC 0.9357768893241882, avg_entr 0.015736958011984825
gc 0
Train Epoch29 Acc 0.982275 (39291/40000), AUC 0.9961882829666138
Test Epoch29 layer0 Acc 0.8768, AUC 0.9415050745010376, avg_entr 0.11260705441236496
Test Epoch29 layer1 Acc 0.8688, AUC 0.9142779111862183, avg_entr 0.02359045296907425
Test Epoch29 layer2 Acc 0.8695, AUC 0.9227063655853271, avg_entr 0.01665809378027916
Test Epoch29 layer3 Acc 0.8695, AUC 0.9328873157501221, avg_entr 0.015826143324375153
Test Epoch29 layer4 Acc 0.8695, AUC 0.9356988668441772, avg_entr 0.015563223510980606
Best AUC 0.9571592807769775
train_loss (2, 5, 30)
valid_acc (5, 30)
valid_AUC (5, 30)
train_acc (30,)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Start Testing
Load ckpt at ckpt/imdb_transformeral_l5_pad400//imdb_transformeral_l5.pt
[[4238  725]
 [ 410 4627]]
Figure(640x480)
tensor([0.0092, 0.0726, 0.0406,  ..., 0.2149, 0.0421, 0.0124])
[[4056  907]
 [ 315 4722]]
Figure(640x480)
tensor([0.0003, 0.0136, 0.0012,  ..., 0.0377, 0.0099, 0.0045])
[[4094  869]
 [ 318 4719]]
Figure(640x480)
tensor([0.0013, 0.0073, 0.0013,  ..., 0.0109, 0.0056, 0.0029])
[[4164  799]
 [ 357 4680]]
Figure(640x480)
tensor([0.0061, 0.0092, 0.0064,  ..., 0.0103, 0.0073, 0.0079])
[[4143  820]
 [ 348 4689]]
Figure(640x480)
tensor([0.0034, 0.0066, 0.0037,  ..., 0.0107, 0.0052, 0.0056])
