total count words 102019
vocab size 30000
found 26754 words in glove
model: TransformerModelML(
  (layers): ModuleList(
    (0): EMBLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Embedding(30000, 300)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=4, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=4, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.weight 9000000
layers.0.ae.g.0.weight 512
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 512
layers.0.ae.h.0.bias 4
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
Total Trainable Params: 13672292
Start Training
gc 0
Train Epoch0 Acc 0.640675 (76881/120000), AUC 0.8643728494644165
Test Epoch0 layer0 Acc 0.9119736842105263, AUC 0.9791256189346313, avg_entr 0.2194439023733139
Save ckpt to ckpt/ag_news_transformeral_l5_pad50//ag_news_transformeral_l5.pt  ,ep 0
Test Epoch0 layer1 Acc 0.9097368421052632, AUC 0.9796003103256226, avg_entr 0.15792185068130493
Save ckpt to ckpt/ag_news_transformeral_l5_pad50//ag_news_transformeral_l5.pt  ,ep 0
Test Epoch0 layer2 Acc 0.9111842105263158, AUC 0.9796874523162842, avg_entr 0.1568310707807541
Save ckpt to ckpt/ag_news_transformeral_l5_pad50//ag_news_transformeral_l5.pt  ,ep 0
Test Epoch0 layer3 Acc 0.9086842105263158, AUC 0.9797687530517578, avg_entr 0.1505512148141861
Save ckpt to ckpt/ag_news_transformeral_l5_pad50//ag_news_transformeral_l5.pt  ,ep 0
Test Epoch0 layer4 Acc 0.9088157894736842, AUC 0.9795761108398438, avg_entr 0.14969336986541748
gc 0
Train Epoch1 Acc 0.9270166666666667 (111242/120000), AUC 0.9840490221977234
Test Epoch1 layer0 Acc 0.9169736842105263, AUC 0.9813328981399536, avg_entr 0.13498862087726593
Save ckpt to ckpt/ag_news_transformeral_l5_pad50//ag_news_transformeral_l5.pt  ,ep 1
Test Epoch1 layer1 Acc 0.9178947368421052, AUC 0.9826797246932983, avg_entr 0.0796755999326706
Save ckpt to ckpt/ag_news_transformeral_l5_pad50//ag_news_transformeral_l5.pt  ,ep 1
Test Epoch1 layer2 Acc 0.9176315789473685, AUC 0.9826712608337402, avg_entr 0.07038424909114838
Test Epoch1 layer3 Acc 0.9184210526315789, AUC 0.9827033281326294, avg_entr 0.060733720660209656
Save ckpt to ckpt/ag_news_transformeral_l5_pad50//ag_news_transformeral_l5.pt  ,ep 1
Test Epoch1 layer4 Acc 0.9178947368421052, AUC 0.9825136065483093, avg_entr 0.05610034987330437
gc 0
Train Epoch2 Acc 0.940725 (112887/120000), AUC 0.9885847568511963
Test Epoch2 layer0 Acc 0.916578947368421, AUC 0.981403112411499, avg_entr 0.1067531630396843
Test Epoch2 layer1 Acc 0.9194736842105263, AUC 0.9808093905448914, avg_entr 0.04535019025206566
Test Epoch2 layer2 Acc 0.9189473684210526, AUC 0.9808699488639832, avg_entr 0.03796587139368057
Test Epoch2 layer3 Acc 0.9189473684210526, AUC 0.9811414480209351, avg_entr 0.033316485583782196
Test Epoch2 layer4 Acc 0.9188157894736843, AUC 0.9799824357032776, avg_entr 0.03062787838280201
gc 0
Train Epoch3 Acc 0.9471416666666667 (113657/120000), AUC 0.990157961845398
Test Epoch3 layer0 Acc 0.9173684210526316, AUC 0.9817907810211182, avg_entr 0.09287306666374207
Test Epoch3 layer1 Acc 0.9164473684210527, AUC 0.9810645580291748, avg_entr 0.03475534915924072
Test Epoch3 layer2 Acc 0.916578947368421, AUC 0.9823740720748901, avg_entr 0.028291279450058937
Test Epoch3 layer3 Acc 0.9167105263157894, AUC 0.9830663204193115, avg_entr 0.02514032833278179
Save ckpt to ckpt/ag_news_transformeral_l5_pad50//ag_news_transformeral_l5.pt  ,ep 3
Test Epoch3 layer4 Acc 0.9168421052631579, AUC 0.9826666712760925, avg_entr 0.022963425144553185
gc 0
Train Epoch4 Acc 0.9524416666666666 (114293/120000), AUC 0.9912768602371216
Test Epoch4 layer0 Acc 0.9176315789473685, AUC 0.9813228249549866, avg_entr 0.08129115402698517
Test Epoch4 layer1 Acc 0.9164473684210527, AUC 0.9784854650497437, avg_entr 0.02965533174574375
Test Epoch4 layer2 Acc 0.9157894736842105, AUC 0.9803237915039062, avg_entr 0.024107517674565315
Test Epoch4 layer3 Acc 0.9160526315789473, AUC 0.9812052249908447, avg_entr 0.021613439545035362
Test Epoch4 layer4 Acc 0.9155263157894736, AUC 0.9808595180511475, avg_entr 0.019790930673480034
gc 0
Train Epoch5 Acc 0.955925 (114711/120000), AUC 0.9921426773071289
Test Epoch5 layer0 Acc 0.9180263157894737, AUC 0.9812914133071899, avg_entr 0.07580998539924622
Test Epoch5 layer1 Acc 0.9153947368421053, AUC 0.978104293346405, avg_entr 0.026896964758634567
Test Epoch5 layer2 Acc 0.9147368421052632, AUC 0.9804001450538635, avg_entr 0.021626736968755722
Test Epoch5 layer3 Acc 0.9144736842105263, AUC 0.9813945293426514, avg_entr 0.01904936693608761
Test Epoch5 layer4 Acc 0.9148684210526316, AUC 0.9802289009094238, avg_entr 0.017531732097268105
gc 0
Train Epoch6 Acc 0.959075 (115089/120000), AUC 0.9929793477058411
Test Epoch6 layer0 Acc 0.9144736842105263, AUC 0.9808648228645325, avg_entr 0.07140498608350754
Test Epoch6 layer1 Acc 0.9143421052631578, AUC 0.9771226644515991, avg_entr 0.02442600205540657
Test Epoch6 layer2 Acc 0.9140789473684211, AUC 0.9791785478591919, avg_entr 0.018297666683793068
Test Epoch6 layer3 Acc 0.9136842105263158, AUC 0.9799410104751587, avg_entr 0.01666332595050335
Test Epoch6 layer4 Acc 0.9131578947368421, AUC 0.9791964292526245, avg_entr 0.015445285476744175
gc 0
Train Epoch7 Acc 0.9611583333333333 (115339/120000), AUC 0.9934539794921875
Test Epoch7 layer0 Acc 0.9139473684210526, AUC 0.9806962013244629, avg_entr 0.06565199792385101
Test Epoch7 layer1 Acc 0.9127631578947368, AUC 0.976799488067627, avg_entr 0.02263289876282215
Test Epoch7 layer2 Acc 0.9123684210526316, AUC 0.9787560105323792, avg_entr 0.01715674437582493
Test Epoch7 layer3 Acc 0.9126315789473685, AUC 0.9800819158554077, avg_entr 0.015290005132555962
Test Epoch7 layer4 Acc 0.9126315789473685, AUC 0.9802289605140686, avg_entr 0.014072196558117867
gc 0
Train Epoch8 Acc 0.9640833333333333 (115690/120000), AUC 0.9947099089622498
Test Epoch8 layer0 Acc 0.9144736842105263, AUC 0.9806309938430786, avg_entr 0.06282839924097061
Test Epoch8 layer1 Acc 0.9123684210526316, AUC 0.9748602509498596, avg_entr 0.02200111374258995
Test Epoch8 layer2 Acc 0.9117105263157895, AUC 0.976445198059082, avg_entr 0.01601976342499256
Test Epoch8 layer3 Acc 0.9117105263157895, AUC 0.9775636196136475, avg_entr 0.013926927000284195
Test Epoch8 layer4 Acc 0.9113157894736842, AUC 0.978632926940918, avg_entr 0.01280147023499012
gc 0
Train Epoch9 Acc 0.965725 (115887/120000), AUC 0.9948604106903076
Test Epoch9 layer0 Acc 0.9148684210526316, AUC 0.9803752899169922, avg_entr 0.061354562640190125
Test Epoch9 layer1 Acc 0.9130263157894737, AUC 0.975807785987854, avg_entr 0.02149210125207901
Test Epoch9 layer2 Acc 0.9121052631578948, AUC 0.976130485534668, avg_entr 0.015278594568371773
Test Epoch9 layer3 Acc 0.9119736842105263, AUC 0.9776260852813721, avg_entr 0.013303226791322231
Test Epoch9 layer4 Acc 0.9117105263157895, AUC 0.9773669242858887, avg_entr 0.012072945013642311
gc 0
Train Epoch10 Acc 0.9669 (116028/120000), AUC 0.9951779842376709
Test Epoch10 layer0 Acc 0.9144736842105263, AUC 0.9804999828338623, avg_entr 0.057964958250522614
Test Epoch10 layer1 Acc 0.9123684210526316, AUC 0.9751697182655334, avg_entr 0.02078917808830738
Test Epoch10 layer2 Acc 0.9110526315789473, AUC 0.976248025894165, avg_entr 0.014605238102376461
Test Epoch10 layer3 Acc 0.9117105263157895, AUC 0.9767647385597229, avg_entr 0.01276735682040453
Test Epoch10 layer4 Acc 0.9119736842105263, AUC 0.9763203859329224, avg_entr 0.011556516401469707
gc 0
Train Epoch11 Acc 0.9673 (116076/120000), AUC 0.9954056739807129
Test Epoch11 layer0 Acc 0.9123684210526316, AUC 0.9802732467651367, avg_entr 0.055856622755527496
Test Epoch11 layer1 Acc 0.9113157894736842, AUC 0.9748870730400085, avg_entr 0.020205052569508553
Test Epoch11 layer2 Acc 0.9117105263157895, AUC 0.9758305549621582, avg_entr 0.013782390393316746
Test Epoch11 layer3 Acc 0.9119736842105263, AUC 0.9769733548164368, avg_entr 0.012132786214351654
Test Epoch11 layer4 Acc 0.9114473684210527, AUC 0.9759787321090698, avg_entr 0.01107445266097784
gc 0
Train Epoch12 Acc 0.9695916666666666 (116351/120000), AUC 0.995885968208313
Test Epoch12 layer0 Acc 0.9130263157894737, AUC 0.9801895618438721, avg_entr 0.053720105439424515
Test Epoch12 layer1 Acc 0.910921052631579, AUC 0.9737423658370972, avg_entr 0.019566241651773453
Test Epoch12 layer2 Acc 0.9113157894736842, AUC 0.9751026630401611, avg_entr 0.01344976108521223
Test Epoch12 layer3 Acc 0.9103947368421053, AUC 0.9769287109375, avg_entr 0.011714896187186241
Test Epoch12 layer4 Acc 0.91, AUC 0.9769426584243774, avg_entr 0.010440155863761902
gc 0
Train Epoch13 Acc 0.9703 (116436/120000), AUC 0.9958826303482056
Test Epoch13 layer0 Acc 0.9128947368421053, AUC 0.9799680709838867, avg_entr 0.05262186378240585
Test Epoch13 layer1 Acc 0.910921052631579, AUC 0.9740438461303711, avg_entr 0.018809940665960312
Test Epoch13 layer2 Acc 0.910921052631579, AUC 0.9758755564689636, avg_entr 0.012383973225951195
Test Epoch13 layer3 Acc 0.9111842105263158, AUC 0.9769622087478638, avg_entr 0.010525614023208618
Test Epoch13 layer4 Acc 0.910921052631579, AUC 0.9764130711555481, avg_entr 0.009482852183282375
gc 0
Train Epoch14 Acc 0.9708833333333333 (116506/120000), AUC 0.9960746169090271
Test Epoch14 layer0 Acc 0.9125, AUC 0.9800230860710144, avg_entr 0.051176976412534714
Test Epoch14 layer1 Acc 0.9093421052631578, AUC 0.9741421937942505, avg_entr 0.01894316077232361
Test Epoch14 layer2 Acc 0.9086842105263158, AUC 0.9752565622329712, avg_entr 0.013129270635545254
Test Epoch14 layer3 Acc 0.9086842105263158, AUC 0.9757102727890015, avg_entr 0.011281969957053661
Test Epoch14 layer4 Acc 0.9086842105263158, AUC 0.975790798664093, avg_entr 0.010178490541875362
gc 0
Train Epoch15 Acc 0.9710833333333333 (116530/120000), AUC 0.996059775352478
Test Epoch15 layer0 Acc 0.9118421052631579, AUC 0.9799173474311829, avg_entr 0.049211472272872925
Test Epoch15 layer1 Acc 0.9090789473684211, AUC 0.9735062122344971, avg_entr 0.018299486488103867
Test Epoch15 layer2 Acc 0.9089473684210526, AUC 0.9747234582901001, avg_entr 0.0124497776851058
Test Epoch15 layer3 Acc 0.9085526315789474, AUC 0.9753881692886353, avg_entr 0.010593143291771412
Test Epoch15 layer4 Acc 0.9084210526315789, AUC 0.9752258062362671, avg_entr 0.009334164671599865
gc 0
Train Epoch16 Acc 0.9721416666666667 (116657/120000), AUC 0.996374249458313
Test Epoch16 layer0 Acc 0.9123684210526316, AUC 0.979935884475708, avg_entr 0.04809816926717758
Test Epoch16 layer1 Acc 0.9094736842105263, AUC 0.9727790355682373, avg_entr 0.017784684896469116
Test Epoch16 layer2 Acc 0.9084210526315789, AUC 0.973418653011322, avg_entr 0.011509586125612259
Test Epoch16 layer3 Acc 0.9082894736842105, AUC 0.9748677611351013, avg_entr 0.009651707485318184
Test Epoch16 layer4 Acc 0.9082894736842105, AUC 0.9734700918197632, avg_entr 0.008560829795897007
gc 0
Train Epoch17 Acc 0.9724916666666666 (116699/120000), AUC 0.9964367747306824
Test Epoch17 layer0 Acc 0.9119736842105263, AUC 0.9798522591590881, avg_entr 0.04687180742621422
Test Epoch17 layer1 Acc 0.9090789473684211, AUC 0.9723277688026428, avg_entr 0.01775284856557846
Test Epoch17 layer2 Acc 0.9089473684210526, AUC 0.9728425741195679, avg_entr 0.01234553661197424
Test Epoch17 layer3 Acc 0.9084210526315789, AUC 0.9743196368217468, avg_entr 0.01052430272102356
Test Epoch17 layer4 Acc 0.9086842105263158, AUC 0.9728276133537292, avg_entr 0.009511073119938374
gc 0
Train Epoch18 Acc 0.97285 (116742/120000), AUC 0.9963814616203308
Test Epoch18 layer0 Acc 0.9122368421052631, AUC 0.9798480868339539, avg_entr 0.04552740976214409
Test Epoch18 layer1 Acc 0.9085526315789474, AUC 0.9720792174339294, avg_entr 0.01761305332183838
Test Epoch18 layer2 Acc 0.9075, AUC 0.9721787571907043, avg_entr 0.012575161643326283
Test Epoch18 layer3 Acc 0.9072368421052631, AUC 0.973856508731842, avg_entr 0.010840573348104954
Test Epoch18 layer4 Acc 0.9078947368421053, AUC 0.9716918468475342, avg_entr 0.009923101402819157
gc 0
Train Epoch19 Acc 0.973075 (116769/120000), AUC 0.9963725209236145
Test Epoch19 layer0 Acc 0.9111842105263158, AUC 0.9797990322113037, avg_entr 0.044765230268239975
Test Epoch19 layer1 Acc 0.9082894736842105, AUC 0.9726080894470215, avg_entr 0.016911836341023445
Test Epoch19 layer2 Acc 0.9077631578947368, AUC 0.9733397364616394, avg_entr 0.01158889476209879
Test Epoch19 layer3 Acc 0.9064473684210527, AUC 0.9738034009933472, avg_entr 0.00991485733538866
Test Epoch19 layer4 Acc 0.9056578947368421, AUC 0.9722540378570557, avg_entr 0.009102990850806236
gc 0
Train Epoch20 Acc 0.9733666666666667 (116804/120000), AUC 0.9966322183609009
Test Epoch20 layer0 Acc 0.9121052631578948, AUC 0.9798192977905273, avg_entr 0.043523721396923065
Test Epoch20 layer1 Acc 0.9092105263157895, AUC 0.9720174074172974, avg_entr 0.01655551791191101
Test Epoch20 layer2 Acc 0.9085526315789474, AUC 0.9723449945449829, avg_entr 0.011586912907660007
Test Epoch20 layer3 Acc 0.9080263157894737, AUC 0.9731694459915161, avg_entr 0.00994380284100771
Test Epoch20 layer4 Acc 0.9071052631578947, AUC 0.9710378646850586, avg_entr 0.008935397490859032
gc 0
Train Epoch21 Acc 0.9735 (116820/120000), AUC 0.9965258836746216
Test Epoch21 layer0 Acc 0.9107894736842105, AUC 0.979756236076355, avg_entr 0.04300182685256004
Test Epoch21 layer1 Acc 0.9077631578947368, AUC 0.9721161127090454, avg_entr 0.016183268278837204
Test Epoch21 layer2 Acc 0.9067105263157895, AUC 0.9723843336105347, avg_entr 0.01050533913075924
Test Epoch21 layer3 Acc 0.9063157894736842, AUC 0.9730949401855469, avg_entr 0.008776558563113213
Test Epoch21 layer4 Acc 0.9060526315789473, AUC 0.971537172794342, avg_entr 0.007982514798641205
gc 0
Train Epoch22 Acc 0.97375 (116850/120000), AUC 0.996598482131958
Test Epoch22 layer0 Acc 0.911578947368421, AUC 0.9797723293304443, avg_entr 0.04162824898958206
Test Epoch22 layer1 Acc 0.9089473684210526, AUC 0.9719786643981934, avg_entr 0.016103306785225868
Test Epoch22 layer2 Acc 0.9077631578947368, AUC 0.9723367691040039, avg_entr 0.011134996078908443
Test Epoch22 layer3 Acc 0.9075, AUC 0.9728464484214783, avg_entr 0.009484823793172836
Test Epoch22 layer4 Acc 0.9072368421052631, AUC 0.9711402654647827, avg_entr 0.00846339762210846
gc 0
Train Epoch23 Acc 0.9737416666666666 (116849/120000), AUC 0.9966616034507751
Test Epoch23 layer0 Acc 0.9111842105263158, AUC 0.9797523617744446, avg_entr 0.04074229300022125
Test Epoch23 layer1 Acc 0.9088157894736842, AUC 0.9716578722000122, avg_entr 0.015700755640864372
Test Epoch23 layer2 Acc 0.9085526315789474, AUC 0.9712324142456055, avg_entr 0.010808675549924374
Test Epoch23 layer3 Acc 0.9084210526315789, AUC 0.9723478555679321, avg_entr 0.009101846255362034
Test Epoch23 layer4 Acc 0.9082894736842105, AUC 0.9690517783164978, avg_entr 0.0080230962485075
gc 0
Train Epoch24 Acc 0.9737916666666667 (116855/120000), AUC 0.9965689778327942
Test Epoch24 layer0 Acc 0.911578947368421, AUC 0.9797612428665161, avg_entr 0.04066028445959091
Test Epoch24 layer1 Acc 0.9089473684210526, AUC 0.9718900918960571, avg_entr 0.015659786760807037
Test Epoch24 layer2 Acc 0.9078947368421053, AUC 0.9718884825706482, avg_entr 0.010813080705702305
Test Epoch24 layer3 Acc 0.9076315789473685, AUC 0.9723430871963501, avg_entr 0.00915300752967596
Test Epoch24 layer4 Acc 0.9075, AUC 0.9699545502662659, avg_entr 0.008254055865108967
gc 0
Train Epoch25 Acc 0.9743333333333334 (116920/120000), AUC 0.9967341423034668
Test Epoch25 layer0 Acc 0.9110526315789473, AUC 0.9797325134277344, avg_entr 0.04026542976498604
Test Epoch25 layer1 Acc 0.9078947368421053, AUC 0.9719894528388977, avg_entr 0.015585310757160187
Test Epoch25 layer2 Acc 0.9063157894736842, AUC 0.9725618958473206, avg_entr 0.010552847757935524
Test Epoch25 layer3 Acc 0.9061842105263158, AUC 0.9727029800415039, avg_entr 0.008729035966098309
Test Epoch25 layer4 Acc 0.905921052631579, AUC 0.9713263511657715, avg_entr 0.00793376099318266
gc 0
Train Epoch26 Acc 0.9738416666666667 (116861/120000), AUC 0.9966919422149658
Test Epoch26 layer0 Acc 0.9110526315789473, AUC 0.97972571849823, avg_entr 0.04000971093773842
Test Epoch26 layer1 Acc 0.9082894736842105, AUC 0.9716625213623047, avg_entr 0.01541844755411148
Test Epoch26 layer2 Acc 0.9075, AUC 0.9716123938560486, avg_entr 0.01068190112709999
Test Epoch26 layer3 Acc 0.9069736842105263, AUC 0.9719955325126648, avg_entr 0.009037948213517666
Test Epoch26 layer4 Acc 0.9069736842105263, AUC 0.969525933265686, avg_entr 0.008130735717713833
gc 0
Train Epoch27 Acc 0.9743666666666667 (116924/120000), AUC 0.9967339038848877
Test Epoch27 layer0 Acc 0.9113157894736842, AUC 0.979730486869812, avg_entr 0.039626672863960266
Test Epoch27 layer1 Acc 0.9084210526315789, AUC 0.9718927145004272, avg_entr 0.015405936166644096
Test Epoch27 layer2 Acc 0.9067105263157895, AUC 0.971708357334137, avg_entr 0.010671637021005154
Test Epoch27 layer3 Acc 0.9063157894736842, AUC 0.9721047878265381, avg_entr 0.008977501653134823
Test Epoch27 layer4 Acc 0.9063157894736842, AUC 0.9696598649024963, avg_entr 0.00813822541385889
gc 0
Train Epoch28 Acc 0.974725 (116967/120000), AUC 0.9966868162155151
Test Epoch28 layer0 Acc 0.9110526315789473, AUC 0.979712963104248, avg_entr 0.03954668715596199
Test Epoch28 layer1 Acc 0.9078947368421053, AUC 0.9718223810195923, avg_entr 0.015192902646958828
Test Epoch28 layer2 Acc 0.906578947368421, AUC 0.9716730117797852, avg_entr 0.010209989733994007
Test Epoch28 layer3 Acc 0.9063157894736842, AUC 0.9718042612075806, avg_entr 0.00843233522027731
Test Epoch28 layer4 Acc 0.905921052631579, AUC 0.9697883129119873, avg_entr 0.007649815175682306
gc 0
Train Epoch29 Acc 0.9744666666666667 (116936/120000), AUC 0.9967359900474548
Test Epoch29 layer0 Acc 0.910921052631579, AUC 0.9797126650810242, avg_entr 0.03928671032190323
Test Epoch29 layer1 Acc 0.9084210526315789, AUC 0.9717526435852051, avg_entr 0.015100259333848953
Test Epoch29 layer2 Acc 0.9080263157894737, AUC 0.9716945290565491, avg_entr 0.010515793226659298
Test Epoch29 layer3 Acc 0.9073684210526316, AUC 0.9721879959106445, avg_entr 0.008933297358453274
Test Epoch29 layer4 Acc 0.9073684210526316, AUC 0.9697190523147583, avg_entr 0.008040137588977814
Best AUC 0.9830663204193115
train_loss (2, 5, 30)
valid_acc (5, 30)
valid_AUC (5, 30)
train_acc (30,)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Start Testing
Load ckpt at ckpt/ag_news_transformeral_l5_pad50//ag_news_transformeral_l5.pt
[[1709   59   89   43]
 [  14 1869   10    7]
 [  43   23 1695  139]
 [  49   18  134 1699]]
Figure(640x480)
tensor([0.0987, 0.0023, 0.0399,  ..., 0.0922, 0.0032, 0.0306])
[[1716   53   83   48]
 [  17 1865   11    7]
 [  47   18 1697  138]
 [  46   14  153 1687]]
Figure(640x480)
tensor([0.0109, 0.0018, 0.0022,  ..., 0.0049, 0.0018, 0.0023])
[[1716   53   81   50]
 [  17 1863   12    8]
 [  45   17 1701  137]
 [  45   13  156 1686]]
Figure(640x480)
tensor([0.0060, 0.0024, 0.0027,  ..., 0.0036, 0.0022, 0.0025])
[[1713   53   83   51]
 [  16 1864   12    8]
 [  43   17 1704  136]
 [  42   13  159 1686]]
Figure(640x480)
tensor([0.0048, 0.0024, 0.0028,  ..., 0.0034, 0.0022, 0.0024])
[[1712   53   84   51]
 [  14 1866   13    7]
 [  42   17 1706  135]
 [  40   14  162 1684]]
Figure(640x480)
tensor([0.0034, 0.0025, 0.0028,  ..., 0.0029, 0.0023, 0.0024])
