total count words 102019
vocab size 30000
found 26754 words in glove
model: TransformerModelML(
  (layers): ModuleList(
    (0): EMBLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Embedding(30000, 300)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=4, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=4, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.weight 9000000
layers.0.ae.g.0.weight 512
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 512
layers.0.ae.h.0.bias 4
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
Total Trainable Params: 13672292
Start Training
gc 0
Train Epoch0 Acc 0.6454666666666666 (77456/120000), AUC 0.8702930212020874
Test Epoch0 layer0 Acc 0.8994736842105263, AUC 0.9758947491645813, avg_entr 0.2442847043275833
Save ckpt to ckpt/ag_news_transformeral_l5_pad150//ag_news_transformeral_l5.pt  ,ep 0
Test Epoch0 layer1 Acc 0.9046052631578947, AUC 0.9777875542640686, avg_entr 0.1661413460969925
Save ckpt to ckpt/ag_news_transformeral_l5_pad150//ag_news_transformeral_l5.pt  ,ep 0
Test Epoch0 layer2 Acc 0.9044736842105263, AUC 0.9778593182563782, avg_entr 0.15745455026626587
Save ckpt to ckpt/ag_news_transformeral_l5_pad150//ag_news_transformeral_l5.pt  ,ep 0
Test Epoch0 layer3 Acc 0.9032894736842105, AUC 0.9778624773025513, avg_entr 0.15201090276241302
Save ckpt to ckpt/ag_news_transformeral_l5_pad150//ag_news_transformeral_l5.pt  ,ep 0
Test Epoch0 layer4 Acc 0.9039473684210526, AUC 0.9777421951293945, avg_entr 0.14919757843017578
gc 0
Train Epoch1 Acc 0.9209666666666667 (110516/120000), AUC 0.9814696907997131
Test Epoch1 layer0 Acc 0.9121052631578948, AUC 0.9794659614562988, avg_entr 0.14609600603580475
Save ckpt to ckpt/ag_news_transformeral_l5_pad150//ag_news_transformeral_l5.pt  ,ep 1
Test Epoch1 layer1 Acc 0.9169736842105263, AUC 0.9813069701194763, avg_entr 0.0881156250834465
Save ckpt to ckpt/ag_news_transformeral_l5_pad150//ag_news_transformeral_l5.pt  ,ep 1
Test Epoch1 layer2 Acc 0.9180263157894737, AUC 0.9810640215873718, avg_entr 0.07104582339525223
Test Epoch1 layer3 Acc 0.9181578947368421, AUC 0.9807907342910767, avg_entr 0.06259499490261078
Test Epoch1 layer4 Acc 0.9184210526315789, AUC 0.9810289144515991, avg_entr 0.05998510122299194
gc 0
Train Epoch2 Acc 0.9352333333333334 (112228/120000), AUC 0.9865177869796753
Test Epoch2 layer0 Acc 0.9161842105263158, AUC 0.981099009513855, avg_entr 0.11386464536190033
Test Epoch2 layer1 Acc 0.9194736842105263, AUC 0.9822400212287903, avg_entr 0.04599780961871147
Save ckpt to ckpt/ag_news_transformeral_l5_pad150//ag_news_transformeral_l5.pt  ,ep 2
Test Epoch2 layer2 Acc 0.9190789473684211, AUC 0.9824236631393433, avg_entr 0.03645241633057594
Save ckpt to ckpt/ag_news_transformeral_l5_pad150//ag_news_transformeral_l5.pt  ,ep 2
Test Epoch2 layer3 Acc 0.9182894736842105, AUC 0.9824391603469849, avg_entr 0.03364431858062744
Save ckpt to ckpt/ag_news_transformeral_l5_pad150//ag_news_transformeral_l5.pt  ,ep 2
Test Epoch2 layer4 Acc 0.9181578947368421, AUC 0.9821153879165649, avg_entr 0.03271820396184921
gc 0
Train Epoch3 Acc 0.9432416666666666 (113189/120000), AUC 0.9888327717781067
Test Epoch3 layer0 Acc 0.9178947368421052, AUC 0.9816232919692993, avg_entr 0.09372176229953766
Test Epoch3 layer1 Acc 0.9184210526315789, AUC 0.9803946614265442, avg_entr 0.034916531294584274
Test Epoch3 layer2 Acc 0.9181578947368421, AUC 0.9819075465202332, avg_entr 0.02888106182217598
Test Epoch3 layer3 Acc 0.9177631578947368, AUC 0.9821240901947021, avg_entr 0.026849977672100067
Test Epoch3 layer4 Acc 0.9177631578947368, AUC 0.9822568297386169, avg_entr 0.025800181552767754
gc 0
Train Epoch4 Acc 0.9483333333333334 (113800/120000), AUC 0.9904522895812988
Test Epoch4 layer0 Acc 0.9181578947368421, AUC 0.9816831946372986, avg_entr 0.08117292076349258
Test Epoch4 layer1 Acc 0.9168421052631579, AUC 0.9795541763305664, avg_entr 0.029401995241642
Test Epoch4 layer2 Acc 0.9172368421052631, AUC 0.9808048009872437, avg_entr 0.02468598261475563
Test Epoch4 layer3 Acc 0.9175, AUC 0.9810003042221069, avg_entr 0.023142945021390915
Test Epoch4 layer4 Acc 0.9173684210526316, AUC 0.9816409945487976, avg_entr 0.02153966948390007
gc 0
Train Epoch5 Acc 0.9527083333333334 (114325/120000), AUC 0.9912636876106262
Test Epoch5 layer0 Acc 0.9167105263157894, AUC 0.9818611145019531, avg_entr 0.07455406337976456
Test Epoch5 layer1 Acc 0.9164473684210527, AUC 0.9782572388648987, avg_entr 0.025354299694299698
Test Epoch5 layer2 Acc 0.9147368421052632, AUC 0.9796754717826843, avg_entr 0.020238952711224556
Test Epoch5 layer3 Acc 0.9156578947368421, AUC 0.9793782830238342, avg_entr 0.018695520237088203
Test Epoch5 layer4 Acc 0.9157894736842105, AUC 0.9797835350036621, avg_entr 0.017497675493359566
gc 0
Train Epoch6 Acc 0.95615 (114738/120000), AUC 0.9921767711639404
Test Epoch6 layer0 Acc 0.9177631578947368, AUC 0.9817658066749573, avg_entr 0.06799490004777908
Test Epoch6 layer1 Acc 0.9168421052631579, AUC 0.9775526523590088, avg_entr 0.024307535961270332
Test Epoch6 layer2 Acc 0.9167105263157894, AUC 0.9805713295936584, avg_entr 0.018873369321227074
Test Epoch6 layer3 Acc 0.9164473684210527, AUC 0.9814085364341736, avg_entr 0.017222385853528976
Test Epoch6 layer4 Acc 0.9161842105263158, AUC 0.9816805124282837, avg_entr 0.016049671918153763
gc 0
Train Epoch7 Acc 0.9590333333333333 (115084/120000), AUC 0.9930577874183655
Test Epoch7 layer0 Acc 0.9169736842105263, AUC 0.9815627336502075, avg_entr 0.0645078793168068
Test Epoch7 layer1 Acc 0.9142105263157895, AUC 0.9767675995826721, avg_entr 0.022218264639377594
Test Epoch7 layer2 Acc 0.9135526315789474, AUC 0.978714644908905, avg_entr 0.017181137576699257
Test Epoch7 layer3 Acc 0.9138157894736842, AUC 0.9799419641494751, avg_entr 0.015763916075229645
Test Epoch7 layer4 Acc 0.9140789473684211, AUC 0.9813660383224487, avg_entr 0.014769076369702816
gc 0
Train Epoch8 Acc 0.9609083333333334 (115309/120000), AUC 0.99383544921875
Test Epoch8 layer0 Acc 0.9173684210526316, AUC 0.9814220666885376, avg_entr 0.061069607734680176
Test Epoch8 layer1 Acc 0.9157894736842105, AUC 0.9752227663993835, avg_entr 0.021122891455888748
Test Epoch8 layer2 Acc 0.9148684210526316, AUC 0.9763118028640747, avg_entr 0.01616968773305416
Test Epoch8 layer3 Acc 0.9157894736842105, AUC 0.9767458438873291, avg_entr 0.014913497492671013
Test Epoch8 layer4 Acc 0.9155263157894736, AUC 0.9780368804931641, avg_entr 0.01390896551311016
gc 0
Train Epoch9 Acc 0.9628 (115536/120000), AUC 0.9940061569213867
Test Epoch9 layer0 Acc 0.9125, AUC 0.9811794757843018, avg_entr 0.057752206921577454
Test Epoch9 layer1 Acc 0.9160526315789473, AUC 0.9754085540771484, avg_entr 0.020286807790398598
Test Epoch9 layer2 Acc 0.9160526315789473, AUC 0.9769039750099182, avg_entr 0.01571325771510601
Test Epoch9 layer3 Acc 0.9160526315789473, AUC 0.9781395196914673, avg_entr 0.014658411964774132
Test Epoch9 layer4 Acc 0.9157894736842105, AUC 0.9791368246078491, avg_entr 0.013497267849743366
gc 0
Train Epoch10 Acc 0.9654 (115848/120000), AUC 0.994731605052948
Test Epoch10 layer0 Acc 0.9173684210526316, AUC 0.9811463356018066, avg_entr 0.054700542241334915
Test Epoch10 layer1 Acc 0.9155263157894736, AUC 0.973949670791626, avg_entr 0.019928989931941032
Test Epoch10 layer2 Acc 0.9151315789473684, AUC 0.976127028465271, avg_entr 0.014582518488168716
Test Epoch10 layer3 Acc 0.915, AUC 0.9768413305282593, avg_entr 0.01349679846316576
Test Epoch10 layer4 Acc 0.9147368421052632, AUC 0.9779698252677917, avg_entr 0.012573489919304848
gc 0
Train Epoch11 Acc 0.9667083333333333 (116005/120000), AUC 0.9951736927032471
Test Epoch11 layer0 Acc 0.9163157894736842, AUC 0.9810608625411987, avg_entr 0.052024513483047485
Test Epoch11 layer1 Acc 0.9140789473684211, AUC 0.9729045629501343, avg_entr 0.019032998010516167
Test Epoch11 layer2 Acc 0.9138157894736842, AUC 0.9742367267608643, avg_entr 0.013232780620455742
Test Epoch11 layer3 Acc 0.9135526315789474, AUC 0.9762223958969116, avg_entr 0.01196799986064434
Test Epoch11 layer4 Acc 0.9132894736842105, AUC 0.9766154885292053, avg_entr 0.010957707650959492
gc 0
Train Epoch12 Acc 0.9672833333333334 (116074/120000), AUC 0.9952660202980042
Test Epoch12 layer0 Acc 0.9157894736842105, AUC 0.9809932708740234, avg_entr 0.05038783326745033
Test Epoch12 layer1 Acc 0.9127631578947368, AUC 0.9730726480484009, avg_entr 0.01943834125995636
Test Epoch12 layer2 Acc 0.9106578947368421, AUC 0.9737069606781006, avg_entr 0.01522858813405037
Test Epoch12 layer3 Acc 0.91, AUC 0.9742019176483154, avg_entr 0.013974047265946865
Test Epoch12 layer4 Acc 0.91, AUC 0.9748767614364624, avg_entr 0.012902592308819294
gc 0
Train Epoch13 Acc 0.96805 (116166/120000), AUC 0.9953750371932983
Test Epoch13 layer0 Acc 0.9177631578947368, AUC 0.9809207916259766, avg_entr 0.04950941354036331
Test Epoch13 layer1 Acc 0.9122368421052631, AUC 0.9732129573822021, avg_entr 0.018911808729171753
Test Epoch13 layer2 Acc 0.9113157894736842, AUC 0.9746177792549133, avg_entr 0.014102126471698284
Test Epoch13 layer3 Acc 0.9123684210526316, AUC 0.9759548306465149, avg_entr 0.013084949925541878
Test Epoch13 layer4 Acc 0.9119736842105263, AUC 0.9755695462226868, avg_entr 0.011896008625626564
gc 0
Train Epoch14 Acc 0.969225 (116307/120000), AUC 0.9958038330078125
Test Epoch14 layer0 Acc 0.9164473684210527, AUC 0.9808444976806641, avg_entr 0.04738636687397957
Test Epoch14 layer1 Acc 0.9122368421052631, AUC 0.9717378616333008, avg_entr 0.017839845269918442
Test Epoch14 layer2 Acc 0.9127631578947368, AUC 0.972385048866272, avg_entr 0.012680929154157639
Test Epoch14 layer3 Acc 0.9125, AUC 0.9734442234039307, avg_entr 0.011457863263785839
Test Epoch14 layer4 Acc 0.9123684210526316, AUC 0.9728520512580872, avg_entr 0.010350820608437061
gc 0
Train Epoch15 Acc 0.970325 (116439/120000), AUC 0.995800256729126
Test Epoch15 layer0 Acc 0.9167105263157894, AUC 0.9807621240615845, avg_entr 0.04618693143129349
Test Epoch15 layer1 Acc 0.9119736842105263, AUC 0.9720443487167358, avg_entr 0.017664114013314247
Test Epoch15 layer2 Acc 0.9121052631578948, AUC 0.971258819103241, avg_entr 0.012602102942764759
Test Epoch15 layer3 Acc 0.9117105263157895, AUC 0.9714305400848389, avg_entr 0.01119784265756607
Test Epoch15 layer4 Acc 0.9118421052631579, AUC 0.9718046188354492, avg_entr 0.010137535631656647
gc 0
Train Epoch16 Acc 0.9700416666666667 (116405/120000), AUC 0.9959424734115601
Test Epoch16 layer0 Acc 0.9168421052631579, AUC 0.9807853698730469, avg_entr 0.0449894554913044
Test Epoch16 layer1 Acc 0.9121052631578948, AUC 0.9721013903617859, avg_entr 0.01722094416618347
Test Epoch16 layer2 Acc 0.911578947368421, AUC 0.9724547863006592, avg_entr 0.01253552082926035
Test Epoch16 layer3 Acc 0.9111842105263158, AUC 0.9735444784164429, avg_entr 0.011525988578796387
Test Epoch16 layer4 Acc 0.9113157894736842, AUC 0.9726492762565613, avg_entr 0.010517904534935951
gc 0
Train Epoch17 Acc 0.9704416666666666 (116453/120000), AUC 0.996015191078186
Test Epoch17 layer0 Acc 0.916578947368421, AUC 0.9807597994804382, avg_entr 0.043355558067560196
Test Epoch17 layer1 Acc 0.9117105263157895, AUC 0.9713873267173767, avg_entr 0.01715528406202793
Test Epoch17 layer2 Acc 0.9107894736842105, AUC 0.9701617360115051, avg_entr 0.01280195452272892
Test Epoch17 layer3 Acc 0.9111842105263158, AUC 0.9705444574356079, avg_entr 0.011383970268070698
Test Epoch17 layer4 Acc 0.9114473684210527, AUC 0.9702902436256409, avg_entr 0.010387455113232136
gc 0
Train Epoch18 Acc 0.9714666666666667 (116576/120000), AUC 0.9962551593780518
Test Epoch18 layer0 Acc 0.9147368421052632, AUC 0.9807153940200806, avg_entr 0.042602039873600006
Test Epoch18 layer1 Acc 0.9117105263157895, AUC 0.9709386825561523, avg_entr 0.01666996069252491
Test Epoch18 layer2 Acc 0.9107894736842105, AUC 0.9701579809188843, avg_entr 0.011889378540217876
Test Epoch18 layer3 Acc 0.9105263157894737, AUC 0.9717932939529419, avg_entr 0.010597947053611279
Test Epoch18 layer4 Acc 0.9102631578947369, AUC 0.9711295366287231, avg_entr 0.009776578284800053
gc 0
Train Epoch19 Acc 0.971875 (116625/120000), AUC 0.9961591958999634
Test Epoch19 layer0 Acc 0.9155263157894736, AUC 0.9807373881340027, avg_entr 0.041821595281362534
Test Epoch19 layer1 Acc 0.9122368421052631, AUC 0.9712921380996704, avg_entr 0.016141047701239586
Test Epoch19 layer2 Acc 0.9105263157894737, AUC 0.9707050919532776, avg_entr 0.012135915458202362
Test Epoch19 layer3 Acc 0.9098684210526315, AUC 0.9707561731338501, avg_entr 0.010731827467679977
Test Epoch19 layer4 Acc 0.91, AUC 0.9695912003517151, avg_entr 0.009906132705509663
gc 0
Train Epoch20 Acc 0.9718333333333333 (116620/120000), AUC 0.9961229562759399
Test Epoch20 layer0 Acc 0.9153947368421053, AUC 0.9807054996490479, avg_entr 0.04049840569496155
Test Epoch20 layer1 Acc 0.9113157894736842, AUC 0.9711695909500122, avg_entr 0.015926411375403404
Test Epoch20 layer2 Acc 0.9106578947368421, AUC 0.970673680305481, avg_entr 0.011815420351922512
Test Epoch20 layer3 Acc 0.9101315789473684, AUC 0.9705277681350708, avg_entr 0.010401600040495396
Test Epoch20 layer4 Acc 0.9101315789473684, AUC 0.9694125652313232, avg_entr 0.009553720243275166
gc 0
Train Epoch21 Acc 0.9720083333333334 (116641/120000), AUC 0.9961479306221008
Test Epoch21 layer0 Acc 0.9152631578947369, AUC 0.9806784987449646, avg_entr 0.03980794921517372
Test Epoch21 layer1 Acc 0.9110526315789473, AUC 0.9710632562637329, avg_entr 0.015761779621243477
Test Epoch21 layer2 Acc 0.9094736842105263, AUC 0.9705144166946411, avg_entr 0.011739548295736313
Test Epoch21 layer3 Acc 0.9097368421052632, AUC 0.9707897901535034, avg_entr 0.010405565612018108
Test Epoch21 layer4 Acc 0.9092105263157895, AUC 0.9704009294509888, avg_entr 0.009611454792320728
gc 0
Train Epoch22 Acc 0.972575 (116709/120000), AUC 0.9962422847747803
Test Epoch22 layer0 Acc 0.9151315789473684, AUC 0.9806852340698242, avg_entr 0.039207976311445236
Test Epoch22 layer1 Acc 0.9114473684210527, AUC 0.9712562561035156, avg_entr 0.015541015192866325
Test Epoch22 layer2 Acc 0.9102631578947369, AUC 0.9715197682380676, avg_entr 0.011681633070111275
Test Epoch22 layer3 Acc 0.9096052631578947, AUC 0.9720168709754944, avg_entr 0.010401579551398754
Test Epoch22 layer4 Acc 0.9096052631578947, AUC 0.9716365337371826, avg_entr 0.009655380621552467
gc 0
Train Epoch23 Acc 0.9724833333333334 (116698/120000), AUC 0.9962377548217773
Test Epoch23 layer0 Acc 0.9155263157894736, AUC 0.9806726574897766, avg_entr 0.03895842656493187
Test Epoch23 layer1 Acc 0.9114473684210527, AUC 0.9710501432418823, avg_entr 0.015201756730675697
Test Epoch23 layer2 Acc 0.9113157894736842, AUC 0.9705374836921692, avg_entr 0.01136858481913805
Test Epoch23 layer3 Acc 0.910921052631579, AUC 0.9701253771781921, avg_entr 0.010016519576311111
Test Epoch23 layer4 Acc 0.9105263157894737, AUC 0.9696404337882996, avg_entr 0.009196317754685879
gc 0
Train Epoch24 Acc 0.9725166666666667 (116702/120000), AUC 0.9964292049407959
Test Epoch24 layer0 Acc 0.9140789473684211, AUC 0.9806554317474365, avg_entr 0.03872225806117058
Test Epoch24 layer1 Acc 0.911578947368421, AUC 0.9708152413368225, avg_entr 0.015306689776480198
Test Epoch24 layer2 Acc 0.9098684210526315, AUC 0.9703803062438965, avg_entr 0.01131537277251482
Test Epoch24 layer3 Acc 0.9098684210526315, AUC 0.9703662991523743, avg_entr 0.009878227487206459
Test Epoch24 layer4 Acc 0.9097368421052632, AUC 0.9695218801498413, avg_entr 0.009056749753654003
gc 0
Train Epoch25 Acc 0.9728083333333334 (116737/120000), AUC 0.9962664246559143
Test Epoch25 layer0 Acc 0.915, AUC 0.9806614518165588, avg_entr 0.038409020751714706
Test Epoch25 layer1 Acc 0.9119736842105263, AUC 0.9710274338722229, avg_entr 0.014640922658145428
Test Epoch25 layer2 Acc 0.9101315789473684, AUC 0.9708077907562256, avg_entr 0.010557979345321655
Test Epoch25 layer3 Acc 0.9101315789473684, AUC 0.9707351922988892, avg_entr 0.009277774021029472
Test Epoch25 layer4 Acc 0.9097368421052632, AUC 0.9699945449829102, avg_entr 0.008503627963364124
gc 0
Train Epoch26 Acc 0.9729583333333334 (116755/120000), AUC 0.9963725805282593
Test Epoch26 layer0 Acc 0.9144736842105263, AUC 0.980644941329956, avg_entr 0.038133006542921066
Test Epoch26 layer1 Acc 0.910921052631579, AUC 0.9708997011184692, avg_entr 0.014750845730304718
Test Epoch26 layer2 Acc 0.9107894736842105, AUC 0.9705126285552979, avg_entr 0.010913124307990074
Test Epoch26 layer3 Acc 0.9105263157894737, AUC 0.9701857566833496, avg_entr 0.009625998325645924
Test Epoch26 layer4 Acc 0.9103947368421053, AUC 0.9697250127792358, avg_entr 0.008885844610631466
gc 0
Train Epoch27 Acc 0.9732416666666667 (116789/120000), AUC 0.9963831305503845
Test Epoch27 layer0 Acc 0.9144736842105263, AUC 0.9806371927261353, avg_entr 0.03810657188296318
Test Epoch27 layer1 Acc 0.9111842105263158, AUC 0.9707612991333008, avg_entr 0.014697251841425896
Test Epoch27 layer2 Acc 0.9098684210526315, AUC 0.9700393676757812, avg_entr 0.010732561349868774
Test Epoch27 layer3 Acc 0.9096052631578947, AUC 0.9691002368927002, avg_entr 0.00943117868155241
Test Epoch27 layer4 Acc 0.9096052631578947, AUC 0.9682014584541321, avg_entr 0.008615763857960701
gc 0
Train Epoch28 Acc 0.9729583333333334 (116755/120000), AUC 0.9964074492454529
Test Epoch28 layer0 Acc 0.9142105263157895, AUC 0.9806360006332397, avg_entr 0.03800065442919731
Test Epoch28 layer1 Acc 0.9111842105263158, AUC 0.9707127809524536, avg_entr 0.01462393905967474
Test Epoch28 layer2 Acc 0.9114473684210527, AUC 0.9703889489173889, avg_entr 0.010893442668020725
Test Epoch28 layer3 Acc 0.9103947368421053, AUC 0.9698150157928467, avg_entr 0.009639831259846687
Test Epoch28 layer4 Acc 0.9103947368421053, AUC 0.9692239761352539, avg_entr 0.008925256319344044
gc 0
Train Epoch29 Acc 0.9731416666666667 (116777/120000), AUC 0.9964691996574402
Test Epoch29 layer0 Acc 0.9151315789473684, AUC 0.9806377291679382, avg_entr 0.03769401088356972
Test Epoch29 layer1 Acc 0.9113157894736842, AUC 0.9708265066146851, avg_entr 0.014584112912416458
Test Epoch29 layer2 Acc 0.9110526315789473, AUC 0.9704971313476562, avg_entr 0.010854429565370083
Test Epoch29 layer3 Acc 0.9105263157894737, AUC 0.9700433015823364, avg_entr 0.009704938158392906
Test Epoch29 layer4 Acc 0.9103947368421053, AUC 0.969423770904541, avg_entr 0.009022829122841358
Best AUC 0.9824391603469849
train_loss (2, 5, 30)
valid_acc (5, 30)
valid_AUC (5, 30)
train_acc (30,)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Start Testing
Load ckpt at ckpt/ag_news_transformeral_l5_pad150//ag_news_transformeral_l5.pt
[[1694   61   98   47]
 [   9 1871   11    9]
 [  42   12 1705  141]
 [  51   15  141 1693]]
Figure(640x480)
tensor([0.0561, 0.0027, 0.0729,  ..., 0.1081, 0.0116, 0.3436])
[[1700   60   80   60]
 [   6 1877    8    9]
 [  42   17 1678  163]
 [  40   14  113 1733]]
Figure(640x480)
tensor([0.0033, 0.0038, 0.0066,  ..., 0.0082, 0.0029, 0.0282])
[[1702   59   77   62]
 [   7 1873    8   12]
 [  44   18 1679  159]
 [  36   13  120 1731]]
Figure(640x480)
tensor([0.0041, 0.0059, 0.0079,  ..., 0.0079, 0.0039, 0.0065])
[[1698   59   79   64]
 [   8 1872    8   12]
 [  44   17 1678  161]
 [  35   13  121 1731]]
Figure(640x480)
tensor([0.0042, 0.0062, 0.0064,  ..., 0.0067, 0.0039, 0.0053])
[[1690   59   83   68]
 [   7 1872    9   12]
 [  40   17 1684  159]
 [  32   12  124 1732]]
Figure(640x480)
tensor([0.0039, 0.0059, 0.0070,  ..., 0.0075, 0.0037, 0.0051])
