total count words 102019
vocab size 30000
found 26754 words in glove
model: LSTMModelML(
  (layers): ModuleList(
    (0): EMBLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Embedding(30000, 300)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=4, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=4, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): LSTMLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): LSTM(300, 300, batch_first=True, bidirectional=True)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): LSTMLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): LSTM(600, 300, batch_first=True, bidirectional=True)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): LSTMLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): LSTM(600, 300, batch_first=True, bidirectional=True)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): LSTMLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): LSTM(600, 300, batch_first=True, bidirectional=True)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.weight 9000000
layers.0.ae.g.0.weight 512
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 512
layers.0.ae.h.0.bias 4
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.weight_ih_l0 360000
layers.1.enc.f.weight_hh_l0 360000
layers.1.enc.f.bias_ih_l0 1200
layers.1.enc.f.bias_hh_l0 1200
layers.1.enc.f.weight_ih_l0_reverse 360000
layers.1.enc.f.weight_hh_l0_reverse 360000
layers.1.enc.f.bias_ih_l0_reverse 1200
layers.1.enc.f.bias_hh_l0_reverse 1200
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.weight_ih_l0 720000
layers.2.enc.f.weight_hh_l0 360000
layers.2.enc.f.bias_ih_l0 1200
layers.2.enc.f.bias_hh_l0 1200
layers.2.enc.f.weight_ih_l0_reverse 720000
layers.2.enc.f.weight_hh_l0_reverse 360000
layers.2.enc.f.bias_ih_l0_reverse 1200
layers.2.enc.f.bias_hh_l0_reverse 1200
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.weight_ih_l0 720000
layers.3.enc.f.weight_hh_l0 360000
layers.3.enc.f.bias_ih_l0 1200
layers.3.enc.f.bias_hh_l0 1200
layers.3.enc.f.weight_ih_l0_reverse 720000
layers.3.enc.f.weight_hh_l0_reverse 360000
layers.3.enc.f.bias_ih_l0_reverse 1200
layers.3.enc.f.bias_hh_l0_reverse 1200
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.weight_ih_l0 720000
layers.4.enc.f.weight_hh_l0 360000
layers.4.enc.f.bias_ih_l0 1200
layers.4.enc.f.bias_hh_l0 1200
layers.4.enc.f.weight_ih_l0_reverse 720000
layers.4.enc.f.weight_hh_l0_reverse 360000
layers.4.enc.f.bias_ih_l0_reverse 1200
layers.4.enc.f.bias_hh_l0_reverse 1200
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
Total Trainable Params: 17265092
Start Training
train_mask {0}
gc 0
Train Epoch0 Acc 0.2501083333333333 (30013/120000), AUC 0.5431612133979797
ep0_train_time 22.74162197113037
Test Epoch0 threshold 0.9 Acc 0.9087171052631579, AUC 0.9765035510063171, avg_entr 0.16305112838745117
ep0_t0.9_test_time 0.14026737213134766
Save ckpt to ckpt/ag_news_lstmal_l5adf_pad75_t0.9_m1//ag_news_lstmal_l5_prefix.pt  ,ep 0
gc 0
Train Epoch1 Acc 0.250225 (30027/120000), AUC 0.5647037625312805
ep1_train_time 22.695964336395264
Test Epoch1 threshold 0.9 Acc 0.9148026315789474, AUC 0.9794712066650391, avg_entr 0.09798995405435562
ep1_t0.9_test_time 0.13301944732666016
Save ckpt to ckpt/ag_news_lstmal_l5adf_pad75_t0.9_m1//ag_news_lstmal_l5_prefix.pt  ,ep 1
gc 0
Train Epoch2 Acc 0.2503 (30036/120000), AUC 0.5662322640419006
ep2_train_time 22.669248580932617
Test Epoch2 threshold 0.9 Acc 0.915625, AUC 0.9801167845726013, avg_entr 0.07843413203954697
ep2_t0.9_test_time 0.13135170936584473
Save ckpt to ckpt/ag_news_lstmal_l5adf_pad75_t0.9_m1//ag_news_lstmal_l5_prefix.pt  ,ep 2
gc 0
Train Epoch3 Acc 0.250325 (30039/120000), AUC 0.5672022104263306
ep3_train_time 22.726019144058228
Test Epoch3 threshold 0.9 Acc 0.9161184210526315, AUC 0.9803479909896851, avg_entr 0.06495994329452515
ep3_t0.9_test_time 0.13328981399536133
Save ckpt to ckpt/ag_news_lstmal_l5adf_pad75_t0.9_m1//ag_news_lstmal_l5_prefix.pt  ,ep 3
gc 0
Train Epoch4 Acc 0.25033333333333335 (30040/120000), AUC 0.5673764944076538
ep4_train_time 22.675560235977173
Test Epoch4 threshold 0.9 Acc 0.9108552631578948, AUC 0.9803276062011719, avg_entr 0.06261367350816727
ep4_t0.9_test_time 0.1328880786895752
gc 0
Train Epoch5 Acc 0.250375 (30045/120000), AUC 0.5676563382148743
ep5_train_time 22.63408398628235
Test Epoch5 threshold 0.9 Acc 0.9136513157894737, AUC 0.9802699089050293, avg_entr 0.053362566977739334
ep5_t0.9_test_time 0.13343191146850586
gc 0
Train Epoch6 Acc 0.25038333333333335 (30046/120000), AUC 0.567715048789978
ep6_train_time 22.678098917007446
Test Epoch6 threshold 0.9 Acc 0.9151315789473684, AUC 0.9800416231155396, avg_entr 0.04973458871245384
ep6_t0.9_test_time 0.14188551902770996
gc 0
Train Epoch7 Acc 0.2504416666666667 (30053/120000), AUC 0.567596435546875
ep7_train_time 22.683582544326782
Test Epoch7 threshold 0.9 Acc 0.9148026315789474, AUC 0.9800281524658203, avg_entr 0.04660577327013016
ep7_t0.9_test_time 0.13399815559387207
gc 0
Train Epoch8 Acc 0.25043333333333334 (30052/120000), AUC 0.5677669048309326
ep8_train_time 22.717596530914307
Test Epoch8 threshold 0.9 Acc 0.9144736842105263, AUC 0.9797494411468506, avg_entr 0.044540200382471085
ep8_t0.9_test_time 0.13289260864257812
gc 0
Train Epoch9 Acc 0.2504166666666667 (30050/120000), AUC 0.5678195357322693
ep9_train_time 22.751916885375977
Test Epoch9 threshold 0.9 Acc 0.9134868421052632, AUC 0.9797220230102539, avg_entr 0.04313863813877106
ep9_t0.9_test_time 0.13399195671081543
Best AUC 0.9803479909896851
train_loss (2, 5, 10)
valid_acc (10, 1)
valid_AUC (10, 1)
train_acc (10,)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Start Testing
Load ckpt at ckpt/ag_news_lstmal_l5adf_pad75_t0.9_m1//ag_news_lstmal_l5_prefix.pt
Test threshold 0.9 Acc 0.9230263157894737, AUC 0.9862196445465088, avg_entr 0.06723767518997192
t0.9_test_time 0.03463387489318848
