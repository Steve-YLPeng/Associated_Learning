total count words 102019
vocab size 30000
found 26754 words in glove
Load ckpt from ckpt/ag_news_transformeral_l5_pad175_m2_fix//ag_news_transformeral_l5_prefix.pt
model: TransformerModelML(
  (layers): ModuleList(
    (0): EMBLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Embedding(30000, 300)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=4, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=4, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.weight 9000000
layers.0.ae.g.0.weight 512
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 512
layers.0.ae.h.0.bias 4
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
Total Trainable Params: 13672292
Start Training
train_mask {0, 1, 2}
gc 9
Train Epoch0 Acc 0.22966666666666666 (27560/120000), AUC 0.3580964207649231
ep0_train_time 61.07675790786743
Test Epoch0 threshold 0.1 Acc 0.9180263157894737, AUC 0.9802325963973999, avg_entr 0.009452304802834988
ep0_t0.1_test_time 0.3864445686340332
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m3_fix//ag_news_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.2 Acc 0.9186842105263158, AUC 0.9815096855163574, avg_entr 0.014337578788399696
ep0_t0.2_test_time 0.3678715229034424
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m3_fix//ag_news_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.3 Acc 0.9188157894736843, AUC 0.9818683862686157, avg_entr 0.023212401196360588
ep0_t0.3_test_time 0.3438758850097656
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m3_fix//ag_news_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.4 Acc 0.9190789473684211, AUC 0.9818364381790161, avg_entr 0.02498750388622284
ep0_t0.4_test_time 0.32961440086364746
Test Epoch0 threshold 0.5 Acc 0.9182894736842105, AUC 0.9818397164344788, avg_entr 0.02631818875670433
ep0_t0.5_test_time 0.3126373291015625
Test Epoch0 threshold 0.6 Acc 0.9181578947368421, AUC 0.9818313121795654, avg_entr 0.02659965306520462
ep0_t0.6_test_time 0.31002092361450195
Test Epoch0 threshold 0.7 Acc 0.9181578947368421, AUC 0.9818313121795654, avg_entr 0.02659965306520462
ep0_t0.7_test_time 0.3106722831726074
Test Epoch0 threshold 0.8 Acc 0.9181578947368421, AUC 0.9818313121795654, avg_entr 0.02659965306520462
ep0_t0.8_test_time 0.3113424777984619
Test Epoch0 threshold 0.9 Acc 0.9181578947368421, AUC 0.9818313121795654, avg_entr 0.02659965306520462
ep0_t0.9_test_time 0.30951452255249023
gc 0
Train Epoch1 Acc 0.24910833333333332 (29893/120000), AUC 0.3423629403114319
ep1_train_time 60.59306812286377
Test Epoch1 threshold 0.1 Acc 0.9175, AUC 0.9803817868232727, avg_entr 0.009149380959570408
ep1_t0.1_test_time 0.3805220127105713
Test Epoch1 threshold 0.2 Acc 0.9177631578947368, AUC 0.9814991354942322, avg_entr 0.013876445591449738
ep1_t0.2_test_time 0.36094093322753906
Test Epoch1 threshold 0.3 Acc 0.9186842105263158, AUC 0.9819857478141785, avg_entr 0.02283313497900963
ep1_t0.3_test_time 0.3379955291748047
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m3_fix//ag_news_transformeral_l5_prefix.pt  ,ep 1
Test Epoch1 threshold 0.4 Acc 0.9181578947368421, AUC 0.982002854347229, avg_entr 0.024376267567276955
ep1_t0.4_test_time 0.32558584213256836
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m3_fix//ag_news_transformeral_l5_prefix.pt  ,ep 1
Test Epoch1 threshold 0.5 Acc 0.9176315789473685, AUC 0.9819568395614624, avg_entr 0.025808360427618027
ep1_t0.5_test_time 0.3178882598876953
Test Epoch1 threshold 0.6 Acc 0.9177631578947368, AUC 0.9819793701171875, avg_entr 0.026134079322218895
ep1_t0.6_test_time 0.3097727298736572
Test Epoch1 threshold 0.7 Acc 0.9177631578947368, AUC 0.9819793701171875, avg_entr 0.026134079322218895
ep1_t0.7_test_time 0.30904459953308105
Test Epoch1 threshold 0.8 Acc 0.9177631578947368, AUC 0.9819793701171875, avg_entr 0.026134079322218895
ep1_t0.8_test_time 0.309368371963501
Test Epoch1 threshold 0.9 Acc 0.9177631578947368, AUC 0.9819793701171875, avg_entr 0.026134079322218895
ep1_t0.9_test_time 0.30901479721069336
gc 0
Train Epoch2 Acc 0.24945 (29934/120000), AUC 0.3513714671134949
ep2_train_time 60.63462805747986
Test Epoch2 threshold 0.1 Acc 0.9171052631578948, AUC 0.979910135269165, avg_entr 0.009335882030427456
ep2_t0.1_test_time 0.3813652992248535
Test Epoch2 threshold 0.2 Acc 0.9169736842105263, AUC 0.9815027713775635, avg_entr 0.013780582696199417
ep2_t0.2_test_time 0.36386799812316895
Test Epoch2 threshold 0.3 Acc 0.9192105263157895, AUC 0.9819202423095703, avg_entr 0.023082628846168518
ep2_t0.3_test_time 0.33585429191589355
Test Epoch2 threshold 0.4 Acc 0.9184210526315789, AUC 0.9820009469985962, avg_entr 0.02480107545852661
ep2_t0.4_test_time 0.3202247619628906
Test Epoch2 threshold 0.5 Acc 0.9184210526315789, AUC 0.9820061922073364, avg_entr 0.026003172621130943
ep2_t0.5_test_time 0.3110330104827881
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m3_fix//ag_news_transformeral_l5_prefix.pt  ,ep 2
Test Epoch2 threshold 0.6 Acc 0.9184210526315789, AUC 0.9820111989974976, avg_entr 0.02640620619058609
ep2_t0.6_test_time 0.31223392486572266
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m3_fix//ag_news_transformeral_l5_prefix.pt  ,ep 2
Test Epoch2 threshold 0.7 Acc 0.9184210526315789, AUC 0.9820111989974976, avg_entr 0.02640620619058609
ep2_t0.7_test_time 0.31103014945983887
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m3_fix//ag_news_transformeral_l5_prefix.pt  ,ep 2
Test Epoch2 threshold 0.8 Acc 0.9184210526315789, AUC 0.9820111989974976, avg_entr 0.02640620619058609
ep2_t0.8_test_time 0.3119056224822998
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m3_fix//ag_news_transformeral_l5_prefix.pt  ,ep 2
Test Epoch2 threshold 0.9 Acc 0.9184210526315789, AUC 0.9820111989974976, avg_entr 0.02640620619058609
ep2_t0.9_test_time 0.31085824966430664
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m3_fix//ag_news_transformeral_l5_prefix.pt  ,ep 2
gc 0
Train Epoch3 Acc 0.24940833333333334 (29929/120000), AUC 0.35526564717292786
ep3_train_time 60.55679249763489
Test Epoch3 threshold 0.1 Acc 0.9192105263157895, AUC 0.9800462126731873, avg_entr 0.008509197272360325
ep3_t0.1_test_time 0.38120079040527344
Test Epoch3 threshold 0.2 Acc 0.9180263157894737, AUC 0.9811688661575317, avg_entr 0.013460907153785229
ep3_t0.2_test_time 0.36049389839172363
Test Epoch3 threshold 0.3 Acc 0.9192105263157895, AUC 0.9819985032081604, avg_entr 0.023101912811398506
ep3_t0.3_test_time 0.3362445831298828
Test Epoch3 threshold 0.4 Acc 0.9189473684210526, AUC 0.9820468425750732, avg_entr 0.02474524825811386
ep3_t0.4_test_time 0.3206520080566406
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m3_fix//ag_news_transformeral_l5_prefix.pt  ,ep 3
Test Epoch3 threshold 0.5 Acc 0.9189473684210526, AUC 0.982020378112793, avg_entr 0.025898493826389313
ep3_t0.5_test_time 0.3154716491699219
Test Epoch3 threshold 0.6 Acc 0.9190789473684211, AUC 0.982048749923706, avg_entr 0.026232486590743065
ep3_t0.6_test_time 0.31009745597839355
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m3_fix//ag_news_transformeral_l5_prefix.pt  ,ep 3
Test Epoch3 threshold 0.7 Acc 0.9190789473684211, AUC 0.982048749923706, avg_entr 0.026232486590743065
ep3_t0.7_test_time 0.31012868881225586
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m3_fix//ag_news_transformeral_l5_prefix.pt  ,ep 3
Test Epoch3 threshold 0.8 Acc 0.9190789473684211, AUC 0.982048749923706, avg_entr 0.026232486590743065
ep3_t0.8_test_time 0.3115365505218506
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m3_fix//ag_news_transformeral_l5_prefix.pt  ,ep 3
Test Epoch3 threshold 0.9 Acc 0.9190789473684211, AUC 0.982048749923706, avg_entr 0.026232486590743065
ep3_t0.9_test_time 0.3115863800048828
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m3_fix//ag_news_transformeral_l5_prefix.pt  ,ep 3
gc 0
Train Epoch4 Acc 0.24938333333333335 (29926/120000), AUC 0.35642868280410767
ep4_train_time 60.65260291099548
Test Epoch4 threshold 0.1 Acc 0.9182894736842105, AUC 0.979808509349823, avg_entr 0.00882133562117815
ep4_t0.1_test_time 0.38181161880493164
Test Epoch4 threshold 0.2 Acc 0.9175, AUC 0.9812508225440979, avg_entr 0.013457882218062878
ep4_t0.2_test_time 0.3643345832824707
Test Epoch4 threshold 0.3 Acc 0.9192105263157895, AUC 0.9819430708885193, avg_entr 0.023116357624530792
ep4_t0.3_test_time 0.3360903263092041
Test Epoch4 threshold 0.4 Acc 0.9186842105263158, AUC 0.9820098876953125, avg_entr 0.024684185162186623
ep4_t0.4_test_time 0.3186805248260498
Test Epoch4 threshold 0.5 Acc 0.9188157894736843, AUC 0.9820369482040405, avg_entr 0.025856904685497284
ep4_t0.5_test_time 0.31014513969421387
Test Epoch4 threshold 0.6 Acc 0.9185526315789474, AUC 0.9820268154144287, avg_entr 0.02626323513686657
ep4_t0.6_test_time 0.3084104061126709
Test Epoch4 threshold 0.7 Acc 0.9185526315789474, AUC 0.9820268154144287, avg_entr 0.02626323513686657
ep4_t0.7_test_time 0.307952880859375
Test Epoch4 threshold 0.8 Acc 0.9185526315789474, AUC 0.9820268154144287, avg_entr 0.02626323513686657
ep4_t0.8_test_time 0.3094959259033203
Test Epoch4 threshold 0.9 Acc 0.9185526315789474, AUC 0.9820268154144287, avg_entr 0.02626323513686657
ep4_t0.9_test_time 0.30837225914001465
gc 0
Train Epoch5 Acc 0.249375 (29925/120000), AUC 0.35626211762428284
ep5_train_time 60.66982102394104
Test Epoch5 threshold 0.1 Acc 0.9182894736842105, AUC 0.9798054695129395, avg_entr 0.008801068179309368
ep5_t0.1_test_time 0.3835413455963135
Test Epoch5 threshold 0.2 Acc 0.9175, AUC 0.9812743067741394, avg_entr 0.013493123464286327
ep5_t0.2_test_time 0.3632056713104248
Test Epoch5 threshold 0.3 Acc 0.9192105263157895, AUC 0.9819432497024536, avg_entr 0.023133616894483566
ep5_t0.3_test_time 0.33600831031799316
Test Epoch5 threshold 0.4 Acc 0.9186842105263158, AUC 0.9820101261138916, avg_entr 0.024705026298761368
ep5_t0.4_test_time 0.3212275505065918
Test Epoch5 threshold 0.5 Acc 0.9188157894736843, AUC 0.982036828994751, avg_entr 0.025878136977553368
ep5_t0.5_test_time 0.31285643577575684
Test Epoch5 threshold 0.6 Acc 0.9185526315789474, AUC 0.9820269346237183, avg_entr 0.02628421224653721
ep5_t0.6_test_time 0.31101131439208984
Test Epoch5 threshold 0.7 Acc 0.9185526315789474, AUC 0.9820269346237183, avg_entr 0.02628421224653721
ep5_t0.7_test_time 0.31035590171813965
Test Epoch5 threshold 0.8 Acc 0.9185526315789474, AUC 0.9820269346237183, avg_entr 0.02628421224653721
ep5_t0.8_test_time 0.309309720993042
Test Epoch5 threshold 0.9 Acc 0.9185526315789474, AUC 0.9820269346237183, avg_entr 0.02628421224653721
ep5_t0.9_test_time 0.3088555335998535
gc 0
Train Epoch6 Acc 0.24938333333333335 (29926/120000), AUC 0.3571614623069763
ep6_train_time 60.59634876251221
Test Epoch6 threshold 0.1 Acc 0.9184210526315789, AUC 0.9798073172569275, avg_entr 0.00883526261895895
ep6_t0.1_test_time 0.3824639320373535
Test Epoch6 threshold 0.2 Acc 0.9175, AUC 0.981321394443512, avg_entr 0.013468736782670021
ep6_t0.2_test_time 0.36172056198120117
Test Epoch6 threshold 0.3 Acc 0.9193421052631578, AUC 0.9819431304931641, avg_entr 0.023137560114264488
ep6_t0.3_test_time 0.33643531799316406
Test Epoch6 threshold 0.4 Acc 0.9186842105263158, AUC 0.9820058345794678, avg_entr 0.024748360738158226
ep6_t0.4_test_time 0.31983399391174316
Test Epoch6 threshold 0.5 Acc 0.9189473684210526, AUC 0.9820367097854614, avg_entr 0.025880785658955574
ep6_t0.5_test_time 0.31201934814453125
Test Epoch6 threshold 0.6 Acc 0.9186842105263158, AUC 0.9820268154144287, avg_entr 0.026286879554390907
ep6_t0.6_test_time 0.30929064750671387
Test Epoch6 threshold 0.7 Acc 0.9186842105263158, AUC 0.9820268154144287, avg_entr 0.026286879554390907
ep6_t0.7_test_time 0.3091084957122803
Test Epoch6 threshold 0.8 Acc 0.9186842105263158, AUC 0.9820268154144287, avg_entr 0.026286879554390907
ep6_t0.8_test_time 0.3086886405944824
Test Epoch6 threshold 0.9 Acc 0.9186842105263158, AUC 0.9820268154144287, avg_entr 0.026286879554390907
ep6_t0.9_test_time 0.3089122772216797
gc 0
Train Epoch7 Acc 0.24941666666666668 (29930/120000), AUC 0.3564053475856781
ep7_train_time 60.72016358375549
Test Epoch7 threshold 0.1 Acc 0.9184210526315789, AUC 0.9798071980476379, avg_entr 0.008837515488266945
ep7_t0.1_test_time 0.3831474781036377
Test Epoch7 threshold 0.2 Acc 0.9175, AUC 0.9813216328620911, avg_entr 0.013470491394400597
ep7_t0.2_test_time 0.36292171478271484
Test Epoch7 threshold 0.3 Acc 0.9193421052631578, AUC 0.9819434881210327, avg_entr 0.023139316588640213
ep7_t0.3_test_time 0.33565664291381836
Test Epoch7 threshold 0.4 Acc 0.9186842105263158, AUC 0.9820106625556946, avg_entr 0.02469768561422825
ep7_t0.4_test_time 0.32033276557922363
Test Epoch7 threshold 0.5 Acc 0.9189473684210526, AUC 0.9820370674133301, avg_entr 0.025882234796881676
ep7_t0.5_test_time 0.3111405372619629
Test Epoch7 threshold 0.6 Acc 0.9186842105263158, AUC 0.9820270538330078, avg_entr 0.026288306340575218
ep7_t0.6_test_time 0.30936455726623535
Test Epoch7 threshold 0.7 Acc 0.9186842105263158, AUC 0.9820270538330078, avg_entr 0.026288306340575218
ep7_t0.7_test_time 0.3109896183013916
Test Epoch7 threshold 0.8 Acc 0.9186842105263158, AUC 0.9820270538330078, avg_entr 0.026288306340575218
ep7_t0.8_test_time 0.3092660903930664
Test Epoch7 threshold 0.9 Acc 0.9186842105263158, AUC 0.9820270538330078, avg_entr 0.026288306340575218
ep7_t0.9_test_time 0.308912992477417
gc 0
Train Epoch8 Acc 0.24936666666666665 (29924/120000), AUC 0.3565903902053833
ep8_train_time 60.87232542037964
Test Epoch8 threshold 0.1 Acc 0.9184210526315789, AUC 0.9798070192337036, avg_entr 0.00884116068482399
ep8_t0.1_test_time 0.3811197280883789
Test Epoch8 threshold 0.2 Acc 0.9175, AUC 0.9813216924667358, avg_entr 0.013472240418195724
ep8_t0.2_test_time 0.3615591526031494
Test Epoch8 threshold 0.3 Acc 0.9193421052631578, AUC 0.9819434881210327, avg_entr 0.023139717057347298
ep8_t0.3_test_time 0.33420586585998535
Test Epoch8 threshold 0.4 Acc 0.9186842105263158, AUC 0.9820107221603394, avg_entr 0.024698184803128242
ep8_t0.4_test_time 0.32068300247192383
Test Epoch8 threshold 0.5 Acc 0.9189473684210526, AUC 0.9820370078086853, avg_entr 0.025882594287395477
ep8_t0.5_test_time 0.30971240997314453
Test Epoch8 threshold 0.6 Acc 0.9186842105263158, AUC 0.9820271134376526, avg_entr 0.026288660243153572
ep8_t0.6_test_time 0.30763769149780273
Test Epoch8 threshold 0.7 Acc 0.9186842105263158, AUC 0.9820271134376526, avg_entr 0.026288660243153572
ep8_t0.7_test_time 0.30797576904296875
Test Epoch8 threshold 0.8 Acc 0.9186842105263158, AUC 0.9820271134376526, avg_entr 0.026288660243153572
ep8_t0.8_test_time 0.3080325126647949
Test Epoch8 threshold 0.9 Acc 0.9186842105263158, AUC 0.9820271134376526, avg_entr 0.026288660243153572
ep8_t0.9_test_time 0.30765628814697266
gc 0
Train Epoch9 Acc 0.249375 (29925/120000), AUC 0.35703927278518677
ep9_train_time 60.77152490615845
Test Epoch9 threshold 0.1 Acc 0.9184210526315789, AUC 0.9798070192337036, avg_entr 0.008827801793813705
ep9_t0.1_test_time 0.38396763801574707
Test Epoch9 threshold 0.2 Acc 0.9175, AUC 0.9813219904899597, avg_entr 0.013473251834511757
ep9_t0.2_test_time 0.36477184295654297
Test Epoch9 threshold 0.3 Acc 0.9193421052631578, AUC 0.9819434285163879, avg_entr 0.02314002253115177
ep9_t0.3_test_time 0.3364253044128418
Test Epoch9 threshold 0.4 Acc 0.9186842105263158, AUC 0.9820106029510498, avg_entr 0.024698583409190178
ep9_t0.4_test_time 0.3229391574859619
Test Epoch9 threshold 0.5 Acc 0.9189473684210526, AUC 0.9820368885993958, avg_entr 0.025882884860038757
ep9_t0.5_test_time 0.31204771995544434
Test Epoch9 threshold 0.6 Acc 0.9186842105263158, AUC 0.982026994228363, avg_entr 0.026288943365216255
ep9_t0.6_test_time 0.3099400997161865
Test Epoch9 threshold 0.7 Acc 0.9186842105263158, AUC 0.982026994228363, avg_entr 0.026288943365216255
ep9_t0.7_test_time 0.31056737899780273
Test Epoch9 threshold 0.8 Acc 0.9186842105263158, AUC 0.982026994228363, avg_entr 0.026288943365216255
ep9_t0.8_test_time 0.31299304962158203
Test Epoch9 threshold 0.9 Acc 0.9186842105263158, AUC 0.982026994228363, avg_entr 0.026288943365216255
ep9_t0.9_test_time 0.31072211265563965
Best AUC 0.982048749923706
train_loss (2, 5, 10)
valid_acc (10, 9)
valid_AUC (10, 9)
train_acc (10,)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Start Testing
Load ckpt at ckpt/ag_news_transformeral_l5_pad175_m3_fix//ag_news_transformeral_l5_prefix.pt
[[1706   61   85   48]
 [   9 1874    7   10]
 [  45   18 1677  160]
 [  44   16  112 1728]]
Figure(640x480)
tensor([5.3614e-04, 5.8170e-08, 1.5237e-03,  ..., 2.0815e-02, 6.6208e-07,
        4.2578e-02])
[[1716   55   69   60]
 [  14 1863   12   11]
 [  46   16 1680  158]
 [  46   13  123 1718]]
Figure(640x480)
tensor([5.5712e-07, 9.5970e-08, 7.5076e-08,  ..., 3.5268e-07, 1.2977e-07,
        9.5468e-08])
[[1719   55   67   59]
 [  15 1862   11   12]
 [  49   16 1678  157]
 [  45   10  125 1720]]
Figure(640x480)
tensor([2.8094e-07, 9.2584e-08, 1.1925e-07,  ..., 5.1921e-07, 8.8819e-08,
        9.6461e-08])
[[  36    0    0 1864]
 [ 903    0    0  997]
 [  32    0    0 1868]
 [   2    0    0 1898]]
Figure(640x480)
tensor([0.3808, 0.3447, 0.3540,  ..., 0.2933, 0.3444, 0.3896])
[[1898    0    0    2]
 [1816    0    0   84]
 [1900    0    0    0]
 [1900    0    0    0]]
Figure(640x480)
tensor([0.2359, 0.0374, 0.0423,  ..., 0.7132, 0.2476, 0.2367])
