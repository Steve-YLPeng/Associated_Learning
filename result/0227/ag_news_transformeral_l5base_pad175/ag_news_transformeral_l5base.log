total count words 102019
vocab size 30000
train size 120000, valid size 3800, test size 3800
found 26754 words in glove
model: TransformerModelML(
  (layers): ModuleList(
    (0): EMBLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Embedding(30000, 300)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=4, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=4, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.weight 9000000
layers.0.ae.g.0.weight 512
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 512
layers.0.ae.h.0.bias 4
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
Total Trainable Params: 13672292
init_time 19.570931673049927
Start Training
gc 0
Train Epoch0 Acc 0.6254333333333333 (75052/120000), AUC 0.857804000377655
ep0_train_time 150.2125415802002
Test Epoch0 layer4 Acc 0.908157894736842, AUC 0.9809308052062988, avg_entr 0.14495407044887543
ep0_l4_test_time 1.2665653228759766
Save ckpt to ckpt/ag_news_transformeral_l5base_pad175//ag_news_transformeral_l5.pt  ,ep 0
gc 0
Train Epoch1 Acc 0.9201583333333333 (110419/120000), AUC 0.9815287590026855
ep1_train_time 149.612788438797
Test Epoch1 layer4 Acc 0.9223684210526316, AUC 0.9826068878173828, avg_entr 0.06084825098514557
ep1_l4_test_time 1.2484006881713867
Save ckpt to ckpt/ag_news_transformeral_l5base_pad175//ag_news_transformeral_l5.pt  ,ep 1
gc 0
Train Epoch2 Acc 0.9351916666666666 (112223/120000), AUC 0.9866862893104553
ep2_train_time 149.07471537590027
Test Epoch2 layer4 Acc 0.9231578947368421, AUC 0.9840307235717773, avg_entr 0.03081689588725567
ep2_l4_test_time 1.2570815086364746
Save ckpt to ckpt/ag_news_transformeral_l5base_pad175//ag_news_transformeral_l5.pt  ,ep 2
gc 0
Train Epoch3 Acc 0.9427083333333334 (113125/120000), AUC 0.9887052774429321
ep3_train_time 149.54118943214417
Test Epoch3 layer4 Acc 0.9236842105263158, AUC 0.9838457107543945, avg_entr 0.02390432171523571
ep3_l4_test_time 1.2635340690612793
gc 0
Train Epoch4 Acc 0.9482416666666666 (113789/120000), AUC 0.9903237819671631
ep4_train_time 149.21072578430176
Test Epoch4 layer4 Acc 0.9244736842105263, AUC 0.9832690358161926, avg_entr 0.016891740262508392
ep4_l4_test_time 1.2514574527740479
gc 0
Train Epoch5 Acc 0.952225 (114267/120000), AUC 0.9915215373039246
ep5_train_time 149.04413199424744
Test Epoch5 layer4 Acc 0.9207894736842105, AUC 0.9827437400817871, avg_entr 0.017456624656915665
ep5_l4_test_time 1.2667319774627686
gc 0
Train Epoch6 Acc 0.955925 (114711/120000), AUC 0.9923216700553894
ep6_train_time 149.7495081424713
Test Epoch6 layer4 Acc 0.9210526315789473, AUC 0.9825670123100281, avg_entr 0.01396342646330595
ep6_l4_test_time 1.2530279159545898
gc 0
Train Epoch7 Acc 0.9585666666666667 (115028/120000), AUC 0.9931806325912476
ep7_train_time 149.0368514060974
Test Epoch7 layer4 Acc 0.9205263157894736, AUC 0.9814754724502563, avg_entr 0.011950919404625893
ep7_l4_test_time 1.273073434829712
gc 0
Train Epoch8 Acc 0.9611833333333333 (115342/120000), AUC 0.9934147000312805
ep8_train_time 149.86593580245972
Test Epoch8 layer4 Acc 0.9207894736842105, AUC 0.9802402257919312, avg_entr 0.010625519789755344
ep8_l4_test_time 1.2455101013183594
gc 0
Train Epoch9 Acc 0.9635333333333334 (115624/120000), AUC 0.994041919708252
ep9_train_time 149.25391125679016
Test Epoch9 layer4 Acc 0.9189473684210526, AUC 0.9773865938186646, avg_entr 0.00900093000382185
ep9_l4_test_time 1.2694451808929443
gc 0
Train Epoch10 Acc 0.9658416666666667 (115901/120000), AUC 0.9943233132362366
ep10_train_time 149.1998634338379
Test Epoch10 layer4 Acc 0.9186842105263158, AUC 0.9778591394424438, avg_entr 0.009888529777526855
ep10_l4_test_time 1.2497003078460693
gc 0
Train Epoch11 Acc 0.9670833333333333 (116050/120000), AUC 0.9947916269302368
ep11_train_time 149.71075582504272
Test Epoch11 layer4 Acc 0.9176315789473685, AUC 0.9791797399520874, avg_entr 0.0092473728582263
ep11_l4_test_time 1.267690896987915
gc 0
Train Epoch12 Acc 0.9691916666666667 (116303/120000), AUC 0.9949913620948792
ep12_train_time 149.01742482185364
Test Epoch12 layer4 Acc 0.915, AUC 0.9778015613555908, avg_entr 0.007382158190011978
ep12_l4_test_time 1.2535781860351562
gc 0
Train Epoch13 Acc 0.9707583333333333 (116491/120000), AUC 0.995207667350769
ep13_train_time 149.55993795394897
Test Epoch13 layer4 Acc 0.9157894736842105, AUC 0.9780230522155762, avg_entr 0.006717638112604618
ep13_l4_test_time 1.2502830028533936
gc 0
Train Epoch14 Acc 0.9722083333333333 (116665/120000), AUC 0.9956084489822388
ep14_train_time 149.29419898986816
Test Epoch14 layer4 Acc 0.9128947368421053, AUC 0.9767942428588867, avg_entr 0.00801884476095438
ep14_l4_test_time 1.2470510005950928
gc 0
Train Epoch15 Acc 0.973775 (116853/120000), AUC 0.9955971240997314
ep15_train_time 149.0788016319275
Test Epoch15 layer4 Acc 0.915, AUC 0.9759647846221924, avg_entr 0.006140075158327818
ep15_l4_test_time 1.2487878799438477
gc 0
Train Epoch16 Acc 0.9754083333333333 (117049/120000), AUC 0.9957883358001709
ep16_train_time 149.57849836349487
Test Epoch16 layer4 Acc 0.9084210526315789, AUC 0.9751631021499634, avg_entr 0.007060257717967033
ep16_l4_test_time 1.2883753776550293
gc 0
Train Epoch17 Acc 0.9766166666666667 (117194/120000), AUC 0.9960775971412659
ep17_train_time 149.06442213058472
Test Epoch17 layer4 Acc 0.9110526315789473, AUC 0.9756564497947693, avg_entr 0.0067225610837340355
ep17_l4_test_time 1.2547357082366943
gc 0
Train Epoch18 Acc 0.9781416666666667 (117377/120000), AUC 0.9961835741996765
ep18_train_time 149.8453085422516
Test Epoch18 layer4 Acc 0.9086842105263158, AUC 0.9706755876541138, avg_entr 0.005595469381660223
ep18_l4_test_time 1.2376782894134521
gc 0
Train Epoch19 Acc 0.9791 (117492/120000), AUC 0.9962987899780273
ep19_train_time 149.19622707366943
Test Epoch19 layer4 Acc 0.9073684210526316, AUC 0.974441409111023, avg_entr 0.005869218613952398
ep19_l4_test_time 1.2529797554016113
gc 0
Train Epoch20 Acc 0.9802166666666666 (117626/120000), AUC 0.996532678604126
ep20_train_time 149.1495759487152
Test Epoch20 layer4 Acc 0.9036842105263158, AUC 0.9702498316764832, avg_entr 0.005662003066390753
ep20_l4_test_time 1.254387617111206
gc 0
Train Epoch21 Acc 0.98145 (117774/120000), AUC 0.9968343377113342
ep21_train_time 149.70532870292664
Test Epoch21 layer4 Acc 0.9092105263157895, AUC 0.9711857438087463, avg_entr 0.005980252753943205
ep21_l4_test_time 1.2499537467956543
gc 0
Train Epoch22 Acc 0.9824416666666667 (117893/120000), AUC 0.9968439340591431
ep22_train_time 149.06781649589539
Test Epoch22 layer4 Acc 0.9092105263157895, AUC 0.9704344272613525, avg_entr 0.004206483718007803
ep22_l4_test_time 1.2462060451507568
gc 0
Train Epoch23 Acc 0.9833833333333334 (118006/120000), AUC 0.9970160722732544
ep23_train_time 149.39890956878662
Test Epoch23 layer4 Acc 0.9078947368421053, AUC 0.9709764719009399, avg_entr 0.005019204691052437
ep23_l4_test_time 1.2731108665466309
gc 0
Train Epoch24 Acc 0.98375 (118050/120000), AUC 0.9971712827682495
ep24_train_time 148.96328687667847
Test Epoch24 layer4 Acc 0.9055263157894737, AUC 0.9706460237503052, avg_entr 0.005162437912076712
ep24_l4_test_time 1.2791404724121094
gc 0
Train Epoch25 Acc 0.985025 (118203/120000), AUC 0.997342586517334
ep25_train_time 148.98420596122742
Test Epoch25 layer4 Acc 0.9021052631578947, AUC 0.9691142439842224, avg_entr 0.0049307928420603275
ep25_l4_test_time 1.2713499069213867
gc 0
Train Epoch26 Acc 0.9853666666666666 (118244/120000), AUC 0.9974731206893921
ep26_train_time 149.5288553237915
Test Epoch26 layer4 Acc 0.9078947368421053, AUC 0.9689967632293701, avg_entr 0.00506721343845129
ep26_l4_test_time 1.251431941986084
gc 0
Train Epoch27 Acc 0.9862 (118344/120000), AUC 0.9977267980575562
ep27_train_time 149.16386651992798
Test Epoch27 layer4 Acc 0.9071052631578947, AUC 0.9669772386550903, avg_entr 0.0033331383019685745
ep27_l4_test_time 1.245757818222046
gc 0
Train Epoch28 Acc 0.986475 (118377/120000), AUC 0.9976997375488281
ep28_train_time 149.79393100738525
Test Epoch28 layer4 Acc 0.9063157894736842, AUC 0.969897985458374, avg_entr 0.003770652459934354
ep28_l4_test_time 1.2519912719726562
gc 0
Train Epoch29 Acc 0.9874916666666667 (118499/120000), AUC 0.9978013634681702
ep29_train_time 149.01184010505676
Test Epoch29 layer4 Acc 0.9031578947368422, AUC 0.9686022400856018, avg_entr 0.004431559704244137
ep29_l4_test_time 1.2591092586517334
gc 0
Train Epoch30 Acc 0.9882666666666666 (118592/120000), AUC 0.9979046583175659
ep30_train_time 149.1090362071991
Test Epoch30 layer4 Acc 0.9060526315789473, AUC 0.9676196575164795, avg_entr 0.004490401595830917
ep30_l4_test_time 1.242790699005127
gc 0
Train Epoch31 Acc 0.9882583333333333 (118591/120000), AUC 0.9979845881462097
ep31_train_time 149.66357278823853
Test Epoch31 layer4 Acc 0.9052631578947369, AUC 0.9651213884353638, avg_entr 0.003910038620233536
ep31_l4_test_time 1.2515685558319092
gc 0
Train Epoch32 Acc 0.9885333333333334 (118624/120000), AUC 0.9981416463851929
ep32_train_time 149.04117274284363
Test Epoch32 layer4 Acc 0.9005263157894737, AUC 0.9666339159011841, avg_entr 0.006196870468556881
ep32_l4_test_time 1.2633919715881348
gc 0
Train Epoch33 Acc 0.9895083333333333 (118741/120000), AUC 0.9980592727661133
ep33_train_time 149.49052238464355
Test Epoch33 layer4 Acc 0.9031578947368422, AUC 0.965654730796814, avg_entr 0.005734024569392204
ep33_l4_test_time 1.2448992729187012
gc 0
Train Epoch34 Acc 0.9897 (118764/120000), AUC 0.998176097869873
ep34_train_time 148.89095163345337
Test Epoch34 layer4 Acc 0.9057894736842105, AUC 0.967194139957428, avg_entr 0.004331541713327169
ep34_l4_test_time 1.2403507232666016
gc 0
Train Epoch35 Acc 0.9901583333333334 (118819/120000), AUC 0.9981847405433655
ep35_train_time 149.32585167884827
Test Epoch35 layer4 Acc 0.9042105263157895, AUC 0.9651396870613098, avg_entr 0.0029703506734222174
ep35_l4_test_time 1.2574691772460938
gc 0
Train Epoch36 Acc 0.9906666666666667 (118880/120000), AUC 0.9983231425285339
ep36_train_time 149.71589159965515
Test Epoch36 layer4 Acc 0.9010526315789473, AUC 0.9663448333740234, avg_entr 0.005293272435665131
ep36_l4_test_time 1.2738831043243408
gc 0
Train Epoch37 Acc 0.9908666666666667 (118904/120000), AUC 0.9983446598052979
ep37_train_time 148.90456342697144
Test Epoch37 layer4 Acc 0.9039473684210526, AUC 0.9669042825698853, avg_entr 0.004615047946572304
ep37_l4_test_time 1.2659664154052734
gc 0
Train Epoch38 Acc 0.991225 (118947/120000), AUC 0.9983143210411072
ep38_train_time 149.71450304985046
Test Epoch38 layer4 Acc 0.905, AUC 0.9655141830444336, avg_entr 0.0029247417114675045
ep38_l4_test_time 1.254547357559204
gc 0
Train Epoch39 Acc 0.99175 (119010/120000), AUC 0.9983192682266235
ep39_train_time 149.10992240905762
Test Epoch39 layer4 Acc 0.9021052631578947, AUC 0.9651215672492981, avg_entr 0.003739996813237667
ep39_l4_test_time 1.243077278137207
gc 0
Train Epoch40 Acc 0.99175 (119010/120000), AUC 0.9984776973724365
ep40_train_time 149.1608157157898
Test Epoch40 layer4 Acc 0.9036842105263158, AUC 0.9651978015899658, avg_entr 0.003208486596122384
ep40_l4_test_time 1.2392511367797852
gc 0
Train Epoch41 Acc 0.9921833333333333 (119062/120000), AUC 0.9985060691833496
ep41_train_time 149.48393845558167
Test Epoch41 layer4 Acc 0.9028947368421053, AUC 0.9634225964546204, avg_entr 0.002994175534695387
ep41_l4_test_time 1.261859655380249
gc 0
Train Epoch42 Acc 0.9924 (119088/120000), AUC 0.9985607862472534
ep42_train_time 148.97650337219238
Test Epoch42 layer4 Acc 0.9034210526315789, AUC 0.9650688171386719, avg_entr 0.004655296914279461
ep42_l4_test_time 1.255702257156372
gc 0
Train Epoch43 Acc 0.9923083333333333 (119077/120000), AUC 0.9986070990562439
ep43_train_time 149.54814291000366
Test Epoch43 layer4 Acc 0.9005263157894737, AUC 0.9619325399398804, avg_entr 0.0036038265097886324
ep43_l4_test_time 1.2490770816802979
gc 0
Train Epoch44 Acc 0.992825 (119139/120000), AUC 0.9987192749977112
ep44_train_time 91.17431282997131
Test Epoch44 layer4 Acc 0.9028947368421053, AUC 0.9591259956359863, avg_entr 0.004770901519805193
ep44_l4_test_time 1.2596635818481445
gc 0
Train Epoch45 Acc 0.993025 (119163/120000), AUC 0.9986904859542847
ep45_train_time 149.59513807296753
Test Epoch45 layer4 Acc 0.9028947368421053, AUC 0.9641111493110657, avg_entr 0.004332853481173515
ep45_l4_test_time 1.2704076766967773
gc 0
Train Epoch46 Acc 0.9933916666666667 (119207/120000), AUC 0.9987058639526367
ep46_train_time 148.88136887550354
Test Epoch46 layer4 Acc 0.9007894736842105, AUC 0.9603306651115417, avg_entr 0.0032821488566696644
ep46_l4_test_time 1.2593514919281006
gc 0
Train Epoch47 Acc 0.9933583333333333 (119203/120000), AUC 0.99882572889328
ep47_train_time 149.4309585094452
Test Epoch47 layer4 Acc 0.905, AUC 0.9638659954071045, avg_entr 0.003271733643487096
ep47_l4_test_time 1.242582082748413
gc 0
Train Epoch48 Acc 0.9936916666666666 (119243/120000), AUC 0.9988486766815186
ep48_train_time 149.0658266544342
Test Epoch48 layer4 Acc 0.9036842105263158, AUC 0.9632188677787781, avg_entr 0.0033700033091008663
ep48_l4_test_time 1.2560272216796875
gc 0
Train Epoch49 Acc 0.99375 (119250/120000), AUC 0.9987970590591431
ep49_train_time 148.98526978492737
Test Epoch49 layer4 Acc 0.9, AUC 0.9607632756233215, avg_entr 0.00386427971534431
ep49_l4_test_time 1.2238099575042725
Best AUC 0.9840307235717773
train_loss (2, 5, 50)
valid_acc (1, 50)
valid_AUC (1, 50)
train_acc (50,)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
total_train+valid_time 7474.002306222916
Start Testing
Load ckpt at ckpt/ag_news_transformeral_l5base_pad175//ag_news_transformeral_l5.pt
Test layer4 Acc 0.9173684210526316, AUC 0.9815570712089539, avg_entr 0.034027986228466034
l4_test_time 1.2414369583129883
