total count words 887881
vocab size 30000
train size 560000, valid size 35000, test size 35000
found 28354 words in glove
model: TransformerModelML(
  (layers): ModuleList(
    (0): EMBLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Embedding(30000, 300)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=14, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=14, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.weight 9000000
layers.0.ae.g.0.weight 1792
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 1792
layers.0.ae.h.0.bias 14
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
Total Trainable Params: 13674862
init_time 57.40645980834961
Start Training
gc 0
Train Epoch0 Acc 0.8337767857142857 (466915/560000), AUC 0.9801246523857117
ep0_train_time 298.9781131744385
Test Epoch0 layer4 Acc 0.9730571428571428, AUC 0.9979385137557983, avg_entr 0.022946881130337715
ep0_l4_test_time 4.959656715393066
Save ckpt to ckpt/dbpedia_14_transformeral_l5base_pad60//dbpedia_14_transformeral_l5.pt  ,ep 0
gc 0
Train Epoch1 Acc 0.9800357142857142 (548820/560000), AUC 0.997639000415802
ep1_train_time 295.8507936000824
Test Epoch1 layer4 Acc 0.9784857142857143, AUC 0.9980305433273315, avg_entr 0.005554872564971447
ep1_l4_test_time 4.963518381118774
Save ckpt to ckpt/dbpedia_14_transformeral_l5base_pad60//dbpedia_14_transformeral_l5.pt  ,ep 1
gc 0
Train Epoch2 Acc 0.9838910714285715 (550979/560000), AUC 0.9980049729347229
ep2_train_time 276.96037673950195
Test Epoch2 layer4 Acc 0.9786857142857143, AUC 0.9973498582839966, avg_entr 0.0032297566067427397
ep2_l4_test_time 4.917642116546631
gc 0
Train Epoch3 Acc 0.9854928571428572 (551876/560000), AUC 0.9981961250305176
ep3_train_time 295.6620879173279
Test Epoch3 layer4 Acc 0.978, AUC 0.9968332052230835, avg_entr 0.0025212375912815332
ep3_l4_test_time 4.991072177886963
gc 0
Train Epoch4 Acc 0.9869089285714285 (552669/560000), AUC 0.9983019828796387
ep4_train_time 295.8563678264618
Test Epoch4 layer4 Acc 0.9784857142857143, AUC 0.9968109130859375, avg_entr 0.0021459495183080435
ep4_l4_test_time 4.924893140792847
gc 0
Train Epoch5 Acc 0.9877928571428571 (553164/560000), AUC 0.9985446929931641
ep5_train_time 295.51469707489014
Test Epoch5 layer4 Acc 0.9788285714285714, AUC 0.9965950846672058, avg_entr 0.0022355378605425358
ep5_l4_test_time 5.012208938598633
gc 0
Train Epoch6 Acc 0.9884196428571429 (553515/560000), AUC 0.9987300634384155
ep6_train_time 295.6248297691345
Test Epoch6 layer4 Acc 0.9794, AUC 0.9964002370834351, avg_entr 0.0022368559148162603
ep6_l4_test_time 4.991726398468018
gc 0
Train Epoch7 Acc 0.9891607142857143 (553930/560000), AUC 0.9988139867782593
ep7_train_time 278.24955105781555
Test Epoch7 layer4 Acc 0.9789428571428571, AUC 0.9962896108627319, avg_entr 0.001767395413480699
ep7_l4_test_time 4.913822174072266
gc 0
Train Epoch8 Acc 0.9896982142857143 (554231/560000), AUC 0.9988306760787964
ep8_train_time 295.6121201515198
Test Epoch8 layer4 Acc 0.9795142857142857, AUC 0.9961223602294922, avg_entr 0.0016924927476793528
ep8_l4_test_time 4.956869125366211
gc 0
Train Epoch9 Acc 0.9902517857142857 (554541/560000), AUC 0.9988548159599304
ep9_train_time 295.9754366874695
Test Epoch9 layer4 Acc 0.9789714285714286, AUC 0.9956853985786438, avg_entr 0.0017093776259571314
ep9_l4_test_time 4.919616460800171
gc 0
Train Epoch10 Acc 0.9906625 (554771/560000), AUC 0.9988880753517151
ep10_train_time 295.7571587562561
Test Epoch10 layer4 Acc 0.9786571428571429, AUC 0.9954397082328796, avg_entr 0.001662486232817173
ep10_l4_test_time 4.946658372879028
gc 0
Train Epoch11 Acc 0.9911339285714286 (555035/560000), AUC 0.9989417195320129
ep11_train_time 295.92361855506897
Test Epoch11 layer4 Acc 0.9785714285714285, AUC 0.9957443475723267, avg_entr 0.0017450018785893917
ep11_l4_test_time 4.992625951766968
gc 0
Train Epoch12 Acc 0.9915482142857143 (555267/560000), AUC 0.9989675283432007
ep12_train_time 266.7085430622101
Test Epoch12 layer4 Acc 0.9782857142857143, AUC 0.9953745007514954, avg_entr 0.0018876094836741686
ep12_l4_test_time 2.3680970668792725
gc 0
Train Epoch13 Acc 0.9918089285714285 (555413/560000), AUC 0.9989480376243591
ep13_train_time 285.965247631073
Test Epoch13 layer4 Acc 0.9781714285714286, AUC 0.99540776014328, avg_entr 0.001851282431744039
ep13_l4_test_time 5.245230436325073
gc 0
Train Epoch14 Acc 0.9922732142857142 (555673/560000), AUC 0.9990320205688477
ep14_train_time 309.75353050231934
Test Epoch14 layer4 Acc 0.9778857142857142, AUC 0.9953800439834595, avg_entr 0.0018052197992801666
ep14_l4_test_time 5.148871898651123
gc 0
Train Epoch15 Acc 0.9925107142857142 (555806/560000), AUC 0.9990233778953552
ep15_train_time 309.6436867713928
Test Epoch15 layer4 Acc 0.9778, AUC 0.9951394200325012, avg_entr 0.001482719206251204
ep15_l4_test_time 5.175841569900513
gc 0
Train Epoch16 Acc 0.9929267857142857 (556039/560000), AUC 0.9990390539169312
ep16_train_time 309.91468811035156
Test Epoch16 layer4 Acc 0.9776285714285714, AUC 0.9949252009391785, avg_entr 0.0015819844556972384
ep16_l4_test_time 5.198591232299805
gc 0
Train Epoch17 Acc 0.9932303571428571 (556209/560000), AUC 0.9990717172622681
ep17_train_time 290.88672137260437
Test Epoch17 layer4 Acc 0.9782285714285714, AUC 0.9948492646217346, avg_entr 0.0015393642242997885
ep17_l4_test_time 5.09865140914917
gc 0
Train Epoch18 Acc 0.9934375 (556325/560000), AUC 0.9990634918212891
ep18_train_time 309.70169377326965
Test Epoch18 layer4 Acc 0.9782571428571428, AUC 0.994819700717926, avg_entr 0.0015324339037761092
ep18_l4_test_time 5.2104573249816895
gc 0
Train Epoch19 Acc 0.9936892857142857 (556466/560000), AUC 0.9990919232368469
ep19_train_time 310.01330614089966
Test Epoch19 layer4 Acc 0.9769428571428571, AUC 0.9947496652603149, avg_entr 0.0015980793396010995
ep19_l4_test_time 5.174759864807129
gc 0
Train Epoch20 Acc 0.9939642857142857 (556620/560000), AUC 0.9991364479064941
ep20_train_time 309.7572259902954
Test Epoch20 layer4 Acc 0.9778285714285714, AUC 0.9944217801094055, avg_entr 0.001555544207803905
ep20_l4_test_time 5.168824911117554
gc 0
Train Epoch21 Acc 0.9941785714285715 (556740/560000), AUC 0.9991423487663269
ep21_train_time 309.77742862701416
Test Epoch21 layer4 Acc 0.9770857142857143, AUC 0.9947152137756348, avg_entr 0.0016776996199041605
ep21_l4_test_time 5.154076814651489
gc 0
Train Epoch22 Acc 0.9943910714285714 (556859/560000), AUC 0.999182403087616
ep22_train_time 290.8322584629059
Test Epoch22 layer4 Acc 0.9780857142857143, AUC 0.9941786527633667, avg_entr 0.0015572060365229845
ep22_l4_test_time 5.101755380630493
gc 0
Train Epoch23 Acc 0.9946410714285714 (556999/560000), AUC 0.9991549253463745
ep23_train_time 309.7973186969757
Test Epoch23 layer4 Acc 0.9776857142857143, AUC 0.994476854801178, avg_entr 0.001457458478398621
ep23_l4_test_time 5.1355040073394775
gc 0
Train Epoch24 Acc 0.9948178571428572 (557098/560000), AUC 0.9991665482521057
ep24_train_time 309.52624440193176
Test Epoch24 layer4 Acc 0.9771428571428571, AUC 0.9942733645439148, avg_entr 0.0014671589015051723
ep24_l4_test_time 5.196436405181885
gc 0
Train Epoch25 Acc 0.9950285714285714 (557216/560000), AUC 0.9992373585700989
ep25_train_time 309.6819348335266
Test Epoch25 layer4 Acc 0.9773714285714286, AUC 0.9939394593238831, avg_entr 0.0012368157040327787
ep25_l4_test_time 5.072851181030273
gc 0
Train Epoch26 Acc 0.9951892857142857 (557306/560000), AUC 0.9992246031761169
ep26_train_time 309.90046548843384
Test Epoch26 layer4 Acc 0.9770571428571428, AUC 0.9944747090339661, avg_entr 0.0014146538451313972
ep26_l4_test_time 5.120418548583984
gc 0
Train Epoch27 Acc 0.9953303571428571 (557385/560000), AUC 0.9992733001708984
ep27_train_time 289.6259434223175
Test Epoch27 layer4 Acc 0.9763714285714286, AUC 0.9945290684700012, avg_entr 0.0015492188977077603
ep27_l4_test_time 5.143521308898926
gc 0
Train Epoch28 Acc 0.9954107142857143 (557430/560000), AUC 0.9992809295654297
ep28_train_time 309.74104404449463
Test Epoch28 layer4 Acc 0.9760857142857143, AUC 0.9937138557434082, avg_entr 0.001359716523438692
ep28_l4_test_time 5.22430944442749
gc 0
Train Epoch29 Acc 0.9956571428571429 (557568/560000), AUC 0.9992527365684509
ep29_train_time 309.8311321735382
Test Epoch29 layer4 Acc 0.9766, AUC 0.9938904643058777, avg_entr 0.001337505178526044
ep29_l4_test_time 5.176548480987549
gc 0
Train Epoch30 Acc 0.9957714285714285 (557632/560000), AUC 0.9992968440055847
ep30_train_time 309.6680119037628
Test Epoch30 layer4 Acc 0.9769428571428571, AUC 0.9939299821853638, avg_entr 0.0012644735397771
ep30_l4_test_time 5.157102823257446
gc 0
Train Epoch31 Acc 0.9959392857142857 (557726/560000), AUC 0.9992968440055847
ep31_train_time 291.45888233184814
Test Epoch31 layer4 Acc 0.9765142857142857, AUC 0.9940155148506165, avg_entr 0.0013800833839923143
ep31_l4_test_time 5.158628463745117
gc 0
Train Epoch32 Acc 0.9959785714285714 (557748/560000), AUC 0.9993129372596741
ep32_train_time 309.91044759750366
Test Epoch32 layer4 Acc 0.9764, AUC 0.9937211871147156, avg_entr 0.001320196082815528
ep32_l4_test_time 5.1691107749938965
gc 0
Train Epoch33 Acc 0.9960857142857142 (557808/560000), AUC 0.9993316531181335
ep33_train_time 310.03986144065857
Test Epoch33 layer4 Acc 0.9758571428571429, AUC 0.9932894110679626, avg_entr 0.0013324039755389094
ep33_l4_test_time 5.0574870109558105
gc 0
Train Epoch34 Acc 0.9962107142857143 (557878/560000), AUC 0.9993540644645691
ep34_train_time 309.9575734138489
Test Epoch34 layer4 Acc 0.9766857142857143, AUC 0.993710994720459, avg_entr 0.0013790284283459187
ep34_l4_test_time 5.104481935501099
gc 0
Train Epoch35 Acc 0.9963178571428571 (557938/560000), AUC 0.9993740916252136
ep35_train_time 309.9776005744934
Test Epoch35 layer4 Acc 0.9758, AUC 0.9933189749717712, avg_entr 0.0013186039868742228
ep35_l4_test_time 5.0782551765441895
gc 0
Train Epoch36 Acc 0.9964785714285714 (558028/560000), AUC 0.9993734955787659
ep36_train_time 286.4691321849823
Test Epoch36 layer4 Acc 0.9759714285714286, AUC 0.993306577205658, avg_entr 0.0011986412573605776
ep36_l4_test_time 5.246438264846802
gc 0
Train Epoch37 Acc 0.9964767857142857 (558027/560000), AUC 0.9993696808815002
ep37_train_time 311.7624819278717
Test Epoch37 layer4 Acc 0.9761428571428571, AUC 0.9933620095252991, avg_entr 0.0015663777012377977
ep37_l4_test_time 5.201862812042236
gc 0
Train Epoch38 Acc 0.9966232142857143 (558109/560000), AUC 0.999417781829834
ep38_train_time 313.2096652984619
Test Epoch38 layer4 Acc 0.9760857142857143, AUC 0.9931162595748901, avg_entr 0.001480115344747901
ep38_l4_test_time 5.2598724365234375
gc 0
Train Epoch39 Acc 0.9966678571428571 (558134/560000), AUC 0.9994244575500488
ep39_train_time 313.31731510162354
Test Epoch39 layer4 Acc 0.9752857142857143, AUC 0.993009626865387, avg_entr 0.0014922043774276972
ep39_l4_test_time 5.279243469238281
gc 0
Train Epoch40 Acc 0.9968125 (558215/560000), AUC 0.999397337436676
ep40_train_time 313.2722144126892
Test Epoch40 layer4 Acc 0.976, AUC 0.992991030216217, avg_entr 0.0013642521807923913
ep40_l4_test_time 5.179598093032837
gc 0
Train Epoch41 Acc 0.9968928571428571 (558260/560000), AUC 0.9994463920593262
ep41_train_time 292.4275333881378
Test Epoch41 layer4 Acc 0.9756285714285714, AUC 0.9931232333183289, avg_entr 0.0013710235944017768
ep41_l4_test_time 5.212190628051758
gc 0
Train Epoch42 Acc 0.9970017857142857 (558321/560000), AUC 0.9994483590126038
ep42_train_time 313.0043168067932
Test Epoch42 layer4 Acc 0.9760857142857143, AUC 0.9931439161300659, avg_entr 0.0014878676738590002
ep42_l4_test_time 5.167801141738892
gc 0
Train Epoch43 Acc 0.9971142857142857 (558384/560000), AUC 0.9994437098503113
ep43_train_time 313.16523337364197
Test Epoch43 layer4 Acc 0.9765428571428572, AUC 0.9930400848388672, avg_entr 0.0015305450651794672
ep43_l4_test_time 5.115959882736206
gc 0
Train Epoch44 Acc 0.9970357142857142 (558340/560000), AUC 0.9994620680809021
ep44_train_time 313.17531514167786
Test Epoch44 layer4 Acc 0.9762857142857143, AUC 0.9934531450271606, avg_entr 0.0015627184184268117
ep44_l4_test_time 5.23249077796936
gc 0
Train Epoch45 Acc 0.9971857142857142 (558424/560000), AUC 0.9994747042655945
ep45_train_time 294.47882199287415
Test Epoch45 layer4 Acc 0.9756, AUC 0.9923538565635681, avg_entr 0.001491546630859375
ep45_l4_test_time 5.162896633148193
gc 0
Train Epoch46 Acc 0.9972535714285714 (558462/560000), AUC 0.9994827508926392
ep46_train_time 312.84803915023804
Test Epoch46 layer4 Acc 0.9751142857142857, AUC 0.99269038438797, avg_entr 0.001780049060471356
ep46_l4_test_time 5.1950459480285645
gc 0
Train Epoch47 Acc 0.9973517857142857 (558517/560000), AUC 0.9995023608207703
ep47_train_time 313.26497745513916
Test Epoch47 layer4 Acc 0.9755714285714285, AUC 0.9925022721290588, avg_entr 0.001275162328965962
ep47_l4_test_time 5.1576244831085205
gc 0
Train Epoch48 Acc 0.9973785714285714 (558532/560000), AUC 0.9994908571243286
ep48_train_time 312.8583607673645
Test Epoch48 layer4 Acc 0.9751714285714286, AUC 0.9926295280456543, avg_entr 0.001681991619989276
ep48_l4_test_time 5.133814096450806
gc 0
Train Epoch49 Acc 0.9974017857142857 (558545/560000), AUC 0.9994987845420837
ep49_train_time 313.2760212421417
Test Epoch49 layer4 Acc 0.9760857142857143, AUC 0.9924803972244263, avg_entr 0.0015012783696874976
ep49_l4_test_time 5.143157005310059
Best AUC 0.9980305433273315
train_loss (2, 5, 50)
valid_acc (1, 50)
valid_AUC (1, 50)
train_acc (50,)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
total_train+valid_time 15380.71143579483
Start Testing
Load ckpt at ckpt/dbpedia_14_transformeral_l5base_pad60//dbpedia_14_transformeral_l5.pt
Test layer4 Acc 0.9796, AUC 0.9979463815689087, avg_entr 0.005522694438695908
l4_test_time 5.187337875366211
