total count words 102019
vocab size 30000
found 26754 words in glove
Load ckpt from ckpt/ag_news_transformeral_l5_pad175_m3_fix//ag_news_transformeral_l5_prefix.pt
model: TransformerModelML(
  (layers): ModuleList(
    (0): EMBLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Embedding(30000, 300)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=4, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=4, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.weight 9000000
layers.0.ae.g.0.weight 512
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 512
layers.0.ae.h.0.bias 4
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
Total Trainable Params: 13672292
Start Training
train_mask {0, 1, 2, 3}
gc 9
Train Epoch0 Acc 0.249875 (29985/120000), AUC 0.2893231213092804
ep0_train_time 70.53358364105225
Test Epoch0 threshold 0.1 Acc 0.9184210526315789, AUC 0.9801546335220337, avg_entr 0.008255070075392723
ep0_t0.1_test_time 0.4074535369873047
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m4_fix//ag_news_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.2 Acc 0.9178947368421052, AUC 0.981158435344696, avg_entr 0.013310800306499004
ep0_t0.2_test_time 0.38083553314208984
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m4_fix//ag_news_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.3 Acc 0.9175, AUC 0.9817988276481628, avg_entr 0.022794578224420547
ep0_t0.3_test_time 0.3401963710784912
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m4_fix//ag_news_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.4 Acc 0.9173684210526316, AUC 0.9819415211677551, avg_entr 0.024205103516578674
ep0_t0.4_test_time 0.3208770751953125
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m4_fix//ag_news_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.5 Acc 0.9173684210526316, AUC 0.9819750189781189, avg_entr 0.025195125490427017
ep0_t0.5_test_time 0.314312219619751
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m4_fix//ag_news_transformeral_l5_prefix.pt  ,ep 0
Test Epoch0 threshold 0.6 Acc 0.9172368421052631, AUC 0.9819681644439697, avg_entr 0.025288013741374016
ep0_t0.6_test_time 0.3121507167816162
Test Epoch0 threshold 0.7 Acc 0.9172368421052631, AUC 0.9819681644439697, avg_entr 0.025288013741374016
ep0_t0.7_test_time 0.31063342094421387
Test Epoch0 threshold 0.8 Acc 0.9172368421052631, AUC 0.9819681644439697, avg_entr 0.025288013741374016
ep0_t0.8_test_time 0.3087494373321533
Test Epoch0 threshold 0.9 Acc 0.9172368421052631, AUC 0.9819681644439697, avg_entr 0.025288013741374016
ep0_t0.9_test_time 0.30844545364379883
gc 0
Train Epoch1 Acc 0.24998333333333334 (29998/120000), AUC 0.272378146648407
ep1_train_time 70.16383004188538
Test Epoch1 threshold 0.1 Acc 0.9180263157894737, AUC 0.9800651669502258, avg_entr 0.008188682608306408
ep1_t0.1_test_time 0.40078234672546387
Test Epoch1 threshold 0.2 Acc 0.9184210526315789, AUC 0.9813007116317749, avg_entr 0.012754200957715511
ep1_t0.2_test_time 0.37273287773132324
Test Epoch1 threshold 0.3 Acc 0.9180263157894737, AUC 0.9819509387016296, avg_entr 0.022760514169931412
ep1_t0.3_test_time 0.3364262580871582
Test Epoch1 threshold 0.4 Acc 0.9176315789473685, AUC 0.9820839166641235, avg_entr 0.024226674810051918
ep1_t0.4_test_time 0.32074689865112305
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m4_fix//ag_news_transformeral_l5_prefix.pt  ,ep 1
Test Epoch1 threshold 0.5 Acc 0.9175, AUC 0.9821257591247559, avg_entr 0.025275470688939095
ep1_t0.5_test_time 0.313385009765625
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m4_fix//ag_news_transformeral_l5_prefix.pt  ,ep 1
Test Epoch1 threshold 0.6 Acc 0.9175, AUC 0.9821193218231201, avg_entr 0.025341806933283806
ep1_t0.6_test_time 0.3139047622680664
Test Epoch1 threshold 0.7 Acc 0.9175, AUC 0.9821193218231201, avg_entr 0.025341806933283806
ep1_t0.7_test_time 0.31061291694641113
Test Epoch1 threshold 0.8 Acc 0.9175, AUC 0.9821193218231201, avg_entr 0.025341806933283806
ep1_t0.8_test_time 0.3091568946838379
Test Epoch1 threshold 0.9 Acc 0.9175, AUC 0.9821193218231201, avg_entr 0.025341806933283806
ep1_t0.9_test_time 0.30822229385375977
gc 0
Train Epoch2 Acc 0.24998333333333334 (29998/120000), AUC 0.27169257402420044
ep2_train_time 70.47577142715454
Test Epoch2 threshold 0.1 Acc 0.9173684210526316, AUC 0.9796957969665527, avg_entr 0.008215408772230148
ep2_t0.1_test_time 0.4054231643676758
Test Epoch2 threshold 0.2 Acc 0.9185526315789474, AUC 0.9813833832740784, avg_entr 0.012999974191188812
ep2_t0.2_test_time 0.37548828125
Test Epoch2 threshold 0.3 Acc 0.9184210526315789, AUC 0.9820224046707153, avg_entr 0.022335002198815346
ep2_t0.3_test_time 0.33696770668029785
Test Epoch2 threshold 0.4 Acc 0.9186842105263158, AUC 0.9821093082427979, avg_entr 0.023773012682795525
ep2_t0.4_test_time 0.3219718933105469
Test Epoch2 threshold 0.5 Acc 0.9182894736842105, AUC 0.9821621179580688, avg_entr 0.024765731766819954
ep2_t0.5_test_time 0.313309907913208
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m4_fix//ag_news_transformeral_l5_prefix.pt  ,ep 2
Test Epoch2 threshold 0.6 Acc 0.9185526315789474, AUC 0.9821884632110596, avg_entr 0.025118011981248856
ep2_t0.6_test_time 0.3133237361907959
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m4_fix//ag_news_transformeral_l5_prefix.pt  ,ep 2
Test Epoch2 threshold 0.7 Acc 0.9185526315789474, AUC 0.9821884632110596, avg_entr 0.025118011981248856
ep2_t0.7_test_time 0.31279706954956055
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m4_fix//ag_news_transformeral_l5_prefix.pt  ,ep 2
Test Epoch2 threshold 0.8 Acc 0.9185526315789474, AUC 0.9821884632110596, avg_entr 0.025118011981248856
ep2_t0.8_test_time 0.31045055389404297
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m4_fix//ag_news_transformeral_l5_prefix.pt  ,ep 2
Test Epoch2 threshold 0.9 Acc 0.9185526315789474, AUC 0.9821884632110596, avg_entr 0.025118011981248856
ep2_t0.9_test_time 0.3129770755767822
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m4_fix//ag_news_transformeral_l5_prefix.pt  ,ep 2
gc 0
Train Epoch3 Acc 0.24999166666666667 (29999/120000), AUC 0.2758195996284485
ep3_train_time 70.32998728752136
Test Epoch3 threshold 0.1 Acc 0.9167105263157894, AUC 0.9797438979148865, avg_entr 0.008380737155675888
ep3_t0.1_test_time 0.40418267250061035
Test Epoch3 threshold 0.2 Acc 0.9177631578947368, AUC 0.9811686277389526, avg_entr 0.012924796901643276
ep3_t0.2_test_time 0.3766942024230957
Test Epoch3 threshold 0.3 Acc 0.9185526315789474, AUC 0.9820864200592041, avg_entr 0.022565696388483047
ep3_t0.3_test_time 0.33953428268432617
Test Epoch3 threshold 0.4 Acc 0.9186842105263158, AUC 0.9821672439575195, avg_entr 0.02394109219312668
ep3_t0.4_test_time 0.3210256099700928
Test Epoch3 threshold 0.5 Acc 0.9182894736842105, AUC 0.9822009205818176, avg_entr 0.02488075941801071
ep3_t0.5_test_time 0.310060977935791
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m4_fix//ag_news_transformeral_l5_prefix.pt  ,ep 3
Test Epoch3 threshold 0.6 Acc 0.9181578947368421, AUC 0.9822094440460205, avg_entr 0.025214601308107376
ep3_t0.6_test_time 0.3139650821685791
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m4_fix//ag_news_transformeral_l5_prefix.pt  ,ep 3
Test Epoch3 threshold 0.7 Acc 0.9181578947368421, AUC 0.9822094440460205, avg_entr 0.025214601308107376
ep3_t0.7_test_time 0.31296277046203613
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m4_fix//ag_news_transformeral_l5_prefix.pt  ,ep 3
Test Epoch3 threshold 0.8 Acc 0.9181578947368421, AUC 0.9822094440460205, avg_entr 0.025214601308107376
ep3_t0.8_test_time 0.312183141708374
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m4_fix//ag_news_transformeral_l5_prefix.pt  ,ep 3
Test Epoch3 threshold 0.9 Acc 0.9181578947368421, AUC 0.9822094440460205, avg_entr 0.025214601308107376
ep3_t0.9_test_time 0.3124725818634033
Save ckpt to ckpt/ag_news_transformeral_l5_pad175_m4_fix//ag_news_transformeral_l5_prefix.pt  ,ep 3
gc 0
Train Epoch4 Acc 0.25 (30000/120000), AUC 0.27792349457740784
ep4_train_time 70.38624358177185
Test Epoch4 threshold 0.1 Acc 0.9173684210526316, AUC 0.9798429608345032, avg_entr 0.008486547507345676
ep4_t0.1_test_time 0.4073483943939209
Test Epoch4 threshold 0.2 Acc 0.9180263157894737, AUC 0.9812209606170654, avg_entr 0.012881758622825146
ep4_t0.2_test_time 0.3730814456939697
Test Epoch4 threshold 0.3 Acc 0.9188157894736843, AUC 0.9820574522018433, avg_entr 0.022521769627928734
ep4_t0.3_test_time 0.33920741081237793
Test Epoch4 threshold 0.4 Acc 0.9189473684210526, AUC 0.9821457266807556, avg_entr 0.023928213864564896
ep4_t0.4_test_time 0.32517027854919434
Test Epoch4 threshold 0.5 Acc 0.9186842105263158, AUC 0.9821715354919434, avg_entr 0.024721061810851097
ep4_t0.5_test_time 0.31276607513427734
Test Epoch4 threshold 0.6 Acc 0.9185526315789474, AUC 0.9821979999542236, avg_entr 0.02505447156727314
ep4_t0.6_test_time 0.31386828422546387
Test Epoch4 threshold 0.7 Acc 0.9185526315789474, AUC 0.9821979999542236, avg_entr 0.02505447156727314
ep4_t0.7_test_time 0.31228113174438477
Test Epoch4 threshold 0.8 Acc 0.9185526315789474, AUC 0.9821979999542236, avg_entr 0.02505447156727314
ep4_t0.8_test_time 0.3125450611114502
Test Epoch4 threshold 0.9 Acc 0.9185526315789474, AUC 0.9821979999542236, avg_entr 0.02505447156727314
ep4_t0.9_test_time 0.3119690418243408
gc 0
Train Epoch5 Acc 0.25 (30000/120000), AUC 0.2771505117416382
ep5_train_time 70.33838486671448
Test Epoch5 threshold 0.1 Acc 0.9168421052631579, AUC 0.9797521233558655, avg_entr 0.008448819629848003
ep5_t0.1_test_time 0.4074864387512207
Test Epoch5 threshold 0.2 Acc 0.9176315789473685, AUC 0.981254518032074, avg_entr 0.012841985560953617
ep5_t0.2_test_time 0.37528228759765625
Test Epoch5 threshold 0.3 Acc 0.9182894736842105, AUC 0.9820622205734253, avg_entr 0.022630175575613976
ep5_t0.3_test_time 0.3374488353729248
Test Epoch5 threshold 0.4 Acc 0.9184210526315789, AUC 0.9821527600288391, avg_entr 0.02392306551337242
ep5_t0.4_test_time 0.3253931999206543
Test Epoch5 threshold 0.5 Acc 0.9181578947368421, AUC 0.9821710586547852, avg_entr 0.024782072752714157
ep5_t0.5_test_time 0.3144075870513916
Test Epoch5 threshold 0.6 Acc 0.9180263157894737, AUC 0.982197642326355, avg_entr 0.025117279961705208
ep5_t0.6_test_time 0.3118307590484619
Test Epoch5 threshold 0.7 Acc 0.9180263157894737, AUC 0.982197642326355, avg_entr 0.025117279961705208
ep5_t0.7_test_time 0.31095409393310547
Test Epoch5 threshold 0.8 Acc 0.9180263157894737, AUC 0.982197642326355, avg_entr 0.025117279961705208
ep5_t0.8_test_time 0.3109562397003174
Test Epoch5 threshold 0.9 Acc 0.9180263157894737, AUC 0.982197642326355, avg_entr 0.025117279961705208
ep5_t0.9_test_time 0.31148624420166016
gc 0
Train Epoch6 Acc 0.25 (30000/120000), AUC 0.27762100100517273
ep6_train_time 70.29134225845337
Test Epoch6 threshold 0.1 Acc 0.9171052631578948, AUC 0.9798276424407959, avg_entr 0.00842991005629301
ep6_t0.1_test_time 0.4077157974243164
Test Epoch6 threshold 0.2 Acc 0.9177631578947368, AUC 0.9812529683113098, avg_entr 0.01284764613956213
ep6_t0.2_test_time 0.3743155002593994
Test Epoch6 threshold 0.3 Acc 0.9185526315789474, AUC 0.9820572137832642, avg_entr 0.022552596405148506
ep6_t0.3_test_time 0.3405799865722656
Test Epoch6 threshold 0.4 Acc 0.9186842105263158, AUC 0.9821509122848511, avg_entr 0.023924754932522774
ep6_t0.4_test_time 0.3275492191314697
Test Epoch6 threshold 0.5 Acc 0.9184210526315789, AUC 0.9821693897247314, avg_entr 0.024783695116639137
ep6_t0.5_test_time 0.3143351078033447
Test Epoch6 threshold 0.6 Acc 0.9182894736842105, AUC 0.9821957349777222, avg_entr 0.02511904016137123
ep6_t0.6_test_time 0.31183695793151855
Test Epoch6 threshold 0.7 Acc 0.9182894736842105, AUC 0.9821957349777222, avg_entr 0.02511904016137123
ep6_t0.7_test_time 0.3122401237487793
Test Epoch6 threshold 0.8 Acc 0.9182894736842105, AUC 0.9821957349777222, avg_entr 0.02511904016137123
ep6_t0.8_test_time 0.3118407726287842
Test Epoch6 threshold 0.9 Acc 0.9182894736842105, AUC 0.9821957349777222, avg_entr 0.02511904016137123
ep6_t0.9_test_time 0.31172728538513184
gc 0
Train Epoch7 Acc 0.24999166666666667 (29999/120000), AUC 0.2763906717300415
ep7_train_time 70.43820214271545
Test Epoch7 threshold 0.1 Acc 0.9172368421052631, AUC 0.9798306822776794, avg_entr 0.00844199676066637
ep7_t0.1_test_time 0.41637349128723145
Test Epoch7 threshold 0.2 Acc 0.9177631578947368, AUC 0.9812530875205994, avg_entr 0.012846876867115498
ep7_t0.2_test_time 0.37577342987060547
Test Epoch7 threshold 0.3 Acc 0.9185526315789474, AUC 0.9820569753646851, avg_entr 0.022551769390702248
ep7_t0.3_test_time 0.33748316764831543
Test Epoch7 threshold 0.4 Acc 0.9186842105263158, AUC 0.982150673866272, avg_entr 0.023923279717564583
ep7_t0.4_test_time 0.3240847587585449
Test Epoch7 threshold 0.5 Acc 0.9184210526315789, AUC 0.9821691513061523, avg_entr 0.0247822143137455
ep7_t0.5_test_time 0.31273770332336426
Test Epoch7 threshold 0.6 Acc 0.9182894736842105, AUC 0.9821956753730774, avg_entr 0.02511751651763916
ep7_t0.6_test_time 0.31024980545043945
Test Epoch7 threshold 0.7 Acc 0.9182894736842105, AUC 0.9821956753730774, avg_entr 0.02511751651763916
ep7_t0.7_test_time 0.31131768226623535
Test Epoch7 threshold 0.8 Acc 0.9182894736842105, AUC 0.9821956753730774, avg_entr 0.02511751651763916
ep7_t0.8_test_time 0.31182217597961426
Test Epoch7 threshold 0.9 Acc 0.9182894736842105, AUC 0.9821956753730774, avg_entr 0.02511751651763916
ep7_t0.9_test_time 0.311586856842041
gc 0
Train Epoch8 Acc 0.25 (30000/120000), AUC 0.27584993839263916
ep8_train_time 70.24961423873901
Test Epoch8 threshold 0.1 Acc 0.9172368421052631, AUC 0.979830265045166, avg_entr 0.008441691286861897
ep8_t0.1_test_time 0.4065849781036377
Test Epoch8 threshold 0.2 Acc 0.9177631578947368, AUC 0.9812526702880859, avg_entr 0.012846643105149269
ep8_t0.2_test_time 0.37154340744018555
Test Epoch8 threshold 0.3 Acc 0.9185526315789474, AUC 0.9820570945739746, avg_entr 0.02255120687186718
ep8_t0.3_test_time 0.33574891090393066
Test Epoch8 threshold 0.4 Acc 0.9186842105263158, AUC 0.982150673866272, avg_entr 0.023923110216856003
ep8_t0.4_test_time 0.32216310501098633
Test Epoch8 threshold 0.5 Acc 0.9184210526315789, AUC 0.9821692705154419, avg_entr 0.024782126769423485
ep8_t0.5_test_time 0.31154870986938477
Test Epoch8 threshold 0.6 Acc 0.9182894736842105, AUC 0.9821956157684326, avg_entr 0.025117427110671997
ep8_t0.6_test_time 0.30856943130493164
Test Epoch8 threshold 0.7 Acc 0.9182894736842105, AUC 0.9821956157684326, avg_entr 0.025117427110671997
ep8_t0.7_test_time 0.3088712692260742
Test Epoch8 threshold 0.8 Acc 0.9182894736842105, AUC 0.9821956157684326, avg_entr 0.025117427110671997
ep8_t0.8_test_time 0.310516357421875
Test Epoch8 threshold 0.9 Acc 0.9182894736842105, AUC 0.9821956157684326, avg_entr 0.025117427110671997
ep8_t0.9_test_time 0.3094468116760254
gc 0
Train Epoch9 Acc 0.25 (30000/120000), AUC 0.2765660881996155
ep9_train_time 70.26446652412415
Test Epoch9 threshold 0.1 Acc 0.9172368421052631, AUC 0.9798302054405212, avg_entr 0.008441437035799026
ep9_t0.1_test_time 0.4063422679901123
Test Epoch9 threshold 0.2 Acc 0.9177631578947368, AUC 0.9812526702880859, avg_entr 0.012846378609538078
ep9_t0.2_test_time 0.3719315528869629
Test Epoch9 threshold 0.3 Acc 0.9185526315789474, AUC 0.9820570945739746, avg_entr 0.02255108766257763
ep9_t0.3_test_time 0.33536291122436523
Test Epoch9 threshold 0.4 Acc 0.9186842105263158, AUC 0.9821507930755615, avg_entr 0.02392294444143772
ep9_t0.4_test_time 0.3224461078643799
Test Epoch9 threshold 0.5 Acc 0.9184210526315789, AUC 0.9821692109107971, avg_entr 0.02478192001581192
ep9_t0.5_test_time 0.31094908714294434
Test Epoch9 threshold 0.6 Acc 0.9182894736842105, AUC 0.9821956157684326, avg_entr 0.025117212906479836
ep9_t0.6_test_time 0.3082151412963867
Test Epoch9 threshold 0.7 Acc 0.9182894736842105, AUC 0.9821956157684326, avg_entr 0.025117212906479836
ep9_t0.7_test_time 0.31023550033569336
Test Epoch9 threshold 0.8 Acc 0.9182894736842105, AUC 0.9821956157684326, avg_entr 0.025117212906479836
ep9_t0.8_test_time 0.3087894916534424
Test Epoch9 threshold 0.9 Acc 0.9182894736842105, AUC 0.9821956157684326, avg_entr 0.025117212906479836
ep9_t0.9_test_time 0.3083457946777344
Best AUC 0.9822094440460205
train_loss (2, 5, 10)
valid_acc (10, 9)
valid_AUC (10, 9)
train_acc (10,)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Start Testing
Load ckpt at ckpt/ag_news_transformeral_l5_pad175_m4_fix//ag_news_transformeral_l5_prefix.pt
[[1705   60   87   48]
 [  13 1868    8   11]
 [  44   19 1684  153]
 [  42   14  123 1721]]
Figure(640x480)
tensor([6.7950e-04, 2.7563e-08, 1.7132e-03,  ..., 4.8153e-02, 1.2257e-07,
        2.4288e-03])
[[1713   57   71   59]
 [  19 1859   10   12]
 [  53   16 1675  156]
 [  49   11  129 1711]]
Figure(640x480)
tensor([2.9801e-06, 4.4506e-08, 3.0427e-08,  ..., 7.8179e-08, 4.7902e-08,
        4.1510e-08])
[[1713   57   71   59]
 [  24 1851   11   14]
 [  55   15 1678  152]
 [  49   10  129 1712]]
Figure(640x480)
tensor([4.0001e-07, 4.1418e-08, 4.3023e-08,  ..., 2.1458e-07, 3.9037e-08,
        4.2209e-08])
[[1711   57   72   60]
 [  25 1852   10   13]
 [  55   15 1677  153]
 [  49   10  129 1712]]
Figure(640x480)
tensor([3.1590e-07, 4.0747e-08, 4.0977e-08,  ..., 2.1931e-07, 3.8902e-08,
        4.4223e-08])
[[1900    0    0    0]
 [1900    0    0    0]
 [1900    0    0    0]
 [1900    0    0    0]]
Figure(640x480)
tensor([0.1100, 0.1588, 0.1457,  ..., 0.5301, 0.1353, 0.1323])
