total count words 102019
vocab size 30000
found 26754 words in glove
model: TransformerModelML(
  (layers): ModuleList(
    (0): EMBLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): Embedding(30000, 300)
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=4, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=4, bias=True)
          (1): Sigmoid()
        )
        (cri): CrossEntropyLoss()
      )
    )
    (1): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (2): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (3): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
    (4): TransLayer(
      (enc): ENC(
        (b): Sequential(
          (0): Linear(in_features=300, out_features=128, bias=True)
          (1): Tanh()
        )
        (f): TransformerEncoder(
          (multi_headed_attention): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=300, out_features=300, bias=True)
              (1): Linear(in_features=300, out_features=300, bias=True)
              (2): Linear(in_features=300, out_features=300, bias=True)
              (3): Linear(in_features=300, out_features=300, bias=True)
            )
            (sdpa): ScaledDotProductAttention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): FeedForward(
            (w_1): Linear(in_features=300, out_features=300, bias=True)
            (w_2): Linear(in_features=300, out_features=300, bias=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder_layer): EncoderLayer(
            (self_attn): MultiHeadAttention(
              (linears): ModuleList(
                (0): Linear(in_features=300, out_features=300, bias=True)
                (1): Linear(in_features=300, out_features=300, bias=True)
                (2): Linear(in_features=300, out_features=300, bias=True)
                (3): Linear(in_features=300, out_features=300, bias=True)
              )
              (sdpa): ScaledDotProductAttention()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): FeedForward(
              (w_1): Linear(in_features=300, out_features=300, bias=True)
              (w_2): Linear(in_features=300, out_features=300, bias=True)
              (relu): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (encoder): Encoder(
            (layers): ModuleList(
              (0): EncoderLayer(
                (self_attn): MultiHeadAttention(
                  (linears): ModuleList(
                    (0): Linear(in_features=300, out_features=300, bias=True)
                    (1): Linear(in_features=300, out_features=300, bias=True)
                    (2): Linear(in_features=300, out_features=300, bias=True)
                    (3): Linear(in_features=300, out_features=300, bias=True)
                  )
                  (sdpa): ScaledDotProductAttention()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (feed_forward): FeedForward(
                  (w_1): Linear(in_features=300, out_features=300, bias=True)
                  (w_2): Linear(in_features=300, out_features=300, bias=True)
                  (relu): ReLU()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (sublayer): ModuleList(
                  (0): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): SublayerConnection(
                    (norm): LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
            (norm): LayerNorm()
          )
        )
        (cri): MSELoss()
      )
      (ae): AE(
        (g): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): Tanh()
        )
        (h): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
        (cri): MSELoss()
      )
    )
  )
)
layers.0.enc.b.0.weight 38400
layers.0.enc.b.0.bias 128
layers.0.enc.f.weight 9000000
layers.0.ae.g.0.weight 512
layers.0.ae.g.0.bias 128
layers.0.ae.h.0.weight 512
layers.0.ae.h.0.bias 4
layers.1.enc.b.0.weight 38400
layers.1.enc.b.0.bias 128
layers.1.enc.f.multi_headed_attention.linears.0.weight 90000
layers.1.enc.f.multi_headed_attention.linears.0.bias 300
layers.1.enc.f.multi_headed_attention.linears.1.weight 90000
layers.1.enc.f.multi_headed_attention.linears.1.bias 300
layers.1.enc.f.multi_headed_attention.linears.2.weight 90000
layers.1.enc.f.multi_headed_attention.linears.2.bias 300
layers.1.enc.f.multi_headed_attention.linears.3.weight 90000
layers.1.enc.f.multi_headed_attention.linears.3.bias 300
layers.1.enc.f.feed_forward.w_1.weight 90000
layers.1.enc.f.feed_forward.w_1.bias 300
layers.1.enc.f.feed_forward.w_2.weight 90000
layers.1.enc.f.feed_forward.w_2.bias 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.1.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.1.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.1.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.1.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.1.enc.f.encoder.norm.a 300
layers.1.enc.f.encoder.norm.b 300
layers.1.ae.g.0.weight 16384
layers.1.ae.g.0.bias 128
layers.1.ae.h.0.weight 16384
layers.1.ae.h.0.bias 128
layers.2.enc.b.0.weight 38400
layers.2.enc.b.0.bias 128
layers.2.enc.f.multi_headed_attention.linears.0.weight 90000
layers.2.enc.f.multi_headed_attention.linears.0.bias 300
layers.2.enc.f.multi_headed_attention.linears.1.weight 90000
layers.2.enc.f.multi_headed_attention.linears.1.bias 300
layers.2.enc.f.multi_headed_attention.linears.2.weight 90000
layers.2.enc.f.multi_headed_attention.linears.2.bias 300
layers.2.enc.f.multi_headed_attention.linears.3.weight 90000
layers.2.enc.f.multi_headed_attention.linears.3.bias 300
layers.2.enc.f.feed_forward.w_1.weight 90000
layers.2.enc.f.feed_forward.w_1.bias 300
layers.2.enc.f.feed_forward.w_2.weight 90000
layers.2.enc.f.feed_forward.w_2.bias 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.2.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.2.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.2.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.2.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.2.enc.f.encoder.norm.a 300
layers.2.enc.f.encoder.norm.b 300
layers.2.ae.g.0.weight 16384
layers.2.ae.g.0.bias 128
layers.2.ae.h.0.weight 16384
layers.2.ae.h.0.bias 128
layers.3.enc.b.0.weight 38400
layers.3.enc.b.0.bias 128
layers.3.enc.f.multi_headed_attention.linears.0.weight 90000
layers.3.enc.f.multi_headed_attention.linears.0.bias 300
layers.3.enc.f.multi_headed_attention.linears.1.weight 90000
layers.3.enc.f.multi_headed_attention.linears.1.bias 300
layers.3.enc.f.multi_headed_attention.linears.2.weight 90000
layers.3.enc.f.multi_headed_attention.linears.2.bias 300
layers.3.enc.f.multi_headed_attention.linears.3.weight 90000
layers.3.enc.f.multi_headed_attention.linears.3.bias 300
layers.3.enc.f.feed_forward.w_1.weight 90000
layers.3.enc.f.feed_forward.w_1.bias 300
layers.3.enc.f.feed_forward.w_2.weight 90000
layers.3.enc.f.feed_forward.w_2.bias 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.3.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.3.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.3.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.3.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.3.enc.f.encoder.norm.a 300
layers.3.enc.f.encoder.norm.b 300
layers.3.ae.g.0.weight 16384
layers.3.ae.g.0.bias 128
layers.3.ae.h.0.weight 16384
layers.3.ae.h.0.bias 128
layers.4.enc.b.0.weight 38400
layers.4.enc.b.0.bias 128
layers.4.enc.f.multi_headed_attention.linears.0.weight 90000
layers.4.enc.f.multi_headed_attention.linears.0.bias 300
layers.4.enc.f.multi_headed_attention.linears.1.weight 90000
layers.4.enc.f.multi_headed_attention.linears.1.bias 300
layers.4.enc.f.multi_headed_attention.linears.2.weight 90000
layers.4.enc.f.multi_headed_attention.linears.2.bias 300
layers.4.enc.f.multi_headed_attention.linears.3.weight 90000
layers.4.enc.f.multi_headed_attention.linears.3.bias 300
layers.4.enc.f.feed_forward.w_1.weight 90000
layers.4.enc.f.feed_forward.w_1.bias 300
layers.4.enc.f.feed_forward.w_2.weight 90000
layers.4.enc.f.feed_forward.w_2.bias 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.0.norm.b 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.a 300
layers.4.enc.f.encoder_layer.sublayer.1.norm.b 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.0.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.1.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.2.bias 300
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.weight 90000
layers.4.enc.f.encoder.layers.0.self_attn.linears.3.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_1.bias 300
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.weight 90000
layers.4.enc.f.encoder.layers.0.feed_forward.w_2.bias 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.0.norm.b 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.a 300
layers.4.enc.f.encoder.layers.0.sublayer.1.norm.b 300
layers.4.enc.f.encoder.norm.a 300
layers.4.enc.f.encoder.norm.b 300
layers.4.ae.g.0.weight 16384
layers.4.ae.g.0.bias 128
layers.4.ae.h.0.weight 16384
layers.4.ae.h.0.bias 128
Total Trainable Params: 13672292
Start Training
gc 0
Train Epoch0 Acc 0.6286833333333334 (75442/120000), AUC 0.8575506806373596
Test Epoch0 layer0 Acc 0.9032894736842105, AUC 0.9767606258392334, avg_entr 0.2401643544435501
Save ckpt to ckpt/ag_news_transformeral_l5_pad125//ag_news_transformeral_l5.pt  ,ep 0
Test Epoch0 layer1 Acc 0.9076315789473685, AUC 0.9787724614143372, avg_entr 0.16411402821540833
Save ckpt to ckpt/ag_news_transformeral_l5_pad125//ag_news_transformeral_l5.pt  ,ep 0
Test Epoch0 layer2 Acc 0.9073684210526316, AUC 0.9789376258850098, avg_entr 0.15327388048171997
Save ckpt to ckpt/ag_news_transformeral_l5_pad125//ag_news_transformeral_l5.pt  ,ep 0
Test Epoch0 layer3 Acc 0.9072368421052631, AUC 0.9790862798690796, avg_entr 0.1497287005186081
Save ckpt to ckpt/ag_news_transformeral_l5_pad125//ag_news_transformeral_l5.pt  ,ep 0
Test Epoch0 layer4 Acc 0.9034210526315789, AUC 0.9790600538253784, avg_entr 0.15397466719150543
gc 0
Train Epoch1 Acc 0.9217916666666667 (110615/120000), AUC 0.9820014238357544
Test Epoch1 layer0 Acc 0.9144736842105263, AUC 0.979777455329895, avg_entr 0.14462023973464966
Save ckpt to ckpt/ag_news_transformeral_l5_pad125//ag_news_transformeral_l5.pt  ,ep 1
Test Epoch1 layer1 Acc 0.9196052631578947, AUC 0.9817787408828735, avg_entr 0.0826643705368042
Save ckpt to ckpt/ag_news_transformeral_l5_pad125//ag_news_transformeral_l5.pt  ,ep 1
Test Epoch1 layer2 Acc 0.9197368421052632, AUC 0.9814252257347107, avg_entr 0.07095518708229065
Test Epoch1 layer3 Acc 0.9196052631578947, AUC 0.9818140268325806, avg_entr 0.06621511280536652
Save ckpt to ckpt/ag_news_transformeral_l5_pad125//ag_news_transformeral_l5.pt  ,ep 1
Test Epoch1 layer4 Acc 0.9181578947368421, AUC 0.9818006157875061, avg_entr 0.06635008752346039
gc 0
Train Epoch2 Acc 0.9367333333333333 (112408/120000), AUC 0.9873165488243103
Test Epoch2 layer0 Acc 0.9139473684210526, AUC 0.9810203313827515, avg_entr 0.11291851103305817
Test Epoch2 layer1 Acc 0.9213157894736842, AUC 0.9823954105377197, avg_entr 0.045246634632349014
Save ckpt to ckpt/ag_news_transformeral_l5_pad125//ag_news_transformeral_l5.pt  ,ep 2
Test Epoch2 layer2 Acc 0.9203947368421053, AUC 0.9830345511436462, avg_entr 0.038530897349119186
Save ckpt to ckpt/ag_news_transformeral_l5_pad125//ag_news_transformeral_l5.pt  ,ep 2
Test Epoch2 layer3 Acc 0.9203947368421053, AUC 0.9833610653877258, avg_entr 0.0364459864795208
Save ckpt to ckpt/ag_news_transformeral_l5_pad125//ag_news_transformeral_l5.pt  ,ep 2
Test Epoch2 layer4 Acc 0.9197368421052632, AUC 0.9830176830291748, avg_entr 0.03697892278432846
gc 0
Train Epoch3 Acc 0.944975 (113397/120000), AUC 0.9894443154335022
Test Epoch3 layer0 Acc 0.9169736842105263, AUC 0.9816940426826477, avg_entr 0.09455063194036484
Test Epoch3 layer1 Acc 0.9214473684210527, AUC 0.9810275435447693, avg_entr 0.034601841121912
Test Epoch3 layer2 Acc 0.9221052631578948, AUC 0.9818212389945984, avg_entr 0.02864857204258442
Test Epoch3 layer3 Acc 0.9213157894736842, AUC 0.9826035499572754, avg_entr 0.02639845758676529
Test Epoch3 layer4 Acc 0.920921052631579, AUC 0.9820892810821533, avg_entr 0.02500799112021923
gc 0
Train Epoch4 Acc 0.94975 (113970/120000), AUC 0.9908050298690796
Test Epoch4 layer0 Acc 0.9173684210526316, AUC 0.981738269329071, avg_entr 0.08318420499563217
Test Epoch4 layer1 Acc 0.9197368421052632, AUC 0.979295015335083, avg_entr 0.03002564236521721
Test Epoch4 layer2 Acc 0.9184210526315789, AUC 0.9812687635421753, avg_entr 0.025178212672472
Test Epoch4 layer3 Acc 0.9190789473684211, AUC 0.9828320145606995, avg_entr 0.022935574874281883
Test Epoch4 layer4 Acc 0.9185526315789474, AUC 0.982075035572052, avg_entr 0.02154429815709591
gc 0
Train Epoch5 Acc 0.9536083333333333 (114433/120000), AUC 0.9918410181999207
Test Epoch5 layer0 Acc 0.9172368421052631, AUC 0.9817690849304199, avg_entr 0.07378645241260529
Test Epoch5 layer1 Acc 0.9186842105263158, AUC 0.978179395198822, avg_entr 0.026400228962302208
Test Epoch5 layer2 Acc 0.9186842105263158, AUC 0.9803107976913452, avg_entr 0.02072858437895775
Test Epoch5 layer3 Acc 0.9181578947368421, AUC 0.9815288782119751, avg_entr 0.01833067461848259
Test Epoch5 layer4 Acc 0.9184210526315789, AUC 0.9804237484931946, avg_entr 0.016400547698140144
gc 0
Train Epoch6 Acc 0.9569583333333334 (114835/120000), AUC 0.9925130605697632
Test Epoch6 layer0 Acc 0.9185526315789474, AUC 0.9815453290939331, avg_entr 0.06790529191493988
Test Epoch6 layer1 Acc 0.9190789473684211, AUC 0.9775950312614441, avg_entr 0.023752178996801376
Test Epoch6 layer2 Acc 0.9188157894736843, AUC 0.977942168712616, avg_entr 0.018618948757648468
Test Epoch6 layer3 Acc 0.9188157894736843, AUC 0.9808117747306824, avg_entr 0.016706161201000214
Test Epoch6 layer4 Acc 0.9184210526315789, AUC 0.9799227714538574, avg_entr 0.015355193056166172
gc 0
Train Epoch7 Acc 0.959925 (115191/120000), AUC 0.9933015704154968
Test Epoch7 layer0 Acc 0.9176315789473685, AUC 0.981501042842865, avg_entr 0.06389768421649933
Test Epoch7 layer1 Acc 0.9173684210526316, AUC 0.9782297611236572, avg_entr 0.02303267829120159
Test Epoch7 layer2 Acc 0.9167105263157894, AUC 0.9798035621643066, avg_entr 0.018131982535123825
Test Epoch7 layer3 Acc 0.9168421052631579, AUC 0.9807089567184448, avg_entr 0.016064094379544258
Test Epoch7 layer4 Acc 0.9169736842105263, AUC 0.9809529781341553, avg_entr 0.014874927699565887
gc 0
Train Epoch8 Acc 0.9633916666666666 (115607/120000), AUC 0.9942857623100281
Test Epoch8 layer0 Acc 0.9171052631578948, AUC 0.9814509153366089, avg_entr 0.06263332068920135
Test Epoch8 layer1 Acc 0.915, AUC 0.9764995574951172, avg_entr 0.02192636951804161
Test Epoch8 layer2 Acc 0.9151315789473684, AUC 0.977893590927124, avg_entr 0.01687530428171158
Test Epoch8 layer3 Acc 0.9148684210526316, AUC 0.9791899919509888, avg_entr 0.014417404308915138
Test Epoch8 layer4 Acc 0.9144736842105263, AUC 0.979008138179779, avg_entr 0.013227051123976707
gc 0
Train Epoch9 Acc 0.964575 (115749/120000), AUC 0.9944666624069214
Test Epoch9 layer0 Acc 0.916578947368421, AUC 0.9813084602355957, avg_entr 0.05847116559743881
Test Epoch9 layer1 Acc 0.9148684210526316, AUC 0.9757341742515564, avg_entr 0.021850166842341423
Test Epoch9 layer2 Acc 0.9148684210526316, AUC 0.9769501686096191, avg_entr 0.01730331778526306
Test Epoch9 layer3 Acc 0.9148684210526316, AUC 0.9766983985900879, avg_entr 0.015214354731142521
Test Epoch9 layer4 Acc 0.9146052631578947, AUC 0.9782648086547852, avg_entr 0.01414084155112505
gc 0
Train Epoch10 Acc 0.9657916666666667 (115895/120000), AUC 0.9945617318153381
Test Epoch10 layer0 Acc 0.915921052631579, AUC 0.9811582565307617, avg_entr 0.05548124387860298
Test Epoch10 layer1 Acc 0.9147368421052632, AUC 0.9750497937202454, avg_entr 0.019268007948994637
Test Epoch10 layer2 Acc 0.9147368421052632, AUC 0.977135181427002, avg_entr 0.014077282510697842
Test Epoch10 layer3 Acc 0.9144736842105263, AUC 0.9780032634735107, avg_entr 0.011920624412596226
Test Epoch10 layer4 Acc 0.9144736842105263, AUC 0.9801242351531982, avg_entr 0.011023111641407013
gc 0
Train Epoch11 Acc 0.9670416666666667 (116045/120000), AUC 0.9953961968421936
Test Epoch11 layer0 Acc 0.9164473684210527, AUC 0.9810280799865723, avg_entr 0.05479658395051956
Test Epoch11 layer1 Acc 0.9146052631578947, AUC 0.9743865728378296, avg_entr 0.01896333135664463
Test Epoch11 layer2 Acc 0.9139473684210526, AUC 0.9749692678451538, avg_entr 0.013716178946197033
Test Epoch11 layer3 Acc 0.9142105263157895, AUC 0.9776341319084167, avg_entr 0.011673945002257824
Test Epoch11 layer4 Acc 0.9140789473684211, AUC 0.9774884581565857, avg_entr 0.010901364497840405
gc 0
Train Epoch12 Acc 0.9686333333333333 (116236/120000), AUC 0.9953163266181946
Test Epoch12 layer0 Acc 0.9163157894736842, AUC 0.9809688925743103, avg_entr 0.0522342175245285
Test Epoch12 layer1 Acc 0.9142105263157895, AUC 0.9740569591522217, avg_entr 0.01886828988790512
Test Epoch12 layer2 Acc 0.9140789473684211, AUC 0.9744207859039307, avg_entr 0.013189592398703098
Test Epoch12 layer3 Acc 0.9136842105263158, AUC 0.9768857955932617, avg_entr 0.011036786250770092
Test Epoch12 layer4 Acc 0.9138157894736842, AUC 0.9780939817428589, avg_entr 0.009938715025782585
gc 0
Train Epoch13 Acc 0.9691 (116292/120000), AUC 0.9956173300743103
Test Epoch13 layer0 Acc 0.9157894736842105, AUC 0.9809551239013672, avg_entr 0.05063238739967346
Test Epoch13 layer1 Acc 0.9134210526315789, AUC 0.9730295538902283, avg_entr 0.01849096082150936
Test Epoch13 layer2 Acc 0.9132894736842105, AUC 0.9727751016616821, avg_entr 0.013166476041078568
Test Epoch13 layer3 Acc 0.9134210526315789, AUC 0.9729918837547302, avg_entr 0.0111110620200634
Test Epoch13 layer4 Acc 0.9128947368421053, AUC 0.9749191403388977, avg_entr 0.010029803961515427
gc 0
Train Epoch14 Acc 0.9698666666666667 (116384/120000), AUC 0.9957091808319092
Test Epoch14 layer0 Acc 0.9152631578947369, AUC 0.9808341264724731, avg_entr 0.04875866696238518
Test Epoch14 layer1 Acc 0.9128947368421053, AUC 0.9733982682228088, avg_entr 0.017806274816393852
Test Epoch14 layer2 Acc 0.9125, AUC 0.9745047092437744, avg_entr 0.01303804013878107
Test Epoch14 layer3 Acc 0.9130263157894737, AUC 0.975735604763031, avg_entr 0.011011810973286629
Test Epoch14 layer4 Acc 0.9125, AUC 0.9763023853302002, avg_entr 0.009941828437149525
gc 0
Train Epoch15 Acc 0.9702416666666667 (116429/120000), AUC 0.99595046043396
Test Epoch15 layer0 Acc 0.9153947368421053, AUC 0.9808000922203064, avg_entr 0.046754591166973114
Test Epoch15 layer1 Acc 0.9131578947368421, AUC 0.9730467796325684, avg_entr 0.01718735322356224
Test Epoch15 layer2 Acc 0.9131578947368421, AUC 0.9735138416290283, avg_entr 0.012027688324451447
Test Epoch15 layer3 Acc 0.9126315789473685, AUC 0.9757458567619324, avg_entr 0.010265286080539227
Test Epoch15 layer4 Acc 0.9130263157894737, AUC 0.976887583732605, avg_entr 0.00940819550305605
gc 0
Train Epoch16 Acc 0.9712833333333334 (116554/120000), AUC 0.9960741400718689
Test Epoch16 layer0 Acc 0.9151315789473684, AUC 0.980827808380127, avg_entr 0.04662293195724487
Test Epoch16 layer1 Acc 0.9132894736842105, AUC 0.9730820655822754, avg_entr 0.016869880259037018
Test Epoch16 layer2 Acc 0.9126315789473685, AUC 0.9730608463287354, avg_entr 0.011894380673766136
Test Epoch16 layer3 Acc 0.9126315789473685, AUC 0.974144458770752, avg_entr 0.00987341906875372
Test Epoch16 layer4 Acc 0.9127631578947368, AUC 0.9755147099494934, avg_entr 0.008969205431640148
gc 0
Train Epoch17 Acc 0.9717666666666667 (116612/120000), AUC 0.9959566593170166
Test Epoch17 layer0 Acc 0.9155263157894736, AUC 0.9808129072189331, avg_entr 0.04519766941666603
Test Epoch17 layer1 Acc 0.9136842105263158, AUC 0.973617434501648, avg_entr 0.016700390726327896
Test Epoch17 layer2 Acc 0.9125, AUC 0.9742030501365662, avg_entr 0.011894422583281994
Test Epoch17 layer3 Acc 0.9132894736842105, AUC 0.9748924374580383, avg_entr 0.01000730786472559
Test Epoch17 layer4 Acc 0.9128947368421053, AUC 0.9763311147689819, avg_entr 0.009153676219284534
gc 0
Train Epoch18 Acc 0.9716666666666667 (116600/120000), AUC 0.9960001707077026
Test Epoch18 layer0 Acc 0.9151315789473684, AUC 0.9807858467102051, avg_entr 0.04385813698172569
Test Epoch18 layer1 Acc 0.9128947368421053, AUC 0.9731265306472778, avg_entr 0.016383100301027298
Test Epoch18 layer2 Acc 0.9111842105263158, AUC 0.9738388061523438, avg_entr 0.011572153307497501
Test Epoch18 layer3 Acc 0.9114473684210527, AUC 0.9753340482711792, avg_entr 0.009754291735589504
Test Epoch18 layer4 Acc 0.9106578947368421, AUC 0.9752994775772095, avg_entr 0.00878719799220562
gc 0
Train Epoch19 Acc 0.972175 (116661/120000), AUC 0.996239423751831
Test Epoch19 layer0 Acc 0.9155263157894736, AUC 0.980793833732605, avg_entr 0.04283212497830391
Test Epoch19 layer1 Acc 0.9138157894736842, AUC 0.9727264642715454, avg_entr 0.015782972797751427
Test Epoch19 layer2 Acc 0.9117105263157895, AUC 0.9728392362594604, avg_entr 0.011238167993724346
Test Epoch19 layer3 Acc 0.9122368421052631, AUC 0.9742213487625122, avg_entr 0.009406693279743195
Test Epoch19 layer4 Acc 0.9118421052631579, AUC 0.9743080139160156, avg_entr 0.008580888621509075
gc 0
Train Epoch20 Acc 0.9728083333333334 (116737/120000), AUC 0.9962987303733826
Test Epoch20 layer0 Acc 0.9151315789473684, AUC 0.9807905554771423, avg_entr 0.041619908064603806
Test Epoch20 layer1 Acc 0.9125, AUC 0.9729743599891663, avg_entr 0.015590265393257141
Test Epoch20 layer2 Acc 0.9113157894736842, AUC 0.973129153251648, avg_entr 0.010969248600304127
Test Epoch20 layer3 Acc 0.9111842105263158, AUC 0.9730547666549683, avg_entr 0.009011800400912762
Test Epoch20 layer4 Acc 0.9110526315789473, AUC 0.9740939736366272, avg_entr 0.008291390724480152
gc 0
Train Epoch21 Acc 0.9729416666666667 (116753/120000), AUC 0.9962354302406311
Test Epoch21 layer0 Acc 0.9157894736842105, AUC 0.9807915687561035, avg_entr 0.040825825184583664
Test Epoch21 layer1 Acc 0.9126315789473685, AUC 0.972810685634613, avg_entr 0.015374301932752132
Test Epoch21 layer2 Acc 0.9113157894736842, AUC 0.9731408357620239, avg_entr 0.010956824757158756
Test Epoch21 layer3 Acc 0.9117105263157895, AUC 0.9739292860031128, avg_entr 0.009196154773235321
Test Epoch21 layer4 Acc 0.9117105263157895, AUC 0.97379070520401, avg_entr 0.008333449251949787
gc 0
Train Epoch22 Acc 0.97305 (116766/120000), AUC 0.9961967468261719
Test Epoch22 layer0 Acc 0.915, AUC 0.9807883501052856, avg_entr 0.03992840275168419
Test Epoch22 layer1 Acc 0.9123684210526316, AUC 0.9728291630744934, avg_entr 0.015132780186831951
Test Epoch22 layer2 Acc 0.9110526315789473, AUC 0.9733418822288513, avg_entr 0.01046678051352501
Test Epoch22 layer3 Acc 0.9111842105263158, AUC 0.9740147590637207, avg_entr 0.008642226457595825
Test Epoch22 layer4 Acc 0.9110526315789473, AUC 0.9736289978027344, avg_entr 0.007969298399984837
gc 0
Train Epoch23 Acc 0.973025 (116763/120000), AUC 0.9963700175285339
Test Epoch23 layer0 Acc 0.9152631578947369, AUC 0.9807944297790527, avg_entr 0.03922712057828903
Test Epoch23 layer1 Acc 0.9126315789473685, AUC 0.9726938605308533, avg_entr 0.014827035367488861
Test Epoch23 layer2 Acc 0.9105263157894737, AUC 0.9725825190544128, avg_entr 0.010364051908254623
Test Epoch23 layer3 Acc 0.9110526315789473, AUC 0.9730513095855713, avg_entr 0.00866098701953888
Test Epoch23 layer4 Acc 0.9110526315789473, AUC 0.9732455015182495, avg_entr 0.007847060449421406
gc 0
Train Epoch24 Acc 0.9734416666666666 (116813/120000), AUC 0.996385931968689
Test Epoch24 layer0 Acc 0.9146052631578947, AUC 0.9807995557785034, avg_entr 0.03911619260907173
Test Epoch24 layer1 Acc 0.9126315789473685, AUC 0.972741961479187, avg_entr 0.014725172892212868
Test Epoch24 layer2 Acc 0.9103947368421053, AUC 0.9726488590240479, avg_entr 0.010206029750406742
Test Epoch24 layer3 Acc 0.9105263157894737, AUC 0.9730515480041504, avg_entr 0.008496228605508804
Test Epoch24 layer4 Acc 0.9102631578947369, AUC 0.9735220670700073, avg_entr 0.007670472841709852
gc 0
Train Epoch25 Acc 0.973275 (116793/120000), AUC 0.9964154362678528
Test Epoch25 layer0 Acc 0.915, AUC 0.980790376663208, avg_entr 0.038849182426929474
Test Epoch25 layer1 Acc 0.9121052631578948, AUC 0.9727537035942078, avg_entr 0.014535278081893921
Test Epoch25 layer2 Acc 0.9103947368421053, AUC 0.9728412628173828, avg_entr 0.009822268038988113
Test Epoch25 layer3 Acc 0.9105263157894737, AUC 0.9730995893478394, avg_entr 0.007970772683620453
Test Epoch25 layer4 Acc 0.9105263157894737, AUC 0.9729592204093933, avg_entr 0.007370786275714636
gc 0
Train Epoch26 Acc 0.973525 (116823/120000), AUC 0.9964001178741455
Test Epoch26 layer0 Acc 0.9151315789473684, AUC 0.9807765483856201, avg_entr 0.038469355553388596
Test Epoch26 layer1 Acc 0.9123684210526316, AUC 0.9726640582084656, avg_entr 0.014307835139334202
Test Epoch26 layer2 Acc 0.9113157894736842, AUC 0.9726114273071289, avg_entr 0.010171332396566868
Test Epoch26 layer3 Acc 0.9114473684210527, AUC 0.9727708101272583, avg_entr 0.008651001378893852
Test Epoch26 layer4 Acc 0.910921052631579, AUC 0.9731426239013672, avg_entr 0.007773378398269415
gc 0
Train Epoch27 Acc 0.9735916666666666 (116831/120000), AUC 0.9963122606277466
Test Epoch27 layer0 Acc 0.9147368421052632, AUC 0.9807789921760559, avg_entr 0.038414034992456436
Test Epoch27 layer1 Acc 0.9123684210526316, AUC 0.9725311994552612, avg_entr 0.01420720387250185
Test Epoch27 layer2 Acc 0.9101315789473684, AUC 0.9724797010421753, avg_entr 0.00979626551270485
Test Epoch27 layer3 Acc 0.9113157894736842, AUC 0.9728243947029114, avg_entr 0.008095480501651764
Test Epoch27 layer4 Acc 0.910921052631579, AUC 0.9729587435722351, avg_entr 0.00730863818898797
gc 0
Train Epoch28 Acc 0.9735583333333333 (116827/120000), AUC 0.9963815808296204
Test Epoch28 layer0 Acc 0.9147368421052632, AUC 0.980778694152832, avg_entr 0.03820912539958954
Test Epoch28 layer1 Acc 0.9122368421052631, AUC 0.9726307988166809, avg_entr 0.014388024806976318
Test Epoch28 layer2 Acc 0.9107894736842105, AUC 0.9727425575256348, avg_entr 0.00949289184063673
Test Epoch28 layer3 Acc 0.9102631578947369, AUC 0.9729664325714111, avg_entr 0.007613698020577431
Test Epoch28 layer4 Acc 0.9102631578947369, AUC 0.9729477167129517, avg_entr 0.00697281351312995
gc 0
Train Epoch29 Acc 0.973575 (116829/120000), AUC 0.9964806437492371
Test Epoch29 layer0 Acc 0.9151315789473684, AUC 0.9807624816894531, avg_entr 0.0380362868309021
Test Epoch29 layer1 Acc 0.9118421052631579, AUC 0.972430944442749, avg_entr 0.014146455563604832
Test Epoch29 layer2 Acc 0.9103947368421053, AUC 0.9722817540168762, avg_entr 0.009637219831347466
Test Epoch29 layer3 Acc 0.9110526315789473, AUC 0.972551703453064, avg_entr 0.007967619225382805
Test Epoch29 layer4 Acc 0.910921052631579, AUC 0.9727340340614319, avg_entr 0.007234356366097927
Best AUC 0.9833610653877258
train_loss (2, 5, 30)
valid_acc (5, 30)
valid_AUC (5, 30)
train_acc (30,)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Figure(640x480)
Start Testing
Load ckpt at ckpt/ag_news_transformeral_l5_pad125//ag_news_transformeral_l5.pt
[[1678   67  117   38]
 [   5 1875   12    8]
 [  31   16 1727  126]
 [  42   21  171 1666]]
Figure(640x480)
tensor([0.0457, 0.0048, 0.1181,  ..., 0.1085, 0.0089, 0.1542])
[[1706   55   84   55]
 [  11 1873    7    9]
 [  39   14 1701  146]
 [  40   14  124 1722]]
Figure(640x480)
tensor([0.0052, 0.0042, 0.0063,  ..., 0.0078, 0.0040, 0.0361])
[[1704   54   83   59]
 [  10 1872   10    8]
 [  41   13 1704  142]
 [  39   14  132 1715]]
Figure(640x480)
tensor([0.0074, 0.0055, 0.0069,  ..., 0.0084, 0.0047, 0.0079])
[[1702   54   85   59]
 [   9 1870   11   10]
 [  36   12 1713  139]
 [  38   14  138 1710]]
Figure(640x480)
tensor([0.0064, 0.0070, 0.0074,  ..., 0.0070, 0.0041, 0.0060])
[[1698   54   90   58]
 [   8 1869   13   10]
 [  36   12 1716  136]
 [  38   14  141 1707]]
Figure(640x480)
tensor([0.0059, 0.0090, 0.0086,  ..., 0.0093, 0.0043, 0.0063])
