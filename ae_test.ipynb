{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m20\u001b[39m):\n\u001b[1;32m     27\u001b[0m     epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m dl:\n\u001b[1;32m     29\u001b[0m         y \u001b[38;5;241m=\u001b[39m y[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     30\u001b[0m         p \u001b[38;5;241m=\u001b[39m model(y)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "from  torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "labels = torch.tensor([i for i in range(10)] * 40)\n",
    "x = torch.rand(400, 50)\n",
    "\n",
    "ds = TensorDataset(labels)\n",
    "dl = DataLoader(ds, batch_size=64, shuffle=True)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Embedding(10, 128),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(128, 10)\n",
    ")\n",
    "\n",
    "fg = nn.Sequential(\n",
    "    nn.Linear(50, 200),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(200, 128),\n",
    "    nn.Tanh()\n",
    ")\n",
    "\n",
    "model = model.cuda()\n",
    "cri = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "opt2 = torch.optim.Adam(fg.parameters(), lr=0.001)\n",
    "for i in range(20):\n",
    "    epoch_loss = 0.0\n",
    "    for x, y in dl:\n",
    "        y = y[0].cuda()\n",
    "        p = model(y)\n",
    "        loss = cri(p, x)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        epoch_loss += loss.item()\n",
    "    print('epoch', i, 'loss', epoch_loss/len(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (/root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (/root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n",
      "total count words 102019\n",
      "vocab size 30000\n",
      "loading glove vocabs...: 100%|██████| 400000/400000 [00:03<00:00, 111425.85it/s]\n",
      "found 26754 words in glove\n",
      "Start Training\n",
      "Train 0 | Acc 87.575 (105090/120000) | EMB 0.3639 + 0.0345 | L1 0.0006 + 0.0198 \n",
      "Acc 91.67105263157895 (6967/7600): 100%|█████| 119/119 [00:00<00:00, 716.83it/s]\n",
      "Test Epoch 0 Acc 91.67105263157895\n",
      "Train 1 | Acc 93.71916666666667 (112463/120000) | EMB 0.3409 + 0.0309 | L1 0.0 +\n",
      "Acc 91.75 (6973/7600): 100%|█████████████████| 119/119 [00:00<00:00, 719.36it/s]\n",
      "Test Epoch 1 Acc 91.75\n",
      "Train 2 | Acc 94.755 (113706/120000) | EMB 0.3408 + 0.031 | L1 0.0 + 0.0143 | L2\n",
      "Acc 91.6842105263158 (6968/7600): 100%|██████| 119/119 [00:00<00:00, 717.34it/s]\n",
      "Test Epoch 2 Acc 91.6842105263158\n",
      "Train 3 | Acc 95.59833333333333 (114718/120000) | EMB 0.3408 + 0.0316 | L1 0.0 +\n",
      "Acc 91.5657894736842 (6959/7600): 100%|██████| 119/119 [00:00<00:00, 710.82it/s]\n",
      "Test Epoch 3 Acc 91.5657894736842\n",
      "Train 4 | Acc 96.28416666666666 (115541/120000) | EMB 0.3408 + 0.0323 | L1 0.0 +\n",
      "Acc 91.48684210526316 (6953/7600): 100%|█████| 119/119 [00:00<00:00, 717.69it/s]\n",
      "Test Epoch 4 Acc 91.48684210526316\n",
      "Train 5 | Acc 96.94833333333334 (116338/120000) | EMB 0.3408 + 0.0331 | L1 0.0 +\n",
      "Acc 91.4342105263158 (6949/7600): 100%|██████| 119/119 [00:00<00:00, 712.51it/s]\n",
      "Test Epoch 5 Acc 91.4342105263158\n",
      "Train 6 | Acc 97.49833333333333 (116998/120000) | EMB 0.3408 + 0.0339 | L1 0.0 +\n",
      "Acc 91.05263157894737 (6920/7600): 100%|█████| 119/119 [00:00<00:00, 714.93it/s]\n",
      "Test Epoch 6 Acc 91.05263157894737\n",
      "Train 7 | Acc 97.94666666666667 (117536/120000) | EMB 0.3408 + 0.0347 | L1 0.0 +\n",
      "Acc 90.9342105263158 (6911/7600): 100%|██████| 119/119 [00:00<00:00, 714.72it/s]\n",
      "Test Epoch 7 Acc 90.9342105263158\n",
      "Train 8 | Acc 98.35666666666667 (118028/120000) | EMB 0.3408 + 0.0354 | L1 0.0 +\n",
      "Acc 90.80263157894737 (6901/7600): 100%|█████| 119/119 [00:00<00:00, 712.75it/s]\n",
      "Test Epoch 8 Acc 90.80263157894737\n",
      "Train 9 | Acc 98.61416666666666 (118337/120000) | EMB 0.3408 + 0.0361 | L1 0.0 +\n",
      "Acc 90.97368421052632 (6914/7600): 100%|█████| 119/119 [00:00<00:00, 703.95it/s]\n",
      "Test Epoch 9 Acc 90.97368421052632\n",
      "Train 10 | Acc 98.8575 (118629/120000) | EMB 0.3408 + 0.0361 | L1 0.0 + 0.0057 |\n",
      "Acc 90.86842105263158 (6906/7600): 100%|█████| 119/119 [00:00<00:00, 713.12it/s]\n",
      "Test Epoch 10 Acc 90.86842105263158\n",
      "Train 11 | Acc 99.0425 (118851/120000) | EMB 0.3408 + 0.0358 | L1 0.0 + 0.0052 |\n",
      "Acc 90.60526315789474 (6886/7600): 100%|█████| 119/119 [00:00<00:00, 715.51it/s]\n",
      "Test Epoch 11 Acc 90.60526315789474\n",
      "Train 12 | Acc 99.1775 (119013/120000) | EMB 0.3408 + 0.0356 | L1 0.0 + 0.0046 |\n",
      "Acc 90.59210526315789 (6885/7600): 100%|█████| 119/119 [00:00<00:00, 715.48it/s]\n",
      "Test Epoch 12 Acc 90.59210526315789\n",
      "Train 13 | Acc 99.26166666666667 (119114/120000) | EMB 0.3408 + 0.0354 | L1 0.0 \n",
      "Acc 90.48684210526316 (6877/7600): 100%|█████| 119/119 [00:00<00:00, 705.74it/s]\n",
      "Test Epoch 13 Acc 90.48684210526316\n",
      "Train 14 | Acc 99.32916666666667 (119195/120000) | EMB 0.3408 + 0.0352 | L1 0.0 \n",
      "Acc 90.36842105263158 (6868/7600): 100%|█████| 119/119 [00:00<00:00, 712.81it/s]\n",
      "Test Epoch 14 Acc 90.36842105263158\n",
      "Train 15 | Acc 99.40583333333333 (119287/120000) | EMB 0.3408 + 0.035 | L1 0.0 +\n",
      "Acc 90.14473684210526 (6851/7600): 100%|█████| 119/119 [00:00<00:00, 715.04it/s]\n",
      "Test Epoch 15 Acc 90.14473684210526\n",
      "Train 16 | Acc 99.43166666666667 (119318/120000) | EMB 0.3408 + 0.0349 | L1 0.0 \n",
      "Acc 90.38157894736842 (6869/7600): 100%|█████| 119/119 [00:00<00:00, 707.61it/s]\n",
      "Test Epoch 16 Acc 90.38157894736842\n",
      "Train 17 | Acc 99.47833333333334 (119374/120000) | EMB 0.3408 + 0.0347 | L1 0.0 \n",
      "Acc 90.55263157894737 (6882/7600): 100%|█████| 119/119 [00:00<00:00, 706.18it/s]\n",
      "Test Epoch 17 Acc 90.55263157894737\n",
      "Train 18 | Acc 99.53583333333333 (119443/120000) | EMB 0.3408 + 0.0346 | L1 0.0 \n",
      "Acc 90.22368421052632 (6857/7600): 100%|█████| 119/119 [00:00<00:00, 707.11it/s]\n",
      "Test Epoch 18 Acc 90.22368421052632\n",
      "Train 19 | Acc 99.56166666666667 (119474/120000) | EMB 0.3408 + 0.0345 | L1 0.0 \n",
      "Acc 90.23684210526316 (6858/7600): 100%|█████| 119/119 [00:00<00:00, 711.87it/s]\n",
      "Test Epoch 19 Acc 90.23684210526316\n",
      "Best acc 91.75\n"
     ]
    }
   ],
   "source": [
    "!python3 dis_train.py --dataset ag_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
