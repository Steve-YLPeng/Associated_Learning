{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "91685f81",
   "metadata": {},
   "source": [
    "## 0208"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e97f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "data = \"dbpedia_14\"\n",
    "text_len = 60\n",
    "model = \"transformeral\"\n",
    "layer = 5\n",
    "epoch = 10\n",
    "lr = 0.0001\n",
    "fix_previous_layer = True\n",
    "#for mask in range(1,1+layer):\n",
    "mask = 5\n",
    "save_path = f\"ckpt/{data}_{model}_l{layer}_pad{text_len}_m{mask}/\"  \n",
    "#load_path = f\"ckpt/{data}_{model}_l{layer}_pad{text_len}_m{mask-1}/\" \n",
    "out_path = f\"result/0117/fix train adapt/{data}_{model}_l{layer}ad_pad{text_len}_m{mask}/\"\n",
    "log = f\"{out_path}/{data}_{model}_l{layer}.log\"\n",
    "\n",
    "with open(log,mode='r') as log:\n",
    "    buffer = log.readlines()\n",
    "    df = pd.DataFrame(buffer,columns=[\"log\"])\n",
    "    df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9491a649",
   "metadata": {},
   "outputs": [],
   "source": [
    "class history(object):\n",
    "    def __init__(self):\n",
    "        self.auc = {}\n",
    "        self.acc = {}\n",
    "        self.entr = {}\n",
    "        for threshold in [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]:\n",
    "            self.auc[threshold] = []\n",
    "            self.acc[threshold] = []\n",
    "            self.entr[threshold] = []\n",
    "            \n",
    "result = history()\n",
    "for line in buffer:\n",
    "    match = re.match('Test Epoch(.)*', line)\n",
    "    if match!=None:\n",
    "        print(match.group())\n",
    "        match = re.findall(\"\\d?\\.\\d+\",match.group(0))\n",
    "        print(match)\n",
    "        threshold = float(match[0])\n",
    "        result.acc[threshold].append(match[1])\n",
    "        result.auc[threshold].append(match[2])\n",
    "        result.entr[threshold].append(match[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad5bb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "for threshold in [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]:\n",
    "    print(len(result.acc[threshold]))\n",
    "    print(len(result.auc[threshold]))\n",
    "    print(len(result.entr[threshold]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3c6b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2/06 - valid/test 80%:20%\n",
    "###      - prefix\n",
    "###      - plot result\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "data = \"ag_news\"\n",
    "text_len = 175\n",
    "\n",
    "\n",
    "for model in [\"linearal\",\"lstmal\",\"transformeral\"]:\n",
    "    result = [[] for _ in range(4)]\n",
    "    for threshold in [.1,.2,.3,.4,.5,.6,.7,.8,.9]:\n",
    "\n",
    "        layer = 5\n",
    "        epoch = 10\n",
    "        lr = 0.0001\n",
    "        fix_previous_layer = False\n",
    "        \n",
    "        mask = 5\n",
    "        out_path = f\"result/0208/test/prefix/{data}_{model}_l{layer}adp_pad{text_len}_t{threshold}_m{mask}_test/\"\n",
    "        log = f\"{out_path}/{data}_{model}_l{layer}.log\"\n",
    "            \n",
    "        with open(log,mode='r') as log:\n",
    "            buffer = log.readlines()\n",
    "            df = pd.DataFrame(buffer,columns=[\"log\"])\n",
    "            df\n",
    "            \n",
    "            for line in buffer:\n",
    "                match = re.match('Test threshold(.)*', line)\n",
    "                if match!=None:\n",
    "                    match = re.findall(\"\\d?\\.\\d+\",match.group(0))\n",
    "                    #print(match[1])\n",
    "                    result[0].append(float(match[1]))\n",
    "                    result[1].append(float(match[2]))\n",
    "                    result[2].append(float(match[3]))\n",
    "            for line in buffer:\n",
    "                match = re.match('t(.)*_test_time(.)*', line)\n",
    "                if match!=None:\n",
    "                    match = re.findall(\"\\d?\\.\\d+\",match.group(0))\n",
    "                    #print(match[1])\n",
    "                    result[3].append(float(match[1]))\n",
    "    print(\"\\n\",model)\n",
    "    for i in range(4):\n",
    "        print(i)\n",
    "        for v in result[i]:\n",
    "            print(v)                    \n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e4e2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2/06 - valid/test 80%:20%\n",
    "###      - fix\n",
    "###      - plot result\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "data = \"ag_news\"\n",
    "text_len = 175\n",
    "\n",
    "for model in [\"linearal\",\"lstmal\",\"transformeral\"]:\n",
    "    result = [[] for _ in range(4)]\n",
    "    for threshold in [.1,.2,.3,.4,.5,.6,.7,.8,.9]:\n",
    "\n",
    "        layer = 5\n",
    "        epoch = 10\n",
    "        lr = 0.0001\n",
    "        fix_previous_layer = False\n",
    "        \n",
    "        mask = 5\n",
    "        out_path = f\"result/0208/test/fix/{data}_{model}_l{layer}adf_pad{text_len}_t{threshold}_m{mask}_test/\"\n",
    "        log = f\"{out_path}/{data}_{model}_l{layer}.log\"\n",
    "            \n",
    "        with open(log,mode='r') as log:\n",
    "            buffer = log.readlines()\n",
    "            df = pd.DataFrame(buffer,columns=[\"log\"])\n",
    "            df\n",
    "            \n",
    "            for line in buffer:\n",
    "                match = re.match('Test threshold(.)*', line)\n",
    "                if match!=None:\n",
    "                    match = re.findall(\"\\d?\\.\\d+\",match.group(0))\n",
    "                    #print(match[1])\n",
    "                    result[0].append(float(match[1]))\n",
    "                    result[1].append(float(match[2]))\n",
    "                    result[2].append(float(match[3]))\n",
    "            for line in buffer:\n",
    "                match = re.match('t(.)*_test_time(.)*', line)\n",
    "                if match!=None:\n",
    "                    match = re.findall(\"\\d?\\.\\d+\",match.group(0))\n",
    "                    #print(match[1])\n",
    "                    result[3].append(float(match[1]))\n",
    "    print(\"\\n\",model)\n",
    "    for i in range(4):\n",
    "        print(i)\n",
    "        for v in result[i]:\n",
    "            print(v)                    \n",
    "\n",
    "                    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ab581dd2",
   "metadata": {},
   "source": [
    "## 0214\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1418cc62",
   "metadata": {},
   "source": [
    "### base adapt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc003623",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 0214\n",
    "### pad base adapt \n",
    "import os \n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#for model in [\"lstmal\"]:\n",
    "for model in [\"lstmal\",\"linearal\",\"transformeral\"]:\n",
    "    total_result = pd.DataFrame(columns=['padding_size', 'model', 'test_threshold', 'test_acc', 'test_auc',\n",
    "                                        'test_avg_entr', 'test_time'])\n",
    "    #data = \"ag_news\"\n",
    "    #for text_len in [25,50,75,100,125,150,175]:\n",
    "    data = \"dbpedia_14\"\n",
    "    for text_len in [20,40,60]:\n",
    "        layer = 5\n",
    "        epoch = 50\n",
    "        lr = 0.0001\n",
    "        save_path = f\"ckpt/{data}_{model}_l{layer}ad_pad{text_len}/\"\n",
    "        #out_path = f\"result/0220/ag_news/base adapt/{data}_{model}_l{layer}ad_pad{text_len}/\"\n",
    "        out_path = f\"result/0220/dbpedia_14/base adapt/{data}_{model}_l{layer}ad_pad{text_len}/\"\n",
    "        \"result/0220/dbpedia/base adapt/dbpedia_14_linearal_l5ad_pad20/dbpedia_14_linearal_l5.log\"\n",
    "        log = f\"{out_path}/{data}_{model}_l{layer}.log\"\n",
    "        \n",
    "        result = pd.DataFrame(columns=[\"padding_size\"])\n",
    "        result_ep = pd.DataFrame(columns=[\"epoch\",\"ep_train_time\"])\n",
    "        result_ep[\"epoch\"] = [*range(epoch)]\n",
    "        list_acc = []\n",
    "        list_auc = []\n",
    "        list_entr = []\n",
    "        list_test_time = []\n",
    "        list_ep_train_time = []\n",
    "        list_ep_train_acc = []\n",
    "        list_ep_train_auc = []\n",
    "        list_ep_train_entr = []\n",
    "        list_ep_valid_time = []\n",
    "        list_ep_valid_acc = []\n",
    "        list_ep_valid_auc = []\n",
    "        list_ep_valid_entr = []\n",
    "        train_time = 0\n",
    "        init_time = 0\n",
    "        best_ep = -1\n",
    "        best_th = []\n",
    "        save_th = []\n",
    "        with open(log,mode='r') as log:\n",
    "            buffer = log.readlines()\n",
    "            df = pd.DataFrame(buffer,columns=[\"log\"])\n",
    "            df\n",
    "            \n",
    "            for line in buffer:\n",
    "                \n",
    "                \n",
    "                match = re.match('Save ckpt to (.)*', line)\n",
    "                if match!=None:\n",
    "                    match = re.findall(\"ep \\d+\",match.group(0))\n",
    "                    best_ep = match[0]\n",
    "                    best_th.append(current_th)\n",
    "                match = re.match('Test threshold(.)*', line)\n",
    "                if match!=None:\n",
    "                    match = re.findall(\"\\d?\\.\\d+\",match.group(0))\n",
    "                    list_acc.append(float(match[1]))\n",
    "                    list_auc.append(float(match[2]))\n",
    "                    list_entr.append(float(match[3]))\n",
    "                    \n",
    "\n",
    "                match = re.match('t(.)*_test_time(.)*', line)\n",
    "                if match!=None:\n",
    "                    match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                    list_test_time.append(float(match[1]))\n",
    "\n",
    "                match = re.match('init_time(.)*', line)\n",
    "                if match!=None:\n",
    "                    match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                    init_time = float(match[0])\n",
    "                match = re.match('(.)*valid_time(.)*', line)\n",
    "                if match!=None:\n",
    "                    match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                    train_time = float(match[0])\n",
    "                    \n",
    "                match = re.match('ep(\\d)+_train_time(.)*', line)\n",
    "                if match!=None:\n",
    "                    match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                    list_ep_train_time.append(float(match[0]))\n",
    "                    \n",
    "                match = re.match('ep(\\d)+_t(.)*_test_time(.)*', line)\n",
    "                if match!=None:\n",
    "                    match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                    list_ep_valid_time.append(float(match[1]))\n",
    "                    \n",
    "                match = re.match('Train Epoch\\d+ Acc(.)*', line)\n",
    "                if match!=None:\n",
    "                    match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                    list_ep_train_acc.append(float(match[0]))\n",
    "                    list_ep_train_auc.append(float(match[1]))\n",
    "                    #print(best_th)\n",
    "                    if len(best_th)>0:\n",
    "                        save_th = best_th\n",
    "                    best_th = []\n",
    "                    \n",
    "                match = re.match('Test Epoch\\d+ threshold(.)*', line)\n",
    "                if match!=None:\n",
    "                    match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                    #print(line)\n",
    "                    #print(match)\n",
    "                    current_th = int(float(match[0])*10)\n",
    "                    list_ep_valid_acc.append(float(match[1]))\n",
    "                    list_ep_valid_auc.append(float(match[2]))\n",
    "                    list_ep_valid_entr.append(float(match[3]))\n",
    "        print(save_th)    \n",
    "        ep_valid_time = np.array(list_ep_valid_time).reshape((epoch,-1))\n",
    "        ep_valid_acc = np.array(list_ep_valid_acc).reshape((epoch,-1))\n",
    "        ep_valid_auc = np.array(list_ep_valid_auc).reshape((epoch,-1))\n",
    "        ep_valid_entr = np.array(list_ep_valid_entr).reshape((epoch,-1))\n",
    "        \n",
    "        result_ep[\"ep_train_acc\"] = list_ep_train_acc \n",
    "        result_ep[\"ep_train_auc\"] = list_ep_train_auc          \n",
    "        \n",
    "        result_ep[\"ep_valid_time\"] = np.sum(ep_valid_time,-1) \n",
    "        result_ep[[f\"ep_valid.{(i+1)}_time\" for i in range(9)]] = ep_valid_time\n",
    "        result_ep[\"ep_train_time\"] = list_ep_train_time \n",
    "        \n",
    "        para_size = 9\n",
    "        result['model'] = [model]*para_size\n",
    "        result['padding_size'] = [text_len]*para_size\n",
    "        result[\"test_threshold\"] = [(1+i)*0.1 for i in range(para_size)]\n",
    "        result[\"best_ep\"] = [best_ep[2:]]*para_size\n",
    "        result[\"best_th\"] = [save_th]*para_size\n",
    "        result[\"best_acc_setting\"] = [list_acc[save_th[-1]-1]]*para_size\n",
    "        result[\"best_auc_setting\"] = [list_auc[save_th[-1]-1]]*para_size\n",
    "        result[\"test_acc\"] = list_acc\n",
    "        result[\"test_auc\"] = list_auc\n",
    "        result[\"test_avg_entr\"] = list_entr\n",
    "        result[\"test_time\"] = list_test_time\n",
    "        result[\"init_time\"] = [init_time]*para_size\n",
    "        result[\"train+valid_time\"] = [train_time]*para_size\n",
    "        result[\"train_time\"] = [result_ep[\"ep_train_time\"].sum()]*para_size\n",
    "        result[\"valid_time\"] = [result_ep[\"ep_valid_time\"].sum()]*para_size\n",
    "        #print(result)\n",
    "        #print(result_ep)\n",
    "\n",
    "        path = f\"result/_csv/{data}/base_ad/\"\n",
    "        title = f\"{data}_{model}_l{layer}ad_pad{text_len}({best_ep})\"\n",
    "        os.makedirs(path,exist_ok=True)\n",
    "        \"\"\"\n",
    "        t = [0.1*(i+1) for i in range(9)]\n",
    "        plt.title(title)\n",
    "        plt.plot(t,result[\"test_time\"],\"o-\") \n",
    "        plt.xlabel(\"threshold\")\n",
    "        plt.ylabel(\"test time\")\n",
    "        plt.savefig(f\"{path}/{title}_plt_test_time.png\", bbox_inches = \"tight\",facecolor='white', transparent=False)\n",
    "        plt.show()\n",
    "        \n",
    "        plt.title(title)\n",
    "        plt.plot(t,result[\"test_acc\"],\"o-\") \n",
    "        plt.xlabel(\"threshold\")\n",
    "        plt.ylabel(\"test acc\")\n",
    "        for i in range(9):\n",
    "            if (i+1) in save_th:\n",
    "                plt.plot((i+1)/10,result[\"test_acc\"][i],\"ro\") \n",
    "        plt.savefig(f\"{path}/{title}_plt_acc.png\", bbox_inches = \"tight\",facecolor='white', transparent=False)\n",
    "        plt.show()\n",
    "\n",
    "        plt.title(title)\n",
    "        plt.plot(t,result[\"test_auc\"],\"o-\") \n",
    "        plt.xlabel(\"threshold\")\n",
    "        plt.ylabel(\"test auc\")\n",
    "        for i in range(9):\n",
    "            if (i+1) in save_th:\n",
    "                plt.plot((i+1)/10,result[\"test_auc\"][i],\"ro\") \n",
    "        plt.savefig(f\"{path}/{title}_plt_auc.png\", bbox_inches = \"tight\",facecolor='white', transparent=False)\n",
    "        plt.show()\n",
    "        \n",
    "        plt.title(title)\n",
    "        plt.plot(t,result[\"test_avg_entr\"],\"o-\") \n",
    "        plt.xlabel(\"threshold\")\n",
    "        plt.ylabel(\"test avg entr\")\n",
    "        plt.savefig(f\"{path}/{title}_plt_entr.png\", bbox_inches = \"tight\",facecolor='white', transparent=False)\n",
    "        plt.show()\n",
    "        \"\"\"\n",
    "        #result.to_csv(f\"{path}/{title}.csv\", index=False)\n",
    "        #result_ep.to_csv(f\"{path}/{title}_ep.csv\", index=False)\n",
    "        total_result = pd.concat([total_result,result],axis=0,ignore_index=True)            \n",
    "        del result\n",
    "\n",
    "    title = f\"{data}_{model}_l{layer}ad\"\n",
    "    total_result.to_csv(f\"{path}/{title}.csv\", index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9494d7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d25fad4e",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ebd86f0b",
   "metadata": {},
   "source": [
    "### base shortcut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1761a49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 0214\n",
    "### pad base sc\n",
    "### 0220 - ag_news/dbpedia_14 \n",
    "###      - testing log有誤 \"Test Epoch49 layer(.)*\"\n",
    "import os \n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for model in [\"lstmal\",\"linearal\",\"transformeral\"]:\n",
    "    total_result = pd.DataFrame(columns=['padding_size', 'model', 'test_acc', 'test_auc',\n",
    "                                        'test_time'])\n",
    "    #data = \"ag_news\"\n",
    "    #for text_len in [25,50,75,100,125,150,175]:\n",
    "    #data = \"dbpedia_14\"\n",
    "    #for text_len in [20,40,60]:\n",
    "    data = \"imdb\"\n",
    "    for text_len in [100,200,300,400,500]:    \n",
    "        layer = 5\n",
    "        epoch = 50\n",
    "        lr = 0.0001\n",
    "        save_path = f\"ckpt/{data}_{model}_l{layer}_pad{text_len}/\"\n",
    "        #out_path = f\"result/0220/ag_news/base sc/{data}_{model}_l{layer}_pad{text_len}/\"\n",
    "        #out_path = f\"result/0220/dbpedia/base sc/{data}_{model}_l{layer}_pad{text_len}/\"\n",
    "        out_path = f\"result/0220/imdb/base sc/{data}_{model}_l{layer}_pad{text_len}/\"\n",
    "        log = f\"{out_path}/{data}_{model}_l{layer}.log\"\n",
    "        \n",
    "        result = pd.DataFrame(columns=[\"padding_size\"])\n",
    "        result_ep = pd.DataFrame(columns=[\"epoch\",\"ep_train_time\"])\n",
    "        result_ep[\"epoch\"] = [*range(epoch)]\n",
    "        list_acc = []\n",
    "        list_auc = []\n",
    "        list_entr = []\n",
    "        list_test_time = []\n",
    "        list_ep_train_time = []\n",
    "        list_ep_train_acc = []\n",
    "        list_ep_train_auc = []\n",
    "        list_ep_train_entr = []\n",
    "        list_ep_valid_time = []\n",
    "        list_ep_valid_acc = []\n",
    "        list_ep_valid_auc = []\n",
    "        list_ep_valid_entr = []\n",
    "        train_time = 0\n",
    "        init_time = 0\n",
    "        best_ep = -1\n",
    "        best_th = []\n",
    "        save_th = []\n",
    "        test_part = False\n",
    "        \n",
    "        para_size = 5\n",
    "        with open(log,mode='r') as log:\n",
    "            buffer = log.readlines()\n",
    "            df = pd.DataFrame(buffer,columns=[\"log\"])\n",
    "            df\n",
    "            \n",
    "            for line in buffer:\n",
    "                \n",
    "                \n",
    "                match = re.match(\"Start Testing\", line)\n",
    "                if match!=None:\n",
    "                    test_part = True\n",
    "                    \n",
    "                match = re.match('Save ckpt to (.)*', line)\n",
    "                if match!=None:\n",
    "                    match = re.findall(\"ep \\d+\",match.group(0))\n",
    "                    best_ep = match[0]\n",
    "                    best_th.append(current_th)\n",
    "                \n",
    "                if data == \"imdb\":\n",
    "                    match = re.match('Test layer\\d+(.)*', line)    \n",
    "                else:    \n",
    "                    match = re.match('Test Epoch49 layer(.)*', line)\n",
    "                if match!=None and test_part==True:\n",
    "                    match = re.findall(\"\\d?\\.\\d+\",match.group(0))\n",
    "                    list_acc.append(float(match[0]))\n",
    "                    list_auc.append(float(match[1]))\n",
    "                    list_entr.append(float(match[2]))\n",
    "                    \n",
    "                if data == \"imdb\":\n",
    "                    match = re.match('l\\d+_test_time(.)*', line)\n",
    "                else:    \n",
    "                    match = re.match('ep49_l\\d_test_time(.)*', line)\n",
    "                if match!=None and test_part==True:\n",
    "                    match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                    list_test_time.append(float(match[0]))\n",
    "\n",
    "                match = re.match('init_time(.)*', line)\n",
    "                if match!=None:\n",
    "                    match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                    init_time = float(match[0])\n",
    "                    \n",
    "                match = re.match('(.)*valid_time(.)*', line)\n",
    "                if match!=None:\n",
    "                    match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                    train_time = float(match[0])\n",
    "                    \n",
    "                match = re.match('ep(\\d)+_train_time(.)*', line)\n",
    "                if match!=None and test_part==False:\n",
    "                    match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                    list_ep_train_time.append(float(match[0]))\n",
    "                    \n",
    "                match = re.match('ep(\\d)+_l\\d+_test_time(.)*', line)\n",
    "                if match!=None and test_part==False:\n",
    "                    match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                    list_ep_valid_time.append(float(match[0]))\n",
    "                    \n",
    "                match = re.match('Train Epoch\\d+ Acc(.)*', line)\n",
    "                if match!=None:\n",
    "                    match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                    list_ep_train_acc.append(float(match[0]))\n",
    "                    list_ep_train_auc.append(float(match[1]))\n",
    "                    #print(best_th)\n",
    "                    if len(best_th)>0:\n",
    "                        save_th = best_th\n",
    "                    best_th = []\n",
    "                    \n",
    "                match = re.match('Test Epoch\\d+ layer(.)*', line)\n",
    "                if match!=None and test_part==False:\n",
    "                    match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                    #print(line)\n",
    "                    #print(match)\n",
    "                    \n",
    "                    list_ep_valid_acc.append(float(match[0]))\n",
    "                    list_ep_valid_auc.append(float(match[1]))\n",
    "                    list_ep_valid_entr.append(float(match[2]))\n",
    "                    \n",
    "                    match = re.findall('\\d+', line)\n",
    "                    current_th = int(match[1])\n",
    "        print(save_th)    \n",
    "        ep_valid_time = np.array(list_ep_valid_time).reshape((epoch,-1))\n",
    "        ep_valid_acc = np.array(list_ep_valid_acc).reshape((epoch,-1))\n",
    "        ep_valid_auc = np.array(list_ep_valid_auc).reshape((epoch,-1))\n",
    "        ep_valid_entr = np.array(list_ep_valid_entr).reshape((epoch,-1))\n",
    "        \n",
    "        result_ep[\"ep_train_acc\"] = list_ep_train_acc \n",
    "        result_ep[\"ep_train_auc\"] = list_ep_train_auc          \n",
    "        \n",
    "        result_ep[\"ep_valid_time\"] = np.sum(ep_valid_time,-1) \n",
    "        result_ep[[f\"ep_valid_l{i}_time\" for i in range(para_size)]] = ep_valid_time\n",
    "        result_ep[\"ep_train_time\"] = list_ep_train_time \n",
    "        \n",
    "        \n",
    "        result['model'] = [model]*para_size\n",
    "        result['padding_size'] = [text_len]*para_size\n",
    "        result[\"test_layer\"] = [(i) for i in range(para_size)]\n",
    "        result[\"best_ep\"] = [best_ep[2:]]*para_size\n",
    "        result[\"best_th\"] = [save_th]*para_size\n",
    "        result[\"best_acc_setting\"] = [list_acc[save_th[-1]-1]]*para_size\n",
    "        result[\"best_auc_setting\"] = [list_auc[save_th[-1]-1]]*para_size\n",
    "        result[\"test_acc\"] = list_acc\n",
    "        result[\"test_auc\"] = list_auc\n",
    "        result[\"test_avg_entr\"] = list_entr\n",
    "        result[\"test_time\"] = list_test_time\n",
    "        result[\"init_time\"] = [init_time]*para_size\n",
    "        result[\"train+valid_time\"] = [train_time]*para_size\n",
    "        result[\"train_time\"] = [result_ep[\"ep_train_time\"].sum()]*para_size\n",
    "        result[\"valid_time\"] = [result_ep[\"ep_valid_time\"].sum()]*para_size\n",
    "        #print(result)\n",
    "        #print(result_ep)\n",
    "\n",
    "        path = f\"result/_csv/{data}/base_sc/\"\n",
    "        title = f\"{data}_{model}_l{layer}_pad{text_len}({best_ep})\"\n",
    "        t = [(i) for i in range(para_size)]\n",
    "        \n",
    "        os.makedirs(path,exist_ok=True)\n",
    "\n",
    "        \"\"\"\n",
    "        plt.title(title)\n",
    "        plt.plot(t,result[\"test_time\"],\"o-\") \n",
    "        plt.xlabel(\"shortcut\")\n",
    "        plt.ylabel(\"test time\")\n",
    "        plt.savefig(f\"{path}/{title}_plt_test_time.png\", bbox_inches = \"tight\",facecolor='white', transparent=False)\n",
    "        plt.show()\n",
    "        \n",
    "        plt.title(title)\n",
    "        plt.plot(t,result[\"test_acc\"],\"o-\") \n",
    "        plt.xlabel(\"shortcut\")\n",
    "        plt.ylabel(\"test acc\")\n",
    "        for i in range(para_size):\n",
    "            if (i) == save_th[-1]:\n",
    "                plt.plot((i),result[\"test_acc\"][i],\"ro\") \n",
    "        plt.savefig(f\"{path}/{title}_plt_acc.png\", bbox_inches = \"tight\",facecolor='white', transparent=False)\n",
    "        plt.show()\n",
    "\n",
    "        plt.title(title)\n",
    "        plt.plot(t,result[\"test_auc\"],\"o-\") \n",
    "        plt.xlabel(\"shortcut\")\n",
    "        plt.ylabel(\"test auc\")\n",
    "        for i in range(para_size):\n",
    "            if (i) == save_th[-1]:\n",
    "                plt.plot((i),result[\"test_auc\"][i],\"ro\") \n",
    "        plt.savefig(f\"{path}/{title}_plt_auc.png\", bbox_inches = \"tight\",facecolor='white', transparent=False)\n",
    "        plt.show()\n",
    "        \n",
    "        plt.title(title)\n",
    "        plt.plot(t,result[\"test_avg_entr\"],\"o-\") \n",
    "        plt.xlabel(\"shortcut\")\n",
    "        plt.ylabel(\"test avg entr\")\n",
    "        plt.savefig(f\"{path}/{title}_plt_entr.png\", bbox_inches = \"tight\",facecolor='white', transparent=False)\n",
    "        plt.show()\n",
    "        \"\"\"\n",
    "        #result.to_csv(f\"{path}/{title}.csv\", index=False)\n",
    "        #result_ep.to_csv(f\"{path}/{title}_ep.csv\", index=False)\n",
    "        total_result = pd.concat([total_result,result],axis=0,ignore_index=True)            \n",
    "        del result\n",
    "    \n",
    "    title = f\"{data}_{model}_l{layer}\"\n",
    "    total_result.to_csv(f\"{path}/{title}.csv\", index=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b7ba3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d52e358d",
   "metadata": {},
   "source": [
    "### fix/prefix adapt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c25b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"ag_news\"\n",
    "for model in [\"lstmal\",\"linearal\",\"transformeral\"]:\n",
    "    total_result = pd.DataFrame(columns=[])\n",
    "    for threshold in [.1,.2,.3,.4,.5,.6,.7,.8,.9]:\n",
    "        for text_len in [175]:\n",
    "            layer = 5\n",
    "            epoch = 10\n",
    "            lr = 0.0001\n",
    "            \n",
    "            result = pd.DataFrame(columns=[\"padding_size\"])\n",
    "            result_ep = pd.DataFrame(columns=[\"epoch\",\"ep_train_time\"])\n",
    "            result_ep[\"epoch\"] = [*range(epoch)]\n",
    "            list_acc = []\n",
    "            list_auc = []\n",
    "            list_entr = []\n",
    "            list_test_time = []\n",
    "            list_init_time = []\n",
    "            list_train_time = []\n",
    "            \n",
    "            for mask in range(1,1+layer):\n",
    "                save_path = f\"ckpt/{data}_{model}_l{layer}adf_pad{text_len}_t{threshold}_m{mask}/\"  \n",
    "                load_path = f\"ckpt/{data}_{model}_l{layer}adf_pad{text_len}_t{threshold}_m{mask-1}/\" \n",
    "                #out_path = f\"result/0220/ag_news/fix adapt/{data}_{model}_l{layer}adf_pad{text_len}_t{threshold}_m{mask}/\"\n",
    "                out_path = f\"result/0214/prefix adapt/{data}_{model}_l{layer}adf_pad{text_len}_t{threshold}_m{mask}/\"\n",
    "                log = f\"{out_path}/{data}_{model}_l{layer}.log\"\n",
    "                \n",
    "                with open(log,mode='r') as log:\n",
    "                    buffer = log.readlines()\n",
    "                    df = pd.DataFrame(buffer,columns=[\"log\"])\n",
    "                    df\n",
    "                    \n",
    "                    \n",
    "                    for line in buffer:\n",
    "                        match = re.match('Test threshold(.)*', line)\n",
    "                        if match!=None:\n",
    "                            match = re.findall(\"\\d?\\.\\d+\",match.group(0))\n",
    "                            list_acc.append(float(match[1]))\n",
    "                            list_auc.append(float(match[2]))\n",
    "                            list_entr.append(float(match[3]))\n",
    "\n",
    "                        match = re.match('t(.)*_test_time(.)*', line)\n",
    "                        if match!=None:\n",
    "                            match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                            list_test_time.append(float(match[1]))\n",
    "                        match = re.match('init_time(.)*', line)\n",
    "                        if match!=None:\n",
    "                            match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                            list_init_time.append(float(match[0]))\n",
    "                        match = re.match('(.)*valid_time(.)*', line)\n",
    "                        if match!=None:\n",
    "                            match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                            list_train_time.append(float(match[0]))\n",
    "                                \n",
    "            result['threshold'] = [threshold]*5\n",
    "            result['model'] = [model]*5\n",
    "            result['padding_size'] = [text_len]*5\n",
    "            result[\"train_mask\"] = [(1+i) for i in range(5)]\n",
    "            \n",
    "            result[\"test_acc\"] = list_acc\n",
    "            result[\"test_auc\"] = list_auc\n",
    "            result[\"test_avg_entr\"] = list_entr\n",
    "            \n",
    "            result[\"init_time\"] = list_init_time\n",
    "            result[\"test_time\"] = list_test_time\n",
    "            result[\"train+valid_time\"] = list_train_time\n",
    "        \n",
    "            #print(result)\n",
    "            total_result = pd.concat([total_result,result],axis=0,ignore_index=True)\n",
    "    \n",
    "    table = total_result[total_result[\"train_mask\"]==5]\n",
    "    #title = f\"{data}_{model}_l{layer}f_pad{text_len}\"\n",
    "    title = f\"{data}_{model}_l{layer}p_pad{text_len}\"\n",
    "    path = f\"result/_csv/{data}\"\n",
    "                   \n",
    "    print(total_result)\n",
    "    total_result.to_csv(f\"{path}/{title}.csv\", index=False)\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.plot(t,table[\"test_acc\"],\"o-\") \n",
    "    plt.xlabel(\"threshold\")\n",
    "    plt.ylabel(\"test acc\")\n",
    "    plt.savefig(f\"{path}/{title}_plt_acc.png\", bbox_inches = \"tight\",facecolor='white', transparent=False)\n",
    "    plt.show()\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.plot(t,table[\"test_auc\"],\"o-\") \n",
    "    plt.xlabel(\"threshold\")\n",
    "    plt.ylabel(\"test auc\")\n",
    "    plt.savefig(f\"{path}/{title}_plt_auc.png\", bbox_inches = \"tight\",facecolor='white', transparent=False)\n",
    "    plt.show()\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.plot(t,table[\"test_time\"],\"o-\") \n",
    "    plt.xlabel(\"threshold\")\n",
    "    plt.ylabel(\"test time\")\n",
    "    plt.savefig(f\"{path}/{title}_plt_test_time.png\", bbox_inches = \"tight\",facecolor='white', transparent=False)\n",
    "    plt.show()\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.plot(t,table[\"test_avg_entr\"],\"o-\") \n",
    "    plt.xlabel(\"threshold\")\n",
    "    plt.ylabel(\"test avg entr\")\n",
    "    plt.savefig(f\"{path}/{title}_plt_entr.png\", bbox_inches = \"tight\",facecolor='white', transparent=False)\n",
    "    plt.show()\n",
    "    #break\n",
    "            "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cbdd66bb",
   "metadata": {},
   "source": [
    "### base fullpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9067b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### fullpath baseline\n",
    "import os \n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "### 2/22 - baseline: valid/test with fullpath\n",
    "data = \"ag_news\"\n",
    "total_result = pd.DataFrame(columns=[])\n",
    "for model in [\"linearal\",\"lstmal\",\"transformeral\"]:\n",
    "    for text_len in [25,50,75,100,125,150,175]:\n",
    "### 2/26 - dbpedia\n",
    "#data = \"dbpedia_14\"\n",
    "#for text_len in [20,40,60]:\n",
    "### 2/28 - imdb 8:1:1\n",
    "#data = \"imdb\"\n",
    "#for text_len in [100,200,300,400,500]:\n",
    "    \n",
    "        \n",
    "        layer = 5\n",
    "        epoch = 50\n",
    "        lr = 0.0001\n",
    "        save_path = f\"ckpt/{data}_{model}_l{layer}base_pad{text_len}/\"\n",
    "        out_path = f\"result/0227/{data}_{model}_l{layer}base_pad{text_len}/\"\n",
    "        log = f\"{out_path}/{data}_{model}_l{layer}base.log\"\n",
    "        \n",
    "        \n",
    "        result = pd.DataFrame(columns=[\"padding_size\"])\n",
    "        result_ep = pd.DataFrame(columns=[\"epoch\",\"ep_train_time\"])\n",
    "        result_ep[\"epoch\"] = [*range(epoch)]\n",
    "        list_acc = []\n",
    "        list_auc = []\n",
    "        list_entr = []\n",
    "        list_test_time = []\n",
    "        list_ep_train_time = []\n",
    "        list_ep_train_acc = []\n",
    "        list_ep_train_auc = []\n",
    "        list_ep_train_entr = []\n",
    "        list_ep_valid_time = []\n",
    "        list_ep_valid_acc = []\n",
    "        list_ep_valid_auc = []\n",
    "        list_ep_valid_entr = []\n",
    "        train_time = 0\n",
    "        init_time = 0\n",
    "        best_ep = -1\n",
    "        best_th = []\n",
    "        save_th = []\n",
    "        test_part = False\n",
    "        \n",
    "        para_size = 1\n",
    "        with open(log,mode='r') as log:\n",
    "            buffer = log.readlines()\n",
    "            df = pd.DataFrame(buffer,columns=[\"log\"])\n",
    "            df\n",
    "            \n",
    "            for line in buffer:\n",
    "                \n",
    "                \n",
    "                match = re.match(\"Start Testing\", line)\n",
    "                if match!=None:\n",
    "                    test_part = True\n",
    "                    \n",
    "                match = re.match('Save ckpt to (.)*', line)\n",
    "                if match!=None:\n",
    "                    match = re.findall(\"ep \\d+\",match.group(0))\n",
    "                    best_ep = match[0]\n",
    "                    best_th.append(current_th)\n",
    "                    \n",
    "                match = re.match('Test layer\\d Acc(.)*', line)\n",
    "                if match!=None and test_part==True:\n",
    "                    match = re.findall(\"\\d?\\.\\d+\",match.group(0))\n",
    "                    list_acc.append(float(match[0]))\n",
    "                    list_auc.append(float(match[1]))\n",
    "                    list_entr.append(float(match[2]))\n",
    "                    \n",
    "\n",
    "                match = re.match('Test layer\\d Acc(.)*', line)\n",
    "                if match!=None and test_part==True:\n",
    "                    match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                    list_test_time.append(float(match[0]))\n",
    "\n",
    "                match = re.match('init_time(.)*', line)\n",
    "                if match!=None:\n",
    "                    match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                    init_time = float(match[0])\n",
    "                    \n",
    "                match = re.match('(.)*valid_time(.)*', line)\n",
    "                if match!=None:\n",
    "                    match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                    train_time = float(match[0])\n",
    "                    \n",
    "                match = re.match('ep(\\d)+_train_time(.)*', line)\n",
    "                if match!=None and test_part==False:\n",
    "                    match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                    list_ep_train_time.append(float(match[0]))\n",
    "                    \n",
    "                match = re.match('ep(\\d)+_l\\d+_test_time(.)*', line)\n",
    "                if match!=None and test_part==False:\n",
    "                    match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                    list_ep_valid_time.append(float(match[0]))\n",
    "                    \n",
    "                match = re.match('Train Epoch\\d+ Acc(.)*', line)\n",
    "                if match!=None:\n",
    "                    match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                    list_ep_train_acc.append(float(match[0]))\n",
    "                    list_ep_train_auc.append(float(match[1]))\n",
    "                    #print(best_th)\n",
    "                    if len(best_th)>0:\n",
    "                        save_th = best_th\n",
    "                    best_th = []\n",
    "                    \n",
    "                match = re.match('Test Epoch\\d+ layer(.)*', line)\n",
    "                if match!=None and test_part==False:\n",
    "                    match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                    #print(line)\n",
    "                    #print(match)\n",
    "                    \n",
    "                    list_ep_valid_acc.append(float(match[0]))\n",
    "                    list_ep_valid_auc.append(float(match[1]))\n",
    "                    list_ep_valid_entr.append(float(match[2]))\n",
    "                    \n",
    "                    match = re.findall('\\d+', line)\n",
    "                    current_th = int(match[1])\n",
    "        print(save_th)    \n",
    "        ep_valid_time = np.array(list_ep_valid_time).reshape((epoch,-1))\n",
    "        ep_valid_acc = np.array(list_ep_valid_acc).reshape((epoch,-1))\n",
    "        ep_valid_auc = np.array(list_ep_valid_auc).reshape((epoch,-1))\n",
    "        ep_valid_entr = np.array(list_ep_valid_entr).reshape((epoch,-1))\n",
    "        \n",
    "        result_ep[\"ep_train_acc\"] = list_ep_train_acc \n",
    "        result_ep[\"ep_train_auc\"] = list_ep_train_auc          \n",
    "        \n",
    "        result_ep[\"ep_valid_time\"] = np.sum(ep_valid_time,-1) \n",
    "        result_ep[[f\"ep_valid_l{i}_time\" for i in range(para_size)]] = ep_valid_time\n",
    "        result_ep[\"ep_train_time\"] = list_ep_train_time \n",
    "        \n",
    "        \n",
    "        result['model'] = [model]*para_size\n",
    "        result['padding_size'] = [text_len]*para_size\n",
    "        result[\"test_layer\"] = [(i) for i in range(para_size)]\n",
    "        result[\"test_acc\"] = list_acc\n",
    "        result[\"test_auc\"] = list_auc\n",
    "        result[\"test_avg_entr\"] = list_entr\n",
    "        result[\"test_time\"] = list_test_time\n",
    "        result[\"init_time\"] = [init_time]*para_size\n",
    "        result[\"train+valid_time\"] = [train_time]*para_size\n",
    "        result[\"train_time\"] = [result_ep[\"ep_train_time\"].sum()]*para_size\n",
    "        result[\"valid_time\"] = [result_ep[\"ep_valid_time\"].sum()]*para_size\n",
    "        result['best_ep'] = best_ep\n",
    "        #print(result)\n",
    "        #print(result_ep)\n",
    "        \"\"\"\n",
    "        path = f\"result/_csv/{data}\"\n",
    "        title = f\"{data}_{model}_l{layer}_pad{text_len}({best_ep})\"\n",
    "        t = [(i) for i in range(para_size)]\n",
    "        try:\n",
    "            os.mkdir(path)\n",
    "        except:\n",
    "            pass \n",
    "        \n",
    "        plt.title(title)\n",
    "        plt.plot(t,result[\"test_time\"],\"o-\") \n",
    "        plt.xlabel(\"shortcut\")\n",
    "        plt.ylabel(\"test time\")\n",
    "        plt.savefig(f\"{path}/{title}_plt_test_time.png\", bbox_inches = \"tight\",facecolor='white', transparent=False)\n",
    "        plt.show()\n",
    "        \n",
    "        plt.title(title)\n",
    "        plt.plot(t,result[\"test_acc\"],\"o-\") \n",
    "        plt.xlabel(\"shortcut\")\n",
    "        plt.ylabel(\"test acc\")\n",
    "        for i in range(para_size):\n",
    "            if (i) == save_th[-1]:\n",
    "                plt.plot((i),result[\"test_acc\"][i],\"ro\") \n",
    "        plt.savefig(f\"{path}/{title}_plt_acc.png\", bbox_inches = \"tight\",facecolor='white', transparent=False)\n",
    "        plt.show()\n",
    "\n",
    "        plt.title(title)\n",
    "        plt.plot(t,result[\"test_auc\"],\"o-\") \n",
    "        plt.xlabel(\"shortcut\")\n",
    "        plt.ylabel(\"test auc\")\n",
    "        for i in range(para_size):\n",
    "            if (i) == save_th[-1]:\n",
    "                plt.plot((i),result[\"test_auc\"][i],\"ro\") \n",
    "        plt.savefig(f\"{path}/{title}_plt_auc.png\", bbox_inches = \"tight\",facecolor='white', transparent=False)\n",
    "        plt.show()\n",
    "        \n",
    "        plt.title(title)\n",
    "        plt.plot(t,result[\"test_avg_entr\"],\"o-\") \n",
    "        plt.xlabel(\"shortcut\")\n",
    "        plt.ylabel(\"test avg entr\")\n",
    "        plt.savefig(f\"{path}/{title}_plt_entr.png\", bbox_inches = \"tight\",facecolor='white', transparent=False)\n",
    "        plt.show()\n",
    "        \"\"\"\n",
    "        #result.to_csv(f\"{path}/{title}.csv\", index=False)\n",
    "        #result_ep.to_csv(f\"{path}/{title}_ep.csv\", index=False)\n",
    "        total_result = pd.concat([total_result,result],axis=0,ignore_index=True)            \n",
    "        del result\n",
    "    path = f\"result/_csv/{data}\"\n",
    "    title = f\"{data}_{model}_l{layer}base\"\n",
    "    total_result.to_csv(f\"{path}/{title}_ep.csv\", index=False)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3e68ddd1",
   "metadata": {},
   "source": [
    "## 0308"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c1e292bc",
   "metadata": {},
   "source": [
    "\n",
    "### fullpath baseline 多次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0c5c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### fullpath baseline\n",
    "### 2/22 - baseline: valid/test with fullpath\n",
    "### 2/26 - dbpedia\n",
    "### 2/28 - imdb 8:1:1\n",
    "### 3/08 - read more files\n",
    "### 3/20 - fullpath training now do shortcut/adaptive inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d8c610",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "total_result = pd.DataFrame(columns=[])\n",
    "\n",
    "#data = \"ag_news\"\n",
    "#for text_len in [25,50,75,100,125,150,175]:\n",
    "\n",
    "#data = \"dbpedia_14\"\n",
    "#for text_len in [20,40,60]:\n",
    "\n",
    "data = \"imdb\"\n",
    "#for text_len in [100,200,300,400,500]:\n",
    "#for test_count in range(5):\n",
    "text_len = 500\n",
    "#text_len = 80\n",
    "#text_len = 177\n",
    "for model in [\"linearal\",\"lstmal\",\"transformeral\"]:\n",
    "    layer = 5\n",
    "    epoch = 20\n",
    "    lr = 0.0001\n",
    "    out_path = f\"result/0318/fullpath+sc_ad/{data}_{model}_l{layer}base512_pad{text_len}/\"\n",
    "\n",
    "    log_list = os.listdir(out_path)\n",
    "    print(log_list)\n",
    "    for log in log_list:\n",
    "        \n",
    "        result = pd.DataFrame(columns=[\"padding_size\"])\n",
    "        result_ep = pd.DataFrame(columns=[\"epoch\",\"ep_train_time\"])\n",
    "        result_ep[\"epoch\"] = [*range(epoch)]\n",
    "        list_acc = []\n",
    "        list_auc = []\n",
    "        list_f1 = []\n",
    "        list_entr = []\n",
    "        list_acc_th = []\n",
    "        list_auc_th = []\n",
    "        list_f1_th = []\n",
    "        list_entr_th = []\n",
    "        list_test_time = []\n",
    "        list_test_time_th = []\n",
    "        list_ep_train_time = []\n",
    "        list_ep_train_acc = []\n",
    "        list_ep_train_auc = []\n",
    "        list_ep_train_entr = []\n",
    "        list_ep_valid_time = []\n",
    "        list_ep_valid_acc = []\n",
    "        list_ep_valid_auc = []\n",
    "        list_ep_valid_entr = []\n",
    "        list_ep_valid_f1 = []\n",
    "        train_time = 0\n",
    "        init_time = 0\n",
    "        best_ep = -1\n",
    "        best_th = []\n",
    "        save_th = []\n",
    "        test_part = False\n",
    "        \n",
    "        #para_size = 1\n",
    "        para_size = 5+9\n",
    "        with open(out_path+log,mode='r') as log:\n",
    "            buffer = log.readlines()\n",
    "            df = pd.DataFrame(buffer,columns=[\"log\"])\n",
    "            df\n",
    "            \n",
    "            for line in buffer:\n",
    "                \n",
    "                \n",
    "                match = re.match(\"Start Testing\", line)\n",
    "                if match!=None:\n",
    "                    test_part = True\n",
    "                    \n",
    "                match = re.match('Save ckpt to (.)*', line)\n",
    "                if match!=None:\n",
    "                    match = re.findall(\"ep \\d+\",match.group(0))\n",
    "                    best_ep = match[0]\n",
    "                    best_th.append(current_th)\n",
    "\n",
    "                match = re.match('init_time(.)*', line)\n",
    "                if match!=None:\n",
    "                    match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                    init_time = float(match[0])\n",
    "                    \n",
    "                match = re.match('(.)*valid_time(.)*', line)\n",
    "                if match!=None:\n",
    "                    match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                    train_time = float(match[0])\n",
    "                    \n",
    "                match = re.match('ep(\\d)+_train_time(.)*', line)\n",
    "                if match!=None and test_part==False:\n",
    "                    match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                    list_ep_train_time.append(float(match[0]))\n",
    "                    \n",
    "                match = re.match('ep(\\d)+_l\\d+_test_time(.)*', line)\n",
    "                if match!=None and test_part==False:\n",
    "                    match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                    list_ep_valid_time.append(float(match[0]))\n",
    "                    \n",
    "                match = re.match('Train Epoch\\d+ Acc(.)*', line)\n",
    "                if match!=None:\n",
    "                    match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                    list_ep_train_acc.append(float(match[0]))\n",
    "                    list_ep_train_auc.append(float(match[1]))\n",
    "                    #print(best_th)\n",
    "                    if len(best_th)>0:\n",
    "                        save_th = best_th\n",
    "                    best_th = []\n",
    "                    \n",
    "                match = re.match('Test Epoch\\d+ layer(.)*', line)\n",
    "                if match!=None and test_part==False:\n",
    "                    match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                    #print(line)\n",
    "                    #print(match)\n",
    "                    \n",
    "                    list_ep_valid_acc.append(float(match[0]))\n",
    "                    list_ep_valid_auc.append(float(match[1]))\n",
    "                    list_ep_valid_entr.append(float(match[2]))\n",
    "                    list_ep_valid_f1.append(float(match[3]))\n",
    "                    \n",
    "                    match = re.findall('\\d+', line)\n",
    "                    current_th = int(match[1])\n",
    "                \n",
    "                ### testing part\n",
    "                match = re.match('Test layer\\d Acc(.)*', line)\n",
    "                if match!=None and test_part==True:\n",
    "                    match = re.findall(\"\\d?\\.\\d+\",match.group(0))\n",
    "                    list_acc.append(float(match[0]))\n",
    "                    list_auc.append(float(match[1]))\n",
    "                    list_entr.append(float(match[2]))\n",
    "                    list_f1.append(float(match[3]))\n",
    "                    \n",
    "                match = re.match('l\\d+_test_time(.)*', line)\n",
    "                if match!=None and test_part==True:\n",
    "                    match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                    list_test_time.append(float(match[0]))\n",
    "                \n",
    "                match = re.match('Test threshold(.)*', line)\n",
    "                if match!=None and test_part==True:\n",
    "                    match = re.findall(\"\\d?\\.\\d+\",match.group(0))\n",
    "                    list_acc_th.append(float(match[1]))\n",
    "                    list_auc_th.append(float(match[2]))\n",
    "                    list_entr_th.append(float(match[3]))\n",
    "                    list_f1_th.append(float(match[4]))\n",
    "                    \n",
    "                match = re.match('t(.)*_test_time(.)*', line)\n",
    "                if match!=None and test_part==True:\n",
    "                    match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                    list_test_time_th.append(float(match[1]))\n",
    "        print(save_th)    \n",
    "        ep_valid_time = np.array(list_ep_valid_time).reshape((epoch,-1))\n",
    "        ep_valid_acc = np.array(list_ep_valid_acc).reshape((epoch,-1))\n",
    "        ep_valid_auc = np.array(list_ep_valid_auc).reshape((epoch,-1))\n",
    "        ep_valid_entr = np.array(list_ep_valid_entr).reshape((epoch,-1))\n",
    "        ep_valid_f1 = np.array(list_ep_valid_entr).reshape((epoch,-1))\n",
    "        \n",
    "        result_ep[\"ep_train_acc\"] = list_ep_train_acc \n",
    "        result_ep[\"ep_train_auc\"] = list_ep_train_auc          \n",
    "        result_ep[\"ep_valid_time\"] = np.sum(ep_valid_time,-1) \n",
    "        result_ep[\"ep_valid_l4_time\"] = ep_valid_time\n",
    "        result_ep[\"ep_train_time\"] = list_ep_train_time \n",
    "        \n",
    "        \n",
    "        result['model'] = [model]*para_size\n",
    "        result['padding_size'] = [text_len]*para_size\n",
    "        #result[\"test_layer\"] = [(i) for i in range(para_size)]\n",
    "        result[\"test_setting\"] = [i for i in range(5)]+[0.1*(i+1) for i in range(9)]\n",
    "        result[\"test_acc\"] = list_acc + list_acc_th\n",
    "        result[\"test_auc\"] = list_auc + list_auc_th\n",
    "        result[\"test_f1\"] = list_f1 + list_f1_th\n",
    "        result[\"test_avg_entr\"] = list_entr + list_entr_th\n",
    "        result[\"test_time\"] = list_test_time + list_test_time_th\n",
    "        result[\"init_time\"] = [init_time]*para_size\n",
    "        result[\"train+valid_time\"] = [train_time]*para_size\n",
    "        result[\"train_time\"] = [result_ep[\"ep_train_time\"].sum()]*para_size\n",
    "        result[\"valid_time\"] = [result_ep[\"ep_valid_time\"].sum()]*para_size\n",
    "        result['best_ep'] = best_ep[3:]\n",
    "        #print(result)\n",
    "        #print(result_ep)\n",
    "        \"\"\"\n",
    "        path = f\"result/_csv/{data}\"\n",
    "        title = f\"{data}_{model}_l{layer}_pad{text_len}({best_ep})\"\n",
    "        t = [(i) for i in range(para_size)]\n",
    "        try:\n",
    "            os.mkdir(path)\n",
    "        except:\n",
    "            pass \n",
    "        \n",
    "        plt.title(title)\n",
    "        plt.plot(t,result[\"test_time\"],\"o-\") \n",
    "        plt.xlabel(\"shortcut\")\n",
    "        plt.ylabel(\"test time\")\n",
    "        plt.savefig(f\"{path}/{title}_plt_test_time.png\", bbox_inches = \"tight\",facecolor='white', transparent=False)\n",
    "        plt.show()\n",
    "        \n",
    "        plt.title(title)\n",
    "        plt.plot(t,result[\"test_acc\"],\"o-\") \n",
    "        plt.xlabel(\"shortcut\")\n",
    "        plt.ylabel(\"test acc\")\n",
    "        for i in range(para_size):\n",
    "            if (i) == save_th[-1]:\n",
    "                plt.plot((i),result[\"test_acc\"][i],\"ro\") \n",
    "        plt.savefig(f\"{path}/{title}_plt_acc.png\", bbox_inches = \"tight\",facecolor='white', transparent=False)\n",
    "        plt.show()\n",
    "\n",
    "        plt.title(title)\n",
    "        plt.plot(t,result[\"test_auc\"],\"o-\") \n",
    "        plt.xlabel(\"shortcut\")\n",
    "        plt.ylabel(\"test auc\")\n",
    "        for i in range(para_size):\n",
    "            if (i) == save_th[-1]:\n",
    "                plt.plot((i),result[\"test_auc\"][i],\"ro\") \n",
    "        plt.savefig(f\"{path}/{title}_plt_auc.png\", bbox_inches = \"tight\",facecolor='white', transparent=False)\n",
    "        plt.show()\n",
    "        \n",
    "        plt.title(title)\n",
    "        plt.plot(t,result[\"test_avg_entr\"],\"o-\") \n",
    "        plt.xlabel(\"shortcut\")\n",
    "        plt.ylabel(\"test avg entr\")\n",
    "        plt.savefig(f\"{path}/{title}_plt_entr.png\", bbox_inches = \"tight\",facecolor='white', transparent=False)\n",
    "        plt.show()\n",
    "        \"\"\"\n",
    "        #result.to_csv(f\"{path}/{title}.csv\", index=False)\n",
    "        #result_ep.to_csv(f\"{path}/{title}_ep.csv\", index=False)\n",
    "        total_result = pd.concat([total_result,result],axis=0,ignore_index=True)            \n",
    "        del result\n",
    "path = f\"result/_csv/{data}\"\n",
    "os.makedirs(path, exist_ok=True)\n",
    "title = f\"{data}_l{layer}base_512\"\n",
    "total_result.to_csv(f\"{path}/{title}.csv\", index=False)\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5a60f5d6",
   "metadata": {},
   "source": [
    "### adapt 多次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e868a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "total_result = pd.DataFrame(columns=[])\n",
    "\n",
    "### 2/04 - valid/test 80%:20%\n",
    "###      - save_path unique for test load\n",
    "###      - \"lstmal\",\"linearal\",\"transformeral\" adapt valid pad save best auc\n",
    "### 2/08 - valid/test 50%:50%\n",
    "#data = \"ag_news\"\n",
    "#for text_len in [25,50,75,100,125,150,175]:\n",
    "#for text_len in [50,75,100,125,150,175]:\n",
    "### 2/18 - dbpedia\n",
    "#data = \"dbpedia_14\"\n",
    "#for text_len in [20,40,60]:\n",
    "data = \"imdb\"\n",
    "\n",
    "text_len = 500\n",
    "#text_len = 177\n",
    "for model in [\"lstmal\",\"linearal\",\"transformeral\"]:\n",
    "    layer = 5\n",
    "    epoch = 30\n",
    "    lr = 0.0001\n",
    "    save_path = f\"ckpt/{data}_{model}_l{layer}ad_pad{text_len}/\"\n",
    "    out_path = f\"result/0315/timer fixed/imdb/{data}_{model}_l{layer}ad_pad{text_len}/\"\n",
    "    log_list = os.listdir(out_path)\n",
    "    print(log_list)\n",
    "    for log in log_list:\n",
    "        \n",
    "        result = pd.DataFrame(columns=[\"padding_size\"])\n",
    "        result_ep = pd.DataFrame(columns=[\"epoch\",\"ep_train_time\"])\n",
    "        result_ep[\"epoch\"] = [*range(epoch)]\n",
    "        list_acc = []\n",
    "        list_auc = []\n",
    "        list_entr = []\n",
    "        list_f1 = []\n",
    "        list_test_time = []\n",
    "        list_ep_train_time = []\n",
    "        list_ep_train_acc = []\n",
    "        list_ep_train_auc = []\n",
    "        list_ep_train_entr = []\n",
    "        list_ep_valid_time = []\n",
    "        list_ep_valid_acc = []\n",
    "        list_ep_valid_auc = []\n",
    "        list_ep_valid_entr = []\n",
    "        train_time = 0\n",
    "        init_time = 0\n",
    "        best_ep = -1\n",
    "        best_th = []\n",
    "        save_th = []\n",
    "        with open(out_path+log,mode='r') as log:\n",
    "            buffer = log.readlines()\n",
    "            df = pd.DataFrame(buffer,columns=[\"log\"])\n",
    "            df\n",
    "            \n",
    "            for line in buffer:\n",
    "                \n",
    "                \n",
    "                match = re.match('Save ckpt to (.)*', line)\n",
    "                if match!=None:\n",
    "                    match = re.findall(\"ep \\d+\",match.group(0))\n",
    "                    best_ep = match[0]\n",
    "                    best_th.append(current_th)\n",
    "                match = re.match('Test threshold(.)*', line)\n",
    "                if match!=None:\n",
    "                    match = re.findall(\"\\d?\\.\\d+\",match.group(0))\n",
    "                    list_acc.append(float(match[1]))\n",
    "                    list_auc.append(float(match[2]))\n",
    "                    list_entr.append(float(match[3]))\n",
    "                    list_f1.append(float(match[4]))\n",
    "                    \n",
    "\n",
    "                match = re.match('t(.)*_test_time(.)*', line)\n",
    "                if match!=None:\n",
    "                    match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                    list_test_time.append(float(match[1]))\n",
    "\n",
    "                match = re.match('init_time(.)*', line)\n",
    "                if match!=None:\n",
    "                    match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                    init_time = float(match[0])\n",
    "                match = re.match('(.)*valid_time(.)*', line)\n",
    "                if match!=None:\n",
    "                    match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                    train_time = float(match[0])\n",
    "                    \n",
    "                match = re.match('ep(\\d)+_train_time(.)*', line)\n",
    "                if match!=None:\n",
    "                    match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                    list_ep_train_time.append(float(match[0]))\n",
    "                    \n",
    "                match = re.match('ep(\\d)+_t(.)*_test_time(.)*', line)\n",
    "                if match!=None:\n",
    "                    match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                    list_ep_valid_time.append(float(match[1]))\n",
    "                    \n",
    "                match = re.match('Train Epoch\\d+ Acc(.)*', line)\n",
    "                if match!=None:\n",
    "                    match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                    list_ep_train_acc.append(float(match[0]))\n",
    "                    list_ep_train_auc.append(float(match[1]))\n",
    "                    #print(best_th)\n",
    "                    if len(best_th)>0:\n",
    "                        save_th = best_th\n",
    "                    best_th = []\n",
    "                    \n",
    "                match = re.match('Test Epoch\\d+ threshold(.)*', line)\n",
    "                if match!=None:\n",
    "                    match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                    #print(line)\n",
    "                    #print(match)\n",
    "                    current_th = int(float(match[0])*10)\n",
    "                    list_ep_valid_acc.append(float(match[1]))\n",
    "                    list_ep_valid_auc.append(float(match[2]))\n",
    "                    list_ep_valid_entr.append(float(match[3]))\n",
    "        print(save_th)    \n",
    "        ep_valid_time = np.array(list_ep_valid_time).reshape((epoch,-1))\n",
    "        ep_valid_acc = np.array(list_ep_valid_acc).reshape((epoch,-1))\n",
    "        ep_valid_auc = np.array(list_ep_valid_auc).reshape((epoch,-1))\n",
    "        ep_valid_entr = np.array(list_ep_valid_entr).reshape((epoch,-1))\n",
    "        \n",
    "        result_ep[\"ep_train_acc\"] = list_ep_train_acc \n",
    "        result_ep[\"ep_train_auc\"] = list_ep_train_auc          \n",
    "        \n",
    "        result_ep[\"ep_valid_time\"] = np.sum(ep_valid_time,-1) \n",
    "        result_ep[[f\"ep_valid.{(i+1)}_time\" for i in range(9)]] = ep_valid_time\n",
    "        result_ep[\"ep_train_time\"] = list_ep_train_time \n",
    "        \n",
    "        para_size = 9\n",
    "        result['model'] = [model]*para_size\n",
    "        result['padding_size'] = [text_len]*para_size\n",
    "        result[\"test_threshold\"] = [(1+i)*0.1 for i in range(para_size)]\n",
    "        result[\"best_ep\"] = [best_ep[2:]]*para_size\n",
    "        result[\"best_th\"] = [save_th]*para_size\n",
    "        result[\"best_acc_setting\"] = [list_acc[save_th[-1]-1]]*para_size\n",
    "        result[\"best_auc_setting\"] = [list_auc[save_th[-1]-1]]*para_size\n",
    "        result[\"best_f1_setting\"] = [list_f1[save_th[-1]-1]]*para_size\n",
    "        result[\"best_entr_setting\"] = [list_entr[save_th[-1]-1]]*para_size\n",
    "        result[\"test_time_setting\"] = [list_test_time[save_th[-1]-1]]*para_size\n",
    "        result[\"test_acc\"] = list_acc\n",
    "        result[\"test_auc\"] = list_auc\n",
    "        result[\"test_f1\"] = list_f1\n",
    "        result[\"test_avg_entr\"] = list_entr\n",
    "        result[\"test_time\"] = list_test_time\n",
    "        result[\"init_time\"] = [init_time]*para_size\n",
    "        result[\"train+valid_time\"] = [train_time]*para_size\n",
    "        result[\"train_time\"] = [result_ep[\"ep_train_time\"].sum()]*para_size\n",
    "        result[\"valid_time\"] = [result_ep[\"ep_valid_time\"].sum()]*para_size\n",
    "        #print(result)\n",
    "        #print(result_ep)\n",
    "\n",
    "        path = f\"result/_csv/{data}/base_ad/\"\n",
    "        title = f\"{data}_{model}_l{layer}ad_pad{text_len}({best_ep})\"\n",
    "        os.makedirs(path,exist_ok=True)\n",
    "        \"\"\"\n",
    "        t = [0.1*(i+1) for i in range(9)]\n",
    "        plt.title(title)\n",
    "        plt.plot(t,result[\"test_time\"],\"o-\") \n",
    "        plt.xlabel(\"threshold\")\n",
    "        plt.ylabel(\"test time\")\n",
    "        plt.savefig(f\"{path}/{title}_plt_test_time.png\", bbox_inches = \"tight\",facecolor='white', transparent=False)\n",
    "        plt.show()\n",
    "        \n",
    "        plt.title(title)\n",
    "        plt.plot(t,result[\"test_acc\"],\"o-\") \n",
    "        plt.xlabel(\"threshold\")\n",
    "        plt.ylabel(\"test acc\")\n",
    "        for i in range(9):\n",
    "            if (i+1) in save_th:\n",
    "                plt.plot((i+1)/10,result[\"test_acc\"][i],\"ro\") \n",
    "        plt.savefig(f\"{path}/{title}_plt_acc.png\", bbox_inches = \"tight\",facecolor='white', transparent=False)\n",
    "        plt.show()\n",
    "\n",
    "        plt.title(title)\n",
    "        plt.plot(t,result[\"test_auc\"],\"o-\") \n",
    "        plt.xlabel(\"threshold\")\n",
    "        plt.ylabel(\"test auc\")\n",
    "        for i in range(9):\n",
    "            if (i+1) in save_th:\n",
    "                plt.plot((i+1)/10,result[\"test_auc\"][i],\"ro\") \n",
    "        plt.savefig(f\"{path}/{title}_plt_auc.png\", bbox_inches = \"tight\",facecolor='white', transparent=False)\n",
    "        plt.show()\n",
    "        \n",
    "        plt.title(title)\n",
    "        plt.plot(t,result[\"test_avg_entr\"],\"o-\") \n",
    "        plt.xlabel(\"threshold\")\n",
    "        plt.ylabel(\"test avg entr\")\n",
    "        plt.savefig(f\"{path}/{title}_plt_entr.png\", bbox_inches = \"tight\",facecolor='white', transparent=False)\n",
    "        plt.show()\n",
    "        \"\"\"\n",
    "        #result.to_csv(f\"{path}/{title}.csv\", index=False)\n",
    "        #result_ep.to_csv(f\"{path}/{title}_ep.csv\", index=False)\n",
    "        total_result = pd.concat([total_result,result],axis=0,ignore_index=True)            \n",
    "        del result\n",
    "\n",
    "title = f\"{data}_l{layer}ad\"\n",
    "total_result.to_csv(f\"{path}/{title}.csv\", index=False)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a95707a4",
   "metadata": {},
   "source": [
    "### shortcut 多次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5220f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "total_result = pd.DataFrame(columns=[])\n",
    "\n",
    "### baseline shortcut\n",
    "\n",
    "### 2/04 - valid/test 80%:20%\n",
    "###      - baseline\n",
    "### 2/14 - valid/test 50%:50%\n",
    "#data = \"ag_news\"\n",
    "#for text_len in [25,50,75,100,125,150,175]:\n",
    "### 2/18 - dbpedia\n",
    "#data = \"dbpedia_14\"\n",
    "#for text_len in [20,40,60]:\n",
    "### 2/28 - imdb 8:1:1\n",
    "data = \"imdb\"\n",
    "#for text_len in [100,200,300,400,500]:\n",
    "\n",
    "text_len = 500\n",
    "#text_len = 177\n",
    "for model in [\"linearal\",\"lstmal\",\"transformeral\",]:\n",
    "    layer = 5\n",
    "    epoch = 30\n",
    "    lr = 0.0001\n",
    "    save_path = f\"ckpt/{data}_{model}_l{layer}_pad{text_len}/\"\n",
    "    out_path = f\"result/0315/timer fixed/imdb/{data}_{model}_l{layer}_pad{text_len}/\"\n",
    "    log_list = os.listdir(out_path)\n",
    "    print(log_list)\n",
    "    for log in log_list:\n",
    "        result = pd.DataFrame(columns=[\"padding_size\"])\n",
    "        result_ep = pd.DataFrame(columns=[\"epoch\",\"ep_train_time\"])\n",
    "        result_ep[\"epoch\"] = [*range(epoch)]\n",
    "        list_acc = []\n",
    "        list_auc = []\n",
    "        list_f1 = []\n",
    "        list_entr = []\n",
    "        list_test_time = []\n",
    "        list_ep_train_time = []\n",
    "        list_ep_train_acc = []\n",
    "        list_ep_train_auc = []\n",
    "        list_ep_train_entr = []\n",
    "        list_ep_valid_time = []\n",
    "        list_ep_valid_acc = []\n",
    "        list_ep_valid_auc = []\n",
    "        list_ep_valid_entr = []\n",
    "        train_time = 0\n",
    "        init_time = 0\n",
    "        best_ep = -1\n",
    "        best_th = []\n",
    "        save_th = []\n",
    "        test_part = False\n",
    "        \n",
    "        para_size = 5\n",
    "        with open(out_path + log,mode='r') as log:\n",
    "            buffer = log.readlines()\n",
    "            df = pd.DataFrame(buffer,columns=[\"log\"])\n",
    "            df\n",
    "            \n",
    "            for line in buffer:\n",
    "                \n",
    "                \n",
    "                match = re.match(\"Start Testing\", line)\n",
    "                if match!=None:\n",
    "                    test_part = True\n",
    "                    \n",
    "                match = re.match('Save ckpt to (.)*', line)\n",
    "                if match!=None:\n",
    "                    match = re.findall(\"ep \\d+\",match.group(0))\n",
    "                    best_ep = match[0]\n",
    "                    best_th.append(current_th)\n",
    "                \n",
    "                match = re.match('Test layer\\d+(.)*', line)    \n",
    "                if match!=None and test_part==True:\n",
    "                    match = re.findall(\"\\d?\\.\\d+\",match.group(0))\n",
    "                    list_acc.append(float(match[0]))\n",
    "                    list_auc.append(float(match[1]))\n",
    "                    list_entr.append(float(match[2]))\n",
    "                    list_f1.append(float(match[3]))\n",
    "                    \n",
    "\n",
    "                match = re.match('l\\d+_test_time(.)*', line)\n",
    "                if match!=None and test_part==True:\n",
    "                    match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                    list_test_time.append(float(match[0]))\n",
    "\n",
    "                match = re.match('init_time(.)*', line)\n",
    "                if match!=None:\n",
    "                    match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                    init_time = float(match[0])\n",
    "                    \n",
    "                match = re.match('(.)*valid_time(.)*', line)\n",
    "                if match!=None:\n",
    "                    match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                    train_time = float(match[0])\n",
    "                    \n",
    "                match = re.match('ep(\\d)+_train_time(.)*', line)\n",
    "                if match!=None and test_part==False:\n",
    "                    match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                    list_ep_train_time.append(float(match[0]))\n",
    "                    \n",
    "                match = re.match('ep(\\d)+_l\\d+_test_time(.)*', line)\n",
    "                if match!=None and test_part==False:\n",
    "                    match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                    list_ep_valid_time.append(float(match[0]))\n",
    "                    \n",
    "                match = re.match('Train Epoch\\d+ Acc(.)*', line)\n",
    "                if match!=None:\n",
    "                    match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                    list_ep_train_acc.append(float(match[0]))\n",
    "                    list_ep_train_auc.append(float(match[1]))\n",
    "                    #print(best_th)\n",
    "                    if len(best_th)>0:\n",
    "                        save_th = best_th\n",
    "                    best_th = []\n",
    "                    \n",
    "                match = re.match('Test Epoch\\d+ layer(.)*', line)\n",
    "                if match!=None and test_part==False:\n",
    "                    match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                    #print(line)\n",
    "                    #print(match)\n",
    "                    \n",
    "                    list_ep_valid_acc.append(float(match[0]))\n",
    "                    list_ep_valid_auc.append(float(match[1]))\n",
    "                    list_ep_valid_entr.append(float(match[2]))\n",
    "                    \n",
    "                    match = re.findall('\\d+', line)\n",
    "                    current_th = int(match[1])\n",
    "        print(save_th)    \n",
    "        ep_valid_time = np.array(list_ep_valid_time).reshape((epoch,-1))\n",
    "        ep_valid_acc = np.array(list_ep_valid_acc).reshape((epoch,-1))\n",
    "        ep_valid_auc = np.array(list_ep_valid_auc).reshape((epoch,-1))\n",
    "        ep_valid_entr = np.array(list_ep_valid_entr).reshape((epoch,-1))\n",
    "        \n",
    "        result_ep[\"ep_train_acc\"] = list_ep_train_acc \n",
    "        result_ep[\"ep_train_auc\"] = list_ep_train_auc          \n",
    "        \n",
    "        result_ep[\"ep_valid_time\"] = np.sum(ep_valid_time,-1) \n",
    "        result_ep[[f\"ep_valid_l{i}_time\" for i in range(para_size)]] = ep_valid_time\n",
    "        result_ep[\"ep_train_time\"] = list_ep_train_time \n",
    "        \n",
    "        \n",
    "        result['model'] = [model]*para_size\n",
    "        result['padding_size'] = [text_len]*para_size\n",
    "        result[\"test_layer\"] = [(i) for i in range(para_size)]\n",
    "        result[\"best_ep\"] = [best_ep[2:]]*para_size\n",
    "        result[\"best_th\"] = [save_th]*para_size\n",
    "        result[\"best_acc_setting\"] = [list_acc[save_th[-1]]]*para_size\n",
    "        result[\"best_auc_setting\"] = [list_auc[save_th[-1]]]*para_size\n",
    "        result[\"best_f1_setting\"] = [list_f1[save_th[-1]]]*para_size\n",
    "        result[\"best_entr_setting\"] = [list_entr[save_th[-1]]]*para_size\n",
    "        result[\"test_time_setting\"] = [list_test_time[save_th[-1]]]*para_size\n",
    "        result[\"test_acc\"] = list_acc\n",
    "        result[\"test_auc\"] = list_auc\n",
    "        result[\"test_f1\"] = list_f1\n",
    "        result[\"test_avg_entr\"] = list_entr\n",
    "        result[\"test_time\"] = list_test_time\n",
    "        result[\"init_time\"] = [init_time]*para_size\n",
    "        result[\"train+valid_time\"] = [train_time]*para_size\n",
    "        result[\"train_time\"] = [result_ep[\"ep_train_time\"].sum()]*para_size\n",
    "        result[\"valid_time\"] = [result_ep[\"ep_valid_time\"].sum()]*para_size\n",
    "        #print(result)\n",
    "        #print(result_ep)\n",
    "\n",
    "        path = f\"result/_csv/{data}/base_sc/\"\n",
    "        title = f\"{data}_{model}_l{layer}_pad{text_len}({best_ep})\"\n",
    "        t = [(i) for i in range(para_size)]\n",
    "        \n",
    "        os.makedirs(path,exist_ok=True)\n",
    "\n",
    "        \"\"\"\n",
    "        plt.title(title)\n",
    "        plt.plot(t,result[\"test_time\"],\"o-\") \n",
    "        plt.xlabel(\"shortcut\")\n",
    "        plt.ylabel(\"test time\")\n",
    "        plt.savefig(f\"{path}/{title}_plt_test_time.png\", bbox_inches = \"tight\",facecolor='white', transparent=False)\n",
    "        plt.show()\n",
    "        \n",
    "        plt.title(title)\n",
    "        plt.plot(t,result[\"test_acc\"],\"o-\") \n",
    "        plt.xlabel(\"shortcut\")\n",
    "        plt.ylabel(\"test acc\")\n",
    "        for i in range(para_size):\n",
    "            if (i) == save_th[-1]:\n",
    "                plt.plot((i),result[\"test_acc\"][i],\"ro\") \n",
    "        plt.savefig(f\"{path}/{title}_plt_acc.png\", bbox_inches = \"tight\",facecolor='white', transparent=False)\n",
    "        plt.show()\n",
    "\n",
    "        plt.title(title)\n",
    "        plt.plot(t,result[\"test_auc\"],\"o-\") \n",
    "        plt.xlabel(\"shortcut\")\n",
    "        plt.ylabel(\"test auc\")\n",
    "        for i in range(para_size):\n",
    "            if (i) == save_th[-1]:\n",
    "                plt.plot((i),result[\"test_auc\"][i],\"ro\") \n",
    "        plt.savefig(f\"{path}/{title}_plt_auc.png\", bbox_inches = \"tight\",facecolor='white', transparent=False)\n",
    "        plt.show()\n",
    "        \n",
    "        plt.title(title)\n",
    "        plt.plot(t,result[\"test_avg_entr\"],\"o-\") \n",
    "        plt.xlabel(\"shortcut\")\n",
    "        plt.ylabel(\"test avg entr\")\n",
    "        plt.savefig(f\"{path}/{title}_plt_entr.png\", bbox_inches = \"tight\",facecolor='white', transparent=False)\n",
    "        plt.show()\n",
    "        \"\"\"\n",
    "        #result.to_csv(f\"{path}/{title}.csv\", index=False)\n",
    "        #result_ep.to_csv(f\"{path}/{title}_ep.csv\", index=False)\n",
    "        total_result = pd.concat([total_result,result],axis=0,ignore_index=True)            \n",
    "        del result\n",
    "    \n",
    "title = f\"{data}_l{layer}\"\n",
    "total_result.to_csv(f\"{path}/{title}.csv\", index=False)\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "53db5bde",
   "metadata": {},
   "source": [
    "### side adapt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e968794d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "total_result = pd.DataFrame(columns=[])\n",
    "\n",
    "#data = \"ag_news\"\n",
    "#for text_len in [25,50,75,100,125,150,175]:\n",
    "\n",
    "#data = \"dbpedia_14\"\n",
    "#for text_len in [20,40,60]:\n",
    "\n",
    "data = \"imdb\"\n",
    "#for text_len in [100,200,300,400,500]:\n",
    "#for test_count in range(5):\n",
    "text_len = 200\n",
    "#text_len = 80\n",
    "#text_len = 175\n",
    "for model in [\"transformeralside\",\"linearalside\",\"lstmalside\"]:\n",
    "    layer = 5\n",
    "    epoch = 50\n",
    "    lr = 0.0005\n",
    "    out_path = f\"result/0328/256 200 0.0005/{data}_{model}_l{layer}_256_sidead/\"\n",
    "    log_list = os.listdir(out_path)\n",
    "    print(log_list)\n",
    "    for log in log_list:\n",
    "        \n",
    "        result = pd.DataFrame(columns=[\"padding_size\"])\n",
    "        result_ep = pd.DataFrame(columns=[\"epoch\",\"ep_train_time\"])\n",
    "        result_ep[\"epoch\"] = [*range(epoch)]\n",
    "        list_acc = []\n",
    "        list_auc = []\n",
    "        list_f1 = []\n",
    "        list_entr = []\n",
    "        list_acc_th = []\n",
    "        list_auc_th = []\n",
    "        list_f1_th = []\n",
    "        list_entr_th = []\n",
    "        list_test_time = []\n",
    "        list_test_time_th = []\n",
    "        list_ep_train_time = []\n",
    "        list_ep_train_acc = []\n",
    "        list_ep_train_auc = []\n",
    "        list_ep_train_entr = []\n",
    "        list_ep_valid_time = []\n",
    "        list_ep_valid_acc = []\n",
    "        list_ep_valid_auc = []\n",
    "        list_ep_valid_entr = []\n",
    "        list_ep_valid_f1 = []\n",
    "        train_time = 0\n",
    "        init_time = 0\n",
    "        best_ep = -1\n",
    "        best_th = []\n",
    "        save_th = []\n",
    "        test_part = False\n",
    "        \n",
    "        #para_size = 1\n",
    "        para_size = 5+9\n",
    "        with open(out_path+log,mode='r') as log:\n",
    "            buffer = log.readlines()\n",
    "            df = pd.DataFrame(buffer,columns=[\"log\"])\n",
    "            df\n",
    "            \n",
    "            for line in buffer:\n",
    "                \n",
    "                \n",
    "                match = re.match(\"Start Testing\", line)\n",
    "                if match!=None:\n",
    "                    test_part = True\n",
    "                    \n",
    "                match = re.match('Save ckpt to (.)*', line)\n",
    "                if match!=None:\n",
    "                    match = re.findall(\"ep \\d+\",match.group(0))\n",
    "                    best_ep = match[0]\n",
    "                    best_th.append(current_th)\n",
    "\n",
    "                match = re.match('init_time(.)*', line)\n",
    "                if match!=None:\n",
    "                    match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                    init_time = float(match[0])\n",
    "                    \n",
    "                match = re.match('(.)*valid_time(.)*', line)\n",
    "                if match!=None:\n",
    "                    match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                    train_time = float(match[0])\n",
    "                    \n",
    "                match = re.match('ep(\\d)+_train_time(.)*', line)\n",
    "                if match!=None and test_part==False:\n",
    "                    match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                    list_ep_train_time.append(float(match[0]))\n",
    "                    \n",
    "                match = re.match('ep(\\d)+_l\\d+_test_time(.)*', line)\n",
    "                if match!=None and test_part==False:\n",
    "                    match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                    list_ep_valid_time.append(float(match[0]))\n",
    "                    \n",
    "                match = re.match('Train Epoch\\d+ Acc(.)*', line)\n",
    "                if match!=None:\n",
    "                    match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                    list_ep_train_acc.append(float(match[0]))\n",
    "                    list_ep_train_auc.append(float(match[1]))\n",
    "                    #print(best_th)\n",
    "                    if len(best_th)>0:\n",
    "                        save_th = best_th\n",
    "                    best_th = []\n",
    "                    \n",
    "                match = re.match('Test Epoch\\d+ layer(.)*', line)\n",
    "                if match!=None and test_part==False:\n",
    "                    match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                    #print(line)\n",
    "                    #print(match)\n",
    "                    \n",
    "                    list_ep_valid_acc.append(float(match[0]))\n",
    "                    list_ep_valid_auc.append(float(match[1]))\n",
    "                    list_ep_valid_entr.append(float(match[2]))\n",
    "                    list_ep_valid_f1.append(float(match[3]))\n",
    "                    \n",
    "                    match = re.findall('\\d+', line)\n",
    "                    current_th = int(match[1])\n",
    "                \n",
    "                ### testing part\n",
    "                match = re.match('Test layer\\d Acc(.)*', line)\n",
    "                if match!=None and test_part==True:\n",
    "                    match = re.findall(\"\\d?\\.\\d+\",match.group(0))\n",
    "                    list_acc.append(float(match[0]))\n",
    "                    list_auc.append(float(match[1]))\n",
    "                    list_entr.append(float(match[2]))\n",
    "                    list_f1.append(float(match[3]))\n",
    "                    \n",
    "                match = re.match('l\\d+_test_time(.)*', line)\n",
    "                if match!=None and test_part==True:\n",
    "                    match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                    list_test_time.append(float(match[0]))\n",
    "                \n",
    "                match = re.match('Test threshold(.)*', line)\n",
    "                if match!=None and test_part==True:\n",
    "                    match = re.findall(\"\\d?\\.\\d+\",match.group(0))\n",
    "                    list_acc_th.append(float(match[1]))\n",
    "                    list_auc_th.append(float(match[2]))\n",
    "                    list_entr_th.append(float(match[3]))\n",
    "                    list_f1_th.append(float(match[4]))\n",
    "                    \n",
    "                match = re.match('t(.)*_test_time(.)*', line)\n",
    "                if match!=None and test_part==True:\n",
    "                    match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                    list_test_time_th.append(float(match[1]))\n",
    "        print(save_th)    \n",
    "        \n",
    "        ep_valid_time = np.array(list_ep_valid_time).reshape((epoch,-1))\n",
    "        ep_valid_acc = np.array(list_ep_valid_acc).reshape((epoch,-1))\n",
    "        ep_valid_auc = np.array(list_ep_valid_auc).reshape((epoch,-1))\n",
    "        ep_valid_entr = np.array(list_ep_valid_entr).reshape((epoch,-1))\n",
    "        ep_valid_f1 = np.array(list_ep_valid_entr).reshape((epoch,-1))\n",
    "        print()    \n",
    "        result_ep[\"ep_train_acc\"] = list_ep_train_acc \n",
    "        result_ep[\"ep_train_auc\"] = list_ep_train_auc          \n",
    "        result_ep[\"ep_valid_time\"] = np.sum(ep_valid_time,-1) \n",
    "        #result_ep[\"ep_valid_l4_time\"] = ep_valid_time\n",
    "        result_ep[\"ep_train_time\"] = list_ep_train_time \n",
    "        \n",
    "        \n",
    "        result['model'] = [model]*para_size\n",
    "        result['padding_size'] = [text_len]*para_size\n",
    "        #result[\"test_layer\"] = [(i) for i in range(para_size)]\n",
    "        result[\"test_setting\"] = [i for i in range(5)]+[0.1*(i+1) for i in range(9)]\n",
    "        result[\"test_acc\"] = list_acc + list_acc_th\n",
    "        result[\"test_auc\"] = list_auc + list_auc_th\n",
    "        result[\"test_f1\"] = list_f1 + list_f1_th\n",
    "        result[\"test_avg_entr\"] = list_entr + list_entr_th\n",
    "        result[\"test_time\"] = list_test_time + list_test_time_th\n",
    "        result[\"init_time\"] = [init_time]*para_size\n",
    "        result[\"train+valid_time\"] = [train_time]*para_size\n",
    "        result[\"train_time\"] = [result_ep[\"ep_train_time\"].sum()]*para_size\n",
    "        result[\"valid_time\"] = [result_ep[\"ep_valid_time\"].sum()]*para_size\n",
    "        result['best_ep'] = best_ep[3:]\n",
    "        #print(result)\n",
    "        #print(result_ep)\n",
    "        #result.to_csv(f\"{path}/{title}.csv\", index=False)\n",
    "        #result_ep.to_csv(f\"{path}/{title}_ep.csv\", index=False)\n",
    "        total_result = pd.concat([total_result,result],axis=0,ignore_index=True)            \n",
    "        del result\n",
    "path = f\"result/_csv/{data}\"\n",
    "os.makedirs(path, exist_ok=True)\n",
    "title = f\"{data}_l{layer}side200\"\n",
    "total_result.to_csv(f\"{path}/{title}.csv\", index=False)\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "751652b6",
   "metadata": {},
   "source": [
    "## 0426"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8bf263",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "datalist = [\"cifar10\", \"cifar100\", \"tinyImageNet\"]\n",
    "model_list = [\"CNN_AL\",\"VGG_AL\",\"resnet_AL\",]\n",
    "for data in datalist:\n",
    "    for model in model_list:\n",
    "        layer = 4\n",
    "        epoch = 200\n",
    "        lr = 0.001\n",
    "        aug_type = \"strong\"\n",
    "        save_path = f\"ckpt/{data}_{model}_l{layer}_{aug_type}/\"\n",
    "        \n",
    "        #out_path = f\"result/0501/adapt rewrite/conf max/{data}_{model}_l{layer}_{aug_type}_max/\"\n",
    "        out_path = f\"result/0418/strong b128/{data}_{model}_l{layer}_{aug_type}/\"\n",
    "\n",
    "        \n",
    "        log_list = os.listdir(out_path)\n",
    "        #print(\"files: \",log_list)\n",
    "        avg_acc = []\n",
    "        avg_time = []\n",
    "        for log in log_list:\n",
    "            para_size = 5+9\n",
    "            with open(out_path+log,mode='r') as log:\n",
    "                buffer = log.readlines()\n",
    "                list_ep_train_acc=[]\n",
    "                list_ep_valid_acc=[]\n",
    "                list_ep_valid_time=[]\n",
    "                for line in buffer:\n",
    "                    match = re.match('Train Epoch\\d+ Acc(.)*', line)\n",
    "                    if match!=None:\n",
    "                        match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                        list_ep_train_acc.append(float(match[0]))\n",
    "\n",
    "                        \n",
    "                    match = re.match('Test Epoch\\d+ layer(.)*', line)\n",
    "                    if match!=None:\n",
    "                        match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                        list_ep_valid_acc.append(float(match[0]))\n",
    "                    \n",
    "                    match = re.match(\"Test threshold \\d+\\.\\d+ Acc(.)*\", line)\n",
    "                    if match!=None:\n",
    "                        match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                        list_ep_valid_acc.append(float(match[1]))\n",
    "                    \n",
    "                    match = re.match('ep\\d+_l\\d+_test_time(.)*', line)\n",
    "                    if match!=None:\n",
    "                        match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                        list_ep_valid_time.append(float(match[0]))\n",
    "                        \n",
    "                    match = re.match('t\\d+\\.\\d+_test_time(.)*', line)\n",
    "                    if match!=None:\n",
    "                        match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                        list_ep_valid_time.append(float(match[1]))    \n",
    "                    match = re.match('Best AUC tensor(.)*', line)\n",
    "                    if match!=None:\n",
    "                        match = re.findall(\"[0-9]+\\.?[0-9]+\",match.group(0))\n",
    "                        best_ep = int(match[1])\n",
    "                        best_acc = float(match[0])\n",
    "                #print(len(list_ep_train_acc))\n",
    "                #print(len(list_ep_valid_acc))\n",
    "                #print(len(list_ep_valid_time))\n",
    "                list_ep_train_acc = np.array(list_ep_train_acc)\n",
    "                list_ep_valid_acc = np.array(list_ep_valid_acc).reshape((-1, 13))\n",
    "                list_ep_valid_time = np.array(list_ep_valid_time).reshape((-1, 13))\n",
    "                \n",
    "                #print(list_ep_valid_acc[best_ep,:])\n",
    "                #print(list_ep_valid_time[best_ep,:])\n",
    "                avg_acc.append(list_ep_valid_acc[best_ep,:])\n",
    "                avg_time.append(list_ep_valid_time[best_ep,:]) \n",
    "        avg_acc = np.array(avg_acc)\n",
    "        avg_time = np.array(avg_time)\n",
    "        avg_acc = (np.mean(avg_acc, axis=0))\n",
    "        avg_time = (np.mean(avg_time, axis=0))\n",
    "        print(best_acc)\n",
    "        #print(f\"best_ep: {best_ep}\")\n",
    "        #print(\" \".join([str(v) for v in avg_acc]))\n",
    "        #print(avg_time)\n",
    "        \"\"\"\n",
    "        print((f\"{data} {model}\"))\n",
    "        print((\"shortcut/adaptive inference\"))\n",
    "        print((\"confidence type: Max yi\"))\n",
    "        plt.title(f\"{model} testing ACC vs Time\")\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.plot(avg_time[:4],avg_acc[:4],\"bo-\", label='shortcut inference')\n",
    "        plt.plot(avg_time[4:],avg_acc[4:],\"ro-\", label='adaptive inference')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        print((\"shortcut inference\"))\n",
    "        plt.title(f\"{model} testing ACC vs shortcut depth\")\n",
    "        plt.xlabel(\"shortcut depth\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        x = [f\"L{i}\" for i in range(4)]\n",
    "        plt.plot(x,avg_acc[:4],\"bo-\", label='shortcut inference')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        print((\"adaptive inference\"))\n",
    "        plt.title(f\"{model} testing ACC vs threshold\")\n",
    "        plt.xlabel(\"threshold\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        x = [0.1*(i+1) for i in reversed(range(9))]\n",
    "        plt.plot(x,avg_acc[4:],\"ro-\", label='adaptive inference')\n",
    "        plt.legend()\n",
    "        plt.show()      \n",
    "        \"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d97cb7a2",
   "metadata": {},
   "source": [
    "### layer-by-layer training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae75e5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for data in [\"cifar100\",]:\n",
    "    for model in [\"CNN_AL\",\"VGG_AL\",\"resnet_AL\",]:\n",
    "        layer = 4\n",
    "        epoch = 400\n",
    "        lr = 0.0001\n",
    "        aug_type = \"strong\"\n",
    "        out_path = f\"result/0509/prefix/300/{data}_{model}_l{layer}_{aug_type}_lbl_pre/\"\n",
    "\n",
    "        \n",
    "        log_list = os.listdir(out_path)\n",
    "        print(\"files: \",log_list)\n",
    "        avg_acc = []\n",
    "        avg_time = []\n",
    "        for log in log_list:\n",
    "            para_size = 5+9\n",
    "            with open(out_path+log,mode='r') as log:\n",
    "                buffer = log.readlines()\n",
    "                list_ep_train_acc=[]\n",
    "                list_ep_valid_acc=[]\n",
    "                list_ep_valid_time=[]\n",
    "                best_ep = []\n",
    "                best_acc = []\n",
    "                for line in buffer:\n",
    "                    \n",
    "                    match = re.match('Test Epoch\\d+ layer(.)*', line)\n",
    "                    if match!=None:\n",
    "                        match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                        list_ep_valid_acc.append(float(match[0]))\n",
    "                    \n",
    "                    match = re.match(\"Test threshold \\d+\\.\\d+ Acc(.)*\", line)\n",
    "                    if match!=None:\n",
    "                        match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                        list_ep_valid_acc.append(float(match[1]))\n",
    "                        \n",
    "                    match = re.match('Best AUC tensor(.)*', line)\n",
    "                    if match!=None:\n",
    "                        match = re.findall(\"[0-9]+\\.?[0-9]+\",match.group(0))\n",
    "                        best_ep.append(int(match[1]))\n",
    "                        best_acc.append(float(match[0]))\n",
    "                list_ep_valid_acc = np.array(list_ep_valid_acc).reshape((4,-1, 13))\n",
    "                print(list_ep_valid_acc.shape)\n",
    "                for i in range(layer):\n",
    "                    #print(list_ep_valid_acc[i,best_ep[i],:])\n",
    "                    print(best_acc[i],best_ep[i])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fbcdd291",
   "metadata": {},
   "source": [
    "### bp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e05b254",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "datalist = [\"cifar10\", \"cifar100\", \"tinyImageNet\"]\n",
    "model_list = [\"CNN\",\"VGG\",\"resnet\",]\n",
    "[\"CNN_AL\",\"VGG_AL\",\"resnet_AL\",]\n",
    "for data in datalist:\n",
    "    for model in model_list:\n",
    "        layer = 4\n",
    "        epoch = 300\n",
    "        lr = 0.001\n",
    "        aug_type = \"strong\"\n",
    "        #save_path = f\"ckpt/{data}_{model}_l{layer}_{aug_type}/\"\n",
    "        save_path = f\"ckpt/{data}_{model}_{aug_type}/\"\n",
    "        \n",
    "        #out_path = f\"result/0501/adapt rewrite/conf max/{data}_{model}_l{layer}_{aug_type}_max/\"\n",
    "        out_path = f\"result/0515/image_bp/{data}_{model}_{aug_type}/\"\n",
    "\n",
    "        \n",
    "        log_list = os.listdir(out_path)\n",
    "        #print(\"files: \",log_list)\n",
    "        avg_acc = []\n",
    "        avg_time = []\n",
    "        for log in log_list:\n",
    "            para_size = 5+9\n",
    "            with open(out_path+log,mode='r') as log:\n",
    "                buffer = log.readlines()\n",
    "                list_ep_train_acc=[]\n",
    "                list_ep_valid_acc=[]\n",
    "                list_ep_valid_time=[]\n",
    "                for line in buffer:\n",
    "                    match = re.match('Train Epoch\\d+ Acc(.)*', line)\n",
    "                    if match!=None:\n",
    "                        match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                        list_ep_train_acc.append(float(match[0]))\n",
    "\n",
    "                        \n",
    "                    match = re.match('Test Epoch\\d+(.)*', line)\n",
    "                    if match!=None:\n",
    "                        match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                        list_ep_valid_acc.append(float(match[0]))\n",
    "                    \n",
    "                    \n",
    "                    match = re.match('ep\\d+_test_time(.)*', line)\n",
    "                    if match!=None:\n",
    "                        match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                        list_ep_valid_time.append(float(match[0]))\n",
    "                        \n",
    "                    match = re.match('t\\d+\\.\\d+_test_time(.)*', line)\n",
    "                    if match!=None:\n",
    "                        match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                        list_ep_valid_time.append(float(match[1]))    \n",
    "                    match = re.match('Best AUC tensor(.)*', line)\n",
    "                    if match!=None:\n",
    "                        match = re.findall(\"[0-9]+\\.?[0-9]+\",match.group(0))\n",
    "                        best_ep = int(match[1])\n",
    "                        best_acc = float(match[0])\n",
    "\n",
    "                list_ep_train_acc = np.array(list_ep_train_acc)\n",
    "                list_ep_valid_acc = np.array(list_ep_valid_acc)\n",
    "                list_ep_valid_time = np.array(list_ep_valid_time)\n",
    "                \n",
    "                #print(list_ep_valid_acc[best_ep,:])\n",
    "                #print(list_ep_valid_time[best_ep,:])\n",
    "                avg_acc.append(list_ep_valid_acc[best_ep])\n",
    "                avg_time.append(list_ep_valid_time[best_ep]) \n",
    "        avg_acc = np.array(avg_acc)\n",
    "        avg_time = np.array(avg_time)\n",
    "        avg_acc = (np.mean(avg_acc, axis=0))\n",
    "        avg_time = (np.mean(avg_time, axis=0))\n",
    "        print(best_ep)\n",
    "        #print(f\"best_ep: {best_ep}\")\n",
    "        #print(\" \".join([str(v) for v in avg_acc]))\n",
    "        #print(avg_time)\n",
    "        \"\"\"\n",
    "        print((f\"{data} {model}\"))\n",
    "        print((\"shortcut/adaptive inference\"))\n",
    "        print((\"confidence type: Max yi\"))\n",
    "        plt.title(f\"{model} testing ACC vs Time\")\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.plot(avg_time[:4],avg_acc[:4],\"bo-\", label='shortcut inference')\n",
    "        plt.plot(avg_time[4:],avg_acc[4:],\"ro-\", label='adaptive inference')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        print((\"shortcut inference\"))\n",
    "        plt.title(f\"{model} testing ACC vs shortcut depth\")\n",
    "        plt.xlabel(\"shortcut depth\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        x = [f\"L{i}\" for i in range(4)]\n",
    "        plt.plot(x,avg_acc[:4],\"bo-\", label='shortcut inference')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        print((\"adaptive inference\"))\n",
    "        plt.title(f\"{model} testing ACC vs threshold\")\n",
    "        plt.xlabel(\"threshold\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        x = [0.1*(i+1) for i in reversed(range(9))]\n",
    "        plt.plot(x,avg_acc[4:],\"ro-\", label='adaptive inference')\n",
    "        plt.legend()\n",
    "        plt.show()      \n",
    "        \"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "54f634bc",
   "metadata": {},
   "source": [
    "# 0621 Final exp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6d883fa3",
   "metadata": {},
   "source": [
    "#### FLOPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32682b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from thop import profile\n",
    "\n",
    "import argparse\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.functional import mse_loss\n",
    "from torchmetrics.functional import r2_score, auroc, f1_score\n",
    "from utils import *\n",
    "# from model import Model\n",
    "from distributed_model import *\n",
    "from distributed_model_cnn import*\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy\n",
    "import gc\n",
    "import time\n",
    "\n",
    "\n",
    "for class_num in [10,100,200]:\n",
    "    #model = CNN_AL(num_layer=4, l1_dim=300, lr=0.0001, class_num=class_num, lab_dim=1024)\n",
    "    #model = VGG_AL(num_layer=4, l1_dim=300, lr=0.0001, class_num=class_num, lab_dim=1024)\n",
    "    model = resnet18_AL(num_layer=4, l1_dim=300, lr=0.0001, class_num=class_num, lab_dim=1024)\n",
    "    layers_flop = []\n",
    "    input = torch.randn(1024, 3, 32, 32)\n",
    "\n",
    "    for layer in range(model.num_layer):\n",
    "        layer_flop = []\n",
    "        m = model.layers[layer].enc.f\n",
    "        flops, params = profile(m, inputs=(input, )) \n",
    "        layer_flop.append(flops)\n",
    "        #print(f\"L{layer} f {flops/1e6}m {params}\")\n",
    "        \n",
    "        input = m(input)\n",
    "        m = model.layers[layer].enc.b\n",
    "        flops, params = profile(m, inputs=(input, )) \n",
    "        layer_flop.append(flops)\n",
    "        #print(f\"L{layer} b {flops/1e6}m {params}\")\n",
    "        \n",
    "        forward = m(input)\n",
    "        m = model.layers[layer].ae.h\n",
    "        flops, params = profile(m, inputs=(forward, )) \n",
    "        layer_flop.append(flops)\n",
    "        #print(f\"L{layer} ae {flops/1e6}m {params}\")\n",
    "        \n",
    "        layers_flop.append(layer_flop)\n",
    "        \n",
    "    #print(layers_flop)\n",
    "    layers_flop_accumulate = layers_flop\n",
    "\n",
    "    for i in range(1,model.num_layer):\n",
    "        layers_flop_accumulate[i][0] = layers_flop_accumulate[i][0] + layers_flop_accumulate[i-1][0]\n",
    "        layers_flop_accumulate[i][2] = layers_flop_accumulate[i][2] + layers_flop_accumulate[i-1][2]\n",
    "\n",
    "    for count in layers_flop_accumulate:\n",
    "        print(sum(count)/1e9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8a9b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from thop import profile\n",
    "\n",
    "import argparse\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.functional import mse_loss\n",
    "from torchmetrics.functional import r2_score, auroc, f1_score\n",
    "from utils import *\n",
    "# from model import Model\n",
    "from distributed_model import *\n",
    "from distributed_model_cnn import*\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy\n",
    "import gc\n",
    "import time\n",
    "\n",
    "class arg:\n",
    "    def __init__(self) -> None:\n",
    "        self.dataset = \"dbpedia_14\"\n",
    "        self.max_len = 80\n",
    "        self.word_vec = 'glove'\n",
    "        self.num_layer = 5\n",
    "        self.batch_train = 256\n",
    "        self.batch_test = 512\n",
    "        self.lr = 0.0001 \n",
    "        self.l1_dim = 300\n",
    "        self.label_emb = 128 \n",
    "        self.task = 'text'\n",
    "        self.emb_dim = 300\n",
    "args = arg()\n",
    "\n",
    "train_loader, valid_loader, class_num, vocab = get_data(args)\n",
    "word_vec = get_word_vector(vocab, 'glove')\n",
    "\n",
    "\n",
    "\n",
    "#model = LSTMModelML(vocab_size=len(vocab), num_layer=args.num_layer, emb_dim=args.emb_dim, l1_dim=args.l1_dim, lab_dim=args.label_emb, class_num=class_num, word_vec=word_vec, lr=args.lr)\n",
    "#model = LinearModelML(vocab_size=len(vocab), num_layer=args.num_layer, emb_dim=args.emb_dim, l1_dim=args.l1_dim, lab_dim=args.label_emb, class_num=class_num, word_vec=word_vec, lr=args.lr)\n",
    "model = TransformerModelML(vocab_size=len(vocab), num_layer=args.num_layer, emb_dim=args.emb_dim, l1_dim=args.l1_dim, lab_dim=args.label_emb, class_num=class_num, word_vec=word_vec, lr=args.lr)\n",
    "\n",
    "\n",
    "for x, y in valid_loader:\n",
    "    layers_flop = []\n",
    "    \n",
    "    for idx in range(model.num_layer):\n",
    "        layer_flop = []\n",
    "        \n",
    "        if isinstance(model,LSTMModelML):\n",
    "            if idx==0:\n",
    "                x_out = x.long()\n",
    "                m = model.layers[idx].enc.f\n",
    "                flops, params = profile(m, inputs=(x_out, )) \n",
    "                x_out = model.layers[idx].enc.f(x.long())\n",
    "            elif idx==1:\n",
    "                m = model.layers[idx].enc.f\n",
    "                flops, params = profile(m, inputs=(x_out, )) \n",
    "                \n",
    "                x_out, (h, c) = model.layers[idx].enc.f(x_out)\n",
    "                x_out = torch.cat((x_out[:, :, :model.l1_dim], x_out[:, :, model.l1_dim:]), dim=-1)\n",
    "            else:\n",
    "                m = model.layers[idx].enc.f\n",
    "                flops, params = profile(m, inputs=(x_out, (h, c))) \n",
    "                x_out, (h, c) = model.layers[idx].enc.f(x_out, (h, c))\n",
    "                x_out = torch.cat((x_out[:, :, :model.l1_dim], x_out[:, :, model.l1_dim:]), dim=-1)\n",
    "            layer_flop.append(flops)\n",
    "            print(f\"L{idx} f {flops/1e6}m {params}\")\n",
    "            # bridge\n",
    "            if idx == 0:\n",
    "                forward = x_out.mean(1)  \n",
    "            else:\n",
    "                forward = torch.sum(h, dim=0)\n",
    "        \n",
    "        if isinstance(model,TransformerModelML):\n",
    "            mask = model.get_mask(x).cpu()\n",
    "            if idx==0: # embed\n",
    "                x_out = x.long()\n",
    "                m = model.layers[idx].enc.f\n",
    "                flops, params = profile(m, inputs=(x_out, )) \n",
    "                x_out = model.layers[idx].enc.f(x.long())          \n",
    "            else:\n",
    "                m = model.layers[idx].enc.f\n",
    "                flops, params = profile(m, inputs=(x_out, mask)) \n",
    "                x_out = model.layers[idx].enc.f(x_out, mask)\n",
    "            layer_flop.append(flops)\n",
    "            print(f\"L{idx} f {flops/1e6}m {params}\")     \n",
    "            if idx == 0:\n",
    "                forward = x_out.mean(1)  \n",
    "            else:\n",
    "                denom = torch.sum(mask, -1, keepdim=True)\n",
    "                forward = torch.sum(x_out * mask.unsqueeze(-1), dim=1) / denom\n",
    "            \n",
    "            \n",
    "        m = model.layers[idx].enc.b\n",
    "        flops, params = profile(m, inputs=(forward, )) \n",
    "        forward = model.layers[idx].enc.b(forward)\n",
    "        \n",
    "        layer_flop.append(flops)\n",
    "        print(f\"L{idx} b {flops/1e6}m {params}\")\n",
    "        \n",
    "        # return\n",
    "   \n",
    "        m = model.layers[idx].ae.h\n",
    "        flops, params = profile(m, inputs=(forward, )) \n",
    "        layer_flop.append(flops)\n",
    "        print(f\"L{idx} ae {flops/1e6}m {params}\")\n",
    "        \n",
    "        layers_flop.append(layer_flop)\n",
    "        \n",
    "    print(layers_flop)\n",
    "    layers_flop_accumulate = layers_flop\n",
    "\n",
    "    for i in range(1,model.num_layer):\n",
    "        layers_flop_accumulate[i][0] = layers_flop_accumulate[i][0] + layers_flop_accumulate[i-1][0]\n",
    "        layers_flop_accumulate[i][2] = layers_flop_accumulate[i][2] + layers_flop_accumulate[i-1][2]\n",
    "\n",
    "    for count in layers_flop_accumulate:\n",
    "        print(sum(count)/1e9)\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f7eeb2bb",
   "metadata": {},
   "source": [
    "### adaptive/shortcut baseline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f01ce02",
   "metadata": {},
   "source": [
    "#### image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ee3663",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os \n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from adjustText import adjust_text  \n",
    "total_result = pd.DataFrame(columns=[])\n",
    "fig, axs = plt.subplots(3, 2)\n",
    "\n",
    "fig.set_size_inches(10, 10)\n",
    "#fig2, axs2 = plt.subplots(3, 2)\n",
    "#fig2.set_size_inches(8, 10)\n",
    "r = -1\n",
    "a=96\n",
    "inference_type = \"\"\n",
    "for data in [\"cifar10\", \"cifar100\", \"tinyImageNet\"]:\n",
    "    r+=1\n",
    "    c = -1\n",
    "    for model in [\"VGG_AL\",\"resnet_AL\",]:\n",
    "        c+=1\n",
    "        aug_type = \"strong\"\n",
    "        layer = 4\n",
    "        epoch = 200\n",
    "        lr = 0.0001\n",
    "        \n",
    "        save_path = f\"ckpt/{data}_{model}_l{layer}_{aug_type}/\"\n",
    "        out_path = f\"result/0518/maxP/{data}_{model}_l{layer}_{aug_type}/\"\n",
    "        #out_path = f\"result/0621/image/baseline/{data}_{model}_l{layer}_{aug_type}_1024/\"\n",
    "        #out_path = f\"result//{data}_{model}_l{layer}_{aug_type}_1024/\"\n",
    "        \n",
    "        \n",
    "        log_list = os.listdir(out_path)\n",
    "        #print(\"files: \",log_list)\n",
    "        avg_acc = []\n",
    "        avg_time = []\n",
    "        avg_distr = []\n",
    "        avg_best_acc = []\n",
    "        avg_best_ep = []\n",
    "        for log in log_list:\n",
    "            para_layer = 4\n",
    "            para_thres = 9\n",
    "            para_size = para_layer+para_thres\n",
    "            with open(out_path+log,mode='r') as log:\n",
    "                buffer = log.readlines()\n",
    "\n",
    "                list_ep_valid_acc=[]\n",
    "                list_ep_valid_time=[]\n",
    "                list_ep_valid_distr=[]\n",
    "                for line in buffer:\n",
    "                    match = re.match('Test Epoch\\d+ layer\\d+ Acc (.)*', line)\n",
    "                    if match!=None:\n",
    "                        match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                        list_ep_valid_acc.append(float(match[0]))\n",
    "                    match = re.match('ep\\d+_l\\d+_test_time(.)*', line)\n",
    "                    if match!=None:\n",
    "                        match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                        list_ep_valid_time.append(float(match[0]))      \n",
    "                          \n",
    "                    match = re.match(\"Test threshold \\d+\\.\\d+ Acc(.)*\", line)\n",
    "                    if match!=None:\n",
    "                        match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                        list_ep_valid_acc.append(float(match[1]))\n",
    "                    match = re.match('t\\d+\\.\\d+_test_time(.)*', line)\n",
    "                    if match!=None:\n",
    "                        match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                        list_ep_valid_time.append(float(match[1]))   \n",
    "                    match = re.match('Best Acc(.)*', line)\n",
    "                    if match!=None:\n",
    "                        match = re.findall(\"[0-9]+\\.?[0-9]+\",match.group(0))\n",
    "                        best_ep = int(match[1])\n",
    "                        best_acc = float(match[0])\n",
    "                    match = re.match('data_distribution(.)*', line)\n",
    "                    if match!=None:\n",
    "                        match = re.findall(\"\\d+\",match.group(0))  \n",
    "                        list_ep_valid_distr.append(match)\n",
    "                       \n",
    "                list_ep_valid_acc = np.array(list_ep_valid_acc).reshape((200,13))\n",
    "                list_ep_valid_time = np.array(list_ep_valid_time).reshape((200,13))\n",
    "                list_ep_valid_distr = np.array(list_ep_valid_distr,dtype=int).reshape((200,9,4))\n",
    "                \n",
    "                avg_distr.append(list_ep_valid_distr[best_ep])\n",
    "                avg_acc.append(list_ep_valid_acc[best_ep])\n",
    "                avg_time.append(list_ep_valid_time[best_ep])\n",
    "                avg_best_acc.append(best_acc)\n",
    "                avg_best_ep.append(best_ep)\n",
    "\n",
    "        #print(np.array(avg_time)[:,para_layer:])\n",
    "        \n",
    "        std_acc = np.std(avg_acc, axis=0)\n",
    "        std_time = np.std(avg_time, axis=0)        \n",
    "        avg_acc = (np.mean(avg_acc, axis=0))\n",
    "        avg_time = (np.mean(avg_time, axis=0))\n",
    "        avg_distr = np.mean(avg_distr, axis=0)\n",
    "        avg_best_acc = np.mean(avg_best_acc, axis=0)\n",
    "        avg_best_ep = np.mean(avg_best_ep, axis=0)\n",
    "        \n",
    "        \n",
    "        result = pd.DataFrame(columns=[\"dataset\",\"model\",\"best_ep\",\"best_acc\",\"val_type\"]\n",
    "                                    +[f'L{i}_acc' for i in range(para_layer)]\n",
    "                                    +[f't0.{i}_acc' for i in range(1,10)]\n",
    "                                    +[f'L{i}_time' for i in range(para_layer)]\n",
    "                                    +[f't0.{i}_time' for i in range(1,10)])\n",
    "        result.loc[0,[f'L{i}_acc' for i in range(para_layer)]+[f't0.{i}_acc' for i in range(1,10)]] = avg_acc\n",
    "        result.loc[0,[f'L{i}_time' for i in range(para_layer)]+[f't0.{i}_time' for i in range(1,10)]] = avg_time\n",
    "        result.loc[0,[\"dataset\",\"model\",\"best_ep\",\"best_acc\",\"val_type\"]] = [data,model,avg_best_ep, avg_best_acc, \"avg\"]\n",
    "        result.loc[1,[f'L{i}_acc' for i in range(para_layer)]+[f't0.{i}_acc' for i in range(1,10)]] = std_acc\n",
    "        result.loc[1,[f'L{i}_time' for i in range(para_layer)]+[f't0.{i}_time' for i in range(1,10)]] = std_time\n",
    "        result.loc[1,[\"dataset\",\"model\",\"best_ep\",\"best_acc\",\"val_type\"]] = [data,model,avg_best_ep, avg_best_acc, \"std\"]\n",
    "        #print(result)\n",
    "        if inference_type == \"layer\":\n",
    "            x = avg_time[:para_layer]\n",
    "            y = avg_acc[:para_layer]\n",
    "            yerr = std_acc[:para_layer]\n",
    "            xerr = std_time[:para_layer]\n",
    "            axs[r,c].errorbar(x, y, xerr=xerr, yerr=yerr, marker='o', mfc='royalblue', ecolor='#FF7000', linestyle='--',ms=4,)\n",
    "            texts = [axs[r,c].text(x[i], y[i], f\"L{i+1}\", fontdict=None) for i in range(4)]\n",
    "\n",
    "        else:    \n",
    "            x = avg_time[para_layer:][[0,2,4,6,8]]\n",
    "            y = avg_acc[para_layer:][[0,2,4,6,8]]\n",
    "            yerr = std_acc[para_layer:][[0,2,4,6,8]]\n",
    "            xerr = std_time[para_layer:][[0,2,4,6,8]]\n",
    "            axs[r,c].errorbar(x=x, y=y, xerr=xerr, yerr=yerr, marker='o', mfc='royalblue', ecolor='#FF7000', linestyle='--',ms=4,)\n",
    "            #texts = [axs[r,c].text(x[i], y[i], f\"t0.{i+1}\", fontdict=None) for i in range(9)]\n",
    "            texts = [axs[r,c].text(x[i], y[i], f\"t0.{i*2+1}\", fontdict=None) for i in range(5)]\n",
    "            \n",
    "\n",
    "        #print(avg_time[para_layer:])\n",
    "        #print(std_time[para_layer:])\n",
    "\n",
    "        adjust_text(texts, ax=axs[r,c])\n",
    "        a+=1\n",
    "        axs[r,c].set_title(f\"({chr(a)}) \"+ model+\" on \"+data)\n",
    "        axs[r,c].set(xlabel='inference time (sec)', ylabel='Accuracy')\n",
    "        #plt.fill_between(x, y-yerr, y+yerr)\n",
    "        \n",
    "        \n",
    "        #axs2[r,c].set_title(f\"({chr(a)}) \"+ model+\" on \"+data)\n",
    "        #axs2[r,c].bar(x=[f\"L{i+1}\" for i in range(4)], height=avg_distr[-1,:])\n",
    "        \n",
    "        total_result = pd.concat([total_result,result],axis=0,ignore_index=True)   \n",
    "fig.tight_layout()\n",
    "\n",
    "#fig2.tight_layout()\n",
    "#fig2.savefig(\"result/_csv/image_data_distr.pdf\", format=\"pdf\")\n",
    "if inference_type == \"layer\":\n",
    "    fig.savefig(\"result/_csv/image_layer.pdf\", format=\"pdf\")\n",
    "else:\n",
    "    fig.savefig(\"result/_csv/image_adapt.pdf\", format=\"pdf\")\n",
    "#fig.show()\n",
    "total_result\n",
    "total_result.to_csv(\"result/_csv/image_baseline.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bb00c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cols = total_result.iloc[:,5:5+para_size].columns\n",
    "pd_acc_avg = total_result[total_result[\"val_type\"]==\"avg\"].iloc[:,5:5+para_size].copy()\n",
    "pd_acc_std = total_result[total_result[\"val_type\"]==\"std\"].iloc[:,5:5+para_size].copy()\n",
    "\n",
    "for c in cols:\n",
    "    pd_acc_avg[cols] = total_result[total_result[\"val_type\"]==\"avg\"][cols].applymap(lambda x: '${0:.2f}\\\\pm'.format(x*100))\n",
    "    pd_acc_std[cols] = total_result[total_result[\"val_type\"]==\"std\"][cols].applymap(lambda x: '{0:.2f}$'.format(x*100))\n",
    "n,m = pd_acc_avg.shape\n",
    "for i in range(n):\n",
    "    for j in range(m):\n",
    "        pd_acc_avg.iloc[i,j] = pd_acc_avg.iloc[i,j]+pd_acc_std.iloc[i,j]\n",
    "pd_acc_avg.to_csv(\"result/_csv/image_acc.csv\", index=False)\n",
    "pd_acc_avg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b40422c3",
   "metadata": {},
   "source": [
    "#### text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fa4eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from adjustText import adjust_text  \n",
    "\n",
    "total_result = pd.DataFrame(columns=[])\n",
    "fig, axs = plt.subplots(3, 2)\n",
    "fig.set_size_inches(10, 10)\n",
    "fig2, axs2 = plt.subplots(3, 2)\n",
    "fig2.set_size_inches(8, 10)\n",
    "r = -1\n",
    "a = 96\n",
    "inference_type = \"\"\n",
    "\n",
    "\n",
    "for data,text_len in [\n",
    "    (\"imdb\",500),\n",
    "    (\"ag_news\",177),\n",
    "    (\"dbpedia_14\",80),\n",
    "]:\n",
    "    \n",
    "    r+=1\n",
    "    c=-1\n",
    "    for model in [\"lstmal\",\"transformeral\"]:\n",
    "        c+=1\n",
    "        layer = 5\n",
    "        epoch = 20\n",
    "        lr = 0.0001\n",
    "\n",
    "        out_path = f\"result/0621/text/baseline/{data}_{model}_l{layer}_pad{text_len}/\"\n",
    "        \n",
    "        log_list = os.listdir(out_path)\n",
    "        #print(\"files: \",log_list)\n",
    "        avg_acc = []\n",
    "        avg_time = []\n",
    "        avg_best_acc = []\n",
    "        avg_best_ep = []\n",
    "        avg_distr = []\n",
    "        for log in log_list:\n",
    "            para_layer = 5\n",
    "            para_thres = 9\n",
    "            para_size = para_layer+para_thres\n",
    "            with open(out_path+log,mode='r') as log:\n",
    "                buffer = log.readlines()\n",
    "\n",
    "                list_ep_valid_acc=[]\n",
    "                list_ep_valid_time=[]\n",
    "                list_ep_valid_distr=[]\n",
    "                for line in buffer:\n",
    "                    match = re.match('Test Epoch\\d+ layer\\d+ Acc (.)*', line)\n",
    "                    if match!=None:\n",
    "                        match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                        list_ep_valid_acc.append(float(match[0]))\n",
    "                    match = re.match('ep\\d+_l\\d+_test_time(.)*', line)\n",
    "                    if match!=None:\n",
    "                        match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                        list_ep_valid_time.append(float(match[0]))      \n",
    "                          \n",
    "                    match = re.match(\"Test threshold \\d+\\.\\d+ Acc(.)*\", line)\n",
    "                    if match!=None:\n",
    "                        match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                        list_ep_valid_acc.append(float(match[1]))\n",
    "                    match = re.match('t\\d+\\.\\d+_test_time(.)*', line)\n",
    "                    if match!=None:\n",
    "                        match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                        list_ep_valid_time.append(float(match[1]))   \n",
    "                    match = re.match('Best Acc(.)*', line)\n",
    "                    if match!=None:\n",
    "                        match = re.findall(\"[0-9]?\\.?[0-9]+\",match.group(0))\n",
    "                        best_ep = int(match[1])\n",
    "                        best_acc = float(match[0])\n",
    "                    match = re.match('data_distribution(.)*', line)\n",
    "                    if match!=None:\n",
    "                        match = re.findall(\"\\d+\",match.group(0))  \n",
    "                        list_ep_valid_distr.append(match)   \n",
    "                list_ep_valid_acc = np.array(list_ep_valid_acc).reshape((-1,para_size))\n",
    "                list_ep_valid_time = np.array(list_ep_valid_time).reshape((-1,para_size))\n",
    "                list_ep_valid_distr = np.array(list_ep_valid_distr,dtype=int).reshape((-1,para_thres,layer))\n",
    "                \n",
    "                avg_distr.append(list_ep_valid_distr[best_ep])\n",
    "                avg_acc.append(list_ep_valid_acc[best_ep])\n",
    "                avg_time.append(list_ep_valid_time[best_ep])\n",
    "                avg_best_acc.append(best_acc)\n",
    "                avg_best_ep.append(best_ep)\n",
    "        \n",
    "        std_acc = np.std(avg_acc, axis=0)\n",
    "        std_time = np.std(avg_time, axis=0) \n",
    "        avg_distr = np.mean(avg_distr, axis=0)   \n",
    "        avg_acc = (np.mean(avg_acc, axis=0))\n",
    "        avg_time = (np.mean(avg_time, axis=0))\n",
    "        avg_best_acc = np.mean(avg_best_acc, axis=0)\n",
    "        avg_best_ep = np.mean(avg_best_ep, axis=0)\n",
    "        \n",
    "        result = pd.DataFrame(columns=[\"dataset\",\"model\",\"best_ep\",\"best_acc\",\"val_type\"]\n",
    "                                    +[f'L{i}_acc' for i in range(para_layer)]\n",
    "                                    +[f't0.{i}_acc' for i in range(1,10)]\n",
    "                                    +[f'L{i}_time' for i in range(para_layer)]\n",
    "                                    +[f't0.{i}_time' for i in range(1,10)])\n",
    "        result.loc[0,[f'L{i}_acc' for i in range(para_layer)]+[f't0.{i}_acc' for i in range(1,10)]] = avg_acc\n",
    "        result.loc[0,[f'L{i}_time' for i in range(para_layer)]+[f't0.{i}_time' for i in range(1,10)]] = avg_time\n",
    "        result.loc[0,[\"dataset\",\"model\",\"best_ep\",\"best_acc\",\"val_type\"]] = [data,model,avg_best_ep, avg_best_acc, \"avg\"]\n",
    "        result.loc[1,[f'L{i}_acc' for i in range(para_layer)]+[f't0.{i}_acc' for i in range(1,10)]] = std_acc\n",
    "        result.loc[1,[f'L{i}_time' for i in range(para_layer)]+[f't0.{i}_time' for i in range(1,10)]] = std_time\n",
    "        result.loc[1,[\"dataset\",\"model\",\"best_ep\",\"best_acc\",\"val_type\"]] = [data,model,avg_best_ep, avg_best_acc, \"std\"]\n",
    "        #print(result)\n",
    "        \n",
    "        if inference_type == \"layer\":\n",
    "            x = avg_time[:para_layer][:4]\n",
    "            y = avg_acc[:para_layer][:4]\n",
    "            yerr = std_acc[:para_layer][:4]\n",
    "            xerr = std_time[:para_layer][:4]\n",
    "            axs[r,c].errorbar(x, y, xerr=xerr, yerr=yerr, marker='o', mfc='royalblue', ecolor='#FF7000', linestyle='--',ms=4,)\n",
    "\n",
    "            texts = [axs[r,c].text(x[i], y[i], f\"L{i+1}\", fontdict=None) for i in range(4)]\n",
    "        else:\n",
    "            x = avg_time[para_layer:][[0,2,4,6,8]]\n",
    "            y = avg_acc[para_layer:][[0,2,4,6,8]]\n",
    "            yerr = std_acc[para_layer:][[0,2,4,6,8]]\n",
    "            xerr = std_time[para_layer:][[0,2,4,6,8]]\n",
    "        \n",
    "            axs[r,c].errorbar(x, y, xerr=xerr, yerr=yerr, marker='o', mfc='royalblue', ecolor='#FF7000', linestyle='--',ms=4,)\n",
    "\n",
    "            texts = [axs[r,c].text(x[i], y[i], f\"t0.{i*2+1}\", fontdict=None) for i in range(5)]\n",
    "        \n",
    "        a+=1\n",
    "        axs[r,c].set_title(f\"({chr(a)}) \"+ model+\" on \"+data)\n",
    "        axs[r,c].set(xlabel='inference time (sec)', ylabel='Accuracy')\n",
    "\n",
    "        adjust_text(texts, ax=axs[r,c])\n",
    "        total_result = pd.concat([total_result,result],axis=0,ignore_index=True)  \n",
    "        \n",
    "        avg_distr[-1:3] = avg_distr[-1:3]+avg_distr[-1:4]\n",
    "        axs2[r,c].set_title(f\"({chr(a)}) \"+ model+\" on \"+data)\n",
    "        axs2[r,c].bar(x=[f\"L{i+1}\" for i in range(4)], height=avg_distr[-1,:4])\n",
    "\n",
    "\n",
    "fig2.tight_layout()\n",
    "fig2.savefig(\"result/_csv/text_data_distr.pdf\", format=\"pdf\") \n",
    "\n",
    "plt.tight_layout()\n",
    "if inference_type == \"layer\":\n",
    "    plt.savefig(\"result/_csv/text_layer.pdf\", format=\"pdf\")\n",
    "else:\n",
    "    plt.savefig(\"result/_csv/text_adapt.pdf\", format=\"pdf\")\n",
    "plt.show()\n",
    "total_result\n",
    "total_result.to_csv(\"result/_csv/text_baseline.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bb1ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = total_result.iloc[:,5:19].columns\n",
    "pd_acc_avg = total_result[total_result[\"val_type\"]==\"avg\"].iloc[:,5:19].copy()\n",
    "pd_acc_std = total_result[total_result[\"val_type\"]==\"std\"].iloc[:,5:19].copy()\n",
    "\n",
    "for c in cols:\n",
    "    pd_acc_avg[cols] = total_result[total_result[\"val_type\"]==\"avg\"][cols].applymap(lambda x: '${0:.2f}\\\\pm'.format(x*100))\n",
    "    pd_acc_std[cols] = total_result[total_result[\"val_type\"]==\"std\"][cols].applymap(lambda x: '{0:.2f}$'.format(x*100))\n",
    "n,m = pd_acc_avg.shape\n",
    "for i in range(n):\n",
    "    for j in range(m):\n",
    "        pd_acc_avg.iloc[i,j] = pd_acc_avg.iloc[i,j]+pd_acc_std.iloc[i,j]\n",
    "pd_acc_avg.to_csv(\"result/_csv/text_acc.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d896e7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2, axs2 = plt.subplots(3, 2)\n",
    "fig2.set_size_inches(8, 10)\n",
    "\n",
    "r=-1\n",
    "c=-1\n",
    "a = 96\n",
    "\n",
    "a+=1\n",
    "r+=1\n",
    "c+=1\n",
    "model = \"lstmAL\"\n",
    "data = \"IMDB\"\n",
    "avg_distr = [6990, 2669, 0, 341]\n",
    "axs2[r,c].set_title(f\"({chr(a)}) \"+ model+\" on \"+data)\n",
    "axs2[r,c].bar(x=[f\"L{i+1}\" for i in range(4)], height=avg_distr)\n",
    "\n",
    "model = \"TransformerAL\"\n",
    "c+=1\n",
    "avg_distr = [7543, 921, 145, 1391]\n",
    "axs2[r,c].set_title(f\"({chr(a)}) \"+ model+\" on \"+data)\n",
    "axs2[r,c].bar(x=[f\"L{i+1}\" for i in range(4)], height=avg_distr)\n",
    "\n",
    "c=-1\n",
    "a+=1\n",
    "r+=1\n",
    "c+=1\n",
    "model = \"lstmAL\"\n",
    "data = \"AGNews\"\n",
    "avg_distr = [6329, 970, 87, 214]\n",
    "axs2[r,c].set_title(f\"({chr(a)}) \"+ model+\" on \"+data)\n",
    "axs2[r,c].bar(x=[f\"L{i+1}\" for i in range(4)], height=avg_distr)\n",
    "\n",
    "model = \"TransformerAL\"\n",
    "c+=1\n",
    "avg_distr = [6367, 1014, 42, 177]\n",
    "axs2[r,c].set_title(f\"({chr(a)}) \"+ model+\" on \"+data)\n",
    "axs2[r,c].bar(x=[f\"L{i+1}\" for i in range(4)], height=avg_distr)\n",
    "\n",
    "c=-1\n",
    "a+=1\n",
    "r+=1\n",
    "c+=1\n",
    "model = \"lstmAL\"\n",
    "data = \"DBpedia\"\n",
    "avg_distr = [0,0,0,0]\n",
    "axs2[r,c].set_title(f\"({chr(a)}) \"+ model+\" on \"+data)\n",
    "axs2[r,c].bar(x=[f\"L{i+1}\" for i in range(4)], height=avg_distr)\n",
    "\n",
    "model = \"TransformerAL\"\n",
    "c+=1\n",
    "avg_distr = [0,0,0,0]\n",
    "axs2[r,c].set_title(f\"({chr(a)}) \"+ model+\" on \"+data)\n",
    "axs2[r,c].bar(x=[f\"L{i+1}\" for i in range(4)], height=avg_distr)\n",
    "\n",
    "fig2.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2ba09daa",
   "metadata": {},
   "source": [
    "### text lbl: load and inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8192bfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from adjustText import adjust_text  \n",
    "import pandas as pd\n",
    "import gc\n",
    "para_layer = 5\n",
    "para_thres = 9\n",
    "columns = [\"dataset\",\"model\",\"val_type\"]+[f'L{i}_acc' for i in range(para_layer)]+[f't0.{i}_acc' for i in range(1,10)]+[f'L{i}_time' for i in range(para_layer)]+[f't0.{i}_time' for i in range(1,10)]\n",
    "total_result = pd.DataFrame(columns=columns)\n",
    "\n",
    "for data,text_len in [\n",
    "    (\"imdb\",500),\n",
    "    (\"ag_news\",177),\n",
    "    (\"dbpedia_14\",80),\n",
    "]:\n",
    "    for model in [\"transformeral\",\"lstmal\"]:    \n",
    "        gc.collect()\n",
    "        layer = 5\n",
    "        epoch = 50\n",
    "        lr = 0.0001\n",
    "        save_path = f\"ckpt/{data}_{model}_l{layer}_pad{text_len}_fix/\"\n",
    "        out_path = f\"result/{data}_{model}_l{layer}_pad{text_len}_fix/\"\n",
    "        log = f\"{out_path}/{data}_{model}_l{layer}_inference.log\"\n",
    "        !mkdir {save_path}\n",
    "        !mkdir {out_path}\n",
    "        !python3 dis_test_rnn_lbl.py --dataset {data} --model {model} --epoch {epoch} \\\n",
    "        --max-len {text_len} \\\n",
    "        --num-layer {layer}\\\n",
    "        --batch-train 256 --batch-test 512 \\\n",
    "        --lr {lr} --l1-dim 300 --label-emb 128 \\\n",
    "        --lr-schedule plateau \\\n",
    "        --save-dir {save_path} \\\n",
    "        --out-dir {out_path} \\\n",
    "        --task text > {log}\n",
    "        \n",
    "        with open(f'{out_path}/test.npy', 'rb') as f:\n",
    "            y =     np.load(f)\n",
    "            yerr =  np.load(f)\n",
    "            x =     np.load(f)\n",
    "            xerr =  np.load(f)\n",
    "            plt.title(f\"{model} in {data}\")\n",
    "            plt.errorbar(x[-1,:para_layer], y[-1,:para_layer], xerr=xerr[-1,:para_layer], yerr=yerr[-1,:para_layer], marker='o', mfc='royalblue', ecolor='#FF7000', linestyle='--',ms=4,)\n",
    "            plt.show()\n",
    "            plt.title(f\"{model} in {data}\")\n",
    "            plt.errorbar(x[-1,para_layer:], y[-1,para_layer:], xerr=xerr[-1,para_layer:], yerr=yerr[-1,para_layer:], marker='o', mfc='royalblue', ecolor='#FF7000', linestyle='--',ms=4,)\n",
    "            plt.show()\n",
    "            \n",
    "            for layer,acc in enumerate(y):\n",
    "                y[layer,layer+1:para_layer] = 0\n",
    "                yerr[layer,layer+1:para_layer] = 0\n",
    "                \n",
    "            \n",
    "            result = pd.DataFrame(columns=columns)\n",
    "            avg = pd.concat([pd.DataFrame([[data,model, \"avg\"]]*para_layer),pd.DataFrame(y*100),pd.DataFrame(x)],axis=1)\n",
    "            avg.columns = columns\n",
    "            std = pd.concat([pd.DataFrame([[data,model, \"std\"]]*para_layer),pd.DataFrame(yerr*100),pd.DataFrame(xerr)],axis=1)\n",
    "            std.columns = columns\n",
    "            \n",
    "            total_result = pd.concat([total_result,avg,std])\n",
    "            total_result.to_csv(\"result/_csv/text_lbl.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cf5bb95e",
   "metadata": {},
   "source": [
    "### image lbl: load and inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be1f707",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from adjustText import adjust_text  \n",
    "import pandas as pd\n",
    "import gc\n",
    "para_layer = 4\n",
    "para_thres = 9\n",
    "columns = [\"dataset\",\"model\",\"val_type\"]+[f'L{i}_acc' for i in range(para_layer)]+[f't0.{i}_acc' for i in range(1,10)]+[f'L{i}_time' for i in range(para_layer)]+[f't0.{i}_time' for i in range(1,10)]\n",
    "total_result = pd.DataFrame(columns=columns)\n",
    "\n",
    "for data in [\"cifar10\",\"cifar100\",\"tinyImageNet\",]:\n",
    "    for model in [\"VGG_AL\",\"resnet_AL\",]:\n",
    "        layer = 4\n",
    "        epoch = 480\n",
    "        lr = 0.0001\n",
    "        aug_type = \"strong\"\n",
    "        save_path = f\"ckpt/{data}_{model}_l{layer}_{aug_type}_lbl_pre/\"\n",
    "        out_path = f\"result/{data}_{model}_l{layer}_{aug_type}_lbl_pre/\"\n",
    "        log = f\"{out_path}/{data}_{model}_l{layer}_inference.log\"\n",
    "        !mkdir {save_path}\n",
    "        !mkdir {out_path}\n",
    "        !python3 dis_test_cnn_lbl.py --dataset {data} --model {model} --epoch {epoch} \\\n",
    "        --num-layer {layer}\\\n",
    "        --batch-train 128 --batch-test 1024\\\n",
    "        --lr {lr} --l1-dim 300 --label-emb 500 \\\n",
    "        --lr-schedule plateau \\\n",
    "        --save-dir {save_path} \\\n",
    "        --out-dir {out_path} \\\n",
    "        --aug-type {aug_type} \\\n",
    "        --task image > {log}\n",
    "        \n",
    "        with open(f'{out_path}/test.npy', 'rb') as f:\n",
    "            y =     np.load(f)\n",
    "            yerr =  np.load(f)\n",
    "            x =     np.load(f)\n",
    "            xerr =  np.load(f)\n",
    "            plt.title(f\"{model} in {data}\")\n",
    "            plt.errorbar(x[-1,:para_layer], y[-1,:para_layer], xerr=xerr[-1,:para_layer], yerr=yerr[-1,:para_layer], marker='o', mfc='royalblue', ecolor='#FF7000', linestyle='--',ms=4,)\n",
    "            plt.show()\n",
    "            plt.title(f\"{model} in {data}\")\n",
    "            plt.errorbar(x[-1,para_layer:], y[-1,para_layer:], xerr=xerr[-1,para_layer:], yerr=yerr[-1,para_layer:], marker='o', mfc='royalblue', ecolor='#FF7000', linestyle='--',ms=4,)\n",
    "            plt.show()\n",
    "            \n",
    "            for layer,acc in enumerate(y):\n",
    "                y[layer,layer+1:para_layer] = 0\n",
    "                yerr[layer,layer+1:para_layer] = 0\n",
    "                \n",
    "            \n",
    "            result = pd.DataFrame(columns=columns)\n",
    "            avg = pd.concat([pd.DataFrame([[data,model, \"avg\"]]*para_layer),pd.DataFrame(y*100),pd.DataFrame(x)],axis=1)\n",
    "            avg.columns = columns\n",
    "            std = pd.concat([pd.DataFrame([[data,model, \"std\"]]*para_layer),pd.DataFrame(yerr*100),pd.DataFrame(xerr)],axis=1)\n",
    "            std.columns = columns\n",
    "            \n",
    "            total_result = pd.concat([total_result,avg,std])\n",
    "            total_result.to_csv(\"result/_csv/image_lbl.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "283e6df2",
   "metadata": {},
   "source": [
    "### sideinput text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e18d2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from adjustText import adjust_text  \n",
    "import pandas as pd\n",
    "import gc\n",
    "para_layer = 4\n",
    "para_thres = 9\n",
    "columns = [\"dataset\",\"model\",\"val_type\"]+[f'L{i}_acc' for i in range(para_layer)]+[f't0.{i}_acc' for i in range(1,10)]+[f'L{i}_time' for i in range(para_layer)]+[f't0.{i}_time' for i in range(1,10)]\n",
    "total_result = pd.DataFrame(columns=columns)\n",
    "\n",
    "for data, text_len, side in [\n",
    "    (\"dbpedia_14\", 80, \"20-20-20-20\"),\n",
    "    (\"imdb\", 200, \"50-50-50-50\"),\n",
    "    (\"ag_news\", 40, \"10-10-10-10\"),  \n",
    "]:\n",
    "\n",
    "    for model in [\"transformeralside\",\"lstmalside\",]:\n",
    "        layer = 4\n",
    "        epoch = 400\n",
    "        lr = 0.0005\n",
    "        save_path = f\"ckpt/{data}_{model}_l{layer}_pad{text_len}_fix/\"\n",
    "        out_path = f\"result/{data}_{model}_l{layer}_pad{text_len}_fix/\"\n",
    "        log = f\"{out_path}/{data}_{model}_l{layer}_inference.log\"\n",
    "        !mkdir {save_path}\n",
    "        !mkdir {out_path}\n",
    "        !python3 dis_test_rnn_lbl.py --dataset {data} --model {model} --epoch {epoch} \\\n",
    "        --max-len {text_len} \\\n",
    "        --num-layer {layer}\\\n",
    "        --batch-train 256 --batch-test 512 \\\n",
    "        --lr {lr} --l1-dim 300 --label-emb 128 \\\n",
    "        --lr-schedule plateau \\\n",
    "        --save-dir {save_path} \\\n",
    "        --out-dir {out_path} \\\n",
    "        --side-dim {side}\\\n",
    "        --task text > {log}\n",
    "        \n",
    "        with open(f'{out_path}/test.npy', 'rb') as f:\n",
    "            y =     np.load(f)\n",
    "            yerr =  np.load(f)\n",
    "            x =     np.load(f)\n",
    "            xerr =  np.load(f)\n",
    "            plt.title(f\"{model} in {data}\")\n",
    "            plt.errorbar(x[-1,:para_layer], y[-1,:para_layer], xerr=xerr[-1,:para_layer], yerr=yerr[-1,:para_layer], marker='o', mfc='royalblue', ecolor='#FF7000', linestyle='--',ms=4,)\n",
    "            plt.show()\n",
    "            plt.title(f\"{model} in {data}\")\n",
    "            plt.errorbar(x[-1,para_layer:], y[-1,para_layer:], xerr=xerr[-1,para_layer:], yerr=yerr[-1,para_layer:], marker='o', mfc='royalblue', ecolor='#FF7000', linestyle='--',ms=4,)\n",
    "            plt.show()\n",
    "            \n",
    "            for layer,acc in enumerate(y):\n",
    "                y[layer,layer+1:para_layer] = 0\n",
    "                yerr[layer,layer+1:para_layer] = 0\n",
    "                \n",
    "            \n",
    "            result = pd.DataFrame(columns=columns)\n",
    "            avg = pd.concat([pd.DataFrame([[data,model, \"avg\"]]*para_layer),pd.DataFrame(y*100),pd.DataFrame(x)],axis=1)\n",
    "            avg.columns = columns\n",
    "            std = pd.concat([pd.DataFrame([[data,model, \"std\"]]*para_layer),pd.DataFrame(yerr*100),pd.DataFrame(xerr)],axis=1)\n",
    "            std.columns = columns\n",
    "            \n",
    "            total_result = pd.concat([total_result,avg,std])\n",
    "            total_result.to_csv(\"result/_csv/text_side_lbl.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb996d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from adjustText import adjust_text  \n",
    "import pandas as pd\n",
    "import gc\n",
    "para_layer = 4\n",
    "para_thres = 9\n",
    "columns = [\"dataset\",\"model\",\"val_type\"]+[f'L{i}_acc' for i in range(para_layer)]+[f't0.{i}_acc' for i in range(1,10)]+[f'L{i}_time' for i in range(para_layer)]+[f't0.{i}_time' for i in range(1,10)]\n",
    "total_result = pd.DataFrame(columns=columns)\n",
    "\n",
    "for data, text_len, side in [\n",
    "    (\"dbpedia_14\", 80, \"20-20-20-20\"),\n",
    "    (\"imdb\", 200, \"50-50-50-50\"),\n",
    "    (\"ag_news\", 40, \"10-10-10-10\"),  \n",
    "]:\n",
    "\n",
    "    for model in [\"transformeralside\",\"lstmalside\",]:\n",
    "        layer = 4\n",
    "        epoch = 400\n",
    "        lr = 0.0005\n",
    "        save_path = f\"ckpt/{data}_{model}_l{layer}_pad{text_len}_fix/\"\n",
    "        out_path = f\"result/{data}_{model}_l{layer}_pad{text_len}_fix/\"\n",
    "        log = f\"{out_path}/{data}_{model}_l{layer}_inference.log\"\n",
    "        !mkdir {save_path}\n",
    "        !mkdir {out_path}\n",
    "        !python3 dis_test_rnn_lbl.py --dataset {data} --model {model} --epoch {epoch} \\\n",
    "        --max-len {text_len} \\\n",
    "        --num-layer {layer}\\\n",
    "        --batch-train 256 --batch-test 512 \\\n",
    "        --lr {lr} --l1-dim 300 --label-emb 128 \\\n",
    "        --lr-schedule plateau \\\n",
    "        --save-dir {save_path} \\\n",
    "        --out-dir {out_path} \\\n",
    "        --side-dim {side}\\\n",
    "        --task text > {log}\n",
    "        \n",
    "        with open(f'{out_path}/test.npy', 'rb') as f:\n",
    "            y =     np.load(f)\n",
    "            yerr =  np.load(f)\n",
    "            x =     np.load(f)\n",
    "            xerr =  np.load(f)\n",
    "            plt.title(f\"{model} in {data}\")\n",
    "            plt.errorbar(x[-1,:para_layer], y[-1,:para_layer], xerr=xerr[-1,:para_layer], yerr=yerr[-1,:para_layer], marker='o', mfc='royalblue', ecolor='#FF7000', linestyle='--',ms=4,)\n",
    "            plt.show()\n",
    "            plt.title(f\"{model} in {data}\")\n",
    "            plt.errorbar(x[-1,para_layer:], y[-1,para_layer:], xerr=xerr[-1,para_layer:], yerr=yerr[-1,para_layer:], marker='o', mfc='royalblue', ecolor='#FF7000', linestyle='--',ms=4,)\n",
    "            plt.show()\n",
    "            \n",
    "            for layer,acc in enumerate(y):\n",
    "                y[layer,layer+1:para_layer] = 0\n",
    "                yerr[layer,layer+1:para_layer] = 0\n",
    "                \n",
    "            \n",
    "            result = pd.DataFrame(columns=columns)\n",
    "            avg = pd.concat([pd.DataFrame([[data,model, \"avg\"]]*para_layer),pd.DataFrame(y*100),pd.DataFrame(x)],axis=1)\n",
    "            avg.columns = columns\n",
    "            std = pd.concat([pd.DataFrame([[data,model, \"std\"]]*para_layer),pd.DataFrame(yerr*100),pd.DataFrame(xerr)],axis=1)\n",
    "            std.columns = columns\n",
    "            \n",
    "            total_result = pd.concat([total_result,avg,std])\n",
    "            total_result.to_csv(\"result/_csv/text_side_lbl.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdea8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import math\n",
    "def confidence(pred, type=\"max\"):\n",
    "    ### shannon entropy as confidence\n",
    "    if type==\"entropy\":\n",
    "        return torch.sum(torch.special.entr(pred),dim=-1) / math.log(pred.size(-1))\n",
    "    ### max pred value as confidence\n",
    "    elif type==\"max\":\n",
    "        return torch.max(pred,dim=-1)[0]\n",
    "    else:\n",
    "        raise ValueError(\"Confidence type not supported: {}\".format(type))\n",
    "\n",
    "\n",
    "input = torch.randn(1024, 100)\n",
    "\n",
    "ep_train_start_time = time.process_time()\n",
    "confidence(input,type=\"max\")\n",
    "torch.cuda.synchronize()\n",
    "print(time.process_time()-ep_train_start_time)\n",
    "\n",
    "ep_train_start_time = time.process_time()\n",
    "confidence(input,type=\"entropy\")\n",
    "torch.cuda.synchronize()\n",
    "print(time.process_time()-ep_train_start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b83fe3",
   "metadata": {},
   "source": [
    "### lbl load log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46d37d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 150, 13)\n",
      "(4, 150, 13)\n",
      "(4, 150, 13)\n",
      "(4, 150, 13)\n",
      "(4, 150, 13)\n",
      "(4, 150, 13)\n",
      "(4, 150, 13)\n",
      "(4, 150, 13)\n",
      "(4, 150, 13)\n",
      "(4, 150, 13)\n",
      "(4, 150, 13)\n",
      "(4, 150, 13)\n",
      "(4, 150, 13)\n",
      "(4, 150, 13)\n",
      "(4, 150, 13)\n",
      "(4, 150, 13)\n",
      "(4, 150, 13)\n",
      "(4, 150, 13)\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from adjustText import adjust_text  \n",
    "total_result = pd.DataFrame(columns=[\"dataset\",\"model\", \"layer\"]\n",
    "                                    +[f'L{i}_acc' for i in range(para_layer)]\n",
    "                                    +[f't0.{i}_acc' for i in range(1,10)])\n",
    "for data in [\"cifar10\",\"cifar100\",\"tinyImageNet\",]:\n",
    "    for model in [\"VGG_AL\",\"resnet_AL\",]:\n",
    "        layer = 4\n",
    "        epoch = 600\n",
    "        lr = 0.0001\n",
    "        aug_type = \"strong\"\n",
    "        save_path = f\"ckpt/{data}_{model}_l{layer}_{aug_type}_lbl500_pre/\"\n",
    "        out_path = f\"result/{data}_{model}_l{layer}_{aug_type}_lbl500_pre/\"\n",
    "        \n",
    "        log_list = os.listdir(out_path)\n",
    "        #print(\"files: \",log_list)\n",
    "        avg_acc = []\n",
    "        avg_time = []\n",
    "        avg_distr = []\n",
    "        avg_best_acc = []\n",
    "        avg_best_ep = []\n",
    "        \n",
    "        for log in log_list:\n",
    "            para_layer = 4\n",
    "            para_thres = 9\n",
    "            para_size = para_layer+para_thres\n",
    "            with open(out_path+log,mode='r') as log:\n",
    "                buffer = log.readlines()\n",
    "                list_best_acc=[]\n",
    "                list_ep_valid_acc=[]\n",
    "                list_ep_valid_time=[]\n",
    "                list_ep_valid_distr=[]\n",
    "                \n",
    "                for line in buffer:\n",
    "                    match = re.match('Test Epoch\\d+ layer\\d+ Acc (.)*', line)\n",
    "                    if match!=None:\n",
    "                        match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                        list_ep_valid_acc.append(float(match[0]))\n",
    "                    match = re.match('ep\\d+_l\\d+_test_time(.)*', line)\n",
    "                    if match!=None:\n",
    "                        match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                        list_ep_valid_time.append(float(match[0]))      \n",
    "                          \n",
    "                    match = re.match(\"Test threshold \\d+\\.\\d+ Acc(.)*\", line)\n",
    "                    if match!=None:\n",
    "                        match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                        list_ep_valid_acc.append(float(match[1]))\n",
    "                    match = re.match('t\\d+\\.\\d+_test_time(.)*', line)\n",
    "                    if match!=None:\n",
    "                        match = re.findall(\"\\d+\\.\\d+\",match.group(0))\n",
    "                        list_ep_valid_time.append(float(match[1]))   \n",
    "                    match = re.match('Best AUC(.)*', line)\n",
    "                    if match!=None:\n",
    "                        match = re.findall(\"[0-9]+\",match.group(0))\n",
    "                        \n",
    "                        best_ep = int(match[-1])\n",
    "                        list_best_acc.append(best_ep)\n",
    "                    match = re.match('data_distribution(.)*', line)\n",
    "                    if match!=None:\n",
    "                        match = re.findall(\"\\d+\",match.group(0))  \n",
    "                        list_ep_valid_distr.append(match)\n",
    "                    match = re.match('Best AUC(.)*', line)\n",
    "\n",
    "                       \n",
    "                list_ep_valid_acc = np.array(list_ep_valid_acc).reshape((-1,150,13))*100\n",
    "\n",
    "                print(list_ep_valid_acc.shape)\n",
    "                for layer in range(4):\n",
    "                    result = pd.DataFrame(columns=[\"dataset\",\"model\", \"layer\"]\n",
    "                                    +[f'L{i}_acc' for i in range(para_layer)]\n",
    "                                    +[f't0.{i}_acc' for i in range(1,10)])\n",
    "                    result.loc[0,[f'L{i}_acc' for i in range(para_layer)]+[f't0.{i}_acc' for i in range(1,10)]] = list_ep_valid_acc[layer,list_best_acc[layer],:]\n",
    "                    result.loc[0,[\"dataset\",\"model\", \"layer\"]] = [data,model, layer]\n",
    "                    total_result = pd.concat([total_result,result],axis=0,ignore_index=True)\n",
    "        \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53ea3d7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>L0_acc</th>\n",
       "      <th>L1_acc</th>\n",
       "      <th>L2_acc</th>\n",
       "      <th>L3_acc</th>\n",
       "      <th>t0.1_acc</th>\n",
       "      <th>t0.2_acc</th>\n",
       "      <th>t0.3_acc</th>\n",
       "      <th>t0.4_acc</th>\n",
       "      <th>t0.5_acc</th>\n",
       "      <th>t0.6_acc</th>\n",
       "      <th>t0.7_acc</th>\n",
       "      <th>t0.8_acc</th>\n",
       "      <th>t0.9_acc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset</th>\n",
       "      <th>model</th>\n",
       "      <th>layer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">cifar10</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">VGG_AL</th>\n",
       "      <th>0</th>\n",
       "      <td>81.840000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>81.840000</td>\n",
       "      <td>81.840000</td>\n",
       "      <td>81.840000</td>\n",
       "      <td>81.840000</td>\n",
       "      <td>81.840000</td>\n",
       "      <td>81.840000</td>\n",
       "      <td>81.840000</td>\n",
       "      <td>81.840000</td>\n",
       "      <td>81.840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>83.780000</td>\n",
       "      <td>91.306667</td>\n",
       "      <td>9.996667</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>87.206667</td>\n",
       "      <td>87.433333</td>\n",
       "      <td>87.643333</td>\n",
       "      <td>87.813333</td>\n",
       "      <td>87.910000</td>\n",
       "      <td>88.040000</td>\n",
       "      <td>88.196667</td>\n",
       "      <td>88.363333</td>\n",
       "      <td>88.603333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84.766667</td>\n",
       "      <td>91.470000</td>\n",
       "      <td>91.580000</td>\n",
       "      <td>9.213333</td>\n",
       "      <td>88.070000</td>\n",
       "      <td>88.336667</td>\n",
       "      <td>88.606667</td>\n",
       "      <td>88.740000</td>\n",
       "      <td>88.886667</td>\n",
       "      <td>89.020000</td>\n",
       "      <td>89.120000</td>\n",
       "      <td>89.350000</td>\n",
       "      <td>89.593333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>85.580000</td>\n",
       "      <td>91.640000</td>\n",
       "      <td>91.646667</td>\n",
       "      <td>91.806667</td>\n",
       "      <td>88.673333</td>\n",
       "      <td>88.940000</td>\n",
       "      <td>89.130000</td>\n",
       "      <td>89.306667</td>\n",
       "      <td>89.433333</td>\n",
       "      <td>89.586667</td>\n",
       "      <td>89.673333</td>\n",
       "      <td>89.833333</td>\n",
       "      <td>90.096667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">resnet_AL</th>\n",
       "      <th>0</th>\n",
       "      <td>84.126667</td>\n",
       "      <td>9.920000</td>\n",
       "      <td>9.980000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>84.126667</td>\n",
       "      <td>84.126667</td>\n",
       "      <td>84.126667</td>\n",
       "      <td>84.126667</td>\n",
       "      <td>84.126667</td>\n",
       "      <td>84.126667</td>\n",
       "      <td>84.126667</td>\n",
       "      <td>84.126667</td>\n",
       "      <td>84.126667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85.383333</td>\n",
       "      <td>89.236667</td>\n",
       "      <td>10.013333</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>87.520000</td>\n",
       "      <td>87.633333</td>\n",
       "      <td>87.703333</td>\n",
       "      <td>87.770000</td>\n",
       "      <td>87.800000</td>\n",
       "      <td>87.840000</td>\n",
       "      <td>87.866667</td>\n",
       "      <td>87.920000</td>\n",
       "      <td>88.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>86.266667</td>\n",
       "      <td>89.626667</td>\n",
       "      <td>89.333333</td>\n",
       "      <td>10.773333</td>\n",
       "      <td>88.176667</td>\n",
       "      <td>88.303333</td>\n",
       "      <td>88.323333</td>\n",
       "      <td>88.323333</td>\n",
       "      <td>88.340000</td>\n",
       "      <td>88.396667</td>\n",
       "      <td>88.480000</td>\n",
       "      <td>88.550000</td>\n",
       "      <td>88.656667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>87.050000</td>\n",
       "      <td>89.800000</td>\n",
       "      <td>89.770000</td>\n",
       "      <td>89.473333</td>\n",
       "      <td>88.656667</td>\n",
       "      <td>88.783333</td>\n",
       "      <td>88.833333</td>\n",
       "      <td>88.836667</td>\n",
       "      <td>88.873333</td>\n",
       "      <td>88.933333</td>\n",
       "      <td>88.966667</td>\n",
       "      <td>89.003333</td>\n",
       "      <td>89.136667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">cifar100</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">VGG_AL</th>\n",
       "      <th>0</th>\n",
       "      <td>57.896667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>57.896667</td>\n",
       "      <td>57.896667</td>\n",
       "      <td>57.896667</td>\n",
       "      <td>57.896667</td>\n",
       "      <td>57.896667</td>\n",
       "      <td>57.896667</td>\n",
       "      <td>57.896667</td>\n",
       "      <td>57.896667</td>\n",
       "      <td>57.896667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61.990000</td>\n",
       "      <td>70.663333</td>\n",
       "      <td>1.013333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>68.670000</td>\n",
       "      <td>68.873333</td>\n",
       "      <td>69.016667</td>\n",
       "      <td>69.076667</td>\n",
       "      <td>69.180000</td>\n",
       "      <td>69.220000</td>\n",
       "      <td>69.313333</td>\n",
       "      <td>69.463333</td>\n",
       "      <td>69.546667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62.383333</td>\n",
       "      <td>70.240000</td>\n",
       "      <td>71.650000</td>\n",
       "      <td>1.136667</td>\n",
       "      <td>69.653333</td>\n",
       "      <td>69.850000</td>\n",
       "      <td>69.990000</td>\n",
       "      <td>70.053333</td>\n",
       "      <td>70.110000</td>\n",
       "      <td>70.186667</td>\n",
       "      <td>70.300000</td>\n",
       "      <td>70.400000</td>\n",
       "      <td>70.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>62.540000</td>\n",
       "      <td>70.186667</td>\n",
       "      <td>71.220000</td>\n",
       "      <td>72.050000</td>\n",
       "      <td>69.950000</td>\n",
       "      <td>70.110000</td>\n",
       "      <td>70.200000</td>\n",
       "      <td>70.316667</td>\n",
       "      <td>70.410000</td>\n",
       "      <td>70.530000</td>\n",
       "      <td>70.596667</td>\n",
       "      <td>70.736667</td>\n",
       "      <td>70.886667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">resnet_AL</th>\n",
       "      <th>0</th>\n",
       "      <td>52.433333</td>\n",
       "      <td>0.956667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>52.433333</td>\n",
       "      <td>52.433333</td>\n",
       "      <td>52.433333</td>\n",
       "      <td>52.433333</td>\n",
       "      <td>52.433333</td>\n",
       "      <td>52.433333</td>\n",
       "      <td>52.433333</td>\n",
       "      <td>52.433333</td>\n",
       "      <td>52.433333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58.856667</td>\n",
       "      <td>66.240000</td>\n",
       "      <td>0.963333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>64.580000</td>\n",
       "      <td>64.753333</td>\n",
       "      <td>64.863333</td>\n",
       "      <td>64.976667</td>\n",
       "      <td>65.026667</td>\n",
       "      <td>65.080000</td>\n",
       "      <td>65.153333</td>\n",
       "      <td>65.286667</td>\n",
       "      <td>65.366667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59.880000</td>\n",
       "      <td>66.190000</td>\n",
       "      <td>67.173333</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>66.070000</td>\n",
       "      <td>66.290000</td>\n",
       "      <td>66.383333</td>\n",
       "      <td>66.496667</td>\n",
       "      <td>66.600000</td>\n",
       "      <td>66.690000</td>\n",
       "      <td>66.826667</td>\n",
       "      <td>66.910000</td>\n",
       "      <td>66.986667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60.236667</td>\n",
       "      <td>66.230000</td>\n",
       "      <td>67.143333</td>\n",
       "      <td>65.256667</td>\n",
       "      <td>65.966667</td>\n",
       "      <td>66.040000</td>\n",
       "      <td>66.143333</td>\n",
       "      <td>66.223333</td>\n",
       "      <td>66.250000</td>\n",
       "      <td>66.253333</td>\n",
       "      <td>66.320000</td>\n",
       "      <td>66.326667</td>\n",
       "      <td>66.366667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">tinyImageNet</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">VGG_AL</th>\n",
       "      <th>0</th>\n",
       "      <td>38.050000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>38.050000</td>\n",
       "      <td>38.050000</td>\n",
       "      <td>38.050000</td>\n",
       "      <td>38.050000</td>\n",
       "      <td>38.050000</td>\n",
       "      <td>38.050000</td>\n",
       "      <td>38.050000</td>\n",
       "      <td>38.050000</td>\n",
       "      <td>38.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38.583333</td>\n",
       "      <td>46.246667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>45.386667</td>\n",
       "      <td>45.523333</td>\n",
       "      <td>45.570000</td>\n",
       "      <td>45.676667</td>\n",
       "      <td>45.693333</td>\n",
       "      <td>45.763333</td>\n",
       "      <td>45.796667</td>\n",
       "      <td>45.793333</td>\n",
       "      <td>45.810000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38.396667</td>\n",
       "      <td>45.940000</td>\n",
       "      <td>47.950000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>47.216667</td>\n",
       "      <td>47.280000</td>\n",
       "      <td>47.380000</td>\n",
       "      <td>47.426667</td>\n",
       "      <td>47.426667</td>\n",
       "      <td>47.480000</td>\n",
       "      <td>47.476667</td>\n",
       "      <td>47.550000</td>\n",
       "      <td>47.593333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38.426667</td>\n",
       "      <td>45.646667</td>\n",
       "      <td>47.730000</td>\n",
       "      <td>46.293333</td>\n",
       "      <td>46.936667</td>\n",
       "      <td>47.090000</td>\n",
       "      <td>47.113333</td>\n",
       "      <td>47.193333</td>\n",
       "      <td>47.266667</td>\n",
       "      <td>47.303333</td>\n",
       "      <td>47.330000</td>\n",
       "      <td>47.336667</td>\n",
       "      <td>47.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">resnet_AL</th>\n",
       "      <th>0</th>\n",
       "      <td>33.160000</td>\n",
       "      <td>0.503333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>33.160000</td>\n",
       "      <td>33.160000</td>\n",
       "      <td>33.160000</td>\n",
       "      <td>33.160000</td>\n",
       "      <td>33.160000</td>\n",
       "      <td>33.160000</td>\n",
       "      <td>33.160000</td>\n",
       "      <td>33.160000</td>\n",
       "      <td>33.160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>35.490000</td>\n",
       "      <td>40.203333</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>40.006667</td>\n",
       "      <td>40.036667</td>\n",
       "      <td>40.103333</td>\n",
       "      <td>40.100000</td>\n",
       "      <td>40.110000</td>\n",
       "      <td>40.130000</td>\n",
       "      <td>40.150000</td>\n",
       "      <td>40.156667</td>\n",
       "      <td>40.163333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36.336667</td>\n",
       "      <td>40.206667</td>\n",
       "      <td>42.760000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>42.726667</td>\n",
       "      <td>42.740000</td>\n",
       "      <td>42.780000</td>\n",
       "      <td>42.766667</td>\n",
       "      <td>42.816667</td>\n",
       "      <td>42.830000</td>\n",
       "      <td>42.890000</td>\n",
       "      <td>42.926667</td>\n",
       "      <td>42.970000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36.346667</td>\n",
       "      <td>40.466667</td>\n",
       "      <td>42.846667</td>\n",
       "      <td>41.973333</td>\n",
       "      <td>42.933333</td>\n",
       "      <td>43.026667</td>\n",
       "      <td>43.016667</td>\n",
       "      <td>42.963333</td>\n",
       "      <td>42.936667</td>\n",
       "      <td>42.906667</td>\n",
       "      <td>42.920000</td>\n",
       "      <td>42.983333</td>\n",
       "      <td>43.046667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 L0_acc     L1_acc     L2_acc     L3_acc  \\\n",
       "dataset      model     layer                                               \n",
       "cifar10      VGG_AL    0      81.840000  10.000000  10.000000  10.000000   \n",
       "                       1      83.780000  91.306667   9.996667  10.000000   \n",
       "                       2      84.766667  91.470000  91.580000   9.213333   \n",
       "                       3      85.580000  91.640000  91.646667  91.806667   \n",
       "             resnet_AL 0      84.126667   9.920000   9.980000  10.000000   \n",
       "                       1      85.383333  89.236667  10.013333  10.000000   \n",
       "                       2      86.266667  89.626667  89.333333  10.773333   \n",
       "                       3      87.050000  89.800000  89.770000  89.473333   \n",
       "cifar100     VGG_AL    0      57.896667   1.000000   1.000000   1.000000   \n",
       "                       1      61.990000  70.663333   1.013333   1.000000   \n",
       "                       2      62.383333  70.240000  71.650000   1.136667   \n",
       "                       3      62.540000  70.186667  71.220000  72.050000   \n",
       "             resnet_AL 0      52.433333   0.956667   1.000000   1.000000   \n",
       "                       1      58.856667  66.240000   0.963333   1.000000   \n",
       "                       2      59.880000  66.190000  67.173333   0.860000   \n",
       "                       3      60.236667  66.230000  67.143333  65.256667   \n",
       "tinyImageNet VGG_AL    0      38.050000   0.500000   0.500000   0.500000   \n",
       "                       1      38.583333  46.246667   0.500000   0.500000   \n",
       "                       2      38.396667  45.940000  47.950000   0.500000   \n",
       "                       3      38.426667  45.646667  47.730000  46.293333   \n",
       "             resnet_AL 0      33.160000   0.503333   0.500000   0.500000   \n",
       "                       1      35.490000  40.203333   0.450000   0.500000   \n",
       "                       2      36.336667  40.206667  42.760000   0.500000   \n",
       "                       3      36.346667  40.466667  42.846667  41.973333   \n",
       "\n",
       "                               t0.1_acc   t0.2_acc   t0.3_acc   t0.4_acc  \\\n",
       "dataset      model     layer                                               \n",
       "cifar10      VGG_AL    0      81.840000  81.840000  81.840000  81.840000   \n",
       "                       1      87.206667  87.433333  87.643333  87.813333   \n",
       "                       2      88.070000  88.336667  88.606667  88.740000   \n",
       "                       3      88.673333  88.940000  89.130000  89.306667   \n",
       "             resnet_AL 0      84.126667  84.126667  84.126667  84.126667   \n",
       "                       1      87.520000  87.633333  87.703333  87.770000   \n",
       "                       2      88.176667  88.303333  88.323333  88.323333   \n",
       "                       3      88.656667  88.783333  88.833333  88.836667   \n",
       "cifar100     VGG_AL    0      57.896667  57.896667  57.896667  57.896667   \n",
       "                       1      68.670000  68.873333  69.016667  69.076667   \n",
       "                       2      69.653333  69.850000  69.990000  70.053333   \n",
       "                       3      69.950000  70.110000  70.200000  70.316667   \n",
       "             resnet_AL 0      52.433333  52.433333  52.433333  52.433333   \n",
       "                       1      64.580000  64.753333  64.863333  64.976667   \n",
       "                       2      66.070000  66.290000  66.383333  66.496667   \n",
       "                       3      65.966667  66.040000  66.143333  66.223333   \n",
       "tinyImageNet VGG_AL    0      38.050000  38.050000  38.050000  38.050000   \n",
       "                       1      45.386667  45.523333  45.570000  45.676667   \n",
       "                       2      47.216667  47.280000  47.380000  47.426667   \n",
       "                       3      46.936667  47.090000  47.113333  47.193333   \n",
       "             resnet_AL 0      33.160000  33.160000  33.160000  33.160000   \n",
       "                       1      40.006667  40.036667  40.103333  40.100000   \n",
       "                       2      42.726667  42.740000  42.780000  42.766667   \n",
       "                       3      42.933333  43.026667  43.016667  42.963333   \n",
       "\n",
       "                               t0.5_acc   t0.6_acc   t0.7_acc   t0.8_acc  \\\n",
       "dataset      model     layer                                               \n",
       "cifar10      VGG_AL    0      81.840000  81.840000  81.840000  81.840000   \n",
       "                       1      87.910000  88.040000  88.196667  88.363333   \n",
       "                       2      88.886667  89.020000  89.120000  89.350000   \n",
       "                       3      89.433333  89.586667  89.673333  89.833333   \n",
       "             resnet_AL 0      84.126667  84.126667  84.126667  84.126667   \n",
       "                       1      87.800000  87.840000  87.866667  87.920000   \n",
       "                       2      88.340000  88.396667  88.480000  88.550000   \n",
       "                       3      88.873333  88.933333  88.966667  89.003333   \n",
       "cifar100     VGG_AL    0      57.896667  57.896667  57.896667  57.896667   \n",
       "                       1      69.180000  69.220000  69.313333  69.463333   \n",
       "                       2      70.110000  70.186667  70.300000  70.400000   \n",
       "                       3      70.410000  70.530000  70.596667  70.736667   \n",
       "             resnet_AL 0      52.433333  52.433333  52.433333  52.433333   \n",
       "                       1      65.026667  65.080000  65.153333  65.286667   \n",
       "                       2      66.600000  66.690000  66.826667  66.910000   \n",
       "                       3      66.250000  66.253333  66.320000  66.326667   \n",
       "tinyImageNet VGG_AL    0      38.050000  38.050000  38.050000  38.050000   \n",
       "                       1      45.693333  45.763333  45.796667  45.793333   \n",
       "                       2      47.426667  47.480000  47.476667  47.550000   \n",
       "                       3      47.266667  47.303333  47.330000  47.336667   \n",
       "             resnet_AL 0      33.160000  33.160000  33.160000  33.160000   \n",
       "                       1      40.110000  40.130000  40.150000  40.156667   \n",
       "                       2      42.816667  42.830000  42.890000  42.926667   \n",
       "                       3      42.936667  42.906667  42.920000  42.983333   \n",
       "\n",
       "                               t0.9_acc  \n",
       "dataset      model     layer             \n",
       "cifar10      VGG_AL    0      81.840000  \n",
       "                       1      88.603333  \n",
       "                       2      89.593333  \n",
       "                       3      90.096667  \n",
       "             resnet_AL 0      84.126667  \n",
       "                       1      88.000000  \n",
       "                       2      88.656667  \n",
       "                       3      89.136667  \n",
       "cifar100     VGG_AL    0      57.896667  \n",
       "                       1      69.546667  \n",
       "                       2      70.600000  \n",
       "                       3      70.886667  \n",
       "             resnet_AL 0      52.433333  \n",
       "                       1      65.366667  \n",
       "                       2      66.986667  \n",
       "                       3      66.366667  \n",
       "tinyImageNet VGG_AL    0      38.050000  \n",
       "                       1      45.810000  \n",
       "                       2      47.593333  \n",
       "                       3      47.300000  \n",
       "             resnet_AL 0      33.160000  \n",
       "                       1      40.163333  \n",
       "                       2      42.970000  \n",
       "                       3      43.046667  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_result.groupby([\"dataset\",\"model\", \"layer\"]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf24216",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
